{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Linear Regression Model\"\n",
        "subtitle: \"Statistical Inference and Extensions\"\n",
        "author: Fernando Rios-Avila\n",
        "format:\n",
        "  revealjs: \n",
        "    slide-number: true\n",
        "    width: 1600\n",
        "    height: 900\n",
        "    code-fold: true\n",
        "    echo: true\n",
        "    css: styles.css  \n",
        "  pdf: default  \n",
        "execute:\n",
        "  freeze: true \n",
        "---\n",
        "\n",
        "\n",
        "## Introduction {.scrollable}\n",
        "\n",
        "-   Linear Regression is the most basic, and still most useful, tool for analyzing data.\n",
        "\n",
        "-   The goal is to find what the relationship between the outcome $y$ and explanatory variables $X's$ is.\n",
        "\n",
        "-   Say that we start with a very simple \"***model***\" that states tries to describe the population function as the following:\n",
        "\n",
        "$$\n",
        "y = h(X,\\varepsilon)\n",
        "$$\n",
        "\n",
        "Here, $X$ represents a set of observed covariates and $\\varepsilon$ the set of unobserved characteristics, with no no pre-defined relationship between these components.\n",
        "\n",
        "-   For now, we will make standard exogeneity assumptions for the identification of the model\n",
        "\n",
        "## Estimation {.scrollable}\n",
        "\n",
        "-   The functional form is unknowable. However, under the ***small*** assumption of Exogeneity of $X$, we could instead consider the Conditional Expectation function (CEF):\n",
        "\n",
        "$$\n",
        "E(y_i|X_i=x) = \\int y f_{y|x}(y)dy\n",
        "$$\n",
        "\n",
        "-   This implies a fully **non-parametric** estimation of the Linear function.\n",
        "\n",
        "-   With this, the outcome $y$ can be decomposed into factors determined by observed characteristics (CEF) and on the error $\\varepsilon$.\n",
        "\n",
        "$$\n",
        "y = E(y|X) + \\varepsilon\n",
        "$$\n",
        "\n",
        "\n",
        "##   \n",
        "\n",
        "-   The CEF is a convenient abstract, but to estimate it, we require assumptions. (Recall the assumptions for unbiased OLS?)\n",
        "\n",
        "-   Namely, we need to impose a linearity assumption, namely:\n",
        "\n",
        "$$\n",
        "E(y_i|X_i=x) = \\beta_0 + \\beta_1 x_1 +\\beta_2 x_2 + ... + \n",
        "\\beta_k x_k = X_i'\\beta \n",
        "$$\n",
        "\n",
        "-   And the solution for $\\beta$ is given by:\n",
        "\n",
        "$$\n",
        "\\beta = \\underset{b}{arg} \\ E(L(y_i-X'_i b))\n",
        "$$\n",
        "\n",
        "Where the loss function $L(x)=x^2$. (Square loss function)\n",
        "\n",
        "-   This implies the following condition:\n",
        "$E[X_i (y_i-X_i'b)]=0 \\rightarrow \\beta = E[X_i'X_i]^{-1}E[X_i'y_i]$\n",
        "\n",
        "\n",
        "## Mata: OLS Estimator {.scrollable}\n",
        "\n",
        "The estimator using Sample equivalents become:\n",
        "\n",
        "$$\n",
        "\\hat \\beta = \n",
        "\\left(\\frac{1}{N} \\sum_i X_i'X_i \\right)^{-1}\n",
        "\\frac{1}{N} \\sum_i X_i'y_i=(X'X)^{-1}X'y\n",
        "$$\n"
      ],
      "id": "43a47ef8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "frause oaxaca, clear\n",
        "keep if lnwage !=.\n",
        "mata:\n",
        "  y = st_data(.,\"lnwage\")\n",
        "  n = rows(y)\n",
        "  x = st_data(.,\"female age educ\"),J(n,1,1)\n",
        "  exx = cross(x,x)/n\n",
        "  exy = cross(x,y)/n\n",
        "  b   = invsym(exx)*exy\n",
        "  b\n",
        "end  "
      ],
      "id": "7babd3f3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference - Distribution of $\\beta's$  \n",
        "\n",
        "Given the model and OLS estimator:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "y &= X\\beta + \\varepsilon \\\\\n",
        "\\hat \\beta &= (X'X)^{-1}X'y\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "If we substitute $y$ in the second equation, we get:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\hat \\beta &= (X'X)^{-1}X'( X\\beta + \\varepsilon) \\\\\n",
        "\\hat \\beta &= \\beta + (X'X)^{-1}X'\\varepsilon) \\\\\n",
        "\\hat \\beta - \\beta &=  (X'X)^{-1}X'\\varepsilon) \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "##\n",
        "\n",
        "Finally:\n",
        "$$\n",
        "\\sqrt N (\\hat\\beta - \\beta) = {\\sqrt N}\\Big[\\frac{1}{N}\\sum (X_iX_i')\\Big]^{-1} \\frac{1}{N}\\sum(X_i\\varepsilon_i)\n",
        "$$\n",
        "\n",
        "- Here $\\varepsilon$ is the true population error. $\\hat\\beta$ is unbiased if the second term has an expectation of Zero. (the error is independent from $X$).\n",
        "\n",
        "-  The first term is assumed fixed $E(X_i X_i')$. And, because $E(X_i\\varepsilon)=0$, and $\\frac{1}{\\sqrt N} \\sum(X_i\\varepsilon)$ is normalized, by CLT we have that:\n",
        "\n",
        "$$\n",
        "\\sqrt N (\\hat\\beta-\\beta)\\sim N(0,E(X_iX_i')^{-1} \\ E(X_iX_i'\\varepsilon_i ^2) \\ E(X_iX_i')^{-1})\n",
        "$$\n",
        "\n",
        "-   From here, the main question is : How do we estimate $E(X_iX'\\varepsilon_i^2)$?\n",
        "\n",
        "## Inference: Estimating SE {.scrollable}\n",
        "\n",
        "-   Lets First Rewrite the last expression:\n",
        "\n",
        "$$\n",
        "Var(\\hat\\beta)=(X'X)^{-1} X'\\Omega X (X'X)^{-1}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$\n",
        "\\Omega=\n",
        "\\left( \n",
        "\\begin{matrix}\n",
        "\\sigma_1^2 & \\sigma_{12} &  ... & \\sigma_{1N}\\\\\n",
        "\\sigma_{21} & \\sigma_{2}^2 & ... & \\sigma_{2N} \\\\\n",
        "...&...&...&...\\\\\n",
        "\\sigma_{N1} & \\sigma_{N2} & ... & \\sigma_{NN}^2 \\\\\n",
        "\\end{matrix}\n",
        "\\right)\n",
        "$$\n",
        "\n",
        "In other words, the variance of $\\hat\\beta$ allows for arbitrary relationship among the errors, as well as heteroskedasticity. This, however is impossible to estimate!, thus we require assumptions\n",
        "\n",
        "## Homoskedasticity and independent samples {.scrollable}\n",
        "\n",
        "With homoskedastic errors $\\sigma^2 = \\sigma_i^2 \\ \\forall i \\in 1,...,N$ . \n",
        "\n",
        "With independent samples $\\sigma_{ij}=0 \\ \\forall \\ i\\neq j$ . \n",
        "$$\n",
        "\\Omega_00=\n",
        "\\left( \n",
        "\\begin{matrix}\n",
        "\\sigma_1^2 & \\sigma_{12} &  ... & \\sigma_{1N}\\\\\n",
        "\\sigma_{21} & \\sigma_{2}^2 & ... & \\sigma_{2N} \\\\\n",
        "...&...&...&...\\\\\n",
        "\\sigma_{N1} & \\sigma_{N2} & ... & \\sigma_{NN}^2 \\\\\n",
        "\\end{matrix}\n",
        "\\right)=I(N)*\\sigma^2 \n",
        "$$\n",
        "\n",
        "Thus \n",
        "$$\n",
        "\\begin{aligned}\n",
        "Var(\\hat\\beta)_{00} &=(X'X)^{-1} X'I(N)\\sigma^2 X (X'X)^{-1} =\\sigma^2 (X'X)^{-1} \\\\\n",
        "\\sigma^2 &= E(\\varepsilon^2)\n",
        "\\end{aligned}\n",
        "$$\n"
      ],
      "id": "956fafad"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| classes: larger\n",
        "mata: e=err = y:-x*b\n",
        "mata: var_b_000 = mean(err:^2) * invsym(x'x)\n",
        "mata: b,sqrt(diagonal(var_b_000))"
      ],
      "id": "02d4dd43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##\n",
        "\n",
        "But, $\\sigma^2$ is not known, so we have to use $\\hat\\sigma^2$ instead, which depends on the sample residuals: \n",
        "$$\n",
        "\\hat\\sigma^2 = \\frac{1}{N-k-1}\\sum \\hat e^2\n",
        "$$ \n",
        "\n",
        "Where we account for the fact true errors are not observed, but rather residuals are estimated, adjusting the degrees of freedom.\n"
      ],
      "id": "4719e502"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| classes: larger\n",
        "\n",
        "mata:\n",
        "    N = rows(y); k = cols(x)\n",
        "    var_b_00 = sum(err:^2)/(N-k) * invsym(x'x)\n",
        "    b,sqrt(diagonal(var_b_00))\n",
        "end"
      ],
      "id": "0e62cb8e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lifting Assumptions: Heteroscedasticity {.scrollable}\n",
        "\n",
        "-   We start by lifting this assumption, which implies the following:\n",
        "\n",
        "$$\n",
        "\\sigma^2_i \\neq \\sigma^2_j \\  \\forall \\ i\\neq j\n",
        "$$ \n",
        "\n",
        "But to estimate this, we need an approximation for $\\sigma^2_i = E(\\varepsilon_i^2) = \\varepsilon_i^2$.\n",
        "\n",
        "-   With this, we can obtain what is known as th White or Eicker-White or Heteroskedasiticy Robust Standard errors.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "Var(\\hat\\beta)_{0} &= (X'X)^{-1} (X \\cdot \\hat e)'(X \\cdot  \\hat e) (X'X)^{-1} \\\\\n",
        " &=(X'X)^{-1} \\sum(X_iX_i'\\hat e^2) (X'X)^{-1}\n",
        "\\end{aligned}\n",
        "$$ \n",
        "\n",
        "Which imposes **NO** penalty to the fact that we are using residuals not errors. If we account for that however, we obtain what is known as HC1, SE, the standard in `Stata`. (when you type `robust`) \n",
        "\n",
        "$$\n",
        "Var(\\hat\\beta)_{1}=\\frac{N}{N-K-1}Var(\\hat\\beta)_{0}\n",
        "$$\n"
      ],
      "id": "e89406e1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| classes: larger\n",
        "\n",
        "mata:\n",
        "    ixx = invsym(x'x)\n",
        "    var_b_0 = ixx * (x:*e)'(x:*e) * ixx\n",
        "    var_b_1 = N/(N-k)*var_b_0\n",
        "    b,sqrt(diagonal(var_b_0)),sqrt(diagonal(var_b_1))\n",
        "end"
      ],
      "id": "b2b4d4b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## But error is not the same as residual! {.scrollable}\n",
        "\n",
        "A residual is model dependent, and should not be confused with the model error $\\hat \\varepsilon \\neq \\varepsilon$. Because of this, additional corrections are needed to obtained unbiased $var(\\hat\\beta)$ estimates. (Degrees of freedom). But other options exists.\n",
        "\n",
        "Redefine the Variance Formula:\n",
        "\n",
        "$$\n",
        "Var(\\hat\\beta)=(X'X)^{-1} (\\sum X_iX_i \\psi_i )  (X'X)^{-1}\n",
        "$$ \n",
        "\n",
        "From here Mackinnon and White (1985) suggest few other options: \n",
        "$$\n",
        "\\begin{matrix}\n",
        "HC0: \\psi_i = \\hat e^2 &\n",
        "HC1: \\psi_i = \\frac{N}{N-K}  \\hat e^2 \\\\\n",
        "HC2: \\psi_i =   \\hat e^2 \\frac{1}{1-h_{ii}} &\n",
        "HC3: \\psi_i =   \\hat e^2 \\frac{1}{(1-h_{ii})^2}\n",
        "\\end{matrix}\n",
        "$$ \n",
        "\n",
        "Where $h_{ii}$ is the ith diagonal element of $X(X'X)^{-1}X'$ and allows you to see how dependent a model is to a single observation.\n",
        "\n",
        "HC2 and HC3 Standard errors are better than HC1 SE, specially when Samples are small.\n",
        "\n",
        "> NOTE: this $h_{ii}$ element is also used to measure the degrees of freedom of a model. Sum it up, and you will see!.\n",
        "\n",
        "## Coding Robust SE\n"
      ],
      "id": "61e97e7f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| classes: larger\n",
        "\n",
        "mata:\n",
        "    // h = diagonal(X invsym(X'x) X') Wrong Way, too many calculations\n",
        "    h = rowsum(x*invsym(x'x):*x)\n",
        "    psi0 = e:^2           ;   psi1 = e:^2*N/(N-k)\n",
        "    psi2 = e:^2:/(1:-h)   ;   psi3 = e:^2:/((1:-h):^2)\n",
        "    var_b_0 = ixx * cross(x,psi0,x) * ixx\n",
        "    var_b_1 = ixx * cross(x,psi1,x) * ixx\n",
        "    var_b_2 = ixx * cross(x,psi2,x) * ixx\n",
        "    var_b_3 = ixx * cross(x,psi3,x) * ixx\n",
        "    b,sqrt(diagonal(var_b_0)),sqrt(diagonal(var_b_1)),\n",
        "    sqrt(diagonal(var_b_2)),sqrt(diagonal(var_b_3))\n",
        "end  "
      ],
      "id": "cff7e290",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or in `Stata`:\n",
        "\n",
        "```stata\n",
        "regress y x1 x2 x3, vce(robust)\n",
        "regress y x1 x2 x3, vce(hc2)\n",
        "regress y x1 x2 x3, vce(hc3)\n",
        "```\n",
        "\n",
        "## GLS and Weighted Least Squares\n",
        "\n",
        "- GLS is a generalization of OLS, that could be used to address heteroskedasticity. \n",
        "- There are two ways to do this: \n",
        "    1.  Transform/weight the data to make it homoskedastic (WLS)\n",
        "    2.  Modify the variance covariance matrix of the errors (GLS)\n",
        "\n",
        "- Call $\\hat h(x)$ the predicted error variance. The GLS estimator for $V_{gls}(\\beta)$ is given by:\n",
        "\n",
        "$$V_{gls}(\\beta)=(X'X)^{-1} \\sum(X_iX_i'\\hat h(x)) (X'X)^{-1}\n",
        "$$\n",
        "\n",
        "- That way, Heteroskedasticity is addressed, but without changing the model estimates $\\beta's$\n",
        "  \n",
        "## Lifting Even more Assumptions: Correlation {.scrollable}\n",
        "\n",
        "-   One assumption we barely consider last semester was the possibility that errors could be correlated across observations. (except for time series and serial correlation)\n",
        "\n",
        "-   For example, families may share similar unobserved factors, So would people interviewed from the same classroom, cohort, city, etc. There could be many dimensions to consider possible correlations!\n",
        "\n",
        "-   In that situation, we may be missmeasuring the magnitude of the errors (probably downward), because the $\\Omega$ is no longer diagonal: $\\sigma_{ij} \\neq 0$ for some $i\\neq j$.\n",
        "\n",
        "    -   But, estimate all parameters in an NxN matrix is unfeasible. We need assumptions!\n",
        "  \n",
        "##\n",
        "### New Assumptions\n",
        "\n",
        "-   Say we have $G$ groups $g=(1â€¦G)$ . We can rewrite the expression for $\\hat\\beta$ as follows:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat\\beta-\\beta &= (X'X)^{-1}\\sum_{g=1}^G X'_g \\varepsilon_g \\\\\n",
        "&=(X'X)^{-1}\\sum_{g=1}^G s_g\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "-   We can assume that individuals are correlated within groups $E(s_g's_g) =\\Sigma_g$ , but they are uncorrelated across groups $E(s_g s_g')=0 \\ \\forall \\ g \\neq g'$ .\n",
        "  \n",
        "-   These groups are typically known as \"**clusters**\"\n",
        "\n",
        "##\n",
        "### Addressing Correlation {.scrollable}\n",
        "\n",
        "-   The idea of correcting for clusters is pretty simple. We just need to come up with an estimator for $\\Sigma_g$ for every cluster, so that:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "Var(\\hat\\beta) &= (X'X)^{-1} \\left( \\sum_{g=1}^N \\Sigma_g \\right) (X'X)^{-1} \\\\ \n",
        "\\Sigma_g &= E( X_g' \\Omega_g X_g) \n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "-   Here $\\Omega_g$ should be an approximation of the variance covariance matrix among the errors of ALL individuals that belong to the same cluster. But how do we approximate it?\n",
        "\n",
        "-   As with the EW - HC standard errors, there are many ways to estimate Clustered Standard errors. See MacKinnon et al (2023) for reference. We will refer only to the simpler ones CV0 and CV1.\n",
        "\n",
        "##\n",
        "\n",
        "-   Recall we approximate $\\sigma^2_i$ with $\\varepsilon_i^2$. Then we can approximate $\\sigma_{ij}$ with $\\varepsilon_j \\varepsilon_i$. More specifically:\n",
        "\n",
        "$$\n",
        "\\Omega_g \\simeq \\varepsilon \\varepsilon' \\ or \\ \\Sigma_g = X'_g \\varepsilon \\varepsilon' X_g = (X'_g \\varepsilon) (\\varepsilon' X_g)\n",
        "$$\n",
        "\n",
        "-   Change $\\varepsilon$ with $\\hat\\varepsilon$, do that for every group, and done! (almost).\n",
        "\n",
        "##  {.scrollable}\n",
        "\n",
        "-   As mentioned earlier, there are many CCSE (clustered consistent SE).\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "CV_0 &= (X'X)^{-1} \\sum_{g=1}^G \\hat \\Sigma_g (X'X)^{-1} \\\\\n",
        "CV_1 &= \\frac{G(N-1)}{(G-1)(N-k-1)}(X'X)^{-1} \\sum_{g=1}^G \\hat \\Sigma_g (X'X)^{-1} \n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "-   Similar to HC. CV0 does not correct for degrees of freedom. CV1, however, accounts for Degrees of freedom in the model, and clusters.\n"
      ],
      "id": "2a867770"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sort isco\n",
        "mata:\n",
        "    // 1st Sort Data (easier in Stata rather than Mata) and reload\n",
        "    y   = st_data(.,\"lnwage\")\n",
        "    x   = st_data(.,\"educ exper female\"),J(1434,1,1) \n",
        "    cvar= st_data(.,\"isco\")\n",
        "    ixx = invsym(cross(x,x)); xy = cross(x,y)\n",
        "    b   = ixx * xy\n",
        "    e   = y:-x*b\n",
        "    // Set the panel info\n",
        "    info = panelsetup(cvar,1); g=rows(info); n=rows(y)\n",
        "    // get X_g'e for all groups: \n",
        "    s_xg_e = panelsum(x:*e,info)\n",
        "    // Sum Sigma_g\n",
        "    sigma_g = s_xg_e's_xg_e\n",
        "    cv0 = ixx*sigma_g*ixx\n",
        "    cv1 =g/(g-1)*(n-1)/(n-k)*ixx*sigma_g*ixx\n",
        "    b,sqrt(diagonal(cv0)),sqrt(diagonal(cv1))\n",
        "end    "
      ],
      "id": "0c04f1f3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "or compare it to\n"
      ],
      "id": "9db00462"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reg lnwage educ exper female, cluster(isco)"
      ],
      "id": "7fd2e7b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the difference {.scrollable}\n"
      ],
      "id": "10fdea34"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clear\n",
        "set scheme white2\n",
        "color_style tableau\n",
        "set seed 1\n",
        "set obs 50\n",
        "gen r1=runiformint(1,4)\n",
        "gen r2=runiformint(1,4)\n",
        "gen id=_n\n",
        "sort r1  r2\n",
        "qui:mata:\n",
        "r1=st_data(.,\"r1\")\n",
        "r2=st_data(.,\"r2\")\n",
        "rr1=J(rows(r1)*rows(r2),4,0)\n",
        "k=0\n",
        "for(i=1;i<=50;i++){\n",
        "    for(j=1;j<=50;j++){\n",
        "        if ((r1[i]==r1[j]) | (r2[i]==r2[j])) {\n",
        "            k++\n",
        "            rr1[k,]=(51-i,j,(r1[i]==r1[j]),(r2[i]==r2[j]) )         \n",
        "        }\n",
        "    }   \n",
        "}\n",
        "rr1=rr1[1..k,]\n",
        "end\n",
        "getmata rr1*=rr1, replace force\n",
        "\n",
        "two (scatter rr11 rr12 if rr13==1,  ms(s) msize(2.1))  ///\n",
        "    (scatter rr11 rr12 if 51-rr11 == rr12, ms(s) msize(2.1) color(gs1)  ) ///\n",
        "    , aspect(1) legend(off)  xtitle(\"\") ytitle(\"\") yscale(off) xscale(off) xsize(6) ysize(6) "
      ],
      "id": "ba80edfc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Multi-way Clustering {.scrollable}\n",
        "\n",
        "::: {.panel-tabset}\n",
        "\n",
        "## First Cluster"
      ],
      "id": "4b70f572"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "two (scatter rr11 rr12 if rr13==1,  ms(s) msize(2.1))  ///\n",
        "    , aspect(1) legend(off)  xtitle(\"\") ytitle(\"\") yscale(off) xscale(off) name(m1, replace) "
      ],
      "id": "e13395dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Second Cluster"
      ],
      "id": "cb28e6ce"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "two (scatter rr11 rr12 if rr14==1,  ms(s) msize(2.1))  ///\n",
        "    , aspect(1) legend(off)  xtitle(\"\") ytitle(\"\") yscale(off) xscale(off) name(m2, replace)    "
      ],
      "id": "2759dcc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combining Clusters"
      ],
      "id": "2b6e6eb2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "two (scatter rr11 rr12 if rr14==1 | rr13==1,  ms(s) msize(2.1))  ///\n",
        "    , aspect(1) legend(off)  xtitle(\"\") ytitle(\"\") yscale(off) xscale(off) name(m3, replace)"
      ],
      "id": "f1fdd2c2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "## Beware of over-clustering {.scrollable}\n",
        "\n",
        "While clustering helps address a problem of \"intragroup\" correlation, it can/should be done with care. It is important to be aware about some unintended problems of over-clustering.\n",
        "\n",
        "1.  CV0 and CV1 work well when you have a large number of Clusters. How many? MHE(2009) says...42 (this is like having large enough samples for Asymptotic variance). If \\# clusters are small, you would do better with other approaches (including CV2 and CV3).\n",
        "\n",
        "2.  When you cluster your standard errors, you will \"most-likely\" generate larger standard errors in your model. Standard recommendation (MHE) is to cluster at the level that makes sense (based on data) and produces largest SE (to be conservative).\n",
        "\n",
        "## Role of clusters\n",
        "\n",
        "![Standard Errors](resources/clse.png)\n",
        "\n",
        "##  {.scrollable}\n",
        "\n",
        "3.  You may also consider that clustering does not work well when sample sizes within cluster are to diverse (micro vs macro clusters)\n",
        "\n",
        "4.  And there is the case where clustering is required among multiple dimensions (see `vcemway`). Where the unobserved correlation could be present in different dimensions.\n",
        "\n",
        "##\n",
        "\n",
        "So what to cluster and how?\n",
        "\n",
        "-   Mackinnon et al (2023) provides a guide on how and when to cluster your standard errors. (some are quite advanced)\n",
        "\n",
        "-   General practice, At least use Robust SE (HC2 or HC3 if sample is small), but use clustered SE for robustness.\n",
        "\n",
        "-   You may want to cluster SE based on some theoretical expectations. Choose -broader- groups for conservative analysis.\n",
        "\n",
        "-   In treatment-causal effect analysis, you may want to cluster at the \"treatment\" level.\n",
        "\n",
        "> But...Beyond hc0/1 and CV0/1 there is not much out there for correcting Standard errors in nonlinear models.\n",
        "\n",
        "# The Bootstrap\n",
        "\n",
        "## If you can't Sandwich ðŸ¥ª, you can re-Sample {.scrollable}\n",
        "\n",
        "-   The discussion above refered to the estimation of SE using $Math$. In other words, it was based on the asymptotic properties of the data. Which may not work in small samples.\n",
        "\n",
        "-   An alternative, often used by practitioners, is using re-sampling methods to obtain approximations to the coefficient distributions of interest.\n",
        "\n",
        "But... How does it work?ðŸ¤”\n",
        "\n",
        "First ask yourself, how does Asymptotic theory work (and econometrics)? ðŸ˜±\n",
        "\n",
        "> [Note: I recommend reading the -simulation- chapter in The effect, and simulation methods chapter in CT.]{.smallcaps}\n",
        "\n",
        "## A Brief Review...again ðŸ˜‡ {.scrollable}\n",
        "\n",
        "If I were to summarize most of the methodologies (ok all) we used last semester, and this one, the properties that have been derived and proofed are based on the assumption that we \"could\" always get more data (frequentist approach).\n",
        "\n",
        "There is population (or super population) from where we can get samples of data (and never repeat data).\n",
        "\n",
        "1.  We get a sample ($y,X$) (of size N)\n",
        "\n",
        "2.  Estimate our model : `method`($y,X$)$\\rightarrow$ $\\beta's$\n",
        "\n",
        "3.  Repeat to infinitum\n",
        "\n",
        "4.  Collect all $\\beta's$ and summarize. (Mean and Standard deviations)\n",
        "\n",
        "Done.\n",
        "\n",
        "The distributions you get from the above exercise should be the same as what your estimation method produces. (in average) (if not, there there is something wrong with the estimation method)\n",
        "\n",
        "##\n",
        "### But we only get 1 Sample! {.scrollable}\n",
        "\n",
        "The truth is we do not have access to multiple samples. Getting more data, is in fact, very expensive. So what to do ?\n",
        "\n",
        "-   Rely on Asymptotic theory\n",
        "\n",
        "-   learn Bayesian Econometrics ðŸ¥º\n",
        "\n",
        "-   or-resample? and do Bootstrap!\n",
        "\n",
        "##\n",
        "### Basic idea of Bootstrapping\n",
        "\n",
        "-   In the ideal scenario, you get multiple samples from your population, Estimate parameters, and done.\n",
        "\n",
        "-   If not possible you do the next best thing. You get your sample (assume is your mini-population),\n",
        "\n",
        "    -   Draw subsamples of same size (with replacement) ($y_i^s,X_i^s$)\n",
        "\n",
        "    -   estimate your model and obtain parameters $\\beta^s_i$\n",
        "\n",
        "    -   Summarize those parameters...and done, you get $Var(\\hat\\beta)$ for ðŸ†“. (or is it?)\n",
        "\n",
        "##\n",
        "### Bootstrapping {.scrollable}\n",
        "\n",
        "-   ðŸ‘¢Bootstrapping is a methodology that allows you to obtain empirical estimations of standard errors making use of the data in hand, and without even knowing about Asymptotic theory (other than how to get means and variances).\n",
        "\n",
        "![Bootstrap Sample](resources/bss.png)\n",
        "\n",
        "-   And of course, it comes in different flavors.\n",
        "\n",
        "##\n",
        "### Standard Bootstrap: {.scrollable}\n",
        "\n",
        "-   **Non-parametric Bootstrap**: You draw subsamples from the main sample. Each observation has the same pr of being selected.\n",
        "\n",
        "    -   Easiest to implement ( `see bootstrap:`)\n",
        "\n",
        "    -   Works in almost all cases, but you may have situations when some covariates are rare.\n",
        "\n",
        "    -   Can allow for \"clusters\" using \"block bootstrapping\". \n",
        "\n",
        "##\n",
        "### Standard Bootstrap: {.scrollable}\n",
        "\n",
        "-   **Parametric Bootstrap:** You estimate your model, make assumptions of your model error.\n",
        "\n",
        "    -   You need to implement it on your own. $y^s=x\\hat b+\\tilde e$ for $\\tilde e \\sim f(\\hat \\theta)$\n",
        "\n",
        "    -   It will not work well if the assumptions of the error modeling are wrong.\n",
        "\n",
        "##\n",
        "### Standard Bootstrap: {.scrollable}\n",
        "\n",
        "-   **Residual bootstrap:** Estimate your model, obtain residuals. Re-sample residuals\n",
        "\n",
        "    -   Again, implement it on your own. $y^s = x\\hat b+\\tilde e$ for $\\tilde e \\sim {\\hat e_1 , ... , \\hat e_N}$\n",
        "    -   It depends even more on the assumptions of the error modeling.\n",
        "\n",
        "##  \n",
        "### Wild Bootstrap\n",
        "\n",
        "Then there are the more advanced (but faster) Bootstrap methods: WildBootstrap\n",
        "\n",
        "-   **UWild bootstrap**: Estimate your model, obtain residuals, and re-sample residual weights.\n",
        "\n",
        "    -   Again...on your own: $y^s = x\\hat b +\\hat e * v$ , where $v \\sim ff()$ where $ff()$ is a \"good\" distribution function. $E(v)=0 \\ \\& \\ Var(v)=1$\n",
        "    -   Re-estimate the model and obtain $\\hat \\beta's$. Repeat and summarize.\n",
        "\n",
        "    -   Actually quite flexible, and works well under heteroskedasticity!\n",
        "\n",
        "    -   It can also allow clustered standard errors. The error $v$ no longer changes by individual, but by group. It also works well with weights.\n",
        "\n",
        "##\n",
        "### Wild Bootstrap: {.scrollable}\n",
        "\n",
        "-   **UWild bootstrap-2** : Estimate your model, obtain Influence functions ðŸ˜± , and re-sample residual weights.\n",
        "\n",
        "    -   This is an extension to the previous option. But with advantages\n",
        "        -   you do not need to *re-estimate* the model. Just look into how the the mean of IF's change.\n",
        "\n",
        "        -   it can be applied to linear and nonlinear model (if you know how to build the IF's)\n",
        "  \n",
        "    -   Works well with clustered and weights.\n",
        "\n",
        "##\n",
        "### Wild Bootstrap: {.scrollable}\n",
        "\n",
        "-   **CWild bootstrap:** Similar UWild Bootstrap, Obtain Influence functions under the Null (imposing restrictions), and use that to test the NULL.\n",
        "\n",
        "    -   No, you do not need to do it on your own. `see bootest` in `Stata`.\n",
        "\n",
        "    -   Works pretty well with small samples and small \\# clusters. Probably the way to go if you really care about Standard errors.\n",
        "\n",
        "\n",
        "## How to Bootstrap? in `Stata`\n",
        "\n",
        "I have a few notes on Bootstrapping here [Bootstrapping in Stata](https://friosavila.github.io/stata_do/stata_do2.html). But let me give you the highlights for the most general case.\n",
        "\n",
        "1.  Most (if not all commands) in `Stata` allow you to obtain bootstrap standard errors, by default. see:`help [cmd]`\n",
        "\n",
        "    they usually have the following syntax:\n",
        "\n",
        "    ```stata{.larger}\n",
        "    [cmd] y x1 x2 x3, vce(bootstrap, options)\n",
        "    regress lnwage educ exper female, vce(bootstrap, reps(100))\n",
        "    ```\n",
        "\n",
        "2.  However, you can also Bootstrap that commands that do not have their own `bootstrap` option.  \n",
        "\n",
        "    ```stata{.larger} \n",
        "    bootstrap:[cmd] y x1 x2 x3, \n",
        "    bootstrap, reps(100):regress lnwage educ exper female\n",
        "    bootstrap, reps(100) cluster(isco):regress lnwage educ exper female\n",
        "    ```\n",
        "\n",
        "##  {.scrollable}\n",
        "\n",
        "3.  This last command may allow you to bootstrap multiple models at the same time, although it does require a bit of programming. (and a do file)\n"
      ],
      "id": "b4ae5b28"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "frause oaxaca, clear\n",
        "gen tchild = kids6 + kids714\n",
        "capture program drop bs_wages_children\n",
        "program bs_wages_children, eclass // eclass is for things like equations\n",
        "    ** Estimate first model\n",
        "    reg lnwage educ exper female\n",
        "    matrix b1 = e(b)\n",
        "    matrix coleq b1 = lnwage\n",
        "    ** Estimate second model\n",
        "    reg tchild educ exper female\n",
        "    matrix b2 = e(b)\n",
        "    matrix coleq b2 = tchild\n",
        "    ** Put things together and post\n",
        "    matrix b = b1 , b2\n",
        "    ereturn post b\n",
        "end\n",
        "bootstrap: bs_wages_children"
      ],
      "id": "f803301a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why does it matter? because you may want to test coefficients individually, or across models. This is only possible if the FULL system is estimated jointly\n",
        "\n",
        "## {.scrollable}\n",
        "\n",
        "### What about Wild Bootstrap? \n",
        "\n",
        "- Wildbootstrap is available using `boottest` (`ssc install bootest`)\n",
        "\n",
        "- And in `Stata18+`, you have `wildbootstrap` (although is meant for clustered SE)\n"
      ],
      "id": "0b3dec5f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "frause oaxaca, clear\n",
        "regress lnwage educ exper female, robust\n",
        "boottest educ, nograph\n",
        "boottest exper, nograph\n",
        "boottest female, nograph"
      ],
      "id": "30faa758",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final words on Bootstrap: {.scrollable}\n",
        "\n",
        "So bootstrap (and its many flavors) are convenient approaches to estimate standard errors and elaborate statistical Inference, but its not infallible.\n",
        "\n",
        "1.  If the re-sampling process does not simulate the true sampling design, we may miss important information when constructing SE.\n",
        "2.  When the parameters are estimated using \"hard\" cutoffs or restricted distributions, it may not produce good approximations for SE.\n",
        "3.  You usually require MANY repetitions (standard = 50, but you probably want 999 or more). The more the better, but has some computational costs. (specially simple bs)\n",
        "4.  Some methods play better with weighted samples, clusters, and other survey designs than others. And some require more know-how than others.\n",
        "\n",
        "So choose your ðŸ”«weapon wisely!\n",
        "\n",
        "# Small Diversion ðŸ¦Œ: The Delta Method\n",
        "\n",
        "## Variance of nonlinear functions {.scrollable}\n",
        "\n",
        "::: incremental\n",
        "-   Some times (perhaps not with simple OLS) you many need to estimate Standard errors for transformations of your main coefficient of interest, or combinations of those coefficients.\n",
        "\n",
        "-   Say that you estimated $\\theta \\sim N(\\mu_\\theta, \\sigma^2_\\theta)$ but are interested in the distribution of $g(\\theta)$. How do you do this?\n",
        "\n",
        "-   Two options:\n",
        "\n",
        "    a)  you re estimate $g(\\theta$) instead, or\n",
        "    b)  you make an approximation, using the **Delta Method**\n",
        "\n",
        "-   How does it work?\\\n",
        ":::\n",
        "\n",
        "##  {.scrollable}\n",
        "\n",
        "-   The **Delta method** uses the linear approximations to *approximate* the otherwise not known distributions.\n",
        "\n",
        "-   Further, It relies on the fact that linear transformations a normal distribution, is on itself normal. For example:\n",
        "\n",
        "$$\n",
        "g(\\hat \\theta) \\simeq g(\\theta) + g'(\\hat\\theta) (\\hat \\theta-\\theta)\n",
        "$$\n",
        "\n",
        "-   This states that the nonlinear function $g(\\theta)$ can be \"locally\" approximated as a linear function in the neighborhood of $g(\\theta)$.\n",
        "\n",
        "-   Predictions above or below are approximated using the slope of the function. $g'(\\theta)$.\n",
        "\n",
        "-   So, if we take the variance, we get:\n",
        "\n",
        "$$\n",
        "Var(g(\\hat \\theta)) \\simeq  Var \\left(g(\\theta)+ g'(\\hat\\theta) (\\hat \\theta-\\theta)\\right) \n",
        "=g'(\\hat\\theta)^2 Var(\\theta)\n",
        "$$\n",
        "\n",
        "## Delta Method: Visualization\n",
        "\n",
        "![](resources/dm.png)\n",
        "\n",
        "## \n",
        "\n",
        "It can go multivariate as well:\\\n",
        "$$\n",
        "\\begin{aligned}\n",
        "g(\\hat \\theta, \\hat \\gamma)-g(\\theta,\\gamma) &\\simeq N(0,\\nabla g ' \\Sigma \\nabla g) \\\\\n",
        "\\nabla g ' &=   [\\begin{matrix}\n",
        "    dg/d\\theta & dg/d\\gamma \n",
        "  \\end{matrix}]\n",
        "\\end{aligned}  \n",
        "$$\n",
        "\n",
        "Although you need to get the partial derivatives of $g(\\theta,\\gamma)$\n",
        "\n",
        "## \n",
        "### Example\n",
        "\n",
        "- Say that you obtain the mean standard error for averages wages for men and women, along with the correlation between the two. \n",
        "\n",
        "- however, you are insterested in estimating the wage ratio, and its variance. How do you do this?\n",
        "- Need to obtain the Gradients $g$\n",
        "\n",
        "$$\n",
        "R = \\frac{ \\mu_f}{\\mu_m};\n",
        "g = \\begin{bmatrix}\n",
        "    \\frac{ \\partial R}{\\partial \\mu_f} \\\\\n",
        "    \\frac{ \\partial R}{\\partial \\mu_m}\n",
        "    \\end{bmatrix} =\n",
        "\\begin{bmatrix}    \n",
        "\\frac{1}{\\mu_m} \\\\  -\\frac{\\mu_f}{\\mu_m^2}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Then the variance of $R$ is:\n",
        "\n",
        "$$Var(R) = g' \\Sigma_\\mu g$$\n",
        "\n",
        "## {.scrollable}\n",
        "### Example in `Stata` \n"
      ],
      "id": "658461f4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| classes: larger\n",
        "frause oaxaca, clear\n",
        "gen wage = exp(lnwage)\n",
        "mean wage, over(female)\n",
        "mata:\n",
        "  mu = st_matrix(\"e(b)\")\n",
        "  vcv = st_matrix(\"e(V)\")\n",
        "  dg = 1/mu[2] \\ -mu[1]/mu[2]^2\n",
        "  var_r = dg'*vcv*dg\n",
        "  sqrt(var_r)\n",
        "end\n",
        "nlcom _b[ c.wage@0.female]/_b[ c.wage@1.female] "
      ],
      "id": "acd53cfd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## So why do we care: {.scrollable}\n",
        "\n",
        "Two reasons:\n",
        "\n",
        "-   Nonlinear models need this kind of approximations to do statistical inference (probit/logit)\n",
        "\n",
        "-   Recall that when using Robust Standard errors Joint hypothesis Should be done with Care...\n",
        "\n",
        "Consider a linear set of restrictions imposed by the $H_0: R\\beta = r$.\n",
        "\n",
        "1.  Estimate the Variance of $R\\beta$\n",
        "\n",
        "$$\n",
        "Var(R\\beta)  = \\nabla (R\\beta)' Var(\\beta) R \\nabla (R\\beta)'= R' Var(\\beta) R\n",
        "$$\n",
        "\n",
        "2.  Estimate the F value for the Linear Hypothesis (Wald Test)\n",
        "\n",
        "$$\n",
        "(R\\hat \\beta-r)' Var(R\\beta)^{-1} (R\\hat \\beta-r)/Q \\sim F(Q,N-K) \n",
        "$$\n",
        "\n",
        "# Linear Model Selection and Regularization\n",
        "\n",
        "![](https://images.datacamp.com/image/upload/v1648205672/image18_a3zz7y.png){fig-align=\"center\"}\n",
        "\n",
        "## What happens when K is too big?  {.scrollable}\n",
        "\n",
        "\n",
        "-   How many variables (max) can you use in a model?\n",
        "\n",
        "    -   $$max \\ k = rank(X'X)$$\n",
        "\n",
        "-   What happens when you add too many variables in a model?\n",
        "\n",
        "    -   Increase Multicolinearity and coefficient variance (too much noise)\n",
        "\n",
        "    -   R2 overly large (without explaining much)\n",
        "\n",
        "    -   Far more difficult to interpret (too many factors)\n",
        "\n",
        "    -   May introduce endogeneity (when it wasnt a problem before)\n",
        "\n",
        "-   How can you solve the problem?\n",
        "\n",
        "    -   You select only a few of the variables, based on theory, and contribution to the model\n",
        "\n",
        "-   What if you can't choose?\n",
        " \n",
        "\n",
        "## ML: We let the ðŸ’»Choose for you {.scrollable}\n",
        "\n",
        "> Before we start. The methodology we will discuss are usually meant to get models with \"good\" predictive power, and some times better interpretability, not so much stat-inference (although its possible)\n",
        "\n",
        "When you do not know how to choose, you could try select a subset of variables from your model such that you maximize **out-of-sample** predictive power\n",
        "\n",
        "This is typically achieved using the following:\n",
        "\n",
        "$$\n",
        "AR^2 = 1-\\frac{SSR}{SST}\\frac{n-1}{n-k-1} \\\\\n",
        "AIC = n^{-1}(SSR + 2k\\hat\\sigma^2) \\\\\n",
        "BIC = n^{-1}(SSR + ln(n) k\\hat\\sigma^2)\n",
        "$$\n",
        "\n",
        "Or using a method known as cross-validation (Comparing predictive power using data not used for model estimation)\n",
        "\n",
        "However, we can always try to estimate a model with all variables!\n",
        "\n",
        "## Ridge and Lasso and ElasticNet {.scrollable}\n",
        "\n",
        "-   Recall that when using OLS to obtain $\\beta's$, we try to minimize the following:\n",
        "\n",
        "$$\n",
        "SSR = \\sum_i(y_i - X_i \\beta)^2 \n",
        "$$\n",
        "\n",
        "-   This has the restrictions of mentioned before ($k < N$). In addition to letting coefficents vary \"too much\"\n",
        "\n",
        "-   An alternative is to Impose additional restrictions so that coefficients do not vary as much. This is known as **Regularization**.\n",
        "   \n",
        "##  {.scrollable}\n",
        "### Ridge Regression\n",
        "\n",
        "- One such approach is **Ridge** regression, which minimizes the following:\n",
        "\n",
        "$$\n",
        "rSS = \\sum_i(y_i - X_i \\beta)^2+ \\lambda \\sum_{k=1}^K\\beta_k^2\n",
        "$$\n",
        "\n",
        "-   This essentially aims to find parameters that reduces SSR, but also \"controls\" for how large $\\beta's$ can be, using a shrinkage penalty that depends on $\\lambda$.\n",
        "\n",
        "\n",
        "-   If $\\lambda = 0$ you get Standard OLS, and if $\\lambda \\rightarrow \\infty$ , you get a situation where all betas (but the constant) are zero. For intermediate values, you may have better models than OLS, because you can balance Bias (when $\\beta's$ are zero) with increase variance (when all $\\beta's$ vary as they \"please\")\n",
        "\n",
        "## {.scrollable}\n",
        "\n",
        "-   We usually start with Ridge, because is relatively Easy to implement, since it has a close form Solution:\n",
        "\n",
        "$$\n",
        " \\beta = (X'X + \\lambda I)^{-1}{X'y}\n",
        "$$\n"
      ],
      "id": "98e36d48"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "set linesize 255\n",
        "frause oaxaca, clear\n",
        "keep if lnwage!=.\n",
        "gen male = 1-female\n",
        "mata:\n",
        "    y = st_data(.,\"lnwage\")\n",
        "    x = st_data(.,\"educ exper female male\")\n",
        "    // Standardization. Need men and SD\n",
        "    mn_x = mean(x)\n",
        "    sd_x = diagonal(sqrt(variance(x)))'\n",
        "    // Centering and addinc constant\n",
        "    x = (x:-mn_x):/sd_x; x = x,J(1434,1,1)\n",
        "    i0 = I(5);i0[5,5]=0\n",
        "    // SD errors as Column, including a 1 for Constant\n",
        "    sd_x=sd_x'\\1\n",
        "    xx = (cross(x,x)) ; xy = (cross(x,y))\n",
        "    bb0 = invsym(xx)*xy \n",
        "    bb1 = invsym(xx:+i0*1)*xy \n",
        "    bb10 = invsym(xx:+i0*10)*xy \n",
        "    bb100 = invsym(xx:+i0*100)*xy \n",
        "    bb1000 = invsym(xx:+i0*1000)*xy \n",
        "    bb10000 = invsym(xx:+i0*10000)*xy\n",
        "    bb100000 = invsym(xx:+i0*100000)*xy\n",
        "    // \n",
        "    bb0:/sd_x,bb1:/sd_x,bb10:/sd_x,bb100:/sd_x,bb1000:/sd_x,bb10000:/sd_x, bb100000:/sd_x\n",
        "end "
      ],
      "id": "eeea3789",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lasso and Elastic Net {.scrollable}\n",
        "\n",
        "-   Ridge is a relatively easy model to understand and estimate, since it has a close form solution. It has the slight disadvantage that you still estimate a coefficient for \"every\" variable (tho some are very small)\n",
        "\n",
        "-   Another approach, that overcomes this advantage is known as Lasso.\n",
        "\n",
        "$$\n",
        "LSS = \\sum_i(y_i - X_i \\beta)^2+ \\lambda \\sum_{k=1}^K |\\beta_k|\n",
        "$$\n",
        "\n",
        "-   and the one known as Elastic net\n",
        "\n",
        "$$\n",
        "eSS = \\sum_i(y_i - X_i \\beta)^2+ \\lambda_L \\sum_{k=1}^K |\\beta_k| + \n",
        "\\lambda_r \\sum_{k=1}^K \\beta_k^2\n",
        "$$\n",
        "\n",
        "\n",
        "## Lasso vs Ridge\n",
        "\n",
        "![](resources/image-394882719.png)\n",
        "\n",
        "## Considerations:\n",
        "\n",
        "As with many methodologies, the benefits from this approaches is not free.\n",
        "\n",
        "1.  You need to choose tuning parameters \"wisely\" using approaches such as AIC, BIC, or cross validation.\n",
        "2.  The model you get may improve prediction, but inference is not as straight forward.\n",
        "3.  It also requires working with Standardized coefficients. (so the same penalty can be used for all variables in the model.\n",
        "\n",
        "Nevertheless, they can be used as starting point for model selection.\n",
        "\n",
        "if interested, look into `Stata` introduction to `Lasso` regression. `help Lasso intro`\n",
        "\n",
        "## Brief Example:\n"
      ],
      "id": "adab4fe6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "qui {\n",
        "frause oaxaca, clear\n",
        "keep if lnwage!=.\n",
        "qui:reg lnwage i.age\n",
        "predict p_ols\n",
        "qui:elasticnet linear lnwage i.age, selection(cv, alllambdas)  alpha(0)\n",
        "predict p_ridge\n",
        "qui:lasso linear lnwage i.age, selection(cv, alllambdas)  \n",
        "predict p_lasso\n",
        "qui:elasticnet linear lnwage i.age, selection(cv, alllambdas)   \n",
        "predict p_elastic\n",
        "}"
      ],
      "id": "35b850c1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](resources/image-1604739776.png){fig-align=\"center\"}\n",
        "\n",
        "## Shrinking Coefficients\n",
        "\n",
        "![Lasso vs Ridge](resources/lasso_ridge.png)\n",
        "\n",
        "# Next: Non & Semi Parametric models"
      ],
      "id": "6576a41d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "nbstata",
      "language": "stata",
      "display_name": "Stata (nbstata)",
      "path": "C:\\Users\\Fernando\\AppData\\Roaming\\jupyter\\kernels\\nbstata"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}