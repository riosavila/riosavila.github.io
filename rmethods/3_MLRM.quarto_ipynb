{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Multiple Regression Analysis: Estimation\"\n",
        "title-slide-attributes:\n",
        "    data-background-image: images/paste-4.png\n",
        "    data-background-size: contain\n",
        "    data-background-opacity: \"0.5\"\n",
        "subtitle: \"The first tool of Many\"\n",
        "author: Fernando Rios-Avila\n",
        "jupyter: nbstata\n",
        "format: \n",
        "  revealjs: \n",
        "    slide-number: true\n",
        "    width: 1600\n",
        "    height: 900\n",
        "    code-fold: true\n",
        "    echo: true\n",
        "    css: styles.css \n",
        "---"
      ],
      "id": "88c4dc25"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: false\n",
        "*| output: false\n",
        "capture program drop model_display\n",
        "program model_display\n",
        "\tsyntax, [format(string asis)]\n",
        "\t\n",
        "\tif missing(\"`format'\") local format \"%5.3f\"\n",
        "\ttempname b\n",
        "\tmatrix `b'=e(b)\n",
        "\tlocal cname:colname `b'\n",
        "\tlocal k = colsof(`b')\n",
        "\tlocal dep =\"`e(depvar)'\"+\"_hat\"\n",
        "\t\n",
        "\tlocal coef = `b'[1,`k']\n",
        "\tif sign(`coef')==-1 local sgn \"-\"\n",
        "\tlocal coef:display %5.3f (`coef')\n",
        "\tlocal todisp \"`dep' = `coef'\"\n",
        "\t\n",
        "\tforvalues i = 1/`=`k'-1' {\n",
        "\t\tlocal coef = `b'[1,`i']\n",
        "\t\tlocal sgn  \"+\"\n",
        "\t\tif sign(`coef')==-1 local sgn \"-\"\n",
        "\t\tlocal coef:display `format' abs(`coef')\n",
        "\t\tlocal cnamex:word `i' of `cname'\n",
        "\t\tlocal todisp `todisp' `sgn' `coef' `cnamex'\n",
        "\t}\n",
        "    display \"{p}`todisp'{p_end}\" _n\n",
        "    display \"N=\" %10.0f `=e(N)' \" R2=\" %5.3f `=e(r2)'\n",
        "end"
      ],
      "id": "7c621ff9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# {background-image=\"https://i.imgflip.com/6rj8vc.jpg\" background-size=\"contain\"}\n",
        "\n",
        "## Why stay with 1 when you can use Many? ... Why not?\n",
        "\n",
        "-   The SLRM we cover last week is a powerful tool to understand the mechanics behind regression analysis, however is too limited.\n",
        "    -   Use one control? to fix everything ?!\n",
        "-   The Natural alternative is to relax the assumption and Make things more flexible.\n",
        "    -   In other words...Allow for adding More controls\n",
        "\n",
        "Thus, instead of: \n",
        "\n",
        "$$y_i = \\beta_0 + \\beta_1 x_i + e_i\n",
        "$$\n",
        "\n",
        "we have to consider:\n",
        "\n",
        "$$y_i = \\beta_0 + \\beta_1 x_{1i} +\\beta_2 x_{2i} + \\dots + \\beta_k x_{ki} + u_i\n",
        "$$\n",
        "\n",
        "How many can we add? and why does it help?\n",
        "\n",
        "## The power of MLR: Why do more controls help?\n",
        "\n",
        "1.  One more explicitly accounts for variables that before were *hidden* in $e_i$.  \n",
        "    We add $x_{2i},x_{3i},\\dots,x_{ki}$ to the model model, and is no longer in $e_i$\n",
        "\n",
        "2.  Allows for richer model specifications and nonlinearities:\n",
        "\n",
        "    Before: $y_i = \\beta_0 + \\beta_1 x_{1i} + e_i$\n",
        "\n",
        "    Now : $y_i = \\beta_0 + \\beta_1 x_{1i} +\\beta_2 x^2_{1i} + \\beta_3 x^{1/2}_{1i} + \\beta_4 x^{-1}_{1i} + \\beta_5 x_{2i}+\\dots+e_i$\n",
        "\n",
        "Thus, we can get closer to the unknown Population function, and explicitly handle *some* endogeneity problems (we control for it).\n",
        "\n",
        "::: {.callout-caution}\n",
        "\n",
        "## With great power...\n",
        "\n",
        "Being able to add more controls is good, but:\n",
        "\n",
        "-   May make things worse (bad controls)\n",
        "-   Or might not be feasible (small Sample)\n",
        "-   Or may be difficult to interpret (unless you know how to)\n",
        ":::\n",
        "\n",
        "# Do assumptions change?\n",
        "\n",
        "Not really, but lets make some math changes:\n",
        "\n",
        "$$y=\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
        "\\end{bmatrix} \n",
        "; \n",
        "X=\\begin{bmatrix}x_1' \\\\ x_2' \\\\ \\vdots \\\\ x_n'\n",
        "\\end{bmatrix} = \\begin{bmatrix} 1 & x_{11} & x_{21} & \\dots &  x_{k1} \n",
        "\\\\ 1 & x_{12} & x_{22} & \\dots &  x_{k2} \n",
        "\\\\ \\vdots & \\vdots  & \\vdots & \\ddots & \\vdots\n",
        "\\\\ 1 & x_{1n} & x_{2n} & \\dots &  x_{kn} \n",
        "\\end{bmatrix};\n",
        "\\beta =\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k \n",
        "\\end{bmatrix}; e=\\begin{bmatrix}e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n\n",
        "\\end{bmatrix} \n",
        " $$\n",
        "\n",
        "$$y=X\\beta + e\n",
        "$$\n",
        "\n",
        "## Mostly the same\n",
        "\n",
        "1.  **Linear in Parameters**: $y = X\\beta + e$ (And this is the pop function)\n",
        "\n",
        "2.  **Random Sampling** from the population of interest. (So errors $e_i$ is independent from $e_j$)\n",
        "\n",
        "3.  **No Perfect Collinearity**:\n",
        "\n",
        "    This is the alternative to $Var(x)>0$ (SLRM), and deserves more attention.\n",
        "\n",
        "-   We want each variable in $X$ to have [***some***]{.bluetxt} independent variation, from all other variables in the model.\n",
        "    -   In the SLRM, the independent variation idea was with respect to the constant.\n",
        "-   [**If**]{.redtxt} a variable was a linear combination of others, then $\\beta's$ cannot be identified. You need to choose what to keep: \n",
        "\n",
        "$$\\begin{aligned}\n",
        "y &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1+X_2) + e \\\\ \n",
        "&=  \\beta_0 + (\\beta_1+\\beta_3) X_1 + (\\beta_2+\\beta_3) X_2 + e  \n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "## \n",
        "\n",
        "4.  **Zero Conditional mean** (Exogeneity): $E(e_i|X)=0$\n",
        "\n",
        "    Requires that the errors and the explanatory variables are uncorrelated. This is \"easier\" to achieve, because we can now move variables form the error to the model.\n",
        "\n",
        "    However, there could be things you can't controls for (and remain lurking in your errors)\n",
        "\n",
        "> I call this the most important assumption, because is the hardest to deal with\n",
        "\n",
        "#### If A1-A4 Hold, then your estimates will be unbiased!\n",
        "\n",
        "5.  **Homoskedasticity** Same as before. Errors dispersion does not change with respect to **all** $X's$. $$Var(e|X)=c\n",
        "    $$\n",
        "\n",
        "Just as with SLRM, this assumption will help with the estimation of Standard Errors.\n",
        "\n",
        "## MLRM estimation\n",
        "\n",
        "As before, not much has changed. We are still interested in finding $\\beta's$ that Minimizes the (squared) error of the model when compared to the observed data:\n",
        "\n",
        "$$\\hat \\beta = \\min_\\beta \\sum (y_i-X_i'\\beta)^2 = \\min_\\beta \\sum (y_i-\\beta_0-\\beta_1 x_{1i}-\\dots-\\beta_k x_{ki})^2\n",
        "$$\n",
        "\n",
        "The corresponding FOC generate $K+1$ equations to identify $K+1$ parameters:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\sum (y_i-X_i'\\beta) &= 0  \\\\\n",
        "\\sum x_{1i}(y_i-X_i'\\beta) &= 0 \\\\\n",
        "\\sum x_{2i}(y_i-X_i'\\beta) &= 0 \\\\ \\dots \\\\\\\n",
        "\\sum x_{ki}(y_i-X_i'\\beta) &= 0 \n",
        "\\end{aligned} \\rightarrow X'(y-X\\beta) =0 \\rightarrow \\hat \\beta = (X'X)^{-1}X'y\n",
        "$$\n",
        "\n",
        "## `mata` Interlute (for those curious) {.scrollable}\n"
      ],
      "id": "adda52cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| classes: larger\n",
        "\n",
        "frause gpa1, clear\n",
        "gen one =1 \n",
        "mata: y=st_data(.,\"colgpa\"); mata: x=st_data(.,\"hsgpa act one\")\n",
        "mata: xx=x'x ; ixx=invsym(xx) ; xy = x'y \n",
        "mata: b = ixx * xy ; b"
      ],
      "id": "12cc6835",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## You got the $\\beta's$, how do you interpret them?\n",
        "\n",
        "Interpretation of MLRM is similar to the SLRM. For **most** cases, you simply look into the coefficients, and interpret effects in terms of Changes:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "y_i = \\hat\\beta_0 + \\hat\\beta_1 x_{1i}  + \\hat\\beta_2 x_{2i} + e_i \\\\\n",
        "\\Delta y_i =  \\hat\\beta_1 \\Delta  x_{1i}  + \\hat\\beta_2 \\Delta  x_{2i} + \\Delta e_i\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Under A1-A5 I can make use the above to make interpretations\n",
        "\n",
        "1.  $\\hat \\beta_0$ has no effect on \"changes\" of $y$. Only its levels.\n",
        "2.  $\\hat \\beta_1$ indicates how much $\\Delta y_i$ will be if $\\Delta x_{1i}$ increases in 1 unit, if both $\\Delta x_{2i}$ and $\\Delta e_i$ remain constant (*Ceteris Paribus*)\n",
        "\n",
        "$\\Delta e_i=0$ by assumption, and $\\Delta x_{2i}=0$ because we are explicitly controlling for it (We impute this based on extrapolations)\n",
        "\n",
        "You could also analyze the effect of $\\Delta x_{1i}$ and $\\Delta x_{2i}$ Simultaneously!\n",
        "\n",
        "## Example\n"
      ],
      "id": "d892640e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| output: asis\n",
        "*| classes: larger\n",
        "\n",
        "qui: frause wage1, clear\n",
        "qui: reg lwage educ exper tenure\n",
        "local b0:display %5.3f _b[_cons]\n",
        "local b1:display %5.3f _b[educ]\n",
        "local b2:display %5.3f _b[exper]\n",
        "local b3:display %5.3f _b[tenure]\n",
        "display \"\\$log(wage) = `b0' + `b1' educ + `b2' exper + `b3' tenure$\""
      ],
      "id": "2eeedb7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   $\\beta_0$ has no effect on changes, but level.\n",
        "    -   If someone has no education, experience or tenure, log(wages) will be 0.284. Why not wages? and Does it make sense to assume 0 education, experience and tenure?\n",
        "-   $\\beta_1$: An additional year of education increases wages in 0.092log points or about 9.2%, if Experience and tenure do not change (*ceteris paribus*).\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "1.  Think of Interpretations as counterfactual: $y_{post} - y_{pre}$\n",
        "\n",
        "2.  Assumption: Other factors (unobserved $e$) remain fixed (is it always credible??)\n",
        "\n",
        "3.  Effects can be combined. What if a person gains 1 year of education but losses 3 of tenure?\n",
        "\n",
        "## More on Interpretation\n",
        "\n",
        "Under A1-A5, you can still interpret results as \"counterfactual\" at the individual level. However, its more common to do it based on Conditional means:\n",
        "\n",
        "$$\\frac {\\Delta E(y|X)}{\\Delta X_k} \\simeq E(y|X_{-k},X_k+1)-E(y|X)\n",
        "$$\n",
        "\n",
        "Which mostly changes Language.\n",
        "\n",
        "> The expected effect of an increase in $X$ in one unit.\n",
        "\n",
        "## Alternative Interpretation: *Partialling out*\n",
        "\n",
        "-   An alternative way of interpreting (and understanding) MLRM is to think about *partialling out* interpretation.\n",
        "\n",
        "-   This interpretation is based on the Frisch-Waugh-Lowell Theorem, which states that the following models should give you the **SAME** $\\beta's$:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "y &= \\color{blue}{\\beta_1 } X_1 + \\beta_2 X_2 + e \\\\\n",
        "(I-P_{X^c_2}) y &= \\color{green}{\\beta_1} (I-P_{X^c_2}) X_1 + e \\\\\n",
        "P_{X^c_2} &= X^c_2 (X'^{c}_2  X^{c}_2) X'^{c}_2 : \\text{Projection Matrix}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "::: callout-tip\n",
        "## Partialling out\n",
        "\n",
        "$\\beta_1$ can be interpreted as the effect of $X_1$ on $y$, after all variation related to $X_2$ has been \"eliminated\".\n",
        "\n",
        "Thus $\\beta_1$ is the effect uniquely driven by $X_1$.\n",
        ":::\n",
        "\n",
        "## Example {.scrollable}\n"
      ],
      "id": "535db69e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| code-fold: false\n",
        "*| classes: larger\n",
        "qui {\n",
        "  frause oaxaca, clear\n",
        "  drop if lnwage==.\n",
        "  reg lnwage educ exper tenure\n",
        "  est sto m1\n",
        "  reg educ        exper tenure\n",
        "  predict r_educ , res\n",
        "  reg lnwage      exper tenure\n",
        "  predict r_lnwage , res\n",
        "  reg r_lnwage r_educ\n",
        "  est sto m2\n",
        "  reg lnwage educ\n",
        "  est sto m3\n",
        "}\n",
        "esttab m1 m2 m3, se  "
      ],
      "id": "a8d0e08e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimator Properties: Unbiased\n",
        "\n",
        "Recall, the estimator of $\\beta's$ when you have multiple dependent variables:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "0  &: \\hat \\beta = (X'X)^{-1} X'y \\\\\n",
        "A1 \\text{ & }  A2 &: \\hat \\beta = (X'X)^{-1} X'(X\\beta + e) \\\\\n",
        "1  &: \\hat \\beta = (X'X)^{-1} X'X\\beta + (X'X)^{-1} X'e \\\\\n",
        "A3 &: det(X'X)\\neq 0 \\rightarrow (X'X)^{-1} \\text{ exists} \\\\\n",
        "2  &: \\hat \\beta = \\beta + (X'X)^{-1} X'e \\\\\n",
        "A4 &: E(e|X)=0 \\rightarrow E[(X'X)^{-1} X'e]=0 \\\\\n",
        "3  &: E(\\hat\\beta)= \\beta \\text{ unbiased} \n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "## Estimator Properties: Variance under Homoskedasticity\n",
        "\n",
        "Lets start with (2). $\\beta's$ are random functions of the errors. Thus its variance will depend on $e$.\n",
        "\n",
        "$$\\begin{aligned}\n",
        "1 &: \\hat \\beta = \\beta + (X'X)^{-1} X'e \\\\\n",
        "2 &:\\hat \\beta - \\beta = (X'X)^{-1} X'e \\\\\n",
        "3 &: Var(\\hat \\beta - \\beta) = Var((X'X)^{-1} X'e) \\\\\n",
        "4 &: Var(\\hat \\beta - \\beta) = (X'X)^{-1} X' Var(e) X (X'X)^{-1}  \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "$Var(e)$ considers variance and covariance of each $e_i$ and its combinations.\n",
        "\n",
        "## \n",
        "\n",
        "By assumption A2, $cov(e_i,e_j)=0$. And by assumption A5 $Var(e_i)=Var(e_j)$.\n",
        "\n",
        "$$\\begin{aligned}\n",
        "Var(\\hat \\beta - \\beta) &= (X'X)^{-1} X' \\sigma_e^2 I X (X'X)^{-1} \\\\\n",
        "Var(\\hat \\beta - \\beta) &= \\sigma_e^2 (X'X)^{-1} \\\\\n",
        "Var(\\hat \\beta_j - \\beta_j) &= \\frac{\\sigma_e^2}{SST_j (1-R^2_j)} \n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "But we do not know $\\sigma^2_e$. Thus, we also \"estimate it\"\n",
        "\n",
        "$$\\hat \\sigma^2_e = \\frac{\\sum \\hat e^2}{N-K-1}\n",
        "$$\n",
        "\n",
        "Which is unbiased estimator for $\\sigma^2_e$ if A1-A5 hold.\n",
        "\n",
        "##  \n",
        "\n",
        "$$\\begin{aligned}\n",
        "Var(\\hat \\beta - \\beta) &= \\sigma_e^2 (X'X)^{-1} \\\\\n",
        "Var(\\hat \\beta_j - \\beta_j) &= \\frac{\\sigma_e^2}{SST_j (1-R^2_j)}  \\\\\\\n",
        "& = \\frac{\\sigma_e^2}{(N-1)Var(X_j) (1-R^2_j)} = \\frac{\\sigma_e^2}{(N-1)Var(X_j)}VIF_j\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "To consider:\n",
        "\n",
        "-   $Var(\\beta)$ increases with $\\sigma_e^2$. More variation in the error, more variation of the coefficients.\n",
        "-   $Var(\\beta)$ decreases with Sample size $N$\n",
        "-   $Var(\\beta)$ also decreases with Variation in $X$\n",
        "-   However, it **increases** if there is less unique variation (Multicolinearity problem and VIF)\n",
        "\n",
        "## Quick Note\n",
        "\n",
        "-   $R^2$ are the same as SLRM: How much of variation is explained by the model.\n",
        "    -   Also $R^2 = corr(y,\\hat y)^2$\n",
        "-   The fitted line goes over the \"mean\" of all variables\n",
        "-   MLRM Fits *hyper*-planes to the data\n",
        "-   Regression through the origin still a bad idea\n",
        "-   Also, under A1-A5 OLS is the **B**est **L**inear **U**nbiased **E**stimator (BLUE)\n",
        "\n",
        "# {background-image=\"https://i.imgflip.com/7u1fzg.jpg\" background-size=\"contain\"}\n",
        "\n",
        "# Using Many controls is Fun!\n",
        "\n",
        "::: incremental\n",
        "-   You can add more *stuff* for better fit (high R\\^2)\n",
        "-   Making sure nothing remains in \"$e$\"\n",
        "-   It would also allow you for very \"flexible\" models\n",
        "-   But...(these things are not necessarily good)\n",
        ":::\n",
        "\n",
        "##  {background-image=\"images/paste-5.png\" background-size=\"contain\"}\n",
        "\n",
        "## Ignoring Variables\n",
        "\n",
        "In the MLRM framework, its easier to see what happens when important variables are ignored.\n",
        "\n",
        "$$\\text{True: } y = b_0 + b_1 x_1 + b_2 x_2 + e\n",
        "$$\n",
        "\n",
        "But instead you estimate the following :\n",
        "\n",
        "$$\\text{Estimated: }y = g_0 + g_1 x_1 + v\n",
        "$$\n",
        "\n",
        "Unless stronger assumptions are imposed, $g_1$ will be a biased estimate of $b_1$.\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\hat g_1 &= \\frac{\\sum \\tilde x_1 \\tilde y}{\\sum \\tilde x_1^2} \n",
        "         = \\frac{\\sum \\tilde x_1 (b_1 \\tilde x_1 +\\tilde b_2 \\tilde x_2 + e) }{\\sum \\tilde x_1^2} \\\\\n",
        "         &= \\frac{b_1 \\sum \\tilde x_1^2}{\\sum \\tilde x_1^2}\n",
        "          + b_2 \\frac{\\sum \\tilde x_1\\tilde x_2}{\\sum \\tilde x_1^2}\n",
        "          +\\frac{\\sum \\tilde x_1 e}{\\sum \\tilde x_1^2} \\\\\n",
        "         &= b_1+b_2 \\delta_1 +\\frac{\\sum \\tilde x_1 e}{\\sum \\tilde x_1^2} \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "## \n",
        "\n",
        "This implies that $g_1$ is biased:\n",
        "\n",
        "$$E(\\hat g_1) = b_1+b_2 \\delta_1\n",
        "$$\n",
        "\n",
        "Where $\\delta_1$ is the coefficient in $x_2=\\delta_0+\\delta_1 x_1 + v$.\n",
        "\n",
        "Implications:\n",
        "\n",
        "-   Unless\n",
        "\n",
        "    -   $\\delta_1$ is zero ($x_1$ and $x_2$ are linearly independent) or,\n",
        "    -   $b_2$ is zero ($x_2$ was irrelevant)\n",
        "\n",
        "    ignoring $x_2$ will generate biased (and inconsistent) estimates for $b_1$.\n",
        "\n",
        "In models with more controls, the direction of the biases will be harder to define, but similar rule's of thumb can be used.\n",
        "\n",
        "## Adding irrelevant controls\n",
        "\n",
        "Adding irrelevant controls will have no effect on bias and consistency.\n",
        "\n",
        "if your model is:\n",
        "\n",
        "$$y=b_0+b_1 x_1 +e\n",
        "$$\n",
        "\n",
        "but you estimate:\n",
        "\n",
        "$$y=g_0+g_1 x_1+g_2 x_2 +v\n",
        "$$\n",
        "\n",
        "your model is still unbiased: \n",
        "\n",
        "$$\\begin{aligned}\n",
        "g &= (X'X)^{-1}X'(X \\beta^+ + e) \\\\\n",
        "    \\beta^+ &= [\\beta \\ ; 0] \\\\\n",
        "g &=  \\beta^+ + (X'X)^{-1}X'e \\rightarrow E(g) = \\beta^+\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "## Adding \"bad\" Controls\n",
        "\n",
        "The worst case, yet hard to see, is when you add \"bad\" Controls, also known as Colliers.\n",
        "\n",
        "For example:\n",
        "\n",
        "-   Say you want to analyze the effect of **education** on **wages**, and you control for **occupation**. Will it create an unbiased estimate for education?\n",
        "    -   No. Your education **affects** your occupation choice. So some of the effect of education will be \"absorbed\" by occupation.\n",
        "-   Say you want to see the impact of **health expenditure** on **health**, and you control for \"#visits to the doctor\"\n",
        "    -   This may also affect your estimates, as expenditure may change how many times you Visits are highly related.\n",
        "\n",
        "In general, you want to avoid using \"channels\" as Controls.\n",
        "\n",
        "## What about Standard Errors\n",
        "\n",
        "::: panel-tabset\n",
        "## Case 1\n",
        "\n",
        "Omitting relevant variables that are correlated to $X's$\n",
        "\n",
        "We wont talk about this. It violates A4, and creates endogeneity\n",
        "\n",
        "## Case 2\n",
        "\n",
        "Omitting relevant variables that are uncorrelated to $X's$\n",
        "\n",
        "-   Omitted variables will be in the error $e$. Thus variance of coefficients will be larger\n",
        "\n",
        "$$\\begin{aligned}\n",
        "True: & y = b_0 + b_1 x_1 + b_2 x_2 + e  \\\\\n",
        "Estimated: & y = g_0 + g_1 x_1 + v   \\\\\n",
        " & Var(e)<Var(v) \\rightarrow Var(b_1)<Var(g_1)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Thus Adding controls in Randomized experiements is still a good idea!\n",
        "\n",
        "## Case 3\n",
        "\n",
        "Adding Irrelevant controls (related to X's)\n",
        "\n",
        "Coefficients are unbiased, and $\\sigma^2_e$ will also be unbiased.\n",
        "\n",
        "However, you may increase Multicolinearity in the model increasing $R_j^2$ and $VIF_j$.\n",
        "\n",
        "Variance of relevant coefficients will be larger.\n",
        "\n",
        "$$\\begin{aligned}\n",
        "True: & y = b_0 + b_1 x_1  + e  \\\\\n",
        "Estimated: & y = g_0 + g_1 x_1 + g_2 x_2 + v   \\\\\n",
        "& Var(b_1)<Var(g_1)\n",
        "\\end{aligned}\n",
        "$$\n",
        ":::\n",
        "\n",
        "# Examples!\n",
        "\n",
        "## Prediction {.scrollable}\n",
        "\n",
        "- You can use MLRM to obtain predictions of outcomes.\n",
        "\n",
        "- They will be subject to the model specification.\n",
        "  \n",
        "- For prediction you do not need to worry about \"endogeneity\" as much. Just on Predictive power (how ??)\n"
      ],
      "id": "8d7dfa7a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| code-fold: false\n",
        "*| classes: larger\n",
        "qui:frause oaxaca, clear\n",
        "gen wage = exp(lnwage)\n",
        "qui:reg wage educ female age agesq single married\n",
        "predict wage_hat\n",
        "list wage wage_hat educ female age agesq single married in 1/5"
      ],
      "id": "194422b7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Efficient Market {.scrollable}\n",
        "\n",
        "- We could use MLRM to test theories, like the Efficient Market Theory. \n",
        "\n",
        "- For housing, the Assessed price of a house should be all information needed to assess the price of the house. (other ammenities should not matter)\n"
      ],
      "id": "d0a05a65"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| code-fold: false\n",
        "*| classes: larger\n",
        "*| output: asis\n",
        "\n",
        "frause hprice1, clear\n",
        "qui:reg price assess bdrms llotsize lsqrft colonial\n",
        "model_display"
      ],
      "id": "4c08a7e1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| code-fold: false\n",
        "*| classes: larger\n",
        "*| output: asis\n",
        "\n",
        "qui:reg lprice lassess bdrms llotsize lsqrft colonial\n",
        "model_display"
      ],
      "id": "c54a5816",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing for Discrimination (*CP*) {.scrollable}\n",
        "\n",
        "- We could test for discrimination: Unexplained differences in outcomes once other factors are kept fixed.\n",
        "\n",
        "- It does require that groups are similar in terms of unobservables.\n"
      ],
      "id": "6dbce929"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| code-fold: false\n",
        "*| output: asis\n",
        "*| classes: larger\n",
        "qui: frause oaxaca, clear\n",
        "qui:reg lnwage female \n",
        "model_display"
      ],
      "id": "0789a04f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| code-fold: false\n",
        "*| output: asis\n",
        "*| classes: larger\n",
        "qui:reg lnwage female educ age agesq single married exper tenure\n",
        "model_display"
      ],
      "id": "76cba11c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Treatment Evaluation\n",
        "\n",
        "- Under Random Assingment SRM was enough to estimate ATTs. \n",
        "- But if assigment was conditionally random, a better approach would be using MLRM\n"
      ],
      "id": "ec7e8b87"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| code-fold: false\n",
        "*| output: asis\n",
        "*| classes: larger\n",
        "frause jtrain98, clear\n",
        "qui:reg earn98 train \n",
        "model_display"
      ],
      "id": "306aff8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| code-fold: false\n",
        "*| output: asis\n",
        "*| classes: larger\n",
        "qui:reg earn98 train earn96 educ age married\n",
        "model_display"
      ],
      "id": "1288b31d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Thats all for Today\n",
        "Next Week: Inference and Asymptotics"
      ],
      "id": "1a65c973"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "nbstata",
      "language": "stata",
      "display_name": "Stata (nbstata)",
      "path": "C:\\Users\\Fernando\\AppData\\Roaming\\jupyter\\kernels\\nbstata"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}