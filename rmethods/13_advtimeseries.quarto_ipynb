{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Times Series Part-II\"\n",
        "subtitle: \"Building Time Machines\"\n",
        "author: Fernando Rios-Avila\n",
        "jupyter: nbstata\n",
        "format: \n",
        "  revealjs: \n",
        "    slide-number: true\n",
        "    width: 1600\n",
        "    height: 900\n",
        "    code-fold: true\n",
        "    code-overflow: wrap\n",
        "    echo: true\n",
        "    css: styles.css \n",
        "    chalkboard: true  \n",
        "---\n",
        "\n",
        "\n",
        "## Defending the Sacred time line\n",
        "\n",
        "![](images/paste-13.png)\n",
        "\n",
        "## Last week\n",
        "\n",
        "-   What we learned last week a few methodologies for analyzing time series data.\n",
        "    -   Static model, dynamic models, use of trends and seasonality, etc.\n",
        "-   All those models, however, were based on very strong assumptions.\n",
        "    -   Strict Exogeneity, strict Homoskedasticity, and no serial correlation are difficult to defend.\n",
        "    -   Statistical inference validity depends on normality assumption of the error.\n",
        "-   Can we relax this assumptions?\n",
        "    -   Yes, but we need to impose additional assumtions to the data\n",
        "\n",
        "## This week\n",
        "\n",
        "-   One way to relax the assumption of Strict Exogeneiety is assume time series are stationary and weakly dependent.\n",
        "\n",
        "    -   With this assumptions, LLN and CLT will hold for TS model.\n",
        "\n",
        "-   What do these two assumptions mean? (stationary and weakly dependent?)\n",
        "\n",
        "-   Namely, these assumptions impose restrictions on how data SHOULD behaive, so we can learn something from the data.\n",
        "\n",
        "    -   This means that what we observe in the data are based on data that has stable patterns. Otherwise, if they are unpredictable, we cannot learn anything. (This is the idea of stationarity)\n",
        "    -   At the same time, data cannot depend on its own past **too much**. So if we look at two periods that are farther apart, its like they are independent for most practical purposes.\n",
        "\n",
        "## Can you recognized this type of data?\n"
      ],
      "id": "235e34bd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: false\n",
        "*| output: false\n",
        "set obs 250\n",
        "set seed 1321\n",
        "gen t = _n\n",
        "tsset t\n",
        "gen y1 =0\n",
        "gen y2 =0\n",
        "gen r = rnormal()\n",
        "set scheme white2\n",
        "color_style tableau\n",
        "replace y1 =l.y1 + r if t>1\n",
        "replace y2 =0.8*l.y2 + r if t>1\n",
        "scatter y1 t, title(\"Non-Stationary\") xtitle(\"time\") name(fig1, replace) connect(l)\n",
        "scatter y2 t, title(\"Stationary\") xtitle(\"time\") name(fig2, replace) connect(l)\n",
        "graph combine fig1 fig2, ysize(6) xsize(12) ycommon\n",
        "graph export images/fig13_1.png, replace\n",
        "\n",
        "\n",
        "two kdensity y1 if t<=125 ||  kdensity y1 if t>125, title(\"Non-Stationary\") name(fig1, replace) legend(off)\n",
        "two kdensity y2 if t<=125 ||  kdensity y2 if t>125, title(\"Stationary\") name(fig2, replace) legend(off)\n",
        "graph combine fig1 fig2, ysize(6) xsize(12) \n",
        "graph export images/fig13_2.png, replace"
      ],
      "id": "4c310a3b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: panel-tabset\n",
        "## Trends\n",
        "\n",
        "![](images/fig13_1.png)\n",
        "\n",
        "## Density\n",
        "\n",
        "![](images/fig13_2.png)\n",
        ":::\n",
        "\n",
        "## Stationary Process\n",
        "\n",
        "-   A Stationary process is one where the characteristics of the distribution remains Stable across time.\n",
        "    -   So we can learn something about it\n",
        "\n",
        "For example consider the series $x=[x_1,x_2,\\dots,x_T]$, where $T$ can be vary large (infinite)\n",
        "\n",
        "-   If the series is stationary, then:\n",
        "\n",
        "$$f(x_t, x_{t+k}) = f(x_s, x_{s+k}) \\forall s, k$$\n",
        "\n",
        "-   In other words, the distribution across time does not change. (is stable, thus stationary)\n",
        "\n",
        "-   Otherwise, if the distribution function changes constantly, we cannot learn about it, thus making sense of regressions would be even more difficult.\n",
        "\n",
        "## Testing for Stationarity\n",
        "\n",
        "-   Testing for stationarity is not easy. (how do you test for stability of densities?)\n",
        "\n",
        "-   Instead, from an empirical point of view, we focus on the first 2 moments of the distribution: mean, variance and covariance.\n",
        "\n",
        "Thus:\n",
        "\n",
        "A series $x=[x_1,x_2,\\dots,x_T]$ is stationary if:\n",
        "\n",
        "1.  $E(x_t)=E(x_s)=\\mu_x$. Constant mean\n",
        "2.  $E(x_t^2)<\\infty$. Finite variance\n",
        "3.  $Var(x_t)=Var(x_s)=\\sigma_x^2$. Constant variance\n",
        "4.  $cov(x_t,x_{t+k})=cov(x_s,x_{s+k})$. Constant covariance\n",
        "\n",
        "The Series repeats itself.\n",
        "\n",
        "## Weakly Dependent Series\n",
        "\n",
        "-   Dependence should be understood as a measure of how much a series depends on its own past.\n",
        "\n",
        "    -   Weakly dependent series are those that depend on its own past, but not too much.\n",
        "\n",
        "-   From the technical point of view:\n",
        "\n",
        "    -   If $x_t$ is weakly dependent, then $\\lim_{k\\rightarrow \\infty} corr(x_t,x_{t+k})=0$ sufficiently fast.\n",
        "\n",
        "-   If this happens, LLN and CLT will hold for TS data.\n",
        "\n",
        "-   Why?\n",
        "\n",
        "    -   Omiting $x_{t+1}$ would typically generate an Omitted Variable Bias.\n",
        "    -   If data are weakly dependent, however, we won't suffer from OMB\n",
        "    -   This is because omitting $x_{t+1}$ is like omitting a unrelated variable.\n",
        "\n",
        "> NOTE: Weakly dependent variables can be non-stationary. (we call them stationary around a trend)\n",
        "\n",
        "## Visualizing weakly dependent vs strongly dependent\n",
        "\n",
        "![](images/paste-14.png)\n",
        "\n",
        "## Example of Weakly Dependent Series\n",
        "\n",
        "-   AR(1) process: $x_t = \\rho x_{t-1} + \\epsilon_t$\n",
        "    -   Mean in constant (not depent on time)\n",
        "    -   Variance is constant (not depent on time)\n",
        "    -   Covariance does not depend on time (just on lag)\n",
        "    -   If $|\\rho|<1$, then $x_t$ is weakly dependent.\n",
        "    \n",
        "-   MA(1) process: $x_t = \\epsilon_t + \\theta \\epsilon_{t-1}$\n",
        "    - Constant mean and variance. \n",
        "    - Covariance does not depend on time. \n",
        "    - Covariance bettween $x_t$ and $x_{t+k}$ is zero for $k>1$.\n",
        "\n",
        "- ARMA(1,1) $x_t = \\rho x_{t-1} + \\epsilon_t + \\theta \\epsilon_{t-1}$\n",
        "    \n",
        "\n",
        "## Special Case AR(1) with $\\rho=1$\n",
        "\n",
        "-   If $\\rho=1$, we have a random walk process $x_t = x_{t-1} + \\epsilon_t$\n",
        "-   This is a process that holds grudges (it remembers its past)\n",
        "\n",
        "$$x_t = x_{t=0}+e_1+e_2+\\dots+e_t$$\n",
        "\n",
        "-   This process has constant mean ($x_0$), but variance and covariance are not constant. With a correlation that dissapears slowly.\n",
        "\n",
        "$$Var(x_t) = t\\sigma^2 \\text{ and } cov(x_t,x_{t+h}) = t\\sigma^2$$    \n",
        "\n",
        "$$corr(x_t,x_{t+h}) = \\frac{t \\sigma^2}{\\sqrt{t(t+h)}\\sigma^2}=\\sqrt{\\frac{t}{t+h}}$$\n",
        "\n",
        "**Economics**: With weakly dependent data, policies are transitory, with persistent data effects are longlasting.\n",
        "\n",
        "## Special Case: non-stationary weakly dependent\n",
        "\n",
        "- There are few cases where a series is weakly dependent but non-stationary.\n",
        "\n",
        "$$x_t = \\rho x_{t-1} + \\delta t + \\epsilon_t$$\n",
        "\n",
        "- $E(x_t)$ is not constant, but this series is still weakly dependent if $|\\rho|<1$.\n",
        "- If the data is detrended, however, it becomes stationary.\n",
        "\n",
        "## Example: Stationary around a trend\n",
        "\n",
        "$x_t = 0.5*x_{t-1}+0.1 t + u_t$\n",
        "\n",
        "![](images/paste-15.png)\n",
        "\n",
        "## Example: Random Walks look with a drift\n",
        "\n",
        "$$x_t = x_{t-1} + 0.1 + \\epsilon_t$$\n",
        "\n",
        "![](images/paste-16.png)\n",
        "\n",
        "\n",
        "\n",
        "## How do things change: Assumptions\n",
        "\n",
        "**A1.** Linear in Parameters (same as before), but all variables are stationary and weakly dependent.\n",
        "\n",
        "**A2.** No Perfect Colinearity\n",
        "\n",
        "**A3.** Zero Conditional Mean: $E(u_t|x_t)=0$ Contemporaneous exogeneity!\n",
        "\n",
        "Omitting lags of $x_t$ is not a problem, because they are weakly \"independent\" of $x_t$.\n",
        "\n",
        "**A1-A3** OLS is consistent.\n",
        "\n",
        "**Why does this matter??**\n",
        "\n",
        "- Because, under Strict exogeneity, we cannot allow for Lags of the outcome to be included in the model.\n",
        "- Under weak exogeneity, lags can be included.\n",
        "\n",
        "## How do things change: Assumptions\n",
        "\n",
        "**A4.** Homoskedasticity: $Var(u_t|x_t)=\\sigma^2$ (also we just need contemporaenous homoskedasticity)\n",
        "\n",
        "**A5.** No Serial Correlation: $cov(u_t,u_s|x_t)=0$ for $t\\neq s$ \n",
        "\n",
        "These two assumptions that make \"life\" easier for estimating standard errors because:\n",
        "\n",
        "- Under **A1-A5**, OLS estimators are asymptotically normal, and all Standard Statistics are applicable\n",
        "\n",
        "\n",
        "\n",
        "## Order of Integration\n",
        "\n",
        "- As you may expect, many interesting time series are not stationary. However, we may want to use for analysis\n",
        "  - to do so, we need to understand their taxonomy (in TS) so we can make them stationary.\n",
        "  \n",
        "-  A series is said to be integrated of order $d$, denoted $I(d)$, if it can be made stationary by taking $d$ differences.\n",
        "\n",
        "- A weakly dependent series is an $I(0)$ process. (is already stationary)\n",
        "- A random walk is an $I(1)$ process. It could be made stationary by taking first differences.\n",
        "  \n",
        "$$x_t = x_{t-1} + \\epsilon_t \\rightarrow \\Delta x_t = \\epsilon_t$$\n",
        "\n",
        "- A series that is $I(2)$ would required two differences to be made stationary.\n",
        "\n",
        "$$x_t =2x_{t-1} - x_{t-2} + \\epsilon_t \\rightarrow \\Delta^2 x_t = \\epsilon_t$$\n",
        "\n",
        "## Unit roots and Spurious Regressions\n",
        "\n",
        "- If a series is $I(1)$, it is said to have a unit root. an $I(2)$ series has two unit roots, etc.\n",
        "\n",
        "- Data that are $I(1)$ tend to look like data with Trends\n",
        "  - If we analyze this data, we may find spurious relationships. t-stats may be high, as well as $R^2$.\n",
        "  - This may lead to incorrect conclusions (unless other stronger assumptions are made)\n",
        "  - In this cases, using trends will not help.\n",
        "\n",
        "## Spurious Regressions: Example\n",
        "\n",
        "$x_t = x_{t-1} + v_t$ & $y_t = y_{t-1} + u_t$\n"
      ],
      "id": "2f4a8635"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: false\n",
        "*| output: false\n",
        "use mdata/sim_ts.dta, clear\n",
        "histogram _sim_1, name(fig1, replace) title(\"T-Statistic\")\n",
        "histogram _b_r2, name(fig2, replace) title(\"R2 distribution\")\n",
        "graph combine fig1 fig2, ysize(6) xsize(12) \n",
        "graph export images/fig13_4.png, replace"
      ],
      "id": "b0cd7400",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](images/fig13_4.png)\n",
        "\n",
        "# How to decide if a series is $I(1)$ or has a unit root? \n",
        "or if its not stationary\n",
        "\n",
        "## Naive Approach. Look into $\\rho$\n",
        "\n",
        "- Naive approach: Look at auto correlation:\n",
        "  - if $corr(x_t,x_{t-1})>0.9$ then $x_t$ is highly persistent, and probably $I(1)$\n",
        "  - Differentiate data and look at auto correlation again.\n",
        "\n",
        "## Formal Approach: Dickey-Fuller Test for Unit Root\n",
        "\n",
        "Model: $y_t = \\alpha + \\rho y_{t-1} + e_t$\n",
        "AModel: $\\Delta y_t = \\alpha + \\theta y_{t-1} + e_t$\n",
        "\n",
        "- $H_0: \\rho=1$ (has a unit root) vs $H_1: \\rho<1$ (is stationary)\n",
        "- $H_0: \\theta=0$ (has a unit root) vs $H_1: \\theta<0$ (is stationary)\n",
        "\n",
        "Its a one tail test, however, the statistic of interest does not follow a t-distribution, but a DF distribution\n",
        "\n",
        "| SL \t| 1% \t| 2.5% \t| 5% \t| 10% |\n",
        "|----\t|----\t|----\t|----\t|----\t|\n",
        "| DF \t|  -3.43  \t| -3.12   \t| -2.86| -2.57|\n",
        "\n",
        "## Augmented Dickey-Fuller Test\n",
        "\n",
        "Allowing for serial correlation, and uses same critial values as before:\n",
        "\n",
        "Model: $\\Delta y_t = \\alpha + \\theta y_{t-1} + \\lambda_1 \\Delta y_{t-1} + \\dots + \\lambda_k \\Delta y_{t-k} + e_t$\n",
        "\n",
        "Same as before. But in practice the additional lags should be choosen based on information criteria.\n",
        "\n",
        "## ADF with a trend\n",
        "\n",
        "Model: $\\Delta y_t = \\alpha + \\delta t +\\theta y_{t-1} + \\lambda_1 \\Delta y_{t-1} + \\dots + \\lambda_k \\Delta y_{t-k} + e_t$\n",
        "\n",
        "- This allows for even more flexibility, or if you believe data is stationary around a trend.\n",
        "\n",
        "- Main difference... critical values are even larger:\n",
        "\n",
        "| SL \t| 1% \t| 2.5% \t| 5% \t| 10% |\n",
        "|----\t|----\t|----\t|----\t|----\t|\n",
        "| DF \t|  -3.96 \t| -3.66   \t| -3.41| -3.12|\n",
        "\n",
        "As before, If you find evidence of unit root, Differentiate and test again.\n",
        "\n",
        "## Example {.scrollable}\n"
      ],
      "id": "04f7eb97"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| classes: larger\n",
        "qui:frause fertil3, clear\n",
        "** Setup as time series\n",
        "tsset year"
      ],
      "id": "a06838bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model: $gfr = \\alpha + \\delta_0 pe_t + \\delta_1 pe_{t-1} + \\delta_2 pe_{t-2}+ e_t$\n"
      ],
      "id": "c08c7818"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| classes: larger\n",
        "reg gfr pe l.pe l2.pe  "
      ],
      "id": "3ddc102a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Are $gfr$ and $pe$ Stationary?\n",
        "\n",
        "Naive approach: Look at auto correlation:\n"
      ],
      "id": "ef9608d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| classes: larger\n",
        "** Naive apprach\n",
        "corr pe l.pe gfr l.gfr"
      ],
      "id": "0776ceab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Formal Approach: Dickey-Fuller Test for Unit Root\n"
      ],
      "id": "0346c120"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reg d.pe l.pe, nohead\n",
        "reg d.gfr l.gfr, nohead"
      ],
      "id": "615d952f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Same conclusions. Are their differences stationary?\n"
      ],
      "id": "987dad03"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "gen dpe = d.pe \n",
        "gen dgfr = d.gfr\n",
        "reg d.dpe l.dpe, nohead\n",
        "reg d.dgfr l.dgfr, nohead"
      ],
      "id": "245d6876",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now it should be better to use model in differences for analysi:\n"
      ],
      "id": "ad72fc2b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reg dgfr dpe l.dpe l2.dpe  "
      ],
      "id": "c547195b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Problem of Serial Correlation\n",
        "\n",
        "- Up to this point, we have assumed that the error term is uncorrelated across time. (no serial correlation)\n",
        "- As with RC analysis, violation of this assumption does not lead to biased estimators of the coefficients (under usual situations), but it does lead to biased standard errors.\n",
        "- Why? If errors are correlated across time, (say possitively) then the variance of the OLS estimator is biased downwards.\n",
        "\n",
        "$$Var(u_t+u_{t+h})=2\\sigma^2 + \\color{red}{2\\rho_{t,t+h}} \\sigma^2$$\n",
        "\n",
        "## Serial Correlation and Lags\n",
        "\n",
        "- If one has a model with Lags, then serial correlation is likely to happen.\n",
        "\n",
        "$$y_t = \\beta_0 + \\beta_1 y_{t-1} + u_t$$\n",
        "\n",
        "- This model simply assumes that $y_{t-1}$ should be uncorrelated with $u_t$. But, it may be that $y_{t-2}$ is correlated with $u_t$.\n",
        "- If that is the case then $Corr(u_t, u_{t-1})\\neq 0$ because it may be picking up that correlation. \n",
        "\n",
        "- On the other hand, if $u_t$ is serially correlated, then $y_{t-1}$ is correlated with $u_t$, causing OLS to be inconsistent. \n",
        "  - This, however, may also indicate that one needs to consider a different model:\n",
        "\n",
        "$$y_t = \\alpha_0 + \\alpha_1 y_{t-1} + \\alpha_2 y_{t-2} + e_t$$\n",
        "\n",
        "- Where $e_t$ is not serially correlated, not correlated with $y_{t-1}$, nor $y_{t-2}$.\n",
        "\n",
        "## Test for Serial Correlation\n",
        "### Strictly Exogenous Regressors\n",
        "\n",
        "Model: $y_t = \\beta_0 + \\beta_1 x_{1,t} + \\dots + \\beta_k x_{k,t} + u_t$\n",
        "and: $u_t=\\rho u_{t-1}+e_t$\n",
        "\n",
        "If there is no serial correlation, then we simply need to test if $\\rho=0$, using a t-statistic.\n",
        "\n",
        "### Durbin Watson Test\n",
        "\n",
        "Under Classical assumptions, one could also use the DW statistic:\n",
        "\n",
        "$$DW = \\frac{\\sum_{t=2}^T (u_t-u_{t-1})^2}{\\sum_{t=1}^T u_t^2}$$\n",
        "\n",
        "where $DW\\simeq 2(1-\\hat{\\rho})$. \n",
        "\n",
        "- if there is no serial correlation, then $DW\\simeq 2$.\n",
        "- If there is possitive serial correlation, then $DW<2$.\n",
        "- A less practical test, but valid in small samples\n",
        "\n",
        "## Test for Serial Correlation\n",
        "### Weakly Exogenous Regressors\n",
        "\n",
        "Model: $y_t = \\beta_0 + \\beta_1 x_{1,t} + \\dots + \\beta_k x_{k,t} + u_t$\n",
        "and: $u_t=\\rho u_{t-1}+\\gamma_1 x_{1,t} + \\dots + \\gamma_k x_{k,t}+e_t$\n",
        "\n",
        "$H0: \\rho=0$ vs $H1: \\rho\\neq 0$\n",
        "\n",
        "### Testing for higher order correlation\n",
        "\n",
        "and: $u_t=\\rho_1 u_{t-1}+\\rho_2 u_{t-2}+\\gamma_1 x_{1,t} + \\dots + \\gamma_k x_{k,t}+e_t$\n",
        "\n",
        "$H0: \\rho_1=0 \\& \\rho_2=0$ vs $H1: \\text{one is not equal to }0$\n",
        "\n",
        "This test is called the Breusch-Godfrey test.\n",
        "\n",
        "## Correcting for Serial Correlation: \n",
        "\n",
        "There are two ways to correct for Serial Correlation:\n",
        "\n",
        "- Prais-Winsten and Cochrane-Orcutt regression (Feasible GLS)\n",
        "  - Requires variables to be strictly exogenous regressors (no lagged dependent variables)\n",
        "- Newey-West Standard Errors (this is the equivalent to Robust)\n",
        "  - General setup.\n",
        "\n",
        "## Prais-Winsten and Cochrane-Orcutt regression\n",
        "\n",
        "Consider the model:\n",
        "\n",
        "$$y_t = \\beta_0 + \\beta_1 x_{1,t} + \\beta_2 x_{2,t} + u_t$$\n",
        "\n",
        "where $u_t=\\rho u_{t-1}+e_{t}$\n",
        "\n",
        "This model has serial correlation, which will affect the standard errors of the OLS estimator.\n",
        "\n",
        "- if we know (or estimate) $\\rho$, we can transform the data and eliminate the serial correlation\n",
        "\n",
        "$$\\begin{aligned}\n",
        "y_t &= \\beta_0 + \\beta_1 x_{1,t} + \\beta_2 x_{2,t} + u_t \\\\\\\n",
        "\\rho y_{t-1} &= \\rho \\beta_0 + \\rho \\beta_1 x_{1,t-1} + \\rho \\beta_2 x_{2,t-1} + \\rho  u_{t-1} \\\\\\\n",
        "\\tilde y_t &= \\beta_0 (1-\\rho) + \\beta_1 \\tilde x_{1,t} + \\beta_2 \\tilde x_{2,t} +  e_t\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "- From here, we can obtain the errors $e_t$ and $u_t$, re estimate $\\rho$, and re estimate the model, until $\\rho$ no longer changes. \n",
        "\n",
        "- This is called the Cochrane-Orcutt procedure.\n",
        " \n",
        "## Prais-Winsten and Cochrane-Orcutt regression {.scrollable}\n",
        "\n",
        "- The Prais-Winsten procedures is also similar to the Cochrane-Orcutt procedure, but you do not \"loose\" the first observation.\n",
        "- Specifically, the first observation is estimated as:\n",
        "\n",
        "$$(1-\\rho^2)^{1/2} y_1 =(1-\\rho^2)^{1/2}\\beta_0 + (1-\\rho^2)^{1/2}\\beta_1 x_{1,1} + (1-\\rho^2)^{1/2}\\beta_2 x_{2,1} + (1-\\rho^2)^{1/2} u_1$$\n",
        "\n",
        "- Other features:\n",
        "  - PW can be more efficient than CO, because of the \"saved observation\"\n",
        "  - Both can be used when serial correlation is of higher order, but only CO can be used if order is 3 or higher\n",
        "  - Both methods are iterative\n"
      ],
      "id": "10b74539"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "frause phillips, clear\n",
        "tsset year\n",
        "reg inf unem"
      ],
      "id": "39f48ffd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "prais inf unem, corc\n",
        "\n",
        "prais inf unem, "
      ],
      "id": "33cc1c79",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Newey-West Standard Errors {.scrollable}\n",
        "\n",
        "- The Newey-West standard errors are similar to the robust standard errors, but they take into account the serial correlation of the error term.\n",
        "\n",
        "- The idea is to estimate an inflation factor that corrects Standard errors for serial correlation.\n",
        "\n",
        "$\\hat v = \\sum_{t=1}^T \\hat a_t^2 + 2 \\sum_{h=1}^g \\left[1-\\frac{h}{g+1}\\right] \\left( \\sum_{t=h+1}^{T} \\hat a_t \\hat a_{t-h}\\right)$\n",
        "\n",
        "with $\\hat a_t = \\hat r_t \\hat u_t$\n",
        "\n",
        "- Then $SE_c (\\beta) = \\sqrt{\\hat v} \\left(\\frac{SE(\\beta)}{\\sigma}\\right)^2$\n",
        "  \n",
        "- In other words, this kind of corrects for the fact that the error term is correlated across time.\n"
      ],
      "id": "f9bdd72c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "newey inf unem, lag(0)\n",
        "newey inf unem, lag(1)\n",
        "newey inf unem, lag(2)"
      ],
      "id": "48b40f09",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cointegration\n",
        "\n",
        "- As previously mentioned, most interesting time series are not stationary. \n",
        "- And, when using non-stationary data, we may find spurious relationships. But what if the relation is not spurious?\n",
        "\n",
        "- Consider the following model: \n",
        "\n",
        "  $y_t = \\beta_0 + \\beta_1 x_{1,t} + \\beta_2 x_{2,t} + u_t$\n",
        "\n",
        "- If $y_t$ and $x_{1,t}$ are $I(1)$, then the model is likely to be spurious (common trends). However, it may be possible that there is a causal relationship between these variables.\n",
        "\n",
        "- If they indeed have a causal relationship, then they are said to be cointegrated.\n",
        "\n",
        "## Cointegration \n",
        "\n",
        "- But how do we know if two variables are cointegrated?\n",
        "\n",
        "**s1:** Check if all variables are $I(1)$. If they are, then you can check for cointegration.\n",
        "\n",
        "**s2:** Estimate the model, and obtain the residuals $\\hat u_t$. \n",
        "\n",
        "**s3:** Test if $\\hat u_t$ is $I(0)$. \n",
        "\n",
        "  - If that is the case, then the variables are cointegrated (Share a long term relationship)\n",
        "  - If not, the relationship is spurious\n",
        "\n",
        "- How do we test if $\\hat u_t$ is $I(0)$? $\\rightarrow$ Unit Root test! \n",
        "\n",
        "| SL \t| 1% \t| 2.5% \t| 5% \t| 10% |\n",
        "|----\t|----\t|----\t|----\t|----\t|\n",
        "| No Trend \t|  -3.9 \t| -3.59   | -3.34| -3.04|\n",
        "| With Trend \t|  -4.32 \t| -4.03 \t| -3.78 | -3.50|\n",
        "\n",
        "## Error Correction Models\n",
        "\n",
        "- If two variables are cointegrated, then they share a long term relationship.\n",
        "- However, you may also be interested in the short term dynamics of the relationship.\n",
        "- To do this, you can use an Error Correction Model (ECM)\n",
        "\n",
        "$\\Delta y_t = \\beta_0 + \\beta_1 \\Delta x_{1,t} + \\beta_2 \\Delta x_{2,t} + \\gamma \\hat u_{t-1}+ e_t$\n",
        "\n",
        "Where $\\gamma$ is the short term correction term. \n",
        "\n",
        "## Other topics of interest\n",
        "\n",
        " \n",
        "- Forcasting\n",
        "  - ARIMA models, and VAR models (Vector Autoregressions) can be used for forcasting.\n",
        "  - Forcating implies making predictions about the future, based on the past, accounting for the errors propagation.\n",
        "  - Variable selection, temporal causation (Granger Causality), and other techniques are used for this.\n",
        "  \n",
        "# And we are done! "
      ],
      "id": "1e31360a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "nbstata",
      "language": "stata",
      "display_name": "Stata (nbstata)",
      "path": "C:\\Users\\Fernando\\AppData\\Roaming\\jupyter\\kernels\\nbstata"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}