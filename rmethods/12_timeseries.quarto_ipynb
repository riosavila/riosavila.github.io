{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Times Series Part-I\"\n",
        "subtitle: \"Basic Regression Analysis\"\n",
        "author: Fernando Rios-Avila\n",
        "jupyter: nbstata\n",
        "format: \n",
        "  revealjs: \n",
        "    slide-number: true\n",
        "    width: 1600\n",
        "    height: 900\n",
        "    code-fold: true\n",
        "    code-overflow: wrap\n",
        "    echo: true\n",
        "    css: styles.css \n",
        "    chalkboard: true  \n",
        "---\n",
        "\n",
        "\n",
        "![](images/paste-8.png)\n",
        "\n",
        "## The nature of time series data\n",
        "\n",
        "-   Time series \"works\" different from Repeated crossection.\n",
        "\n",
        "    -   You do not have access to a random sample. (Window of time if fixed)\n",
        "\n",
        "    -   You have access to a single \"random\" time line\n",
        "\n",
        "-   And in time series, one needs to be quite aware that Data has Baggage...What you see today is the product of everything that happens in the far past.\n",
        "\n",
        "-   This is what we call Past Dependent, or simple serial correlation.\n",
        "\n",
        "    -   And is why we need to be careful when we use time series data.\n",
        "\n",
        "## The nature of time series data\n",
        "\n",
        "-   Data cannot not be arbitrarily reordered. (Past affect future)\n",
        "\n",
        "    -   Typical features: serial correlation/nonindependence of observations\n",
        "\n",
        "-   Randomness of the data comes from the uncertainty of shocks that affects a variable over time, not from sampling.\n",
        "\n",
        "-   Your \"Sample\" is one realized path that you observe in a narrow window of time.\n",
        "\n",
        "-   Because observations are no longer independent, we will need to worry about correlation across time.\n",
        "\n",
        "-   In fact, because data may be strongly correlated across time (say your age), it may generate some problems when applying OLS.\n",
        "\n",
        "    -   Highly correlated data (high innertia) will have common \"trends\" that do not necessarity reflect the causal relationship between variables.\n",
        "\n",
        "-   So, we must learn \"new\" tools to deal with this problem.\n",
        "\n",
        "# Basic TS models\n",
        "\n",
        "## 1: Static model\n",
        "\n",
        "-   The static model is the simplest model for analyzing time series data. (like SLRM)\n",
        "-   A Static model aims to find correlations between contemporaneous variables.\n",
        "    -   Implicity, this **assumes** there are no dynamic interactions among variables\n",
        "\n",
        "$$GDP_t = a_0 + a_1 educ_t + a_2 Invest_t + a_3 Unemp_t + u_t$$\n",
        "\n",
        "Education, investment and Unemployment rate are assumed to affect GDP contemporaneously. But Lags of Leads of the data has no effect on GDP.\n",
        "\n",
        "-   These models are not useful for Forecasting, and Only produces reasonable estimates under very strong assumptions (we will see this later).\n",
        "\n",
        "## 2: Finite Distributed Lag model (FDL)\n",
        "\n",
        "-   The FDL model is a simple extension of the static model that allows for dynamic interactions of independent variables.\n",
        "    -   ***Finite*** Because we choose How far back (lags) to add to the model\n",
        "    -   ***Distributed*** Because each lag will have a different effect on the dependent variable.\n",
        "-   Simple Example: $$fr_t = a_0 + a_1 te_t +e_t$$ $$fr_t = a_0 + a_1 te_t + a_2 te_{t-1}+ a_3 te_{t-2}+e_t$$\n",
        "\n",
        "$fr_t$: Fertility Rate; $te_t$: Tax exemption\n",
        "\n",
        "This is an FDL model with 2 lags.\n",
        "\n",
        "## \n",
        "\n",
        "-   More Generality, FDL of order **q** is defined as:\n",
        "\n",
        "$$y_t = a_0 + \\sum_{k=0}^q \\delta_k z_{t-k} + e_t$$\n",
        "\n",
        "-   You can choose Lags using F-statistic, but also considering the \"loss\" of Degrees of freedom.\n",
        "    -   More lags, less data to estimate the coefficients, more coefficients to estimate\n",
        "    -   Coefficients may suffer from High Collinearity\n",
        "    -   Allow us to draw inference on Duration of effects.\n",
        "-   Two Types of Effects:\n",
        "    -   Transitory effects $\\frac{\\partial y_t}{\\partial z_{t-q}}=\\delta_q$\n",
        "    -   Permanent effect $\\frac{\\partial y_t}{\\partial z}=\\sum \\frac{\\partial y_t}{\\partial z_{t-q}}=\\sum \\delta_k$\n",
        "\n",
        "## \n",
        "### What do you expect to see?\n",
        "\n",
        "::: {layout-ncol=2}\n",
        "\n",
        "![Transitory](images/paste-11.png)\n",
        "\n",
        "![Permanent](images/paste-12.png)\n",
        "\n",
        ":::\n",
        "\n",
        "- **Transitory** effects measure the short-term effect on outcome (Only of the additional unit)\n",
        "- **Permanent** effects measure the long-term effect on outcome (adding up Transitory effects)\n",
        "\n",
        "## 3: Infinite Distributed Lag model (IDL)\n",
        "\n",
        "- This is a more advanced model that allows for the effects of independent variables to last forever, but how?\n",
        "  - A model with infinite number of lags cannot be estimated...unless some restrictions are imposed.\n",
        "\n",
        "$$ \\text{Wrong: } y_t = a_0 + \\sum_{k=0}^{\\infty} \\delta_k z_{t-k} + e_t$$\n",
        "$$ \\text{Better: } y_t = a_0 + \\sum_{k=0}^{\\infty} \\gamma \\delta^k z_{t-k} + e_t$$\n",
        "\n",
        "- So we went from pretending to estimate an infinite number of coefficients $\\delta_k$ to estimating only two parameters $\\gamma$ and $\\delta$. \n",
        "    - This is called the **Geometric Distributed Lag** model.\n",
        "\n",
        "##\n",
        "\n",
        "- GDL requires an additional \"Trick\":\n",
        "\n",
        "$$\\begin{aligned}\n",
        "y_t &= a_0 + \\gamma z_t + \\gamma \\rho z_{t-1} + \\dots + e_t \\\\\n",
        "y_{t-1} &= a_0 + \\gamma z_{t-1} + \\gamma \\rho z_{t-2} + \\dots + e_{t-1}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "- Subtracting the second equation (times $\\rho$ ) from the first one, we get:\n",
        "  \n",
        "$$y_t =  \\rho y_{t-1} + a_0 (1-\\rho) + \\gamma z_t  + v_{t}$$\n",
        "\n",
        "Which requires really strong assumptions!\n",
        "\n",
        "- The short and Long effects are:\n",
        "\n",
        "$$\\text{Short}\\frac{\\partial y_t}{\\partial z_{t-k}}=\\gamma \\rho^k \\text{ and }\n",
        "\\text{Long}\\frac{\\partial y_t}{\\partial z}=\\frac{\\gamma}{1-\\rho}$$\n",
        "\n",
        "\n",
        "## 4: Rational Distributed Lag model (RDL)\n",
        "\n",
        "- Because IDL imposes strong assumptions on coefficients, we can relax them by allowing for lags. This is called the RDL model.\n",
        "$$y_t = a_0 + \\gamma_0 z_t + \\gamma_1 z_t  +\\delta y_{t-1} + e_t- \\rho e_{t-1}$$\n",
        "\n",
        "- Which has the following short and long effects:\n",
        "\n",
        "$$\\text{ Short:}\\frac{\\partial y_t}{\\partial z_t} = \\gamma_0  $$\n",
        "$$\\text{ Short:}\\frac{\\partial y_t}{\\partial z_{t-k}} = \\rho^{k-1}(\\rho \\gamma_0 + \\gamma_1) $$\n",
        "$$\\text{ Long:}\\frac{\\partial y_t}{\\partial z} = \\frac{\\gamma_0 + \\gamma_1}{1-\\rho}\n",
        "$$\n",
        "\n",
        "# Assumptions\n",
        "At least for M1 and M2\n",
        "\n",
        "## Assumptions: M1 and M2\n",
        "\n",
        "A1. Linear in Parameters: Same old, same old, $y_t = \\beta_0 + \\beta_1 x_{1t} + \\dots + \\beta_k x_{kt} + u_t$\n",
        "\n",
        "A2. No Perfect Collinearity: Also Same old, same old \n",
        "\n",
        "## \n",
        "### The Stronger ones\n",
        "\n",
        "$$X=\\begin{pmatrix}\n",
        "x_{11} & x_{12} & \\dots & x_{1k} \\\\\n",
        "x_{21} & x_{22} & \\dots & x_{2k} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{T1} & x_{T2} & \\dots & x_{Tk} \n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "A3. Zero Conditional Mean\n",
        "\n",
        "$$E(u_t|X)=0\n",
        "$$\n",
        "\n",
        "So that $X$ is strictly Exogenous (across all possible times).\n",
        "\n",
        "Not only $x_t$ should not be affected by $u_t$, but neither should $x_{t-1}$ nor $x_{t+1}$\n",
        "\n",
        "**A1-A3** will guarantee that OLS is unbiased.\n",
        "\n",
        "## What about Std Errors?\n",
        "\n",
        "A4: Strong Homoskedasticity\n",
        "\n",
        "$$Var(u_t|X)=\\sigma^2\n",
        "$$\n",
        "\n",
        "A5: No Serial Correlation (Correlation across time of the errors)\n",
        "\n",
        "$$Corr(u_t,u_s|X)=0 \\text{ for all } t\\neq s\n",
        "$$\n",
        "\n",
        "Also difficult to fulfill, because unobserved may have inertia, and depend on past values.\n",
        "\n",
        "##\n",
        "\n",
        "Nevertheless, A1-A5: Standard errors can be estimated using the usual formula:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\hat{Var}(\\hat{\\beta}) &= \\hat{\\sigma}^2(X'X)^{-1} \\\\\n",
        "\\hat{Var}(\\hat{\\beta_k}) &= \\frac{\\hat{\\sigma}^2}{SST_k(1-R^2_k)} \\\\\n",
        "\\hat \\sigma^2 &= \\frac{1}{T-k-1}\\sum_{t=1}^T \\hat{u}_t^2\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Which are BLUE! (Best Linear Unbiased Estimators)\n",
        "\n",
        "A6: **Normality**, The $\\beta$'s are normally distributed, and F-tests and t-tests are valid.\n",
        "\n",
        "## Example: The effet of inflation and Deficit on Interest rates {.scrollable}\n",
        "\n",
        "Model: $i_t = \\beta_0 + \\beta_1 inf_t + \\beta_2 def_t + u_t$\n",
        "\n",
        "A1: $\\checkmark$ (but questionable)\n",
        "\n",
        "A2: $\\checkmark$ (almost never a problem)\n",
        "\n",
        "A3: NO! Deficits and inflation today may affect adjustments in the future ($u_{t+1}$), Similarly,  $u_t$ may have to be adjusted in the future using Deficits and inflation.\n",
        "\n",
        "A4: Perhaps? Usually there is a direct relationship between deficit and uncertainty, which will generate heteroskedasticity.\n",
        "\n",
        "A5: NO! There could be many things in $u_t$ that are correlated across time. (taxes?)\n",
        "\n",
        "A6: NO...the errors are almost never normal\n"
      ],
      "id": "689dbb87"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| code-fold: false\n",
        "*| classes: larger\n",
        "\n",
        "frause intdef, clear\n",
        "reg i3 inf def"
      ],
      "id": "3791da2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extending the Basic Model\n",
        "\n",
        "## 1: Event Studies\n",
        "\n",
        "- We can use Dummies to represent Transitory shocks (events) on the outcome\n",
        "  - Dummies for the impact of Covid (if we assume effect was transitory), 0 for all periods except for months we were at home.\n",
        "- Or use Dummies to capture permanent changes in the outcome\n",
        "  - Dummies for ChatGPT. 0 before the introduction, 1 after\n",
        "- Possible to use Lags of Dummies to see the dynamics of the impact.\n",
        "  - With Time series may not be as useful, because its easy to mix event effects with trends, although one could also directly control for trends. \n",
        "\n",
        "$$FRate_t = 98.7 + 0.08 PE_t - 24.24 WW2 - 31.6 Pill_t+ e_t$$\n",
        "\n",
        "- $Pill_t$, $WW2_t$ are dummies for the introduction of the pill (permanent) and WW2 (transitory) effects on Fertility rate.\n",
        "\n",
        "## 2: Logs and Growth models\n",
        "\n",
        "- Very Similar to what was done in Cross Sectional Models. \n",
        "\n",
        "- Using Logs of the Dep variable changes the interpretation of the coefficients.\n",
        "\n",
        "$$\\Delta log(x)\\simeq \\%\\Delta x$$\n",
        "\n",
        "- Because of that, you can use \"log-models\" and a trend to estimate the growth rates.\n",
        "\n",
        "`reg log_gdp year`\n",
        "\n",
        "The coefficient of `year` should give you the average growth rate of GDP.\n",
        "\n",
        "- But the model can also be used in levels to identify trends.\n",
        "\n",
        "## 3: Trends and Seasonality{.scrollable}\n",
        "\n",
        "- Trends are very common in time series data.\n",
        "  - Because of the \"inertia\" of the data, its very common to see variables sharing common trends even if they are completely unrelated. (GDP and age)\n",
        "- Ignoring this may cause problems, as one may identify spurious relationships. (things that look to have significant effects, even tho they are not related)\n",
        "  \n",
        "- Consider the following model (investment on housing, and housing prices):\n"
      ],
      "id": "356a7fb3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| classes: larger\n",
        "*| echo: true\n",
        "qui:frause hseinv,clear\n",
        "reg  linvpc lprice"
      ],
      "id": "b8521f01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- If we estimate this model, we find a very strong relationship, perhaps because of common trends. Adding a trend, however, may change the results.\n",
        "\n",
        "$$E(log(invpc_t)|x) = -20.04 -0.38 log(price) + 0.009 year $$\n"
      ],
      "id": "d40c5c82"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| classes: larger\n",
        "*| echo: true\n",
        "\n",
        "reg linvpc lprice  year"
      ],
      "id": "b2754e48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3: Trends and Seasonality {.scrollable}\n",
        "\n",
        "- Just as time series are characterized by trends, they are also characterized by seasonality.\n",
        "  - Seasonality is the presence of regular patterns in the data that repeat over fixed periods of time.\n",
        "  - Seasonality is a form of \"deterministic\" variation, because it is predictable.\n",
        "- For example, If you look at Public expenditure, you will see that its higher the last year that a president is in office. (election year)\n",
        "- Similarly, you will see higher expenditure in December, because of Christmas.\n",
        "- As with trends, this may cause spurious relations, thus, its recommended to control for seasonality adding dummies.\n",
        "  - quarter, month, day of the week, year after election, etc.\n",
        "\n",
        "- As simple as adding dummies for each month, or quarter, etc.\n",
        "\n",
        "## 4: $R^2$ and Spurious Regressions\n",
        "\n",
        "- One of the consequences of spurious regressions is that the $R^2$ will be inflated. (caputred by the common trend or seasonality\n",
        "- Even if we add trends or seaonalities, the default $R^2$ will be too large. (Because it still describes ALL variation)\n",
        "\n",
        "- A better approach to understand the true explanatory power of the model is to use an **$R^2$** that adjusts for trends and seasonality.\n",
        "\n",
        "$$y_t = \\beta_0 + \\beta_1 x_{1t} + \\beta_2 x_{2t} + \\theta \\times t + \\sum \\gamma_k \\times D_k + u_t$$\n",
        "\n",
        "Where $D_k$ are dummies for seasonality, and $\\theta \\times t$ is a trend. \n",
        "\n",
        "## 4: $R^2$ and Spurious Regressions\n",
        "\n",
        "- To estimate the adjusted $R^2$ it may be better to use de-trended and de-seasonalized data.\n",
        "\n",
        "$$\\tilde w_t = w_t - E(w_t| t , D_1, D_2, \\dots, D_k) \\forall w \\in {y, x_1, x_2}$$\n",
        "\n",
        "- Estimate model\n",
        "\n",
        "$$\\tilde y_t = \\beta_1 \\tilde x_{1t} + \\beta_2 \\tilde x_{2t} + u_t$$\n",
        "\n",
        "- Calculate the $R^2$ using the \"correct\" $SST$ and $SSE$ using the demeaned data.\n",
        "\n",
        "$$aR^2 = 1-\\frac{\\sum \\hat u^2_t}{\\sum \\tilde y^2_t}$$\n",
        "\n",
        "# Thats all for today!\n",
        "Next week, Advanced Time series"
      ],
      "id": "50f31de0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "nbstata",
      "language": "stata",
      "display_name": "Stata (nbstata)",
      "path": "C:\\Users\\Fernando\\AppData\\Roaming\\jupyter\\kernels\\nbstata"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}