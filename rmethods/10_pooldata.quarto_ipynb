{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Pool Cross-section and Panel Data\"\n",
        "subtitle: \"One year is no longer enough\"\n",
        "author: Fernando Rios-Avila\n",
        "jupyter: nbstata\n",
        "format: \n",
        "  revealjs: \n",
        "    slide-number: true\n",
        "    width: 1600\n",
        "    height: 900\n",
        "    code-fold: true\n",
        "    code-overflow: wrap\n",
        "    echo: true\n",
        "    css: styles.css \n",
        "    chalkboard: true  \n",
        "---\n",
        "\n",
        "\n",
        "## Pooling Data together: Cross-section and Panel Data\n",
        "\n",
        "- Up to this point, we have cover the analysis of cross-section data. \n",
        "  - Many individuals at a single point in time.\n",
        "- Towards the end of the semester, We will also cover the analysis of time series data.\n",
        "  - A single individual across time.\n",
        "- Today, we will cover the analysis of panel data and repeated crossection: Many individuals across time.\n",
        "  \n",
        "- This type of data, also known as longitudinal data, has advantages over crossection, as it provides more information that helps dealing with the unknown of $e$.\n",
        "\n",
        "- And its often the only way to answer certain questions.\n",
        "  \n",
        "## Pooling independent crossections\n",
        "\n",
        "- We first consider the case of independent crossections. \n",
        "  - We have access to surveys that may be collected regularly. (Household budget surveys)\n",
        "  - We assume that individuals across this surveys are independent from each other (no panel structure).\n",
        "- This scenario is typically used for increasing sample-sizes and thus power of analysis (*larger N smaller SE*)\n",
        "- Only minor considerations are needed when analyzing this type of data.\n",
        "  - We need to account for the fact Data comes from different years. This can be done by including year dummies.\n",
        "  - May need to Standardize variables to make them comparable across years. (inflation adjustments, etc.)\n",
        "\n",
        "## Example {.scrollable}\n",
        "\n",
        "Lets use the data `fertil1` to estimate the changes in fertility rates across time. This data comes from the *General Social Survey*.\n"
      ],
      "id": "45e8bd11"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "frause fertil1, clear\n",
        "regress kids educ age agesq black east northcen west farm othrural town smcity i.year, robust  "
      ],
      "id": "0a2f464d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- This allow us to see how fertility rates have changed across time.\n",
        "- One could even interact the year dummies with other variables to see how the effect of other variables have changed across time.\n"
      ],
      "id": "1deef900"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "frause cps78_85, clear\n",
        "regress lwage i.year##c.(educ i.female) exper expersq union, robust cformat(%5.4f)"
      ],
      "id": "f4df7a21",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Good old Friend: Chow test\n",
        "\n",
        "- The Chow test can be used to test whether the coefficients of a regression model are the same across two groups. \n",
        "  - we have seen this test back when we were discussing dummy variables.\n",
        "- We can also use this test to check if coefficients of a regression model are the same across two time periods. (Has the wage structure changed across time?)\n",
        "  - This is the case of interest here.\n",
        "- Not much changes with before. Although it can be a bit more tedious to code.\n",
        "\n",
        "## Example {.scrollable}\n"
      ],
      "id": "ff66c901"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "frause cps78_85, clear\n",
        "regress lwage i.year##c.(educ i.female exper expersq i.union), robust"
      ],
      "id": "2d0b8936",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| code-fold: false\n",
        "test 85.year#c.educ 85.year#1.female 85.year#c.exper   85.year#c.expersq 85.year#1.union"
      ],
      "id": "863b4f5d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Pool Crossection for Causal Inference\n",
        "\n",
        "- One advantage of pooling crossection data is that it could to be used to estimate causal effects using a method known as Differences in Differences (DnD)\n",
        "\n",
        "- Consider the following case:\n",
        "  - There was a project regarding the construction of an incinerator in a city. You are asked to evaluate what the impact of this was on the prices of houses around the area. \n",
        "  - You have access to data for two years: 1978 and 1981.\n",
        "  - In 1978, there was no information about the project. In 1981, the project was announced, but it only began operations in 1985.\n",
        "\n",
        "##\n",
        "\n",
        "- we could start estimating the project using the simple model:\n",
        "$$rprice = \\beta_0 + \\beta_1 nearinc + e$$\n",
        "\n",
        "using only 1981 data. But this would not be a good idea. Why?\n"
      ],
      "id": "69786142"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| classes: larger\n",
        "frause kielmc, clear\n",
        "regress rprice nearinc if year == 1981, robust"
      ],
      "id": "6cc747e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \n",
        "\n",
        "- We could also estimate the model using only 1971 data.\n",
        "  What would this be showing us?\n"
      ],
      "id": "6e1deede"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| classes: larger\n",
        "regress rprice nearinc if year == 1978, robust"
      ],
      "id": "3ee5843c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##\n",
        "\n",
        "- So, using 1981 data we capture the Total price difference between houses near and far from the incinerator. \n",
        "  - This captures both the announcement effect of the project, but also other factors (where would an incinerator be built?).\n",
        "- Using 1978 data we capture the price difference between houses near and far from the incinerator in the absence of the project. \n",
        "  - This captures the effect of other factors that may be correlated with the incinerator project.\n",
        "- Use both to see the impact!\n",
        "\n",
        "$$Effect = -30688.27-(-18824.37)= -11863.9$$\n",
        "\n",
        "- This is in essence a DnD model\n",
        "\n",
        "## Difference in Differences\n",
        "\n",
        "\n",
        "|  | Control| Treatment | Treat-Control |\n",
        "|---|---|---|---|\n",
        "| Pre-            | $\\bar y_{00}$ | $\\bar y_{10}$| $\\bar y_{10}$-$\\bar y_{00}$ |\n",
        "| Post-           | $\\bar y_{01}$ | $\\bar y_{11}$ | $\\bar y_{10}$-$\\bar y_{00}$ |\n",
        "| Post-pre        | $\\bar y_{01}$-$\\bar y_{00}$ | $\\bar y_{11}$-$\\bar y_{10}$ | DD  |\n",
        "  \n",
        "- Post-Pre: \n",
        "  - Trend changes for the control\n",
        "  - Trend changes for the treated: A mix of the impact of the treatment and the trend change.\n",
        "- Treat-Control: \n",
        "  - Baseline difference when looking at Pre-period\n",
        "  - Total Price differentials when looking at Post-period: Mix of the impact of the treatment and the baseline difference.\n",
        "\n",
        "- Take the Double Difference and you get the **treatment effect**.\n",
        "\n",
        "## Difference in Differences: Regression {.scrollable}\n",
        "\n",
        "- This could also be achieved using a regression model:\n",
        "\n",
        "$$ y = \\beta_0 + \\beta_1 post + \\beta_2 treat + \\beta_3 post*treat + e$$\n",
        "\n",
        "Where $\\beta_3$ is the treatment effect. (only for 2x2 DD)\n"
      ],
      "id": "f716960e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "regress rprice nearinc##y81, robust"
      ],
      "id": "16ee4a05",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Difference in Differences: Regression + controls \n",
        "\n",
        "- One advantage of DD is that it can control for those unobserved factors that may be correlated with outcome. \n",
        "  - Without controls, however, estimates may not have enough precision.\n",
        "- But, we could add controls!\n",
        "\n",
        "$$ y = \\beta_0 + X \\gamma + \\beta_1 post + \\beta_2 treat + \\beta_3 post*treat + e$$\n",
        "\n",
        "- But its not as easy as it may seem! (just adding regressions is not a good approach)\n",
        "\n",
        "- This method requires other assumptions! ($\\gamma$ is fixed), which may be very strong.\n",
        "\n",
        "\n",
        ">[**Note:**]{.redtxt} For DD to work, you need to assume the two groups follow the same path in the absence of the treatment. (Parallel trends assumption)\n",
        ">\n",
        ">Otherwise, you are just using trend differences!\n",
        "\n",
        "## Diff in Diff in Diff\n",
        "\n",
        "An Alternative approach is to use a triple difference model.\n",
        "\n",
        "Setup:\n",
        "\n",
        "- You still have two groups: Control and Treatment (which are easily identifiable)\n",
        "- You have two time periods: Pre and Post (which are also easily identifiable)\n",
        "- You have a different sample, where you can identify controls and treatment, as well as the pre- and post- periods. This sample was not treated!\n",
        "\n",
        "Estimation: \n",
        "\n",
        "- Estimate the DD for the Original Sample, and the new untreated sample. \n",
        "- Obtaining the difference between these two estimates will give you the triple difference.\n",
        "\n",
        "Example: Smoking ban analysis based on age. (DD) But using both treated and untreated States (DDD)\n",
        "\n",
        "## General Framework and Pseudo Panels\n",
        "\n",
        "- One general Structure for Policy analysis is the use of Pseudo Panels structure.\n",
        "  - Pseudo panels are a way to use repeated crossection data, but controlling for some unobserved heterogeneity across specific groups. (the pseudo panels)\n",
        "- For Pseudo-panels, we need to identify a group that could be followed across time. \n",
        "  - This cannot be a group of individuals (repeated crosection). \n",
        "  - But we could use groups of states, cohorts (year of birth), etc.\n",
        "- In this case, the data would look like this:\n",
        "$$y_{igt} = \\lambda_t + \\alpha_g + \\beta x_{gt} + z_{igt}\\gamma +  e_{igt}$$\n",
        "\n",
        "- Where $g$ is the group, $t$ is the time, and $i$ is the individual.\n",
        "- This model can be estimated by using dummies. (one dummy for each group and time-period)\n",
        "- And $\\beta$ is the coefficient of interest. (impact of the Policy $x_{gt}$).\n",
        "  - This may ony work if we assume $\\beta$ is constant across time and groups.\n",
        "\n",
        "## Alternative\n",
        "\n",
        "- We could also use a more general model:\n",
        "$$y_{igt} = \\lambda_{gt}+ \\beta x_{gt} + z_{igt}\\gamma +  e_{igt}$$\n",
        "\n",
        "- where $\\lambda_{gt}$ is a group-time fixed effect. (Dummy for each group-time combination) \n",
        "  - Nevertheless, while more flexible, this also imposes other types of assumptions, and might even be unfeasible if we have a large number of groups and time periods.\n",
        "\n",
        "- Still, we require $\\beta$ to be homogenous. If that is not the case, you may still suffer from contamination bias.\n",
        "\n",
        "# Panel data\n",
        "Baby steps: 2 period panel data\n",
        "\n",
        "## 2-period Panel data\n",
        "\n",
        "- Panel Data, or longitudinal data, is a type of data that has information about the same individual across time. \n",
        "\n",
        "- The simplest Structure is one where individuals are followed over only 2 periods. \n",
        "  \n",
        "- The main advantage of panel data (even two periods version) is that it allows us to control for unobserved heterogeneity across individuals.\n",
        "  - But only if you want to assume fixed effects are constant across time.\n",
        "\n",
        "- Main caveat. For the methods we will see next, you cannot esitmate the effect of variables that are time fixed (or have little if any variation across time).\n",
        "- \n",
        "## \n",
        "\n",
        "- So how does this reflects in the model specification?\n",
        "\n",
        "$$y_{it} = \\beta_0 + \\beta_1 x_{it} + \\beta_2 z_{t} + \\beta_3 w_{i} + e_i + e_t + e_{it}$$\n",
        "\n",
        "- Where $i$ refers to individuals or panel units, and $t$ refers to time periods.\n",
        "- Also, $X's$, $X's$ $W's$ are variables that vary across individual and time, across time or across individuals.\n",
        "- There are also three types of errors. Those that contains unobserved that vary across individuals $e_i$, across time $e_t$, and across individuals and time $e_{it}$ (Idiosyncratic error).\n",
        "  \n",
        "- $e_i$ is usually referred to as the individual fixed effect, and $e_t$ as the time fixed effect.\n",
        "\n",
        "- In a 2 period panel, controlling for time-effects is may not be necessary (its just one dummy)\n",
        "\n",
        "- What is more concerning is the unobserved individual fixed effect. \n",
        " \n",
        "This is pretty similar to the generalized Pooling model we saw before.\n",
        "\n",
        "## How estimation changes\n",
        "\n",
        "For time use, we assume we control with a single dummy.\n",
        "\n",
        "1. You can choose to \"ignore\" individual effects. \n",
        "\n",
        "$$y_{it} = \\beta_0 + \\beta_1 x_{it} + \\beta_2 w_{i} + \\delta t + \\underbrace{e_i + e_{it}}_{v_{it}}$$\n",
        "\n",
        "   - Requires $e_i$ to be uncorrelated with $x_{it}$ (otherwise is biased), and Standard Errors will need to be clustered at the individual level.\n",
        "\n",
        "2. You can aim to estimate **all** individual fixed effects using dummies (FE estimator).\n",
        "$$y_{it} = \\beta_0 + \\beta_1 x_{it} + \\delta t + \\sum \\alpha_i D_i + e_{it})$$\n",
        "\n",
        "   - Time fixed variables cannot be estimated anymore\n",
        "  \n",
        "## \n",
        "\n",
        "3. You can estimate the model in differences (FD estimator)\n",
        "\n",
        "$$\\begin{aligned}\n",
        "y_{i1} &= \\beta_0 + \\beta_1 x_{i1} + \\delta + e_i + e_{i1} \\\\\n",
        "y_{i0} &= \\beta_0 + \\beta_1 x_{i0} + e_i + e_{i0} \\\\\n",
        "\\Delta y_{i} &= \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta_1 \\Delta x_{i1} + \\delta + \\Delta e_{i}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "- Now you have only 1 observation per panel, instead of 2. And the result would be identical to FE estimator.\n",
        "\n",
        "## Example {.scrollable}\n"
      ],
      "id": "0f7661ff"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "** This data is in wide format\n",
        "frause slp75_81, clear\n",
        "** Lets reshape it so its in standard long format\n",
        "qui:gen id = _n\n",
        "qui:reshape long educ gdhlth marr slpnap totwrk yngkid, i(id) j(year)\n",
        "qui:gen dtime = year==81\n",
        "qui:xtset id dtime\n",
        "** Regression as Pool Crossection\n",
        "qui: reg slpnap dtime totwrk educ marr yngkid gdhlth male,  \n",
        "local cname:colnames e(b)\n",
        "display \"`cname'\"\n",
        "est sto m1\n",
        "** using FE\n",
        "qui: areg slpnap dtime totwrk educ marr yngkid gdhlth male, absorb(id)  \n",
        "est sto m2\n",
        "** using FD\n",
        "qui: reg d.slpnap d.dtime d.totwrk d.educ d.marr d.yngkid d.gdhlth d.male, \n",
        "matrix b = e(b)\n",
        "matrix V = e(V)\n",
        "matrix colname b = `cname'\n",
        "matrix colname V = `cname'\n",
        "matrix rowname V = `cname'\n",
        "adde repost b=b V=V, rename\n",
        "est sto m3\n",
        "esttab m1 m2 m3, se r2"
      ],
      "id": "d887afe8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Nx2 DID with Panel \n",
        "\n",
        "- Using panel data also allows you to analyze policies using canonical DID approach (2x2).\n",
        "  - It simplifies the process because one of the differences can be estiamated right away.\n",
        "- Consider the Standard DID Model:\n",
        "$$y_{it} = a_0 + a_1 post + \\color{red}{a_2 treat} + \\color{green}{a_3 post \\times treat}+ e_{it} $$\n",
        "\n",
        "- With panel data, you can extend this allowing for \"multiple groups\", and controling for whether Received treatment or not: \n",
        "\n",
        "$$y_{it} = a_0 + a_1 dtime + \\color{red}{a_i} + \\color{green}{a_3 treated} + e_{it} $$\n",
        "$$\\Delta y_{i} =  a_1 + \\color{green}{a_3 treated} + \\Delta e_{i} $$\n",
        "\n",
        "- Most effective way to control for unobserved individual effects!\n",
        "  \n",
        "##\n",
        "\n",
        ":::{.callout-warning}\n",
        "\n",
        "## Beware of the TWFE\n",
        "\n",
        "**warning** you may be tempted to say, with more time-periods, just add more dummies. But that would be wrong!\n",
        "\n",
        "Look for all the new literature on DID with Multiple periods and treatment timing\n",
        "\n",
        ":::\n",
        "\n",
        "## Panel Data with more than 2 periods\n",
        "\n",
        "- So far we have covered the case when you observe individuals for two periods only. What if you observe them for more than two periods?\n",
        "\n",
        "- The model is pretty much the same.\n",
        "  - You will mostly need to add an additional time dummy.\n",
        "  - You also have multiple approaches that would allow you to estimate the model.\n",
        "  - Adding Dummies still work.\n",
        "  - And Difference in Differences still work.\n",
        "  - You can also choose Randome-effect model (next class)\n",
        "\n",
        "## Differencing\n",
        "\n",
        "- Consider the following model:\n",
        "\n",
        "$$y_{it} = \\beta_0 + \\beta_1 x_{it} +a_i + \\delta_2 T_2 + \\delta_3 T_3 + e_{it}$$\n",
        "\n",
        "- Where $T_2$ and $T_3$ are time dummies for the second and third periods. If apply a first differences transformation, we get:\n",
        "\n",
        "$$\\Delta y_{it} = \\beta_1 \\Delta x_{it} + \\delta_2 + \\delta_3 T_3 + \\Delta e_{it}$$\n",
        "\n",
        "## \n",
        "\n",
        "- Mechanically, its the same as before. However,analytically, this imposes stronger assumptions\n",
        "\n",
        "$$Corr(\\Delta x_{it}, \\Delta e_{it}) =Corr(x_{it}-x_{it-1}, e_{it}-e_{it-1}) = 0$$\n",
        "\n",
        "- This is a stronger assumption than $Corr(x_{it}, e_{it}) = 0$. It implies that $x_{it}$ has to be strictly exogenous to errors $e_{it}$, regardless of timing.\n",
        "\n",
        "- One may want to also consider using Other Standard error corrections.\n",
        "\n",
        "## FD is to be used with caution\n",
        "\n",
        "- FD is an easy estimation with panel data. However, it should be used with caution\n",
        "  - It depends much more on the strict exogeneity assumption. (may create biases)\n",
        "  - If one adds Lagged Dep variables as controls, further problems may arise. \n",
        "  - Issues with measurement error are magnified when using First Differences as well. (and can be difficult to address)\n",
        "\n",
        "- But, there are still other methods that could be used to estimate panel data models with multiple periods:\n",
        "  - FE (dummy inclusion approach)\n",
        "  - Random Effects (next class)\n",
        "  - Correlated Random Effects (next class)\n",
        "\n",
        "# {background-image=\"https://i.imgflip.com/7zjmz3.jpg\" background-size=\"contain\"}\n",
        "\n",
        "# Thats all for today \n",
        "Next class...Advance Panel Data Methods"
      ],
      "id": "07296713"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "nbstata",
      "language": "stata",
      "display_name": "Stata (nbstata)",
      "path": "C:\\Users\\Fernando\\AppData\\Roaming\\jupyter\\kernels\\nbstata"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}