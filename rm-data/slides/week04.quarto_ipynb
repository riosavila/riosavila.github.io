{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Generalizing from Data\"\n",
        "author: \n",
        "  - name: Fernando Rios-Avila\n",
        "    affiliation: Levy Economics Institute  \n",
        "date: last-modified\n",
        "date-format: long\n",
        "format:\n",
        "  revealjs:\n",
        "    theme: [ clean2.scss]\n",
        "    slide-number: true\n",
        "    footer: \"*Rios-Avila and Cia*\"\n",
        "    width:  1200\n",
        "    height: 675\n",
        "jupyter: nbstata    \n",
        "execute: \n",
        "  freeze: auto    \n",
        "  cache: true\n",
        "---\n",
        "\n",
        "\n",
        "## Generalization\n",
        "\n",
        "-   Sometimes we analyze a dataset with the goal of learning about patterns in that dataset alone.\n",
        "    -   In such cases there is no need to generalize our findings to other datasets.\n",
        "    -   Example: We search for a good deal among offers of hotels, all we care about are the observations in our dataset.\n",
        "-   Often we analyze a dataset in order to learn about patterns that may be true in other situations.\n",
        "    -   We are interested in finding the relationship between our dataset and the situation we care about.\n",
        "    -   Example: Will the treatment we are studying work in other settings?\n",
        "\n",
        "# Generalization: Inference and External Validity {.left}\n",
        "\n",
        "## Generalization\n",
        "\n",
        "-   Goal: Generalize the results from a single dataset to other situations.\n",
        "\n",
        "-   The act of generalization is called **inference**: we infer something from our data about a more general sitatuation.\n",
        "\n",
        "    -   For this we want to test hypothesis based on our estimates (evidence)\n",
        "\n",
        "-   Two Things to consider\n",
        "\n",
        "    1.  Statistical inference: the process of using data (in hand) to infer the properties of a population. Identify **general pattern**.\n",
        "    2.  External validity: the extent to which our data represents the general pattern we care about in **other settings**.\n",
        "\n",
        "## Statistical inference\n",
        "\n",
        "-   There are several statistical methods to make inference.\n",
        "-   The general pattern (*`A model`*) is an abstract thing that may or may not exist.\n",
        "-   If we can assume that the general pattern exists, the tools of statistical inference can be very helpful.\n",
        "    -   If we find a positive relationship between two variables in our data, we can use statistical inference to say if the same relationship is [**likely**]{.red} in the population.\n",
        "\n",
        "## General patterns 1: Population and representative sample\n",
        "\n",
        "-   The cleanest example of **representative data** is a **representative** sample of a well-defined population.\n",
        "\n",
        "-   A sample is representative of a population if the distribution of all variables is very similar in the sample and the population. $$f(y,x,z,...)_{sample} \\approx f(y,x,z,...)_{population}$$\n",
        "\n",
        "-   Random sampling is the best way to achieve a representative sample.\n",
        "\n",
        "## General patterns 2: No population but general pattern\n",
        "\n",
        "-   \"Representation\" is less straightforward in other setups.\n",
        "-   There isn't a \"population\" from which a random sample was drawn on purpose.\n",
        "    -   Using the past to uncover a pattern of the future. (Time series)\n",
        "    -   Use analogy to generalize patterns on Products A into Products B. (requires external Validity)\n",
        "-   Instead, we should think of our data as one that represents a **general pattern** (a model).\n",
        "    -   $X\\beta$ exists, and each year is a random realization.\n",
        "    -   $X\\beta$ exists, and each product is a random version.\n",
        "-   You can think of a \"general pattern\" as the \"true\" model dictating the data.\n",
        "\n",
        "## External validity\n",
        "\n",
        "-   How likely is that what we learn is relevant other situations we care about?\n",
        "\n",
        "-   Are our findings unique to our data? or can they happen \"out there\"?\n",
        "\n",
        "    -   With external validity, our data can tell what to expect.\n",
        "    -   No external validity: whatever we learn from our data, may turn out to be not relevant at all.\n",
        "\n",
        "-   This has been a problem with RCTs in economics: the results are not always generalizable to other settings.\n",
        "\n",
        "## The process of inference\n",
        "\n",
        "The process of inference: \n",
        "\n",
        "1. Consider a statistic we may care about, such as the mean.\n",
        "2. Compute its estimated value from a dataset. \n",
        "3. Infer the value in the population, that our data represents.\n",
        "\n",
        "It is good practice to divide the inference problem into two: \n",
        "\n",
        "1. Use statistical inference to learn about the population the data represents. \n",
        "2. Assess external validity: Assess how the data in hand represents the population we care about.\n",
        "\n",
        "## Stock market returns: Inference\n",
        "\n",
        "-   **Task**: Assess the likelihood of experiencing a **loss** of 5% on an investment portfolio from one day to the next\n",
        "-   **Data**: day-to-day returns on the S&P 500, from 25 August 2006 to 26 August 2016: 2,519 days.\n",
        "-   **Finding**: 0.5% of the days in the dataset have a loss of 5% or more.\n",
        "-   Inference problem:\n",
        "    -   How can we generalize this finding? What can we infer from this 0.5% chance for the next calendar year?\n",
        "\n",
        "# Repeated samples {.left}\n",
        "\n",
        "## Repeated samples\n",
        "\n",
        "-   Normally, There is one sample. But, theoretical framework assumes you could obtain many (repeated) samples. ([Frequentist approach]{.blue})\n",
        "-   The goal of **statistical inference** is learning the value of a statistic in the population **represented** by our data.\n",
        "    -   But, each repeated samples, would give a different value of the statistic.\n",
        "-   Because of the different values, the **statistic** obtained with repeated samples will have a **distribution**\n",
        "    -   This is the **sampling distribution**.\n",
        "-   The **standard deviation** of the sampling distribution is what is called the **standard error** of the statistic (typical error across random samples).\n",
        "\n",
        "## Repeated samples properties\n",
        "\n",
        "The sampling distribution has three important properties:\n",
        "\n",
        "1.  **Unbiasedness**: The average of the values in repeated samples is equal to its true value (=the value in the entire population / general pattern).\n",
        "2.  **Asymptotic normality**: The sampling distribution is approximately normal. With large sample size, it is very very close.\n",
        "3.  **Root-n convergence**: The standard error (the standard deviation of the sampling distribution) is smaller the larger the samples, with a proportionality factor of the square root of the sample size.\n",
        "\n",
        "## Repeated samples\n",
        "\n",
        "-   Easier concept:\n",
        "    -   When data is sample from a well-defined population - many other samples could have turned out instead of what we have.\n",
        "    -   Example: Mexican firms - random sample - population of firms.\n",
        "-   Harder concept:\n",
        "    -   Some times there is no clear definition of population. (but there is a model).\n",
        "    -   Data of returns on an investment portfolio is as a particular realization of the history of returns that could have turned out differently.\n",
        "        -   Multiverse: many possible histories of returns, we see only one.\n",
        "\n",
        "## Case study\n",
        "\n",
        "### Stock market returns: A simulation\n",
        "\n",
        "-   We can not rerun history many many times...\n",
        "-   So we will run a **Simulation** exercise - to better understand how repeated samples work.\n",
        "-   Suppose the 11-year dataset is the population - the fraction of days with 5%+ losses is 0.5% in the entire 11 years' data. That's the true value.\n",
        "-   We assume we have **only** 500 days of daily returns in our dataset.\n",
        "-   Task: estimate the true value of the fraction in the 11-year period from the data we have using a simulation exercise.\n",
        "\n",
        "## Stock market returns: A simulation\n"
      ],
      "id": "f202d5cd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| code-fold: false\n",
        "*| output: false\n",
        "\n",
        "use data_slides/sp500.dta, clear\n",
        "gen return = (value - value[_n-1])/value[_n-1]\n",
        "gen lost5 = return < -0.05\n",
        "set seed 1\n",
        "\n",
        "** Simulation\n",
        "gen mn_lost5=.\n",
        "forvalues i = 1/1000 {\n",
        "    preserve\n",
        "      qui:sample 500, count\n",
        "      sum lost5 , meanonly\n",
        "    restore\n",
        "    qui:replace mn_lost5 = r(mean) in `i'\n",
        "}"
      ],
      "id": "dcec778a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stock market returns: A simulation\n"
      ],
      "id": "553e7a70"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: false\n",
        "*| fig-align: center\n",
        "qui:set scheme white2\n",
        "qui:color_style tableau\n",
        "histogram mn_lost5,d"
      ],
      "id": "a05598b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Measuring uncertainty\n",
        "\n",
        "How bad is the error in our estimate?\n",
        "\n",
        "## The standard error and the confidence interval\n",
        "\n",
        "-   Confidence interval (CI) is a measure of statistical inference that allows some margin of error.\n",
        "-   The CI defines a range where we can expect the true value in the population, with a probability.\n",
        "-   Probability tells how likely it is that the true value is in that range, if we were to draw many **repeated samples**.\n",
        "\n",
        "## If E(X) and SE(X) are known\n",
        "\n",
        "-   If we know the true value of a statistic and its standard error, then we can calculate the CI.\n",
        "    -   The CI is centered around the true value of the statistic.\n",
        "    -   This CI is the range of values that we can expect the sample statistic to fall in, with a certain probability.\n",
        "    -   for example, 95% CI: the sample statistic will fall within the CI in 95% of the repeated samples.\n",
        "    -   But in 5% of the cases, the sample statistic will fall outside the CI.\n",
        "\n",
        "## If E(X) and SE(X) are not known\n",
        "\n",
        "-   When we say \"95% CI\", we mean that if we were to draw many repeated samples, the true value would fall within the CI in 95% of the cases.\n",
        "-   However, it also means that in 5% of the cases, the true value would fall outside the CI.\n",
        "    -   This means, some times (5% of the cases) we will be wrong.\n",
        "\n",
        "## Confidence interval\n",
        "\n",
        "-   CI is almost always **symmetric** around the estimated value of the statistic in our dataset. (if we assume normality of the sampling distribution)\n",
        "\n",
        "-   How to calculate the CI?\n",
        "\n",
        "    -   Get estimated value.\n",
        "    -   Define probability, confidence level (Say 95%).\n",
        "    -   Calculate CI with the use of SE. $$95\\% CI= \\hat\\mu \\pm 1.96SE$$\n",
        "\n",
        "-   Under Normality, 90% CI is the ±1.645SE interval, the 99 % CI is the ±2.576SE.\n",
        "\n",
        "-   But we commonly use the rule of 2: ±2SE.\n",
        "\n",
        "## Calculating the standard error\n",
        "\n",
        "-   Estimating the sample mean $\\bar{x}$ is easy. But how do we estimate the standard error?\n",
        "    -   In reality, we don't get to observe the **sampling distribution**. Instead, we observe a single dataset.\n",
        "    -   That dataset is one of many potential samples that could have been drawn from the population.\n",
        "-   **Good news**: We can get a very good idea of how the sampling distribution would look like - good estimate of the standard error - even from a single sample.\n",
        "-   Getting SE – Option 1: Use a formula. $\\leftarrow$ Theoretical approach.\n",
        "-   Getting SE – Option 2: Simulate $\\leftarrow$, The bootstrap method.\n",
        "\n",
        "## Calculating the standard error\n",
        "\n",
        "Consider the statistic of the sample mean.\n",
        "\n",
        "-   Assume the values of $x$ are ***independent*** across observations in the dataset.\n",
        "-   $\\bar{x}$ is the estimate of the true mean value of $x$ in the population.\n",
        "-   *Assume* sampling distribution is approximately normal, with the true value as its mean.\n",
        "\n",
        "The standard error formula for the estimated $\\bar{x}$ is $$SE (\\bar{x}) = \\frac{1}{\\sqrt{n}} Std[x]$$\n",
        "\n",
        "where $Std[x]$ is the standard deviation of the variable $x$ in the data and $n$ is the number of observations in the data.\n",
        "\n",
        "## The standard error formula\n",
        "\n",
        "-   The standard error is larger...\n",
        "    -   the larger the standard deviation of the variable.\n",
        "    -   the smaller the sample \n",
        "-   The larger the standard error, the wider the confidence interval, and the less precise the estimate (wider CI).\n",
        "\n",
        "# External validity\n",
        "\n",
        "## External validity\n",
        "\n",
        "-   In statistical inference the CI represents the uncertainty about the true value of the statistic in the population that our data represents.\n",
        "-   But What is the population, we care about? How close is our data to this?\n",
        "-   External validity: Can we generalize the pattern we found in our data to other situations?\n",
        "-   High external validity: if our data is close to the population.\n",
        "-   External validity is as important as statistical inference, but it is not a statistical question.\n",
        "\n",
        "## External validity\n",
        "\n",
        "-   The three most important challenges to external validity are:\n",
        "    -   Time: we have data on the past, but we care about the future.\n",
        "    -   Space: our data is on one country, but interested how a pattern would hold elsewhere in the world.\n",
        "    -   Sub-groups: our data is on 25-30 year old people. Would a pattern hold on younger / older people?\n",
        "\n",
        "## External validity: Portafolio Example\n",
        "\n",
        "-   Daily 5%+ loss probability with a 95% CI \\[0.2, 0.8\\] in our sample. This captures uncertainty.\n",
        "-   External Validity: Would this data be representative of the events of one year in the future?\n",
        "    -   Probably not, because the future is uncertain.\n",
        "    -   Our data: 2006-2016 dataset includes the financial crisis and great recession of 2008-2009. uncertain if the future will have similar events.\n",
        "-   Hence, the real CI is likely to be substantially wider.\n",
        "\n",
        "## External validity: Managers Example\n",
        "\n",
        "-   Manager and firm size evidence in Mexico.\n",
        "-   How to think about external validity?\n",
        "    -   Would the same patterns hold in other countries? Develped countries? Emerging markets?\n",
        "    -   Would the same patterns hold in other sectors? Other industries?\n",
        "-   Only Mexico? only firms of a certain size?\n",
        "\n",
        "\n",
        "# The bootstrap {.left background-image=\"images/bootstraps.jpg\" background-position=\"right\" background-size=\"contain\"}\n",
        "\n",
        "## The bootstrap\n",
        "\n",
        "-   Bootstrap is a method to create synthetic samples that are [*similar*]{.red} but [*different*]{.blue}.\n",
        "-   An method that is very useful in general.\n",
        "    -   The method you use, when you don't know...\n",
        "-   It is essential for many advanced statistics applications such as machine learning.\n",
        "-   The bootstrap is a method to estimate uncertainty in a statistic, that uses the data itself.\n",
        "\n",
        "> to lift oneself by one's bootstraps\n",
        "\n",
        "## The bootstrap\n",
        "\n",
        "-   The bootstrap method takes the original dataset and draws many repeated samples (with replacement) of the size of that dataset.\n",
        "\n",
        "-   Say you have a dataset of 10 observations, named 1, 2, 3, ..., 10.\n",
        "\n",
        "    -   Bootstrap sample 1: 2, 5, 5, 7, 8, 9, 9, 9, 10, 10.\n",
        "    -   Bootstrap sample 2: 1, 1, 2, 3, 4, 5, 6, 7, 8 , 9.\n",
        "    -   And so on, repeated many times.\n",
        "\n",
        "-   Each new sample is called a ***bootstrap sample***.\n",
        "\n",
        "## The bootstrap\n",
        "\n",
        ":::{.columns}\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "- a Bsample is (almost) always the same size as the original dataset.\n",
        "- Some Data is repeated, some is left out.\n",
        "- Typically, we require between 500-10,000 bootstrap samples. (Stata's default is 50)\n",
        "- Computationally intensive, but feasible\n",
        ":::\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "![](images/paste-4.png)\n",
        ":::\n",
        "\n",
        "\n",
        ":::\n",
        "\n",
        "## The bootstrap method: How does it work?\n",
        "\n",
        "-   For each BSample, you estimate the statistic of interest. (e.g. mean)\n",
        "-   The distribution of the statistic across these repeated bootstrap samples is a good approximation to the sampling distribution.\n",
        "-   In this case, the bootstrap Standard Error is the standard deviation of the statistic across the bootstrap samples.\n",
        "-   Also, the 95% CI is the 2.5th and 97.5th percentiles of the distribution of the statistic across the bootstrap samples. Or you can use the estimated SE.\n",
        "   \n",
        "## Stock market returns: The Bootstrap standard error\n",
        "### Stata Corner\n",
        "\n",
        "*Bootstraping* in Stata can be easy. Most commands have a built-in bootstrap option. Otherwise, we can program it!\n"
      ],
      "id": "17edc010"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| code-fold: false\n",
        "*| output: false\n",
        "display \"Bootstrap\"\n",
        "bootstrap mean=r(mean), nowarn reps(1000) seed(1) dots(100): sum lost5, meanonly\n",
        "est sto m1\n",
        "display \"Formula\"\n",
        "mean lost5\n",
        "est sto m2"
      ],
      "id": "e8affff1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stock market returns: The Bootstrap standard error\n",
        "### Stata Corner\n"
      ],
      "id": "5164f64f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: false\n",
        "*| code-fold: false\n",
        "*| output: asis\n",
        "set linesize 255\n",
        "esttab m1 m2, ci wide nonumber  b(%5.4f) md compress nostar note(\"\") varlabel(mean \"Formula\" lost5 \"Bootstrap\") "
      ],
      "id": "b7205307",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generalization - Summary\n",
        "\n",
        "-   Generalization is a key task - finding beyond the actual dataset.\n",
        "-   This process is made up of discussing statistical inference and external validity.\n",
        "-   Statistical inference generalizes from our dataset to the population using a variety of statistical tools.\n",
        "-   External validity is the concept of discussing beyond the population for a general pattern we care about; an important but typically somewhat speculative process. \n",
        "\n",
        "\n",
        "# **Break**\n",
        "\n",
        "# Hypothesis Testing\n",
        "\n",
        "## Motivation\n",
        "\n",
        "-   The internet allowed the emergence of specialized online retailers while brick-and-mortar shops also sell goods on the main street. How to measure price inflation in the age of these options?\n",
        "-   To help answer this, we can collect and compare online and offline prices of the same products and test if they are the same.\n",
        "\n",
        "# Hypothesis Testing\n",
        "\n",
        "The truth is out there, but its unknowable.\n",
        "\n",
        "## The logic of hypothesis testing\n",
        "\n",
        "-   A hypothesis is a statement about the population parameter, of which we are not sure if true or not.\n",
        "-   Hypothesis testing = analyze our data to make a decision on the hypothesis\n",
        "-   **Reject** the hypothesis if there is enough evidence against it.\n",
        "-   **Don't reject** it if there isn't enough evidence against it.\n",
        "    -   But NEVER accept it as true.\n",
        "-   Important *asymmetry* here: rejecting a hypothesis is a more conclusive decision than not rejecting it.\n",
        "\n",
        "##  Inference\n",
        "\n",
        "-   Testing a hypothesis: making inference with a focus on a specific statement.\n",
        "    -   Hypothesis: It is cheaper to buy online than offline.\n",
        "-   Can answer questions about the population represented by our data.\n",
        "-   But, It is an inference: have to assess external validity.\n",
        "\n",
        "## The setup\n",
        "\n",
        "-   Define the the statistic we want to test, $s$ (e.g. mean).\n",
        "-   We are interested in the **true** value of $s$, $s_{true}$.\n",
        "    -   This implies the true value in the population.\n",
        "-   The value of the statistic in our data is its estimated value, denoted by a hat on top $\\hat{s}$.\n",
        "\n",
        "## Hypothesis testing: H0 vs HA\n",
        "\n",
        "-   Need to formally state the question as two **competing** hypotheses of which only one can be true:\n",
        "    -   a null hypothesis $H_0$ and an alternative hypothesis $H_a$.\n",
        "-   They are formulated in terms of the unknown true value of the statistic. (we now the sample value)\n",
        "-   Together they cover all possibilities.\n",
        "\n",
        "> $H_0$: Online and offline prices are the same. $H_a$: Online and offline prices are different.\n",
        "\n",
        "## The Null is protected\n",
        "\n",
        "> Innocent (H0) until proven guilty (Ha)\n",
        "\n",
        "-   Testing a hypothesis $H_0$= see if there is enough evidence in our data to **reject** the null.\n",
        "\n",
        "-   The null is protected: We start assuming the Null is true\n",
        "    -   If we have **strong** evidence against it, we reject it\n",
        "    -   If not, we don't reject it.\n",
        "\n",
        "\n",
        "## Types of testing: $H_0$ vs $H_a$\n",
        "\n",
        "-   There are two types of Hypothesis:\n",
        "    -   Two-sided alternative: We are interested if the true value of the statistic is different from the hypothesized value.\n",
        "\n",
        "$$H_0: \\theta = 42 \\ \\ vs  \\ \\ H_A: \\theta \\neq 42$$\n",
        "\n",
        "-   One-sided alternative: We are interested if the true value of the statistic is greater or smaller than the hypothesized value. $$H_0: \\theta \\leq 42 \\ \\ vs  \\ \\ H_A: \\theta > 42$$\n",
        "\n",
        "## The logic of hypothesis testing\n",
        "\n",
        "-   $H_A$ is (often) what I want to prove\n",
        "-   $H_0$ is what I wanna reject so that we can prove $H_A$\n",
        "-   $H_0$ is not rejected\n",
        "    -   not enough evidence or\n",
        "    -   true (ie $H_A$ is false)\n",
        "-   I can never say $H_0$ is true.\n",
        "\n",
        "## Case Study - online vs offline prices\n",
        "\n",
        "-   **Question**: Do the online and offline prices of the same products differ?\n",
        "-   This data includes 10 to 50 products in each retail store included in the survey (the largest retailers in the U.S. that sell their products both online and offline).\n",
        "-   The products were selected by the data collectors in offline stores, and they were matched to the same products the same stores sold online.\n",
        "-   The statistic of interest is the difference in average prices.\n",
        "\n",
        "## \n",
        "\n",
        "-   Each product $i$ has an off-line and on-line price.\n",
        "\n",
        "-   The statistic with $n$ observations (products) in the data, is: $$s = \\bar{p}_\\text{diff} = \\frac{1}{n} \\sum_{i=1}^n (p_{i,\\text{online}} - p_{i,\\text{offline}})$$\n",
        "\n",
        "-   The average of the price differences is equal to the difference of the average prices $$\\frac{1}{n} \\sum_{i=1}^n (p_{i,\\text{online}} - p_{i,\\text{offline}}) = \\frac{1}{n} \\sum_{i=1}^n p_{i,\\text{online}} - \\frac{1}{n} \\sum_{i=1}^n p_{i,\\text{offline}}$$\n",
        "\n",
        "## \n",
        "\n",
        "Descriptive statistics of the difference:\n",
        "\n",
        "-   The mean difference is USD -0.05: online prices are, on average, 5 cents lower in this dataset.\n",
        "-   Spread around this average: Std: USD 10\n",
        "-   Extreme values matter: Range: -380 — USD +415.\n",
        "-   Of the 6439 products, 64% have the same online and offline price, for 87%, the difference within ±1 dollars.\n",
        "\n",
        "## Case Study - External validity:\n",
        "\n",
        "-   The products in the data may not represent all products sold at these stores.\n",
        "    -   Bias? Were the products selected randomly?\n",
        "-   Strictly: The findings refer to products sond online-offline by large retail stores. And those selected by the people collecing the data.\n",
        "-   More broadly: price differences among all products in the U.S. sold both online and offline by the same retailers.\n",
        "    -   May not be representative of smaller retailers\n",
        "\n",
        "# Testing\n",
        "\n",
        "Good old t-test\n",
        "\n",
        "## T-test\n",
        "\n",
        "-   The t-test is the testing procedure based on the t-statistic\n",
        "-   We compare the estimated value of the statistic $\\hat{s}$ to zero. ($H_0$)\n",
        "-   Evidence to reject the null is based on difference between $\\hat{s}$ and zero.\n",
        "    -   Reject the null if difference large (its un unlikely to be zero).\n",
        "    -   Not reject the null if the difference is small ( not enough evidence against it).\n",
        "\n",
        "## T-test\n",
        "\n",
        "-   The test statistic is a statistic that measures the (standardized) distance of the estimated value from what the true value would be if $H_0$ was true.\n",
        "-   Uses estimated value of $s$ ($\\hat{s}$) and the standard error of estimate (SE ($\\hat{s}$)).\n",
        "-   Consider $H_0: s_\\text{true} = 0, H_A: s_\\text{true} \\neq 0$. The t-statistic for this hypotheses is: \n",
        "$$t = \\frac{\\hat{s}}{\\text{SE}(\\hat{s})}$$\n",
        "\n",
        "\n",
        "\n",
        "## T-test\n",
        "\n",
        "When $\\hat{s}$ is the average of a variable $x$, the t-statistic is simply $$t = \\frac{\\bar{x}}{\\text{SE}(\\bar{x})}$$\n",
        "\n",
        "When $\\hat{s}$ is the average of a variable $x$ minus a number, the t-statistic is $$t = \\frac{\\bar{x} - \\text{number}}{\\text{SE}(\\bar{x})}$$\n",
        "\n",
        "When $\\hat{s}$ is the difference between two averages, say, $\\bar{x}_A$ and $\\bar{x}_B$, the t-statistic is $$t = \\frac{\\bar{x}_A - \\bar{x}_B}{\\text{SE}(\\bar{x}_A - \\bar{x}_B)}$$\n",
        "\n",
        "## T-test\n",
        "\n",
        "-   While we can use SE to calculate the t-statistic, SE may be more difficult to calculate in some situations.\n",
        "\n",
        "    -   Different samples, different SE, etc\n",
        "\n",
        "-   Some times you may want to use Bootstrap to calculate SE.\n",
        "\n",
        "-   `Stata` Corner: `ttest` command in Stata calculates the t-statistic for a difference in means.\n",
        "\n",
        "# Generalization\n",
        "\n",
        "## Making a decision\n",
        "\n",
        "-   Once you obtain your t-statistic (or other relevant statistic), you need to make a decision regarding the null hypothesis.\n",
        "-   In hypothesis testing the decision is based on a **clear** rule specified in advance. A critical value.\n",
        "    -   This makes the decision straightforward + transparent\n",
        "    -   Helps avoid personal bias:put more weight on the evidence that supports our prejudices.\n",
        "\n",
        "## Making a decision: decision rule/Critical value\n",
        "\n",
        "-   The **Critical value** is a **threshold** that determines if the test statistic is large enough to reject the null.\n",
        "    -   Recall, we start assuming the null is true.\n",
        "    -   Then we need to test if our evidence (estimates) is different enough from the null to reject it.\n",
        "    -   The critical value is what determines how different is different enough.\n",
        "-   Null rejected if the test statistic is larger than the critical value\n",
        "\n",
        "## Making a decision: Possible outcomes\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "- Some times we are right: \n",
        "  - Reject the null when it is false, \n",
        "  - or do not reject the null when it is true. \n",
        " \n",
        "- But, We can be wrong: \n",
        "  - Reject the null even though it is true\n",
        "  - or do not reject the null even though is false.\n",
        "  \n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "|                     | $H_0$ is true           | $H_0$ is false           |\n",
        "|---------------------|-------------------------|--------------------------|\n",
        "| Do not reject $H_0$ | Correct                 | False negative (Type II) |\n",
        "| Reject $H_0$        | False positive (TYPE I) | Correct                  |\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Making a decision: Error of type I and II\n",
        "\n",
        "-   Both types of errors are wrong but\n",
        "\n",
        "-   During Testing the null is protected: we only reject it if there is enough evidence against it.\n",
        "\n",
        "-   The background assumption\n",
        "\n",
        "    -   wrongly rejecting the null (a false positive) is a bigger mistake than wrongly accepting it (a false negative).\n",
        "\n",
        "-   Decision rule (critical value) is chosen in a way that makes false positives **rare**.\n",
        "\n",
        "## Making a decision: Critical values\n",
        "\n",
        "-   A commonly applied critical value for a t-statistic is ±2 (or 1.96), a 95% confidence level, or a 5% level of significance (alpha).\n",
        "\n",
        "-   Other critical values can be set: 10% (1.65), 1% (2.58), etc.\n",
        "\n",
        "-   That choice of 5% means that we tolerate a 5% chance for being wrong when rejecting the null (1/20).\n",
        "\n",
        "\n",
        "## Making a decision: In a picture\n",
        "\n",
        "![](images/paste-5.png){fig-align=\"center\"}\n",
        "\n",
        "## False negative (FN) and False positive (FP)\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "-   Fixing the chance of FP affects the chance of FN at the same time.\n",
        "-   A FN arises when the t-statistic is within the critical values and we don't reject the null even though the null is not true.\n",
        "-   This can happen if Sample is small or The difference between true value and null is small\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/paste-6.png)\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Size and power of the test\n",
        "\n",
        "Under the null: \n",
        "\n",
        "- Size of the test: the probability of committing a false positive. \n",
        "- Level of significance: The maximum probability of false positives we tolerate.\n",
        "\n",
        "Under the alternative: \n",
        "\n",
        "- Power of the test: the probability of avoiding a false negative \n",
        "- Highpower is more likely if:\n",
        "  - The sample size is large\n",
        "  - The null is far from the true value\n",
        "  - The standard error is small\n",
        "\n",
        "## Recap\n",
        "\n",
        "- In hypothesis testing we make decisions by a rule\n",
        "  -  A false positive: decision to reject the null when it is true.\n",
        "  -  A false negative: decision not to reject the nullwhen it is false.\n",
        "- The level of significance is the maximum probability of a false positive that we\n",
        "tolerate ($\\alpha$=5%).\n",
        "- The power of the test is the probability of avoiding a false negative. \n",
        "  - In statistical testing we fix the level of significance of the test to be small (5%, 1%) and hope for high power (based on design).\n",
        "- Tests with more observations have more power in general.\n",
        "\n",
        "# The p-value\n",
        "\n",
        "## The p-value\n",
        "\n",
        "-   The p-values are an alternative approach to do hypothesis testing.\n",
        "    -   Before we choose a critical value for a given \"significance level\" (5%, 1%, etc).\n",
        "    -   This approach suggests using the model significance.\n",
        "        -  The smallest significance level at which we can reject $H_0$ in the data\n",
        "        - or largest probability of a false positive that we can tolerate.\n",
        "- Calculatiion Will depend on the test statistic and sampling distribution.\n",
        "- Remember, you can never be certain! (P is never zero)\n",
        "\n",
        "## What p-value to pick?\n",
        "\n",
        "-   p-value is about a **trade-off**. Large (10-15%) or small (1%) depends on scenarios\n",
        "-   Guilty beyond reasonable doubt? (life or death scenario) \n",
        "    -   Pick a conservative value, like 1% or lower\n",
        "-   Proof of concept? (a new idea, a new product)\n",
        "    -   It's great if it works at 5%, but even 10-15% means it's much more likely to be true\n",
        "\n",
        "\n",
        "## Case Study - Comparing online and offline prices: Testing hypotheses\n",
        "\n",
        "-   Let's fix the level of significance at 5%.\n",
        "    -   The value of the statistic in the dataset is -0.054. Its standard error is 0.124.\n",
        "    -   The t-statistic is 0.44. This is well within ±2.\n",
        "-   Don't reject the null hypothesis of zero difference.\n",
        "-   The p-value of the test is 0.66.\n",
        "-   So we don't reject the null\n",
        "-   We have not \"proven\" that online and offline prices are the same, but we have not found evidence that they are different.\n",
        "\n",
        "# Multiple test\n",
        "\n",
        "## Multiple testing: motivation\n",
        "\n",
        "-   Medical dataset: data on 400 patients\n",
        "-   A particular heart disease binary variable and 100 feature of life style (sport, eating, health background, socio-economic factors)\n",
        "-   Look for a pattern – is the heart disease equally likely for poor vs rich, take vitamins vs not, etc.\n",
        "-   You test **one-by-one**\n",
        "-   You find that for half a dozen factors, there is a difference\n",
        "-   is there any problem with this procedure?\n",
        "\n",
        "## Multiple testing\n",
        "\n",
        "-   The pre-set level of significance / p-value are defined for a **single** test\n",
        "-   but, In many cases, you will consider doing many many tests.\n",
        "    -   Different measures (mean, median, range, etc)\n",
        "    -   Different products, retailers, countries\n",
        "    -   Different measures of management quality\n",
        "-   For multiple tests, you cannot use the same approach as for a single one.\n",
        "-   You need to be even **more** conservative in rejecting the null.\n",
        "   \n",
        "## Multiple testing: Example\n",
        "\n",
        "- Consider 100 tests. The Nulls are true for all tests.\n",
        "- Set $\\alpha$=5% for each test.\n",
        "- In the data, even if the null is true, you will reject 5% of the time. (false positives)\n",
        "- However, if you do \"use the evidence\" from all tests, it would seem that the null is false in 99.4% of the cases. (by chance)\n",
        "  - This is p-hacking. Choosing what works!\n",
        " \n",
        "## Multiple testing: Example\n",
        "\n",
        "-   There are various ways to deal with probabilities of false positives when testing multiple hypotheses.\n",
        "    -   Often complicated.\n",
        "-   Possible Solution: If you have a few dozens of cases, just use a strict criteria (such as 0.1-0.5% instead than 1-5%) for rejecting null hypotheses.\n",
        "-   A very strict such adjustment is the Bonferroni correction that suggests dividing the single hypothesis value by the number of hypotheses. \n",
        "-   Other methods exists, but are similar in spirit.\n",
        "-   **Risk**: by being more conservative, you are more likely to obtain false negatives.\n",
        "\n",
        "## Summary\n",
        "\n",
        "Testing in statistics means making a decision about the value of a statistic in the general pattern represented by the data. \n",
        "\n",
        "- Hypothesis starts with explicitly stating $H_0$ and $H_A$. \n",
        "- A statistical test rejects $H_0$ if there is enough evidence against it; otherwise it does not reject it. \n",
        "- Testing multiple hypotheses at the same time is a tricky business; it pays to be very conservative with rejecting the null.\n",
        "\n",
        "# {background-image=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Thats_all_folks.svg/795px-Thats_all_folks.svg.png\"}"
      ],
      "id": "a7358f2e"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "nbstata",
      "language": "stata",
      "display_name": "Stata (nbstata)",
      "path": "C:\\Users\\Fernando\\AppData\\Roaming\\jupyter\\kernels\\nbstata"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}