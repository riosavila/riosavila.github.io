{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Model Building for Prediction\"\n",
        "subtitle: \"Last's Weeks on Steroids\"\n",
        "author: \n",
        "  - name: Fernando Rios-Avila\n",
        "    affiliation: Levy Economics Institute  \n",
        "date: last-modified\n",
        "date-format: long\n",
        "format:\n",
        "  revealjs:\n",
        "    theme: [ clean2.scss]\n",
        "    slide-number: true\n",
        "    footer: \"*Rios-Avila and Cia*\"\n",
        "    width:  1280\n",
        "    height: 720\n",
        "---\n",
        "\n",
        "\n",
        "## Motivation\n",
        "\n",
        "-   You want to predict apartment rental prices using location, size, amenities, and other features. But with so many variables available,\n",
        "    -   how should you specify the candidate models?\n",
        "    -   Which variables should they include, in what functional forms, and with what interactions?\n",
        "    -   How can you make sure the candidates include truly effective predictive models?\n",
        "-   You want to predict hourly sales for a new shop, based on data from a similar existing shop.\n",
        "    -   How should you define your y variable, and how should you select predictor variables for regression models to find the best fit?\n",
        "    -   Finally, how can you evaluate the prediction in a way that informs decision-makers about the uncertainty of your prediction?\n",
        "\n",
        "## What is old? whats the new?\n",
        "\n",
        "-   We have learned about the basics of prediction.\n",
        "\n",
        "    -   We need to worry about the out-of-sample prediction error. Not the in-sample error.\n",
        "\n",
        "-   We know we can use cross-validation to select the best model.\n",
        "\n",
        "-   And we know we can construct the best model by hand, because we have domain knowledge.\n",
        "\n",
        "But what if we have a lot of variables? What if we have a lot of data?\n",
        "\n",
        "## As $K$ grows large $KxK$ grows larger\n",
        "\n",
        "-   1 variable, 1 model\n",
        "-   2 variables, 3 models\n",
        "-   3 variables, 7 models\n",
        "-   10 variables, 1023 models!\n",
        "-   20 variables, 1,048,575 models!\n",
        "\n",
        "## Prediction process: key steps\n",
        "\n",
        "-   Start with defining your \"question\" - what you want to predict.\n",
        "-   Based on the question, the target variable is defined (operationalize in the data).\n",
        "    -   Followed by defining the sample to be used (as the target)\n",
        "    -   Determine the target variable and functional form (label engineering)\n",
        "-   Define the list of predictors (feature engineering)\n",
        "    -   Build the model(s)\n",
        "    -   evaluate the model\n",
        "    -   make the prediction\n",
        "\n",
        "## Sample design\n",
        "\n",
        "-   In a prediction exercise, we are interested in predicting target for units that look those in the live data.\n",
        "    -   Select a sample that is representative of the live/target data.\n",
        "-   But, there is a trade-off:\n",
        "    -   Keep observations close to what we'll have in live data,\n",
        "    -   Or aim for a large sample size.\n",
        "-   Sample design is eventually a compromise\n",
        "\n",
        "## Sample design: filtering\n",
        "\n",
        "-   Before settling on a model, we need to design the sample.\n",
        "-   Filtering our data to match the business/ policy question.\n",
        "-   It may involve dropping observations based on key **predictor** values.\n",
        "    -   We would be looking for 3-4 star hotels, not all of them.\n",
        "\n",
        "## Sample design: Spotting errors\n",
        "\n",
        "-   For prediction exercise, we should spend more time on finding and deleting errors.\n",
        "    -   We have no chance predicting extreme values, and certainly not errors.\n",
        "    -   They provide no information, and they may distort the model.\n",
        "-   Keeping an extreme value that is likely to be an error, will have a high cost - the quadratic errors in the loss function will tilt the curve and our prediction will be off for most observations.\n",
        "    -   OLS is very sensitive to outliers.\n",
        "-   **Stronger** focus on dropping observations we think are errors.\n",
        "-   If data is missing, You may either drop the observation or try to understand why it is missing, and use that in the prediction.\n",
        "\n",
        "## Case study of used cars: Sample design\n",
        "\n",
        "-   Dropping hybrid cars, manual gear, truck\n",
        "-   Drop cars without a clean title (i.e., cars that had to be removed from registration due to a major accident)\n",
        "-   Drop when suspect cars with clearly erroneous data on miles run,\n",
        "-   Drop cars in a fair (=bad) condition, cars that are new\n",
        "-   Data cleaning resulted in 281 observations ( I kept Fair and new)\n",
        "\n",
        "## Label engineering - defining target\n",
        "\n",
        "-   We need to define what will our target variable be.\n",
        "-   In some cases, this requires no action, the business question may define it:\n",
        "    -   the price of the hotel is one such case.\n",
        "-   Often it requires thinking and decision-making about definition.\n",
        "    -   How to define **default**, **injury**, **purchase**\n",
        "        -   Binary vs continuous.\n",
        "        -   Log vs level\n",
        "\n",
        "## Label engineering - log vs level\n",
        "\n",
        "-   When price is the target variable, its relation to predictor variables is often closer to linear when expressed in log price.\n",
        "-   Log differences approximate relative, or percentage, differences, and relative price differences are often more stable.\n",
        "-   The related technical advantage is that the distribution of log prices is often close to normal, which makes linear regressions give better approximation to average differences.\n",
        "    -   Also, Log() is not the only transformation that can be used.\n",
        "-   Choosing the right functional form is important but not always easy.\n",
        "    -   Econometrics vs prediction mindset\n",
        "\n",
        "## Label engineering - log vs level\n",
        "\n",
        "-   When the target variable is expressed in log terms, we want to predict the value of the target variable ($\\hat y$) not its $log(y)$.\n",
        "-   Simply doing this $e^{\\log y}$ is not the same as obtaining $\\hat y$.\n",
        "\n",
        "Technical details:\n",
        "\n",
        "$\\hat{y}_j = e^{ \\widehat{\\log y}_j + \\hat e_j}$\n",
        "\n",
        "-   But, because $\\hat e_j$ is not observed, we need to approximate it via \"Some\" method.\n",
        "    -   If we assume that the error term is normally distributed: $$\\hat{y}_j = e^{\\widehat{\\ln y}_j} e^{\\hat{\\sigma}^2/2}$$\n",
        "\n",
        "## Used cars case study: Label engineering - log?\n",
        "\n",
        "-   Business case is about price itself, continuous\n",
        "-   But model can have level or log price as target\n",
        "-   Look at some patterns\n",
        "-   Compare model performance\n",
        "-   Log vs level model - some coefficients easier interpreted\n",
        "-   When we have two cars of same age and type; the one with 10% more miles in the odometer is predicted to be sold for 0.5% less.\n",
        "-   SE version is 1300 dollar more costly.\n",
        "\n",
        "## Used cars case study: Label engineering - log?\n",
        "\n",
        "| Model | Point prediction | 80% PI: upper bound | 80% PI: lower bound |\n",
        "|----|----|----|----|\n",
        "| in logs | 8.63 | 8.18 | 9.08 |\n",
        "| Recalculated to level | 5,932 | 3,783 | 9,301 |\n",
        "| In levels | 6,073 | 4,317 | 7,829 |\n",
        "\n",
        "Asymetric for Log-model, Symetric for Linear Model\n",
        "\n",
        "Pick what works better\n",
        "\n",
        "## Feature engineering\n",
        "\n",
        "-   Requires the most effort\n",
        "-   Feature engineering - defining the list and functional form of variables we will consider as predictor.\n",
        "-   Importantly, we use both domain knowledge - information about the actual market, product or the society - and statistics to make decisions.\n",
        "\n",
        "## Feature engineering - checklist\n",
        "\n",
        "1.  What to do with missing values\n",
        "2.  Dealing with ordered categorical values - continuous or set of binaries\n",
        "3.  How to use text to create variables (Identify key words)\n",
        "4.  Selecting functional form\n",
        "5.  Thinking interactions\n",
        "\n",
        "## What to do with missing values\n",
        "\n",
        "-   Missing at random: Observations with missing variables are not systematically different from rest, may replace with sample mean and add binary flag.\n",
        "-   Missing systematically, by nonrandom selection: Must analyze reasons, may simply mean =0, look at the source of the data / questionnaire.\n",
        "-   If very few missing and it is random, do not do anything.\n",
        "-   Few cases, you may want to impute or look for proxies.\n",
        "\n",
        "## What to do with different type of variables\n",
        "\n",
        "-   Binary (e.g, yes/no; male/female; 1/2) – create a 0/1 binary variable\n",
        "-   String / factor – check values, and create a set of binaries.\n",
        "-   Continuous – nothing to do. Make sure it is stored as number. Perhaps Winsorize.\n",
        "-   Text – Natural Language Processing. Mining the text to get useful info.\n",
        "    -   Counting words, looking for key words, etc.\n",
        "\n",
        "## Case study: Predicting Airbnb Apartment Prices\n",
        "\n",
        "-   London,UK\n",
        "-   <http://insideairbnb.com>\n",
        "-   50K observations\n",
        "-   94 variables, including many binaries for location and amenities\n",
        "-   Key variables: size, type, location, amenities\n",
        "-   Quantitative target: - price (in USD)\n",
        "-   In reality: GBP\n",
        "\n",
        "## Case study: Predicting Airbnb Apartment Prices\n",
        "\n",
        "-   Key issue is to look at variables and think functional form\n",
        "-   Guests to accommodate goes up to 16, but most apartments accommodate 1 through 7. Keep as is. Add variables for type. No need for complicated models\n",
        "-   Regarding other predictors, we have several binary variables, which we kept as they were: type of bed, type of property (apartment, house, room), cancellation policy.\n",
        "-   Look at possible need for interactions by domain knowledge / visualization\n",
        "\n",
        "## Graphical way of finding relationships\n",
        "\n",
        "![](images/paste-17.png)\n",
        "\n",
        "## Graphical way of finding interactions\n",
        "\n",
        "![](images/paste-20.png)\n",
        "\n",
        "# Trying all models?\n",
        "\n",
        "## Model building\n",
        "\n",
        "Two methods to build models:\n",
        "\n",
        "-   by hand - mix domain knowledge and statistics\n",
        "-   by smart algorithms = machine learning\n",
        "\n",
        "## Model building and selection: Build model by hand\n",
        "\n",
        "-   Use domain knowledge drives picking key variables\n",
        "-   Drop garbage - drop variables those that are useless. May be because of poor coverage or quality, or they may be irrelevant.\n",
        "-   Look at a pairwise correlations. Multi-collinearity is an issue for smaller datasets\n",
        "-   Prefer variables that are easier to update - cheaper operation of a prediction model used in production\n",
        "-   Matters when you have relatively many variables compared to size of observations\n",
        "\n",
        "## Selecting Variables in Regressions by LASSO\n",
        "\n",
        "-   Key question: which features to enter into model, how to select?\n",
        "    -   By hand – domain knowledge. Advantage: interpretation, external validity\n",
        "    -   Disadvantage: with many features it's very hard. Esp. with many possible interactions!\n",
        "-   There is room for an automatic selection process.\n",
        "-   Some are computationally very intensive (compare every option?)\n",
        "-   Advantage: no need to use outside info\n",
        "-   Disadvantage: may be sensitive to overfitting, hard to interpret\n",
        "\n",
        "## LASSO idea\n",
        "\n",
        "-   LASSO (the acronym of Least Absolute Shrinkage and Selection Operator) is a method to select variables to include in a linear regression to produce good predictions and avoid overfitting.\n",
        "-   LASSO is a shrinkage method: it shrinks coefficients towards zero to reduce variance\n",
        "    -   Cost is in bias - LASSO is not unbiased\n",
        "    -   Unlike OLS\n",
        "-   LASSO is a feature selection method as well\n",
        "\n",
        "## LASSO process\n",
        "\n",
        "-   It starts with a large set of potential predictor variables that, typically, include many interactions, polynomials for nonlinear patterns, etc.\n",
        "-   LASSO modifies the way regression coefficients are estimated by adding a penalty term for too many coefficients.\n",
        "-   The way its penalty works makes LASSO assign zero coefficients to variables whose inclusion does not improve the fit of the regression much.\n",
        "-   Assigning zero coefficients to some variables means not including them in the regression.\n",
        "\n",
        "## Side note: LASSO vs BIC\n",
        "\n",
        "-   The purpose is similar to the adjusted in-sample measures of fit, such as the BIC.\n",
        "-   To find a regression that balances fitting the data and the number of variables.\n",
        "-   But its result is different:\n",
        "    -   instead of producing a better measure of fit to help find the best one\n",
        "    -   it alters coefficients to produce a better regression directly.\n",
        "\n",
        "## LASSO\n",
        "\n",
        "Consider the linear regression with i=1...n observations and k variables, denoted 1...k:\n",
        "\n",
        "$$y^E = \\beta_0 + \\sum_{j=1}^k \\beta_jx_j$$\n",
        "\n",
        "Coefficients are estimated by OLS: which minimizes the sum of squared residuals:\n",
        "\n",
        "$$\\min_\\beta \\sum_{i=1}^N (y_i - (\\beta_0 + \\sum_{j=1}^k \\beta_jx_{ij}))^2$$\n",
        "\n",
        "LASSO modifies this minimization by a penalty term:\n",
        "\n",
        "$$\\min_\\beta \\sum_{i=1}^N (y_i - (\\beta_0 + \\sum_{j=1}^k \\beta_jx_{ij}))^2 + \\lambda \\sum_{j=1}^k |\\beta_j|$$\n",
        "\n",
        "## LASSO: how it works\n",
        "\n",
        "-   $\\lambda$ — tuning parameter.\n",
        "-   weight for penalty term vs OLS fit –\\> Strength of the variable selection\n",
        "-   Main effect of this constraint is to force many coefficients to zero.\n",
        "-   Best way to keep the sum of the absolute value of the coefficients low while maximizing fit –\\> zero coefficients on variables whose inclusion improves fit only a little.\n",
        "-   This adjustment gets rid of the weakest predictors.\n",
        "\n",
        "## LASSO: how it works\n",
        "\n",
        "-   The value of the tuning parameter $\\lambda$ drives the strength of this selection.\n",
        "-   Larger $\\lambda$ values lead to more aggressive selection and thus fewer variables left in the regression.\n",
        "-   But how can one specify a $\\lambda$ value that leads to the best prediction?\n",
        "-   We don't need, the algorithm does\n",
        "-   The LASSO algorithm can numerically solve for coefficients and the $\\lambda$ parameter at once.\n",
        "-   This makes it fast.\n",
        "-   Unlike OLS, we have no closed form solutions.\n",
        "\n",
        "## Other shrinkage methods\n",
        "\n",
        "-   So LASSO is a shrinkage method: it shrinks coefficients towards zero to reduce variance\n",
        "-   There are other ways, other functional forms\n",
        "-   Ridge regression has a quadratic penalty:\n",
        "\n",
        "$$\\min_\\beta \\sum_{i=1}^N (y_i - (\\beta_0 + \\sum_{j=1}^k \\beta_jx_{ij}))^2 + \\lambda \\sum_{j=1}^k \\beta_j^2$$\n",
        "\n",
        "-   No coefficient is shrunk to zero. But close...\n",
        "\n",
        "## Lasso and Ridge\n",
        "\n",
        "-   Lasso, Ridge regressions called regularization\n",
        "-   LASSO is \"L1\", Ridge is \"L2\"\n",
        "-   Both may help reduce overfitting\n",
        "-   LASSO also acts as feature selection model\n",
        "-   Elastic net helps find a parameter between $|\\beta_j|$ and $\\beta_j^2$ via cross validation\n",
        "\n",
        "## Airbnb Pricing Model building\n",
        "\n",
        "-   Process: build many models that differ in terms of features:\n",
        "    -   Which predictors are included\n",
        "    -   Functional form of predictors\n",
        "-   Here: specified eight linear regression models for predicting price.\n",
        "-   Data has 4393 observations. This is our original data.\n",
        "-   80% is our work set (3515 observations), the rest we will use for diagnostics.\n",
        "\n",
        "## Versions of the Airbnb apartment price prediction models\n",
        "\n",
        "| Mod | Predictor variables | N var | N coeff |\n",
        "|-----------------|---------------------|-----------------|-----------------|\n",
        "| M1 | guests accommodated, linearly | 1 | 2 |\n",
        "| M2 | = M1 + N beds, N days review, type: property, room, bed type | 6 | 8 |\n",
        "| M3 | = M2 + bathroom, cancellation, review score, N reviews (3 cat)+ F(miss) | 11 | 15 |\n",
        "| M4 | = M3 + N guest squared, square+cubic for days since 1st review | 11 | 17 |\n",
        "| M5 | = M4 + room type + N reviews interacted with property type | 11 | 22 |\n",
        "| M6 | =M5 + air conditioning, pets allowed - interacted with property type | 13 | 28 |\n",
        "| M7 | =M6 + all other amenities | 70 | 72 |\n",
        "| M8 | =M7 + all other amenities interacted with property type + bed type | 70 | 293 |\n",
        "\n",
        "## Comparing model fit measures\n",
        "\n",
        "| Model   | N predictors | R-squared | BIC       | Training RMSE | Test RMSE |\n",
        "|---------|--------------|-----------|-----------|---------------|-----------|\n",
        "| \\(1\\)   | 1            | 0.40      | 36042     | 40.48         | 40.16     |\n",
        "| \\(2\\)   | 7            | 0.48      | 35598     | 37.73         | 37.38     |\n",
        "| \\(3\\)   | 14           | 0.51      | 35478     | 36.78         | 36.51     |\n",
        "| \\(4\\)   | 16           | 0.57      | 24076     | 31.95         | 32.24     |\n",
        "| \\(5\\)   | 21           | 0.57      | 24096     | 31.82         | 32.18     |\n",
        "| \\(6\\)   | 27           | 0.58      | 24113     | 31.60         | 32.19     |\n",
        "| **(7)** | **71**       | 0.61      | **24281** | 30.42         | **31.77** |\n",
        "| \\(8\\)   | 293          | 0.66      | 25675     | 28.04         | 51.41     |\n",
        "\n",
        "## Training and test set RMSE for eight models\n",
        "\n",
        "-   Training RMSE falls with complexity\n",
        "-   Test RMSE falls then rises\n",
        "-   We pick Model M7 based on lowest CV RMSE.\n",
        "\n",
        "## The LASSO model\n",
        "\n",
        "-   Start with M8 and appr 300 candidate variables in the regression.\n",
        "-   We ran the LASSO algorithm with 5-fold cross-validation for selecting the optimal value for λ.\n",
        "-   LASSO regression just marginally better but: LASSO is automatic, a great advantage.\n",
        "-   Here: domain knowledge helped create M7. In other cases, LASSO could be great.\n",
        "\n",
        "# Post prediction analysis, diagnostics\n",
        "\n",
        "## Evaluating the Prediction Using a Holdout Set\n",
        "\n",
        "-   Model selection: selecting the best model using cross-validation\n",
        "-   Once we have picked the best model, we advised going back and using the entire original data for the final estimate and to make a prediction.\n",
        "-   What part of the data should we use to evaluate that final prediction?\n",
        "-   The solution is **a random split** before we do the analysis.\n",
        "-   Work set: We do all of the work using one part of the data: model building, selecting the best model and then making the prediction itself.\n",
        "-   Holdout set: another part of the data for evaluating the prediction itself. **Don't touch till the end**.\n",
        "\n",
        "## The holdout set\n",
        "\n",
        "-   To do diagnostics and give a good estimate of how the model may work in the live data\n",
        "    -   Additional twist to the process\n",
        "    -   The holdout set.\n",
        "-   Holdout set is set is not used in any way for modelling – taken out in the beginning\n",
        "    -   This avoids cross-contamination\n",
        "-   Used to give best guess for performance in live data\n",
        "-   Used to do diagnostics of our model\n",
        "\n",
        "## Post-prediction diagnostics\n",
        "\n",
        "-   Post-prediction diagnostics - understand better how our model works\n",
        "-   We look at prediction interval to learn about what precision we may expect to see of the estimates.\n",
        "-   We look at how the model work for different classes of observations\n",
        "-   such as young and old cars.\n",
        "\n",
        "## Cross-validation and holdout set procedure\n",
        "\n",
        "1.  Starting with the original data, split it into a larger work set and a smaller holdout set.\n",
        "2.  Further split the work set into training sets and test sets for k-fold cross-validation.\n",
        "3.  Build models and select the best model using that training-test split.\n",
        "4.  Re-estimate the best model using all observations in the work set.\n",
        "5.  Take the estimated best model and apply it to the holdout set.\n",
        "6.  Evaluate the prediction using the holdout set.\n",
        "\n",
        "## Illustration of the uses of the original data and the live data\n",
        "\n",
        "![](images/paste-21.png)\n",
        "\n",
        "## Post-prediction diagnostics\n",
        "\n",
        "-   Post-prediction diagnostics - understand better how our model works\n",
        "-   We look at prediction interval to learn about what precision we may expect to see of the estimates.\n",
        "-   We look at how the model work for different classes of observations\n",
        "-   such as young and old cars.\n",
        "\n",
        "## Data work and holdout\n",
        "\n",
        "-   Data has 4393 observations. This is our original data.\n",
        "-   random 20% holdout set with 878 observations.\n",
        "-   The remaining 80% is our work set (3515 observations).\n",
        "-   Work set will be used for cross-validation with several folds of training and test sets.\n",
        "\n",
        "## Diagnostics\n",
        "\n",
        "-   Chose the OLS estimated M7.\n",
        "-   What can we say about model performance?\n",
        "-   After estimating the model on all observations in the work sample, we calculated its RMSE in the holdout sample. The RMSE for M7 is 41\n",
        "-   Higher than CV RMSE, could be other way around.\n",
        "-   Look at diagnostics on the holdout set.\n",
        "\n",
        "## Diagnostics: prices\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/paste-22.png)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "-   y-y-hat plot\n",
        "-   higher values not really caught.\n",
        ":::\n",
        ":::::\n",
        "\n",
        "## Diagnostics: variation by size\n",
        "\n",
        "::::: columns\n",
        "::: {.column width=\"50%\"}\n",
        "![](images/paste-24.png)\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "-   The model generates a very wide 80% PI for average apartment\n",
        "-   bar plot with PI bands\n",
        "-   wide intervals\n",
        "-   linear and thus, hurts small numbers more\n",
        ":::\n",
        ":::::\n",
        "\n",
        "## Prediction with Big Data\n",
        "\n",
        "-   The principles of prediction are the same with Big Data as with moderate-sized data\n",
        "-   Big Data leads to smaller estimation error.\n",
        "-   This reduction makes the total prediction error smaller\n",
        "-   The magnitude of irreducible error, and problems with external validity, remain the same with Big Data\n",
        "\n",
        "## Prediction with Big Data\n",
        "\n",
        "-   Another upside is that large number of rows sometimes comes with large number of variables\n",
        "-   Room for more complex models\n",
        "-   Consideration: computing power (But there is AWS and Cloud Computing)\n",
        "-   When N is too large, we can take a random sample and select the best model with the help of usual cross-validation using that random sample\n",
        "\n",
        "## Summary\n",
        "\n",
        "-   Our aim was to build a prediction model for pricing apartments\n",
        "-   We built a model, M7, with domain knowledge, and a horse race between models of various complexity\n",
        "-   Picked the winner by cross-validated RMSE\n",
        "-   The model is useful for predication, but there is a great deal of uncertainty as suggested by diagnostics (on the holdout set)\n",
        "\n",
        "## Think external validity\n",
        "\n",
        "-   Future dataset will look different\n",
        "-   Think about how much\n",
        "-   Really matters in prediction\n",
        "-   If uncertain, pick simpler model\n",
        "\n",
        "## Main takeaways\n",
        "\n",
        "-   We can never evaluate all possible models to find the best one\n",
        "-   Model building is important to specify models that are likely among the best\n",
        "-   LASSO is an algorithm that can help in model building, by selecting the x variables and their functional forms\n",
        "-   Exploratory data analysis and domain knowledge remain important alongside powerful algorithms, for assessing and improving the external validity of predictions\n",
        "\n",
        "# Stata Corner\n",
        "\n",
        "## Stata: LASSO\n",
        "\n",
        "- Lasso is one of the few machine learning algorithms that is available in Stata. \n",
        "  - Stata also has a feature for elastic net and Ridge.\n",
        "  - for now just focus on LASSO. `help lasso`\n",
        "- The syntax\n",
        "\n",
        "```stata\n",
        " lasso model depvar [(alwaysvars)] othervars [if] [in] [weight] [, options]\n",
        "```\n",
        "- It has various selection options (`selection()`) but we can use the default.\n",
        "\n",
        "## Stata: LASSO {.scrollable}\n"
      ],
      "id": "653978f7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "codefold": false
      },
      "source": [
        "*| echo: true\n",
        "webuse cattaneo2, clear\n",
        "lasso linear bweight c.mage##c.mage c.fage##c.fage c.mage#c.fage c.fedu##c.medu ///\n",
        "    i.(mmarried mhisp fhisp foreign alcohol msmoke fbaby prenatal1), nolog\n",
        "ereturn display"
      ],
      "id": "888aebd2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stata: LASSO {.scrollable}\n"
      ],
      "id": "960483df"
    },
    {
      "cell_type": "code",
      "metadata": {
        "codefold": false
      },
      "source": [
        "*| echo: true\n",
        " \n",
        "qui: ssc install elasticregress\n",
        "webuse cattaneo2, clear\n",
        "lassoregress bweight c.mage##c.mage c.fage##c.fage c.mage#c.fage c.fedu##c.medu ///\n",
        "    i.(mmarried mhisp fhisp foreign alcohol msmoke fbaby prenatal1), \n",
        "ereturn display"
      ],
      "id": "df820a19",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "nbstata",
      "language": "stata",
      "display_name": "Stata (nbstata)",
      "path": "C:\\Users\\Fernando\\AppData\\Roaming\\jupyter\\kernels\\nbstata"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}