{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Modeling Probabilities\"\n",
        "subtitle: \"to be or not to be\"\n",
        "author: \n",
        "  - name: Fernando Rios-Avila\n",
        "    affiliation: Levy Economics Institute  \n",
        "date: last-modified\n",
        "date-format: long\n",
        "format:\n",
        "  revealjs:\n",
        "    theme: [ clean2.scss]\n",
        "    slide-number: true\n",
        "    footer: \"*Rios-Avila and Cia*\"\n",
        "    width:  1300\n",
        "    height: 675\n",
        "jupyter: nbstata    \n",
        "execute: \n",
        "  cache: true\n",
        "  freeze: auto\n",
        "---  \n",
        "\n",
        "\n",
        "## What we will see today\n",
        "\n",
        "- Linear Probability Model - LPM\n",
        "- Logit & probit\n",
        "- Goodness of fit\n",
        "- Diagnostics\n",
        "- Summary\n",
        "\n",
        "## Motivation\n",
        "\n",
        "- What are the health benefits of not smoking? Considering the 50+ population, we can investigate if differences in smoking habits are correlated with differences in health status.\n",
        "  - good health vs bad health\n",
        "  \n",
        "## Binary events\n",
        "\n",
        "- Some outcomes are things that either happen or don't happen, which can be captured by binary variables\n",
        "  - e.g. a person is healthy or not, a person is employed or not, a person is a smoker or not. We dont see a person that is half healthy, half employed, or half a smoker.\n",
        "- How can we model these events?\n",
        "  - We have seen this before. Instead of modeling the value itself, we model the probability of the event happening. \n",
        "\n",
        "$$E[y] = P[y = 1]$$\n",
        "\n",
        "- In fact, the average of a 0–1 event is the probability of that event happening. Which can also be estimated as conditional probabilities:\n",
        "\n",
        "$$E[y|x_1, x_2, ...] = P[y = 1|x_1, x_2, ...]$$\n",
        "\n",
        "- Good news, we can use the same tools we have been using to model these probabilities.\n",
        "  \n",
        "# Modelling events: LPM\n",
        "\n",
        "## LPM: Linear probability model\n",
        "\n",
        "- Linear Probability Model (LPM) is a **linear regression** with a binary dependent variable\n",
        "  - It has the goal of modeling the probability of an event happening\n",
        "- A linear regressions with binary dependent variables shows:\n",
        "  - differences in expected $y$ by $x$, represent diferences in probability of $y = 1$ by $x$.\n",
        "- Introduce notation for probability:\n",
        "  $y^P = P[y = 1|x_1, x_2, . . .]$\n",
        "- Linear probability model (LPM) regression is\n",
        "  $y^P = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$\n",
        "\n",
        "## LPM: Interpretation\n",
        "\n",
        "$$y^P = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$$\n",
        "\n",
        "- So far nothing changes in terms of modelling or estimation. However, the interpretation of the coefficients changes.\n",
        "  - $y^P$ denotes the probability that the dependent variable is one, conditional on the right-hand-side variables of the model.\n",
        "  - $\\beta_0$ shows the *predicted* probability of $y$ if all $x$ are zero.\n",
        "  - $\\beta_1$ shows the difference in the probability that $y = 1$ for observations that are different in $x_1$ but are the same in terms of $x_2$. (ceteris paribus)\n",
        "\n",
        "## LPM: Modelling\n",
        "\n",
        "- Linear probability model (**LPM**) can be estimated using OLS. (just like linear regression)\n",
        "- We can use all transformations in $x$, that we used before:\n",
        "  - Log, Polinomials, Splines, dummies, interactions, etc. They all work.\n",
        "- All formulae and interpretations for standard errors, confidence intervals, hypotheses and p-values of tests are the same.\n",
        "- [**IMPORTANT**]{.red} Heteroskedasticity robust error are essential in this case!\n",
        "  - By construction LPMs are heteroskedastic!\n",
        "  - And ignoring this fact will lead to biased standard errors and confidence intervals.\n",
        "  \n",
        "## LPM: Prediction\n",
        "\n",
        "- Predicted values - $\\hat{y}_P$ - may be problematic. Although they are calculated the same way, they need to be interpreted as **probabilities**.\n",
        "\n",
        "$$\\hat{y}^P = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2$$\n",
        "\n",
        "- Predicted values **need to be** between 0 and 1 because they are probabilities\n",
        "- But in LPM, predictions may be below 0 and above 1. No formal bounds in the model.\n",
        "  - more likely than certain, less likely than impossible\n",
        "  \n",
        "## LPM: Prediction\n",
        "\n",
        "- When to get worried?:\n",
        "  - With continuous variables that can take any value (GDP, Population, sales, etc), this could be a serious issue (extrapolation)\n",
        "    - We need to check if predictions are within the 0-1 range at least \"in-sample\". But, this is not a guarantee that it will be the case \"out-of-sample\".\n",
        "  - With binary variables, no problem ('saturated models') (interpolation)\n",
        "    - Not problem because \"simple\" means will always be between 0 and 1.\n",
        "- So, a problem if goal is prediction!\n",
        "- Not a big issue for inference → uncover patterns of association.\n",
        "  - But it may give biased estimates...(in **theory**)\n",
        "\n",
        "## CS: Does smoking pose a health risk?\n",
        "\n",
        "> This is on of the few datasets from the book that is not directly available from their website. \n",
        "> If interested, you need to go over the repository, and follow the instructions to access the data.\n",
        "\n",
        "Thus, we will use a different dataset to illustrate the concepts.\n",
        "\n",
        "## CS: Does smoking during pregnancy affect birth weight?\n",
        "\n",
        "- The question is whether, and by how much, smoking during pregnancy affects the likelihood that a baby is born with low birth weight.\n",
        "- We will use \"lbw\" dataset from Stata's example datasets.\n",
        "- The dataset contains information on 189 observations of mothers and their newborns.\n",
        "  -  `low` is a binary variable indicating whether the baby was born with low birth weight (<2500gr <5.5lbs).\n",
        "  -  `smoke` is a binary variable indicating whether the mother smoked during pregnancy.\n",
        "     \n",
        "## Data\n",
        "\n",
        "- $low = 1$ if baby was born with low birth weight\n",
        "- $low = 0$ if baby was born with normal weight\n",
        "  -Some demographic information on all individual\n",
        "- We exclude women <15 years old and >40 years old  \n",
        "- Also exclude women with Weight > 200lbs (before pregnancy)\n",
        "  \n",
        "## LPM: in Stata\n",
        "\n",
        "- Start with a simple univariate model: $P[low|smoke] = \\alpha + \\beta[smoke]$"
      ],
      "id": "9d010a92"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| code-fold: false\n",
        "webuse lbw, clear\n",
        "drop if age < 15 | age > 40 | lwt > 200\n",
        "reg low smoke, robust"
      ],
      "id": "bade17f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LPM: Interpretation\n",
        "\n",
        "Interpretation: \n",
        "\n",
        "- The coefficient on `smoker` shows the difference in the probability of a baby.\n",
        "- Babies are **15.7** percentage points more likely to be born with low birth weight if the mother smoked during pregnancy.\n",
        "  - Are you comparing Apples to apples?\n",
        "  - Lets add additional controls to capture other factors\n",
        "\n",
        "## LPM: with many regressors I\n",
        "\n",
        "- Multiple regression – closer to causality\n",
        "- Compare women who are very similar in many respects but are different in smoking habits\n",
        "- Smokers / non-smokers – different in many other behaviors and conditions:\n",
        "  - personal traits (age, race)\n",
        "  - behavior pre-pregnancy (Pre-pregnancy weight)\n",
        "  - Medical history (History of Hypertension)\n",
        "  - background for pregnancy (Number of prenatal visits, Previous premature labor)\n",
        "\n",
        "## LPM with many regressors II\n",
        "\n",
        "- May also consider functional form selection or interactions\n",
        "- Trial and error, or theory-based\n",
        "- Useful to check bivariate relationships (scatter plots, Lpoly, correlations)\n",
        "  - For now, assume linear relationships\n",
        "   \n",
        "## LPM with many regressors III"
      ],
      "id": "cbcc9b14"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| code-fold: true\n",
        "*| output: asis\n",
        "qui {\n",
        "gen any_premature = ptl >0\n",
        "ren ftv no_of_visits_1tr\n",
        "ren ht hist_hyper\n",
        "ren lwt wgt_bef_preg\n",
        "reg low smoke age i.race any_premature hist_hyper  no_of_visits_1tr wgt_bef_preg, robust nohead\n",
        "}\n",
        "est store lpm_results\n",
        "esttab lpm_results,   se  wide nonumber ///\n",
        "collabel(b se) md drop(1.race) nomtitle b(3) nonotes"
      ],
      "id": "ca31a92c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Robust standard errors in parentheses]{.table-note} \n",
        "[<sup>\\*</sup> *p* < 0.05, <sup>\\*\\*</sup> *p* < 0.01, <sup>\\*\\*\\*</sup> *p* < 0.001]{.table-note }\n",
        "\n",
        "## Detour: Regression Tables\n",
        "\n",
        "- If need to show many explanatory variables\n",
        "- Do not show table 12*2 rows, people will not see it.\n",
        "  - Avoid copy pasting from your document! Those tables are unwieldy.\n",
        "- Either only show selected variables (smoke + 2-3 others)\n",
        "- Or may need to create two columns. (a bit more work)\n",
        "  - In my case, Wide format did the trick.\n",
        "- Make site you have title, N of observations, footnote on SE, stars.\n",
        "- SE, stars: many different notations. Check carefully.\n",
        "  - `esttab` default is $p^{***}= p<0.001$, $0.01$ and $0.05$ \n",
        "  - In papers there is $p^{***}=p<0.01$, $0.05$ and $0.1$.\n",
        "\n",
        "## Does smoking pose a health risk for the baby?\n",
        "\n",
        "- Coefficient on smoking during pregnancy is **-.151**.\n",
        "  - Women who smoked during pregnancy are **15.1** percentage points more likely to have a baby with low birth weight.\n",
        "- The 95% confidence interval is relatively wide $[0.002, 0.300]$, but it does not contain zero\n",
        "- Age, Race?, Nr of Visits and Pre-pregnancy weight do not seem to be factors\n",
        "-  Hypertension and previous premature labor are significant factors, increasing the probability of low birth weight by 40pp and 27.5pp, respectively.\n",
        "\n",
        "## LPM's predicted probabilities\n"
      ],
      "id": "a76c4f1a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| code-fold: true\n",
        "*| fig-align: center\n",
        "qui: reg low smoke age i.race any_premature hist_hyper  no_of_visits_1tr wgt_bef_preg, robust nohead\n",
        "qui: predict low_hat\n",
        "qui:histogram low_hat\n",
        "sum low_hat"
      ],
      "id": "b85bfd94",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of LPM's predicted probabilities\n",
        "\n",
        ":::{.panel-tabset}\n",
        "\n",
        "## What to do\n",
        "\n",
        "- Drill down in distribution:\n",
        "  - Looking at the composition of people: top vs bottom part of probability distribution\n",
        "  - Look at average values of covariates for top and bottom X% of predicted probabilities!\n",
        "\n",
        "## What we find"
      ],
      "id": "0f97d7d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| code-fold: true\n",
        "sort low_hat\n",
        "set linesize 255\n",
        " \n",
        "qui: gen flag = 1 if _n<=5\n",
        "qui: replace  flag = 2 if _n>=_N-4\n",
        "list low_hat smoke age race any_premature hist_hyper  no_of_visits_1tr wgt_bef_preg   if flag==1\n",
        "list low_hat smoke age race any_premature hist_hyper  no_of_visits_1tr wgt_bef_preg   if flag==2"
      ],
      "id": "bb6bff28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "# Modelling events: <br> [log/prob]it \n",
        "\n",
        "## Probability models: logit and probit\n",
        "\n",
        "- **Prediction:** predicted probability need to be between 0 and 1\n",
        "  - Thus, for prediction, we **must** use non-linear models\n",
        "  - Actually, its a quasi-linear model. \n",
        "- The model, itself, is linear in the parameters\n",
        "- but need to relate this to the probability of the $y = 1$ event, using a nonlinear function that maps the linear index into a 0-1 range: **'Link function'**\n",
        "\n",
        "$$\\begin{aligned}\n",
        "XB &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... \\\\\n",
        "y^P &= F(XB) \\rightarrow y^P \\in (0,1)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "- Two options: Logit and probit – different link function\n",
        "\n",
        "\n",
        "## Link functions I.\n",
        "\n",
        "Call $XB = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...$\n",
        "\n",
        "- The Logit:  \n",
        "  $y^P = \\Lambda(XB) = \\frac{\\exp(XB)}{1 + \\exp(XB)}$\n",
        "\n",
        "- The probit:  \n",
        "  $y^P = \\Phi(XB) \\rightarrow \\Phi(z) = \\int_{-\\infty}^z \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz$\n",
        "\n",
        "where $\\Lambda()$ is called logistic function, and $\\Phi()$ is the cumulative distribution function (CDF) of the standard normal distribution.\n",
        "\n",
        "\n",
        "## Link functions II.\n",
        "\n",
        "::: {.columns}\n",
        "\n",
        ":::{.column width=40%}\n",
        "- Both link functions are S-shaped curves bounded between 0 and 1.\n",
        "- There is but a small difference between the two.\n",
        "- but estimated coefficients will be different.\n",
        ":::\n",
        "\n",
        ":::{.column width=60%}\n"
      ],
      "id": "ec158ddb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| fig-align: center\n",
        "*| echo: true\n",
        "*| code-fold: true\n",
        "qui {\n",
        "clear\n",
        "range p 0 1 202\n",
        "drop if p==0 | p==1 \n",
        "gen x = invnormal(p)\n",
        "gen y = (x+rnormal())>0\n",
        "reg  y x\n",
        "predict y_1\n",
        "logit y x\n",
        "predict y_2\n",
        "probit y x\n",
        "predict y_3\n",
        "drop if abs(x)>2\n",
        "two (line y_1 x ) (line y_2 x) (line y_3 x), ///\n",
        "legend(order(1 \"LPM\" 2 \"Logit\" 3 \"Probit\") pos(3) ring(0) col(1)) \n",
        "}"
      ],
      "id": "f54614e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        ":::\n",
        "\n",
        " \n",
        "\n",
        "## Logit and probit interpretation\n",
        "\n",
        "- Both the probit and the logit transform the $\\beta_0 + \\beta_1 x_1 + ...$ linear combination using a link function that shows an S-shaped curve.\n",
        "- The slope of this curve keeps changing as we change whatever is inside, but it's steepest when $y^P = 0.5$ (inflection point)\n",
        "- The difference in $y^P$ corresponds to changes in probabilities, between any two values of $x$.\n",
        "- To find how much is related to a particular $x$, You need to take the partial derivatives. \n",
        "- **Important consequence**: no direct interpretation of the raw coefficient values!\n",
        "  - Thus, always know if you are interpreting the raw coefficients or the marginal differences.\n",
        "\n",
        "## Marginal differences (marginal effects)\n",
        "\n",
        "> **NOTE** As before, the word \"effect\" should be used with caution. In the book, they use \"marginal differences\" instead. as a more neutral term.\n",
        " \n",
        "- Link functions makes associates $\\Delta x$ into $\\Delta y_P$. we do not interpret raw coefficients! (except for direction)\n",
        "- Instead, transform them into 'marginal differences' for interpretation purposes\n",
        "\n",
        "- They are also called 'marginal effects' or 'average marginal effects (AME)' or 'average partial effects'.\n",
        " \n",
        "- Average marginal difference has the same interpretation as the coefficient of linear probability models, but with caveats.\n",
        " \n",
        "## Marginal differences: Discrete $x$\n",
        "\n",
        "- if $x$ is a categorical (0-1), the marginal difference is the difference in the predicted probability of $y = 1$, that corresponds to a change from $x = 0$ to $x = 1$.\n",
        "\n",
        "$$\\Delta y^P = y^P(x = 1) - y^P(x = 0)$$\n",
        "\n",
        "- Then we simply \"average\" this difference across all observations.\n",
        "\n",
        "## Marginal differences: continous $x$\n",
        "\n",
        "- If $x$ is continuous, the marginal difference is calculated as the derivative (for a small change in $x$).\n",
        "\n",
        "$$\\frac{\\partial y^P}{\\partial x_1} = \\beta_1 \\cdot f(XB)$$\n",
        "\n",
        "- Which is then averaged across all observations to report a single number.\n",
        "\n",
        "- In practice, we interpret this as the change in the probability of $y = 1$ for a one-unit change in $x_1$.\n",
        "\n",
        "## How to estimate this models?\n",
        "### Maximum likelihood estimation!\n",
        "\n",
        "- When estimating a logit or probit model, we use 'maximum likelihood' estimation.\n",
        "  - See 11.U2 for details.\n",
        "- Idea for maximum likelihood is another way to get coefficient estimates. \n",
        "  - **1st** You specify a (conditional) distribution, that you will use during the estimation. \n",
        "    - This is logistic for logit and normal for probit model.\n",
        "  - **2nd** You maximize this function w.r.t. your $\\beta$ parameters → gives the maximum likelihood for this model.\n",
        "- Different from OLS: No closed form solution → need to use search algorithms.\n",
        "  - Thus... more computationally intensive.\n",
        "\n",
        "## Predictions for LMP, Logit and Probit I.\n"
      ],
      "id": "ba688d46"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| code-fold: true\n",
        "*| fig-align: center\n",
        " \n",
        "qui {\n",
        "  webuse lbw, clear\n",
        "  drop if age < 15 | age > 40 | lwt > 200\n",
        "  gen any_premature = ptl >0\n",
        "  ren ftv no_of_visits_1tr\n",
        "  ren ht hist_hyper\n",
        "  ren lwt wgt_bef_preg\n",
        "  gen black = 2.race\n",
        "  gen other = 3.race\n",
        "  reg low smoke age i.black i.other i.any_premature hist_hyper , robust nohead\n",
        "  predict low_hat_ols\n",
        "  est sto lpm_results\n",
        "  logit low smoke age black other any_premature hist_hyper , robust nohead\n",
        "  predict low_hat_logit\n",
        "   probit low smoke age black other any_premature hist_hyper , robust nohead\n",
        "  predict low_hat_probit\n",
        "   two (scatter  low_hat_logit low_hat_probit low_hat_ols) ///\n",
        "      (line low_hat_ols low_hat_ols, sort), ///\n",
        "      legend(order(1 \"Logit\" 2 \"Probit\" 3 \"LPM\") pos(3) ring(0) col(1))\n",
        "}"
      ],
      "id": "41c17a83",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Coefficient results for logit and probit {.smaller}\n"
      ],
      "id": "98a89071"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| code-fold: true\n",
        "*| output: asis\n",
        " \n",
        "qui {\n",
        "  logit low smoke age i.black i.other i.any_premature hist_hyper , robust nohead\n",
        "  est sto logit_results\n",
        "  margins, dydx(*) post\n",
        "  est sto logit_mfx\n",
        "  probit low smoke age i.black i.other i.any_premature hist_hyper , robust nohead  \n",
        "  est sto probit_results\n",
        "  margins, dydx(*) post\n",
        "  est sto probit_mfx\n",
        "}\n",
        "esttab lpm_results logit_results  logit_mfx probit_results probit_mfx, se  nonumber drop(0.*) ///\n",
        "mtitle(LPM Logit Logit_MFX Probit Probit_MFX) collabel(none) md ///\n",
        "star(* 0.10 ** 0.05 *** 0.01) nonotes "
      ],
      "id": "a77a0df2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Standard errors in parentheses]{.table-note} \n",
        "[<sup>\\*</sup> *p* < 0.1, <sup>\\*\\*</sup> *p* < 0.04, <sup>\\*\\*\\*</sup> *p* < 0.01]{.table-note }\n",
        "\n",
        "## Does smoking pose a health risk?– logit and probit\n",
        "\n",
        "- LPM – interpret the coefficients as usual.\n",
        "- Logit, probit - Interpret the marginal differences. Basically the same.\n",
        "  - Marginal differences are essentially the same across the logit and the probit.\n",
        "  - **Essentially** the same as the corresponding LPM coefficients.\n",
        "- Happens often:\n",
        "  - Often LPM is good enough for interpretation.\n",
        "  - Check if logit/probit very different.\n",
        "    - if so, Investigate functional forms if yes.\n",
        "\n",
        "# Goodness of fit measures\n",
        "\n",
        "## Goodness of fit\n",
        "\n",
        "- There is no generally accepted goodness of fit measure\n",
        "  - This is because we do not observe probabilities only 1 and 0, so we cannot FIT those probabilities.\n",
        "- There are, however, other options to evaluate the quality of the model.\n",
        "  - R-squared\n",
        "  - Brier score\n",
        "  - Pseudo R-squared\n",
        "  - log-loss\n",
        "\n",
        "## Goodness of fit: R-squared\n",
        "\n",
        "$$R^2 = 1 - \\frac{\\sum_{i=1}^n (\\hat{y}_P^i - y_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$$\n",
        "\n",
        "- R-squared is not useful for binary outcomes\n",
        "  - It can be calculated, but it lacks the interpretation we had for linear models, because we are fitting the probabilities, not the outcomes.\n",
        "  \n",
        "## Brier score\n",
        "$$\\text{Brier} = \\frac{1}{n}\\sum_{i=1}^n (\\hat{y}_P^i - y_i)^2$$\n",
        "\n",
        "- The Brier score is the average distance (mean squared difference) between predicted probabilities and the actual value of $y$.\n",
        "- Smaller the Brier score, the better.\n",
        "- When comparing two predictions, the one with the smaller Brier score is the better prediction because it produces less (squared) error on average.\n",
        "- Related to a main concept in prediction: mean squared error (MSE)\n",
        "\n",
        "## Pseudo R2\n",
        "\n",
        "$$pR^2 = 1 - \\frac{\\text{Log-likelihood}_{\\text{model}}}{\\text{Log-likelihood}_{\\text{intercept only}}}$$\n",
        "\n",
        "- It is similar to the R-squared, as it measures the goodness of fit, tailored to nonlinear models and binary outcomes.\n",
        "  - Most widely used: McFadden's R-squared (`Stata` uses this)\n",
        "- Computes the ratio of log-likelihood of the model vs **intercept only**.\n",
        "\n",
        "- Can be computed for the logit and the probit but not for the linear probability model. (unless you re-define the Log-likelihood)\n",
        "\n",
        "## Log-loss\n",
        "\n",
        "$$\\text{Log-loss} = \\frac{1}{n}\\sum_{i=1}^n \\left[y_i \\log(\\hat{y}_P^i) + (1-y_i) \\log(1-\\hat{y}_P^i) \\right]$$\n",
        "\n",
        "- The log-loss is a measured derived from the log-likelihood function. It measures how much observed data dissagrees with the predicted probabilities.\n",
        "\n",
        "- The smaller (close to zero) the log-loss, the better the model.\n",
        "  \n",
        "## Practical use\n",
        "\n",
        "- There are several measured of model fit, but they often give the same ranking of models.\n",
        "- Do not use R-squared. Even for LPM, it has no interpretation.\n",
        "- If using probit vs logit: pseudo R-squared may be used to rank logit and probit models.\n",
        "- Use, especially for prediction: Brier score is a metric that can be computed for all models and is used in prediction.\n",
        "\n",
        "## Bias of the predictions\n",
        "\n",
        "- Post-prediction: we may be interested to study some features of our model\n",
        "- One specific goal: evaluating the bias of the prediction.\n",
        "  - Probability predictions are unbiased if they are right on average = the average of predicted probabilities is equal to the actual probability of the outcome.\n",
        "  - If the prediction is unbiased, the bias is zero.\n",
        "- Unless the model is really bad, unconditional bias is not a big issue.\n",
        "  - Only Probit will be biased.\n",
        "\n",
        "## Calibration\n",
        "\n",
        "- Unbiasedness refers to the whole distribution of probability predictions. \n",
        "- A finer and stricter concept is calibration\n",
        "- A prediction is well calibrated if the actual probability of the outcome is equal to the predicted probability for each and every value of the predicted probability.\n",
        "  $$E(y|y^P) = y^P$$\n",
        "    \n",
        "- 'Calibration curve' is used to show this.\n",
        "- A model may be unbiased (right on average) but not well calibrated\n",
        "  - underestimate high probability events and overestimate low probability ones\n",
        "\n",
        "## Calibration curve\n",
        "\n",
        "- Horizontal axis shows the values of all predicted probabilities ($\\hat{y}_P$).\n",
        "- Vertical axis shows the fraction of $y = 1$ observations for all observations with the corresponding predicted probability.\n",
        "- The closer the curve is to the 45-degree line, the better the calibration.\n",
        "  \n",
        "In practice:\n",
        "\n",
        "- Create bins for predicted probabilities and make comparisons of the actual event's probability.\n",
        "- Or use non-parametric methods to estimate the calibration curve.\n",
        "\n",
        "## Calibration curve\n"
      ],
      "id": "58f9e11e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| echo: true\n",
        "*| code-fold: true\n",
        "*| fig-align: center\n",
        "\n",
        "two (lpoly low low_hat_ols) (lpoly low low_hat_logit) ///\n",
        "(lpoly low low_hat_probit) (function y = x, range(0 1) lcolor(black%50)), ///\n",
        "legend(order(1 \"LPM\" 2 \"Logit\" 3 \"Probit\") ) scale(1.5) ///\n",
        "xtitle(\"Predicted probability\") ytitle(\"Avg outcome\")"
      ],
      "id": "6ab409b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Probability models summary\n",
        "\n",
        "- Find patterns when $y$ is binary can be done using model probability with regressions\n",
        "- Linear probability model is mostly good enough, easy inference.\n",
        "  - to-go for data exploration, quick analysis, and diagnostics\n",
        "  - but, predicted values could be below 0, above 1\n",
        "- Logit (and probit) - better when aim is prediction, predicted values strictly between 0-1\n",
        "- Most often, LPM, logit, probit - similar inference\n",
        "- Use marginal (average) differences (with logit/probit)\n",
        "- No trivial goodness of fit. Brier score or pseudo-R-Squared.\n",
        "- Calibration is important for prediction."
      ],
      "id": "1af68b3f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "nbstata",
      "language": "stata",
      "display_name": "Stata (nbstata)",
      "path": "C:\\Users\\Fernando\\AppData\\Roaming\\jupyter\\kernels\\nbstata"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}