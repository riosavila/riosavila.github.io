[
  {
    "objectID": "Stata_Basics.html",
    "href": "Stata_Basics.html",
    "title": "Stata-Basics",
    "section": "",
    "text": "You can use command window to type and excute commands directly into Stata.\nGreat for interactive exploration and analysis…\nBut highly recommended that Final analysis is “always” done in a “do-file”\n\n\n\n\n. display \"Hola\"\nHola\n\n. display \"2+2=\" 2+2\n2+2=4\n\n. display \"The probability that z &gt;1.95 is \" %5.3f 1-normal(1.95)\nThe probability that z &gt;1.95 is 0.026\n\n. \n\n\n\n\n\n\n99% of Stata commands come with Extensive help.\nIf you do not know how to use a command, or about a test just “ask for help”\n\nhelp help\n[R] help -- Display help in Stata\n-------------------------------------------------------------------------------\nStata's help system\n    There are several kinds of help available to the Stata user. For more\n    information, see Advice on getting help.  The information below is\n    technical details about Stata's help command.\n-------------------------------------------------------------------------------\nSyntax\n        help [command_or_topic_name] [, nonew name(viewername)\n              marker(markername)]\nMenu\n    Help &gt; Stata command...\nDescription\n    The help command displays help information about the specified command or\n    topic.  help launches a new Viewer to display help for the specified\n    command or topic or displays help on the console in Stata for\n    Unix(console).  If help is not followed by a command or a topic name,\n    Stata displays advice for using the help system and documentation.\nFor estimation commands, and specialized tests, help even provides links to the manuals.\nThe manuals have extensive detailed information on methods, formulas, references, and examples.\n\n\nOf course there is 1% that is “documented/undocumented” or truly undocumented.\nMost Community-contributed commands also have helpfiles, but are not always fully documented.\nYou could also ask for helps on “topics”: help sample selection\n\n\n\n\n\nStata, for all practical purposes, is self-contained.\n\nYou do not need outside sources to analyze your data, estimate models, create tables, etc.\n\nHowever, many users provide add-ons that may help to make your work “easier”\n\nMain Stata repository : Boston College Statistical Software Components (SSC) archive\n\n\n** For using Wooldridge Book Datasets\nssc install frause, replace \n** For Easiy tables\nnet install estout, replace from(https://raw.githubusercontent.com/benjann/estout/master/)\n** My own installer for extra utilities\nnet install fra, replace from(https://friosavila.github.io/stpackages) \nfra install fra_tools, replace\n\nIf at any point there is code that produces an error, and there is no help, let me know.\n\n\n\n\n\nStata Files have format dta.\nLoading Stata-data into Stata is very easy.\n\nDouble-click (opens a new Stata)\nDrag and Drop into your Stata instance\nLoad it from menu File&gt;open\nor using a do-file or command window\n\nOther Formats required extra work.\n\nUse other software to “translate” it into Stata\nMenu: File&gt;import&gt; many choices\n\n\n\n\n\n\n\n. ** Most Stata example files\n. ** Syntax:   sysuse  [filename], [clear]\n. sysuse dir\n  abortion.dta    citytemp.dta    nlsw88.dta      titanic.dta\n  auto.dta        citytemp4.dta   nlswide1.dta    tsline1.dta\n  auto16.dta      educ99gdp.dta   oaxaca.dta      tsline2.dta\n  auto2.dta       gapminder.dta   pop2000.dta     uslifeexp.dta\n  autornd.dta     gnp96.dta       ri.dta          uslifeexp2.dta\n  avocado.dta     lifeexp.dta     sandstone.dta   voter.dta\n  bplong.dta      mortgages.dta   scorecard.dta   xtline1.dta\n  bpwide.dta      mroz.dta        snow.dta        yule.dta\n  cancer.dta      network1.dta    sp500.dta\n  castle.dta      network1a.dta   surface.dta\n  census.dta      nhefs.dta       texas.dta\n\n. \n\n\n\n\n\n** Web data from Stata\nwebuse \"data-file-address\", clear\n** From other sites\nwebuse set [webaddress]\nwebuse data-file-address, clear \nwebuse set \n** from frause and Wooldrige\nfrause , dir\nfrause wage1, clear\n** from anyadress\nuse \"filename-adress\", clear\nuse \"https://friosavila.github.io/playingwithstata/data2/wage1.dta\", clear\n\n\n\n\n\nsysusewebusefrause\n\n\n\n\n\n. sysuse auto, clear\n(1978 automobile data)\n\n. des\n\nContains data from C:\\Program Files\\Stata17/ado\\base/a/auto.dta\n Observations:            74                  1978 automobile data\n    Variables:            12                  13 Apr 2020 17:45\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nmake            str18   %-18s                 Make and model\nprice           int     %8.0gc                Price\nmpg             int     %8.0g                 Mileage (mpg)\nrep78           int     %8.0g                 Repair record 1978\nheadroom        float   %6.1f                 Headroom (in.)\ntrunk           int     %8.0g                 Trunk space (cu. ft.)\nweight          int     %8.0gc                Weight (lbs.)\nlength          int     %8.0g                 Length (in.)\nturn            int     %8.0g                 Turn circle (ft.)\ndisplacement    int     %8.0g                 Displacement (cu. in.)\ngear_ratio      float   %6.2f                 Gear ratio\nforeign         byte    %8.0g      origin     Car origin\n-------------------------------------------------------------------------------\nSorted by: foreign\n\n. list in 1/3\n\n     +------------------------------------------------------------------------+\n  1. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Concord | 4,099 |  22 |     3 |      2.5 |    11 |  2,930 |    186 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       40     |          121     |         3.58     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n     +------------------------------------------------------------------------+\n  2. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Pacer   | 4,749 |  17 |     3 |      3.0 |    11 |  3,350 |    173 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       40     |          258     |         2.53     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n     +------------------------------------------------------------------------+\n  3. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Spirit  | 3,799 |  22 |     . |      3.0 |    12 |  2,640 |    168 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       35     |          121     |         3.08     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n. \n\n\n\n\n\n\n\n. webuse smoking, clear\n(Smoking and mortality data)\n\n. des\n\nContains data from https://www.stata-press.com/data/r17/smoking.dta\n Observations:        17,260                  Smoking and mortality data\n    Variables:            16                  27 Dec 2020 15:21\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nserno           int     %10.0g                Serial number\ncigs            byte    %10.0g                Daily cigarette consumption\nsysbp           int     %10.0g                Systolic blood pressure (mm Hg)\ndiasbp          float   %10.0g                Diastolic blood pressure (mm Hg)\nmap             float   %9.0g                 Mean arterial pressure (mm Hg)\nage             byte    %10.0g                Age (years)\nht              double  %10.0g                Height (cm)\nwt              double  %10.0g                Weight (kg)\nchol            double  %10.0g                Cholesterol (mmol/l)\ngradd1          byte    %8.0g                 Job grade 1\ngradd2          byte    %8.0g                 Job grade 2\ngradd3          byte    %8.0g                 Job grade 3\nall10           byte    %8.0g                 Ten year mortality\npyar            double  %10.0g                Years of follow-up\nchd             byte    %10.0g                Censoring (0 = censored, 1 = died\n                                                of CHD)\njobgrade        byte    %10.0g                Job grade\n-------------------------------------------------------------------------------\nSorted by: \n\n. list cigs map age ht gradd1 in 1/3\n\n     +-----------------------------------------+\n     | cigs        map   age       ht   gradd1 |\n     |-----------------------------------------|\n  1. |    0         97    46   154.94        0 |\n  2. |    0   97.66666    55   179.07        1 |\n  3. |    0         82    43   173.99        1 |\n     +-----------------------------------------+\n\n. \n\n\n\n\n\n\n\n. frause smoking, clear\n(A.C.Cameron & P.K.Trivedi (2022): Microeconometrics Using Stata, 2e)\n\n. des\n\nContains data from https://friosavila.github.io/playingwithstata/data2/smoking.\n&gt; dta\n Observations:         1,209                  A.C.Cameron & P.K.Trivedi\n                                                (2022): Microeconometrics Using\n                                                Stata, 2e\n    Variables:             7                  5 Oct 2022 13:08\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nstate           byte    %14.0g     state      State no\nyear            int     %9.0g                 Year\ncigsale         float   %9.0g                 Cigarette sale per capita (in\n                                                packs)\nlnincome        float   %9.0g                 Log state per capita GDP\nbeer            float   %9.0g                 Beer consumption per capita\nage15to24       float   %9.0g                 Percent of state population aged\n                                                15–24 years\nretprice        float   %9.0g                 Retail price of cigarettes\n-------------------------------------------------------------------------------\nSorted by: year\n\n. list in 1/3\n\n     +-----------------------------------------------------------------------+\n     |        state   year   cigsale   lnincome   beer   age15~24   retprice |\n     |-----------------------------------------------------------------------|\n  1. | Rhode Island   1970     123.9          .      .   .1831579       39.3 |\n  2. |    Tennessee   1970      99.8          .      .   .1780438       39.9 |\n  3. |      Indiana   1970     134.6          .      .   .1765159       30.6 |\n     +-----------------------------------------------------------------------+\n\n. \n\n\n\n\n\n\n\n\n\nSummary Statistics are essential before starting basic analysis. Stata gives you many options. Although not all of them are easy to export.\n\n\n\n\n. frause oaxaca, clear\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n. *summarize [varlist] [if] [in] [weight] [, options]\n. summarize if female==1, sep(0)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      lnwage |        683    3.266761    .5700236    .507681   5.259097\n        educ |        888    11.06025     2.26024          5       17.5\n       exper |        683    12.13769     8.32663          0   41.91667\n      tenure |        683    6.605051    6.727475          0   40.08333\n        isco |        683    3.920937    1.762983          1          9\n      female |        888           1           0          1          1\n         lfp |        888    .7691441    .4216179          0          1\n         age |        888    39.88401    10.72665         18         62\n       agesq |        888    1705.666    879.4667        324       3844\n      single |        888    .3018018    .4592984          0          1\n     married |        888    .5236486    .4997219          0          1\n    divorced |        888    .1745495    .3797953          0          1\n       kids6 |        888     .286036    .6726639          0          4\n     kids714 |        888        .375    .7538888          0          4\n          wt |        888    .9883364    .4030975   .5302977   3.181786\n\n. \n\n\n\n\n\n. *   tabstat varlist [if] [in] [weight] [, options]\n. tabstat educ exper tenure age married, by(female)\n\nSummary statistics: Mean\nGroup variable: female (sex of respondent (1=female))\n\n  female |      educ     exper    tenure       age   married\n---------+--------------------------------------------------\n       0 |   11.8004  14.07684  9.003107  38.51647  .5230567\n       1 |  11.06025  12.13769  6.605051  39.88401  .5236486\n---------+--------------------------------------------------\n   Total |  11.40134  13.15324  7.860937  39.25379  .5233758\n------------------------------------------------------------\n\n. tabstat educ exper tenure , by(female) stats(p10 p50 p90)\n\nSummary statistics: p10, p50, p90\nGroup variable: female (sex of respondent (1=female))\n\n  female |      educ     exper    tenure\n---------+------------------------------\n       0 |      10.5  1.416667  .4166667\n         |      10.5  11.08333      6.25\n         |      17.5  31.33333     23.25\n---------+------------------------------\n       1 |         9      2.25  .3333333\n         |      10.5     10.75      4.25\n         |      12.5    23.375     15.75\n---------+------------------------------\n   Total |         9  1.833333  .4166667\n         |      10.5  10.91667  5.291667\n         |        15  28.08333  19.41667\n----------------------------------------\n\n. \n\n\n\n\n\n. ssc install table1\nchecking table1 consistency and verifying not already installed...\nall files already exist and are up to date.\n\n. * see help table1\n. table1, by(female) vars(lnwage contn %3.2f \\ age contn %2.1f \\ married bin)\n  +--------------------------------------------------------------------+\n  | Factor                         female = 0    female = 1    p-value |\n  |--------------------------------------------------------------------|\n  | N                              759           888                   |\n  |--------------------------------------------------------------------|\n  | log hourly wages, mean (SD)    3.44 (0.48)   3.27 (0.57)    &lt;0.001 |\n  |--------------------------------------------------------------------|\n  | age of respondent, mean (SD)   38.5 (11.3)   39.9 (10.7)     0.012 |\n  |--------------------------------------------------------------------|\n  | married                        397 (52.3%)   465 (52.4%)      0.98 |\n  +--------------------------------------------------------------------+\n\n. qui:table1, by(female) vars(lnwage contn %3.2f \\ age contn %2.1f \\ married bi\n&gt; n) saving(m1.xls)\nfile m1.xls already exists\nr(602);\n\n\nYou can see the file here\n\nThere are other options from Stata as well. see help dtable and help table.\nOr you could construct some yourself with the help of estout and esttab.\nSee here for a quick guide on tables.\n\n\n\n\n\nTwo main commands:\n\ngenerate (or gen for short): Creates new variables as a function of others in the data. One can apply system functions. Example:\n\ngen var1 = 1\ngen var2 = _n\ngen wage = exp(lnwage)\ngen age_educ = age * educ\n\nreplace: replaces values in an already existing variable.\n\nreplace wage = 0 if wage==.\nreplace age_educ = . if female==1\n\negen: Advanced variable generating function. It applies a single function to a variable or list of variables to create a third one.\n\negen wage_mean=mean(exp(lnwage)), by(female)\negen wage_p10=pctile(lnwage), by(female) p(10)\n\n\nTo delete a variable, you can use drop varname/varlist or drop2 varname/varlist\n\ndrop is the official. Stops if the variable does not exist.\ndrop2 an addon. Will still work even if a variable name does not exist.\nRequires using full variable name.\n\n\n\n\n\n. gen var1 = exp(lnwage)\n(213 missing values generated)\n\n. gen xar2 = exp(lnwage)+married\n(213 missing values generated)\n\n. drop x\n\n. des var1 xar2\nvariable xar2 not found\nr(111);\n\n\n\n\n\n. gen xar2 = exp(lnwage)+married\n(213 missing values generated)\n\n. drop2 x\nvariable x not found\n\n. \n\n\n\n\n\n\nStata is case sensitive.\n\nYou can create variables with names one, One, OnE, ONE, etc.\n“file addresses” and commands are also case sensitive\n\nIn Stata, variable names cannot can only start with a letter or “_“. Otherwise, it will give you an error.\nOnce variables are created, you could “label” them\nlabel var variable_name \"Description\"\nYou can name other components of a dataset as well. See help label\n\n\n\n\n\nStata can create figures and plots for data exploration\n\n\n\n\n\n\n. webuse dui, clear\n(Fictional data on monthly drunk driving citations)\n\n. scatter citations fines\n\n. \n\n\n\n\n\n\n\n\n\n\n\n\n. two (scatter citations fines if csize==1)  ///\n&gt;     (scatter citations fines if csize==2), ///\n&gt;     legend(order(1 \"Small\" 2 \"Medium\"))\n\n. \n\n\n\n\n\n\n\n\n\n\nThe limitation. User written plotting commands do not interact well with Official plotting commands.\n\n\n\n\n\n\nCommand Window is effective to provide interactive analysis\nAt the end of your session, you can recover everything you did, clicking on the History Section, and save everythig, or just specific commands.\nThe best approach, however, is to ALWAYS use a do-file.\nFirst of all: Create a working directory. A folder in your computer that will hold your project, work, paper, homework, etc. (highly recommended)\nCreate a dofile: For simple projects a single file will suffice, but multiple may be needed for larger ones.\nTo start a dofile, simply type doedit \"filename\" in your command window.\n\nIf file exists in your “working directory” (type cd to see where you “are”), it will open it.\nOtherwise, a new file will be created\n\ndo-files are the best approach to save your work, and keep track of your analysis.\nGeneral Suggestion: Allways add comments to it, to know what you are doing\n\n\n*  You can always Start a command like this\n// Or like this\n/*\nBut you can always add a large comment using \"/*\" to start\nand \"*/\" to end it\n*/\n\n/* You could also add comments at the end of a command */\nsysuse auto, clear  // Loading Auto Dataset, after \"clearing\" the one currently in memory\n\n// Or as I did before, break a long command in various lines using \"///\"\n// Comments after \"///\" are possible\nregress price     /// Dep variable\n        mpg       /// indep variable \n        i.foreign, /// Foreign Dummy\n        robust     // Request Robust Standard errors.\n        \n** Last one has only two \"/\", because line ends there\n\n(1978 automobile data)\n\nLinear regression                               Number of obs     =         74\n                                                F(2, 71)          =      12.72\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.2838\n                                                Root MSE          =     2530.9\n\n------------------------------------------------------------------------------\n             |               Robust\n       price | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -294.1955   60.33645    -4.88   0.000     -414.503   -173.8881\n             |\n     foreign |\n    Foreign  |   1767.292   607.7385     2.91   0.005     555.4961    2979.088\n       _cons |   11905.42   1362.547     8.74   0.000     9188.573    14622.26\n------------------------------------------------------------------------------\n\n\n\n\n\n\nMost commands in Stata have the following syntax:\n\n[by varlist:/prefix] command [varlist]  [if exp] [in range] [weight] [using filename] [,options]\n\neverything in [] are optional.\n[by varlist:/prefix]: by is used to execute the command by groups.\nprefix to request additional manipulation (advanced use)\ncommand: The command itself that will process the data\nvarlist: For Estimation commands include the dependent (first) and independent variables (everything else)\n[if exp] [in range]: To restrict samples\n[weight]: Request the use of weights ie: [fw = wgt_var] or [pw = wgt_var]\n[using filename]: Some commands allow you to use this to work with not-yet loaded datasets or files.\n[options]: Options requesting specific behaivior, statistics, etc\n\n\n\n\n\nUse github, as an additional data-repository\nCombine Stata, python and nbstat to create Jupyter notebooks.\nYou can also use Quarto to create full dynamic reports."
  },
  {
    "objectID": "Stata_Basics.html#command-window",
    "href": "Stata_Basics.html#command-window",
    "title": "Stata-Basics",
    "section": "",
    "text": "You can use command window to type and excute commands directly into Stata.\nGreat for interactive exploration and analysis…\nBut highly recommended that Final analysis is “always” done in a “do-file”\n\n\n\n\n. display \"Hola\"\nHola\n\n. display \"2+2=\" 2+2\n2+2=4\n\n. display \"The probability that z &gt;1.95 is \" %5.3f 1-normal(1.95)\nThe probability that z &gt;1.95 is 0.026\n\n."
  },
  {
    "objectID": "Stata_Basics.html#help",
    "href": "Stata_Basics.html#help",
    "title": "Stata-Basics",
    "section": "",
    "text": "99% of Stata commands come with Extensive help.\nIf you do not know how to use a command, or about a test just “ask for help”\n\nhelp help\n[R] help -- Display help in Stata\n-------------------------------------------------------------------------------\nStata's help system\n    There are several kinds of help available to the Stata user. For more\n    information, see Advice on getting help.  The information below is\n    technical details about Stata's help command.\n-------------------------------------------------------------------------------\nSyntax\n        help [command_or_topic_name] [, nonew name(viewername)\n              marker(markername)]\nMenu\n    Help &gt; Stata command...\nDescription\n    The help command displays help information about the specified command or\n    topic.  help launches a new Viewer to display help for the specified\n    command or topic or displays help on the console in Stata for\n    Unix(console).  If help is not followed by a command or a topic name,\n    Stata displays advice for using the help system and documentation.\nFor estimation commands, and specialized tests, help even provides links to the manuals.\nThe manuals have extensive detailed information on methods, formulas, references, and examples.\n\n\nOf course there is 1% that is “documented/undocumented” or truly undocumented.\nMost Community-contributed commands also have helpfiles, but are not always fully documented.\nYou could also ask for helps on “topics”: help sample selection"
  },
  {
    "objectID": "Stata_Basics.html#installing-programs",
    "href": "Stata_Basics.html#installing-programs",
    "title": "Stata-Basics",
    "section": "",
    "text": "Stata, for all practical purposes, is self-contained.\n\nYou do not need outside sources to analyze your data, estimate models, create tables, etc.\n\nHowever, many users provide add-ons that may help to make your work “easier”\n\nMain Stata repository : Boston College Statistical Software Components (SSC) archive\n\n\n** For using Wooldridge Book Datasets\nssc install frause, replace \n** For Easiy tables\nnet install estout, replace from(https://raw.githubusercontent.com/benjann/estout/master/)\n** My own installer for extra utilities\nnet install fra, replace from(https://friosavila.github.io/stpackages) \nfra install fra_tools, replace\n\nIf at any point there is code that produces an error, and there is no help, let me know."
  },
  {
    "objectID": "Stata_Basics.html#loading-data",
    "href": "Stata_Basics.html#loading-data",
    "title": "Stata-Basics",
    "section": "",
    "text": "Stata Files have format dta.\nLoading Stata-data into Stata is very easy.\n\nDouble-click (opens a new Stata)\nDrag and Drop into your Stata instance\nLoad it from menu File&gt;open\nor using a do-file or command window\n\nOther Formats required extra work.\n\nUse other software to “translate” it into Stata\nMenu: File&gt;import&gt; many choices\n\n\n\n\n\n\n\n. ** Most Stata example files\n. ** Syntax:   sysuse  [filename], [clear]\n. sysuse dir\n  abortion.dta    citytemp.dta    nlsw88.dta      titanic.dta\n  auto.dta        citytemp4.dta   nlswide1.dta    tsline1.dta\n  auto16.dta      educ99gdp.dta   oaxaca.dta      tsline2.dta\n  auto2.dta       gapminder.dta   pop2000.dta     uslifeexp.dta\n  autornd.dta     gnp96.dta       ri.dta          uslifeexp2.dta\n  avocado.dta     lifeexp.dta     sandstone.dta   voter.dta\n  bplong.dta      mortgages.dta   scorecard.dta   xtline1.dta\n  bpwide.dta      mroz.dta        snow.dta        yule.dta\n  cancer.dta      network1.dta    sp500.dta\n  castle.dta      network1a.dta   surface.dta\n  census.dta      nhefs.dta       texas.dta\n\n. \n\n\n\n\n\n** Web data from Stata\nwebuse \"data-file-address\", clear\n** From other sites\nwebuse set [webaddress]\nwebuse data-file-address, clear \nwebuse set \n** from frause and Wooldrige\nfrause , dir\nfrause wage1, clear\n** from anyadress\nuse \"filename-adress\", clear\nuse \"https://friosavila.github.io/playingwithstata/data2/wage1.dta\", clear"
  },
  {
    "objectID": "Stata_Basics.html#basic-data-description",
    "href": "Stata_Basics.html#basic-data-description",
    "title": "Stata-Basics",
    "section": "",
    "text": "sysusewebusefrause\n\n\n\n\n\n. sysuse auto, clear\n(1978 automobile data)\n\n. des\n\nContains data from C:\\Program Files\\Stata17/ado\\base/a/auto.dta\n Observations:            74                  1978 automobile data\n    Variables:            12                  13 Apr 2020 17:45\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nmake            str18   %-18s                 Make and model\nprice           int     %8.0gc                Price\nmpg             int     %8.0g                 Mileage (mpg)\nrep78           int     %8.0g                 Repair record 1978\nheadroom        float   %6.1f                 Headroom (in.)\ntrunk           int     %8.0g                 Trunk space (cu. ft.)\nweight          int     %8.0gc                Weight (lbs.)\nlength          int     %8.0g                 Length (in.)\nturn            int     %8.0g                 Turn circle (ft.)\ndisplacement    int     %8.0g                 Displacement (cu. in.)\ngear_ratio      float   %6.2f                 Gear ratio\nforeign         byte    %8.0g      origin     Car origin\n-------------------------------------------------------------------------------\nSorted by: foreign\n\n. list in 1/3\n\n     +------------------------------------------------------------------------+\n  1. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Concord | 4,099 |  22 |     3 |      2.5 |    11 |  2,930 |    186 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       40     |          121     |         3.58     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n     +------------------------------------------------------------------------+\n  2. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Pacer   | 4,749 |  17 |     3 |      3.0 |    11 |  3,350 |    173 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       40     |          258     |         2.53     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n     +------------------------------------------------------------------------+\n  3. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Spirit  | 3,799 |  22 |     . |      3.0 |    12 |  2,640 |    168 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       35     |          121     |         3.08     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n. \n\n\n\n\n\n\n\n. webuse smoking, clear\n(Smoking and mortality data)\n\n. des\n\nContains data from https://www.stata-press.com/data/r17/smoking.dta\n Observations:        17,260                  Smoking and mortality data\n    Variables:            16                  27 Dec 2020 15:21\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nserno           int     %10.0g                Serial number\ncigs            byte    %10.0g                Daily cigarette consumption\nsysbp           int     %10.0g                Systolic blood pressure (mm Hg)\ndiasbp          float   %10.0g                Diastolic blood pressure (mm Hg)\nmap             float   %9.0g                 Mean arterial pressure (mm Hg)\nage             byte    %10.0g                Age (years)\nht              double  %10.0g                Height (cm)\nwt              double  %10.0g                Weight (kg)\nchol            double  %10.0g                Cholesterol (mmol/l)\ngradd1          byte    %8.0g                 Job grade 1\ngradd2          byte    %8.0g                 Job grade 2\ngradd3          byte    %8.0g                 Job grade 3\nall10           byte    %8.0g                 Ten year mortality\npyar            double  %10.0g                Years of follow-up\nchd             byte    %10.0g                Censoring (0 = censored, 1 = died\n                                                of CHD)\njobgrade        byte    %10.0g                Job grade\n-------------------------------------------------------------------------------\nSorted by: \n\n. list cigs map age ht gradd1 in 1/3\n\n     +-----------------------------------------+\n     | cigs        map   age       ht   gradd1 |\n     |-----------------------------------------|\n  1. |    0         97    46   154.94        0 |\n  2. |    0   97.66666    55   179.07        1 |\n  3. |    0         82    43   173.99        1 |\n     +-----------------------------------------+\n\n. \n\n\n\n\n\n\n\n. frause smoking, clear\n(A.C.Cameron & P.K.Trivedi (2022): Microeconometrics Using Stata, 2e)\n\n. des\n\nContains data from https://friosavila.github.io/playingwithstata/data2/smoking.\n&gt; dta\n Observations:         1,209                  A.C.Cameron & P.K.Trivedi\n                                                (2022): Microeconometrics Using\n                                                Stata, 2e\n    Variables:             7                  5 Oct 2022 13:08\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nstate           byte    %14.0g     state      State no\nyear            int     %9.0g                 Year\ncigsale         float   %9.0g                 Cigarette sale per capita (in\n                                                packs)\nlnincome        float   %9.0g                 Log state per capita GDP\nbeer            float   %9.0g                 Beer consumption per capita\nage15to24       float   %9.0g                 Percent of state population aged\n                                                15–24 years\nretprice        float   %9.0g                 Retail price of cigarettes\n-------------------------------------------------------------------------------\nSorted by: year\n\n. list in 1/3\n\n     +-----------------------------------------------------------------------+\n     |        state   year   cigsale   lnincome   beer   age15~24   retprice |\n     |-----------------------------------------------------------------------|\n  1. | Rhode Island   1970     123.9          .      .   .1831579       39.3 |\n  2. |    Tennessee   1970      99.8          .      .   .1780438       39.9 |\n  3. |      Indiana   1970     134.6          .      .   .1765159       30.6 |\n     +-----------------------------------------------------------------------+\n\n."
  },
  {
    "objectID": "Stata_Basics.html#summary-statistics",
    "href": "Stata_Basics.html#summary-statistics",
    "title": "Stata-Basics",
    "section": "",
    "text": "Summary Statistics are essential before starting basic analysis. Stata gives you many options. Although not all of them are easy to export.\n\n\n\n\n. frause oaxaca, clear\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n. *summarize [varlist] [if] [in] [weight] [, options]\n. summarize if female==1, sep(0)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      lnwage |        683    3.266761    .5700236    .507681   5.259097\n        educ |        888    11.06025     2.26024          5       17.5\n       exper |        683    12.13769     8.32663          0   41.91667\n      tenure |        683    6.605051    6.727475          0   40.08333\n        isco |        683    3.920937    1.762983          1          9\n      female |        888           1           0          1          1\n         lfp |        888    .7691441    .4216179          0          1\n         age |        888    39.88401    10.72665         18         62\n       agesq |        888    1705.666    879.4667        324       3844\n      single |        888    .3018018    .4592984          0          1\n     married |        888    .5236486    .4997219          0          1\n    divorced |        888    .1745495    .3797953          0          1\n       kids6 |        888     .286036    .6726639          0          4\n     kids714 |        888        .375    .7538888          0          4\n          wt |        888    .9883364    .4030975   .5302977   3.181786\n\n. \n\n\n\n\n\n. *   tabstat varlist [if] [in] [weight] [, options]\n. tabstat educ exper tenure age married, by(female)\n\nSummary statistics: Mean\nGroup variable: female (sex of respondent (1=female))\n\n  female |      educ     exper    tenure       age   married\n---------+--------------------------------------------------\n       0 |   11.8004  14.07684  9.003107  38.51647  .5230567\n       1 |  11.06025  12.13769  6.605051  39.88401  .5236486\n---------+--------------------------------------------------\n   Total |  11.40134  13.15324  7.860937  39.25379  .5233758\n------------------------------------------------------------\n\n. tabstat educ exper tenure , by(female) stats(p10 p50 p90)\n\nSummary statistics: p10, p50, p90\nGroup variable: female (sex of respondent (1=female))\n\n  female |      educ     exper    tenure\n---------+------------------------------\n       0 |      10.5  1.416667  .4166667\n         |      10.5  11.08333      6.25\n         |      17.5  31.33333     23.25\n---------+------------------------------\n       1 |         9      2.25  .3333333\n         |      10.5     10.75      4.25\n         |      12.5    23.375     15.75\n---------+------------------------------\n   Total |         9  1.833333  .4166667\n         |      10.5  10.91667  5.291667\n         |        15  28.08333  19.41667\n----------------------------------------\n\n. \n\n\n\n\n\n. ssc install table1\nchecking table1 consistency and verifying not already installed...\nall files already exist and are up to date.\n\n. * see help table1\n. table1, by(female) vars(lnwage contn %3.2f \\ age contn %2.1f \\ married bin)\n  +--------------------------------------------------------------------+\n  | Factor                         female = 0    female = 1    p-value |\n  |--------------------------------------------------------------------|\n  | N                              759           888                   |\n  |--------------------------------------------------------------------|\n  | log hourly wages, mean (SD)    3.44 (0.48)   3.27 (0.57)    &lt;0.001 |\n  |--------------------------------------------------------------------|\n  | age of respondent, mean (SD)   38.5 (11.3)   39.9 (10.7)     0.012 |\n  |--------------------------------------------------------------------|\n  | married                        397 (52.3%)   465 (52.4%)      0.98 |\n  +--------------------------------------------------------------------+\n\n. qui:table1, by(female) vars(lnwage contn %3.2f \\ age contn %2.1f \\ married bi\n&gt; n) saving(m1.xls)\nfile m1.xls already exists\nr(602);\n\n\nYou can see the file here\n\nThere are other options from Stata as well. see help dtable and help table.\nOr you could construct some yourself with the help of estout and esttab.\nSee here for a quick guide on tables."
  },
  {
    "objectID": "Stata_Basics.html#creating-variables",
    "href": "Stata_Basics.html#creating-variables",
    "title": "Stata-Basics",
    "section": "",
    "text": "Two main commands:\n\ngenerate (or gen for short): Creates new variables as a function of others in the data. One can apply system functions. Example:\n\ngen var1 = 1\ngen var2 = _n\ngen wage = exp(lnwage)\ngen age_educ = age * educ\n\nreplace: replaces values in an already existing variable.\n\nreplace wage = 0 if wage==.\nreplace age_educ = . if female==1\n\negen: Advanced variable generating function. It applies a single function to a variable or list of variables to create a third one.\n\negen wage_mean=mean(exp(lnwage)), by(female)\negen wage_p10=pctile(lnwage), by(female) p(10)\n\n\nTo delete a variable, you can use drop varname/varlist or drop2 varname/varlist\n\ndrop is the official. Stops if the variable does not exist.\ndrop2 an addon. Will still work even if a variable name does not exist.\nRequires using full variable name.\n\n\n\n\n\n. gen var1 = exp(lnwage)\n(213 missing values generated)\n\n. gen xar2 = exp(lnwage)+married\n(213 missing values generated)\n\n. drop x\n\n. des var1 xar2\nvariable xar2 not found\nr(111);\n\n\n\n\n\n. gen xar2 = exp(lnwage)+married\n(213 missing values generated)\n\n. drop2 x\nvariable x not found\n\n."
  },
  {
    "objectID": "Stata_Basics.html#variables-management",
    "href": "Stata_Basics.html#variables-management",
    "title": "Stata-Basics",
    "section": "",
    "text": "Stata is case sensitive.\n\nYou can create variables with names one, One, OnE, ONE, etc.\n“file addresses” and commands are also case sensitive\n\nIn Stata, variable names cannot can only start with a letter or “_“. Otherwise, it will give you an error.\nOnce variables are created, you could “label” them\nlabel var variable_name \"Description\"\nYou can name other components of a dataset as well. See help label"
  },
  {
    "objectID": "Stata_Basics.html#plots-in-stata",
    "href": "Stata_Basics.html#plots-in-stata",
    "title": "Stata-Basics",
    "section": "",
    "text": "Stata can create figures and plots for data exploration\n\n\n\n\n\n\n. webuse dui, clear\n(Fictional data on monthly drunk driving citations)\n\n. scatter citations fines\n\n. \n\n\n\n\n\n\n\n\n\n\n\n\n. two (scatter citations fines if csize==1)  ///\n&gt;     (scatter citations fines if csize==2), ///\n&gt;     legend(order(1 \"Small\" 2 \"Medium\"))\n\n. \n\n\n\n\n\n\n\n\n\n\nThe limitation. User written plotting commands do not interact well with Official plotting commands."
  },
  {
    "objectID": "Stata_Basics.html#saving-your-work",
    "href": "Stata_Basics.html#saving-your-work",
    "title": "Stata-Basics",
    "section": "",
    "text": "Command Window is effective to provide interactive analysis\nAt the end of your session, you can recover everything you did, clicking on the History Section, and save everythig, or just specific commands.\nThe best approach, however, is to ALWAYS use a do-file.\nFirst of all: Create a working directory. A folder in your computer that will hold your project, work, paper, homework, etc. (highly recommended)\nCreate a dofile: For simple projects a single file will suffice, but multiple may be needed for larger ones.\nTo start a dofile, simply type doedit \"filename\" in your command window.\n\nIf file exists in your “working directory” (type cd to see where you “are”), it will open it.\nOtherwise, a new file will be created\n\ndo-files are the best approach to save your work, and keep track of your analysis.\nGeneral Suggestion: Allways add comments to it, to know what you are doing\n\n\n*  You can always Start a command like this\n// Or like this\n/*\nBut you can always add a large comment using \"/*\" to start\nand \"*/\" to end it\n*/\n\n/* You could also add comments at the end of a command */\nsysuse auto, clear  // Loading Auto Dataset, after \"clearing\" the one currently in memory\n\n// Or as I did before, break a long command in various lines using \"///\"\n// Comments after \"///\" are possible\nregress price     /// Dep variable\n        mpg       /// indep variable \n        i.foreign, /// Foreign Dummy\n        robust     // Request Robust Standard errors.\n        \n** Last one has only two \"/\", because line ends there\n\n(1978 automobile data)\n\nLinear regression                               Number of obs     =         74\n                                                F(2, 71)          =      12.72\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.2838\n                                                Root MSE          =     2530.9\n\n------------------------------------------------------------------------------\n             |               Robust\n       price | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -294.1955   60.33645    -4.88   0.000     -414.503   -173.8881\n             |\n     foreign |\n    Foreign  |   1767.292   607.7385     2.91   0.005     555.4961    2979.088\n       _cons |   11905.42   1362.547     8.74   0.000     9188.573    14622.26\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "Stata_Basics.html#estimation-commands",
    "href": "Stata_Basics.html#estimation-commands",
    "title": "Stata-Basics",
    "section": "",
    "text": "Most commands in Stata have the following syntax:\n\n[by varlist:/prefix] command [varlist]  [if exp] [in range] [weight] [using filename] [,options]\n\neverything in [] are optional.\n[by varlist:/prefix]: by is used to execute the command by groups.\nprefix to request additional manipulation (advanced use)\ncommand: The command itself that will process the data\nvarlist: For Estimation commands include the dependent (first) and independent variables (everything else)\n[if exp] [in range]: To restrict samples\n[weight]: Request the use of weights ie: [fw = wgt_var] or [pw = wgt_var]\n[using filename]: Some commands allow you to use this to work with not-yet loaded datasets or files.\n[options]: Options requesting specific behaivior, statistics, etc"
  },
  {
    "objectID": "Stata_Basics.html#adv-options-for-saving-work.",
    "href": "Stata_Basics.html#adv-options-for-saving-work.",
    "title": "Stata-Basics",
    "section": "",
    "text": "Use github, as an additional data-repository\nCombine Stata, python and nbstat to create Jupyter notebooks.\nYou can also use Quarto to create full dynamic reports."
  },
  {
    "objectID": "rmethods2/session_7.html#what-is-monte-carlo-simulation",
    "href": "rmethods2/session_7.html#what-is-monte-carlo-simulation",
    "title": "Research Methods II",
    "section": "What is Monte Carlo Simulation?",
    "text": "What is Monte Carlo Simulation?\n\nMonte Carlo simulation are a generic name given to methods that use random numbers to simulate a process.\nIn econometrics, Monte Carlo methods are used to study the properties of estimators, and to evaluate the performance of statistical tests.\nThis can be a useful tool to understand some of the properties of estimators, or even problems related to violations of assumptions.\nIt can also be used to evaluate the performance of estimators in finite samples, and to compare different estimators."
  },
  {
    "objectID": "rmethods2/session_7.html#example-mean-vs-median",
    "href": "rmethods2/session_7.html#example-mean-vs-median",
    "title": "Research Methods II",
    "section": "Example: Mean vs Median",
    "text": "Example: Mean vs Median\n\nWhich of this estimators is more robust and efficient, when samples are small ?\nLets setup a program that would simulate this:\n\n\n// define a program\ncapture program drop mean_vs_median\nprogram define mean_vs_median, eclass\n  syntax, [nobs(int 100)]\n  clear\n  ** Set  # of obs\n  set obs `nobs'\n  ** Generate a random variable\n  gen x = rnormal(0,1)\n  ** Calculate mean and median\n  qui:sum x,d\n  ** Store results\n  matrix b = r(mean), r(p50)\n  ** post results\n  matrix colname b = \"mean\" \"median\"\n  ereturn post b\nend\nmean_vs_median\nereturn display\n\n\n\n\n\nNumber of observations (_N) was 0, now 100.\n------------------------------------------------------------------------------\n             | Coefficient\n-------------+----------------------------------------------------------------\n        mean |  -.1445693\n      median |  -.1970271\n------------------------------------------------------------------------------\n\n\nNow that the program is SET, lets run it 1000 times:\n\nset seed 101\nsimulate, reps(1000): mean_vs_median, nobs(500)\nsum\n\n\n      Command: mean_vs_median, nobs(500)\n\nSimulations (1,000): .........10.........20.........30.........40.........50...\n&gt; ......60.........70.........80.........90.........100.........110.........120\n&gt; .........130.........140.........150.........160.........170.........180.....\n&gt; ....190.........200.........210.........220.........230.........240.........2\n&gt; 50.........260.........270.........280.........290.........300.........310...\n&gt; ......320.........330.........340.........350.........360.........370........\n&gt; .380.........390.........400.........410.........420.........430.........440.\n&gt; ........450.........460.........470.........480.........490.........500......\n&gt; ...510.........520.........530.........540.........550.........560.........57\n&gt; 0.........580.........590.........600.........610.........620.........630....\n&gt; .....640.........650.........660.........670.........680.........690.........\n&gt; 700.........710.........720.........730.........740.........750.........760..\n&gt; .......770.........780.........790.........800.........810.........820.......\n&gt; ..830.........840.........850.........860.........870.........880.........890\n&gt; .........900.........910.........920.........930.........940.........950.....\n&gt; ....960.........970.........980.........990.........1,000 done\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     _b_mean |      1,000     .001419     .043875  -.1660358   .1525889\n   _b_median |      1,000    .0000899    .0544004  -.1851489   .1853068\n\n\nConclusion, when N=100, and the distribution is normal, the mean is more efficient than the median.\nChange assumptions, from N to t-distribution\n\n// define a program\ncapture program drop mean_vs_median\nprogram define mean_vs_median, eclass\n  syntax, [nobs(int 100) rt(int 5)]\n  clear\n  set obs `nobs'\n  gen x = rt(`rt')\n  qui:sum x,d\n  matrix b = r(mean), r(p50)\n  matrix colname b = \"mean\" \"median\"\n  ereturn post b\nend\n set seed 101\nsimulate, reps(1000) nodots: mean_vs_median, nobs(500) rt(2)\nsum\nsimulate, reps(1000) nodots: mean_vs_median, nobs(500) rt(4)\nsum\nsimulate, reps(1000) nodots: mean_vs_median, nobs(500) rt(6)\nsum\n\n\n\n      Command: mean_vs_median, nobs(500) rt(2)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     _b_mean |      1,000   -.0199475    .2203307  -4.369244   .6070946\n   _b_median |      1,000    .0002406    .0640181  -.1970648   .1889922\n\n      Command: mean_vs_median, nobs(500) rt(4)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     _b_mean |      1,000    .0007863    .0647392  -.2136446   .2056455\n   _b_median |      1,000    .0024002    .0602134  -.1902502   .2014696\n\n      Command: mean_vs_median, nobs(500) rt(6)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     _b_mean |      1,000    .0007195    .0531958  -.1984012   .2054568\n   _b_median |      1,000    .0008841    .0580835  -.1863388   .2035508"
  },
  {
    "objectID": "rmethods2/session_7.html#properties-of-estimators",
    "href": "rmethods2/session_7.html#properties-of-estimators",
    "title": "Research Methods II",
    "section": "Properties of estimators",
    "text": "Properties of estimators\n\nMonte Carlo methods can also be used to study the properties of estimators.\nConsider the following example:\n\nWe want to study the properties of the OLS estimator when the error term is heteroskedastic.\n\n\n\\[y_i = \\beta_0 + \\beta_1 x_i + u_i*exp(\\gamma x_i)\\]\n\nWhat are the consequences of heteroskedasticity in the OLS estimator?\nlets set up a simulation to study this.\n\n\n// define a program\ncapture program drop ols_hetero\nprogram define ols_hetero, eclass\n  syntax, [nobs(int 100) b0(real 1) b1(real 1) gamma(real 1)]\n  clear\n  set obs `nobs'\n  gen x = rnormal(0,1)\n  gen u = rnormal(0,1)\n  gen y = `b0' + `b1' * x + u*exp(`gamma'*x)\n  // run regression (under homoskedasticity)\n  qui:reg y x\n  // store results\n  matrix b = _b[_cons], _se[_cons], _b[x], _se[x]\n  matrix colname b = \"b0\" \"se0\" \"b1\" \"se1\"\n  ereturn post b\nend\nsimulate, reps(1000) nodots: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\nsum \n\n\n\n      Command: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       _b_b0 |      1,000    .9979773    .1210228   .6013685   1.434932\n      _b_se0 |      1,000    .1196379    .0220658   .0737395   .2717805\n       _b_b1 |      1,000    .9864205    .2661498   .0342865     1.8922\n      _b_se1 |      1,000    .1195836    .0210792   .0808713   .2756631\n\n\nCorrecting for heteroskedasticity\nWe can correct for heteroskedasticity using robust standard errors.\n\n// define a program\ncapture program drop ols_hetero\nprogram define ols_hetero, eclass\n  syntax, [nobs(int 100) b0(real 1) b1(real 1) gamma(real 1)]\n  clear\n  set obs `nobs'\n  gen x = rnormal(0,1)\n  gen u = rnormal(0,1)\n  gen y = `b0' + `b1' * x + u*exp(`gamma'*x)\n  // run regression (under homoskedasticity)\n  qui:reg y x, robust\n  // store results\n  matrix b = _b[_cons], _se[_cons], _b[x], _se[x]\n  matrix colname b = \"b0\" \"se0\" \"b1\" \"se1\"\n  ereturn post b\nend\nsimulate, reps(1000) nodots: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\nsum \n\n\n\n      Command: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       _b_b0 |      1,000     1.00342    .1180407   .6255065     1.3921\n      _b_se0 |      1,000    .1171892    .0219696   .0807092   .3564852\n       _b_b1 |      1,000     1.00355    .2589393  -.2493259   1.956252\n      _b_se1 |      1,000    .2407364    .0913925   .1092693   1.174903\n\n\nor using weighted least squares.\n\n// define a program\ncapture program drop ols_hetero\nprogram define ols_hetero, eclass\n  syntax, [nobs(int 100) b0(real 1) b1(real 1) gamma(real 1)]\n  clear\n  set obs `nobs'\n  gen x = rnormal(0,1)\n  gen u = rnormal(0,1)\n  gen y = `b0' + `b1' * x + u*exp(`gamma'*x)\n  // run regression (under homoskedasticity)\n  qui:reg y x, \n  predict uhat, resid\n  gen lnuhat2 = ln(uhat^2)\n  reg lnuhat2 x\n  predict lnhx\n  gen hx=exp(lnhx)\n  // store results\n  qui:reg y x [w=1/hx], \n  matrix b = _b[_cons], _se[_cons], _b[x], _se[x]\n  matrix colname b = \"b0\" \"se0\" \"b1\" \"se1\"\n  ereturn post b\nend\nsimulate, reps(1000) nodots: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\nsum \n\n\n\n      Command: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       _b_b0 |      1,000    1.001033    .0465683   .8545606   1.252005\n      _b_se0 |      1,000    .0478398    .0084316   .0288236   .1462429\n       _b_b1 |      1,000    1.001063    .0277128   .8875045   1.301835\n      _b_se1 |      1,000    .0270911    .0091106   .0088032   .1357872"
  },
  {
    "objectID": "rmethods2/session_7.html#more-on-monte-carlo-simulations",
    "href": "rmethods2/session_7.html#more-on-monte-carlo-simulations",
    "title": "Research Methods II",
    "section": "More on Monte Carlo Simulations",
    "text": "More on Monte Carlo Simulations\n\nYou can use Monte Carlo simulations to study the properties of new estimators as well. (most often)\nThe structure of the simulation, however, will depend on the estimator you want to study, and may not be fully generalizable.\n\nNotice that in the previous example, we assumed that all data needed to be simulated.\nBut, we could just as well simulate only “parts” of the data, and use observed data for the rest.\n\n\n\nfrause oaxaca, clear\nprobit lfp female educ age agesq married divorced\npredict lfp_xb, xb\nmatrix b=e(b)\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\nIteration 0:  Log likelihood = -634.26553  \nIteration 1:  Log likelihood = -453.80541  \nIteration 2:  Log likelihood = -429.25759  \nIteration 3:  Log likelihood = -428.40991  \nIteration 4:  Log likelihood = -428.40986  \nIteration 5:  Log likelihood = -428.40986  \n\nProbit regression                                       Number of obs =  1,647\n                                                        LR chi2(6)    = 411.71\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -428.40986                             Pseudo R2     = 0.3246\n\n------------------------------------------------------------------------------\n         lfp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -1.644408   .1498206   -10.98   0.000    -1.938051   -1.350765\n        educ |   .1051167   .0251728     4.18   0.000     .0557789    .1544544\n         age |   .1045008   .0385916     2.71   0.007     .0288627    .1801389\n       agesq |  -.0012596   .0004463    -2.82   0.005    -.0021344   -.0003848\n     married |  -1.692076   .1887409    -8.97   0.000    -2.062001    -1.32215\n    divorced |    -.68518   .2319883    -2.95   0.003    -1.139869   -.2304912\n       _cons |   .4413072   .7648821     0.58   0.564    -1.057834    1.940448\n------------------------------------------------------------------------------\n\n\n\n* Latent model LFP =1(lfp_xb + e&gt;0)\ncapture program drop probit_sim\nprogram  probit_sim, eclass\n  capture drop lfp_hat\n  gen lfp_hat=(lfp_xb + rnormal(0,1))&gt;0\n  probit lfp_hat female educ age agesq married divorced, from(b, copy)\nend\nsimulate _b _se, reps(500) nodots: probit_sim\nren lfp_hat* *\nsum ,sep(7)\n\n\n\n      Command: probit_sim\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n   _b_female |        500    -1.67208    .1594671  -2.262682  -1.280128\n     _b_educ |        500     .108218    .0251913   .0453378   .1974491\n      _b_age |        500    .1022977     .038849   -.004108   .2443264\n    _b_agesq |        500   -.0012359    .0004483  -.0029406  -.0000835\n  _b_married |        500   -1.720405    .1903062  -2.425838  -1.247641\n _b_divorced |        500   -.7046396    .2310935  -1.387282   .1018858\n     _b_cons |        500    .5167748    .7899037  -1.920398   2.650355\n-------------+---------------------------------------------------------\n  _se_female |        500    .1548574    .0187049   .1234346   .2547092\n    _se_educ |        500    .0255194    .0012991   .0223761    .029805\n     _se_age |        500    .0389973    .0017384   .0336116   .0466778\n   _se_agesq |        500    .0004507    .0000184   .0003941   .0005336\n _se_married |        500    .1987176    .0234131   .1538517   .3142596\n_se_divorced |        500    .2410041    .0207673    .199707   .3420741\n    _se_cons |        500    .7709869    .0397739   .6426999   .9831291\n\n\n\nTo some extent, this is similar to the imputation methods we have seen before.\nMore complex versions of this can be used to elaborate micro-simulations."
  },
  {
    "objectID": "rmethods2/session_7.html#what-are-micro-simulations",
    "href": "rmethods2/session_7.html#what-are-micro-simulations",
    "title": "Research Methods II",
    "section": "What are micro-simulations?",
    "text": "What are micro-simulations?\n\nMicro-simulation is a technique that is used to make micro units act and interact in a way that it is possible to aggregate to the level of interest.\nA micro simulation model can be seen as a set of rules, which operates on a sample of micro units (individuals, households, firms, etc.) to produce a set of outcomes.\nThe goal is to produce synthetic datasets that can be used to estimate the effects of policy changes.\nBecause micro-simulations are based on micro-data, they have the potential to capture the heterogeneity of the population. (decisions, preferences, etc.)"
  },
  {
    "objectID": "rmethods2/session_7.html#section",
    "href": "rmethods2/session_7.html#section",
    "title": "Research Methods II",
    "section": "",
    "text": "We can also think about micro-simulations as a way to simulate/predict/impute the behavior of a population of interest.\n\nThus a lot of what you learned in terms of Modeling, imputations, and matching can be applied here.\n\n\nSo what do we need?\n\nWe need a population of micro units (individuals, households, firms, etc.) that is representative of the population of interest. (survey data)\nDetails on a policy change that we want to simulate. (policy parameters)\nA set of rules that describe how the micro units interact with each other and with the policy change. (model for behavior)\nAn outcome of interest that we want to study. (outcome variable)"
  },
  {
    "objectID": "rmethods2/session_7.html#how-do-we-do-it",
    "href": "rmethods2/session_7.html#how-do-we-do-it",
    "title": "Research Methods II",
    "section": "How do we do it?",
    "text": "How do we do it?\n\nDepending on the type of analysis we want to do, the structure of a micro-simulation can be very simple or very complex.\nConsider the following example:\n\nWe want to study the effect of a policy that aims to increase the minimum wage in the labor market. What effects would this have?\n\nHigher wages ?\nincrease/decrease in employment?\n\nChanges in the distribution of wages?\nChanges in the Economic structure?\n\n\nMore complex models require more sophisticated interactions between the micro/and macro units and the policy change.\nHowever, simpler models can be useful at least to study first order/statistical effects of a policy change."
  },
  {
    "objectID": "rmethods2/session_7.html#example-the-case-of-higher-education",
    "href": "rmethods2/session_7.html#example-the-case-of-higher-education",
    "title": "Research Methods II",
    "section": "Example: The case of higher education",
    "text": "Example: The case of higher education\n\nConsider the following. The government wants to increase educational attainment in the population.\nTo do so, they want to evaluate the impact that a 2 additional years for people with less than 12 years of education would have on the population.\nHow do we do this?\nFor simplicilty , lets use the oaxaca dataset"
  },
  {
    "objectID": "rmethods2/session_7.html#section-1",
    "href": "rmethods2/session_7.html#section-1",
    "title": "Research Methods II",
    "section": "",
    "text": "We can start by modeling the effect of education on wages.\n\\[log(wage)= \\beta X + \\beta_e educ + u\\]\n\nfrause oaxaca, clear\nreg lnwage educ exper tenure female age, \npredict  res, res\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n      Source |       SS           df       MS      Number of obs   =     1,434\n-------------+----------------------------------   F(5, 1428)      =    101.03\n       Model |   105.60186         5   21.120372   Prob &gt; F        =    0.0000\n    Residual |  298.517944     1,428  .209046179   R-squared       =    0.2613\n-------------+----------------------------------   Adj R-squared   =    0.2587\n       Total |  404.119804     1,433  .282009633   Root MSE        =    .45722\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0743986   .0052612    14.14   0.000     .0640782    .0847191\n       exper |   .0022628   .0018958     1.19   0.233    -.0014562    .0059817\n      tenure |   .0021805   .0019783     1.10   0.271    -.0017001    .0060611\n      female |  -.1321951   .0254327    -5.20   0.000    -.1820846   -.0823056\n         age |   .0136344   .0017751     7.68   0.000     .0101523    .0171165\n       _cons |   1.985785   .0732567    27.11   0.000     1.842083    2.129488\n------------------------------------------------------------------------------\n(213 missing values generated)\n\n\nID the policy change\n\nclonevar educ2=educ\nreplace educ=educ+2 if educ&lt;12\npredict yhat2, xb\n\n(1,099 real changes made)\n(213 missing values generated)\n\n\nSo what is the effect on wages? (if there is no selection bias)\n\ngen wage = exp(lnwage)\ngen wage2 = exp(yhat2+res)\ngen wage_diff = wage2-wage\ntabstat wage_diff\nreplace educ = educ2\n\n(213 missing values generated)\n(213 missing values generated)\n(213 missing values generated)\n\n    Variable |      Mean\n-------------+----------\n   wage_diff |  2.947924\n------------------------\n(1,099 real changes made)\n\n\nWage has increased in 2.95.\nBut is this the only effect??"
  },
  {
    "objectID": "rmethods2/session_7.html#section-2",
    "href": "rmethods2/session_7.html#section-2",
    "title": "Research Methods II",
    "section": "",
    "text": "What about the effect on employment?\nThis is a more complex model, and we need further assumptions\n\nAssume anyone who wants to work, will find a job.\nThose employed remain employed\nThose non-employed will transition to employment marginally\n\n\n\\[P(lfp=1|X,educ)= \\beta X + \\beta_e educ + u\n\\]\nFirst model the probability of employment\n\nfrause oaxaca, clear\nqui:probit lfp educ female age single married kids6 kids714\npredict lfp_xb, xb\npredict pr_org, pr\n\nreplace lfp_xb = lfp_xb + _b[educ] * 2 if educ&lt;12\ngen plfp = normal(lfp_xb)\n\n** Before and after the policy change\nsum plfp pr_org \n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(1,099 real changes made)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        plfp |      1,647    .8995986    .1614962   .0777393   .9999999\n      pr_org |      1,647    .8707874    .1925782   .0604887   .9999999\n\n\nSo now we have the original probability of employment and the probability of employment after the policy change.\nHow do we know who will transition from not working to working?\n\nOption 1. Assign new workers based on the relative change in the probability of employment.\n\n\ngen dprob = (plfp-pr_org)/pr_org\nclonevar lfp_post1 = lfp\nreplace lfp_post1 =1 if lfp==0 & dprob&gt;runiform()\nsum  lfp_post1 lfp\n\n(35 real changes made)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n   lfp_post1 |      1,647    .8919247    .3105698          0          1\n         lfp |      1,647     .870674    .3356624          0          1\n\n\n\nOption 2. Simulate employment status based on the original and post-policy probability of employment.\n\n\n** Option 2\ndrop2 unf lfp_org lfp_post\ngen unf = runiform()\ngen lfp_org  = pr_org&gt;unf\ngen lfp_post = plfp  &gt;unf\n\nsum lfp_post  lfp_org\n\nvariable unf not found\nvariable lfp_org not found\nvariable lfp_post not found\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n    lfp_post |      1,647    .9046752    .2937523          0          1\n     lfp_org |      1,647    .8737098    .3322771          0          1\n\n\n\nBoth options are valid, but\n\nFirst one requires to impute wages for the new workers only.\nSecond one requires to impute wages for the entire population."
  },
  {
    "objectID": "rmethods2/session_7.html#section-3",
    "href": "rmethods2/session_7.html#section-3",
    "title": "Research Methods II",
    "section": "",
    "text": "Imputing wages:\n\nWe could assume no selection bias.\nWe may need to use only data available for everyone, or use imputed data (exper tenure) (perhaps at 0?)\n\n\nreg lnwage educ female age single married kids6 kids714, \npredict  res, res\npredict lnwage_hat, xb\n** Simulating Known wages component\nreplace lnwage_hat = lnwage_hat + _b[educ] * 2 if educ&lt;12\n** Simulating random component\n** For those already working:\nreplace lnwage_hat = lnwage_hat + res if lfp==1\n** Simulate unobserved\nqui: sum res, \nreplace lnwage_hat = lnwage_hat + rnormal(0,r(sd)) if lfp_post1==1 & lfp==0\n\ngen wage_post = exp(lnwage_hat) if lfp_post1==1 | lfp==1\ngen wage = exp(lnwage)\nsum wage wage_post\nsgini wage wage_post\n\n\n      Source |       SS           df       MS      Number of obs   =     1,434\n-------------+----------------------------------   F(7, 1426)      =     79.23\n       Model |  113.158257         7  16.1654653   Prob &gt; F        =    0.0000\n    Residual |  290.961547     1,426  .204040355   R-squared       =    0.2800\n-------------+----------------------------------   Adj R-squared   =    0.2765\n       Total |  404.119804     1,433  .282009633   Root MSE        =    .45171\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0709179   .0050262    14.11   0.000     .0610584    .0807774\n      female |  -.1427501   .0244736    -5.83   0.000    -.1907583   -.0947419\n         age |    .016475   .0014033    11.74   0.000     .0137222    .0192277\n      single |  -.0711724   .0443651    -1.60   0.109    -.1582002    .0158554\n     married |  -.0977016   .0379654    -2.57   0.010    -.1721755   -.0232276\n       kids6 |   .1085073   .0239699     4.53   0.000     .0614873    .1555272\n     kids714 |   .0656187    .019681     3.33   0.001      .027012    .1042254\n       _cons |   1.999222   .0902563    22.15   0.000     1.822173    2.176272\n------------------------------------------------------------------------------\n(213 missing values generated)\n(1,099 real changes made)\n(1,434 real changes made)\n(35 real changes made)\n(178 missing values generated)\n(213 missing values generated)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        wage |      1,434    32.39167    16.12498   1.661434   192.3077\n   wage_post |      1,469    35.19396    16.94609   1.873127   221.6129\n\nGini coefficient for wage, wage_post\n\n-----------------------\n    Variable |      v=2\n-------------+---------\n        wage |   0.2460\n   wage_post |   0.2356\n-----------------------\n\n\nThis is equivalent to running a single imputation round. However, we could repeat the process M times to get a measure for the precission of our estimates.\nWhat else can we do?\n\nWe could go further and also simulate where would people work, and how would the economy change.\nWe could also account for selection problems\nor make a more explicit model for distributional analysis."
  },
  {
    "objectID": "rmethods2/session_7.html#st-micro-simulation",
    "href": "rmethods2/session_7.html#st-micro-simulation",
    "title": "Research Methods II",
    "section": "1st Micro-simulation",
    "text": "1st Micro-simulation\na Review\n\nWe have just use a simple micro-simulation to study the effect of a policy change (education) on wages and employment.\nThis simulation had many assumptions, and we could have done it in many different ways.\nAmong others, we assume no selection bias, with an instantaneous change in education, and no again of the population.\nWe also made important assumptions regarding the transition from non-employment to employment, and the imputation of wages."
  },
  {
    "objectID": "rmethods2/session_7.html#not-the-only-way-to-do-it",
    "href": "rmethods2/session_7.html#not-the-only-way-to-do-it",
    "title": "Research Methods II",
    "section": "Not the only way to do it",
    "text": "Not the only way to do it\n\nWhile many micro-simulations are based on stochastic simultions, there are other ways to do it.\nIn Hotckiss et al. (forthcoming), we use a deterministic micro-simulation to study the effect of tax-reforms on welfare changes and its distribution for the - first\nHow did we do it?"
  },
  {
    "objectID": "rmethods2/session_7.html#tax-reform-on-households-welfare",
    "href": "rmethods2/session_7.html#tax-reform-on-households-welfare",
    "title": "Research Methods II",
    "section": "Tax Reform on Households Welfare",
    "text": "Tax Reform on Households Welfare\n\nWe concentrated mostly on couple households with young children (0-18 years old).\nUsing a heckman selection model, we impute wages for the non-working people (based on PMM).\nUsing observed and imputed wages, we impute the tax liability for each household before and after the reform. (TAX-SIM), and estimate after-tax wages.\nEstimate Household Labor Supply based on HH utility and Non-linear Tobit model\nUse HLS models, we make predictions for Labor Supply changes and utility changes given the Reform.\n\nThe Outcome was how much better/worse off would households be after the reform."
  },
  {
    "objectID": "rmethods2/session_7.html#intro",
    "href": "rmethods2/session_7.html#intro",
    "title": "Research Methods II",
    "section": "Intro",
    "text": "Intro\n\nAt Levy, we have also constructed a micro-simulation model to study employment simulations.\nMethod first developed for estimating the impact of the American Recovery and Reinvestment Act of 2009\n\nConvert spending into jobs by industry and occupation (I/O matrix)\nAssign potential workers to jobs\nPredict earnings and hours\n\nFor the work with LIMEW and LIMTIP, this has also been used for distributional analysis of employment assigment and services use."
  },
  {
    "objectID": "rmethods2/session_7.html#limm-step-i",
    "href": "rmethods2/session_7.html#limm-step-i",
    "title": "Research Methods II",
    "section": "LIMM: Step I",
    "text": "LIMM: Step I\nJob Creation\n\nConsider a policy: Road construction, Services provision, etc.\n\nCalculate changes in final demand for each industry the policy creates\nUsing I-O tables estimate change in total output for each industry\nUse that change in output to estimate change demand for labor inputs\n\nTransform the changes from labor imputs to generated jobs (consider wages)\n\nDistribute changes across occupations (within industry)\n\nUsing, for example, shares of employment by occupation within industry\n\n\nWith this we have a total number of jobs created by industry and occupation."
  },
  {
    "objectID": "rmethods2/session_7.html#limm-step-ii",
    "href": "rmethods2/session_7.html#limm-step-ii",
    "title": "Research Methods II",
    "section": "LIMM: Step II",
    "text": "LIMM: Step II\nJob Assigment\n\nGiven the Total change in Jobs, we need to assign workers to those jobs.\n\nWho are the potential workers?\n\nNot in LF: Potential, but not looking for work (may depend on characteristics). Avoid retired, disabled and students\nUnemployed: Most likely to take a job (pool may not be large enough)\nUnderemployed: Working part-time, but willing to work full-time, or people aiming for better jobs. (May create job openings)\nand Employed: May be willing to change jobs (may create job openings)"
  },
  {
    "objectID": "rmethods2/session_7.html#section-4",
    "href": "rmethods2/session_7.html#section-4",
    "title": "Research Methods II",
    "section": "",
    "text": "Job Assigment II\n\nTwo Steps, modeling job creation and job assigment\nA probit/logit model would be estimated to predict the likelihood of working, and this will be used to assign jobs.\n\nJob assignment is done using a multinomial model (mprobit) to predict likelihoods of working on a given occupation & industry.\nIdeally, you would like to do both at the same time, but that is a very complex model to estimate. Instead, each one is estimated separately.\n\n\n\\[\\begin{aligned}\nI^*(ind = 1 ) &= \\beta_1 X + e_1 \\\\\nI^*(ind = 2 ) &= \\beta_2 X + e_2 \\\\\n&\\vdots \\\\\nI^*(ind = k ) &= \\beta_k X + e_k\n\\end{aligned}\n\\]\nWhere \\(I^*(ind = k )\\) is the latent likelihood of working on industry \\(k\\) given characteristics. Mprobit or Mlogit depends on the distribution of \\(e_k\\).\nYou choose Industry \\(k\\) if \\(I^*(ind = k )\\) is the highest among all industries."
  },
  {
    "objectID": "rmethods2/session_7.html#section-5",
    "href": "rmethods2/session_7.html#section-5",
    "title": "Research Methods II",
    "section": "",
    "text": "Job Assigment III\n\nJob Assigment is done as follows:\n\nFor each potential worker, Calculate Prob of working. Those with the highest probability are assigned jobs first.\nFor Worker, \\(i\\), the individual is assigned to the industry with the highest likelihood of working. (until all jobs are assigned)\n\nCan be done followed by doing similar assigment for occupation within industry.\nOr combine both industry and occupation (p_o * p_i)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nJob allocation must consider jobs available and likehood of working on a given Occupation/Industry.\nMay want to avoid deterministic assignment.\ncould assigned based on random draws from the distribution of likelihoods."
  },
  {
    "objectID": "rmethods2/session_7.html#section-6",
    "href": "rmethods2/session_7.html#section-6",
    "title": "Research Methods II",
    "section": "",
    "text": "Earning contributions\n\nBecause some of the newly created jobs go to people who are employed in Family businesses (Farm and non farm income), one has to account for the “loss” of income from those jobs.\nThis is done by estimating the contribution of each member to the family business, and imputing the loss of income from the now “formally employed” member.\n\nFor example, modeling farm income, predicting that income without the family member, and imputing the difference as the loss of income."
  },
  {
    "objectID": "rmethods2/session_7.html#section-7",
    "href": "rmethods2/session_7.html#section-7",
    "title": "Research Methods II",
    "section": "",
    "text": "Job Assigment IV: Wages and Hours\n\nHours and Wages are imputed using a heckman model, and multiple stage process\n\n\nUse a probit model to predict the likelihood of working.\nGiven the model obtain the inverse mills ratios. \\(imr = \\phi(\\alpha'x)/\\Phi(\\alpha'x)\\)\nModel wages and hours using IMR as a regressor. (Heckit model) (This includes info on imputed occupations/industry)\nUse Imputed wages and hours to Match data to “donor” individuals. (PMM)\nFor people “potentially” leaving family business, compare new wages to family business contributions"
  },
  {
    "objectID": "rmethods2/session_7.html#section-8",
    "href": "rmethods2/session_7.html#section-8",
    "title": "Research Methods II",
    "section": "",
    "text": "Hours of HP re-assigment\n\nPeople taking jobs may have to change their contributions to household production. (less time for HP)\nBut the rest of the family may also have to adjust their contributions to HP. (more time for HP?)\n\nThis is done via matching, where the “donor” are working individuals with similar family characteristics.\nOr All working individuals\nCurrently, “recipients” are all individuals in the household, with atleast one new worker.\n\nMay be more sensible to impute hours\nOne may argue those not working may have to increase hours of HP. Or do nothing\n\n\n\nOther considerations\n\nAssigment of Child Care services"
  },
  {
    "objectID": "rmethods2/session_7.html#assessing-the-quality-of-the-simulation",
    "href": "rmethods2/session_7.html#assessing-the-quality-of-the-simulation",
    "title": "Research Methods II",
    "section": "Assessing the quality of the simulation",
    "text": "Assessing the quality of the simulation\n\nIn principle, there is no way to know if the simulation is correct.\n\nThere is no “true” value to compare to.\nHowever, one may want to at least ensure the simulation replicates the data.\nOne can potetially use the simulation to predict changes of a past policy, and compare to the actual changes.\n\nWe do want to make “sanity” checks\n\nAre results consistent and plausible?\nAre distributions of post-assigment outcomes consistent with the data?"
  },
  {
    "objectID": "rmethods2/session_7.html#limitations",
    "href": "rmethods2/session_7.html#limitations",
    "title": "Research Methods II",
    "section": "Limitations",
    "text": "Limitations\n\nSince we use existing data, we make the implicit assumption “no behavioral changes” in other aspects\n\nThings change as far as we can model them.\n\nResults are typically point estimates, and do not account for uncertainty.\n\nWe can use multiple imputation, monte carlo simulations, or bootstrapping to account for uncertainty.\n\nThe results are only as good as the data we use and the assumptions we make.\n\nWe can use sensitivity analysis to test the robustness of the results to changes in assumptions."
  },
  {
    "objectID": "rmethods2/session_5.html#statistical-significance-1",
    "href": "rmethods2/session_5.html#statistical-significance-1",
    "title": "Research Methods II",
    "section": "Statistical Significance",
    "text": "Statistical Significance\n\nWhat is statistical significance?\n\nStatistical significance is a way of determining if an observed effect is due to chance.\n\nWe typically use statistical significance to determine if the results of a study are meaningful, using various criteria.\n\nIs the p-value less than 0.05?\nDoes the 95% confidence interval include zero?\nis the t-statistic greater than 1.96?\n\nBut what exactly does that tell us?"
  },
  {
    "objectID": "rmethods2/session_5.html#back-to-the-basics",
    "href": "rmethods2/session_5.html#back-to-the-basics",
    "title": "Research Methods II",
    "section": "Back to the basics",
    "text": "Back to the basics\n\nAssume you are testing for the effectiveness of a medicine that treats the common cold.\n\nHow do you know if the treatment is effective? (i.e., Does reduce the duration of the cold?)\nWe make an hypothesis!\nThe null hypothesis is that there is no effect of the treatment. (H0: no effect)\nYou collect some data and find out that it reduces the duration of the cold by 1 days.\nIs this a significant effect?"
  },
  {
    "objectID": "rmethods2/session_5.html#section-2",
    "href": "rmethods2/session_5.html#section-2",
    "title": "Research Methods II",
    "section": "",
    "text": "Depends…\n\nThe sample size (how many people were in the study?)\n\nIf the sample size is 10,000, then a 1 day reduction in the duration of the cold may be significant.\nBut if the sample size is 10, then a 1 day reduction may be due to chance.\n\nThe effect size\n\nIf the effect size is 0.1 days, may not be significant.\nIf the effect size is 10 days, may be significant."
  },
  {
    "objectID": "rmethods2/session_5.html#section-3",
    "href": "rmethods2/session_5.html#section-3",
    "title": "Research Methods II",
    "section": "",
    "text": "Then what?\n\nStatistical significance is a way of determining if the observed effect is due to chance.\nTo do this, we required assumptions about the distribution under the Null Hypothesis.\n\nAssume that the data is normally distributed.\nAnd that there is a 4% chance you observe a value as extreme as the one you observed\n\nIn that case you would say, the effect is significant.\n\n\nJust couple of caveats:\n\nSignicance can be achieved by either measuring a “large” effect\nor by measuring a “small” effect with a large sample size. (very precisely)\n\nThus, finding no significance could mean the effect is noise or that the sample size is too small.\nRecall Hypothesis can be true or not. We only know if the data is consistent with the hypothesis or not."
  },
  {
    "objectID": "rmethods2/session_5.html#section-4",
    "href": "rmethods2/session_5.html#section-4",
    "title": "Research Methods II",
    "section": "",
    "text": "From Significance to Power\n\nThe power of a statistical test represents the probability of detecting an effect, given that the effect is real.\nFor example, say a drug has the effect of reducing the duration of the common cold by 1 day. But you do not know this\nInstead you make your hypothesis. How likely is that the effect is significant?\n\nYou find the effect is not significant at 10% level.\n\nWhat is happening?\n\nThe effect was true, yet we find no significance.\nThe power of the test was low. (sample size was too small)\n\nIn General, Setting high significance levels will reduce the power of the test."
  },
  {
    "objectID": "rmethods2/session_5.html#section-5",
    "href": "rmethods2/session_5.html#section-5",
    "title": "Research Methods II",
    "section": "",
    "text": "If At First You Don’t Succeed, Try, Try Again\n\nRQuestionTestingResults\n\n\n\n\n\n\n\n\n\n\n\n\nCartoon from xkcd, by Randall Munroe"
  },
  {
    "objectID": "rmethods2/session_5.html#section-6",
    "href": "rmethods2/session_5.html#section-6",
    "title": "Research Methods II",
    "section": "",
    "text": "Multiple Hypothesis Testing\n\nIn the previous example, the “SAME” hypothesis was tested multiple times.\nYet, it was still compared to the same significance level. Is this correct?\n\nConsider the following:\n\nYou collect 100 data points, from a normal distribution, with mean 0 and standard deviation 1.\nYou know there is only a 5% chance that the mean is greater (abs) than 0.196 (95% confidence interval)\n\nSo you run the same experiment 100 times - How many times do you expect to find a mean greater than 0.196? - What are the chances of finding a mean greater than 0.196 at least once?"
  },
  {
    "objectID": "rmethods2/session_5.html#section-7",
    "href": "rmethods2/session_5.html#section-7",
    "title": "Research Methods II",
    "section": "",
    "text": "A1: 5% of the time\nA2:\n\nPr of finding any “significant” effect in one experiment: \\(1 - 0.95 = 0.05\\)\nPr of finding any “significant” effect in two experiments: \\(1-0.95^2 = 0.0975\\)\nPr of finding no effect in 100 experiments: \\(1-0.95^{100}  = 0.99408\\)\n\n\nSo if you run the experiment enough times, you are almost certain to find a “significant” effect.\n\nAlso, While a single experiment has a 5% chance of finding a “significant” effect (alpha = 0.05), the “alpha” for 2 experiments is 0.0975!"
  },
  {
    "objectID": "rmethods2/session_5.html#section-9",
    "href": "rmethods2/session_5.html#section-9",
    "title": "Research Methods II",
    "section": "",
    "text": "Controlling for Multiple Hypothesis Testing\n\nThere are various ways to control for multiple hypothesis testing.\nSIDAK = \\(\\alpha_{adj} = (1-\\alpha_{tg})^{1/n}\\)\nBONFERRONI = \\(\\alpha_{adj} = \\alpha_{tg}/n\\)\nHOLM = \\(\\alpha_{adj,i} = \\alpha_{tg}/(n-i+1)\\)\n\nWhere \\(\\alpha_{tg}\\) is the target \\(\\alpha\\) level (e.g., 0.05), and \\(n\\) is the number of tests, and \\(\\alpha_{i,adj}\\) is the adjusted \\(\\alpha\\) level.\n\nThere is also Uniform Confidence Intervals (see here)\n\n\nThere is a caveat. They are designed to control for Type I errors (False positives), but they increase the chances of Type II errors. (Less power)"
  },
  {
    "objectID": "rmethods2/session_5.html#missing-data",
    "href": "rmethods2/session_5.html#missing-data",
    "title": "Research Methods II",
    "section": "Missing Data",
    "text": "Missing Data\n\nMissing data is a common problem in empirical research.\nDue to various reasons, some observations may be missing.\n\nRefusal to answer a question\nData entry errors/ommissions\nData loss\netc.\n\nThis can be a problem for various reasons:\n\nMissing data can produced biased and inconsistent estimates.\nIt may also reduce sample size, and thus power. (Potentially making estimation unfeasible)\n\nSo what can we do?"
  },
  {
    "objectID": "rmethods2/session_5.html#types-of-missing-data",
    "href": "rmethods2/session_5.html#types-of-missing-data",
    "title": "Research Methods II",
    "section": "Types of Missing Data",
    "text": "Types of Missing Data\n\nMissing Completely at Random (MCAR)\n\nThe probability of missing data does not depend on any observed or unobserved data.\nThis is the best case scenario. (this is like sampling)\n\nMissing at Random (MAR)\n\nThe probability of missing data depends on observed data.\nSecond Best: Its possible to address the problem using various methods.\n\nMissing Not at Random (MNAR)\n\nThe probability of missing data depends on unobserved data.\nWorst case scenario: It is usually very difficult to address"
  },
  {
    "objectID": "rmethods2/session_5.html#what-its-done-and-what-can-be-done",
    "href": "rmethods2/session_5.html#what-its-done-and-what-can-be-done",
    "title": "Research Methods II",
    "section": "What its done, and what can be done",
    "text": "What its done, and what can be done\n\nComplete Case Analysis (CCA)\n\nDrop observations with missing data.\nThis is the default in most statistical software.\nThis is a bad idea, unless the data is MCAR.\n\nImputation\n\nReplace missing values with a value.\nThis is a better idea, but it depends on the type of missing data.\nRequires modeling the missing data mechanism, and outcome model.\n\nReweighting\n\nWeight observations to account for missing data.\nRequires modeling the missing data mechanism"
  },
  {
    "objectID": "rmethods2/session_5.html#reweighting",
    "href": "rmethods2/session_5.html#reweighting",
    "title": "Research Methods II",
    "section": "Reweighting",
    "text": "Reweighting\nConsider the following example: \\[\\begin{aligned}\n\\text{Pop}&: y = x\\beta+ \\epsilon \\\\\n\\text{Miss Mech }&: p(nmiss|x) = F(x\\gamma) \\\\\n\\text{Miss Reg }&: m\\times y = m\\times x \\beta + m\\times \\epsilon  \n\\end{aligned}\n\\]\n\nWhere \\(m\\) is an indicator of missingness, and \\(F\\) is the function of missing.\nDefine the Weights as \\(w = \\frac{1}{1-p(nmiss|x)}\\)\nThen, we could use WLS to estimate the model of interest:\n\n\\[w \\times m\\times y = w \\times  m\\times x \\beta + w \\times  m\\times \\epsilon\\]"
  },
  {
    "objectID": "rmethods2/session_5.html#example",
    "href": "rmethods2/session_5.html#example",
    "title": "Research Methods II",
    "section": "Example",
    "text": "Example\n\nCodeResults\n\n\nfrause oaxaca, clear\ndrop if lnwage ==.\n** Modeling Missing\nreg lnwage c.(educ exper tenure female age)## c.(educ exper tenure female age)  \npredict lxb\nqui:sum lxb \nreplace lxb = normal((lxb -r(mean))/r(sd))\ngen lnwage2 = lnwage if lxb &lt;runiform()\ngen dwage = lnwage2!=.\n** Modeling Missing data\nlogit dwage educ exper tenure female age\npredict prw, pr\ngen wgt = 1/prw\n** Estimating the model\nreg lnwage educ exper tenure female age\nreg lnwage2 educ exper tenure female age\nreg lnwage2 educ exper tenure female age [w=wgt]\n** Repeat the process 1000 times"
  },
  {
    "objectID": "rmethods2/session_5.html#imputation-mean-and-predictive-mean",
    "href": "rmethods2/session_5.html#imputation-mean-and-predictive-mean",
    "title": "Research Methods II",
    "section": "Imputation: Mean and Predictive Mean",
    "text": "Imputation: Mean and Predictive Mean\n\nThe second approach is to impute the missing values. AKA Substitute the unobserved values with some prediction we can construct.\n\nConsider the case of a single variable \\(Z\\) with missing values, and assume we have a model for \\(Z\\):\n\\[Z = X\\beta + \\epsilon\n\\]"
  },
  {
    "objectID": "rmethods2/session_5.html#section-10",
    "href": "rmethods2/session_5.html#section-10",
    "title": "Research Methods II",
    "section": "",
    "text": "We could “predict” missing values using the mean of the observed values:\n\n\\[\\hat{Z} = \\bar{Z} = \\frac{1}{n}\\sum_{i=1}^n Z_i\\]\n\nOr we could use the predicted values from the model:\n\n\\[\\hat{Z} = X\\hat{\\beta}\\]\nNeither is a good idea, even under MCAR. (Why?)\n\nWe are getting rid of ALL uncertainty (variance) in the missing values."
  },
  {
    "objectID": "rmethods2/session_5.html#section-12",
    "href": "rmethods2/session_5.html#section-12",
    "title": "Research Methods II",
    "section": "",
    "text": "Better Approach: Stochastic Imputation\n\nA better approach of imputation is to use a model to predict not only the “known” variation (Conditional mean), but also the “unknown” variation (Conditional variance).\nSo, we can use the model to predict the missing values, but we add some noise to the prediction.\n\n\\[\\tilde z = X\\hat{\\beta} + \\hat \\epsilon\\]\n\nWhere \\(\\hat \\epsilon\\) is a “random” residual obtain based on the model assumptions.\n\\(\\tilde z\\) is a stochastic imputation of \\(z\\)."
  },
  {
    "objectID": "rmethods2/session_5.html#section-13",
    "href": "rmethods2/session_5.html#section-13",
    "title": "Research Methods II",
    "section": "",
    "text": "Even Better: Account for the uncertainty in the model\n\nWe can also account for the uncertainty in the model by considering the uncertainty in the model parameters, and the error:\n\n\\[z = X\\beta + \\epsilon\n\\]\n\nUnder normality assumptions, we could estimate the model using MLE, and obtain the variance covariance matrix of the parameters.\n\n\\[\n\\begin{pmatrix}\n\\hat{\\beta} \\\\ \\hat{\\sigma}^2 \\end{pmatrix}\n\\sim N\\left(\\begin{bmatrix} \\beta \\\\\n\\sigma\n\\end{bmatrix}, \\begin{bmatrix}\nV_{\\beta} & 0 \\\\\n0 & V_{\\sigma}\n\\end{bmatrix}\\right)\n\\]\n\nSo, we can get \\(\\tilde \\beta\\) and \\(\\tilde \\sigma\\), from random draws from the distribution above, and then use them to impute the missing values."
  },
  {
    "objectID": "rmethods2/session_5.html#section-14",
    "href": "rmethods2/session_5.html#section-14",
    "title": "Research Methods II",
    "section": "",
    "text": "Even Better than before: Multiple Imputation\n\nThe previous methods assumed you only need one imputation to solve the Imputation problem.\nOne, however, may not be enough to account for the uncertainty in the imputation process.\nSo, we can repeat the imputation process multiple times, and obtain multiple imputed values for each missing data.\nWith multiple imputed values, we can estimate the model of interest multiple times, and then combine the results using Rubin’s rules."
  },
  {
    "objectID": "rmethods2/session_5.html#section-15",
    "href": "rmethods2/session_5.html#section-15",
    "title": "Research Methods II",
    "section": "",
    "text": "Call M the number of imputations, and \\(m\\) the imputation index.\n\n\\[\\beta_{MI} = \\frac{1}{M}\\sum_{m=1}^M \\beta_m\\]\n\\[V_{MI} = \\frac{1}{M}\\sum_{m=1}^M V_m + \\left(\\frac{M+1}{M}\\right)Var(\\beta_m)\\]\n\nWhere \\(V_m\\) is the VCV matrix of the parameters for each imputation, and \\(Var(\\beta_m)\\) is the variance of the parameters across imputations.\n\n\\[df = (M-1) \\left( 1 + \\frac{M}{M+1}\\frac{Var_m}{Var_B}\\right)^2\\]"
  },
  {
    "objectID": "rmethods2/session_5.html#section-17",
    "href": "rmethods2/session_5.html#section-17",
    "title": "Research Methods II",
    "section": "",
    "text": "Example: Stata\n\nfrause oaxaca, clear\ndrop if lnwage ==.\n\n** Modeling Missing\nforeach i in  educ exper tenure age {\n    gen m_`i' = `i' if runiform()&gt;.25\n}\n\n\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n(362 missing values generated)\n(311 missing values generated)\n(387 missing values generated)\n(348 missing values generated)\n\n\nSetting data for -mi- commands\n\nmi set wide\nmi register imputed m_*\nmi impute chain (reg) m_* = lnwage single female, add(10) \n\n\nConditional models:\n           m_exper: regress m_exper m_age m_educ m_tenure lnwage single\n                     female\n             m_age: regress m_age m_exper m_educ m_tenure lnwage single\n                     female\n            m_educ: regress m_educ m_exper m_age m_tenure lnwage single\n                     female\n          m_tenure: regress m_tenure m_exper m_age m_educ lnwage single\n                     female\n\nPerforming chained iterations ...\n\nMultivariate imputation                     Imputations =       10\nChained equations                                 added =       10\nImputed: m=1 through m=10                       updated =        0\n\nInitialization: monotone                     Iterations =      100\n                                                burn-in =       10\n\n            m_educ: linear regression\n           m_exper: linear regression\n          m_tenure: linear regression\n             m_age: linear regression\n\n------------------------------------------------------------------\n                   |               Observations per m             \n                   |----------------------------------------------\n          Variable |   Complete   Incomplete   Imputed |     Total\n-------------------+-----------------------------------+----------\n            m_educ |       1072          362       362 |      1434\n           m_exper |       1123          311       311 |      1434\n          m_tenure |       1047          387       387 |      1434\n             m_age |       1086          348       348 |      1434\n------------------------------------------------------------------\n(Complete + Incomplete = Total; Imputed is the minimum across m\n of the number of filled-in observations.)\n\n\nEstmating the model(s):\n\nmi estimate, post: regress lnwage m_* single female\nest sto m1\nregress lnwage educ exper tenure age single  female\n\n\nMultiple-imputation estimates                   Imputations       =         10\nLinear regression                               Number of obs     =      1,434\n                                                Average RVI       =     0.3607\n                                                Largest FMI       =     0.5511\n                                                Complete DF       =       1427\nDF adjustment:   Small sample                   DF:     min       =      31.27\n                                                        avg       =     293.52\n                                                        max       =   1,277.02\nModel F test:       Equal FMI                   F(   6,  403.1)   =      63.74\nWithin VCE type:          OLS                   Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      m_educ |   .0780927   .0062465    12.50   0.000     .0656678    .0905176\n     m_exper |   .0034867    .002416     1.44   0.154    -.0013503    .0083237\n    m_tenure |   .0023203    .002857     0.81   0.423    -.0035046    .0081451\n       m_age |   .0099868   .0023905     4.18   0.000     .0052212    .0147524\n      single |    -.09733   .0309067    -3.15   0.002    -.1580571    -.036603\n      female |  -.1226849   .0255255    -4.81   0.000    -.1727613   -.0726084\n       _cons |   2.102173   .1059333    19.84   0.000     1.889132    2.315213\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     1,434\n-------------+----------------------------------   F(6, 1427)      =     86.35\n       Model |  107.645555         6  17.9409258   Prob &gt; F        =    0.0000\n    Residual |  296.474249     1,427  .207760511   R-squared       =    0.2664\n-------------+----------------------------------   Adj R-squared   =    0.2633\n       Total |  404.119804     1,433  .282009633   Root MSE        =    .45581\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0753085    .005253    14.34   0.000     .0650041    .0856129\n       exper |   .0026545   .0018941     1.40   0.161     -.001061    .0063701\n      tenure |   .0022932   .0019725     1.16   0.245    -.0015761    .0061625\n         age |   .0111437   .0019397     5.75   0.000     .0073388    .0149486\n      single |  -.0918932   .0292993    -3.14   0.002    -.1493674   -.0344189\n      female |  -.1289592   .0253754    -5.08   0.000    -.1787363   -.0791822\n       _cons |   2.100201   .0816356    25.73   0.000     1.940063     2.26034\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods2/session_5.html#what-about-ldv-models",
    "href": "rmethods2/session_5.html#what-about-ldv-models",
    "title": "Research Methods II",
    "section": "What about LDV models?",
    "text": "What about LDV models?\n\nThe method sketched above (OLS) can also be extended to other models\nConsider Logit models\n\nS1: Estimate Logit model: \\(P(y=1|X) = F(X\\beta)\\)\nS2: Draw \\(\\beta\\) from the distribution, call it \\(\\tilde\\beta\\)\nS3: Draw \\(y\\) from a Bernoulli distribution: \\(y \\sim Bernoulli(F(X\\tilde\\beta))\\)\n\nSimilar procedures can be done for other models."
  },
  {
    "objectID": "rmethods2/session_5.html#other-methods-hotdecking",
    "href": "rmethods2/session_5.html#other-methods-hotdecking",
    "title": "Research Methods II",
    "section": "Other Methods: HotDecking",
    "text": "Other Methods: HotDecking\n\nHotdecking is a method of imputation that uses the observed values of the data to impute the missing values.\nBecause it uses data from the empirical distribution (observed data), it produces “valid” imputations for any kind of data.\nThe idea is to find a pool of potential “donors” for the one with missing data. (similar observations)\nThen select one candidate and use its data to impute the missing values."
  },
  {
    "objectID": "rmethods2/session_5.html#section-18",
    "href": "rmethods2/session_5.html#section-18",
    "title": "Research Methods II",
    "section": "",
    "text": "Definition of a “donor”\n\nDonors are identified as observations with similar characteristics to the one with missing data. (close to the missing observation)\nFinding potential donors is easy when there is low dimensional data, but it becomes more difficult as the number of variables increases."
  },
  {
    "objectID": "rmethods2/session_5.html#example-1",
    "href": "rmethods2/session_5.html#example-1",
    "title": "Research Methods II",
    "section": "Example",
    "text": "Example\nImputing data for wages in oaxaca.\n\nfrause oaxaca, clear\n** ID pool of donors based on age and gender\negen id_pool = group(age female)\n** Now, for each missing observation select a \"random\" donor\ngen misswage =missing(lnwage)\nbysort id_pool (misswage):egen smp = sum(misswage==0)\nbysort id_pool: gen draw = runiformint(1, smp)\nbysort id_pool: replace lnwage = lnwage[draw] if misswage==1\nsum lnwage if misswage==0\nsum lnwage if misswage==1 \nsum lnwage \n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 real changes made)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      lnwage |      1,434    3.357604    .5310458    .507681   5.259097\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      lnwage |        213    3.342183    .5482005    .507681   5.259097\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      lnwage |      1,647     3.35561    .5331507    .507681   5.259097"
  },
  {
    "objectID": "rmethods2/session_5.html#section-19",
    "href": "rmethods2/session_5.html#section-19",
    "title": "Research Methods II",
    "section": "",
    "text": "With many variables, we often estimating some distance measure and/or data reduction to ID “close” observations:\n\nPropensity Score (based on logit/probit/regress).\n\n\\[D(X,X_0) = abs(G(X) - G(X_0))\\]\n\nMahalanobis distance (based on X covariance matrix)\n\n\\[D(X,X_0) = \\sqrt{(X-X_0)'\\Sigma^{-1}(X-X_0)}\\]\n\nAffinity score: based on some linear combination of variables.\n\n\\[D(X,X_0) = \\frac{1}{K}\\sum_{i=1}^K \\alpha_i f\\left(\\frac{X_i - X_{0i}}{h}\\right)\\]"
  },
  {
    "objectID": "rmethods2/session_5.html#section-20",
    "href": "rmethods2/session_5.html#section-20",
    "title": "Research Methods II",
    "section": "",
    "text": "Once distances are estimated, donor pools can be defined based on the distance measure.\n\nSay, all observations with distance less than 0.1.\n\nAnd the donor can be selected randomly from the pool. (or weighted by distance)\nThis approaches could be vary computationally intensive, because it requires estimating \\(N\\times N\\) distances.\nComplexity may be reduced by using data reduction techniques, such as PCA, FA or propensity scores"
  },
  {
    "objectID": "rmethods2/session_5.html#section-21",
    "href": "rmethods2/session_5.html#section-21",
    "title": "Research Methods II",
    "section": "",
    "text": "Example: Stata\nUsing single Score (Data reduction)\n\nfrause oaxaca, clear\ndrop if lnwage ==.\n// 25% of data is missing\ngen mlnwage = lnwage if runiform()&gt;.25\ngen misswage =missing(mlnwage)\nqui:logit misswage  educ age agesq female single married\npredict psc, xb\nqui:reg mlnwage  educ age agesq female single married\npredict lnwh, xb\nqui:pca educ age agesq female single married \nqui:predict pc1, score\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n(351 missing values generated)\n\n\nFor imputation, lets do something simple, Use data from the closet observation (with lower score) as donor.\n\nforeach i in psc lnwh pc1 {\n \n    drop2   lnwage_`i'\n    gen lnwage_`i' = mlnwage    \n    sort `i'\n    replace lnwage_`i'=lnwage_`i'[_n-1] if lnwage_`i'==. & lnwage_`i'[_n-1]!=.\n    *replace lnwage_`i'=lnwage_`i'[_n+1] if lnwage_`i'==. & lnwage_`i'[_n+1]!=.\n    qui:_regress lnwage_`i'  educ age agesq female single married\n    matrix b`i' = e(b)\n    matrix coleq b`i'=`i'\n    \n}\n\nvariable lnwage_psc not found\n(351 missing values generated)\n(351 real changes made)\nvariable lnwage_lnwh not found\n(351 missing values generated)\n(351 real changes made)\nvariable lnwage_pc1 not found\n(351 missing values generated)\n(351 real changes made)\n\n\nEstimate models 1000 times, and lets see results"
  },
  {
    "objectID": "rmethods2/session_5.html#comparison-of-methods",
    "href": "rmethods2/session_5.html#comparison-of-methods",
    "title": "Research Methods II",
    "section": "Comparison of methods",
    "text": "Comparison of methods"
  },
  {
    "objectID": "rmethods2/session_3.html#what-is-inequality",
    "href": "rmethods2/session_3.html#what-is-inequality",
    "title": "Research Methods II",
    "section": "What is Inequality?",
    "text": "What is Inequality?\n\nEconomic inequality refers to how economic variables are distributed among individuals in a group, among groups in a population, or among countries.\nInequality of What?\n\ninequality of opportunities, for example access to employment or education\ninequality of outcomes, for example material dimensions of human well-being, such as the level of income, educational attainment, health status and so on.\n\nFor now we will focus on income inequality."
  },
  {
    "objectID": "rmethods2/session_3.html#how-do-you-analyze-measure-inequality",
    "href": "rmethods2/session_3.html#how-do-you-analyze-measure-inequality",
    "title": "Research Methods II",
    "section": "How do you analyze (measure) inequality?",
    "text": "How do you analyze (measure) inequality?\n\nThere are various approaches that have been used for the analysis of Inequality\n\nIntuitive approach\n\nUnaxiomatic approach used to describe inequality.\n\nNormative approach-Social welfare\n\nUses explicit concepts of welfare functions to quantify inequality\n\nInformation theory\n\nQuantifies inequality treating it as a problem of comparing income distribution probabilities.\n\nAxiomatic approach\n\nUses a series of axioms to create measures of inequality"
  },
  {
    "objectID": "rmethods2/session_3.html#preliminaries",
    "href": "rmethods2/session_3.html#preliminaries",
    "title": "Research Methods II",
    "section": "Preliminaries",
    "text": "Preliminaries\n\nRegardless of the approach, there are some basic steps required to measure inequality\n\nDefine the population of interest\nDefine the measure of interest\nAdjust for prices (if necessary)\nAdjust for individual heterogeneity (needs) (if necessary)"
  },
  {
    "objectID": "rmethods2/session_3.html#mathematical-preliminaries",
    "href": "rmethods2/session_3.html#mathematical-preliminaries",
    "title": "Research Methods II",
    "section": "Mathematical Preliminaries",
    "text": "Mathematical Preliminaries\n\nLet \\(y_i\\) be the income of individual \\(i\\) in the population. Assume that \\(y_i&gt;&gt;0\\).\nAssume that \\(y_i\\) can be characterized by a probability distribution function \\(f(y)\\).\n\n\\[\\begin{aligned}\ny_i &\\sim f(y) \\rightarrow \\int_{-\\infty}^{z} f(y) dy = F(z) \\\\\nF(0)&=0 \\ \\& \\ F(\\infty)=1 \\\\\nF(Q_y(p)) &=p \\rightarrow Q_y(p) = F^{-1}(p)\n\\end{aligned}\n\\]\nThe \\(p_{th}\\) quantile of \\(y_i\\) is the value \\(Q_y(p)\\) such that \\(p\\) percent of the population has income below \\(Q_y(p)\\)."
  },
  {
    "objectID": "rmethods2/session_3.html#mathematical-preliminaries-1",
    "href": "rmethods2/session_3.html#mathematical-preliminaries-1",
    "title": "Research Methods II",
    "section": "Mathematical Preliminaries",
    "text": "Mathematical Preliminaries\nMean of Standard of Living:\n\\[\\mu_y = E(y) = \\int_{-\\infty}^{\\infty} y f(y) dy=\\int_0^1Q(p)dp\n\\]\nFinally, the inequality measure can be written as:\n\\[I(y)=I(\\mu_y, f(.)) = I(\\mu_y, F(.))\n\\]"
  },
  {
    "objectID": "rmethods2/session_3.html#visualization-tools",
    "href": "rmethods2/session_3.html#visualization-tools",
    "title": "Research Methods II",
    "section": "Visualization tools",
    "text": "Visualization tools\n\nThere are several tools that can be used to visualize income distribution:\n\nDensity Function/Histogram\nPen parade/Cumulative Distribution Function\nLorenz Curve"
  },
  {
    "objectID": "rmethods2/session_3.html#section",
    "href": "rmethods2/session_3.html#section",
    "title": "Research Methods II",
    "section": "",
    "text": "Density/HistogramPlots\n\n\n\nDensity functions and histograms are used to visualize the distribution of income in the population.\nThey could be used to detect multimodality, skewness, etc\nAnd could be used to compare distributions across groups.\nStata Commands\nhistogram varname [weight] [if]\nkdensity varname [weight] [if]\n\n\n\n\n\nCode\nset scheme white2\nset linesize 255\ncolor_style tableau\nqui:frause oaxaca, clear\nsum wt, meanonly\ngen int wt2 = round(wt/r(min))\nqui:two histogram lnwage [fw=wt2] ///\n    || kdensity lnwage [w=wt2], ///\n    ysize(5) xsize(9) xtitle(\"Log Wages\") ///\n    legend(order(1 \"Histogram\" 2 \"Kernel Density\") pos(6) col(2))"
  },
  {
    "objectID": "rmethods2/session_3.html#section-1",
    "href": "rmethods2/session_3.html#section-1",
    "title": "Research Methods II",
    "section": "",
    "text": "PenParade/CDFplot\n\n\n\nA different approach to visualize the distribution.\nThe Pen Parade plots the values of the variable of interest in ascending order.\n\ny-axis: Q(p); x-axis: p\n\nThe CDF plots the cumulative distribution of the variable of interest.\n\ny-axis: p; x-axis: Q(p)\n\nThey give you a sense of the distribution, and easy comparison across high and low values.\n\n\n\n\nCode\nqui:pctile qlnwage = lnwage [w=wt], nq(100)\nqui:gen  qwage = exp(qlnwage)\nqui:gen p = _n if _n&lt;100\nscatter qwage p, connect(l) name(m1, replace) ysize(5) xsize(8) title(\"Pen Parade\")\nscatter p qwage, connect(l) name(m2, replace) ysize(5) xsize(8) title(\"Cumulative Distribution Function\")"
  },
  {
    "objectID": "rmethods2/session_3.html#section-2",
    "href": "rmethods2/session_3.html#section-2",
    "title": "Research Methods II",
    "section": "",
    "text": "Lorenz CurveCodePlotAlternative:\n\n\n\nPerhaps the most popular tool to visualize income distribution.\nThis curve plots the cumulative share of income vs the cumulative share of population.\n\nHow much of total income is held by the bottom X% of the population ?\n\n“Easy” to read:\n\nThe closest it is to the 45 degree line, the more equal the distribution.\n\nNot easy to implement with negative and zero incomes.\nComparison across groups may be ambiguous.\nits always increasing at an increasing rate respect to X%\n\n\n\nAssume data is sorted by income\n\n\nx-axis: Cum share of population\n\\[P_j = \\frac{\\sum_{i=1}^j w_i}{\\sum_{i=1}^n w_i}\\]\n\ny-axis: Cum share of income\n\\[LC_j = \\frac{\\sum_{i=1}^j y_i w_i}{\\sum_{i=1}^n y_iw_i}\\]\n\n\nfrause oaxaca, clear\ngen wage = exp(lnwage)\nsort wage wt  // sort by income and weight \n// Estimate Totals for non missing data\negen twage = sum(wage * wt) if wage!=.\negen tpop  = sum(wt)        if wage!=.\n// get cumulative shares\ngen lc_i = sum( (wage*wt/twage) )*100 if wage!=.\ngen p_i  = sum( (wt/tpop) )*100      if wage!=.\n\n\n\n\n\nCode\ntwo (line lc_i p_i) /// Lorenz Curve\n    ( function x, range(0 100) ) , /// 45 degree line\n    aspect(1) ysize(5) xsize(8) ///\n    xtitle(\"Cumulative Share of Population\") ///\n    ytitle(\"Cumulative Share of Income\") ///\n    legend(off)\n\n\n\n\n\nLorenz Curve\n\n\n\n\n\n\nssc install glcurve // installs command for Generalized Lorenz Curve\nglcurve wage [aw = wt], /// provides variable and weight\n    lorenz // Request ploting the Lorenz Curve"
  },
  {
    "objectID": "rmethods2/session_3.html#inequality-measures",
    "href": "rmethods2/session_3.html#inequality-measures",
    "title": "Research Methods II",
    "section": "Inequality Measures",
    "text": "Inequality Measures\nThere are several measures of inequality. The most popular are:\n\nInterquantile Range (IQR) (or normalizations) \\[IQR(\\#1,\\#2) = Q(\\#2) - Q(\\#1)\n\\]\nInterquantile Share Ratio (Palma ratio (10/40)) \\[ISR(\\#1,\\#2) = \\frac{1-LC(\\#2)}{LC(\\#1)}\n\\]\nCoefficient of Variation (CV) \\[CV = \\frac{\\sigma_y}{\\mu_y}\n\\]"
  },
  {
    "objectID": "rmethods2/session_3.html#inequality-measures-1",
    "href": "rmethods2/session_3.html#inequality-measures-1",
    "title": "Research Methods II",
    "section": "Inequality Measures",
    "text": "Inequality Measures\n\nLorenz Curve:\n\n\\[LC(p) = \\frac{\\int_0^p Q_y(u)du}{\\int_0^1 Q_y(u)du}\n= \\frac{1}{\\mu_y} \\int_0^p Q_y(u)du\n\\]\n\nProperties 1: Lorenz Curve is a non-decreasing function of \\(p\\).\n\n\\[\\frac{\\partial LC(p)}{\\partial p} = \\frac{Q_y(p)}{\\mu_y} \\geq 0\\]\n\nProperties 2: Lorenz Curve is a concave function of \\(p\\) (increases at a fasterate). \\[\\frac{\\partial^2 LC(p)}{\\partial p^2} = \\frac{1}{\\mu_y f(y)} \\geq 0\\]"
  },
  {
    "objectID": "rmethods2/session_3.html#inequality-measures-gini-coefficient",
    "href": "rmethods2/session_3.html#inequality-measures-gini-coefficient",
    "title": "Research Methods II",
    "section": "Inequality Measures: Gini Coefficient",
    "text": "Inequality Measures: Gini Coefficient\n\nThe Gini coefficient is the most popular measure of inequality.\nIt is defined as (2x) the area between the Lorenz Curve and the 45 degree line.\n\n\\[Gini(y) = 2 \\int_0^1 (p-LC(p)) dp\\]\n\nwhere \\(p-LC(p)\\) is the “loss” of income the Bottom \\(p\\) percent of the population experiences.\nIt is bounded between 0 (perfect Equality) and 1 (complete Inequality).\nWhen Lorenz do not cross, Gini provides unambiguous ranking of inequality.\n\n\\[Gini(y) = \\frac{2}{\\mu_y} Cov(y_p,p)\\]"
  },
  {
    "objectID": "rmethods2/session_3.html#implementation",
    "href": "rmethods2/session_3.html#implementation",
    "title": "Research Methods II",
    "section": "Implementation",
    "text": "Implementation\n\nStata has plenty of commands that can be used to estimate Gini\n\nsearch gini for few examples\n\nI suggest 3 commands:\n\nfastgini (ssc install fastgini)\nineqdeco (ssc install ineqdeco)\nsgini (ssc install sgini)\nrif (ssc install rif)"
  },
  {
    "objectID": "rmethods2/session_3.html#section-3",
    "href": "rmethods2/session_3.html#section-3",
    "title": "Research Methods II",
    "section": "",
    "text": "capture:ssc install sgini\nsgini wage \n\n\nGini coefficient for wage\n\n-----------------------\n    Variable |      v=2\n-------------+---------\n        wage |   0.2460\n-----------------------\n\n\n\ncapture:ssc install ineqdeco\nineqdeco wage [pw=wt]\n\n \nPercentile ratios\n\n----------------------------------------------------------\n  All obs |    p90/p10     p90/p50     p10/p50     p75/p25\n----------+-----------------------------------------------\n          |      3.154       1.694       0.537       1.771\n----------------------------------------------------------\n  \nGeneralized Entropy indices GE(a), where a = income difference\n sensitivity parameter, and Gini coefficient\n\n----------------------------------------------------------------------\n  All obs |     GE(-1)       GE(0)       GE(1)       GE(2)        Gini\n----------+-----------------------------------------------------------\n          |    0.23199     0.14240     0.12282     0.13398     0.26273\n----------------------------------------------------------------------\n   \nAtkinson indices, A(e), where e &gt; 0 is the inequality aversion parameter\n\n----------------------------------------------\n  All obs |     A(0.5)        A(1)        A(2)\n----------+-----------------------------------\n          |    0.06292     0.13273     0.31693\n----------------------------------------------"
  },
  {
    "objectID": "rmethods2/session_3.html#other-inequality-measures",
    "href": "rmethods2/session_3.html#other-inequality-measures",
    "title": "Research Methods II",
    "section": "Other Inequality Measures",
    "text": "Other Inequality Measures\n\nThere are other approaches that can be used to measure inequality.\n\nNormative approach-Social welfare: Uses explicit concepts of welfare functions to quantify inequality\n\n\n\\[I_A(y,\\varepsilon) = 1 - \\left( \\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{y_i}{\\mu_y}\\right)^{1-\\varepsilon} \\right)^\\frac{1}{1-\\varepsilon}\n\\]\nwhere is a measure of inequality aversion.\n\nInformation theory: Quantifies inequality treating it as a problem of comparing income distribution probabilities. How far are we from Full Entropy\n\n\\[I_{GE}(Y,\\alpha)=\\frac{1}{\\alpha(1-\\alpha)}\\left[\\frac{1}{N} \\sum \\left(\\frac{y_i}{\\mu_y}\\right)^\\alpha -1\\right]\n\\]\n\nAxiomatic approach\n\nUses a series of axioms to create measures of inequality"
  },
  {
    "objectID": "rmethods2/session_3.html#significance-test",
    "href": "rmethods2/session_3.html#significance-test",
    "title": "Research Methods II",
    "section": "Significance test",
    "text": "Significance test\n\nAs discussed in Session 1, we can use a t-test to compare means.\n\nThis requires estimating the standard error of the mean, use mean command, or regress\n\nSimilarly, it may be as important to test whether two distributions (or inequality measures) are different.\n\nThis requires estimating the standard error of the inequality measure.\nThis is not as straightforward as the mean.\n\nEasiest methods:\n\nBootstrap: requires bootstrap weights for survey data.\nInfluence function: requires deriving the influence function of the inequality measure."
  },
  {
    "objectID": "rmethods2/session_3.html#bootstrap",
    "href": "rmethods2/session_3.html#bootstrap",
    "title": "Research Methods II",
    "section": "Bootstrap",
    "text": "Bootstrap\n\nBootstrap its a non-parametric method to estimate the standard error of a statistic. Its based on Resampling and re-estimating data.\n\nbootstrap gini=r(coeff): sgini wage\n\n\n\nwarning: sgini does not set e(sample), so no observations will be excluded from the resampling because of missing values or other reasons. To exclude observations, press Break, save the data, drop any observations that are to be excluded, and rerun\n         bootstrap.\n\nBootstrap results                                        Number of obs = 1,647\n                                                         Replications  =    50\n\n      Command: sgini wage\n         gini: r(coeff)\n\n------------------------------------------------------------------------------\n             |   Observed   Bootstrap                         Normal-based\n             | coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        gini |   .2460329   .0063639    38.66   0.000     .2335599    .2585059\n------------------------------------------------------------------------------\n\n\n\nRIF (Recentered Influence Function) is a method that uses the moment conditions to estimate the standard error of a statistic.\n\nrifhdreg wage , rif(gini)\n\n\n\nLinear regression                               Number of obs     =      1,434\n                                                F(0, 1433)        =       0.00\n                                                Prob &gt; F          =          .\n                                                R-squared         =     0.0000\n                                                Root MSE          =     .24613\n\n------------------------------------------------------------------------------\n             |               Robust\n        wage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _cons |   .2460329   .0064995    37.85   0.000     .2332833    .2587825\n------------------------------------------------------------------------------\nDistributional Statistic: gini\nSample Mean    RIF gini :  .24603\n\n\n\nBetter yet, because you can use regressions (RIF-regressions), you can use weights, and test for differences in inequality across groups.\n\nrifhdreg wage ibn.female [pw=wt], rif(gini) over(female) noconstant\n\n\n\nLinear regression                               Number of obs     =      1,434\n                                                F(2, 1432)        =     640.56\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.5056\n                                                Root MSE          =     .25631\n\n------------------------------------------------------------------------------\n             |               Robust\n        wage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |\n          0  |   .2377884   .0084103    28.27   0.000     .2212907    .2542862\n          1  |   .2820059   .0128488    21.95   0.000     .2568015    .3072103\n------------------------------------------------------------------------------\nDistributional Statistic: gini\nSample Mean    RIF gini :  .25805\n\n\nrifhdreg wage i.female [pw=wt], rif(gini) over(female)\n\n\n\nLinear regression                               Number of obs     =      1,434\n                                                F(1, 1432)        =       8.29\n                                                Prob &gt; F          =     0.0040\n                                                R-squared         =     0.0073\n                                                Root MSE          =     .25631\n\n------------------------------------------------------------------------------\n             |               Robust\n        wage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    1.female |   .0442175   .0153565     2.88   0.004     .0140938    .0743412\n       _cons |   .2377884   .0084103    28.27   0.000     .2212907    .2542862\n------------------------------------------------------------------------------\nDistributional Statistic: gini\nSample Mean    RIF gini :  .25805"
  },
  {
    "objectID": "rmethods2/session_3.html#introduction",
    "href": "rmethods2/session_3.html#introduction",
    "title": "Research Methods II",
    "section": "Introduction",
    "text": "Introduction\n\nSome times, we may be interested in determining what factors, and what extent, explain inequality.\n\nBut what do we mean by “explain”?\n\nThere are several approaches to decompose inequality.\n\nby sources: Explain how inequality is related to source of income\nby groups: How much of the inequality is explained by inequality within groups, and between groups.\n\nWe could also consider how inequality gaps are related to characteristics or returns to such characteristics."
  },
  {
    "objectID": "rmethods2/session_3.html#decompose-by-sources",
    "href": "rmethods2/session_3.html#decompose-by-sources",
    "title": "Research Methods II",
    "section": "Decompose by sources",
    "text": "Decompose by sources\n\nSome inequality indices are well suited to decompose inequality by sources.\n\nVariance Decomposition (Shorrocks 1982)\nGini Decomposition by source (Lerman and Yitzhaki 1985)\n\nThe idea is assess inequality (or concentration) of each source of income, and then combine them to obtain their contribution to overall inequality."
  },
  {
    "objectID": "rmethods2/session_3.html#section-4",
    "href": "rmethods2/session_3.html#section-4",
    "title": "Research Methods II",
    "section": "",
    "text": "Variance Decomposition\nSetup: \\[\\begin{aligned}\nY &= y_1 + y_2 + \\cdots + y_n  \\\\\nI_V(Y) &= V(Y) = Cov(Y,Y) \\\\\n&= Cov(Y,y_1) + Cov(Y,y_2) + \\cdots \\\\\n\\end{aligned}\n\\]\nCovariance, however could be rewritten as: \\(Cov(Y,y_k)=\\rho_k \\sigma_Y \\sigma_{y_k}\\), thus\n\\[\\begin{aligned}\nV(y) = \\sigma^2_Y &= \\rho_1 \\sigma_Y \\sigma_{y_1} + \\rho_2 \\sigma_Y \\sigma_{y_2} + \\cdots \\\\\n\\sigma_Y&=   \\rho_1 \\sigma_{y_1} + \\rho_2 \\sigma_{y_2} + \\cdots  \\\\\n\\end{aligned}\n\\]\nFinally, if we divide all by \\(\\mu_y\\) and multiply by \\(\\mu_{yk}/\\mu_{yk}\\), we get:\n\\[\\begin{aligned}\n\\frac{\\sigma_Y}{\\mu_y} = CV(y) &=   \\rho_1 \\frac{1}{\\mu_{y}} \\frac{\\mu_{y1}}{\\mu_{y1}}\\sigma_{y_1} + \\rho_2 \\frac{1}{\\mu_{y}} \\frac{\\mu_{y2}}{\\mu_{y2}}  \\sigma_{y_2} + \\cdots  \\\\\n&= \\rho_1 sh_1 CV(y_1) + \\rho_2 sh_2 CV(y_2) + \\cdots  \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_3.html#example-stata",
    "href": "rmethods2/session_3.html#example-stata",
    "title": "Research Methods II",
    "section": "Example Stata",
    "text": "Example Stata\nssc install ineqfac\n\nfrause limew1972.dta, clear\ncapture: ssc install ineqfac\nineqfac basemi inchomewealth incnonhomewealth govtr_n pubcon tx_n valhp [aw=hhwgt]\n\n \nInequality decomposition by factor components (Shorrocks' method)\n-------------------------------------------------------------------------------\nFactor              |  100*s_f     S_f   100*m_f/m  rho_f   CV_f CV_f/CV(Total)\n--------------------+----------------------------------------------------------\nbasemi              |   29.115    0.284   44.434   0.711   0.900    0.922\ninchomewealth       |    1.872    0.018    4.302   0.318   1.335    1.367\nincnonhomewealth    |   43.136    0.421    7.905   0.754   7.065    7.235\ngovtr_n             |   -0.979   -0.010    5.868  -0.107   1.528    1.565\npubcon              |    2.091    0.020    8.303   0.238   1.035    1.060\ntx_n                |   15.629    0.153   11.462   0.754   1.767    1.809\nvalhp               |    9.136    0.089   17.726   0.502   1.003    1.027\n--------------------+----------------------------------------------------------\nTotal               |  100.000    0.977  100.000   1.000   0.977    1.000\n-------------------------------------------------------------------------------\nNote: s_f is the proportionate contribution of factor f\n to inequality of Total, where ...\n s_f = rho_f*sd(f)/sd(Total) \n     = rho_f*[m(factor_f)/m(totvar)]*[CV(factor_f)/CV(totvar)].\n S_f = s_f*CV(Total). rho_f = corr(f,Total). \n m_f = mean(f). sd(f) = std.dev. of f. CV_f = sd(f)/m_f."
  },
  {
    "objectID": "rmethods2/session_3.html#section-5",
    "href": "rmethods2/session_3.html#section-5",
    "title": "Research Methods II",
    "section": "",
    "text": "Gini Decomposition by source\n\nThe Gini index could also be decomposed by source of income.\n\n\\[\\begin{aligned}\nGini(y) &= \\frac{2}{\\mu_y} Cov(y,F(y)) = \\frac{2}{\\mu_y} Cov\\left(\\sum y_k,F(y)\\right) =\\frac{2}{\\mu_y} \\sum Cov\\left( y_k,F(y)\\right)\n\\end{aligned}\n\\]\n\nSo Gini is the sum of the covariance of each source of income with \\(F(y)\\)\n\n\\[\\begin{aligned}\nGini(y) &= \\sum \\frac{2}{\\mu_y} \\times Cov( y_k,F(y)) \\times \\frac{\\mu_{yk}}{\\mu_{yk}} \\times \\frac{Cov( y_k,F(y_k))}{Cov ( y_k,F(y_k))} \\\\\n&=\\sum \\frac{Cov ( y_k,F(y) )}{Cov ( y_k,F(y_k) )} \\times \\frac{2Cov( y_k,F(y_k)}{\\mu_{yk}} \\times \\frac{\\mu_{yk}}{\\mu_y}  \\\\\n&= \\sum R_k \\times G_k \\times sh_k\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_3.html#section-6",
    "href": "rmethods2/session_3.html#section-6",
    "title": "Research Methods II",
    "section": "",
    "text": "Gini Decomposition by source\n\\[\\begin{aligned}\nGini(y)&= \\sum R_k \\times G_k \\times sh_k\n\\end{aligned}\n\\]\n\n\\(R_k\\) = Gini Correlation; \\(G_k\\) = Gini of source \\(k\\); \\(sh_k\\) = share of source \\(k\\) in total income.\n\\(R_k\\time G_k\\) is the Concentration Index of \\(Y_k\\)"
  },
  {
    "objectID": "rmethods2/session_3.html#example-stata-1",
    "href": "rmethods2/session_3.html#example-stata-1",
    "title": "Research Methods II",
    "section": "Example Stata",
    "text": "Example Stata\nYou probably already have it installed\nssc install sgini\n\nsgini basemi inchomewealth incnonhomewealth govtr_n pubcon tx_n valhp [aw=hhwgt], source\n\n\nGini coefficient for basemi, inchomewealth, incnonhomewealth, govtr_n, pubcon, tx_n, valhp\n\nNote: inchomewealth has 690 negative observations (used in calculations).\nNote: incnonhomewealth has 10359 negative observations (used in calculations).\n-----------------------\n    Variable |      v=2\n-------------+---------\n      basemi |   0.4787\ninchomewea~h |   0.6161\nincnonhome~h |   0.9503\n     govtr_n |   0.7099\n      pubcon |   0.4913\n        tx_n |   0.6036\n       valhp |   0.5021\n-----------------------\n\nDecomposition by source:\n  TOTAL =  basemi +  inchomewealth +  incnonhomewealth +  govtr_n +  pubcon +  tx_n +  valhp\n\n\nParameter: v=2\n--------------------------------------------------------------------------------\n             |    Share   Coeff.    Corr.    Conc.  Contri.  %Contri. Elasticity\n    Variable |        s        g        r    c=g*r    s*g*r   s*g*r/G  s*g*r/G-s\n-------------+------------------------------------------------------------------\n      basemi |   0.4443   0.4787   0.8870   0.4246   0.1887   0.4868    0.0425\ninchomewea~h |   0.0430   0.6161   0.4617   0.2845   0.0122   0.0316   -0.0114\nincnonhome~h |   0.0790   0.9503   0.7724   0.7340   0.0580   0.1497    0.0707\n     govtr_n |   0.0587   0.7099  -0.2298  -0.1631  -0.0096  -0.0247   -0.0834\n      pubcon |   0.0830   0.4913   0.4593   0.2257   0.0187   0.0483   -0.0347\n        tx_n |   0.1146   0.6036   0.8776   0.5297   0.0607   0.1567    0.0420\n       valhp |   0.1773   0.5021   0.6601   0.3314   0.0587   0.1516   -0.0257\n-------------+------------------------------------------------------------------\n       TOTAL |   1.0000   0.3876   1.0000   0.3876   0.3876   1.0000    0.0000\n--------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods2/session_3.html#decompose-by-groups",
    "href": "rmethods2/session_3.html#decompose-by-groups",
    "title": "Research Methods II",
    "section": "Decompose by groups",
    "text": "Decompose by groups\n\nThere is a second type of decomposition one may be interested in.\n\nHow much of the inequality is explained by inequality within groups, and between groups.\n\nFor example, consider two cases:\n\nTwo (eq size) groups that have access to the same level of income, but within each group, all resources are held by one individual.\nTwo (eq size) groups, one has 80% of the income, and the other 20%, but within groups income is equally distributed.\n\nIt is possible to understand the source of inequality by decomposing it by groups.\n\nEntropy Indices (and Atkinson) are well suited for this type of decomposition. (see help ineqdeco)\nGINI is not as straight forward but possible."
  },
  {
    "objectID": "rmethods2/session_3.html#section-7",
    "href": "rmethods2/session_3.html#section-7",
    "title": "Research Methods II",
    "section": "",
    "text": "GINI Decomposition by groups\n\nDecomposition of the GINI coefficient by groups Milanovic and Yitzhaki (2002)\nThe method: To decompose the GINI by groups, one can use the following:\n\n\\[\\begin{aligned}\nGini(y) &= \\sum_{k=1}^K s_k O_k Gini(y_k) + Gini_{bw}\n\\end{aligned}\n\\]\n\nwhere \\(s_i\\) is the share of group \\(i\\) in total income, \\(Gini(y_k)\\) is the Gini of group \\(k\\), \\(O_k\\) is a measure of overlapping across groups, and \\(Gini_{bw}\\) is the Gini between groups.\n\n\\[Gini_{bw} = \\frac{2}{\\mu_y} Cov(\\mu_i,\\bar F_i)\\]"
  },
  {
    "objectID": "rmethods2/session_3.html#section-8",
    "href": "rmethods2/session_3.html#section-8",
    "title": "Research Methods II",
    "section": "",
    "text": "Overlapping\n\nOverlapping \\(O_k\\) measures to what extend the distribution of income in group \\(k\\) overlaps with the distribution of income in other groups.\nIf there is no overlapping, then \\(O_k=p_i\\) (the population share of group k, and incomes are fully stratified).\nOtherwise, this adjustment factors ensures that the sum of the Gini of each group is equal to the Gini of the total population + Between Gini."
  },
  {
    "objectID": "rmethods2/session_3.html#example-stata-2",
    "href": "rmethods2/session_3.html#example-stata-2",
    "title": "Research Methods II",
    "section": "Example Stata",
    "text": "Example Stata\nssc install anogi (Tom Masterson is one of the authors)\nssc install moremata (needed for anogi)\n\ncapture:ssc install anogi\ncapture:ssc install moremata\nanogi limew [aw= hhwgt ], by(educl) detail\n\n\nAnalysis of Gini\n\n--------------------------------------------------\n                          |      Coef.          %\n--------------------------+-----------------------\nOverall Gini              |   .3875623     100.00\n                          |\nG_wo = sum s_i*G_i*O_i    |   .3405469      87.87\nG_b                       |   .0470154      12.13\n                          |\nIG   = sum s_i*G_i        |   .3657639      94.38\nIGO  = sum s_i*G_i(O_i-1) |   -.025217      -6.51\nBGp  = G_bp               |   .1317698      34.00\nBGO  = G_b - G_bp         |  -.0847544     -21.87\n--------------------------+-----------------------\nMean of limew             |    20403.6\nN. of obs                 |      44872\nN. of subgroups           |          4\n--------------------------------------------------\n\n\nDetailed statistics for subgroups\n\n-------------------------------------------------------------------------------------------\n             |         N          p       mean          s          G          O          F \n-------------+-----------------------------------------------------------------------------\n           1 |  18244.58   .4065916   15466.07   .3081992   .3844549   .9986868   .3963075 \n           2 |  14552.19   .3243045   21019.76   .3340981    .338758   .9239628   .5363091 \n           3 |  5609.305   .1250068   22705.72   .1391112   .3640736   .9435821   .5546081 \n           4 |  6465.924   .1440971   30951.73   .2185916   .3817625   .8370495   .6634934 \n-------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods2/session_1.html#why-are-we-here",
    "href": "rmethods2/session_1.html#why-are-we-here",
    "title": "Research Methods II",
    "section": "Why are we here?",
    "text": "Why are we here?\n\nIf you are reading this, you are probably interested in the Microeconometrics path of Research methods II.\nThis course assumes you are familiar with the basics of econometrics and statistics.\n\nWe will not cover questions from Research methods I. You probably know the answers to those questions already!\n\nThis course will focus on tools that are commonly used in empirical research in economics:\n\nWe will emphasize the use of Household Surveys (with the issues it entails)\nAnd focus on applied econometrics using cross-sectional data, for distributional analysis and policy simulation.\n\nThus, we will do much more use of empirical tools and software than we did in Research methods I.\nWe will have 7 Sessions, and 2 homeworks ."
  },
  {
    "objectID": "rmethods2/session_1.html#what-is-a-survey",
    "href": "rmethods2/session_1.html#what-is-a-survey",
    "title": "Research Methods II",
    "section": "What is a Survey?",
    "text": "What is a Survey?\n\nA Survey is a source of data that aims to collect information from a population of interest, to underestand some characteristics, behaviors, or opinions of that population as a whole.\n\nThe population of interest can be individuals, households, firms, etc.\n\nThey can be useful to identify and analyze policy questions.\nHowever, they are secondary data, and thus have limitations in terms of the questions that can be answered with them.\n\nYou cannot answer questions that require data that was not collected.\nThey can also be limited to Interviewee recall, or willingness to answer.\nOr how accessible the population of interest is."
  },
  {
    "objectID": "rmethods2/session_1.html#example-of-surveys",
    "href": "rmethods2/session_1.html#example-of-surveys",
    "title": "Research Methods II",
    "section": "Example of Surveys",
    "text": "Example of Surveys\n\nCurrent Population Survey (CPS)\n\nMonthly survey of 60,000 households in the US.\n\nAmerican Community Survey (ACS)\n\nAnnual survey of 3.5 million households in the US.\n\nAmerican Time Use Survey (ATUS)\n\nAnnual survey of 6,000 individuals in the US.\n\nEnterprise Surveys - WB (ES)\n\nSurvey of firms in developing countries. Different years of Collection"
  },
  {
    "objectID": "rmethods2/session_1.html#what-makes-a-good-survey",
    "href": "rmethods2/session_1.html#what-makes-a-good-survey",
    "title": "Research Methods II",
    "section": "What makes a good survey?",
    "text": "What makes a good survey?\n\nA Good survey is one that allows you to obtain estimates of statistics of interest for the population with “Tolerable” levels of Accuracy.\nTo do this, you need to have a good sampling design (representation and “independence of the population”) and a good questionnaire design (questions that are clear and easy to answer).\nA good survey needs to be representative of the population of interest. To do this appropriately, data will be collected based on a frame that will be used to select the sample."
  },
  {
    "objectID": "rmethods2/session_1.html#types-of-data-seletion",
    "href": "rmethods2/session_1.html#types-of-data-seletion",
    "title": "Research Methods II",
    "section": "Types of Data Seletion",
    "text": "Types of Data Seletion\n\nSimpleClusteredStratified\n\n\n\nEach observation in the “frame” has the same probability of being selected in the sample.\nIt may be difficult to implement in practice, because of cost and logistics. (distance)\nIt could also have problems of representativeness for small groups. (rare events)\n\n\n\n\nUsing some criteria, location for example, the population is divided into clusters.\nFor the sample selection, certain clusters are selected at random, and “some” observations within each selected.\nThis is more feasible in practice, because takes advantage of the “clustering” of the population.\nHowever, one may need to account for possible “common shocks” that people within the same cluster may have.\n\n\n\n\nSome times, statistics are required to accurately represent certain groups of the population. (by region, race, income level, etc)\nIn such cases, data can be collected in a way that ensures that the sample has enough observations for each group of interest.\n\nIt would be as collecting multiple samples, one for each group of interest.\n\nWithin each Strata, it would also be possible to use a simple or clustered sampling design."
  },
  {
    "objectID": "rmethods2/session_1.html#what-to-be-aware-of",
    "href": "rmethods2/session_1.html#what-to-be-aware-of",
    "title": "Research Methods II",
    "section": "What to be aware of?",
    "text": "What to be aware of?\n\nThe sampling design is important to ensure that the sample is representative of the population of interest.\nHowever, there are limitations:\n\nNot every-one selected will respond to the survey. (Is it random? )\nRarely one assumes equal probability of selection. (Different sampling weights)\nUse of Stratatification and Clustering may require special treatment.\n\nSomething else: Panel data\n\nBecause of attrition, it can be difficult to analyze if representativeity is required.\nHowever, it can be useful to analyze dynamics."
  },
  {
    "objectID": "rmethods2/session_1.html#descriptive-statistics",
    "href": "rmethods2/session_1.html#descriptive-statistics",
    "title": "Research Methods II",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\nOnce you have your data, you can start analyzing it by simply applying your survey weights.\n\nPoint estimates are straightforward to obtain.\n\nHowever, when considering the estimation of precision of the estimates (Variance and Standard Errors), there are two approaches that are important to consider:\nFinite Population Approach:\n\nAssociated with data description\nAssumes that the population is finite, and thus Selection probabilities are not independent.\n\nSuperpopulation Approach:\n\nAssociated with data modeling.\nPopulation is infinite. Selection probabilties are independent.\n\nFor practical purposes, the difference between the two approaches is not that important.\n\nWith large enough samples, he “finite sample correction” is negligible."
  },
  {
    "objectID": "rmethods2/session_1.html#basic-summary-statistics",
    "href": "rmethods2/session_1.html#basic-summary-statistics",
    "title": "Research Methods II",
    "section": "Basic Summary Statistics:",
    "text": "Basic Summary Statistics:\n\n\\(n\\) is sample size. \\(N\\) is population size.\n\\(a_i\\) an indicator of belonging to the sample. \\(\\sum a_i=n\\)\nAssume \\(y_i\\) is the outcome of interest, Say income. \\[\\texttt{mean: }\\hat{\\mu}_y=\\bar{y} = \\frac{1}{n}\\sum_{i=1}^N a_i y_i = \\frac{1}{n}\\sum_{j=1}^n y_j\\] \\[\\texttt{Variance: }\\hat \\sigma^2_y = \\frac{1}{n-1} \\sum_{i=1}^N a_i (y_i - \\hat{\\bar{y}} )^2\\] \\[\\texttt{V of mean: }\\widehat{V}(\\bar{y}) = \\frac{\\sigma^2_y }{n} * fpc\\]\n\nWhere \\(fpc = \\frac{N-n}{N-1}\\) is the finite population correction."
  },
  {
    "objectID": "rmethods2/session_1.html#accounting-for-weights",
    "href": "rmethods2/session_1.html#accounting-for-weights",
    "title": "Research Methods II",
    "section": "Accounting for Weights",
    "text": "Accounting for Weights\n\nThe previous formulas did not account for weights.\nWeights are factors used to “reweight/expand” the sample to make it representative of the population.\nThey can are typically related to the inverse of the probability of selection.\nSimply said, a weight \\(w_i=\\frac{1}{n*\\pi_i}\\) is a measure of how many observations in the population are represented by observation \\(i\\) in the sample.\n\nBut how do we account for weights in the formulas?"
  },
  {
    "objectID": "rmethods2/session_1.html#summary-statistics-with-weights",
    "href": "rmethods2/session_1.html#summary-statistics-with-weights",
    "title": "Research Methods II",
    "section": "Summary Statistics with Weights",
    "text": "Summary Statistics with Weights\nPopulation: \\[\\hat N = \\sum_{i=1}^n w_i\\]\nNormalized weights \\[v_i = \\frac{n w_i}{ \\sum w_i} \\rightarrow E(v_i) = 1\\]\nMean: \\[\\hat{\\mu}_y=\\bar{y} = \\frac{1}{\\hat N}\\sum_{i=1}^n w_i y_i = \\frac{1}{n}\\sum v_i y_i\\]"
  },
  {
    "objectID": "rmethods2/session_1.html#variance-with-weights",
    "href": "rmethods2/session_1.html#variance-with-weights",
    "title": "Research Methods II",
    "section": "Variance with Weights",
    "text": "Variance with Weights\nVariances are a bit more complicated. Normally you would consider:\n\\[Var(\\bar y) = \\frac{1}{n} \\hat \\sigma^2_y =\\frac{1}{n} \\frac{1}{n-1} \\sum_{i=1}^n v_i (y_i - \\bar y)^2\\]\nHowever, with survey weights you need to consider something else: \\[Var(\\bar y) = \\frac{1}{n} \\sum_{i=1}^n v_i^2 (y_i - \\bar y)^2\\]\nWhich is similar to “robust” Standard errors in OLS.\n\nWhat about Clusters and Strata?"
  },
  {
    "objectID": "rmethods2/session_1.html#how-to-account-for-weights-in-stata",
    "href": "rmethods2/session_1.html#how-to-account-for-weights-in-stata",
    "title": "Research Methods II",
    "section": "How to account for weights in Stata?",
    "text": "How to account for weights in Stata?\nLets use data two Examples.\n\nFirst Labor Force Survey: Oaxaca\nSecond National Health and Nutrition Examination Survey (NHANES)\n\n\n frause oaxaca, clear\n** Create Weights &lt;- This will be provided\nsum wt, meanonly\nreplace wt = round(wt/r(min))\ngen wage = exp(lnwage)\ntab wt\n\n\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(1,647 real changes made)\n(213 missing values generated)\n\n   sampling |\n    weights |      Freq.     Percent        Cum.\n------------+-----------------------------------\n          1 |        489       29.69       29.69\n          2 |        924       56.10       85.79\n          3 |        160        9.71       95.51\n          4 |         64        3.89       99.39\n          5 |          8        0.49       99.88\n          6 |          2        0.12      100.00\n------------+-----------------------------------\n      Total |      1,647      100.00\n\n\nWeights Distribution\n\ntab wt\n\n\n   sampling |\n    weights |      Freq.     Percent        Cum.\n------------+-----------------------------------\n          1 |        489       29.69       29.69\n          2 |        924       56.10       85.79\n          3 |        160        9.71       95.51\n          4 |         64        3.89       99.39\n          5 |          8        0.49       99.88\n          6 |          2        0.12      100.00\n------------+-----------------------------------\n      Total |      1,647      100.00\n\n\nSummary Statistics:\n\n** unweighted\nsum wage,d\n** weighted\nsum wage [aw=wt],d\n\n\n                            wage\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%     3.907204       1.661434\n 5%      11.8007       1.873127\n10%      17.4216       2.197802       Obs               1,434\n25%     23.19902       2.442003       Sum of wgt.       1,434\n\n50%     30.08896                      Mean           32.39167\n                        Largest       Std. dev.      16.12498\n75%      38.5662       137.3627\n90%     49.95005       152.6252       Variance       260.0151\n95%     58.71271       164.8352       Skewness       2.486954\n99%     85.47008       192.3077       Kurtosis       18.23702\n\n                            wage\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%     3.453689       1.661434\n 5%     7.370678       1.873127\n10%     15.83933       2.197802       Obs               1,434\n25%     21.72247       2.442003       Sum of wgt.       2,686\n\n50%     29.48271                      Mean            31.5322\n                        Largest       Std. dev.      16.32811\n75%     38.46154       137.3627\n90%     49.95005       152.6252       Variance       266.6071\n95%     58.11497       164.8352       Skewness       2.081806\n99%     85.47008       192.3077       Kurtosis       14.68647\n\n\nAccounting for weights for summary Statistics\n\n** unweighted\nmean wage \n** weighted\nmean wage [pw=wt]\nmean wage [pw=wt], over(female)\n\n\nMean estimation                          Number of obs = 1,434\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n        wage |   32.39167   .4258186      31.55637    33.22696\n--------------------------------------------------------------\n\nMean estimation                          Number of obs = 1,434\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n        wage |    31.5322   .4765835      30.59733    32.46708\n--------------------------------------------------------------\n\nMean estimation                           Number of obs = 1,434\n\n---------------------------------------------------------------\n              |       Mean   Std. err.     [95% conf. interval]\n--------------+------------------------------------------------\nc.wage@female |\n           0  |   33.87649   .6152059      32.66969    35.08329\n           1  |   28.76133    .722173       27.3447    30.17796\n---------------------------------------------------------------\n\n\nTables and cross tables\n\n** unweighted\ntab educ female\n\ntab educ female [w=wt]\n\n\n           |   sex of respondent\n  years of |      (1=female)\n education |         0          1 |     Total\n-----------+----------------------+----------\n         5 |         6         23 |        29 \n         9 |        47         93 |       140 \n      9.75 |         8         26 |        34 \n        10 |        10         42 |        52 \n      10.5 |       376        447 |       823 \n      11.5 |         7         14 |        21 \n        12 |       111         87 |       198 \n      12.5 |        56         80 |       136 \n        15 |        60         13 |        73 \n      17.5 |        78         63 |       141 \n-----------+----------------------+----------\n     Total |       759        888 |     1,647 \n\n           |   sex of respondent\n  years of |      (1=female)\n education |         0          1 |     Total\n-----------+----------------------+----------\n         5 |        17         45 |        62 \n         9 |       104        181 |       285 \n      9.75 |        14         52 |        66 \n        10 |        22         80 |       102 \n      10.5 |       725        833 |     1,558 \n      11.5 |        19         27 |        46 \n        12 |       201        148 |       349 \n      12.5 |       108        157 |       265 \n        15 |       111         21 |       132 \n      17.5 |       149        111 |       260 \n-----------+----------------------+----------\n     Total |     1,470      1,655 |     3,125 \n\n\nBetter approach: svyset\n\nwebuse nhanes2f, clear\nsvyset psuid /// Cluster\n       [pweight=finalwgt], /// Survey Weight as Inverse of Prob of Selection\n       strata(stratid)     // Strata Identifier\nmean zinc      \nmean zinc [pw=finalwgt]\nsvy: mean zinc      \n\n\nSampling weights: finalwgt\n             VCE: linearized\n     Single unit: missing\n        Strata 1: stratid\n Sampling unit 1: psuid\n           FPC 1: &lt;zero&gt;\n\nMean estimation                          Number of obs = 9,189\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n        zinc |   86.51518   .1510744      86.21904    86.81132\n--------------------------------------------------------------\n\nMean estimation                          Number of obs = 9,189\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n        zinc |   87.18207   .1828747      86.82359    87.54054\n--------------------------------------------------------------\n(running mean on estimation sample)\n\nSurvey: Mean estimation\n\nNumber of strata = 31            Number of obs   =       9,189\nNumber of PSUs   = 62            Population size = 104,176,071\n                                 Design df       =          31\n\n--------------------------------------------------------------\n             |             Linearized\n             |       Mean   std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n        zinc |   87.18207   .4944827      86.17356    88.19057\n--------------------------------------------------------------"
  },
  {
    "objectID": "rmethods2/session_1.html#testing-significance-across-2-groups",
    "href": "rmethods2/session_1.html#testing-significance-across-2-groups",
    "title": "Research Methods II",
    "section": "Testing Significance across 2 groups",
    "text": "Testing Significance across 2 groups\nConsider two groups with the following characteristics:\n\n\n\nGroup\nN\nmean\nVar\n\n\n\n\n1\n100\n45\n32.56\n\n\n2\n150\n55\n21.97\n\n\n\n\nIs the difference in means statistically significant?\nTest \\(H_0: \\mu_1 = \\mu_2\\) vs \\(H_1: \\mu_1 \\neq \\mu_2\\)\n\n\\[t = \\frac{\\mu_2 - \\mu_1}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n=\\frac{10}{\\sqrt{\\frac{32.56}{100}+\\frac{21.97}{150}}}=\\frac{10}{\\sqrt{.47207}} = 14.55453\n\\]\n\nIf \\(t\\) is large enough, we can reject the null hypothesis.\nBut this does not work if you have weights…"
  },
  {
    "objectID": "rmethods2/session_1.html#testing-significance-across-2-groups-1",
    "href": "rmethods2/session_1.html#testing-significance-across-2-groups-1",
    "title": "Research Methods II",
    "section": "Testing Significance across 2 groups",
    "text": "Testing Significance across 2 groups\n\nBut you can use OLS to test the difference in means with weights!\n\nMake sure you use “robust” standard errors, or “pw” option.\n\n\n\n frause oaxaca, clear\n** Create Weights &lt;- This will be provided\nsum wt, meanonly\nreplace wt = round(wt/r(min))\ngen wage = exp(lnwage)\nreg wage i.female [pw=wt],\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(1,647 real changes made)\n(213 missing values generated)\n(sum of wgt is 2,686)\n\nLinear regression                               Number of obs     =      1,434\n                                                F(1, 1432)        =      29.05\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.0244\n                                                Root MSE          =     16.133\n\n------------------------------------------------------------------------------\n             |               Robust\n        wage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    1.female |  -5.115156   .9490209    -5.39   0.000    -6.976776   -3.253536\n       _cons |   33.87649   .6154206    55.05   0.000     32.66927    35.08371\n------------------------------------------------------------------------------\n\n\n\nyou can also use svy: regress for complex designs.\n\nsee here for additional examples"
  },
  {
    "objectID": "rmethods2/session_1.html#io-tables-1",
    "href": "rmethods2/session_1.html#io-tables-1",
    "title": "Research Methods II",
    "section": "IO-Tables",
    "text": "IO-Tables\n\nIO tables stand for Input-Output tables. They are a way to represent the production structure of an economy.\nThey provide a Static representation of the Economy\nEach Row represents the production of a sector, and each column represents the use of that production by other sectors as Inputs.\nThe information it contains represent a snapshot of the economy at a given point in time.\nIt can be used to simulate changes in production, labor demand, and total production in the economy, under specific assumptions (production function)"
  },
  {
    "objectID": "rmethods2/session_1.html#section",
    "href": "rmethods2/session_1.html#section",
    "title": "Research Methods II",
    "section": "",
    "text": "Consider a Economy with K=3 sectors: Agriculture, Manufacturing and Services, with a Final consumer agent (households)\nEach sector (i) produces a good (\\(X_i\\)), which is sold to other sectors or the final consumer.\n\\(X_{ij}\\) is the quantity of goods sector \\(i\\) sells to sector \\(j\\), and \\(y_i\\) the final consumption by households.\n\\(X_{ji}\\) is also the quantity of goods sector \\(i\\) uses from sector \\(j\\) as inputs.\n\\(L_i\\) is the amount of labor used by sector \\(i\\).\nIn a simple Economy, (value) Total labor demand is equal to total Household income. \\(L_a + L_m + L_s = L = y_a + y_m + y_s\\)"
  },
  {
    "objectID": "rmethods2/session_1.html#section-1",
    "href": "rmethods2/session_1.html#section-1",
    "title": "Research Methods II",
    "section": "",
    "text": "This simple Economy can be represented as follows:\n\\[\n\\begin{aligned}\nX_{11} + X_{12} + X_{13} + Y_{1}  &= X_{1} \\\\\nX_{21} + X_{22} + X_{23} + Y_{2} &= X_{2} \\\\\nX_{31} + X_{32} + X_{33} + Y_{3} &= X_{3} \\\\\nL_1 + L_2 + L_3 + 0 &= L\n\\end{aligned}\n\\]\n\n\n\n\nS1\nS2\nS3\nHH\n\n\n\n\n\nSupply:\n\n\n\n\n\n\n\nS1\n\\(X_{11}\\)\n\\(X_{12}\\)\n\\(X_{13}\\)\n\\(Y_{1}\\)\n\\(X_{1}\\)\n\n\nS2\n\\(X_{21}\\)\n\\(X_{22}\\)\n\\(X_{23}\\)\n\\(Y_{2}\\)\n\\(X_{2}\\)\n\n\nS3\n\\(X_{31}\\)\n\\(X_{32}\\)\n\\(X_{33}\\)\n\\(Y_{3}\\)\n\\(X_{3}\\)\n\n\nHH\n\\(L_1\\)\n\\(L_2\\)\n\\(L_3\\)\n0\n\\(L\\)"
  },
  {
    "objectID": "rmethods2/session_1.html#section-2",
    "href": "rmethods2/session_1.html#section-2",
    "title": "Research Methods II",
    "section": "",
    "text": "From the consumption/inputs Side, we could also write the equations as:\n\\[\n\\begin{aligned}\nX_{11} &+ X_{21} &+ X_{31} &+ L_{1}  &= X_{1} \\\\\nX_{12} &+ X_{22} &+ X_{32} &+ L_{2} &= X_{2} \\\\\nX_{13} &+ X_{23} &+ X_{33} &+ L_{3} &= X_{3} \\\\\nY_1 &+ Y_2 &+ Y_3 &  &= Y\n\\end{aligned}\n\\]\nAnd from here we can get the technical coefficients:\n\\[\n\\begin{aligned}\na_{11} X_1 &+ a_{21} X_1 &+ a_{31} X_1 &+ \\lambda_{1} X_1 &= X_1 \\\\\na_{12} X_2 &+ a_{22} X_2 &+ a_{32} X_2 &+ \\lambda_{2} X_2 &= X_2 \\\\\na_{13} X_3 &+ a_{23} X_3 &+ a_{33} X_3 &+ \\lambda_{3} X_3 &= X_3 \\\\\n\\delta_1 Y &+ \\delta_2 Y & + \\delta_3 Y &  &= Y\n\\end{aligned}\n\\]\nWhere \\(a_{ij} = \\frac{X_{ij}}{X_j}\\) and \\(\\lambda_i = \\frac{L_i}{X_i}\\)\n\\[a_{1i}+a_{2i}+a_{3i}+\\lambda_i = 1 \\ \\& \\ \\delta_1+\\delta_2+\\delta_3 =1\\]"
  },
  {
    "objectID": "rmethods2/session_1.html#section-3",
    "href": "rmethods2/session_1.html#section-3",
    "title": "Research Methods II",
    "section": "",
    "text": "With this, we can write the IO table\n\\[\n\\begin{aligned}\na_{11} X_1 &+ a_{12} X_2 &+ a_{13} X_3  &+ \\delta_1 Y &= X_{1} \\\\\na_{21} X_1 &+ a_{22} X_2 &+ a_{23} X_3  &+ \\delta_2 Y &= X_{2} \\\\\na_{31} X_1 &+ a_{32} X_2 &+ a_{33} X_3  &+ \\delta_3 Y &= X_{3} \\\\\n\\lambda_1 X_1 &+ \\lambda_2 X_2 &+ \\lambda_3 X_3 &\\ \\ &= L\n\\end{aligned}\n\\]\nOr into Matrix Form\n\\[\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13}  \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\lambda_1 & \\lambda_2 & \\lambda_3\n\\end{pmatrix}\n\\begin{pmatrix}\nX_1 \\\\ X_2 \\\\ X_3\n\\end{pmatrix} +\n\\begin{pmatrix}\nY_1 \\\\ Y_2 \\\\ Y_3 \\\\ 0\n\\end{pmatrix}=\n\\begin{pmatrix}\nX_1 \\\\ X_2 \\\\ X_3 \\\\ L\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "rmethods2/session_1.html#section-4",
    "href": "rmethods2/session_1.html#section-4",
    "title": "Research Methods II",
    "section": "",
    "text": "Solve for Production sectors:\n\\[\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13}  \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix}\n\\begin{pmatrix}\nX_1 \\\\ X_2 \\\\ X_3\n\\end{pmatrix} +\n\\begin{pmatrix}\nY_1 \\\\ Y_2 \\\\ Y_3\n\\end{pmatrix}=\n\\begin{pmatrix}\nX_1 \\\\ X_2 \\\\ X_3\n\\end{pmatrix}\n\\]\n\\[\n\\begin{pmatrix}\nY_1 \\\\ Y_2 \\\\ Y_3\n\\end{pmatrix}=I \\begin{pmatrix} X_1 \\\\ X_2 \\\\ X_3 \\end{pmatrix} -\nA \\begin{pmatrix} X_1 \\\\ X_2 \\\\ X_3 \\end{pmatrix} = (I-A) \\begin{pmatrix} X_1 \\\\ X_2 \\\\ X_3 \\end{pmatrix}\n\\]\nFinally:\n\\[\n\\begin{pmatrix} X_1 \\\\ X_2 \\\\ X_3 \\end{pmatrix} = (I-A)^{-1} \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\end{pmatrix}\n\\rightarrow\n\\begin{pmatrix} \\Delta X_1 \\\\ \\Delta X_2 \\\\ \\Delta X_3 \\end{pmatrix} = (I-A)^{-1}\n\\begin{pmatrix} \\Delta Y_1 \\\\ \\Delta Y_2 \\\\ \\Delta Y_3 \\end{pmatrix}\n\\]"
  },
  {
    "objectID": "rmethods2/session_1.html#example",
    "href": "rmethods2/session_1.html#example",
    "title": "Research Methods II",
    "section": "Example",
    "text": "Example\n\nConsider the following IO table for a simple economy with 3 sectors and a final consumer.\n\n\n\n\n\n\n\n\n\n\n\nSector\nAgriculture\nManufacturing\nServices\nFinal Consumer\n\n\n\n\nAgriculture\n102\n103\n153\n129\n\n\nManufacturing\n133\n124\n77\n99\n\n\nServices\n71\n92\n51\n165\n\n\nHouseholds\n181\n114\n98\n0\n\n\n\nS1: What is the total production of each sector?\n\n\\(X_1 = 102 + 103 + 153 + 129 = 487\\)\n\\(X_2 = 133 + 124 + 77 + 99 = 433\\)\n\\(X_3 = 71 + 92 + 51 + 165 = 379\\)\n\nS2: What are the technical coefficients?\n\n\n\n\n\n\n\n\n\n\nSector\nAgriculture\nManufacturing\nServices\nFinal Consumer\n\n\n\n\nAgriculture\n\\(a_{11}\\) = 0.209\n0.238\n0.404\n0.328\n\n\nManufacturing\n0.273\n0.286\n0.203\n0.252\n\n\nServices\n0.146\n0.212\n0.135\n0.420\n\n\nHouseholds\n0.372\n0.263\n0.259\n0.000\n\n\n\nThis captures a snapshot of an economy. And could use to simulate changes in production and labor demand.\nS3. How much would production change if the final consumer demand for Agriculture increases in 20%?\n\\[\\Delta X = (I-A)^-1\n\\begin{pmatrix} 0.2 * 129 \\\\ 0 \\\\ 0 \\end{pmatrix} =\n\\begin{pmatrix} 45.54 \\\\ 21.08 \\\\ 12.84 \\end{pmatrix}\n\\]\nS4: How much would labor demand change if the final consumer demand for Agriculture increases in 20%? \\[\\Delta L_i = \\lambda_i * \\Delta X_i\\]\n\\(\\Delta L_1 = 0.372 * 45.54 = 16.95 ; \\Delta L_2 =  0.263 * 21.08 = 5.544 ; \\Delta L_3 =  0.259 * 12.84 = 3.326\\)"
  },
  {
    "objectID": "rmethods2/session_1.html#example---stata",
    "href": "rmethods2/session_1.html#example---stata",
    "title": "Research Methods II",
    "section": "Example - Stata",
    "text": "Example - Stata\nmata: Input Data\n\nmata: x  = (102, 103, 153 \\ 133, 124, 77 \\ 71, 92, 51)\nmata: y  = (129 \\ 99 \\ 165)\nmata: hh = ( 181 , 114 , 98)\n\nEstimate Total Production\n\nmata: tp = colsum(x):+hh ; tp\n\n         1     2     3\n    +-------------------+\n  1 |  487   433   379  |\n    +-------------------+\n\n\nEstimate Technical Coefficients\n\n// Technical Coefficients\nmata:ai = x:/tp ; ai\n\n                 1             2             3\n    +-------------------------------------------+\n  1 |  .2094455852   .2378752887   .4036939314  |\n  2 |   .273100616   .2863741339   .2031662269  |\n  3 |  .1457905544   .2124711316   .1345646438  |\n    +-------------------------------------------+\n\n\nEstimate Change in Demand: 20% increase in Agriculture\n\n// Technical Coefficients\nmata:dy = y :* (.2 \\ 0 \\ 0); dy\n\n          1\n    +--------+\n  1 |  25.8  |\n  2 |     0  |\n  3 |     0  |\n    +--------+\n\n\nEstimate Change in Production\n\n// Change in Production\nmata:dx = qrinv(I(3)-ai)*dy; dx\n\n                 1\n    +---------------+\n  1 |  45.54130481  |\n  2 |  21.08637814  |\n  3 |  12.84872246  |\n    +---------------+\n\n\nEstimate Change in Labor Demand\n\nmata:dl = (hh:/tp)':*dx; dl\n\n                 1\n    +---------------+\n  1 |   16.9260291  |\n  2 |  5.551609949  |\n  3 |  3.322360954  |\n    +---------------+"
  },
  {
    "objectID": "rmethods2/session_1.html#sam-social-accounting-matrix-1",
    "href": "rmethods2/session_1.html#sam-social-accounting-matrix-1",
    "title": "Research Methods II",
    "section": "SAM: Social Accounting Matrix",
    "text": "SAM: Social Accounting Matrix\n\nSAM can be thought as an upgraded version of IO-tables.\nThey are a way to organize information about the production structure of an economy, but also the distribution of resources.\n\nThis it will not only register production of goods and services, but also transfers of resources between sectors and agents.\n\nYou can also use it as basis for a plausible model of the economy.\n\nPrediction of changes in production, income and distribution."
  },
  {
    "objectID": "rmethods2/session_1.html#example-1",
    "href": "rmethods2/session_1.html#example-1",
    "title": "Research Methods II",
    "section": "Example",
    "text": "Example\n\nClosed Economy No GoV SAM\n\n\n\nProduction\nConsumption\nAccumulation\nTotals\n\n\n\n\nProduction\n\n\\(C\\)\n\\(I\\)\n\\(C+I\\)\n\n\nConsumption\n\\(Y\\)\n\n\n\\(Y\\)\n\n\nAccumulation\n\n\\(S\\)\n\n\\(S\\)\n\n\nTotals\n\\(Y\\)\n\\(C+S\\)\n\\(I\\)\n\n\n\n\n\nGoods/services are Transfered from left to Top-right\nMonetary Transfers are from Top-right to left"
  },
  {
    "objectID": "rmethods2/session_1.html#example-2",
    "href": "rmethods2/session_1.html#example-2",
    "title": "Research Methods II",
    "section": "Example 2",
    "text": "Example 2\n\nOpen Economy with GoV SAM\n\n\n\n\n\n\n\n\n\n\n\n\nS1\nS2\nS3\nS4\nS5\nTotals\n\n\n\n\nS1: Prod\n\n\\(C\\)\n\\(G\\)\n\\(I\\)\n\\(E\\)\n\\(C+G+I+E\\)\n\n\nS2: HH\n\\(Y\\)\n\n\n\n\n\\(Y\\)\n\n\nS3: Gov\n\n\\(Tx\\)\n\n\n\n\\(Tx\\)\n\n\nS4: K acc\n\n\\(S_h\\)\n\\(S_g\\)\n\n\\(S_f\\)\n\\(S_h+S_g+S_f\\)\n\n\nS5: RofW\n\\(M\\)\n\n\n\n\n\\(M\\)\n\n\nTotals\n\\(Y+M\\)\n\\(C+S_h+Tx\\)\n\\(G+S_g\\)\n\\(I\\)\n\\(E+S_f\\)\n\n\n\n\nHere \\(S1\\) and \\(S2\\) is what we had in the IO table. Thus, we could further expand the SAM to include more sectors and agents."
  },
  {
    "objectID": "rmethods2/session_1.html#example-3",
    "href": "rmethods2/session_1.html#example-3",
    "title": "Research Methods II",
    "section": "Example 3",
    "text": "Example 3\n\nMoreDetailed SAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nS1\nS2\nS3\nS4\nS5\nS6\nS7\nS8\n\n\n\n\nS1: Act\n\n\\(Gds\\)\n\n\n\n\n\n\n\n\nS2: Commod\n\\(IntGds\\)\n\n\n\n\\(C\\)\n\\(G\\)\n\\(I\\)\n\\(E\\)\n\n\nS3: Factors\n\\(VA\\)\n\n\n\n\n\n\n\\(FE\\)\n\n\nS4: Enter\n\n\n\\(Prof\\)\n\n\n\\(ETr\\)\n\n\n\n\nS5: HH\n\n\n\\(Wage\\)\n\\(DProf\\)\n\n\\(Tr\\)\n\n\\(REM\\)\n\n\nS6: Gov\n\\(ITx\\)\n\n\\(FTx\\)\n\\(ETx\\)\n\\(DTx\\)\n\n\n\\(Tarf\\)\n\n\nS7: K acc\n\n\n\n\\(RetY\\)\n\\(S_h\\)\n\\(S_g\\)\n\n\\(KTrM\\)\n\n\nS8: RofW\n\n\\(M\\)\n\\(FM\\)\n\n\\(REMA\\)\n\\(TrA\\)\n\\(KtrA\\)"
  },
  {
    "objectID": "rmethods2/session_1.html#other-extensions",
    "href": "rmethods2/session_1.html#other-extensions",
    "title": "Research Methods II",
    "section": "Other Extensions",
    "text": "Other Extensions\n\nSAM also allow you to do further extensions to include more agents (heterogenous)\n\nGreen-Industry\nInformal Sector\nHouseholds by income level\netc."
  },
  {
    "objectID": "rmethods2/session_1.html#what-to-get-from-today",
    "href": "rmethods2/session_1.html#what-to-get-from-today",
    "title": "Research Methods II",
    "section": "What to get from today?",
    "text": "What to get from today?\n\nHow to use weights in Stata to account for survey design, and how to obtain summary statistics.\nHow to test differences in means across groups.\nHow to use IO tables to simulate changes in production and labor demand.\nUnderstand how SAM can be used to represent an Economy"
  },
  {
    "objectID": "rmethods2/HomeWork2.html",
    "href": "rmethods2/HomeWork2.html",
    "title": "Homework II",
    "section": "",
    "text": "Submit a document containing your answers to the following questions and a do file with the code used in Stata to produce the answers. Ensure your program works as submitted and produces the reported answers for full credit."
  },
  {
    "objectID": "rmethods2/HomeWork2.html#instructions",
    "href": "rmethods2/HomeWork2.html#instructions",
    "title": "Homework II",
    "section": "",
    "text": "Submit a document containing your answers to the following questions and a do file with the code used in Stata to produce the answers. Ensure your program works as submitted and produces the reported answers for full credit."
  },
  {
    "objectID": "rmethods2/HomeWork2.html#part-i-significance-testing-15pts",
    "href": "rmethods2/HomeWork2.html#part-i-significance-testing-15pts",
    "title": "Homework II",
    "section": "Part I: Significance Testing (15pts)",
    "text": "Part I: Significance Testing (15pts)\nIn this section, use simulation to demonstrate: - How rejecting the null hypothesis probability increases with the number of tests. - Benferroni correction application to control for multiple testing. - Joint testing usage to control for multiple testing.\n\nTasks\nRefer to the file hw2.do for a template program simulating data for N individuals and K variables. Modify this program to:\n\nTest the null hypothesis that the mean of each variable is zero with a significance level of 10%. Indicate if you reject the null for any variable.\nRepeat the test in 1 using a modified significance level of 10%/K (Benferroni correction). Indicate if you reject the null for any variable.\nConduct a joint test that the mean of all K variables is zero, using a 10% significance level.\n\nRepeat the above tasks 1000 times and report the proportion of times you reject the null hypothesis in each case (summary tables). Provide an explanation for your results.\nFor the parameters K and N, use the following combinations:\n\nK = 2, N = 20\nK = 5, N = 20\nK = 2, N = 100\nK = 5, N = 100\n\nUsing the code, you need to change the parameters K and N to run each simulation. Create a table with the results for each combination of K and N, and explain."
  },
  {
    "objectID": "rmethods2/HomeWork2.html#part-ii-imputation-and-statistical-matching-30pts",
    "href": "rmethods2/HomeWork2.html#part-ii-imputation-and-statistical-matching-30pts",
    "title": "Homework II",
    "section": "Part II: Imputation and Statistical Matching (30pts)",
    "text": "Part II: Imputation and Statistical Matching (30pts)\nImputation and Statistical Matching are methods for handling missing data. Use any of these methods on the dataset cps_imput_miss.dta, to address the problem of missing wages.\nThe dataset focuses on couple-households with or without children (below 15 years old) and no other family members. Their only source of income comes from wages.\nThe dataset includes variables ending in _1 for the husband and _2 for the wife. Household-level variables and the variable cutoff indicating the poverty line are also provided\n\nThe problem\nThe problem on this dataset is that in 50% of the households, husband’s wages are missing. And we are interested in estimating the poverty rate and for the sample.\n\n\nTasks\n\nImpute missing wages for husbands and estimate the poverty rate for the sample.\n\nWrite a report explaining the steps you have taken for imputation, including model specification. Provide a table with poverty rates for the entire sample, by husband education level, and race. Offer a brief explanation of the results.\n\n\nNote: Include a do-file with the code for imputation and summary statistics."
  },
  {
    "objectID": "rmethods2/HomeWork2.html#part-iii-micro-simulations-55pts",
    "href": "rmethods2/HomeWork2.html#part-iii-micro-simulations-55pts",
    "title": "Homework II",
    "section": "Part III: Micro-Simulations (55pts)",
    "text": "Part III: Micro-Simulations (55pts)\nMicro-simulations are a valuable tool for studying the effects of policy changes. In this section, use micro-simulations to explore the impact of an Employer of Last Resort (ELR) program on the labor market of Tanga-Mandapio, a small Pacific country similar to the United States.\n\nConsiderations\n\nThe ELR program guarantees a job to anyone between 18 and 65 years old who is not employed.\nThe program offers different wages based on education levels, based on the following schedule:\n\n\nWages per hour offered by the ELR program\n\n\nEducation\nWage\n\n\n\n\nLess than High School\n10\n\n\nHigh School\n15\n\n\nSome College\n20\n\n\nCollege\n25\n\n\nGraduate School\n30\n\n\n\n\nAll jobs offered by the program are full time (40 hours per week), full year (48 weeks per year).\nThere is a budget of 100 Million dollars per year for the program.\n\n\n\nTasks\nUse the following steps to simulate the impact of the ELR program:\n\nDetermine eligibility for the ELR program.\nEstimate a logit model to predict employment probability considering only those employed and those who are eligible for the program.\nUse model predictions to assign jobs until the budget is exhausted, prioritizing those with higher employment probability.\nRecalculate total household income accounting for new jobs.\n\nWrite a report explaining the simulation steps, including model specification, and discuss the program’s impact:\n\nHow many people are eligible for the program by education level?\nWhat would the the budget requirement be to guarantee a job to all unemployed individuals.\nJobs created, impact on the unemployment rate by education.\nImpact on poverty and inequality using cutoff and Income Percapita (hhincome/hhsize).\n\nProvide summary statistics for poverty before and after the simulation, by gender and race.\n\nNote: Include a do-file with the code for the simulation and summary statistics.\n\n\n\nRemarks\n\nThe data file is tanga_mandapio. Assume each person has a weight of 1.\nhhincome is total household income, and incwage is individual wage income.\ncutoff is the poverty line for a household.\neduc_g is the aggregated education level.\nExplore the dataset for a better understanding of available variables.\n\n\n\nHints on How to proceed\n\nCreate a dummy emp that is equal to 1 if employed and 0 otherwise only for people age = 18 to 65.\nEstimate your logit model using emp as dependent variable, as a function of characteristics.\nUsing predicted Probabilities, assign jobs to those eligible for the program, using those with highest predicted probability first.\n\nFirst for the unemployed, sort data by predicted probability (highest to lowest) and assign jobs until the budget is exhausted.(each persons anual wage is wagex40x48, where wages depends on his education level)\n\nTo recalculate total household income, you can simply create gen new_hhincome = hhincome + new_wage_elr where new_wage_elr the sum of ELR wages for each household. Some households may have more than one person employed by the ELR program.\nHouseholds are identified with the variable serial."
  },
  {
    "objectID": "rmethods/table/tab1.html",
    "href": "rmethods/table/tab1.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "asd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOz\n\nlbs\n\nKgs\n\nGr\n\n\n\n\n\nmale\n3.123\n\n0.195\n\n0.089\n\n88.605\n\n\n\n\n(1.071)\n[2.916]\n(0.067)\n[2.916]\n(0.030)\n[2.916]\n(30.389)\n[2.916]\n\n\nwhite\n5.404\n\n0.338\n\n0.153\n\n153.346\n\n\n\n\n(1.392)\n[3.882]\n(0.087)\n[3.882]\n(0.039)\n[3.882]\n(39.497)\n[3.882]\n\n\ncigs\n-0.480\n\n-0.030\n\n-0.014\n\n-13.628\n\n\n\n\n(0.091)\n[-5.288]\n(0.006)\n[-5.288]\n(0.003)\n[-5.288]\n(2.577)\n[-5.288]\n\n\nlfaminc\n1.053\n\n0.066\n\n0.030\n\n29.867\n\n\n\n\n(0.632)\n[1.664]\n(0.040)\n[1.664]\n(0.018)\n[1.664]\n(17.946)\n[1.664]\n\n\n_cons\n110.603\n\n6.913\n\n3.138\n\n3138.351\n\n\n\n\n(2.071)\n[53.410]\n(0.129)\n[53.410]\n(0.059)\n[53.410]\n(58.760)\n[53.410]\n\n\nN\n1388\n\n1388\n\n1388\n\n1388\n\n\n\nR2\n0.046\n\n0.046\n\n0.046\n\n0.046"
  },
  {
    "objectID": "rmethods/Midterm.html",
    "href": "rmethods/Midterm.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Midterm\n\nExplain how to implement a Chow test using two separate regressions. and why you cannot use it if the underlying model is heteroskedastic."
  },
  {
    "objectID": "rmethods/homework_3.html",
    "href": "rmethods/homework_3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Consider the dataset labsup (available using frause). This dataset has information on the number of hours worked per week, hourly wage, and years of education for a sample balck and hispanic women. The goal is to determine the effect of an additional child has on the number of hours worked per week.\nTo do this, estimate a model of the form:\n\\[\\begin{aligned}\nhours &=b_0 + b_1 kids + b_2 educ + b_3 age + b_4 age^2 \\\\\n&+ b_5 hispanic + b_6 black+ b_7 nonmomi + u\n\\end{aligned} \\tag{1}\\]\n(see variable description in the datafile)\nQ1-1. (10pts) Estimate the model using OLS. What is the effect of an additional child on the number of hours worked per week? Is this effect statistically significant? What about the rest of the variables?\nQ1-2. (10pts) It is believed that Number of childrens may be correlated with the model error (endogenous). Explain why this may be the case. (You can consult the original paper)\nQ1-3. (20pts) There are two possible candidates for IV in this case. samesex (if the first two children had the same sex), and multi2nd - if the family had twins the second during the second pregnancy. Estimate Equation 1, using each of these variables as IV separately, and both of them together. While doing this answer:\n\nAre the instruments strong individually? Are they strong together? (F-statistic of first stage)\nHow does the effect of kids on Hours worked change when using the different IV’s. Are there any differences?\nBased on all specifications, is there any evidence that the number of children is endogenous?\n\n\n\nConsider the dataset smoke (available using frause). This dataset has information on few demographic characteristics, cigarate prices, income, as well as number of cigarates smoked per day.\nIn this case, “Cigarates smoked per day” is a kind of limited dependent variable, because not everyone will smoke, and the number of cigarates people smoke is an integer. Under this considerations estimate the following:\nQ2-1. (10pts) Estimate a Linear probability model (LPM) and probit model, analyzing the probability of smoking as function of demographics, log of income and log of prices. Analyze the results and compare the two models (magnitudes of the effects). What would you say the impact of restaurant smoking bans is on the probability of smoking?\nQ2-2. (10pts) Now, say that you are also interested in analyzing the relations of the different factors on the number of cigarates smoked per day. Estimate a Poisson model and a Tobit model (with data censored at 0). Analyze the results and compare the two models (magnitudes of the effects). What would you say the impact of restaurant smoking bans is on the number of cigarates smoked per day? what about the impact of a 10% increase in cigarate prices?\n\nNote: For the Tobit model, you can use the tobit, ll(0), and will need to use margins, margins, dydx(*) ystar(0,.) to get the marginal effects that are comparable across models.\n\nQ2-3. (10pts) You are also asked to use the Tobit results to analyze the probability of smoking. To do this, you will need to use the marginal effects from the Tobit model margins, dydx(*) pr(0,.). Compare the results of this exercise with the results from the LPM and probit models. Would you reach the same conclusions? if not, what are the implications for the tobit assumptions of a tobit model?\n\n\n\nConsider the dataset driving (available using frause). This dataset has information at the state level, from 1980 to 2004, on statistics regarding traffic accidents, fatalities, and laws.\nSee datafile for variable description.\nQ3-1. (10pts) Estimate a model of the form:\n\\[\\begin{aligned}\ntotfatrte &= b_0 + b_1 \\mathbb{1}(seatbealt=1) + b_2 \\mathbb{1}(seatbealt=2) + b_3 minage  \\\\\n& + b_4 zerotol + b_5 unem + b_6 perc14\\_24 + b_7 vehicmiles + e\n\\end{aligned} \\tag{2}\\]\nAnd interpret the results. is there any effect that seems unexpected? Explain why this may be the case.\nQ3-2. (5pts) One of the reasons one may be finding unexpected results is because of unobserved heterogeneity across states. Can you explain what kind of factors are there that could be related to both accidents and the laws? (think about the political process of passing laws)\nQ3-3. (10pts) To control for unobserved heterogeneity, estimate three models:\n- Random effect model\n- Fixed effect model\n- Correlated Random effect model\nHow do the results from the Random effect model and Fixed effect model compare to each other? what about compared to Equation 2?\nQ3-4. (5pts) Using the results from the Correlated Random effect model, test for which of the models is more appropriate, between Random Effects and Fixed effects. What do you conclude?"
  },
  {
    "objectID": "rmethods/homework_3.html#part-ii-mle-and-nonlinear-models-30pts",
    "href": "rmethods/homework_3.html#part-ii-mle-and-nonlinear-models-30pts",
    "title": "Homework 3",
    "section": "",
    "text": "Consider the dataset smoke (available using frause). This dataset has information on few demographic characteristics, cigarate prices, income, as well as number of cigarates smoked per day.\nIn this case, “Cigarates smoked per day” is a kind of limited dependent variable, because not everyone will smoke, and the number of cigarates people smoke is an integer. Under this considerations estimate the following:\nQ2-1. (10pts) Estimate a Linear probability model (LPM) and probit model, analyzing the probability of smoking as function of demographics, log of income and log of prices. Analyze the results and compare the two models (magnitudes of the effects). What would you say the impact of restaurant smoking bans is on the probability of smoking?\nQ2-2. (10pts) Now, say that you are also interested in analyzing the relations of the different factors on the number of cigarates smoked per day. Estimate a Poisson model and a Tobit model (with data censored at 0). Analyze the results and compare the two models (magnitudes of the effects). What would you say the impact of restaurant smoking bans is on the number of cigarates smoked per day? what about the impact of a 10% increase in cigarate prices?\n\nNote: For the Tobit model, you can use the tobit, ll(0), and will need to use margins, margins, dydx(*) ystar(0,.) to get the marginal effects that are comparable across models.\n\nQ2-3. (10pts) You are also asked to use the Tobit results to analyze the probability of smoking. To do this, you will need to use the marginal effects from the Tobit model margins, dydx(*) pr(0,.). Compare the results of this exercise with the results from the LPM and probit models. Would you reach the same conclusions? if not, what are the implications for the tobit assumptions of a tobit model?"
  },
  {
    "objectID": "rmethods/homework_3.html#part-iii-panel-data-30pts",
    "href": "rmethods/homework_3.html#part-iii-panel-data-30pts",
    "title": "Homework 3",
    "section": "",
    "text": "Consider the dataset driving (available using frause). This dataset has information at the state level, from 1980 to 2004, on statistics regarding traffic accidents, fatalities, and laws.\nSee datafile for variable description.\nQ3-1. (10pts) Estimate a model of the form:\n\\[\\begin{aligned}\ntotfatrte &= b_0 + b_1 \\mathbb{1}(seatbealt=1) + b_2 \\mathbb{1}(seatbealt=2) + b_3 minage  \\\\\n& + b_4 zerotol + b_5 unem + b_6 perc14\\_24 + b_7 vehicmiles + e\n\\end{aligned} \\tag{2}\\]\nAnd interpret the results. is there any effect that seems unexpected? Explain why this may be the case.\nQ3-2. (5pts) One of the reasons one may be finding unexpected results is because of unobserved heterogeneity across states. Can you explain what kind of factors are there that could be related to both accidents and the laws? (think about the political process of passing laws)\nQ3-3. (10pts) To control for unobserved heterogeneity, estimate three models:\n- Random effect model\n- Fixed effect model\n- Correlated Random effect model\nHow do the results from the Random effect model and Fixed effect model compare to each other? what about compared to Equation 2?\nQ3-4. (5pts) Using the results from the Correlated Random effect model, test for which of the models is more appropriate, between Random Effects and Fixed effects. What do you conclude?"
  },
  {
    "objectID": "rmethods/homework_1.html",
    "href": "rmethods/homework_1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Read Chapter 2 of -The Effect- by Nick Hungtington-Klein. here\nWrite a small research proposal that will answer answer the following:\n\nResearch question,\nIdentify your main dependent variable, and the variable(s) you want to analyze the causal effect of.\nDescribe relevant factors that may need to be considered for the analysis (Economic model).\nDescribe how those factors may relate to the outcome, and your variable(s) of interest.\nDescribe an ideal Experiment you may run to identify the effect. Is it a feasible experiment?\nIs there any data you could use to answer this question?"
  },
  {
    "objectID": "rmethods/homework_1.html#note",
    "href": "rmethods/homework_1.html#note",
    "title": "Homework 1",
    "section": "Note",
    "text": "Note\nSearch among all datasets available in frause, and use the data from these datasets to decide what variables to analyze, and controls to use in your homework.\nExamples in the textbook can be useful as guides to consider here.\nYou are free to explore other sources. If so, include the data along with your homework.\nParticularly unique answers (based on completeness, detail, novelty) may receive extra points."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#the-problem-endogeneity",
    "href": "rmethods/8_iv2sls.html#the-problem-endogeneity",
    "title": "Instrumental Variables and 2SLS",
    "section": "The problem: Endogeneity",
    "text": "The problem: Endogeneity\n\nAs I mentioned at the beginning, one of the most important assumptions required to analyze data and obtain correct estimations and draw inference was A4: No endogeneity or \\(E(e|X)=0\\)\n\nEndogeneity is a problem that occurs because the error is related to \\(X\\).\nThis is a proble,, because we can no longer assume the error is, in average, constant when analyzing changes in \\(X's\\)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section",
    "href": "rmethods/8_iv2sls.html#section",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "Why did it happen?\n\nUsually because important variables are omitted\n\nAdd them back, or at least proxies?\n\nIncorrect functional form\n\nTry making it more flexible?\n\nData has measurement error\n\nGet better data?\n\nSample is endogenous (other treatments are necessary)\nReverse causality (you dont know which cause which)\nSimultenaity, similar to omitted variables. There is another factor that caused both the outcome and explanatory variable"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#a-pair-of-solutions",
    "href": "rmethods/8_iv2sls.html#a-pair-of-solutions",
    "title": "Instrumental Variables and 2SLS",
    "section": "A pair of Solutions",
    "text": "A pair of Solutions\n\nToday we will cover one approach that could help with many (not all) the situations that could cause endogeneity.\nTo apply this approach, however, we need:\n\nMore data (more variables with specific properties)\nMore information on how the “system” works.\n\nThese methods are:\n\nIV - Instrumental variable\n2sls - Two-stage Least squares\n\n\n\nNOTE: These two methods are almost interchangable.\n\nIV refers to cases with 1 endogenous variable and 1 “instrument”\n2sls refers to cases with 1+ endogenous variables and “instruments”"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#what-is-an-instrumental-variable",
    "href": "rmethods/8_iv2sls.html#what-is-an-instrumental-variable",
    "title": "Instrumental Variables and 2SLS",
    "section": "What is an “Instrumental Variable”",
    "text": "What is an “Instrumental Variable”\n\nI have mentioned a few times the word “instrument” But what are they really?\n\nInstruments: the heros/variables that will “save us” of endogeneity.\n\nThey have at least 2 properties:\n\nInstruments should be exogenous to the model\n\n\nDoes not appear in the specification, thus \\(Z\\) has no DIRECT effect on the outcome \\(y\\).\n\nEffect exists only through the endogenous variable.\n\nAlso that there is no correlation between the model error and \\(Z\\).\n\n\nThe instrument is relevant and related to \\(x\\) (endogenous variable)\n\n\nPreferably, you need a variable that is not only correlated with \\(X\\) but determines changes in \\(X\\).\nWe also need this effect to be monotonic!\n\nIn many instances we even want an instrument that is just as good as [conditionally] random."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#how-instruments-work-the-math",
    "href": "rmethods/8_iv2sls.html#how-instruments-work-the-math",
    "title": "Instrumental Variables and 2SLS",
    "section": "How Instruments work: The math",
    "text": "How Instruments work: The math\nThe problem \\(corr(x_1,e) \\neq 0\\): \\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + e || \\tilde w = w-\\bar w  \\\\\n\\tilde \\beta_1 &=\\frac{\\sum \\tilde x_1 \\tilde y}{\\sum \\tilde x_1^2}=\n\\frac{\\sum \\tilde x_1 (\\beta_1 \\tilde x_1 + e)}{\\sum \\tilde x_1^2} \\\\\n& = \\beta_1 + \\frac{\\sum \\tilde x_1 e}{\\sum \\tilde x_1^2}\n\\end{aligned}\n\\]\nHow IV works \\(corr(z_1,e) \\neq 0\\):\n\\[\\begin{aligned}\n\\hat \\beta_1 &=\\frac{\\sum \\tilde z_1 \\tilde y}{\\sum \\tilde z_1 \\tilde x_1}=\n\\frac{\\sum \\tilde z_1 (\\beta_1 \\tilde x_1 + e)}{\\sum \\tilde z_1 \\tilde x_1} \\\\\n& = \\beta_1 + \\frac{\\sum \\tilde z_1 e}{\\sum \\tilde z_1 \\tilde x_1}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#how-instruments-work-the-intuition",
    "href": "rmethods/8_iv2sls.html#how-instruments-work-the-intuition",
    "title": "Instrumental Variables and 2SLS",
    "section": "How Instruments work: The intuition",
    "text": "How Instruments work: The intuition\n\nOne way of thinking about how IV works is by realizing not ALL changes in \\(X_1\\) are endogenous. Some are due to \\(e\\), but some are due other factors.\n\nwe just can’t differentiate them\nIf we could use (in the regression), only exogenous changes, (or omit endogenous ones), we could estimate our models correctly.\n\nWhat IV’s do is to identify Part of the exogenous component (the one related to \\(Z\\)), and use only THAT variation to identify coefficients.\n\nThis would do a reasonable work, as long as the instrument is relevant, and instrument exogenous."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#example",
    "href": "rmethods/8_iv2sls.html#example",
    "title": "Instrumental Variables and 2SLS",
    "section": "Example",
    "text": "Example\n\\[\\begin{aligned}\ne &\\sim chi(2)-2 ; z = chi(2)-2 ; x = chi(2)-2+z+e \\\\\ny &=1+x+e\n\\end{aligned}\n\\]\n\n\nCode\n** Montecarlo Simulation\nset linesize 255\nclear\nset seed 10101\nset obs 1000\nqui:mata:\nk = 1000; n=1000\nb1=bc = b = J(k,2,0)\nfor(i=1;i&lt;=k;i++){\n    e = rchi2(n,1,2):-2\n    e1 = rchi2(n,1,2):-2\n    z = rchi2(n,1,2):-2,J(n,1,1)\n    x = rchi2(n,1,2):-2:+z[,1]:+e ,J(n,1,1)\n    x1 = rchi2(n,1,2):-2:+z[,1]:+e1,J(n,1,1)\n    y = 1:+x[,1]:+e\n    y1 = 1:+x[,1]:+e1\n    xx = cross(x,x)\n    b[i,] = (invsym(xx)*cross(x,y))'\n    bc[i,] = (invsym(cross(z,x))*cross(z,y))'\n    b1[i,] = (invsym(xx)*cross(x,y1))'\n}\nend\ngetmata bb*=b\ngetmata bc*=bc\ngetmata b_*=b1\nset scheme white2\ncolor_style tableau\ntwo kdensity bb1 ||  kdensity bc1  || kdensity b_1, ///\nlegend(order(1 \"X Endogenous\" 2 \"IV\" 3 \"X Exogenous\"))"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#se-and-statistical-inference",
    "href": "rmethods/8_iv2sls.html#se-and-statistical-inference",
    "title": "Instrumental Variables and 2SLS",
    "section": "SE and Statistical Inference",
    "text": "SE and Statistical Inference\n\nSE have a different structure compared to OLS.\nIn the simplest case (one dep variable that is endogenous), and under the assumption of Homoskedasticity, SE for \\(\\beta_1\\) are given by:\n\n\\[\\begin{aligned}Var_{iv}(\\beta_1) = \\frac{\\hat\\sigma^2_e}{SST_x R^2_{x|z}} \\\\\n\\hat\\sigma^2_e =\\frac{ \\sum (y-\\hat\\beta_0-\\hat\\beta_1 x)^2}{n-2}\n\\end{aligned}\n\\]\n\nOnce they are obtained t-stats can be used as usual"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#examples-of-ivs",
    "href": "rmethods/8_iv2sls.html#examples-of-ivs",
    "title": "Instrumental Variables and 2SLS",
    "section": "Examples of IV’s",
    "text": "Examples of IV’s\n\n\nEducation:\n\nFathers Education\nDistance to School\n# Siblins\n\n\nVeteran Status:\n\nVietnam Lottery Ticket\n\nOther:\n\nRain\nShift-Share\nJudge FE\n\n\n\nIV SE will be necessarily larger than OLS, because there is less variation of \\(x\\) used to identify \\(\\beta\\).\nIf \\(x\\) is used as its own instrument, the estimator will be that of OLS."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#weak-and-endogenous-instruments",
    "href": "rmethods/8_iv2sls.html#weak-and-endogenous-instruments",
    "title": "Instrumental Variables and 2SLS",
    "section": "Weak and endogenous instruments:",
    "text": "Weak and endogenous instruments:\n\nWhile instruments can be used to address Endogeneity problems, finding good instruments can be hard.\n\nThey can be “weak-instruments”\nor not fully exogenous\n\nIf this happens, IV can be worse than endogeneity:\n\n\\[\\begin{aligned}\nplim \\beta_{ols} &= \\beta_1 + corr(x,u) \\frac{\\sigma_u}{\\sigma_x} \\\\\nplim \\beta_{iv}  &= \\beta_1 + \\frac{corr(z,u)}{corr(z,x)} \\frac{\\sigma_u}{\\sigma_x}\n\\end{aligned}\n\\]\n\nwe will talk about the “weak-instruments” later today."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#iv-with-mlr",
    "href": "rmethods/8_iv2sls.html#iv-with-mlr",
    "title": "Instrumental Variables and 2SLS",
    "section": "IV with MLR",
    "text": "IV with MLR\nAdding controls…is easy!\n\\[y = \\beta_0 + \\gamma_1 y_1 + X\\beta + e ; z \\text{ instrument for } y\n\\]\nWe still assume 1 endogenous variable (\\(y_1\\)) and one instrument (\\(z\\))\n\\[X =\\begin{bmatrix} 1 & y_1 & x_1 & x_2 & x_3 \\end{bmatrix} ;\nZ =\\begin{bmatrix} 1 & z & x_1 & x_2 & x_3 \\end{bmatrix}\n\\]\nthen \\(\\hat\\beta_{iv}\\) is given by\n\\[\\hat\\beta_{iv} = (Z'X)^{-1}{Z'y}\n\\]\n\nSame assumptions needed, except that instrument strength is measured by the \\(corr(y_1-E[y_1|X], z-E[z|X] )\\)\n\nMulticollinearity problem can be a problem here"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#iv-and-treatment-effects",
    "href": "rmethods/8_iv2sls.html#iv-and-treatment-effects",
    "title": "Instrumental Variables and 2SLS",
    "section": "IV and Treatment Effects",
    "text": "IV and Treatment Effects\nOne small note:\n\nWhen analyzing the model of interest, we could also try to analyze the reduced form model:\n\n\\[y = \\beta_0 + \\lambda z + X \\beta + e\\]\n\nThis model will not give you the effect of \\(y_1\\) (endogenous variable), but can be just as interesting, specially when instrument and endogenous variables are dummies.\n\nIn such case \\(\\lambda\\) will represent the “intention-to-treat” effect, rather than “treatment” effect."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#sls-many-ys-many-zs",
    "href": "rmethods/8_iv2sls.html#sls-many-ys-many-zs",
    "title": "Instrumental Variables and 2SLS",
    "section": "2SLS: Many Ys many Zs",
    "text": "2SLS: Many Ys many Zs\n\nThere could be situations where you have not one, but many instruments.\n\nThis may be rare, as even finding a single instrument can be hard\n\nYou may also have the situation where there is more than one endogenous variable!\n\nIn such case, you need at least as many IVs as endogenous variables\n\n\n\nThe same assumptions as before apply. IV’s need to be exogenous, but relevant to explain the endogenous variables.\nContrary to intuition, ALL instruments are used to analyze ALL exogenous variables"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#sls-estimation",
    "href": "rmethods/8_iv2sls.html#sls-estimation",
    "title": "Instrumental Variables and 2SLS",
    "section": "2SLS: Estimation",
    "text": "2SLS: Estimation\nConsider the following:\n\n\\(y\\) is the variable of interest\n\\(x1, x2\\) set of exogenous variables\n\\(y1, y2\\) set of endogenous variables\n\\(z1,z2,z3\\) set of instruments for \\(y1\\) and \\(y2\\)\n\n\\(X's\\) and \\(Z's\\) are exogenous:"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section-1",
    "href": "rmethods/8_iv2sls.html#section-1",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "Model of interesed: \\[y = a_0 + a_1 y_1 + a_2 y_2 + b_1 x_1 + b_2 x_2 +  b_3 x_3 + e\n\\]\nFirst Stage\n\\[\\begin{aligned}\ny_1 = \\gamma^1_0 + \\gamma^1_1 z_1+ \\gamma^1_2 z_2+ \\gamma^1_3 z_3+\\lambda^1_1 x_1 + \\lambda^1_2 x_2 +  \\lambda^1_3 x_3  + v_1 \\rightarrow \\hat y_1 \\\\\ny_2 = \\gamma^2_0 + \\gamma^2_1 z_1+ \\gamma^2_2 z_2+ \\gamma^2_3 z_3+\\lambda^2_1 x_1 + \\lambda^2_2 x_2 +  \\lambda^2_3 x_3  + v_2 \\rightarrow \\hat y_1\n\\end{aligned}\n\\]\nSecond Stage:\n\\[y = a_0 + a_1 \\hat y_1 + a_2 \\hat y_2 + b_1 x_1 + b_2 x_2 +  b_3 x_3 + e\n\\]\nThis last model should work, because \\(\\hat y_1\\) and \\(\\hat y_2\\) are exogenous (if \\(Zs\\) are).\nStandard Errors, however, need to be adjusted appropietly. (all two-step estimators need this)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#matrix-math",
    "href": "rmethods/8_iv2sls.html#matrix-math",
    "title": "Instrumental Variables and 2SLS",
    "section": "Matrix Math",
    "text": "Matrix Math\n\\[\\begin{aligned}\nX &= \\begin{bmatrix} 1 & y_k & x  \\end{bmatrix} \\\\\nZ &=\\begin{bmatrix} 1 & z   & x \\end{bmatrix}\n\\end{aligned}\n\\]\nThen\n\\[\\begin{aligned}\n\\beta_{2sls} &= [X'Z(Z'Z)^{-1}Z'X]^{-1}[X'Z(Z'Z)^{-1}Z'y] \\\\\n\\beta_{2sls} &= [X'P_z X]^{-1}[X'P_z y] \\\\\n\\beta_{2sls} &= [\\hat X'X]^{-1}[\\hat X'y]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#multicolinearity-and-2sls",
    "href": "rmethods/8_iv2sls.html#multicolinearity-and-2sls",
    "title": "Instrumental Variables and 2SLS",
    "section": "Multicolinearity and 2sls",
    "text": "Multicolinearity and 2sls\n\nWhen Appplying 2sls, the problem of Multicolinearity could be stronger.\n\nBecause of MCL, Standard errors will increase (less individual variation.)\nBecause of IV, only a fraction of the variation is used for the analysis.\nBotton line: Combined have conisderate results."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#weak-instruments",
    "href": "rmethods/8_iv2sls.html#weak-instruments",
    "title": "Instrumental Variables and 2SLS",
    "section": "Weak Instruments",
    "text": "Weak Instruments\n\nWeak Instruments create problems when using IV’s and 2SLS.\n\nA weak instrument is one that doesnt have much explanatory power on dep variable, once all other controls are taken into account.\nIf the instrument is too weak, the bias it generates could larger than OLS.\nThe distribution of coefficent is no longer normal, so its harder to make inference.\n\n\n\nHow do we test for it?"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section-2",
    "href": "rmethods/8_iv2sls.html#section-2",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "We usually test for weak instruments when analyzing the “first-stage” regression.\n\\[y_2 = \\gamma_0 + \\gamma_1 z_1 + \\gamma_2 z_2 + \\gamma_3 x_1 + \\gamma_4 x_2 + e\n\\]\n\nThe null is: \\(H_0: \\gamma_1 = \\gamma_2 =0\\), or the instruments are weak.\nBased on Stock and Yogo (2005), the general recommendation is to get an F&gt;10, to reject the Null.\nHowever, new evidence and research suggest that F=10 is not large enough.\n\nOne should either use F&gt;104 (To keep the same t) (Lee McCrary Moreira Porter, 2020)\nor use an alternative t-critica (3.4 for an F=10)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#examples",
    "href": "rmethods/8_iv2sls.html#examples",
    "title": "Instrumental Variables and 2SLS",
    "section": "Examples",
    "text": "Examples\n\n\nCode\n/*\n* This code is only to show the process. Too long to run in quarto\ncapture program drop simx \nprogram simx, eclass\nclear\nlocal N `1'\nlocal F `2'\nlocal sig = sqrt(`N'/ `F')\nset obs `N'\ngen e = rnormal()\ngen z = rnormal() \ngen x = 1 + z + (e+rnormal())*sqrt(.5)*`sig'\n*sqrt(10) \ngen y = 1 + (e+rnormal())*sqrt(.5)\nreg x z, \nmatrix b=(_b[z]/_se[z])^2\nivreg y (x=z), \nmatrix b=b,_b[x],_b[x]/_se[x]\nereturn post b\nend\n\ntempfile f1 f2 f3 f4 f5\nparallel initialize 14\nparallel sim, reps(5000): simx 500 10\ngen F=10\nsave `f1'\nparallel sim, reps(5000): simx 500 20\ngen F=20\nsave `f2'\nparallel sim, reps(5000): simx 500 40\ngen F=40\nsave `f3'\nparallel sim, reps(5000): simx 500 80\ngen F=80\nsave `f4'\nparallel sim, reps(5000): simx 500 160\ngen F=160\nsave `f5'\n\nclear \nappend using `f1'\nappend using `f2'\nappend using `f3'\nappend using `f4'\nappend using `f5'\n\nren (*) (f_stat b_coef t_stat)\n*/\nuse mdata/ivweak, clear\nset scheme white2\ncolor_style tableau\n\njoy_plot t_stat, over(F) xline(-1.96 1.96) xtitle(t-Stat) dadj(2)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#iv-for-measurement-errors-two-wrongs-make-one-right",
    "href": "rmethods/8_iv2sls.html#iv-for-measurement-errors-two-wrongs-make-one-right",
    "title": "Instrumental Variables and 2SLS",
    "section": "IV for Measurement errors: Two wrongs make one right",
    "text": "IV for Measurement errors: Two wrongs make one right\n\nAs described previously when independent variables have measurement errors (Classical), using the variable with errors will produced biased coefficients.\nIf you have multiple variables with Mesurement error, however, its possible to use IV to correct the problem.\n\nThis is done by using one variable as the instrument of the other.\n\nFOr this strategy to work, we need the error to be classical. That is, Uncorrelated across each other, and unrelated to the model error.\n\nConsider the following:\n\\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + e \\\\\n\\check x_1 &= x_1 + u_1 \\\\\n\\tilde x_1 &= x_1 + u_2 \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section-3",
    "href": "rmethods/8_iv2sls.html#section-3",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "If we use each model, independently, we will have biased coefficients\nIf we combined the, Biases will remain present (albeit lower)\nIf we use one as instrument of the other tho:\n\n\\[\n\\begin{aligned}\n\\beta_{1,iv} &= \\frac{cov(\\check x_1, y)}{cov(\\check x_1, \\tilde x_1)} or \\beta_{1,iv} = \\frac{cov(\\tilde x_1, y)}{cov(\\check x_1, \\tilde x_1)} \\\\\n& = \\frac{cov(x_1 + u_1, \\beta_0 + \\beta_1 x_1 + e)}{cov(x_1 + u_2, x_1 + u_1)} \\\\\n& = \\frac{\\beta_1 cov(x_1,x_1) + cov(x_1, e) + \\beta_1 cov(u_1, x_1) + cov(x_1,e)}{cov(x_1 , x_1 )} \\\\\n& = \\beta_1\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#example-1",
    "href": "rmethods/8_iv2sls.html#example-1",
    "title": "Instrumental Variables and 2SLS",
    "section": "Example",
    "text": "Example\n\n\nCode\nclear\nset obs 1000\ngen x = rnormal()\ngen y = 1 + x + rnormal() \ngen x1 = x + rnormal()\ngen x2 = x + rnormal()\nqui: regress y x\nest sto m1\nqui: regress y x1\nest sto m2\nqui: regress y x2\nest sto m3\ngen x1_x2 = (x1 + x2)/2\nqui: regress y x1_x2\nest sto m3b\nqui:ivregress 2sls  y (x1=x2)\nest sto m4\nqui:ivregress 2sls  y (x2=x1)\nest sto m5\nesttab m1 m2 m3 m3b m4 m5, se\n\n\nNumber of observations (_N) was 0, now 1,000.\n\n------------------------------------------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)             (5)             (6)   \n                        y               y               y               y               y               y   \n------------------------------------------------------------------------------------------------------------\nx                   1.006***                                                                                \n                 (0.0320)                                                                                   \n\nx1                                  0.520***                                        1.018***                \n                                 (0.0277)                                        (0.0640)                   \n\nx2                                                  0.502***                                        1.038***\n                                                 (0.0277)                                        (0.0654)   \n\nx1_x2                                                               0.682***                                \n                                                                 (0.0301)                                   \n\n_cons               1.010***        0.996***        0.988***        0.983***        0.974***        0.956***\n                 (0.0313)        (0.0380)        (0.0384)        (0.0360)        (0.0438)        (0.0451)   \n------------------------------------------------------------------------------------------------------------\nN                    1000            1000            1000            1000            1000            1000   \n------------------------------------------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#testing-for-endogeneity-and-overidentifying-restrictions",
    "href": "rmethods/8_iv2sls.html#testing-for-endogeneity-and-overidentifying-restrictions",
    "title": "Instrumental Variables and 2SLS",
    "section": "Testing for Endogeneity and Overidentifying restrictions",
    "text": "Testing for Endogeneity and Overidentifying restrictions\n\nIn general, Instrumental variables are a great tool to deal with A4 violations.\n\nbut, it can be hard to find the perfect IV, and you may still have problems if its Weak.\n\nIn that case, it may be useful to asnwer…Do you have an Endogeneity problem? (empirically rather than theoretically)\n\nTest:\n\nEstimate first stage, and save predicted residuals \\(\\hat v\\):\n\n\\[y_2 = \\gamma_0 + \\gamma_1 z_1 + \\gamma_2 x_1 + \\gamma_3 x_2 + v\n\\]\nYou expect residuals to be endogenous\n\nEstimate main model “adding” the residuals first stage\n\n\\[y = \\beta_0 + \\beta_1 y_2 + \\beta_2 x_1 + \\beta_3 x_2 + \\theta \\hat v + e\n\\]\nTest for \\(H_0: \\theta=0\\)."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section-4",
    "href": "rmethods/8_iv2sls.html#section-4",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "If the model was endogenous, Then \\(\\theta\\) will be different from zero, and \\(\\beta's\\) different from the case without controlling for it.\nOtherwise, we reject presence of endogeneity.\nThis method has the added advantage:\nFor the simple and exactly identified case, 2sls and adding residuals provide the same solution, except for SE. (its called Control function approach)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#overidentifying-restrictions",
    "href": "rmethods/8_iv2sls.html#overidentifying-restrictions",
    "title": "Instrumental Variables and 2SLS",
    "section": "overidentifying restrictions",
    "text": "overidentifying restrictions\n\nSome times, you may have access to multiple potential instruments.\n\nBut what if this instruments give you different results?\nIf you expect/believe a single effect exists, then there may be a problem. (one of they may be endogenous)\n\nSo how do we test if the instruments are exogenous?\n\nFirst you need more instruments than endogenous variables."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section-5",
    "href": "rmethods/8_iv2sls.html#section-5",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "S1. Estimate Structural Equation \\[y=\\beta_0 + \\gamma y_2 + \\beta_1 x_1 + e | y_2 \\sim z_1, z_2\n\\]\nS2. Auxiliary equation \\[\n\\hat e = \\delta_0 + \\delta_1 z_1 + \\delta_2 z2 + \\delta_3 x_1 + v\n\\]\nS3. Test for Overall Fitness. \\(nR^2\\sim \\chi^2(q_{iv})\\) with \\(q_{iv} = \\# over IVs\\)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#ivs-as-lates",
    "href": "rmethods/8_iv2sls.html#ivs-as-lates",
    "title": "Instrumental Variables and 2SLS",
    "section": "IV’s as LATES",
    "text": "IV’s as LATES\nNOTE\n\n\n\n\n\n\nImportant\n\n\nWhen describing this test, I mentioned that one would be typically worried if observing multiple coefficients when using different IV’s.\nHowever, IV’s can identify different effects, because different IV’s may affect different people differently.\n\\(Z1\\) may affect, say, only men. \\(z_2\\) only women, \\(z_3\\) only highly educated, etc.\nWhen analyzing IV’s will be important to undertand who would be affected by the instrument the most, because that may explain why effects vary."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#example-2",
    "href": "rmethods/8_iv2sls.html#example-2",
    "title": "Instrumental Variables and 2SLS",
    "section": "Example",
    "text": "Example\nIgnoring Potential endogeneity\n\nfrause mroz, clear\ndrop if lwage==.\n** But with Endogeneity\nreg lwage educ exper expersq\n\n(325 observations deleted)\n\n      Source |       SS           df       MS      Number of obs   =       428\n-------------+----------------------------------   F(3, 424)       =     26.29\n       Model |  35.0222967         3  11.6740989   Prob &gt; F        =    0.0000\n    Residual |  188.305144       424  .444115906   R-squared       =    0.1568\n-------------+----------------------------------   Adj R-squared   =    0.1509\n       Total |  223.327441       427  .523015084   Root MSE        =    .66642\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .1074896   .0141465     7.60   0.000     .0796837    .1352956\n       exper |   .0415665   .0131752     3.15   0.002     .0156697    .0674633\n     expersq |  -.0008112   .0003932    -2.06   0.040    -.0015841   -.0000382\n       _cons |  -.5220406   .1986321    -2.63   0.009    -.9124667   -.1316144\n------------------------------------------------------------------------------\n\n\nFirst Stage for different IVs\n\nqui: reg educ fatheduc exper expersq\npredict r1 , res\ntest fatheduc\nadde scalar fiv = r(F)\nadde scalar pfiv = r(p)\nest sto m1\nqui: reg educ motheduc exper expersq\npredict r2 , res\ntest motheduc\nadde scalar fiv = r(F)\nadde scalar pfiv = r(p)\nest sto m2\nqui: reg educ fatheduc motheduc exper expersq\npredict r3 , res\ntest motheduc fatheduc\nadde scalar fiv = r(F)\nadde scalar pfiv = r(p)\nest sto m3\n\nesttab m1 m2 m3 , scalar(fiv pfiv) sfmt(%5.2f %5.3f) order(fatheduc motheduc exper expersq) se ///\nstar(* 0.1 ** 0.05 *** 0.01)\n\n\n ( 1)  fatheduc = 0\n\n       F(  1,   424) =   87.74\n            Prob &gt; F =    0.0000\n\n ( 1)  motheduc = 0\n\n       F(  1,   424) =   73.95\n            Prob &gt; F =    0.0000\n\n ( 1)  motheduc = 0\n ( 2)  fatheduc = 0\n\n       F(  2,   423) =   55.40\n            Prob &gt; F =    0.0000\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                     educ            educ            educ   \n------------------------------------------------------------\nfatheduc            0.271***                        0.190***\n                 (0.0289)                        (0.0338)   \n\nmotheduc                            0.268***        0.158***\n                                 (0.0311)        (0.0359)   \n\nexper              0.0468          0.0489          0.0452   \n                 (0.0411)        (0.0417)        (0.0403)   \n\nexpersq          -0.00115        -0.00128        -0.00101   \n                (0.00123)       (0.00124)       (0.00120)   \n\n_cons               9.887***        9.775***        9.103***\n                  (0.396)         (0.424)         (0.427)   \n------------------------------------------------------------\nN                     428             428             428   \nfiv                 87.74           73.95           55.40   \npfiv                0.000           0.000           0.000   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01\n\n\nUsing parents education as instruments\n\n* SMALL requests Df adjustment\nqui:ivregress 2sls lwage (educ= fatheduc ) exper expersq, small\nest sto m1\nqui:ivregress 2sls lwage (educ= motheduc ) exper expersq, small\nest sto m2\nqui:ivregress 2sls lwage (educ= fatheduc motheduc) exper expersq, small\nest sto m3\nesttab m1 m2 m3 , se mtitle(IV:Father IV:Mother IV:Parents) ///\nstar(* 0.1 ** 0.05 *** 0.01)\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                IV:Father       IV:Mother      IV:Parents   \n------------------------------------------------------------\neduc               0.0702**        0.0493          0.0614*  \n                 (0.0344)        (0.0374)        (0.0314)   \n\nexper              0.0437***       0.0449***       0.0442***\n                 (0.0134)        (0.0136)        (0.0134)   \n\nexpersq         -0.000882**     -0.000922**     -0.000899** \n               (0.000401)      (0.000406)      (0.000402)   \n\n_cons             -0.0611           0.198          0.0481   \n                  (0.436)         (0.473)         (0.400)   \n------------------------------------------------------------\nN                     428             428             428   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01\n\n\n** Testing for endogeneity\n\n* Instrumenting education\nqui:reg  lwage educ  exper expersq r1\nest sto m1\nqui:reg  lwage educ  exper expersq r2\nest sto m2\nqui:reg  lwage educ  exper expersq r3\nest sto m3\n\nesttab m1 m2 m3 , se mtitle(IV:Father IV:Mother IV:Parents) ///\nstar(* 0.1 ** 0.05 *** 0.01)\n\n*see -estat endogenous- after ivregress 2sls\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                IV:Father       IV:Mother      IV:Parents   \n------------------------------------------------------------\neduc               0.0702**        0.0493          0.0614** \n                 (0.0341)        (0.0366)        (0.0310)   \n\nexper              0.0437***       0.0449***       0.0442***\n                 (0.0133)        (0.0133)        (0.0132)   \n\nexpersq         -0.000882**     -0.000922**     -0.000899** \n               (0.000397)      (0.000398)      (0.000396)   \n\nr1                 0.0450                                   \n                 (0.0375)                                   \n\nr2                                 0.0684*                  \n                                 (0.0397)                   \n\nr3                                                 0.0582*  \n                                                 (0.0348)   \n\n_cons             -0.0611           0.198          0.0481   \n                  (0.433)         (0.463)         (0.395)   \n------------------------------------------------------------\nN                     428             428             428   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01\n\n\nOverid Test\n\nqui:ivregress 2sls lwage (educ= fatheduc motheduc) exper expersq, small\npredict resid, res\nestat overid \nreg resid fatheduc motheduc exper expersq, notable robust\n\n\n  Tests of overidentifying restrictions:\n\n  Sargan (score) chi2(1) =  .378071  (p = 0.5386)\n  Basmann chi2(1)        =  .373985  (p = 0.5408)\n\nLinear regression                               Number of obs     =        428\n                                                F(4, 423)         =       0.11\n                                                Prob &gt; F          =     0.9791\n                                                R-squared         =     0.0009\n                                                Root MSE          =     .67521"
  },
  {
    "objectID": "rmethods/6_Heteros.html#what-is-heteroskedasticity",
    "href": "rmethods/6_Heteros.html#what-is-heteroskedasticity",
    "title": "Multiple Regression Analysis",
    "section": "What is Heteroskedasticity?",
    "text": "What is Heteroskedasticity?\n\nMathematically: \\[Var(e|x=c_1)\\neq Var(e|x=c_2)\\]\nThis means: the conditional variance of the errors is not constant across control characteristics."
  },
  {
    "objectID": "rmethods/6_Heteros.html#consequences",
    "href": "rmethods/6_Heteros.html#consequences",
    "title": "Multiple Regression Analysis",
    "section": "Consequences",
    "text": "Consequences\nWhat happens when you have heteroskedastic errors?\n\nIn terms of \\(\\beta's\\) and \\(R^2\\) and \\(R^2_{adj}\\), nothing. Coefficients and Goodness of fit are still unbiased and consistent.\nBut, Coefficients standard errors are based on the simplifying assumption of normality. Thus Variances will be bias!.\n\nIf variances are biased, then all statistics will be wrong."
  },
  {
    "objectID": "rmethods/6_Heteros.html#section",
    "href": "rmethods/6_Heteros.html#section",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "How bad can it be?\nSetup:\n\\(y = e\\) where \\(e \\sim N(0,\\sigma_e^2h(x))\\)\n\\(x = uniform(-1,1)\\)\n\n\nCode\n/*capture program drop sim_het\nprogram sim_het, eclass\n    clear\n    set obs 500 \n    gen x = runiform(-1,1)\n    gen u = rnormal()\n    ** Homoskedastic\n    gen y_1 = u*2\n    ** increasing first, decreasing later\n    gen y_4 = u*sqrt(9*abs(x))\n    replace x = x-2\n    reg y_1 x\n    matrix b=_b[x],_b[x]/_se[x]\n    reg y_4 x\n    matrix b=b,_b[x],_b[x]/_se[x]\n    matrix coleq   b = h0 h0 h3 h3 \n    matrix colname b = b  t  b  t \n    ereturn post b\nend\nqui:simulate , reps(1000) dots(100):sim_het\nsave mdata/simulate.dta, replace*/\nuse mdata/simulate.dta, replace\ntwo (kdensity h0_b_t) (kdensity h3_b_t) ///\n    (function y = normalden(x), range(-4 4) lw(2) color(gs5%50)), ///\n    legend(order(3 \"Normal\" 1 \"With Homoskedasticty\" 2 \"with Heteroskedasticity\"))\ngraph export images/fig6_1.png, replace height(1000)"
  },
  {
    "objectID": "rmethods/6_Heteros.html#what-to-do-about-it-1",
    "href": "rmethods/6_Heteros.html#what-to-do-about-it-1",
    "title": "Multiple Regression Analysis",
    "section": "What to do about it?",
    "text": "What to do about it?\n\nSo, If errors are heteroskedastic, then all statistics (t-stats, F-stats, chi2’s) are wrong.\nBut, there are solutions…many solutions\n\nGLS: Generalized Least Squares\nWLS: Weighted Least Squares\nFGLS: Feasible Generealized Least Squares\nWFLS: Weighted FGLS\nHC0-HC3: Heteroskedasticity consistent SE\n\nSome of them are more involved than others.\nBut before trying to do that, lets first ask…do we have a problem?"
  },
  {
    "objectID": "rmethods/6_Heteros.html#detecting-the-problem",
    "href": "rmethods/6_Heteros.html#detecting-the-problem",
    "title": "Multiple Regression Analysis",
    "section": "Detecting the Problem",
    "text": "Detecting the Problem\n\nConsider the model:\n\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +\\beta_3 x_3 +e\n\\]\n\nWe usually start with the assumption that errors are homoskedastic \\(Var(e|x's)=\\sigma^2_c\\).\nHowever, now we want to allow for the possibility of heteroskedasiticity. ie, that variance is some function of X.\n\nWe have to test if the conditional variance is a function that varies with \\(x\\):\n\n\n\\[Var(e|x)=f(x_1,x_2,\\dots,x_k) \\sim a_0+a_1x_1 + a_2 x_2 + \\dots + a_k x_k+v\\]"
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-1",
    "href": "rmethods/6_Heteros.html#section-1",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "\\[Var(e|x)=f(x_1,x_2,\\dots,x_k) \\sim a_0+a_1x_1 + a_2 x_2 + \\dots + a_k x_k+v\\]\n\nThis expression says the conditional variance can vary with \\(X's\\).\nIt could be as flexible as needed, but linear is usually enough.\n\nWith this the Null hypothesis is: \\[H_0: a_1 = a_2 = \\dots = a_k=0 \\text{ vs } H_1: H_0 \\text{ is false}\n\\]\nEasy enough, but do we KNOW \\(Var(e|x)\\) ? can we model the equation?"
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-2",
    "href": "rmethods/6_Heteros.html#section-2",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "We don’t!.\n\nBut we can use \\(\\hat e^2\\) instead. The assumption is that \\(\\hat e^2\\) is a good enough approximation for the condional variance \\(Var(e|x)\\).\nWith this, the test for heteroskedasticty can be implemented using the following recipe.\n\n\nEstimate \\(y=x\\beta+e\\) and obtain predicted model errors \\(\\hat e\\).\nModel \\(\\hat e^2 = \\color{green}{h(x)}+v\\), as a proxy for the variance model.\n\n\\(h(x)\\) could be estimated using some linear or nonlinear functional forms.\n\nTest if conditional variance changes with respect to any explanatory variables.\n\nThe null is H0: Errors are Homoskedastic. Rejection the error suggests you have Heteroskedasticity.\nNote: Depending on Model specification, and test used, there are various Heteroskedasticity tests."
  },
  {
    "objectID": "rmethods/6_Heteros.html#heteroskedasticity-tests",
    "href": "rmethods/6_Heteros.html#heteroskedasticity-tests",
    "title": "Multiple Regression Analysis",
    "section": "Heteroskedasticity tests:",
    "text": "Heteroskedasticity tests:\n\\[\\begin{aligned}\n\\text{Model}: y &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + e \\\\\n\\hat e & = y - (\\hat \\beta_0 + \\hat\\beta_1 x_1 +\\hat \\beta_2 x_2 +\\hat \\beta_3 x_3)\n\\end{aligned}\n\\]\nBreusch-Pagan test:\n\\[\\begin{aligned}\n\\hat e^2 & = \\gamma_0 + \\gamma_1 x_1 +\\gamma_2 x_2 +\\gamma_3 x_3 + v \\\\\nH_0 &: \\gamma_1=\\gamma_2=\\gamma_3=0 \\\\\nF &= \\frac{R^2_{\\hat e^2}/k}{(1-R^2_{\\hat e^2})/(n-k-1)} \\\\\nLM &=N R^2_{\\hat e^2} \\sim \\chi^2(k) \\leftarrow BP-test\n\\end{aligned}\n\\]\n\nEasy and simple, but only considers “linear” Heteroskedasticity"
  },
  {
    "objectID": "rmethods/6_Heteros.html#heteroskedasticity-tests-1",
    "href": "rmethods/6_Heteros.html#heteroskedasticity-tests-1",
    "title": "Multiple Regression Analysis",
    "section": "Heteroskedasticity tests:",
    "text": "Heteroskedasticity tests:\n\\[\\begin{aligned}\n\\text{Model}: y &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + e \\\\\n\\hat e & = y - (\\hat \\beta_0 + \\hat\\beta_1 x_1 +\\hat \\beta_2 x_2 +\\hat \\beta_3 x_3)\n\\end{aligned}\n\\]\nWhite:\n\\[\\begin{aligned}\n\\hat e^2 & = \\gamma_0 + \\sum \\gamma_{1,k} x_k + \\sum \\gamma_{2,k} x_k^2 + \\sum_k \\sum_{j\\neq k} \\gamma_{3,j,k} x_j x_k + v \\\\\nH_0 &: \\text{ All } \\gamma's =0 \\\\\nF &= \\frac{R^2_{\\hat e^2}/q}{(1-R^2_{\\hat e^2})/(n-q-1)} \\\\\nLM &=N R^2_{\\hat e^2} \\sim \\chi^2(q)\n\\end{aligned}\n\\]\n\\(q\\) is the total number of coefficients in the model (not counting the intercept.)\n\nAccounts for nonlinearities, but gets “messy” with more variables."
  },
  {
    "objectID": "rmethods/6_Heteros.html#heteroskedasticity-tests-2",
    "href": "rmethods/6_Heteros.html#heteroskedasticity-tests-2",
    "title": "Multiple Regression Analysis",
    "section": "Heteroskedasticity tests:",
    "text": "Heteroskedasticity tests:\n\\[\\begin{aligned}\n\\text{Model}: y &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + e \\\\\n\\hat e & = y - (\\hat \\beta_0 + \\hat\\beta_1 x_1 +\\hat \\beta_2 x_2 +\\hat \\beta_3 x_3)\n\\end{aligned}\n\\]\nModified White:\n\\[\\begin{aligned}\n\\hat y &= y - \\hat e \\\\\n\\hat e^2 & = \\gamma_0 + \\gamma_1 \\hat y + \\gamma_2 \\hat y^2 + \\dots + v \\\\\nH_0 &: \\gamma_1 = \\gamma_2 = \\dots =0 \\\\\nF &= \\frac{R^2_{\\hat e^2}/ h }{(1-R^2_{\\hat e^2})/(n-h-1)} \\\\\nLM &=N R^2_{\\hat e^2} \\sim \\chi^2(h)\n\\end{aligned}\n\\]\n\\(h\\) is the total number of coefficients in the model (not counting the intercept.)\n\nAccounts for nonlinearities (because of how \\(\\hat y\\) is constructed), and is simpler to implement.\nBut, nonlinearity is restricted."
  },
  {
    "objectID": "rmethods/6_Heteros.html#example",
    "href": "rmethods/6_Heteros.html#example",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\nHousing prices:\n\\[\\begin{aligned}\nprice &= \\beta_0 + \\beta_1 lotsize + \\beta_2 sqft + \\beta_3 bdrms + e_1 \\\\\nlog(price) &= \\beta_0 + \\beta_1 log(lotsize) + \\beta_2 log(sqft) + \\beta_3 bdrms + e_2 \\\\\n\\end{aligned}\n\\]\n\nfrause hprice1, clear\nreg price lotsize sqrft bdrms \npredict res, res\npredict price_hat\ngen res2=res^2\ndisplay \"BP-test\"\nreg res2  lotsize sqrft bdrms, notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\ndisplay \"White Test\"\nreg res2  c.(lotsize sqrft bdrms)##c.(lotsize sqrft bdrms), notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\ndisplay \"MWhite Test\"\nreg res2  price_hat c.price_hat#c.price_hat, notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(3, 84)        =     57.46\n       Model |  617130.701         3  205710.234   Prob &gt; F        =    0.0000\n    Residual |  300723.805        84   3580.0453   R-squared       =    0.6724\n-------------+----------------------------------   Adj R-squared   =    0.6607\n       Total |  917854.506        87  10550.0518   Root MSE        =    59.833\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     lotsize |   .0020677   .0006421     3.22   0.002     .0007908    .0033446\n       sqrft |   .1227782   .0132374     9.28   0.000     .0964541    .1491022\n       bdrms |   13.85252   9.010145     1.54   0.128    -4.065141    31.77018\n       _cons |  -21.77031   29.47504    -0.74   0.462    -80.38466    36.84405\n------------------------------------------------------------------------------\n(option xb assumed; fitted values)\nBP-test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(3, 84)        =      5.34\n       Model |   701213780         3   233737927   Prob &gt; F        =    0.0020\n    Residual |  3.6775e+09        84  43780003.5   R-squared       =    0.1601\n-------------+----------------------------------   Adj R-squared   =    0.1301\n       Total |  4.3787e+09        87  50330276.7   Root MSE        =    6616.6\n\nnR^2: 14.092386\np(chi2) 0.003\nWhite Test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(9, 78)        =      5.39\n       Model |  1.6784e+09         9   186492378   Prob &gt; F        =    0.0000\n    Residual |  2.7003e+09        78    34619265   R-squared       =    0.3833\n-------------+----------------------------------   Adj R-squared   =    0.3122\n       Total |  4.3787e+09        87  50330276.7   Root MSE        =    5883.8\n\nnR^2: 33.731659\np(chi2) 0.000\nMWhite Test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(2, 85)        =      9.64\n       Model |   809489395         2   404744697   Prob &gt; F        =    0.0002\n    Residual |  3.5692e+09        85  41991113.9   R-squared       =    0.1849\n-------------+----------------------------------   Adj R-squared   =    0.1657\n       Total |  4.3787e+09        87  50330276.7   Root MSE        =    6480.1\n\nnR^2: 16.268416\np(chi2) 0.000\n\n\n\nfrause hprice1, clear\nreg lprice llotsize lsqrft bdrms \npredict res, res\npredict price_hat\ngen res2=res^2\ndisplay \"BP-test\"\nreg res2  llotsize lsqrft bdrms, notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\ndisplay \"White Test\"\nreg res2  c.(llotsize lsqrft bdrms)##c.(llotsize lsqrft bdrms), notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\ndisplay \"MWhite Test\"\nreg res2  price_hat c.price_hat#c.price_hat, notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(3, 84)        =     50.42\n       Model |  5.15504028         3  1.71834676   Prob &gt; F        =    0.0000\n    Residual |  2.86256324        84  .034078134   R-squared       =    0.6430\n-------------+----------------------------------   Adj R-squared   =    0.6302\n       Total |  8.01760352        87  .092156362   Root MSE        =     .1846\n\n------------------------------------------------------------------------------\n      lprice | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    llotsize |   .1679667   .0382812     4.39   0.000     .0918404     .244093\n      lsqrft |   .7002324   .0928652     7.54   0.000     .5155597    .8849051\n       bdrms |   .0369584   .0275313     1.34   0.183    -.0177906    .0917074\n       _cons |  -1.297042   .6512836    -1.99   0.050    -2.592191    -.001893\n------------------------------------------------------------------------------\n(option xb assumed; fitted values)\nBP-test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(3, 84)        =      1.41\n       Model |  .022620168         3  .007540056   Prob &gt; F        =    0.2451\n    Residual |  .448717194        84  .005341871   R-squared       =    0.0480\n-------------+----------------------------------   Adj R-squared   =    0.0140\n       Total |  .471337362        87  .005417671   Root MSE        =    .07309\n\nnR^2: 4.2232484\np(chi2) 0.238\nWhite Test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(9, 78)        =      1.05\n       Model |  .051147864         9  .005683096   Prob &gt; F        =    0.4053\n    Residual |  .420189497        78  .005387045   R-squared       =    0.1085\n-------------+----------------------------------   Adj R-squared   =    0.0057\n       Total |  .471337362        87  .005417671   Root MSE        =     .0734\n\nnR^2: 9.5494489\np(chi2) 0.388\nMWhite Test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(2, 85)        =      1.73\n       Model |  .018464046         2  .009232023   Prob &gt; F        =    0.1830\n    Residual |  .452873315        85  .005327921   R-squared       =    0.0392\n-------------+----------------------------------   Adj R-squared   =    0.0166\n       Total |  .471337362        87  .005417671   Root MSE        =    .07299\n\nnR^2: 3.4472889\np(chi2) 0.178\n\n\nCan you do this in Stata? Yes, estat hettest. But look into the options. There are many more options in that command."
  },
  {
    "objectID": "rmethods/6_Heteros.html#what-do-you-do-when-you-have-heteroskedasticity",
    "href": "rmethods/6_Heteros.html#what-do-you-do-when-you-have-heteroskedasticity",
    "title": "Multiple Regression Analysis",
    "section": "What do you do when you have Heteroskedasticity?",
    "text": "What do you do when you have Heteroskedasticity?\nWe need to fix!\n\nRecall, the problem is that \\(Var(e|X)\\neq c\\)\nThis affects how standard errors are estimated (we required homoskedasticity). But what happens when Homoskedasticity doesnt hold?\n\nWe can “fix/change” the model, so its no longer heteroskedastic, and Standard Inference works. (FGLS, WLS)\nWe neec to account for heteroskedasticity when estimating the variance covariance model.\n\n\nSo lets learn to Fix it first"
  },
  {
    "objectID": "rmethods/6_Heteros.html#how-do-we-fix-heteroskedasticity",
    "href": "rmethods/6_Heteros.html#how-do-we-fix-heteroskedasticity",
    "title": "Multiple Regression Analysis",
    "section": "How do we Fix Heteroskedasticity?",
    "text": "How do we Fix Heteroskedasticity?\n\nIn order to address the problem of heteroskedasticity, we require knowledge of why the model is heteroskedastic, or what is generating it.\n\n\\[Var(e|X)=h(x)\\sigma^2_e\n\\]\n\nWhere \\(h(x)\\) is the “source” of heteroskedasticity, which may be a known or estimated function of \\(x\\).\n\nWhich should be an strictly possitive function of \\(x's\\)."
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-4",
    "href": "rmethods/6_Heteros.html#section-4",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Knowledge is power\n\nIf you know \\(h(x)\\), correcting heteroskedasticity is “easy”. Consider the following:\n\n\\[\\begin{aligned}\ny &= b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3 +e \\\\\nVar(e|x) &=x_1 \\sigma_e^2 || h(x)=x_1\n\\end{aligned}\n\\]\nYou can correct Heteroskedasticity in two ways:\n\nTransform model by dividing everything by \\(\\sqrt{h(x)}\\): \\[\\begin{aligned}\n\\frac{y}{\\sqrt{x_1}} &= b_0 \\frac{1}{\\sqrt{x_1}}+ b_1 \\sqrt{x_1} + b_2 \\frac{x_2}{\\sqrt{x_1}} + b_3 \\frac{x_3}{\\sqrt{x_1}} +\\frac{e}{\\sqrt{x_1}} \\\\\nVar\\left(\\frac{e}{\\sqrt{x_1}}|x\\right) &= \\frac{1}{x_1} x_1\\sigma_e^2=\\sigma_e^2\n\\end{aligned}\n\\]\n\nThe new error is Homoskedastic (but has no constant)!"
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-5",
    "href": "rmethods/6_Heteros.html#section-5",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Estimate the model using by \\(\\frac{1}{h(x)}\\) as weights: \\[\\begin{aligned}\n\\beta=\\min_\\beta \\sum \\frac{1}{h(x)} (y-(b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3))^2\n\\end{aligned}\n\\]\n\n\nSame solution as before, and there is no need to “transform” data, or keep track of a constant.\nThis is often called WLS (weighted least squares) or GLS (Generalized Least Squares)."
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-6",
    "href": "rmethods/6_Heteros.html#section-6",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Interestingly: These approaches are more efficient than Standard OLS.\n\nUses more information (heteroskedasticity)\nMakes better use of information (More weight to better data) Standard errors are smaller.\n\nt-stats, F-stats, etc now are valid.\nCoefficients will NOT be the same as before.\n\\(R^2\\) is less useful\nHeteroskedasticty test on transformed data may required added work."
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-7",
    "href": "rmethods/6_Heteros.html#section-7",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "FGLS: We do not know \\(h(x)\\), but we can guess\n\nIf \\(h(x)\\) is not known, we can use an auxiliary model to estimate it:\n\n\\[\\begin{aligned}\nVar(e|x) &= \\sigma^2 \\exp(\\delta_0 + \\delta_1 x_1 + \\delta_2 x_2 +\\dots) \\exp v \\\\\n\\hat e^2 &= \\sigma^2 \\exp(\\delta_0 + \\delta_1 x_1 + \\delta_2 x_2 +\\dots) \\exp v \\\\\nlog(\\hat e^2) &= \\delta_0 + \\delta_1 x_1 + \\delta_2 x_2 +\\dots+ v \\\\\nlog(\\hat e^2) &= \\delta_0 + \\delta_1 \\hat y + \\delta_2 \\hat y^2 + \\dots+ v \\\\\n\\widehat{\\log h(x)} &= \\hat \\delta_0 + \\hat \\delta_1 x_1 + \\hat \\delta_2 x_2 + \\dots = x \\hat \\delta \\\\\n\\hat h(x) &= \\exp (x \\hat \\delta) \\text{ or } \\hat h(x)=e^{x \\hat \\delta}\n\\end{aligned}\n\\]\n\nProceed as before (weighted or transformed)\nIts call Feasible GLS, because we need to estimate \\(h(x)\\)."
  },
  {
    "objectID": "rmethods/6_Heteros.html#do-not-correct-account-for-it-gls-and-fgls",
    "href": "rmethods/6_Heteros.html#do-not-correct-account-for-it-gls-and-fgls",
    "title": "Multiple Regression Analysis",
    "section": "Do not Correct, account for it: GLS and FGLS",
    "text": "Do not Correct, account for it: GLS and FGLS\nRecall “Long” variance formula:\n\\[Var(\\beta)=\\color{brown}{(X'X)^{-1}}\\color{green}{X}'\\color{red}{Var(e|X)}\\color{green}{X}\\color{brown}{(X'X)^{-1}}\n\\]\n\nThe red part is a \\(N\\times N\\) VCOV matrix of ALL erros. It can be Simplified with what we know!\n\n\\[\\begin{aligned}\nVar_{gls/fgls}(\\beta)&=\\sigma^2_{\\tilde e} \\color{brown}{(X'X)^{-1}}\\color{green}{X}'\\color{red}{ \\Omega_h(x) }\\color{green}{X}\\color{brown}{(X'X)^{-1}} \\\\\n\\sigma^2_{\\tilde e} &= \\frac{1}{N-k-1} \\sum \\frac{\\hat e^2}{h(x)} \\\\\n\\Omega_h(x) [i,j] &= h(x_i) & \\text{ if } i=j \\\\\n& = 0 & \\text{ if } i\\neq j \\\\\n\\end{aligned}\n\\]\n\nSE are corrected, but coefficients remain the same!"
  },
  {
    "objectID": "rmethods/6_Heteros.html#do-not-correct-account-for-it-white-sandwich-formula",
    "href": "rmethods/6_Heteros.html#do-not-correct-account-for-it-white-sandwich-formula",
    "title": "Multiple Regression Analysis",
    "section": "Do not Correct, account for it: White Sandwich Formula",
    "text": "Do not Correct, account for it: White Sandwich Formula\n\nWhat if we do not want to even try guessing \\(h(x)\\)?\nyou can use Robust Standard errors!\n\nHeteroskedastic Consistent SE to Heterosedasticity of unknown form.\n\n\nLet me present to you, the Sandwitch Formula: \\[Var(\\beta)=c \\color{brown}{(X'X)^{-1}}\\color{green}{X}'\\color{red}{\\Omega}\\color{green}{X}\\color{brown}{(X'X)^{-1}}\n\\]\n\\[\\begin{aligned}\n\\Omega [i,j] &= \\hat e_i^2 & \\text{ if } i=j \\\\\n& = 0 & \\text{ if } i\\neq j \\\\\n\\end{aligned}\n\\]\n\nThe best approximation to conditional variance is equal to \\(\\hat e_i^2\\). (plus assuming no correlation)\nValid in large samples, but can be really bad in smaller ones.\nThere are other versions. See HC0 HC1 HC2 HC3."
  },
  {
    "objectID": "rmethods/6_Heteros.html#what-if-did-hx-and-it-was-wrong",
    "href": "rmethods/6_Heteros.html#what-if-did-hx-and-it-was-wrong",
    "title": "Multiple Regression Analysis",
    "section": "What if did \\(h(x)\\), and it was wrong",
    "text": "What if did \\(h(x)\\), and it was wrong\n\nUsing FGLS will change coefficients a bit. If they change a lot, It could indicate other assumptions in the model are incorrect. (functional form or exogeneity)\nIn either case, you could always combine FGLS with Robust Standard Errors!"
  },
  {
    "objectID": "rmethods/6_Heteros.html#statistical-inference",
    "href": "rmethods/6_Heteros.html#statistical-inference",
    "title": "Multiple Regression Analysis",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nIf applying GLS or FGLS via transformations or reweighting. All we did before is valid.\nIf using Robust standard errors (HC), t-stats are constructed as usual, but\nF-stats formulas are no longer valid.\n\nInstead…use the long formula\n\\[\\begin{aligned}\nH_0: & R_{q,k+1}\\beta_{k+1,1}=c_{q,1} \\\\\n\\Sigma_R &= R_{q,k+1} V^r_\\beta R'_{q,k+1} \\\\\nF-stat &= \\frac 1 q (R\\beta-c)' \\Sigma_R^{-1} (R\\beta-c)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-8",
    "href": "rmethods/6_Heteros.html#section-8",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Extra: Prediction and SE\nPrediction SE:\n\nIf you are using GLS, Formulas seen before apply with the following modification: \\(Var(e|X=x_0)=\\sigma^2_{\\tilde e} h(x_0)\\)\nIf you are using FGLS, its not that simple because of the two-step modeling\n\nFor Prediction with Logs\n\nYou need to take into account Heteroskedasticity\n\n\\[\\hat y_i = \\exp \\left( \\widehat{log y_i}+\\hat \\sigma_{\\tilde e}^2 \\hat h_i /2 \\right)\n\\]"
  },
  {
    "objectID": "rmethods/6_Heteros.html#example-1",
    "href": "rmethods/6_Heteros.html#example-1",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\n\nfrause smoke, clear\ngen age_40sq=(age-40)^2\n** Default\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn\nest sto m1\npredict cig_hat\npredict cig_res,res\n** GLS: h(x)=lincome Weighted\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn [aw=1/lincome]\nest sto m2\n** FGLS: h(x) = f(cigs_hat)\ngen lcres=log(cig_res^2)\nqui:reg lcres c.cig_hat##c.cig_hat##c.cig_hat \npredict aux\ngen hx=exp(aux)\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn [aw=1/hx]\nest sto m3\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn , robust\nest sto m4\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn [aw=1/lincome], robust\nest sto m5\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn [aw=1/hx], robust\nest sto m6\nset linesize 255\n\n\n\nCode\nesttab m1 m2 m3 m4 m5 m6, gaps mtitle(default GLS FGLS Rob GLS-Rob FGLS-Rob) ///\nnonum cell( b( fmt( 3) star ) se( par(( )) ) p( par([ ]) ) ) ///\nstar(* .1 ** 0.05 *** 0.01  )\n\n\n\n------------------------------------------------------------------------------------------------------------\n                  default             GLS            FGLS             Rob         GLS-Rob        FGLS-Rob   \n                   b/se/p          b/se/p          b/se/p          b/se/p          b/se/p          b/se/p   \n------------------------------------------------------------------------------------------------------------\nlincome             0.880           0.926           1.005**         0.880           0.926*          1.005   \n                  (0.728)         (0.672)         (0.422)         (0.596)         (0.559)         (0.651)   \n                  [0.227]         [0.169]         [0.017]         [0.140]         [0.098]         [0.123]   \n\nlcigpric           -0.751          -1.525          -4.572          -0.751          -1.525          -4.572   \n                  (5.773)         (5.696)         (4.260)         (6.035)         (6.067)         (9.651)   \n                  [0.897]         [0.789]         [0.284]         [0.901]         [0.802]         [0.636]   \n\neduc               -0.501***       -0.477***       -0.610***       -0.501***       -0.477***       -0.610***\n                  (0.167)         (0.166)         (0.115)         (0.162)         (0.159)         (0.115)   \n                  [0.003]         [0.004]         [0.000]         [0.002]         [0.003]         [0.000]   \n\nage                 0.049           0.048           0.041           0.049*          0.048           0.041   \n                  (0.034)         (0.033)         (0.026)         (0.030)         (0.029)         (0.032)   \n                  [0.146]         [0.147]         [0.108]         [0.099]         [0.101]         [0.197]   \n\nage_40sq           -0.009***       -0.009***       -0.007***       -0.009***       -0.009***       -0.007***\n                  (0.002)         (0.002)         (0.001)         (0.001)         (0.001)         (0.002)   \n                  [0.000]         [0.000]         [0.000]         [0.000]         [0.000]         [0.001]   \n\nrestaurn           -2.825**        -2.776**        -3.383***       -2.825***       -2.776***       -3.383***\n                  (1.112)         (1.108)         (0.722)         (1.008)         (0.992)         (0.696)   \n                  [0.011]         [0.012]         [0.000]         [0.005]         [0.005]         [0.000]   \n\n_cons              10.797          13.184          25.712          10.797          13.184          25.712   \n                 (24.145)        (23.656)        (17.120)        (25.401)        (25.478)        (41.539)   \n                  [0.655]         [0.577]         [0.134]         [0.671]         [0.605]         [0.536]   \n------------------------------------------------------------------------------------------------------------\nN                     807             807             807             807             807             807   \n------------------------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/6_Heteros.html#lpm-revised",
    "href": "rmethods/6_Heteros.html#lpm-revised",
    "title": "Multiple Regression Analysis",
    "section": "LPM revised",
    "text": "LPM revised\n\nWhat was wrong with LPM?\n\nFixed marginal effects (depends on functional form)\nMay predict p&gt;1 or p&lt;0\nIt is Heteroskedastic by construction\n\nBut now we know how to deal with this! GLS (why not FGLS) and Robust\nIn LPM: \\(Var(y|x)=p(x)(1-p(x)) = \\hat y (1-\\hat y)\\)\n\nWe can use this to transform or weight the data!\nOnly works if \\(0&lt;p(x)&lt;1\\)."
  },
  {
    "objectID": "rmethods/6_Heteros.html#lpm-example",
    "href": "rmethods/6_Heteros.html#lpm-example",
    "title": "Multiple Regression Analysis",
    "section": "LPM Example",
    "text": "LPM Example\n\nfrause gpa1, clear\n** LPM\ngen parcoll = (fathcoll | mothcoll)\nreg pc hsgpa act parcoll\npredict res_1, res\n\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =      1.98\n       Model |  1.40186813         3  .467289377   Prob &gt; F        =    0.1201\n    Residual |  32.3569971       137  .236182461   R-squared       =    0.0415\n-------------+----------------------------------   Adj R-squared   =    0.0205\n       Total |  33.7588652       140  .241134752   Root MSE        =    .48599\n\n------------------------------------------------------------------------------\n          pc | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .0653943   .1372576     0.48   0.635    -.2060231    .3368118\n         act |   .0005645   .0154967     0.04   0.971    -.0300792    .0312082\n     parcoll |   .2210541    .092957     2.38   0.019      .037238    .4048702\n       _cons |  -.0004322   .4905358    -0.00   0.999     -.970433    .9695686\n------------------------------------------------------------------------------\n\n\n\npredict pchat\ngen hx = pchat*(1-pchat)\nsum pchat hx\n\n(option xb assumed; fitted values)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       pchat |        141    .3971631    .1000667   .1700624   .4974409\n          hx |        141    .2294822    .0309768   .1411412   .2499934\n\n\n\nreg pc hsgpa act parcoll [w=1/hx]\npredict res_2, res\n\n(analytic weights assumed)\n(sum of wgt is 628.1830743667746)\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =      2.22\n       Model |  1.54663033         3  .515543445   Prob &gt; F        =    0.0882\n    Residual |  31.7573194       137  .231805251   R-squared       =    0.0464\n-------------+----------------------------------   Adj R-squared   =    0.0256\n       Total |  33.3039497       140  .237885355   Root MSE        =    .48146\n\n------------------------------------------------------------------------------\n          pc | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .0327029   .1298817     0.25   0.802    -.2241292     .289535\n         act |    .004272   .0154527     0.28   0.783    -.0262847    .0348286\n     parcoll |   .2151862   .0862918     2.49   0.014       .04455    .3858224\n       _cons |   .0262099   .4766498     0.05   0.956    -.9163323    .9687521\n------------------------------------------------------------------------------\n\n\n\n** Testing for Heteroskedasticity\nreplace res_1 = res_1^2\nreplace res_2 = res_2^2/hx\ndisplay \"Default\"\nreg res_1 hsgpa act parcoll, notable\ndisplay \"Weighted\"\nreg res_2 hsgpa act parcoll, notable\n\n(141 real changes made)\n(141 real changes made)\nDefault\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =      2.82\n       Model |  .133163365         3  .044387788   Prob &gt; F        =    0.0412\n    Residual |  2.15497574       137   .01572975   R-squared       =    0.0582\n-------------+----------------------------------   Adj R-squared   =    0.0376\n       Total |  2.28813911       140  .016343851   Root MSE        =    .12542\n\nWeighted\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =      0.63\n       Model |  .874194807         3  .291398269   Prob &gt; F        =    0.5980\n    Residual |  63.5472068       137  .463848225   R-squared       =    0.0136\n-------------+----------------------------------   Adj R-squared   =   -0.0080\n       Total |  64.4214016       140  .460152868   Root MSE        =    .68106"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#how-do-you-know-if-what-you-see-is-relevant",
    "href": "rmethods/4_MLRM_IA.html#how-do-you-know-if-what-you-see-is-relevant",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "How do you know if what you see is relevant?",
    "text": "How do you know if what you see is relevant?\n\nLast time, we talk a bit about the estimation of MLRM. For those who do not remember:\n\n\\[\\hat\\beta=(X'X)^{-1}X'y\n\\]\n\nWe also defined how, under A5 (homoskedasticity), we can estimate the variance covariance of coefficients:\n\n\\[Var(\\beta) = \\frac{\\sum \\hat e^2}{N-K-1} (X'X)^{-1}\n\\]\n\nThe next question: how to know how precise your estimates are?\nThat should be simple, just divide coefficient by its Standard error. The larger this is, the more precise, and more significant.\nIs this enough to say something about the population coefficients?\n\n(lets assume A1-A5 holds)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#distribution-of-coefficients",
    "href": "rmethods/4_MLRM_IA.html#distribution-of-coefficients",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Distribution of coefficients",
    "text": "Distribution of coefficients\n\nThe right answer is…Perhaps.\nUnless you know something about the distribution of \\(\\beta's\\), it would be hard to make any inferences from the estimates. Why?\nBecause not all distributions are made equal!\n\n\n\nCode\nclear\nrange x -4 4 1000\ngen funiform = 0 \nreplace funiform = 1/(2*sqrt(3)) if inrange(x,-sqrt(3),sqrt(3))\n\ngen fnormal = 0 \nreplace fnormal = normalden(x)\n\ngen fchi2 = 0 \nreplace fchi2 = sqrt(8)*chi2den(4,x*sqrt(8)+4)\n\ninteg funiform x, gen(F1)\ninteg fnormal x, gen(F2)\ninteg fchi2 x, gen(F3)\n\nset scheme white2\ncolor_style egypt\nreplace x = x + 1.5\ntwo (area funiform x           , pstyle(p1) color(%20)) ///\n    (area funiform x if F1&lt;0.05, pstyle(p1) color(%80)) /// \n    (area funiform x if F1&gt;0.95, pstyle(p1) color(%80)) /// \n    (area fnormal  x           , pstyle(p2) color(%20)) ///  \n    (area fnormal  x if F2&lt;0.05, pstyle(p2) color(%80)) /// \n    (area fnormal  x if F2&gt;0.95, pstyle(p2) color(%80)) /// \n    (area fchi2    x           , pstyle(p3) color(%20)) /// \n    (area fchi2    x if F3&gt;0.95, pstyle(p3) color(%80)) /// \n    (area fchi2    x if F3&lt;0.05, pstyle(p3) color(%80)), ///\n    xlabel(-4 / 4) legend(order(2 \"Uniform\" 5 \"Normal\" 8 \"C-Chi2\")) /// \n    xtitle(\"Beta hat Distribution\") ///\n    xline( 0, lstyle(1) lwidth(1)) xline(1.5)\n\ngraph export images/f4_1.png, replace width(1200)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#not-all-distributions-are-the-same",
    "href": "rmethods/4_MLRM_IA.html#not-all-distributions-are-the-same",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Not all Distributions are the Same",
    "text": "Not all Distributions are the Same"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#new-assumption",
    "href": "rmethods/4_MLRM_IA.html#new-assumption",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "New Assumption",
    "text": "New Assumption\n\nA6: Errors are normal \\(e\\sim N(0,\\sigma^2_e)\\).\n\nA1-A6 are the Classical Linear Model Assumption\nThis assumes the outcome is “conditionally” normal. \\(y|X \\sim N(X\\beta,\\sigma^2_e)\\)\nAnd with this assumption OLS is no longer blue. Its now BUE!"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-1",
    "href": "rmethods/4_MLRM_IA.html#section-1",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Why does it matter?\n\nIf you combine two variables with the same distributions, the combined variable will not have the same distributions as the “parents”\nExcept with normals! if you add two -normal- distributions together. The outcome will also be normal. (Dont believe me try it)\n\nRecall:\n\\[\\hat \\beta=\\beta + (X'X)^{-1}X'e\n\\]\nIf \\(e\\) is normal, then \\(\\beta's\\) will also be normal\nAnd this works for ANY Sample size!"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-2",
    "href": "rmethods/4_MLRM_IA.html#section-2",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "If \\(e\\) normal then \\(\\beta\\) is normal\n\nIf \\(\\hat \\beta's\\) are normal, then we can use this distribution to make inferences about \\(\\beta's\\) using normal distribution.\nThis is good, because we know how to do math with Normal distributions. And can used the modified Ratio:\n\n\\[z_j = \\frac{\\hat \\beta_j - \\beta_j}{sd(\\hat\\beta)}\\sim N(0,1)\n\\]\n\nWhere \\(\\beta_j\\) is what you think the True Population parameter is (your hypothesis), and \\(\\hat\\beta_j\\) is what you estimate in your data.\nDepending on the size of this, you can either reject your hypothesis, or not Reject it.\n\nbut do we “know” \\(sd(\\beta)\\)?"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-3",
    "href": "rmethods/4_MLRM_IA.html#section-3",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Do we “know” \\(sd(\\beta)\\)?\nWe don’t, which is why we can use a normal directly. Instead we use a t-distribution, which uses \\(se(\\hat\\beta )\\)\n\\[t_j = \\frac{\\hat \\beta_j - \\beta_j}{se(\\hat\\beta)}\\sim t_{N-k-1}\n\\]\nThen\n\nIf \\(e\\) is normal, \\(\\beta\\) will be normal.\nWhen Samples are “small” Standardized \\(\\beta\\) will follow a t-distribution\nBut, as \\(N\\rightarrow \\infty\\), \\(t_{N-k-1}\\sim N(0,1)\\)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#testing-hypothesis",
    "href": "rmethods/4_MLRM_IA.html#testing-hypothesis",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Testing Hypothesis",
    "text": "Testing Hypothesis\n\nThe idea of hypothesis testing is contrasting the “evidence” from your data (estimates) with the beliefs we have about the population.\n\n\\[y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + e\n\\]\nSay I have two hypothesis.\n\n\\(x_2\\) has no effect on \\(y\\). ie \\(H_0: \\beta_2 = 0\\)\n\\(x_1\\) has an effect equal to 1. ie \\(H_0: \\beta_1 = 1\\)\n\nNotice we make hypothesis about the population coefficients not the estimates\n\n\nI can “test” each hypothesis separately using a “t-statistic”\n\\[ \\color{green}{t_2=\\frac{\\hat \\beta_2 - 0}{se(\\hat \\beta_2)}} ;\nt_1=\\frac{\\hat \\beta_1 - 1}{se(\\hat \\beta_1)}\n\\]"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#types-of-hypothesis",
    "href": "rmethods/4_MLRM_IA.html#types-of-hypothesis",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Types of Hypothesis:",
    "text": "Types of Hypothesis:\nWhen talking about hypothesis testing there are two types:\n\nOne sided: when your alternative hypothesis compares your null to something either strictly larger, or strictly smaller than your hypothesis.\n\nEducation has no effect on wages vs Returns to education are positive.\nSkipping class has no effect on grades vs Skiping class reduces grades.\n\nTwo sided: When your alternative hypothesis is to say, “its different than”\n\nReturns to education is 10%, vs is not 10%\nSkipping class reduces grades in 0.5 points, vs not 0.5points\n\n\nIn both cases, you use the same t-statistic.\n\\[t_\\beta=\\frac{\\hat\\beta - \\beta_{hyp}}{se(\\hat \\beta)} \\sim t_{N-k-1}\n\\]"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-4",
    "href": "rmethods/4_MLRM_IA.html#section-4",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "What changes are the “thresholds” to Judge something significant or not.\nOne sided test:\n\\[\\begin{aligned}\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k&gt;\\beta^{hyp}_k \\\\\n& t_{\\beta_k}&gt;t_{N-k-1}(1-\\alpha) \\\\\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k&lt;\\beta^{hyp}_k \\\\\n& t_{\\beta_k}&lt;-t_{N-k-1}(1-\\alpha)\n\\end{aligned}\n\\]\n\nWhere \\(\\alpha\\) is your level of significance, and \\(t_{N-k-1}(1-\\alpha)\\) is the critical value.\n\\(\\alpha\\) determines the “risk” of commiting an error type I: Rejecting the Null when its true.\nIntuitively, the smaller \\(\\alpha\\) is, the more possitive (negative) “t” needs to be reject the Null."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-5",
    "href": "rmethods/4_MLRM_IA.html#section-5",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Two sided test:\n\\[\\begin{aligned}\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k \\neq \\beta^{hyp}_k \\\\\n& | t_{\\beta_k} | &gt;t_{N-k-1}(1-\\alpha/2)\n\\end{aligned}\n\\]\n\nSimilar to before, except the one needs to consider both tails of the distribution to determine critical values (see \\(t_{N-k-1}(1-\\alpha/2)\\))\nIntuitively, the smaller \\(\\alpha\\) is, the larger the absolute value of “t” needs to be reject the Null.\n\nBut, what is an error type I? and why we don’t we “accept” \\(H_0\\) s?"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#why-we-never-accept",
    "href": "rmethods/4_MLRM_IA.html#why-we-never-accept",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Why we never accept?:",
    "text": "Why we never accept?:\n\nAs stated few times before, \\(\\hat \\beta\\) are just approximations to the true \\(\\beta\\) coefficients. Its the “evidence” you have based on the data available.\nWith this evidence, you can reject some hypothesis. (Some more strongly than others)\nHowever, there could exists many scenarios that would fit the evidence."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-7",
    "href": "rmethods/4_MLRM_IA.html#section-7",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Code\nclear\nrange x -5 5 1000\ngen fx = normalden(x) \nset scheme white2\ncolor_style tableau\ngen xx = x+1\ntwo (area fx x , pstyle(p1) color(%20)) ///\n    (area fx x if x&lt;invnormal(.025), pstyle(p1) color(%80) ) ///\n    (area fx x if x&gt;invnormal(.975), pstyle(p1) color(%80) ) ///\n    (area fx xx , pstyle(p2) color(%20)) ///\n    (area fx xx if x&lt;invnormal(.025), pstyle(p2) color(%80) ) /// \n    (area fx xx if x&gt;invnormal(.975), pstyle(p2) color(%80) ) ///\n    , xline(1.8) legend(order(1 \"H0: b=0\" 4 \"H0: b=1\"))  ///\n    xlabel(-4(2)4)  ylabel(0(.1).5)  xsize(8) ysize(4)\ngraph export images/f4_2.png, height(1000)  replace"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#what-about-type-error-i-and-ii",
    "href": "rmethods/4_MLRM_IA.html#what-about-type-error-i-and-ii",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "What about Type error I and II?",
    "text": "What about Type error I and II?\n\n\nBecause we do not know the truth, we are bound to commit errors in our assessment of the data.\nSo given the data evidence and the hypothesis, there could be 2 scenarios:\n\nGOOD: You either reject when \\(H_0\\) is false, or not reject when \\(H_0\\) is true.\n\\(TE-I\\): You reject \\(H_0\\) when it is true,\n\\(TE-II\\): Not reject \\(H_0\\) when it is false (Something else was true)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-8",
    "href": "rmethods/4_MLRM_IA.html#section-8",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Code\nclear\nrange x -5 5 1000\ngen fx = normalden(x) \n\ngen xxx=x+3 \ntwo (area fx x , pstyle(p1) color(%20)) ///\n    (area fx xxx, pstyle(p2) color(%20)) ///\n    (area fx x if x&gt;2, pstyle(p1) color(%80)) ///\n    (area fx xxx if xxx &lt;2, pstyle(p2) color(%80))  ///\n    ,legend(order(1 \"H0\"3 \"Type I \" 2 \"H1\"  4 \"Type II \") cols(2))   ///\n    xlabel(-4(2)7)  ylabel(0(.1).5) xsize(8) ysize(4)  \ngraph export images/f4_3.png, height(1000)  replace"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#example-determinants-of-college-gpa",
    "href": "rmethods/4_MLRM_IA.html#example-determinants-of-college-gpa",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Example: Determinants of College GPA",
    "text": "Example: Determinants of College GPA\n\n\nCode\nfrause gpa1, clear\nreg colgpa hsgpa act skipped\n\n\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =     13.92\n       Model |  4.53313314         3  1.51104438   Prob &gt; F        =    0.0000\n    Residual |  14.8729663       137  .108561798   R-squared       =    0.2336\n-------------+----------------------------------   Adj R-squared   =    0.2168\n       Total |  19.4060994       140  .138614996   Root MSE        =    .32949\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .4118162   .0936742     4.40   0.000     .2265819    .5970505\n         act |   .0147202   .0105649     1.39   0.166    -.0061711    .0356115\n     skipped |  -.0831131   .0259985    -3.20   0.002    -.1345234   -.0317028\n       _cons |   1.389554   .3315535     4.19   0.000     .7339295    2.045178\n------------------------------------------------------------------------------\n\n\n\nHypothesis: Skipping classes has no effect on College GPA.\n\n\\[H_0: \\beta_{skip} = 0 \\text{ vs } H_1: \\beta_{skip} \\neq 0\n\\]\n\nTest, \\(a=95%\\), \\(|t_{skip}|=3.2\\) vs \\(t_{n-k-1}(0.975)\\):\n\n\nCode\ndisplay invt(141-4,0.975)\n\n1.9774312\n\nConclusion: \\(H_0\\) is rejected."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-9",
    "href": "rmethods/4_MLRM_IA.html#section-9",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Hyp: Skipping college has no effect on College GPA vs has a negative effect\n\n\\[H_0: \\beta_{skip} = 0 \\text{ vs } H_1: \\beta_{skip}&lt;0\n\\]\n\nTest, \\(a=95\\%\\), \\(|t_{skip}|=3.2\\) vs \\(t_{n-k-1}(0.95)=1.6560\\)\nAlso Reject \\(H_0\\)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-10",
    "href": "rmethods/4_MLRM_IA.html#section-10",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "\\(t_{ACT}=1.39\\)\nHyp: ACT has no effect on College GPA vs It has a non-zero effect\nHyp: ACT has no effect on College GPA vs it has a positive effect\nCritical:\n\n\\(t_{137}(0.95)=1.6560\\) Donot Reject\\(H_0\\) with \\(\\alpha = 5\\%\\)\n\\(t_{137}(0.90)=1.2878\\) Reject \\(H_0\\) with \\(\\alpha = 10\\%\\)\n\nEach GPA point in highschool translates into half a point in College GPA. vs Is less than .5\n\n\ntest hsgpa = 0.5\nlincom hsgpa - 0.5\n\n\n ( 1)  hsgpa = .5\n\n       F(  1,   137) =    0.89\n            Prob &gt; F =    0.3482\n\n ( 1)  hsgpa = .5\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |  -.0881838   .0936742    -0.94   0.348    -.2734181    .0970505\n------------------------------------------------------------------------------\n\n\n\nCritical at 5%: \\(t=-1.6560\\)\nCannot Reject \\(H_0\\)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#p-values",
    "href": "rmethods/4_MLRM_IA.html#p-values",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "p-values",
    "text": "p-values\n\nSomething you may or may not have noticed. The significance level \\(\\alpha\\) can be choosen by the researcher.\n\nConventional levels are 10%, 5% and 1%.\n\nThis may lead to researchers choosing any value that would make their theory fit.\n\nThere is a better alternative. Using \\(p-values\\) to capture the smallest significance level that you could use to reject your Null.\n\n\\[p-value = P(|t|&gt;|t-stat|) \\text{ or } p-value = 2*P(|t|&gt;|t-stat|)\n\\]\n\nThe smallest the better! (for rejection)\nHow?\n\nOne tail : display 1-t(df = n-k-1, |t-stat|)\ntwo tails: display 2-2*t(df = n-k-1, |t-stat|)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-11",
    "href": "rmethods/4_MLRM_IA.html#section-11",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Code\nclear\nrange x -5 5 1000\ngen fx = normalden(x) \nlocal p1:display %5.1f 100*2*(1-normal(1.5)) \nlocal p2:display %5.1f 100*2*(1-normal(2.5)) \ntwo (area fx x , pstyle(p1) color(%20)) ///\n    (area fx x if x&lt;-1.5, pstyle(p3) color(%80) ) ///\n    (area fx x if x&gt;+1.5, pstyle(p3) color(%80) ) ///\n    (area fx x if x&lt;invnormal(.025), pstyle(p1) color(%80) ) ///\n    (area fx x if x&gt;invnormal(.975), pstyle(p1) color(%80) ) ///\n    (area fx x if x&lt;-2.5, pstyle(p2) color(%80) ) ///\n    (area fx x if x&gt;+2.5, pstyle(p2) color(%80) ) , ///\n    xline(-1.5 2.5) legend(order(4 \"{&alpha}=5%\" 6 \"p-value = `p2'%\" 2 \"p-value = `p1'%\")) ///\n    ylabel(0(.1).5) xlabel(-5 0 5 -1.5 2.5)\ngraph export images/f4_4.png, replace width(1200)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#note-on-statistical-significance",
    "href": "rmethods/4_MLRM_IA.html#note-on-statistical-significance",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Note on Statistical Significance",
    "text": "Note on Statistical Significance\n\nStatistically significant doesnt mean meaninful. And lack of it, doesnt mean is not important\n\nKeep in mind that SE may be larger or smaller due to other factors (N or Mcollinearity)\n\nBe careful of discussing the effect size. (a 1US increase in min wage is different from 1chp in min Wage)\nIf non-significant, pay attention to the magnitude and relevance for your research. Does it have the correct sign?\nIncorrect signs with significant results. Either there is something wrong, or you found something interesting."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#confidence-intervals",
    "href": "rmethods/4_MLRM_IA.html#confidence-intervals",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nThis is the third approach to assess how precise or significant an estimate is. You provide a Range of possible values, given the level of coverage, and SE.\n\n\\[CI(\\beta_i) = [\\hat \\beta_i - \\hat \\sigma_{\\beta_i} t_{n-k-1}(1-\\alpha),\\hat \\beta_i +\\hat \\sigma_{\\beta_i} t_{n-k-1}(1-\\alpha)]\n\\]\n\nInterpretation:\n\nIf we were to draw M samples, the true beta would be in this interval \\(1-\\alpha\\%\\) of the time.\n\nIt allows you to see what other “hypothesis” would be consistent with the evidence of the estimate (you wouldnt be able to reject the Null)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#ci-vs-t-critical-and-p-values",
    "href": "rmethods/4_MLRM_IA.html#ci-vs-t-critical-and-p-values",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "CI vs T-critical and P values",
    "text": "CI vs T-critical and P values\n\nt-stat and p-values are calculated based on standardized coefficients (ratio of coefficient and SE)\nCI are calculated based on the actual coefficient and SE.\n\nIf the p-value of a t-statistic is exactly 0.05, then the 95% CI will not include 0 (at the limit), and the t-critical (\\(\\alpha=5\\%\\)) will be the same as the t-statistic.\nIn other words. If you use the same \\(\\alpha\\), your conclusions would be the same regardless of using t-stat, p-value or CI."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-13",
    "href": "rmethods/4_MLRM_IA.html#section-13",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Code\nclear\nrange x -5 5 1000\ngen fx = normalden(x) \nlocal p1:display %5.1f 100*2*(1-normal(1.5)) \nlocal p2:display %5.1f 100*2*(1-normal(2.5)) \ngen xx = x+2\n\ntwo (area fx x , pstyle(p1) color(%10)) ///\n    (area fx x if x&lt;invnormal(.025), pstyle(p1) color(%60) ) ///\n    (area fx x if x&gt;invnormal(.975), pstyle(p1) color(%60) ) ///\n    (area fx xx , color(gs1%10) ) ///\n    (area fx xx if x&lt;invnormal(.005),  color(gs1%80) ) ///\n    (area fx xx if x&gt;invnormal(.995),  color(gs1%80) ) ///\n    (area fx xx if x&lt;invnormal(.025),  color(gs1%60) ) ///\n    (area fx xx if x&gt;invnormal(.975),  color(gs1%60) ) ///\n    (area fx xx if x&lt;invnormal(.05),  color(gs1%40) ) ///\n    (area fx xx if x&gt;invnormal(.95),  color(gs1%40) ), ///\n    xline(2) legend(order(5 \"CI-1%\" 7 \"CI-5%\" 9 \"CI-10%\")) ///\n    ylabel(0(.1).5) xlabel(-5 0 5 )  \ngraph export images/f4_5.png, replace width(1000)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#testing-linear-combinations",
    "href": "rmethods/4_MLRM_IA.html#testing-linear-combinations",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Testing Linear Combinations:",
    "text": "Testing Linear Combinations:\n\nYou may be interested in testing particular linear combinations of coefficients:\n\n\\(b_1 - b_2 =0 ; b_2+b_3=1 ; 2*b_4-b_5=b_6\\)\n\nDoing this is “simple”. Because is a single linear combination, you can still use “t-stat”.\n\n\\(t-stat = \\frac{2*\\hat b_4 -\\hat b_5 -\\hat b_6}{se(2*\\hat b_4 -\\hat b_5 -\\hat b_6)}\\)\n\nJust need SE for combined coefficients (requires knowing Variances and Covariances)\nEasy way, you could use Stata:\n\nreg y x1 x2 x3 x4 x5 x6\nlincom x1-x2 or lincom 2*x4-x5-x6\ntest (x1-x2=0) (x2+x3=1) (2*x4-x5=x6), mtest"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-14",
    "href": "rmethods/4_MLRM_IA.html#section-14",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Harder Way: (if you dare)\nMatrix Multiplication\nAssume Constant is the last coefficient: \\[V( 2*b_4-b_5- b_6) = R' V R ; R = [0,0,0,2,-1,-1]\n\\]\nwhere R are the restrictions, and V is the variance covariance matrix of \\(\\beta's\\).\nThen your t-stat\n\\[t-stat = \\frac{2*b_4-b_5- b_6}{\\sqrt{V(2*b_4-b_5- b_6)}}\n\\]"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-15",
    "href": "rmethods/4_MLRM_IA.html#section-15",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Alternative: Substitution\n\nOne can manipulate the regression model to consider a model with the contrained coefficient.\nOnce model is estimated, it simplifies testing:\n\n\\[\\begin{aligned}\n& y = b_0 + b_1 x_1 + b_2  x_2 + b_3 x_3 + e  \\\\\nh0: & b_1 - 2b_2 +b_3=0 \\rightarrow \\theta = b_1 - 2b_2 +b_3 \\rightarrow b_1 = \\theta + 2b_2 - b_3 \\\\\n& y = b_0 + ( \\theta + 2b_2 - b_3) x_1 + b_2 x_2 + b_3 x_3 + e \\\\\n& y = b_0 +  \\theta x_1 + b_2( x_2 +2 x_1) + b_3 (x_3-x_1) + e \\\\\n& y = b_0 +  \\theta x_1 + b_2 \\tilde x_2 + b_3 \\tilde x_3 + e \\\\\n\\end{aligned}\n\\]\nHere testing for \\(\\theta=0\\) is the same as testing for \\(b_1 - 2b_2 +b_3\\) in the original model.\n\n\n\n\n\n\nNote\n\n\nAlways ask something like this in Midterm, so brush up your math."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#testing-multiple-restrictions",
    "href": "rmethods/4_MLRM_IA.html#testing-multiple-restrictions",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Testing Multiple Restrictions",
    "text": "Testing Multiple Restrictions\nWhat if you are interested in testing multiple restrictions:\n\\[\\begin{aligned}\ny &= b_0 + b_1 x_1 + b_2 x_2 +b_3 x_3 + e \\\\\n& H_0: b_1 = 0 ; b_2 - b_3 =0 \\\\\n& H_1: H_0 \\text{ is false}\n\\end{aligned}\n\\]\nEasy way: Stata command test allows you to do this\nOtherwise, you can do it by hand:"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-16",
    "href": "rmethods/4_MLRM_IA.html#section-16",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Estimate unrestricted model (original) and “save” \\(SSR_{ur}\\) or \\(R_{ur}^2\\)\nImpose restrictions on the model and “save” \\(SSR_r\\) or \\(R_{r}^2\\)\nEstimate F-stat:\n\n\\[F_{q,n-k-1} = \\frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(n-k-1)}  \\text{ or }\n\\frac{(R^2_{ur}-R^2_r)/q}{(1-R^2_{ur})/(n-k-1)} \\sim F(q,n-k-1)\n\\]\n\\(SSR\\) Sum of Squared Residuals, \\(q\\) number of restrictions\n\nIdea, you are comparing how the overall fitness of the model changes with restrictions.\nIf restrictions slightly decreases the model Fitness, you cannot be rejected them.\nOtherwise, They are rejected! (you just dont know which)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#overall-model-significance",
    "href": "rmethods/4_MLRM_IA.html#overall-model-significance",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Overall Model Significance",
    "text": "Overall Model Significance\nOne test, we often don’t do anymore, is testing the overall fitness of a model:\n\\[H_0: x_1, x_2, \\dots , x_k \\text{ do not explain y}\n\\]\n\\[H_0: \\beta_1=\\beta_2=\\dots=\\beta_k =0\n\\]\nWhere we kind of suggest that a model with only an intercept is better than the one with covariates.\n\\[F_{q,n-k-1} = \\frac{(R^2_{ur}-\\color{red}{R^2_r})/q}{(1-R^2_{ur})/(n-k-1)} \\sim F(q,n-k-1)\n\\]\nIn this case \\(\\color{red}{R^2_r}=0\\)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#not-for-the-faint-for-heart",
    "href": "rmethods/4_MLRM_IA.html#not-for-the-faint-for-heart",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Not For the faint for heart",
    "text": "Not For the faint for heart\nMatrix form for F-Stat! Restrictions:\n\\[H_0: R_{q,k+1}\\beta_{k+1,1}=c_{q,1}\n\\]\nFirst. Define matrix with all Matrix Restriction \\[\n\\Sigma_R = R_{q,k+1} V_\\beta R'_{q,k+1}\n\\]\nSecond: F-statistic\n\\[\nF-stat = \\frac 1 q (R\\beta-c)' \\Sigma_R^{-1} (R\\beta-c)\n\\]"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#example",
    "href": "rmethods/4_MLRM_IA.html#example",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Example",
    "text": "Example\n\nfrause hprice1, clear\nreg lprice lasses bdrms llotsize lsqrft\n\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(4, 83)        =     70.58\n       Model |  6.19607473         4  1.54901868   Prob &gt; F        =    0.0000\n    Residual |  1.82152879        83   .02194613   R-squared       =    0.7728\n-------------+----------------------------------   Adj R-squared   =    0.7619\n       Total |  8.01760352        87  .092156362   Root MSE        =    .14814\n\n------------------------------------------------------------------------------\n      lprice | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     lassess |   1.043065    .151446     6.89   0.000     .7418453    1.344285\n       bdrms |   .0338392   .0220983     1.53   0.129    -.0101135    .0777918\n    llotsize |   .0074379   .0385615     0.19   0.848    -.0692593    .0841352\n      lsqrft |  -.1032384   .1384305    -0.75   0.458     -.378571    .1720942\n       _cons |    .263743   .5696647     0.46   0.645    -.8692972    1.396783\n------------------------------------------------------------------------------\n\n\n\ntest (lasses=1)\ntest (lasses=1) (bdrms=llotsize=lsqrft=0)\n\n\n ( 1)  lassess = 1\n\n       F(  1,    83) =    0.08\n            Prob &gt; F =    0.7768\n\n ( 1)  lassess = 1\n ( 2)  bdrms - llotsize = 0\n ( 3)  bdrms - lsqrft = 0\n ( 4)  bdrms = 0\n\n       F(  4,    83) =    0.67\n            Prob &gt; F =    0.6162\n\n\n\nfrause mlb1, clear\nreg lsalary years gamesyr bavg hrunsyr rbisy\n\n\n      Source |       SS           df       MS      Number of obs   =       353\n-------------+----------------------------------   F(5, 347)       =    117.06\n       Model |  308.989208         5  61.7978416   Prob &gt; F        =    0.0000\n    Residual |  183.186327       347  .527914487   R-squared       =    0.6278\n-------------+----------------------------------   Adj R-squared   =    0.6224\n       Total |  492.175535       352  1.39822595   Root MSE        =    .72658\n\n------------------------------------------------------------------------------\n     lsalary | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       years |   .0688626   .0121145     5.68   0.000     .0450355    .0926898\n     gamesyr |   .0125521   .0026468     4.74   0.000     .0073464    .0177578\n        bavg |   .0009786   .0011035     0.89   0.376    -.0011918     .003149\n     hrunsyr |   .0144295    .016057     0.90   0.369    -.0171518    .0460107\n      rbisyr |   .0107657    .007175     1.50   0.134    -.0033462    .0248776\n       _cons |   11.19242   .2888229    38.75   0.000     10.62435    11.76048\n------------------------------------------------------------------------------\n\n\n\ntest bavg hrunsyr rbisy\n\n\n ( 1)  bavg = 0\n ( 2)  hrunsyr = 0\n ( 3)  rbisyr = 0\n\n       F(  3,   347) =    9.55\n            Prob &gt; F =    0.0000"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#introduction",
    "href": "rmethods/4_MLRM_IA.html#introduction",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Introduction",
    "text": "Introduction\n\nWhen considering the topic of asymptotic theory, there are few concepts that are important ton consider.\n\nAsymtotics refer to properties of OLS when \\(N\\rightarrow \\infty\\)\nWhen samples grow, we are more concern about consistency rather than “just” unbiased estimators.\nWe are also concern with how flexible is the normality assumption when samples grow large."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#what-is-consistency",
    "href": "rmethods/4_MLRM_IA.html#what-is-consistency",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "What is consistency?",
    "text": "What is consistency?\n\nUp until now, we have been concerned with Unbiased estimates\n\n\\[E(\\hat\\beta)=\\beta\n\\]\n\nIn large samples, this is no longer enough. One requires Consistency!\n\nConsistency says that as \\(N\\rightarrow \\infty\\) then \\(plim \\hat \\beta = \\beta\\).\n\\(p(|\\hat \\beta - \\beta|&lt;\\varepsilon) = 1\\) or that The variance shrinks to zero, or we can estimate \\(\\beta\\) almost surely.\nThis is also known as asymptotic unbiasness.\n\nIn linear regression analysis, consistency can be achieved with a weaker A4’: \\(Cov(e,x)=0\\), assuming that we require only linear independence."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#consistency-vs-bias",
    "href": "rmethods/4_MLRM_IA.html#consistency-vs-bias",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Consistency vs Bias",
    "text": "Consistency vs Bias\n\nConsistent and UnbiasedConsistent and Biased"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#what-about-normality-assumption",
    "href": "rmethods/4_MLRM_IA.html#what-about-normality-assumption",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "What about Normality Assumption?",
    "text": "What about Normality Assumption?\n\nEverything we have seen so far was possible under the normality assumption of the errors.\n\nif \\(e\\) is normal, then \\(b\\) is normal (even in small samples), thus we can use \\(t\\), \\(F\\), etc\n\nBut what if this assumption fails? would we care?\n\n\n\nPerhaps. If your sample is small, \\(b\\) will not be normal, and standard procedures will not work.\nIn large Samples, however, \\(\\beta's\\) will be normal, even if \\(e\\) is not. Thanks to CLT"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-18",
    "href": "rmethods/4_MLRM_IA.html#section-18",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Good news\n\nBottom line, when \\(N\\) is large, you do not need \\(e\\) to be normal.\nif A1-A5 hold, you can rely on asymptotic normality!\nThus you can still use t’s and F’s, but you can also use LM"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#lm-lagrange-multiplier",
    "href": "rmethods/4_MLRM_IA.html#lm-lagrange-multiplier",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "LM-Lagrange Multiplier",
    "text": "LM-Lagrange Multiplier\n\nWhile you can still use t-stat and F-stat to draw inference from your model, there is a better test (given the large sample): Lagrange Multiplier Statistic\nThe idea: Does impossing restrictions affect the model Fitness?\n\n\nRegress \\(y\\) on restricted \\(x_1,\\dots,x_{k-q}\\), and obtain \\(\\tilde e\\)\nRegress \\(\\tilde e\\) on all \\(x's\\), and obtain \\(R^2_e\\).\n\nIf the excluded regressors were not significant, the \\(R^2_e\\) should be very small.\n\nCompare \\(nR^2_e\\) with \\(\\chi^2(n,1-\\alpha)\\), and draw conclusions."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#example-1",
    "href": "rmethods/4_MLRM_IA.html#example-1",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Example:",
    "text": "Example:\n\nfrause crime1, clear\nqui: reg narr86 pcnv avgsen tottime ptime86 qemp86\n** H0: avgsen=0 and tottime=0\ntest (avgsen=0) (tottime=0)\n\n\n ( 1)  avgsen = 0\n ( 2)  tottime = 0\n\n       F(  2,  2719) =    2.03\n            Prob &gt; F =    0.1310\n\n\n\nqui: reg narr86 pcnv                ptime86 qemp86\n* Predict residuals of constrained model\npredict u_tilde , res\n* regress residuals againts all variables\nreg u_tilde  pcnv avgsen tottime ptime86 qemp86\n\n\n      Source |       SS           df       MS      Number of obs   =     2,725\n-------------+----------------------------------   F(5, 2719)      =      0.81\n       Model |  2.87904835         5  .575809669   Prob &gt; F        =    0.5398\n    Residual |  1924.39392     2,719  .707757969   R-squared       =    0.0015\n-------------+----------------------------------   Adj R-squared   =   -0.0003\n       Total |  1927.27297     2,724  .707515773   Root MSE        =    .84128\n\n------------------------------------------------------------------------------\n     u_tilde | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        pcnv |  -.0012971    .040855    -0.03   0.975    -.0814072    .0788129\n      avgsen |  -.0070487   .0124122    -0.57   0.570     -.031387    .0172897\n     tottime |   .0120953   .0095768     1.26   0.207    -.0066833     .030874\n     ptime86 |  -.0048386   .0089166    -0.54   0.587    -.0223226    .0126454\n      qemp86 |   .0010221   .0103972     0.10   0.922    -.0193652    .0214093\n       _cons |  -.0057108   .0331524    -0.17   0.863    -.0707173    .0592956\n------------------------------------------------------------------------------\n\n\n\nCode\ndisplay \"Chi2(2)=\" `=e(N)*e(r2)'\ndisplay \"Its p-value=\" %5.4f `=1-chi2(2, `=e(N)*e(r2)' )'\n\nChi2(2)=4.0707294 Its p-value=0.1306\nTry making it a program if you “dare”"
  },
  {
    "objectID": "rmethods/2_SRA.html#the-simple-regression-model",
    "href": "rmethods/2_SRA.html#the-simple-regression-model",
    "title": "Simple Regression Model",
    "section": "The Simple Regression Model",
    "text": "The Simple Regression Model\n\nAs we saw in the previous slides, one of the important steps when doing empirical analysis is to develop a model that describes reality.\nThis model is quite abstract, as it rarely provides guidance regarding on How should you build your econometric model.\nIn this chapter, we introduce the first (boring) tool to solve this problem. The simple Regression model or SRL"
  },
  {
    "objectID": "rmethods/2_SRA.html#what-is-a-simple-regression-model-srm",
    "href": "rmethods/2_SRA.html#what-is-a-simple-regression-model-srm",
    "title": "Simple Regression Model",
    "section": "What is a Simple Regression Model (SRM) ?",
    "text": "What is a Simple Regression Model (SRM) ?\n\nA Simple regression model is known as such because it aims to capture the relationship between two variables.\nIt does not mean it ignores other factors, but rather, bundles them together as part of a Bag of Holding or error. In its most flexible setup, a simple regression model can be written as:\n\n\\[y = f(x,u)\n\\]\nThis model simply says that there is some relationship between:\n\n\\(y\\), your outcome, dependent, explained, response, variable\nand \\(x\\), your independent, explanatory, regression, variable\n\nwhereas everything else not considered is assumed to be part of the unobserved \\(u\\)."
  },
  {
    "objectID": "rmethods/2_SRA.html#from-abstract-to-concrete",
    "href": "rmethods/2_SRA.html#from-abstract-to-concrete",
    "title": "Simple Regression Model",
    "section": "From Abstract to Concrete",
    "text": "From Abstract to Concrete\n\nA good reason why one should start thinking about the model as shown earlier is to acknowledge that we Do not know the functional form between \\(x\\) and \\(y\\).\nFurther, we don’t even know how \\(u\\) interacts with \\(x\\).\n\nThis brings us to the first step one should do (almost always) when analyzing data…Create a plot to see if there is any relationship in the data"
  },
  {
    "objectID": "rmethods/2_SRA.html#simple-scatter-1",
    "href": "rmethods/2_SRA.html#simple-scatter-1",
    "title": "Simple Regression Model",
    "section": "Simple Scatter 1",
    "text": "Simple Scatter 1\n\n\nCode\n** To download all Wooldrige Files\nqui: ssc install frause, replace\n** for some additional color schemes\nqui: ssc install color_style\nset scheme white2\ncolor_style tableau\n** Loads file wage1\nfrause wage1, clear\nscatter wage educ\n\n\n\n\nWe observe a positive relationship between Wages and years of education\nThis relationship does not seem to be linear"
  },
  {
    "objectID": "rmethods/2_SRA.html#simple-scatter-2",
    "href": "rmethods/2_SRA.html#simple-scatter-2",
    "title": "Simple Regression Model",
    "section": "Simple Scatter 2",
    "text": "Simple Scatter 2"
  },
  {
    "objectID": "rmethods/2_SRA.html#even-more-concrete",
    "href": "rmethods/2_SRA.html#even-more-concrete",
    "title": "Simple Regression Model",
    "section": "Even more Concrete",
    "text": "Even more Concrete\n\nThis first “model” provides little guidance for the modeling itself.\nThe Simple Linear Regression Model corrects for that, establishing a specific relationship between the variables of interest and the error: \\[y = \\beta_0 + \\beta_1 x + u\\]\n\nThis model has a lot packed in.\n\nIt imposes a relationship between \\(y\\) and \\(x\\) (linear)\nAnd addresses the fact that there could be other factors not considered \\(u\\). Impossing the assumption they are additive errors.\n\nIt also assumes the population relationships: \\[E(y|x) = \\beta_0 + \\beta_1 x\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#what-can-we-learn-from-it",
    "href": "rmethods/2_SRA.html#what-can-we-learn-from-it",
    "title": "Simple Regression Model",
    "section": "What can we learn from it?",
    "text": "What can we learn from it?\n\\[E(y|x) = \\beta_0 + \\beta_1 x\\]\nThis is your Population Regresson function. To interpret it, we need to assume \\(u\\) is fixed (ceteris paribus). This implies that \\[E(u|x)=c=0\\]\nWhich says that the errors are mean independent of \\(x\\). Thus, for all practical purposes, when \\(x\\) changes, we will assume \\(u\\) is as good as fixed.\nUnder these conditions, we can interpret the coefficients:\n\n\\(\\beta_0\\) is the constant, or expected outcome when \\(x=0\\).\n\\(\\beta_1\\) is the slope of \\(x\\), or the expected change in \\(y\\) when \\(x\\) changes in 1 unit:\n\n\\[\\Delta y = \\beta_1 \\Delta x \\rightarrow \\frac{\\Delta y}{\\Delta x} = \\beta_1\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#example",
    "href": "rmethods/2_SRA.html#example",
    "title": "Simple Regression Model",
    "section": "Example",
    "text": "Example\n\nSoybean and Yield Fertilizer:\n\n\\[yield = \\beta_0 + \\beta_1 fertilizer + u\\]\n\\(\\beta_1\\) Effect of Fertilizer (an additional dosage) on Soybean Yield\n\nSimple wage equation\n\n\\[wage = \\beta_0 + \\beta_1 educ + u\n\\]\n\\(\\beta_1\\) Change in wages given an additional year of education."
  },
  {
    "objectID": "rmethods/2_SRA.html#deriving-coefficients-ordinary-least-squares---ols",
    "href": "rmethods/2_SRA.html#deriving-coefficients-ordinary-least-squares---ols",
    "title": "Simple Regression Model",
    "section": "Deriving Coefficients: Ordinary Least Squares - OLS",
    "text": "Deriving Coefficients: Ordinary Least Squares - OLS\n\nThere are an infinite number of candiates for \\(\\beta_0 \\& \\beta_1\\).\nOLS, is one of the multiple methods that allows us to estimate the coefficients of a SLRM1.\nThe goal is to Choose parameters \\(\\beta={\\beta_0,\\beta_1}\\) that “minimizes” the Squared of the residuals.\n\nIn other words, OLS aims to maximize Explantion power by minimizing errors.\n\n\nSimple Linear Regression Model"
  },
  {
    "objectID": "rmethods/2_SRA.html#visualization",
    "href": "rmethods/2_SRA.html#visualization",
    "title": "Simple Regression Model",
    "section": "Visualization",
    "text": "Visualization\n\n\nCode\nset seed 10\nclear\nrange x -2 2 20\ngen y = 1 + x + rnormal()\ncolor_style tableau\ntwo (scatter  y x) ///\n    (function y = 0.5 + 2*x, range(-2 2)) ///\n    (function y = 2 + 0.5*x, range(-2 2)) ///\n    (function y = 1 + 1*x  , range(-2 2)) , ///\n    legend(order(2 \"y=0.5+2x\" 3 \"y=2+0.5x\" 4 \"y=1+1x\"))\ngraph export images/fig2_1.png , replace width(1000)\n\ngen y1=.5+2*x\ngen y2=2+0.5*x\ngen y3=+1+1*x\n\n\negen u21 = sum((y-y1)^2)\negen u22 = sum((y-y2)^2)\negen u23 = sum((y-y3)^2)\nreg y x\negen u24 = sum((y-_b[_cons] - _b[x]*x)^2)\nlabel var u21 \"SSR-model 1\"\nlabel var u22 \"SSR-model 2\"\nlabel var u23 \"SSR-model 3\"\nlabel var u24 \"SSR-model 4\"\n\ntwo (scatter y x) ///\n    (function y = 0.5 + 2*x, range(-2 2)) ///\n    (rspike y y1 x), ylabel(-4/6) ytitle(y) ///\n    legend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))\ngraph export images/fig2_2.png , replace width(1000) \n    \ntwo (scatter y x) ///\n    (function y = 2 + 0.5*x, range(-2 2)) ///\n    (rspike y y2 x), ylabel(-4/6) ytitle(y) ///\n    legend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))\ngraph export images/fig2_3.png , replace width(1000) \n    \ntwo (scatter y x) ///\n    (function y = 1 + 1*x, range(-2 2)) ///\n    (rspike y y3 x) , ylabel(-4/6) ytitle(y) ///\n    legend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))\ngraph export images/fig2_4.png , replace width(1000)    \n\nreg y x\npredict yh\ntwo (scatter y x) ///\n    (function y = _b[_cons] + _b[x]*x, range(-2 2)) ///\n    (rspike y yh x) , ylabel(-4/6) ytitle(y) ///\n    legend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))    \ngraph export images/fig2_5.png , replace width(1000)    \nclonevar u24x=u24\ngraph bar u24 u21 u22 u23 u24x , ///\n   legend(order( 2 \"SSR-Model 1\" 3 \"SSR-Model 2\" 4 \"SSR-Model 3\" 5 \"Sample Fit\") ///\n   ring(0) pos(2)) xsize(5) ysize(8) scale(1.5) bar(1, color(gs0%0))\n   \ngraph export images/fig2_5x.png , replace width(300)    \n\n\n\nOptionsOpt1Opt2Opt3Opt4"
  },
  {
    "objectID": "rmethods/2_SRA.html#just-a-minimization-problem",
    "href": "rmethods/2_SRA.html#just-a-minimization-problem",
    "title": "Simple Regression Model",
    "section": "Just a Minimization Problem",
    "text": "Just a Minimization Problem\n\\[y_i =\\beta_0 + \\beta_1 x_i + u_i \\rightarrow u_i = y_i - \\beta_0 - \\beta_1 x_i\n\\]\n\\[{\\hat\\beta_0,\\hat\\beta_1} = \\min_{\\beta_0,\\beta_1} = SSR =\\sum_{i=1}^N u_i^2 = \\sum_{i=1}^N (y-\\beta_0 - \\beta_1 x_i)^2 \\\\\n\\]\nFirst Order Conditions:\n\\[\n\\begin{aligned}\n\\frac{\\partial SSR}{\\partial \\beta_0} &= -2 \\sum (y_i-\\beta_0 - \\beta_1 x_i) = -2 \\sum u_i =0 \\\\\n\\frac{\\partial SSR}{\\partial \\beta_1} &= -2 \\sum x_i (y_i-\\beta_0 - \\beta_1 x_i) =- 2 \\sum x_i u_i =0\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#just-a-minimization-problem-1",
    "href": "rmethods/2_SRA.html#just-a-minimization-problem-1",
    "title": "Simple Regression Model",
    "section": "Just a Minimization Problem",
    "text": "Just a Minimization Problem\nSimilar conditions as before (but now Mathematically):\n\\[\\begin{aligned}\n\\sum u_i &=0 \\rightarrow nE(e) = 0 \\\\\n\\sum x_i u_i &=0 \\rightarrow nE(x*e) \\rightarrow  n Cov(x,e) =0  \n\\end{aligned}\n\\]\nAnd the First Order Conditions simply provide a system of \\(k+1\\) equations with \\(k+1\\) unknowns.\n\\[\\begin{aligned}\n\\hat\\beta_0 &= \\bar y - \\beta_1 \\bar x \\\\\n\\hat\\beta_1 &= \\frac{\\sum (x_i-\\bar x)(y_i-\\bar y)}{\\sum (x_i-\\bar x)^2}\n= \\frac{\\hat \\rho \\hat \\sigma_x \\hat \\sigma_y}{\\hat \\sigma_x^2}\n= \\frac{\\hat \\rho \\hat \\sigma_y}{\\hat \\sigma_y}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#interpretation",
    "href": "rmethods/2_SRA.html#interpretation",
    "title": "Simple Regression Model",
    "section": "Interpretation?",
    "text": "Interpretation?\n\\[\\hat\\beta_0 = \\bar y - \\hat \\beta_1 \\bar x\\]\n\n\\(\\beta_0\\) is usually estimated as a “residual”, thus is often of little of no interest.\n\nExpected outcome when \\(X=0\\)\n\n\n\\[\n\\hat\\beta_1 = \\frac {cov(x,y)}{var(x)}= \\hat \\rho\\frac{ \\hat \\sigma_x \\hat \\sigma_y}{\\hat \\sigma_x^2}\n= \\hat \\rho\\frac{ \\hat \\sigma_y}{\\hat \\sigma_x}\n\\]\n\n\\(\\beta_1\\) is a slope, which is directly related to the correlation between \\(y\\) and \\(x\\).\n\nIt can only be estimated if \\(\\sigma_x\\)&gt;&gt;0\n\n\nAlso, this \\(\\hat y = \\hat \\beta_0 + \\hat \\beta_1 x\\) becomes your sample regression function\n\nwhere \\(\\hat y\\) is the fitted value of \\(y\\) (proyection or prediction), given some value of \\(x\\)."
  },
  {
    "objectID": "rmethods/2_SRA.html#visualization-1",
    "href": "rmethods/2_SRA.html#visualization-1",
    "title": "Simple Regression Model",
    "section": "Visualization",
    "text": "Visualization\n\n\nCode\ngen y0 = 0\ntwo (scatter y x) ///\n    (function y = _b[_cons] + _b[x]*x, range(-2.1 2.1)) ///\n    (rspike y y0 x, color(gs9%50) lw(1)) ///\n    , ylabel(-4/6) ytitle(y) yline(0) ///\n    title(\"Data\")   legend(off)\ngraph export images/fig2_6.png, replace width(1000)\ntwo (scatter y x) ///\n    (function y = _b[_cons] + _b[x]*x, range(-2.1 2.1)) ///\n    (rspike yh y0 x, color(gs9%50) lw(1)) ///\n    , ylabel(-4/6) ytitle(y) yline(0) ///\n    title(\"Prediction\") legend(off)\ngraph export images/fig2_7.png, replace width(1000) \ntwo (scatter y x) ///\n    (function y = _b[_cons] + _b[x]*x, range(-2.1 2.1)) ///\n    (rspike y yh x, color(gs9%50) lw(1)) ///\n    , ylabel(-4/6) ytitle(y) yline(0) ///\n    title(\"Residual\")   legend(off)\ngraph export images/fig2_8.png, replace width(1000) \n\n\n\nDataPredictionResiduals"
  },
  {
    "objectID": "rmethods/2_SRA.html#properties-of-the-estimator",
    "href": "rmethods/2_SRA.html#properties-of-the-estimator",
    "title": "Simple Regression Model",
    "section": "Properties of the Estimator",
    "text": "Properties of the Estimator\n\nBased on F.O.C., we know the following:\n\n\\[\n\\sum_i^n \\hat u_i = 0 \\ \\& \\ \\sum_i^n x_i \\hat u_i = 0\n\\]\nIn average \\(u_i\\) is zero, and uncorrelated with \\(x\\), and \\(\\bar y , \\bar x\\) is on the regression line\n\nBy construction \\(y_i = \\hat y_i + \\hat u_i\\), so that\n\n\\[\n\\begin{aligned}\n    \\sum_{i=1}^n(y_i-\\bar y)^2 &=\n    \\sum_{i=1}^n(y_i-\\hat y)^2  +\n    \\sum_{i=1}^n(\\hat y-\\bar y)^2  \\\\\n    SST &= SSE + SSR\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#properties-of-the-estimator-1",
    "href": "rmethods/2_SRA.html#properties-of-the-estimator-1",
    "title": "Simple Regression Model",
    "section": "Properties of the Estimator",
    "text": "Properties of the Estimator\n\nGoodness of FIT is defined as\n\n\\[R^2= 1-\\frac {SSR} {SST}=\\frac {SSE} {SST}\\]\n\nHow much of the Data variation (SST) is explained by the model (SSE), or\n1-amount not explained by the model"
  },
  {
    "objectID": "rmethods/2_SRA.html#some-discussion",
    "href": "rmethods/2_SRA.html#some-discussion",
    "title": "Simple Regression Model",
    "section": "Some Discussion",
    "text": "Some Discussion\nWe now know how to estimate coefficients given some data, but we need to ask the questions:\n\nHow do we know if the estimated coefficients are indeed appropriate for the population parameters?\nHow can we know the precision (or lack there off) of the estimates\nRemember, \\(\\hat \\beta's\\) depend on the sample. Different Samples will lead to different estimates. Thus \\(\\hat \\beta's\\) are random.\nIn repeated sampling scenarios, we could empirically obtain the distribution of the estimated parameters, and verify if estimations are unbiased.\nHowever, we can also do that based on analytical solutions. Lets see those assumptions"
  },
  {
    "objectID": "rmethods/2_SRA.html#assumption-1",
    "href": "rmethods/2_SRA.html#assumption-1",
    "title": "Simple Regression Model",
    "section": "Assumption 1:",
    "text": "Assumption 1:\n\n\n\n\n\n\nLinear in Parameters\n\n\nWe need to assume the population model is linear in parameters:\n\\[y_i = \\beta_0 + \\beta_1 x_i + u_i\\]\n\n\n\nIn other words, we need to assume that the model we chose is a good representation of what the true population model is.\n\nAdditive error, with a linear relationship between \\(x_i\\) on \\(y_i\\).\nWe can make it more flexible using some transformations of \\(x_i\\)."
  },
  {
    "objectID": "rmethods/2_SRA.html#assumption-2",
    "href": "rmethods/2_SRA.html#assumption-2",
    "title": "Simple Regression Model",
    "section": "Assumption 2:",
    "text": "Assumption 2:\n\n\n\n\n\n\nRandom Sampling\n\n\nThe data we are using is collected from a Random sample of the population, for which the linear model is valid.\n\n\n\n\nData should be representative from the population (for whom the Linear model Holds)\nThe Data Sampling should not depend the data collected, specially the dependent variable.\nAlso helps to ensure units “unobservables” \\(u's\\) are independent from each other."
  },
  {
    "objectID": "rmethods/2_SRA.html#assumption-3",
    "href": "rmethods/2_SRA.html#assumption-3",
    "title": "Simple Regression Model",
    "section": "Assumption 3:",
    "text": "Assumption 3:\n\n\n\n\n\n\nThere is variation in the explanatory variable\n\n\n\\[\\sum_{i=1}^n (x_i - \\bar x)^2 &gt;0\n\\]\n\n\n\nIf there is no variation in the data, there are no slopes to estmate, and a solution cannot be found to the linear model."
  },
  {
    "objectID": "rmethods/2_SRA.html#assumption-4",
    "href": "rmethods/2_SRA.html#assumption-4",
    "title": "Simple Regression Model",
    "section": "Assumption 4:",
    "text": "Assumption 4:\n\n\n\n\n\n\nZero Conditional Mean\n\n\n\\[E(u_i)= E(u_i|x_i) = 0\n\\]\n\n\n\n\nWe expect unobserved factors \\(u_i\\) to have a zero average effect on the outcome. This helps identify the constant \\(\\beta_0\\).\nWe also expect that the expected value of \\(u_i\\) to be zero for any value of \\(x\\)."
  },
  {
    "objectID": "rmethods/2_SRA.html#unbiased-coefficients",
    "href": "rmethods/2_SRA.html#unbiased-coefficients",
    "title": "Simple Regression Model",
    "section": "Unbiased Coefficients:",
    "text": "Unbiased Coefficients:\nIf Assumptions 1-4 Hold, then OLS allows you to estimate the coefficents of the linear Regression model.\n\\[\\hat \\beta_1 = \\frac {\\sum \\tilde x_i \\tilde y_i}{\\sum \\tilde x_i^2} , \\tilde x_i=x_i - \\bar x\n\\]\n\\[\\begin{aligned}\n\\hat \\beta_1 &= \\frac {\\sum \\tilde x_i (\\beta_1 \\tilde x_i +e)}{\\sum \\tilde x_i^2} = \\beta_1 \\frac {\\sum  \\tilde x_i^2 }{\\sum \\tilde x_i^2} + \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2}  \\\\  \nE(\\hat \\beta_1) &= \\beta_1\n\\end{aligned}\n\\]\nWhile coefficients can be different for each sample, In average, they will be the same as the true parameters."
  },
  {
    "objectID": "rmethods/2_SRA.html#variance-of-ols-estimators",
    "href": "rmethods/2_SRA.html#variance-of-ols-estimators",
    "title": "Simple Regression Model",
    "section": "Variance of OLS Estimators",
    "text": "Variance of OLS Estimators\nHow precise are the estimates?\n\\[\\hat \\beta_1 = \\beta_1 + \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2}\n\\]\n\nIf we assume \\(x's\\) are assume fixed, the distribution from \\(\\beta's\\) will depend only on the variation of the error \\(u_i\\).\nThus we need to impose an additional assumption on this errors, to estimate the variance of \\(\\beta's\\). (at least for convinience)"
  },
  {
    "objectID": "rmethods/2_SRA.html#assumption-5",
    "href": "rmethods/2_SRA.html#assumption-5",
    "title": "Simple Regression Model",
    "section": "Assumption 5:",
    "text": "Assumption 5:\n\n\n\n\n\n\nErrors are Homoskedastic\n\n\n\\[E(u_i^2)= E(u_i ^2 | x_i) = \\sigma_u ^2\n\\]\n\n\n\n\nThis simplifying assumption states that the “distribution” of the errors is constant, regardless of \\(x\\)."
  },
  {
    "objectID": "rmethods/2_SRA.html#visualization-2",
    "href": "rmethods/2_SRA.html#visualization-2",
    "title": "Simple Regression Model",
    "section": "Visualization",
    "text": "Visualization\n\n\nCode\nclear\nset scheme white2\nset obs 1000\ngen x = runiform(-2 , 2)    \ngen u = rnormal()\ngen y1 = x + u\ngen y2 = x + u*abs(x)\ngen y3 = x + u*(2-abs(x))\ngen y4 = x + u*(sin(2*x))\n\n\ntwo scatter y1 x, name(m1, replace)\nscatter y2 x, name(m2, replace)\nscatter y3 x, name(m3, replace)\nscatter y4 x, name(m4, replace)\ngraph combine m1 m2 m3 m4\ngraph export images/fig2_9.png, replace width(1000)"
  },
  {
    "objectID": "rmethods/2_SRA.html#sampling-variance-of-ols",
    "href": "rmethods/2_SRA.html#sampling-variance-of-ols",
    "title": "Simple Regression Model",
    "section": "Sampling Variance of OLS",
    "text": "Sampling Variance of OLS\nWe Start with:\n\\[\n\\hat \\beta_1 - \\beta_1 = \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2}\n\\]\nAnd apply the Variance operator:\n\\[\\begin{aligned}\nVar(\\hat \\beta_1 - \\beta_1) &= Var \\left( \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2} \\right)=\n            \\frac{\\tilde x_1 Var(u_i)}{(\\sum x_i^2)^2}+\\frac{\\tilde x_2^2 Var(u_i)}{(\\sum x_i^2)^2}+...+\\frac{\\tilde x_n^2 Var(u_i)}{(\\sum x_i^2)^2} \\\\\n            &= \\frac {\\sum  \\tilde x_i^2 Var( u_i) }{(\\sum \\tilde x_i^2)^2} =\\frac {\\sum  \\tilde x_i^2 \\sigma_u^2 }{(\\sum \\tilde x_i^2)^2} \\\\\n            &= \\sigma_u^2 \\frac {\\sum  \\tilde x_i^2 }{(\\sum \\tilde x_i^2)^2} = \\frac{\\sigma_u^2}{\\sum \\tilde x_i^2}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#last-piece-of-the-puzze-sigma2_u",
    "href": "rmethods/2_SRA.html#last-piece-of-the-puzze-sigma2_u",
    "title": "Simple Regression Model",
    "section": "Last Piece of the Puzze \\(\\sigma^2_u\\)",
    "text": "Last Piece of the Puzze \\(\\sigma^2_u\\)\nTo estimate \\(Var(\\beta's)\\) we also need \\(\\sigma^2_u\\). But \\(u\\) is not observed, since we only observe \\(\\hat u\\).\n\\[\\hat u_i = u_i + (\\beta_0 - \\hat \\beta_0) + (\\beta_1 - \\hat \\beta_1)*x_i\n\\]\nAnd since to estimate \\(\\hat u_i\\) we need to estimate \\(\\beta_0\\) and \\(\\beta_1\\), we “lose” degrees of freedom that will require adjustment.\nSo, we use the following:\n\\[\\hat \\sigma^2_u = \\frac {1}{N-2} \\sum_{i=1}^N \\hat u_i^2\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#examples",
    "href": "rmethods/2_SRA.html#examples",
    "title": "Simple Regression Model",
    "section": "Examples",
    "text": "Examples\nDeriving OLS \\(\\beta's\\):\n\n** Wage and Education: Example 2.7\n\nfrause wage1, clear\nmata: y = st_data(.,\"wage\")\nmata: x = st_data(.,\"educ\")\nmata: b1 = sum( (x :- mean(x)) :* (y :- mean(y)) ) / sum( (x :- mean(x)):^2 ) \nmata: b0 = mean(y)-mean(x)*b1\nmata: b1, b0\n\n                 1             2\n    +-----------------------------+\n  1 |  .5413592547   -.904851612  |\n    +-----------------------------+\n\n\nSST = SSE + SSR\n\nmata: yh  = b0:+b1*x\nmata: sst = sum( (y:-mean(y)):^2 )\nmata: sse = sum( (yh:-mean(y)):^2 )\nmata: ssr = sum( (y:-yh):^2 )\nmata: sst, sse, ssr, sse + ssr\n\n                 1             2             3             4\n    +---------------------------------------------------------+\n  1 |  7160.414291   1179.732036   5980.682255   7160.414291  |\n    +---------------------------------------------------------+\n\n\n\\(R^2\\):\n\nmata: sse/sst , 1-ssr/sst\n\n                1            2\n    +---------------------------+\n  1 |  .164757511   .164757511  |\n    +---------------------------+\n\n\n\\(\\hat\\sigma_\\beta\\)\n\nmata: sig2_u = ssr / (rows(y)-2)\nmata: sst_x  = sum( (x:-mean(x)):^2 )\nmata: sig_b1 = sqrt( sig2_u / sst_x )\nmata: sig_b0 = sqrt( sig2_u * mean(x:^2) / sst_x ) \nmata: sig_b1 , sig_b0\n\n                 1             2\n    +-----------------------------+\n  1 |  .0532480368   .6849678211  |\n    +-----------------------------+\n\n\nStata command:\n\n\nCode\nregress wage educ\n\n\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =    103.36\n       Model |  1179.73204         1  1179.73204   Prob &gt; F        =    0.0000\n    Residual |  5980.68225       524  11.4135158   R-squared       =    0.1648\n-------------+----------------------------------   Adj R-squared   =    0.1632\n       Total |  7160.41429       525  13.6388844   Root MSE        =    3.3784\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .5413593    .053248    10.17   0.000     .4367534    .6459651\n       _cons |  -.9048516   .6849678    -1.32   0.187    -2.250472    .4407687\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/2_SRA.html#expanding-on-slrm-units-of-measure",
    "href": "rmethods/2_SRA.html#expanding-on-slrm-units-of-measure",
    "title": "Simple Regression Model",
    "section": "Expanding on SLRM: Units of Measure",
    "text": "Expanding on SLRM: Units of Measure\nFirst thing you should always consider doing is obtaining some summary statistics.\nwithout that its difficult o understand the magnitud of the coefficients and their effects.\n\n\nCode\nfrause ceosal1, clear\ndisplay \"***Variables Description***\"\ndes salary roe\ndisplay \"***Summary Statistics***\"\nsum salary roe\ndisplay \"***Simple Regression***\"\nreg salary roe\n\n\n***Variables Description***\n\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nsalary          int     %9.0g                 1990 salary, thousands $\nroe             float   %9.0g                 return on equity, 88-90 avg\n***Summary Statistics***\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      salary |        209     1281.12    1372.345        223      14822\n         roe |        209    17.18421    8.518509         .5       56.3\n***Simple Regression***\n\n      Source |       SS           df       MS      Number of obs   =       209\n-------------+----------------------------------   F(1, 207)       =      2.77\n       Model |  5166419.04         1  5166419.04   Prob &gt; F        =    0.0978\n    Residual |   386566563       207  1867471.32   R-squared       =    0.0132\n-------------+----------------------------------   Adj R-squared   =    0.0084\n       Total |   391732982       208  1883331.64   Root MSE        =    1366.6\n\n------------------------------------------------------------------------------\n      salary | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         roe |   18.50119   11.12325     1.66   0.098    -3.428196    40.43057\n       _cons |   963.1913   213.2403     4.52   0.000     542.7902    1383.592\n------------------------------------------------------------------------------\n\n\nNow, Changing scales has no effect on \\(R^2\\), nor the coefficient to Standard error ratio (\\(t-stat\\))\nIt could allow for easier interpretation of the results.\n\ngen saldol=salary*1000\ngen roedec=roe / 100\nqui: reg salary roe\nest sto m1\nqui: reg saldol roe\nest sto m2\nqui: reg salary roedec\nest sto m3\nesttab m1 m2 m3, se r2\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                   salary          saldol          salary   \n------------------------------------------------------------\nroe                 18.50         18501.2                   \n                  (11.12)       (11123.3)                   \n\nroedec                                             1850.1   \n                                                 (1112.3)   \n\n_cons               963.2***     963191.3***        963.2***\n                  (213.2)      (213240.3)         (213.2)   \n------------------------------------------------------------\nN                     209             209             209   \nR-sq                0.013           0.013           0.013   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/2_SRA.html#expanding-on-slrm-nonlinearities",
    "href": "rmethods/2_SRA.html#expanding-on-slrm-nonlinearities",
    "title": "Simple Regression Model",
    "section": "Expanding on SLRM: Nonlinearities",
    "text": "Expanding on SLRM: Nonlinearities\nIt is possible to incorporate some nonlinearities by using “log” transformations:\n\\[\n\\begin{aligned}\nlog(y) &= \\beta_0 + \\beta_1 x + e \\rightarrow & 100\\beta_1 \\simeq \\frac{\\% \\Delta y}{\\Delta x} \\\\\ny &= \\beta_0 + \\beta_1 log(x) + e \\rightarrow & \\frac {\\beta_1}{100} \\simeq \\frac{\\Delta y}{\\% \\Delta x} \\\\\nlog(y) &= \\beta_0 + \\beta_1 log(x) + e \\rightarrow & \\beta_1 =\\frac{\\% \\Delta y}{\\% \\Delta x}\n\\end{aligned}\n\\]\n\nThis allows us to estimate other interesting relationships with a the SRM. Specifically Semi-elasticities and Elasticities.\nThis transformation compresses the distribution of a variable, potentially addressing problems of Heteroskedasticity (non-constant variance)\n\n\n\nCode\n*** Example 2.10\nfrause wage1, clear\ngen lnwage = log(wage)\ngen lneduc = log(educ)\ntwo scatter wage educ     || lfit wage educ    , ///\n    name(m1, replace) title(lin-lin) legend(off)\ntwo scatter lnwage educ   || lfit lnwage educ  , ///\n    name(m2, replace) title(log-lin) legend(off)\ntwo scatter wage lneduc   || lfit wage lneduc  , ///\n    name(m3, replace) title(lin-log) legend(off)\ntwo scatter lnwage lneduc || lfit lnwage lneduc, ///\n    name(m4, replace) title(log-log) legend(off)\ngraph combine m1 m2 m3 m4, \ngraph export images/fig2_10.png, width(1000) replace\n\n\n\n\n\n\n\nCode\nset linesize 255\nqui: reg wage educ\nest sto m1\nqui: reg lnwage educ\nest sto m2\nqui: reg wage lneduc\nest sto m3\nqui: reg lnwage lneduc\nest sto m4\nesttab m1 m2 m3 m4, se r2 nonumber  compress nostar nogaps\n\n\n\n--------------------------------------------------\n                wage    lnwage      wage    lnwage\n--------------------------------------------------\neduc           0.541    0.0827                    \n            (0.0532) (0.00757)                    \nlneduc                             5.330     0.825\n                                 (0.608)  (0.0864)\n_cons         -0.905     0.584    -7.460    -0.445\n             (0.685)  (0.0973)   (1.532)   (0.218)\n--------------------------------------------------\nN                526       526       524       524\nR-sq           0.165     0.186     0.128     0.149\n--------------------------------------------------\nStandard errors in parentheses\n\n\n\n\nAn additional year of education\n\n1. Increases hourly wages in 54cnts\n\n2. Increases hourly wages in 8.3%\n\n3. A 1% increase in years of education (about 1.5months) increases wages in 5.3cnts\n\n4. A 1% increase in years of education would increase wages in 0.82%."
  },
  {
    "objectID": "rmethods/2_SRA.html#expanding-on-slrm-using-dummies",
    "href": "rmethods/2_SRA.html#expanding-on-slrm-using-dummies",
    "title": "Simple Regression Model",
    "section": "Expanding on SLRM: Using Dummies",
    "text": "Expanding on SLRM: Using Dummies\n\nA SLRM can also be done using Dummy variables. (Those that take only two values: 0 or 1)\nThis type of modeling may be observed when evaluating programs (Were you treated?(Tr=1) or not (Tr=0))\nAnd can be used to Easily compare means across two groups:\n\n\\[wage = \\beta_0 + \\beta_1 female + e\n\\]\nIn this particular case, both \\(\\beta_0 \\& \\beta_1\\) have clear interpretation:\n\\[\n\\begin{aligned}\n\\beta_0 &= E(wage|female=0) \\\\\n\\beta_1 &= E(wage|female=1) - E(wage|female=0)\n\\end{aligned}\n\\]\nIn most Software, you need to either Create the new variable explicitly, or use internal code to make it for you:\n\nfrause wage1, clear\n** verify Coding\nssc install fre, replace\nfre female\n** create your own\ngen is_male = female==0\n** Regression using Newly created variable\nreg wage is_male\n** Regression using Stata \"factor notation\"\nreg wage i.female\n\nchecking fre consistency and verifying not already installed...\nall files already exist and are up to date.\n\nfemale -- =1 if female\n-----------------------------------------------------------\n              |      Freq.    Percent      Valid       Cum.\n--------------+--------------------------------------------\nValid   0     |        274      52.09      52.09      52.09\n        1     |        252      47.91      47.91     100.00\n        Total |        526     100.00     100.00           \n-----------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =     68.54\n       Model |  828.220467         1  828.220467   Prob &gt; F        =    0.0000\n    Residual |  6332.19382       524  12.0843394   R-squared       =    0.1157\n-------------+----------------------------------   Adj R-squared   =    0.1140\n       Total |  7160.41429       525  13.6388844   Root MSE        =    3.4763\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     is_male |    2.51183   .3034092     8.28   0.000     1.915782    3.107878\n       _cons |   4.587659   .2189834    20.95   0.000     4.157466    5.017852\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =     68.54\n       Model |  828.220467         1  828.220467   Prob &gt; F        =    0.0000\n    Residual |  6332.19382       524  12.0843394   R-squared       =    0.1157\n-------------+----------------------------------   Adj R-squared   =    0.1140\n       Total |  7160.41429       525  13.6388844   Root MSE        =    3.4763\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    1.female |   -2.51183   .3034092    -8.28   0.000    -3.107878   -1.915782\n       _cons |   7.099489   .2100082    33.81   0.000     6.686928     7.51205\n------------------------------------------------------------------------------\n\n\nif the Dummy is a treatment, and Assumption 4 Holds, then you can use this to estimate Average Treatment Effects (ATE) aka Average Casual Effects. (Usually requires randomization)\n\\[\n\\begin{aligned}\ny_i &= y_i(0)(1-D) + y_i(1)D  \\\\\ny_i &= y_i(0) + (y_i(1)-y_i(0))*D \\\\\ny_i &= \\bar y_0 + u_i(0) + (\\bar y_1 - \\bar y_0)*D + (u_i(1)-u_i(0))*D \\\\\ny_i &= \\alpha_0 + \\tau_{ate} D + u_i\n\\end{aligned}\n\\]\nBut we expect \\(u_i(1)-u_i(0)=0\\)\n\n** Example 2.14\nfrause jtrain2, clear\n** Training was Randomly assigned\nreg re78 i.train\n\n\n      Source |       SS           df       MS      Number of obs   =       445\n-------------+----------------------------------   F(1, 443)       =      8.04\n       Model |  348.013451         1  348.013451   Prob &gt; F        =    0.0048\n    Residual |  19177.6432       443  43.2903909   R-squared       =    0.0178\n-------------+----------------------------------   Adj R-squared   =    0.0156\n       Total |  19525.6566       444  43.9767041   Root MSE        =    6.5795\n\n------------------------------------------------------------------------------\n        re78 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     1.train |   1.794343   .6328536     2.84   0.005     .5505748    3.038111\n       _cons |   4.554802    .408046    11.16   0.000     3.752856    5.356749\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#defending-the-sacred-time-line",
    "href": "rmethods/13_advtimeseries.html#defending-the-sacred-time-line",
    "title": "Times Series Part-II",
    "section": "Defending the Sacred time line",
    "text": "Defending the Sacred time line"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#last-week",
    "href": "rmethods/13_advtimeseries.html#last-week",
    "title": "Times Series Part-II",
    "section": "Last week",
    "text": "Last week\n\nWhat we learned last week a few methodologies for analyzing time series data.\n\nStatic model, dynamic models, use of trends and seasonality, etc.\n\nAll those models, however, were based on very strong assumptions.\n\nStrict Exogeneity, strict Homoskedasticity, and no serial correlation are difficult to defend.\nStatistical inference validity depends on normality assumption of the error.\n\nCan we relax this assumptions?\n\nYes, but we need to impose additional assumtions to the data"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#this-week",
    "href": "rmethods/13_advtimeseries.html#this-week",
    "title": "Times Series Part-II",
    "section": "This week",
    "text": "This week\n\nOne way to relax the assumption of Strict Exogeneiety is assume time series are stationary and weakly dependent.\n\nWith this assumptions, LLN and CLT will hold for TS model.\n\nWhat do these two assumptions mean? (stationary and weakly dependent?)\nNamely, these assumptions impose restrictions on how data SHOULD behaive, so we can learn something from the data.\n\nThis means that what we observe in the data are based on data that has stable patterns. Otherwise, if they are unpredictable, we cannot learn anything. (This is the idea of stationarity)\nAt the same time, data cannot depend on its own past too much. So if we look at two periods that are farther apart, its like they are independent for most practical purposes."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#can-you-recognized-this-type-of-data",
    "href": "rmethods/13_advtimeseries.html#can-you-recognized-this-type-of-data",
    "title": "Times Series Part-II",
    "section": "Can you recognized this type of data?",
    "text": "Can you recognized this type of data?\n\nTrendsDensity"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#stationary-process",
    "href": "rmethods/13_advtimeseries.html#stationary-process",
    "title": "Times Series Part-II",
    "section": "Stationary Process",
    "text": "Stationary Process\n\nA Stationary process is one where the characteristics of the distribution remains Stable across time.\n\nSo we can learn something about it\n\n\nFor example consider the series \\(x=[x_1,x_2,\\dots,x_T]\\), where \\(T\\) can be vary large (infinite)\n\nIf the series is stationary, then:\n\n\\[f(x_t, x_{t+k}) = f(x_s, x_{s+k}) \\forall s, k\\]\n\nIn other words, the distribution across time does not change. (is stable, thus stationary)\nOtherwise, if the distribution function changes constantly, we cannot learn about it, thus making sense of regressions would be even more difficult."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#testing-for-stationarity",
    "href": "rmethods/13_advtimeseries.html#testing-for-stationarity",
    "title": "Times Series Part-II",
    "section": "Testing for Stationarity",
    "text": "Testing for Stationarity\n\nTesting for stationarity is not easy. (how do you test for stability of densities?)\nInstead, from an empirical point of view, we focus on the first 2 moments of the distribution: mean, variance and covariance.\n\nThus:\nA series \\(x=[x_1,x_2,\\dots,x_T]\\) is stationary if:\n\n\\(E(x_t)=E(x_s)=\\mu_x\\). Constant mean\n\\(E(x_t^2)&lt;\\infty\\). Finite variance\n\\(Var(x_t)=Var(x_s)=\\sigma_x^2\\). Constant variance\n\\(cov(x_t,x_{t+k})=cov(x_s,x_{s+k})\\). Constant covariance\n\nThe Series repeats itself."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#weakly-dependent-series",
    "href": "rmethods/13_advtimeseries.html#weakly-dependent-series",
    "title": "Times Series Part-II",
    "section": "Weakly Dependent Series",
    "text": "Weakly Dependent Series\n\nDependence should be understood as a measure of how much a series depends on its own past.\n\nWeakly dependent series are those that depend on its own past, but not too much.\n\nFrom the technical point of view:\n\nIf \\(x_t\\) is weakly dependent, then \\(\\lim_{k\\rightarrow \\infty} corr(x_t,x_{t+k})=0\\) sufficiently fast.\n\nIf this happens, LLN and CLT will hold for TS data.\nWhy?\n\nOmiting \\(x_{t+1}\\) would typically generate an Omitted Variable Bias.\nIf data are weakly dependent, however, we won’t suffer from OMB\nThis is because omitting \\(x_{t+1}\\) is like omitting a unrelated variable.\n\n\n\nNOTE: Weakly dependent variables can be non-stationary. (we call them stationary around a trend)"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#visualizing-weakly-dependent-vs-strongly-dependent",
    "href": "rmethods/13_advtimeseries.html#visualizing-weakly-dependent-vs-strongly-dependent",
    "title": "Times Series Part-II",
    "section": "Visualizing weakly dependent vs strongly dependent",
    "text": "Visualizing weakly dependent vs strongly dependent"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#example-of-weakly-dependent-series",
    "href": "rmethods/13_advtimeseries.html#example-of-weakly-dependent-series",
    "title": "Times Series Part-II",
    "section": "Example of Weakly Dependent Series",
    "text": "Example of Weakly Dependent Series\n\nAR(1) process: \\(x_t = \\rho x_{t-1} + \\epsilon_t\\)\n\nMean in constant (not depent on time)\nVariance is constant (not depent on time)\nCovariance does not depend on time (just on lag)\nIf \\(|\\rho|&lt;1\\), then \\(x_t\\) is weakly dependent.\n\nMA(1) process: \\(x_t = \\epsilon_t + \\theta \\epsilon_{t-1}\\)\n\nConstant mean and variance.\nCovariance does not depend on time.\nCovariance bettween \\(x_t\\) and \\(x_{t+k}\\) is zero for \\(k&gt;1\\).\n\nARMA(1,1) \\(x_t = \\rho x_{t-1} + \\epsilon_t + \\theta \\epsilon_{t-1}\\)"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#special-case-ar1-with-rho1",
    "href": "rmethods/13_advtimeseries.html#special-case-ar1-with-rho1",
    "title": "Times Series Part-II",
    "section": "Special Case AR(1) with \\(\\rho=1\\)",
    "text": "Special Case AR(1) with \\(\\rho=1\\)\n\nIf \\(\\rho=1\\), we have a random walk process \\(x_t = x_{t-1} + \\epsilon_t\\)\nThis is a process that holds grudges (it remembers its past)\n\n\\[x_t = x_{t=0}+e_1+e_2+\\dots+e_t\\]\n\nThis process has constant mean (\\(x_0\\)), but variance and covariance are not constant. With a correlation that dissapears slowly.\n\n\\[Var(x_t) = t\\sigma^2 \\text{ and } cov(x_t,x_{t+h}) = t\\sigma^2\\]\n\\[corr(x_t,x_{t+h}) = \\frac{t \\sigma^2}{\\sqrt{t(t+h)}\\sigma^2}=\\sqrt{\\frac{t}{t+h}}\\]\nEconomics: With weakly dependent data, policies are transitory, with persistent data effects are longlasting."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#special-case-non-stationary-weakly-dependent",
    "href": "rmethods/13_advtimeseries.html#special-case-non-stationary-weakly-dependent",
    "title": "Times Series Part-II",
    "section": "Special Case: non-stationary weakly dependent",
    "text": "Special Case: non-stationary weakly dependent\n\nThere are few cases where a series is weakly dependent but non-stationary.\n\n\\[x_t = \\rho x_{t-1} + \\delta t + \\epsilon_t\\]\n\n\\(E(x_t)\\) is not constant, but this series is still weakly dependent if \\(|\\rho|&lt;1\\).\nIf the data is detrended, however, it becomes stationary."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#example-stationary-around-a-trend",
    "href": "rmethods/13_advtimeseries.html#example-stationary-around-a-trend",
    "title": "Times Series Part-II",
    "section": "Example: Stationary around a trend",
    "text": "Example: Stationary around a trend\n\\(x_t = 0.5*x_{t-1}+0.1 t + u_t\\)"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#example-random-walks-look-with-a-drift",
    "href": "rmethods/13_advtimeseries.html#example-random-walks-look-with-a-drift",
    "title": "Times Series Part-II",
    "section": "Example: Random Walks look with a drift",
    "text": "Example: Random Walks look with a drift\n\\[x_t = x_{t-1} + 0.1 + \\epsilon_t\\]"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#how-do-things-change-assumptions",
    "href": "rmethods/13_advtimeseries.html#how-do-things-change-assumptions",
    "title": "Times Series Part-II",
    "section": "How do things change: Assumptions",
    "text": "How do things change: Assumptions\nA1. Linear in Parameters (same as before), but all variables are stationary and weakly dependent.\nA2. No Perfect Colinearity\nA3. Zero Conditional Mean: \\(E(u_t|x_t)=0\\) Contemporaneous exogeneity!\nOmitting lags of \\(x_t\\) is not a problem, because they are weakly “independent” of \\(x_t\\).\nA1-A3 OLS is consistent.\nWhy does this matter??\n\nBecause, under Strict exogeneity, we cannot allow for Lags of the outcome to be included in the model.\nUnder weak exogeneity, lags can be included."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#how-do-things-change-assumptions-1",
    "href": "rmethods/13_advtimeseries.html#how-do-things-change-assumptions-1",
    "title": "Times Series Part-II",
    "section": "How do things change: Assumptions",
    "text": "How do things change: Assumptions\nA4. Homoskedasticity: \\(Var(u_t|x_t)=\\sigma^2\\) (also we just need contemporaenous homoskedasticity)\nA5. No Serial Correlation: \\(cov(u_t,u_s|x_t)=0\\) for \\(t\\neq s\\)\nThese two assumptions that make “life” easier for estimating standard errors because:\n\nUnder A1-A5, OLS estimators are asymptotically normal, and all Standard Statistics are applicable"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#order-of-integration",
    "href": "rmethods/13_advtimeseries.html#order-of-integration",
    "title": "Times Series Part-II",
    "section": "Order of Integration",
    "text": "Order of Integration\n\nAs you may expect, many interesting time series are not stationary. However, we may want to use for analysis\n\nto do so, we need to understand their taxonomy (in TS) so we can make them stationary.\n\nA series is said to be integrated of order \\(d\\), denoted \\(I(d)\\), if it can be made stationary by taking \\(d\\) differences.\nA weakly dependent series is an \\(I(0)\\) process. (is already stationary)\nA random walk is an \\(I(1)\\) process. It could be made stationary by taking first differences.\n\n\\[x_t = x_{t-1} + \\epsilon_t \\rightarrow \\Delta x_t = \\epsilon_t\\]\n\nA series that is \\(I(2)\\) would required two differences to be made stationary.\n\n\\[x_t =2x_{t-1} - x_{t-2} + \\epsilon_t \\rightarrow \\Delta^2 x_t = \\epsilon_t\\]"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#unit-roots-and-spurious-regressions",
    "href": "rmethods/13_advtimeseries.html#unit-roots-and-spurious-regressions",
    "title": "Times Series Part-II",
    "section": "Unit roots and Spurious Regressions",
    "text": "Unit roots and Spurious Regressions\n\nIf a series is \\(I(1)\\), it is said to have a unit root. an \\(I(2)\\) series has two unit roots, etc.\nData that are \\(I(1)\\) tend to look like data with Trends\n\nIf we analyze this data, we may find spurious relationships. t-stats may be high, as well as \\(R^2\\).\nThis may lead to incorrect conclusions (unless other stronger assumptions are made)\nIn this cases, using trends will not help."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#spurious-regressions-example",
    "href": "rmethods/13_advtimeseries.html#spurious-regressions-example",
    "title": "Times Series Part-II",
    "section": "Spurious Regressions: Example",
    "text": "Spurious Regressions: Example\n\\(x_t = x_{t-1} + v_t\\) & \\(y_t = y_{t-1} + u_t\\)"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#naive-approach.-look-into-rho",
    "href": "rmethods/13_advtimeseries.html#naive-approach.-look-into-rho",
    "title": "Times Series Part-II",
    "section": "Naive Approach. Look into \\(\\rho\\)",
    "text": "Naive Approach. Look into \\(\\rho\\)\n\nNaive approach: Look at auto correlation:\n\nif \\(corr(x_t,x_{t-1})&gt;0.9\\) then \\(x_t\\) is highly persistent, and probably \\(I(1)\\)\nDifferentiate data and look at auto correlation again."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#formal-approach-dickey-fuller-test-for-unit-root",
    "href": "rmethods/13_advtimeseries.html#formal-approach-dickey-fuller-test-for-unit-root",
    "title": "Times Series Part-II",
    "section": "Formal Approach: Dickey-Fuller Test for Unit Root",
    "text": "Formal Approach: Dickey-Fuller Test for Unit Root\nModel: \\(y_t = \\alpha + \\rho y_{t-1} + e_t\\) AModel: \\(\\Delta y_t = \\alpha + \\theta y_{t-1} + e_t\\)\n\n\\(H_0: \\rho=1\\) (has a unit root) vs \\(H_1: \\rho&lt;1\\) (is stationary)\n\\(H_0: \\theta=0\\) (has a unit root) vs \\(H_1: \\theta&lt;0\\) (is stationary)\n\nIts a one tail test, however, the statistic of interest does not follow a t-distribution, but a DF distribution\n\n\n\nSL\n1%\n2.5%\n5%\n10%\n\n\n\n\nDF\n-3.43\n-3.12\n-2.86\n-2.57"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#augmented-dickey-fuller-test",
    "href": "rmethods/13_advtimeseries.html#augmented-dickey-fuller-test",
    "title": "Times Series Part-II",
    "section": "Augmented Dickey-Fuller Test",
    "text": "Augmented Dickey-Fuller Test\nAllowing for serial correlation, and uses same critial values as before:\nModel: \\(\\Delta y_t = \\alpha + \\theta y_{t-1} + \\lambda_1 \\Delta y_{t-1} + \\dots + \\lambda_k \\Delta y_{t-k} + e_t\\)\nSame as before. But in practice the additional lags should be choosen based on information criteria."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#adf-with-a-trend",
    "href": "rmethods/13_advtimeseries.html#adf-with-a-trend",
    "title": "Times Series Part-II",
    "section": "ADF with a trend",
    "text": "ADF with a trend\nModel: \\(\\Delta y_t = \\alpha + \\delta t +\\theta y_{t-1} + \\lambda_1 \\Delta y_{t-1} + \\dots + \\lambda_k \\Delta y_{t-k} + e_t\\)\n\nThis allows for even more flexibility, or if you believe data is stationary around a trend.\nMain difference… critical values are even larger:\n\n\n\n\nSL\n1%\n2.5%\n5%\n10%\n\n\n\n\nDF\n-3.96\n-3.66\n-3.41\n-3.12\n\n\n\nAs before, If you find evidence of unit root, Differentiate and test again."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#example",
    "href": "rmethods/13_advtimeseries.html#example",
    "title": "Times Series Part-II",
    "section": "Example",
    "text": "Example\n\n\nCode\nqui:frause fertil3, clear\n** Setup as time series\ntsset year\n\n\n\nTime variable: year, 1913 to 1984\n        Delta: 1 unit\n\n\nThe model: \\(gfr = \\alpha + \\delta_0 pe_t + \\delta_1 pe_{t-1} + \\delta_2 pe_{t-2}+ e_t\\)\n\n\nCode\nreg gfr pe l.pe l2.pe  \n\n\n\n      Source |       SS           df       MS      Number of obs   =        70\n-------------+----------------------------------   F(3, 66)        =      0.14\n       Model |  159.461148         3   53.153716   Prob &gt; F        =    0.9383\n    Residual |  25832.9717        66  391.408663   R-squared       =    0.0061\n-------------+----------------------------------   Adj R-squared   =   -0.0390\n       Total |  25992.4329        69  376.701926   Root MSE        =    19.784\n\n------------------------------------------------------------------------------\n         gfr | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          pe |\n         --. |  -.0158445    .140256    -0.11   0.910    -.2958747    .2641856\n         L1. |  -.0213365   .2152292    -0.10   0.921    -.4510555    .4083826\n         L2. |   .0539005   .1381132     0.39   0.698    -.2218513    .3296524\n             |\n       _cons |   93.15791   4.499654    20.70   0.000     84.17406    102.1418\n------------------------------------------------------------------------------\n\n\nAre \\(gfr\\) and \\(pe\\) Stationary?\nNaive approach: Look at auto correlation:\n\n\nCode\n** Naive apprach\ncorr pe l.pe gfr l.gfr\n\n\n(obs=71)\n\n             |                 L.                L.\n             |       pe       pe      gfr      gfr\n-------------+------------------------------------\n          pe |\n         --. |   1.0000\n         L1. |   0.9636   1.0000\n         gfr |\n         --. |   0.0086   0.0188   1.0000\n         L1. |  -0.0300  -0.0296   0.9765   1.0000\n\n\n\nFormal Approach: Dickey-Fuller Test for Unit Root\n\n\nCode\nreg d.pe l.pe, nohead\nreg d.gfr l.gfr, nohead\n\n\n------------------------------------------------------------------------------\n        D.pe | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          pe |\n         L1. |  -.0521147   .0316692    -1.65   0.104    -.1152931    .0110637\n             |\n       _cons |   6.426196   3.808601     1.69   0.096    -1.171754    14.02415\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\n       D.gfr | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         gfr |\n         L1. |  -.0222798   .0260053    -0.86   0.395     -.074159    .0295994\n             |\n       _cons |   1.304937   2.548821     0.51   0.610    -3.779822    6.389695\n------------------------------------------------------------------------------\n\n\nSame conclusions. Are their differences stationary?\n\n\nCode\ngen dpe = d.pe \ngen dgfr = d.gfr\nreg d.dpe l.dpe, nohead\nreg d.dgfr l.dgfr, nohead\n\n\n(1 missing value generated)\n(1 missing value generated)\n------------------------------------------------------------------------------\n       D.dpe | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         dpe |\n         L1. |  -.7666022   .1181882    -6.49   0.000    -1.002443   -.5307613\n             |\n       _cons |   .8901863   2.103074     0.42   0.673    -3.306432    5.086804\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\n      D.dgfr | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        dgfr |\n         L1. |   -.713481   .1158143    -6.16   0.000    -.9445848   -.4823772\n             |\n       _cons |  -.6332004   .5027213    -1.26   0.212    -1.636365    .3699644\n------------------------------------------------------------------------------\n\n\nNow it should be better to use model in differences for analysi:\n\n\nCode\nreg dgfr dpe l.dpe l2.dpe  \n\n\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(3, 65)        =      6.56\n       Model |  293.259859         3  97.7532864   Prob &gt; F        =    0.0006\n    Residual |  968.199959        65   14.895384   R-squared       =    0.2325\n-------------+----------------------------------   Adj R-squared   =    0.1971\n       Total |  1261.45982        68  18.5508797   Root MSE        =    3.8595\n\n------------------------------------------------------------------------------\n        dgfr | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         dpe |\n         --. |  -.0362021   .0267737    -1.35   0.181     -.089673    .0172687\n         L1. |  -.0139706   .0275539    -0.51   0.614    -.0689997    .0410584\n         L2. |   .1099896   .0268797     4.09   0.000     .0563071    .1636721\n             |\n       _cons |  -.9636787   .4677599    -2.06   0.043     -1.89786   -.0294976\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#the-problem-of-serial-correlation",
    "href": "rmethods/13_advtimeseries.html#the-problem-of-serial-correlation",
    "title": "Times Series Part-II",
    "section": "The Problem of Serial Correlation",
    "text": "The Problem of Serial Correlation\n\nUp to this point, we have assumed that the error term is uncorrelated across time. (no serial correlation)\nAs with RC analysis, violation of this assumption does not lead to biased estimators of the coefficients (under usual situations), but it does lead to biased standard errors.\nWhy? If errors are correlated across time, (say possitively) then the variance of the OLS estimator is biased downwards.\n\n\\[Var(u_t+u_{t+h})=2\\sigma^2 + \\color{red}{2\\rho_{t,t+h}} \\sigma^2\\]"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#serial-correlation-and-lags",
    "href": "rmethods/13_advtimeseries.html#serial-correlation-and-lags",
    "title": "Times Series Part-II",
    "section": "Serial Correlation and Lags",
    "text": "Serial Correlation and Lags\n\nIf one has a model with Lags, then serial correlation is likely to happen.\n\n\\[y_t = \\beta_0 + \\beta_1 y_{t-1} + u_t\\]\n\nThis model simply assumes that \\(y_{t-1}\\) should be uncorrelated with \\(u_t\\). But, it may be that \\(y_{t-2}\\) is correlated with \\(u_t\\).\nIf that is the case then \\(Corr(u_t, u_{t-1})\\neq 0\\) because it may be picking up that correlation.\nOn the other hand, if \\(u_t\\) is serially correlated, then \\(y_{t-1}\\) is correlated with \\(u_t\\), causing OLS to be inconsistent.\n\nThis, however, may also indicate that one needs to consider a different model:\n\n\n\\[y_t = \\alpha_0 + \\alpha_1 y_{t-1} + \\alpha_2 y_{t-2} + e_t\\]\n\nWhere \\(e_t\\) is not serially correlated, not correlated with \\(y_{t-1}\\), nor \\(y_{t-2}\\)."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#test-for-serial-correlation",
    "href": "rmethods/13_advtimeseries.html#test-for-serial-correlation",
    "title": "Times Series Part-II",
    "section": "Test for Serial Correlation",
    "text": "Test for Serial Correlation\nStrictly Exogenous Regressors\nModel: \\(y_t = \\beta_0 + \\beta_1 x_{1,t} + \\dots + \\beta_k x_{k,t} + u_t\\) and: \\(u_t=\\rho u_{t-1}+e_t\\)\nIf there is no serial correlation, then we simply need to test if \\(\\rho=0\\), using a t-statistic.\nDurbin Watson Test\nUnder Classical assumptions, one could also use the DW statistic:\n\\[DW = \\frac{\\sum_{t=2}^T (u_t-u_{t-1})^2}{\\sum_{t=1}^T u_t^2}\\]\nwhere \\(DW\\simeq 2(1-\\hat{\\rho})\\).\n\nif there is no serial correlation, then \\(DW\\simeq 2\\).\nIf there is possitive serial correlation, then \\(DW&lt;2\\).\nA less practical test, but valid in small samples"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#test-for-serial-correlation-1",
    "href": "rmethods/13_advtimeseries.html#test-for-serial-correlation-1",
    "title": "Times Series Part-II",
    "section": "Test for Serial Correlation",
    "text": "Test for Serial Correlation\nWeakly Exogenous Regressors\nModel: \\(y_t = \\beta_0 + \\beta_1 x_{1,t} + \\dots + \\beta_k x_{k,t} + u_t\\) and: \\(u_t=\\rho u_{t-1}+\\gamma_1 x_{1,t} + \\dots + \\gamma_k x_{k,t}+e_t\\)\n\\(H0: \\rho=0\\) vs \\(H1: \\rho\\neq 0\\)\nTesting for higher order correlation\nand: \\(u_t=\\rho_1 u_{t-1}+\\rho_2 u_{t-2}+\\gamma_1 x_{1,t} + \\dots + \\gamma_k x_{k,t}+e_t\\)\n\\(H0: \\rho_1=0 \\& \\rho_2=0\\) vs \\(H1: \\text{one is not equal to }0\\)\nThis test is called the Breusch-Godfrey test."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#correcting-for-serial-correlation",
    "href": "rmethods/13_advtimeseries.html#correcting-for-serial-correlation",
    "title": "Times Series Part-II",
    "section": "Correcting for Serial Correlation:",
    "text": "Correcting for Serial Correlation:\nThere are two ways to correct for Serial Correlation:\n\nPrais-Winsten and Cochrane-Orcutt regression (Feasible GLS)\n\nRequires variables to be strictly exogenous regressors (no lagged dependent variables)\n\nNewey-West Standard Errors (this is the equivalent to Robust)\n\nGeneral setup."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#prais-winsten-and-cochrane-orcutt-regression",
    "href": "rmethods/13_advtimeseries.html#prais-winsten-and-cochrane-orcutt-regression",
    "title": "Times Series Part-II",
    "section": "Prais-Winsten and Cochrane-Orcutt regression",
    "text": "Prais-Winsten and Cochrane-Orcutt regression\nConsider the model:\n\\[y_t = \\beta_0 + \\beta_1 x_{1,t} + \\beta_2 x_{2,t} + u_t\\]\nwhere \\(u_t=\\rho u_{t-1}+e_{t}\\)\nThis model has serial correlation, which will affect the standard errors of the OLS estimator.\n\nif we know (or estimate) \\(\\rho\\), we can transform the data and eliminate the serial correlation\n\n\\[\\begin{aligned}\ny_t &= \\beta_0 + \\beta_1 x_{1,t} + \\beta_2 x_{2,t} + u_t \\\\\\\n\\rho y_{t-1} &= \\rho \\beta_0 + \\rho \\beta_1 x_{1,t-1} + \\rho \\beta_2 x_{2,t-1} + \\rho  u_{t-1} \\\\\\\n\\tilde y_t &= \\beta_0 (1-\\rho) + \\beta_1 \\tilde x_{1,t} + \\beta_2 \\tilde x_{2,t} +  e_t\n\\end{aligned}\n\\]\n\nFrom here, we can obtain the errors \\(e_t\\) and \\(u_t\\), re estimate \\(\\rho\\), and re estimate the model, until \\(\\rho\\) no longer changes.\nThis is called the Cochrane-Orcutt procedure."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#prais-winsten-and-cochrane-orcutt-regression-1",
    "href": "rmethods/13_advtimeseries.html#prais-winsten-and-cochrane-orcutt-regression-1",
    "title": "Times Series Part-II",
    "section": "Prais-Winsten and Cochrane-Orcutt regression",
    "text": "Prais-Winsten and Cochrane-Orcutt regression\n\nThe Prais-Winsten procedures is also similar to the Cochrane-Orcutt procedure, but you do not “loose” the first observation.\nSpecifically, the first observation is estimated as:\n\n\\[(1-\\rho^2)^{1/2} y_1 =(1-\\rho^2)^{1/2}\\beta_0 + (1-\\rho^2)^{1/2}\\beta_1 x_{1,1} + (1-\\rho^2)^{1/2}\\beta_2 x_{2,1} + (1-\\rho^2)^{1/2} u_1\\]\n\nOther features:\n\nPW can be more efficient than CO, because of the “saved observation”\nBoth can be used when serial correlation is of higher order, but only CO can be used if order is 3 or higher\nBoth methods are iterative\n\n\n\n\nCode\nfrause phillips, clear\ntsset year\nreg inf unem\n\n\n\nTime variable: year, 1948 to 2003\n        Delta: 1 unit\n\n      Source |       SS           df       MS      Number of obs   =        56\n-------------+----------------------------------   F(1, 54)        =      3.58\n       Model |   31.599858         1   31.599858   Prob &gt; F        =    0.0639\n    Residual |  476.815691        54   8.8299202   R-squared       =    0.0622\n-------------+----------------------------------   Adj R-squared   =    0.0448\n       Total |  508.415549        55  9.24391907   Root MSE        =    2.9715\n\n------------------------------------------------------------------------------\n         inf | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |   .5023782   .2655624     1.89   0.064    -.0300424    1.034799\n       _cons |   1.053566   1.547957     0.68   0.499    -2.049901    4.157033\n------------------------------------------------------------------------------\n\n\n\n\nCode\nprais inf unem, corc\n\nprais inf unem, \n\n\n\nIteration 0:   rho = 0.0000\nIteration 1:   rho = 0.5721\nIteration 2:   rho = 0.7204\nIteration 3:   rho = 0.7683\nIteration 4:   rho = 0.7793\nIteration 5:   rho = 0.7815\nIteration 6:   rho = 0.7819\nIteration 7:   rho = 0.7820\nIteration 8:   rho = 0.7820\nIteration 9:   rho = 0.7820\nIteration 10:  rho = 0.7820\n\nCochrane–Orcutt AR(1) regression with iterated estimates\n\n      Source |       SS           df       MS      Number of obs   =        55\n-------------+----------------------------------   F(1, 53)        =      5.09\n       Model |  23.3857044         1  23.3857044   Prob &gt; F        =    0.0282\n    Residual |  243.353574        53  4.59157686   R-squared       =    0.0877\n-------------+----------------------------------   Adj R-squared   =    0.0705\n       Total |  266.739278        54  4.93961626   Root MSE        =    2.1428\n\n------------------------------------------------------------------------------\n         inf | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |  -.6639584   .2942027    -2.26   0.028    -1.254054   -.0738626\n       _cons |   7.287078   2.163178     3.37   0.001     2.948291    11.62586\n-------------+----------------------------------------------------------------\n         rho |   .7820093\n------------------------------------------------------------------------------\nDurbin–Watson statistic (original)    = 0.801482\nDurbin–Watson statistic (transformed) = 1.600203\n\nIteration 0:   rho = 0.0000\nIteration 1:   rho = 0.5721\nIteration 2:   rho = 0.7350\nIteration 3:   rho = 0.7792\nIteration 4:   rho = 0.7871\nIteration 5:   rho = 0.7883\nIteration 6:   rho = 0.7885\nIteration 7:   rho = 0.7885\nIteration 8:   rho = 0.7885\nIteration 9:   rho = 0.7885\n\nPrais–Winsten AR(1) regression with iterated estimates\n\n      Source |       SS           df       MS      Number of obs   =        56\n-------------+----------------------------------   F(1, 54)        =      8.39\n       Model |   38.377534         1   38.377534   Prob &gt; F        =    0.0054\n    Residual |  246.917431        54  4.57254502   R-squared       =    0.1345\n-------------+----------------------------------   Adj R-squared   =    0.1185\n       Total |  285.294965        55  5.18718118   Root MSE        =    2.1384\n\n------------------------------------------------------------------------------\n         inf | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |  -.7139659   .2897858    -2.46   0.017    -1.294951   -.1329804\n       _cons |   7.999443   2.048343     3.91   0.000     3.892762    12.10612\n-------------+----------------------------------------------------------------\n         rho |   .7885234\n------------------------------------------------------------------------------\nDurbin–Watson statistic (original)    = 0.801482\nDurbin–Watson statistic (transformed) = 1.913928"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#newey-west-standard-errors",
    "href": "rmethods/13_advtimeseries.html#newey-west-standard-errors",
    "title": "Times Series Part-II",
    "section": "Newey-West Standard Errors",
    "text": "Newey-West Standard Errors\n\nThe Newey-West standard errors are similar to the robust standard errors, but they take into account the serial correlation of the error term.\nThe idea is to estimate an inflation factor that corrects Standard errors for serial correlation.\n\n\\(\\hat v = \\sum_{t=1}^T \\hat a_t^2 + 2 \\sum_{h=1}^g \\left[1-\\frac{h}{g+1}\\right] \\left( \\sum_{t=h+1}^{T} \\hat a_t \\hat a_{t-h}\\right)\\)\nwith \\(\\hat a_t = \\hat r_t \\hat u_t\\)\n\nThen \\(SE_c (\\beta) = \\sqrt{\\hat v} \\left(\\frac{SE(\\beta)}{\\sigma}\\right)^2\\)\nIn other words, this kind of corrects for the fact that the error term is correlated across time.\n\n\n\nCode\nnewey inf unem, lag(0)\nnewey inf unem, lag(1)\nnewey inf unem, lag(2)\n\n\n\nRegression with Newey–West standard errors      Number of obs     =         56\nMaximum lag = 0                                 F(  1,        54) =       4.11\n                                                Prob &gt; F          =     0.0477\n\n------------------------------------------------------------------------------\n             |             Newey–West\n         inf | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |   .5023782   .2479249     2.03   0.048     .0053187    .9994378\n       _cons |   1.053566   1.379772     0.76   0.448    -1.712711    3.819842\n------------------------------------------------------------------------------\n\nRegression with Newey–West standard errors      Number of obs     =         56\nMaximum lag = 1                                 F(  1,        54) =       3.25\n                                                Prob &gt; F          =     0.0769\n\n------------------------------------------------------------------------------\n             |             Newey–West\n         inf | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |   .5023782    .278577     1.80   0.077    -.0561351    1.060892\n       _cons |   1.053566   1.464589     0.72   0.475    -1.882759     3.98989\n------------------------------------------------------------------------------\n\nRegression with Newey–West standard errors      Number of obs     =         56\nMaximum lag = 2                                 F(  1,        54) =       3.13\n                                                Prob &gt; F          =     0.0827\n\n------------------------------------------------------------------------------\n             |             Newey–West\n         inf | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |   .5023782   .2841794     1.77   0.083    -.0673673    1.072124\n       _cons |   1.053566   1.424115     0.74   0.463    -1.801613    3.908744\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#cointegration",
    "href": "rmethods/13_advtimeseries.html#cointegration",
    "title": "Times Series Part-II",
    "section": "Cointegration",
    "text": "Cointegration\n\nAs previously mentioned, most interesting time series are not stationary.\nAnd, when using non-stationary data, we may find spurious relationships. But what if the relation is not spurious?\nConsider the following model:\n\\(y_t = \\beta_0 + \\beta_1 x_{1,t} + \\beta_2 x_{2,t} + u_t\\)\nIf \\(y_t\\) and \\(x_{1,t}\\) are \\(I(1)\\), then the model is likely to be spurious (common trends). However, it may be possible that there is a causal relationship between these variables.\nIf they indeed have a causal relationship, then they are said to be cointegrated."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#cointegration-1",
    "href": "rmethods/13_advtimeseries.html#cointegration-1",
    "title": "Times Series Part-II",
    "section": "Cointegration",
    "text": "Cointegration\n\nBut how do we know if two variables are cointegrated?\n\ns1: Check if all variables are \\(I(1)\\). If they are, then you can check for cointegration.\ns2: Estimate the model, and obtain the residuals \\(\\hat u_t\\).\ns3: Test if \\(\\hat u_t\\) is \\(I(0)\\).\n\nIf that is the case, then the variables are cointegrated (Share a long term relationship)\nIf not, the relationship is spurious\nHow do we test if \\(\\hat u_t\\) is \\(I(0)\\)? \\(\\rightarrow\\) Unit Root test!\n\n\n\n\nSL\n1%\n2.5%\n5%\n10%\n\n\n\n\nNo Trend\n-3.9\n-3.59\n-3.34\n-3.04\n\n\nWith Trend\n-4.32\n-4.03\n-3.78\n-3.50"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#error-correction-models",
    "href": "rmethods/13_advtimeseries.html#error-correction-models",
    "title": "Times Series Part-II",
    "section": "Error Correction Models",
    "text": "Error Correction Models\n\nIf two variables are cointegrated, then they share a long term relationship.\nHowever, you may also be interested in the short term dynamics of the relationship.\nTo do this, you can use an Error Correction Model (ECM)\n\n\\(\\Delta y_t = \\beta_0 + \\beta_1 \\Delta x_{1,t} + \\beta_2 \\Delta x_{2,t} + \\gamma \\hat u_{t-1}+ e_t\\)\nWhere \\(\\gamma\\) is the short term correction term."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#other-topics-of-interest",
    "href": "rmethods/13_advtimeseries.html#other-topics-of-interest",
    "title": "Times Series Part-II",
    "section": "Other topics of interest",
    "text": "Other topics of interest\n\nForcasting\n\nARIMA models, and VAR models (Vector Autoregressions) can be used for forcasting.\nForcating implies making predictions about the future, based on the past, accounting for the errors propagation.\nVariable selection, temporal causation (Granger Causality), and other techniques are used for this."
  },
  {
    "objectID": "rmethods/11_advpanel.html#the-old-way-last-class",
    "href": "rmethods/11_advpanel.html#the-old-way-last-class",
    "title": "Advanced Panel Data",
    "section": "The old way (last class)",
    "text": "The old way (last class)\n\nLast class we introduce the basic panel data model:\n\n\\[y_{it} = \\alpha + \\beta x_{it} + \\alpha_i + \\delta_t + \\epsilon_{it}\\]\n\nThis model could be estimated using First Differences approach.\n\n\\[\\Delta y_{it} = \\beta \\Delta x_{it} + \\delta_t + \\Delta \\epsilon_{it}\\]\n\nIt elimitates the \\(\\alpha_i\\), and constrains how \\(\\delta_t\\) is estimated.\nIt allows you to related how changes in \\(x_{it}\\) are related to changes in \\(y_{it}\\).\n\nThus Fixed variables across time cannot be identified.\n\nIt requires strong assumptions of strict exogeneity and no serial correlation."
  },
  {
    "objectID": "rmethods/11_advpanel.html#the-new-way-this-class",
    "href": "rmethods/11_advpanel.html#the-new-way-this-class",
    "title": "Advanced Panel Data",
    "section": "The new way (This class)",
    "text": "The new way (This class)\n\nToday we are going to describe the use of three other methods:\n\nFixed Effects (FE)\nRandom Effects (RE)\nCorrelated Random effects (CRE) \\(\\simeq\\) FE+RE\n\nThis method require their own methods, but could be used in more flexible scenarios.\nWhat do we mean Fixed effects? Random Effects? Correlated Random Effects?\n\nAll this will be estimation methods that relate to the same model.\nHowever, in all cases, we assume the unobserved are fixed factors across time. We simply identify them differently."
  },
  {
    "objectID": "rmethods/11_advpanel.html#fixed-effects-estimation",
    "href": "rmethods/11_advpanel.html#fixed-effects-estimation",
    "title": "Advanced Panel Data",
    "section": "Fixed Effects Estimation",
    "text": "Fixed Effects Estimation\nLets consider the following model: \\[y_{it} = \\beta_1 x_{it} + \\beta_2 z_{it} + \\alpha_i + \\epsilon_{it}\\]\nwhich doesnt include a time-fixed effect, nor time-invariante factors.\n\nThis could be estiamted simply adding dummies for each individual in the data set. (too many dummies). Instead consider the following\nNow, for each person, lets estimate the average characteristics \\(\\bar w = \\frac{1}{T} \\sum_{t=1}^T w_{it}\\). We could apply this to the model above an dobtain:\n\n\\[\\bar y_{i} = \\beta_1 \\bar x_{i} + \\beta_2 \\bar z_{i} + \\alpha_i + \\bar \\epsilon_{i}\\]\n\nThis no longer change across time.\nIt is a model interesting on itself. It captures Between Effects."
  },
  {
    "objectID": "rmethods/11_advpanel.html#section",
    "href": "rmethods/11_advpanel.html#section",
    "title": "Advanced Panel Data",
    "section": "",
    "text": "Now, lets substract this from the original model:\n\n\\[y_{it}-\\bar y_{i} = \\beta_1 (x_{it}- \\bar x_{i}) + \\beta_2 (z_{it}- \\bar z_{i}) + e_{it} - \\bar \\epsilon_{i}\\] \\[\\tilde y_{it} = \\beta_1 \\tilde  x_{it} + \\beta_2 \\tilde  z_{it} + \\tilde \\epsilon_{it}\\]\n\nWhat we have just done is apply the within transformation. The model above now captures the relationship between \\(X's\\) and \\(Y's\\) using only changes within each individual.\n\nThis “ignores” variation across individuals.\n\nThis within transformation eliminates all time-invariant factors, including \\(\\alpha_i\\).\n\nAlso of interest: * This model could now be estimated using OLS * Its an application of the FWL theorem. (we partial out the time-invariant factors) * If done by OLS, you need to correct the Degrees of freedom. (NT-N-k)"
  },
  {
    "objectID": "rmethods/11_advpanel.html#expanding-the-model-time-fixed-effects",
    "href": "rmethods/11_advpanel.html#expanding-the-model-time-fixed-effects",
    "title": "Advanced Panel Data",
    "section": "Expanding the model: Time fixed effects",
    "text": "Expanding the model: Time fixed effects\n\nNow, lets consider the following model:\n\n\\[y_{it} = \\beta_1 x_{it} + \\beta_2 z_{it} + \\alpha_i + \\delta_t + \\epsilon_{it}\\]\n\nWe could apply the same transformation as before, but now we need to consider the \\(\\delta_t\\).\n\nTypically, the number of time periods is small, and we could control for them using dummies. (need to be explicit about it)\nAltenativelly, One may need to use a Douple Demeaning approach.\n\n\n\\[\\tilde y_{it} = y_{it} - \\bar y_i - \\bar y_t + \\bar y\\]\nwhere \\(\\bar y_t\\) is the average across individuals, and \\(\\bar y\\) is the overall average of \\(y_{it}\\).\n\nThis will work as intended if the panel is balanced."
  },
  {
    "objectID": "rmethods/11_advpanel.html#when-panel-is-not-balanced",
    "href": "rmethods/11_advpanel.html#when-panel-is-not-balanced",
    "title": "Advanced Panel Data",
    "section": "When Panel is not balanced:",
    "text": "When Panel is not balanced:\n\nIf panel is not balanced, you need to demean data using interative methods.\nLets assume that \\(\\bar y=0\\). We would need to demean the data many times as follows:\n\n\\[ \\overline{ty}_{it} = y_{it} - \\bar y_i - \\bar y_t \\] \\[ \\overline{tty}_{it} = \\overline{ty}_{it} - \\overline{ty}_i - \\overline{ty}_t \\] \\[ \\overline{ttty}_{it} = \\overline{tty}_{it} - \\overline{tty}_i - \\overline{tty}_t \\]\nSo on and so forth, until there is no more variation in the transformed data.\n\ni.e. \\(\\overline{t \\dots ty}_i = \\overline{t\\dots t y}_t=0\\)\n\nNOTE: There are more efficient ways to do this."
  },
  {
    "objectID": "rmethods/11_advpanel.html#fe-vs-fd-balance-panel",
    "href": "rmethods/11_advpanel.html#fe-vs-fd-balance-panel",
    "title": "Advanced Panel Data",
    "section": "FE vs FD: Balance Panel",
    "text": "FE vs FD: Balance Panel\n\nFE and FD both aim to estimate the model by “eliminating” individual effects \\(\\alpha_i\\).\nWith \\(T=2\\), both will give you the same results.\nWith \\(T\\geq 3\\), you may need to choose based on assumptions on the error\n\nif \\(e_{it}\\) is serially uncorrelated, then FE is more efficient. Otherwise, FD may be better (if correlation is strong)\n\nOtherwise, typical suggestionis to try both, and evaluate the results.\nIn general, there are few arguments to choose between FD and FE.\n\nEmpirically, People use FE, because its the default in most software."
  },
  {
    "objectID": "rmethods/11_advpanel.html#fe-vs-fd-unbalanced-panel",
    "href": "rmethods/11_advpanel.html#fe-vs-fd-unbalanced-panel",
    "title": "Advanced Panel Data",
    "section": "FE vs FD: Unbalanced Panel",
    "text": "FE vs FD: Unbalanced Panel\n\nUnbalance panel data occures when different units are observed over different time periods.\n\nSome periods may or may not overlap, some may skip periods, etc\nIt may be more important understanding why one observes this kind of missing data problem.\n\nIf this is the case FD may be more difficult to use, because it requires data with regular time gaps. (Observations with missing data may be dropped)\nWith FE, you make most use of available data. Only those with “singletons” (units observed only once) are dropped."
  },
  {
    "objectID": "rmethods/11_advpanel.html#random-effects-models",
    "href": "rmethods/11_advpanel.html#random-effects-models",
    "title": "Advanced Panel Data",
    "section": "Random Effects Models",
    "text": "Random Effects Models\n\nEven if not done by hand, FE estimation is very computationally intensive and inneficient, because it requires estimating a large set of coefficients for indivuals.\n\nThis, however, its important if we believe that \\(\\alpha_i\\) are correlated with \\(x_{it}\\).\n\nIf \\(\\alpha_i\\) were uncorrelated with \\(x_{it}\\), then we could use a more efficient Approach: Random Effects model.\n\n\\(Corr(\\alpha_i, x_{it})=0\\) can be a very hard assumption to make.\n\nIf this is the case, we could estimate the model using OLS or Pool-OLS. Both would be consistent.\n\nHowever, the standard errors would be biased, because of the correlation across errors.\n\n\n\\[Corr(e_{it}+a_i,e{is}+a_i)=\\frac{\\sigma^2_a}{\\sigma^2_a+\\sigma^2_e}\\]"
  },
  {
    "objectID": "rmethods/11_advpanel.html#random-effects-models-se-estimation",
    "href": "rmethods/11_advpanel.html#random-effects-models-se-estimation",
    "title": "Advanced Panel Data",
    "section": "Random Effects Models: SE estimation",
    "text": "Random Effects Models: SE estimation\n\nThere are two ways to estimate the standard errors in a Random Effects model:\n\nOne could be to apply “clustered-robust” standard errors, using the individual as the cluster.\n\nIts a genereric solution to Clustering…Specially appropriate if we do not know how Clustering happens. (but we know\n\nThe second One is apply GLS. Since we know the “theoretical” correlation across errors, we could use this to transform the data, and estimate SE."
  },
  {
    "objectID": "rmethods/11_advpanel.html#section-1",
    "href": "rmethods/11_advpanel.html#section-1",
    "title": "Advanced Panel Data",
    "section": "",
    "text": "First, define:\n\\[\\theta = 1- \\left[ \\frac{\\sigma^2_e}{\\sigma^2_e+T \\sigma^2_a}\\right]^{1/2}\\]\n\nAll variables in the model (inclulding the constant) should be transformed using a quasi-differentiation as follows:\n\n\\[\\tilde w_{it} = w_{it} - \\theta \\bar w_i\\]\n\nThis transformation will eliminate the correlation across errors, and allow us to estimate the model using OLS. \\[y_{it}-\\theta \\bar y_i = \\beta_0 (1-\\theta) + b_1 (x_{it} - \\theta \\bar x_i) + b_2 (z_{it} - \\theta \\bar x_i) + v_{it} - \\theta \\bar v_i\\]"
  },
  {
    "objectID": "rmethods/11_advpanel.html#section-2",
    "href": "rmethods/11_advpanel.html#section-2",
    "title": "Advanced Panel Data",
    "section": "",
    "text": "Last pieces of the puzzle:\n\nEstimate the main model using pool OLS. \\(y_{it}=\\beta_0 + \\beta_1 x_{it} + v_{it}\\)\nEstimate \\(\\sigma^2_a\\) as: \\(\\hat \\sigma^2_a = \\frac{\\sum_{i=1}^N \\sum_{t=1}^{T-1}\\sum_{s=t+1}^{T} \\hat v_{it} \\hat v_{is}}{NT(T-1)/2 - (k+1)}\\)\nEstimate \\(\\sigma^2_e\\) as: \\(\\hat \\sigma_e^2 = \\hat\\sigma^2_v - \\hat \\sigma^2_a\\)\n\n\nBiggest Advantage of RE model is that you can now obtain effects for Time-invariant variables.\nIt is also more efficient, because you do not need to estimate individual effects, just capture the distribution of \\(\\alpha_i\\)."
  },
  {
    "objectID": "rmethods/11_advpanel.html#example",
    "href": "rmethods/11_advpanel.html#example",
    "title": "Advanced Panel Data",
    "section": "Example",
    "text": "Example\nIn Stata, you could estimate the panel models using the xtreg command.\nThis command has options for Fixed Effects, Between Effects and Random Effects.\n\n\nCode\nfrause wagepan, clear\n** Good idea to Set the data as panel data\nxtset nr year\n\n\n\n\n\n\nPanel variable: nr (strongly balanced)\n Time variable: year, 1980 to 1987\n         Delta: 1 unit\n\n\n\n\nCode\n** Pool OLS\nqui: reg lwage educ black hisp exper expersq married union,\nest sto m1\n** pool OLS with Clustered SE\nqui: reg lwage educ black hisp exper expersq married union, cluster(nr)\nest sto m2\n** Random Effects: Default\nqui:xtreg lwage educ black hisp exper expersq married union, re\nest sto m3\n** Fixed Effects\nqui:xtreg lwage educ black hisp exper expersq married union, fe\nest sto m4\nesttab m1 m2 m3 m4, se b(4) noomit nonumber mtitle(OLS OLS_CL RE FE)\n\n\n\n----------------------------------------------------------------------------\n                      OLS          OLS_CL              RE              FE   \n----------------------------------------------------------------------------\neduc               0.0994***       0.0994***       0.1012***                \n                 (0.0047)        (0.0092)        (0.0089)                   \n\nblack             -0.1438***      -0.1438**       -0.1441**                 \n                 (0.0236)        (0.0501)        (0.0476)                   \n\nhisp               0.0157          0.0157          0.0202                   \n                 (0.0208)        (0.0392)        (0.0426)                   \n\nexper              0.0892***       0.0892***       0.1121***       0.1168***\n                 (0.0101)        (0.0124)        (0.0083)        (0.0084)   \n\nexpersq           -0.0028***      -0.0028**       -0.0041***      -0.0043***\n                 (0.0007)        (0.0009)        (0.0006)        (0.0006)   \n\nmarried            0.1077***       0.1077***       0.0628***       0.0453*  \n                 (0.0157)        (0.0261)        (0.0168)        (0.0183)   \n\nunion              0.1801***       0.1801***       0.1074***       0.0821***\n                 (0.0171)        (0.0276)        (0.0178)        (0.0193)   \n\n_cons             -0.0347         -0.0347         -0.1075          1.0649***\n                 (0.0646)        (0.1201)        (0.1107)        (0.0267)   \n----------------------------------------------------------------------------\nN                    4360            4360            4360            4360   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/11_advpanel.html#pool-ols-vs-re-vs-fe",
    "href": "rmethods/11_advpanel.html#pool-ols-vs-re-vs-fe",
    "title": "Advanced Panel Data",
    "section": "Pool OLS vs RE vs FE",
    "text": "Pool OLS vs RE vs FE\n\nWe now know how to analyze panel data using three Stretegies: Pool OLS, Fixed Effects and, Random Effects.\n\nFE is usually prefered to RE, because is more consistent by relaxing the assumption of no correlation between \\(a_i\\) and \\(x_{it}\\) (explicit control). Its less efficient.\nRE may be prefer to FE if the correlation between \\(a_i\\) and \\(x_{it}\\) is small. Its more efficient, and allows to estimate effects for time-invariant variables.\nRE and POLS will be consistent under the same assumptions. However, RE will remove some of the serial correlation, and may have less bias than OLS (even if \\(a_i\\) and \\(x_{it})\\) are correlated.\n\nChoosing between RE and POLs is rarely considered. (RE would be the default in most cases)\nHowever, Choosing between FE vs RE is common: Hausman Test"
  },
  {
    "objectID": "rmethods/11_advpanel.html#hausman-test",
    "href": "rmethods/11_advpanel.html#hausman-test",
    "title": "Advanced Panel Data",
    "section": "Hausman Test",
    "text": "Hausman Test\n\nHausman test is used to determine which model to use between two estimators.\n\nYou assume FE is consistent (but not efficient).\nYou estimate the model using RE. If RE estimates are close to FE, then RE is consistent and efficient (preferred)\nOthewise, we suspect RE are inconsistent, and we use FE.\n\nFor most applied work, however, FE is generally prefered to RE\n\n\n\nCode\n** Hausman Test\n*** Consistent model FE\nqui:xtreg lwage educ black hisp exper expersq married union, fe\nest sto fe\n*** Efficient under H0\nqui:xtreg lwage educ black hisp exper expersq married union, re\nest sto re\nhausman fe re\n\n\n\n                 ---- Coefficients ----\n             |      (b)          (B)            (b-B)     sqrt(diag(V_b-V_B))\n             |       fe           re         Difference       Std. err.\n-------------+----------------------------------------------------------------\n       exper |    .1168467     .1121195        .0047272        .0016276\n     expersq |   -.0043009    -.0040689        -.000232        .0001269\n     married |    .0453033     .0627951       -.0174918        .0073427\n       union |    .0820871     .1073789       -.0252917        .0073636\n------------------------------------------------------------------------------\n                          b = Consistent under H0 and Ha; obtained from xtreg.\n           B = Inconsistent under Ha, efficient under H0; obtained from xtreg.\n\nTest of H0: Difference in coefficients not systematic\n\n    chi2(4) = (b-B)'[(V_b-V_B)^(-1)](b-B)\n            =  31.45\nProb &gt; chi2 = 0.0000"
  },
  {
    "objectID": "rmethods/11_advpanel.html#correlated-random-effects",
    "href": "rmethods/11_advpanel.html#correlated-random-effects",
    "title": "Advanced Panel Data",
    "section": "Correlated Random Effects",
    "text": "Correlated Random Effects\n\nThe CRE model is an alternative approach that combines some of the features of RE and FE estimators. \\[y_{it} = \\beta_1 x_{it} + \\beta_2 z_{i} + \\alpha_i + \\epsilon_{it}\\]\nOne way to look at the “individual” fixed effect is to model it as a function of fixed effects:\n\n\\[\\alpha_i = \\alpha + \\gamma_1 \\bar x_{i} + \\gamma_2 z_{i} + r_i\\]\n\nIn this case, we assume the fixed unobserved effect \\(\\alpha_i\\) could be written as a function of avg observed characteristics, and fixed factors.\nAnd we assume that \\(r_i\\) would be uncorrelated with \\(x_{it}\\) , \\(\\bar x_i\\) and \\(z_{i}\\)."
  },
  {
    "objectID": "rmethods/11_advpanel.html#section-3",
    "href": "rmethods/11_advpanel.html#section-3",
    "title": "Advanced Panel Data",
    "section": "",
    "text": "If we combine this with the main regression we have:\n\n\\[y_{it} = \\beta_1 x_{it} + \\beta_2 z_{i} + \\alpha + \\gamma_1 \\bar x_{i} + \\gamma_2 z_{i} + r_i + \\epsilon_{it}\\] \\[y_{it} = \\alpha + \\beta_1 x_{it} + \\beta_2 z_{i} + \\gamma_1 \\bar x_{i} + \\underbrace{\\nu_{it}}_{ r_i + \\epsilon_{it}}\\]\n\nWhich we could now estimate using Pool OLS or RE. There is no more need to worry about correlation between \\(r_i\\) and \\(x_{it}\\).\nDifferences and Advantages:\n\nWe must include individual level average characteristics.\nWe can now estimate effects for time-invariant variables.\nWe can test for FE vs RE models."
  },
  {
    "objectID": "rmethods/11_advpanel.html#correlated-random-effects-fe-vs-re",
    "href": "rmethods/11_advpanel.html#correlated-random-effects-fe-vs-re",
    "title": "Advanced Panel Data",
    "section": "Correlated Random Effects: FE vs RE",
    "text": "Correlated Random Effects: FE vs RE\n\nCRE estimates for Time varying variables are identical to FE. \\[\\hat \\beta_{cre}=\\hat \\beta_{fe}\\]\nCRE estimates shows clearly why RE are more efficient (RE imposes \\(\\gamma=0\\))\nThus, we can test for FE vs RE using the following test:\n\n\\[H_0: \\gamma = 0 \\text{ or } RE \\] \\[H_a: \\gamma \\neq 0 \\text{ or } FE \\]"
  },
  {
    "objectID": "rmethods/11_advpanel.html#example-1",
    "href": "rmethods/11_advpanel.html#example-1",
    "title": "Advanced Panel Data",
    "section": "Example",
    "text": "Example\n\n\nCode\nforeach i of varlist exper expersq married union {\n  bysort nr: egen mn_`i'=mean(`i')\n}\nxtreg lwage educ black hisp exper expersq married union mn_*, re\nest sto m2\n** FE vs RE\ntest mn_exper mn_expersq mn_married mn_union\n\n\n\nRandom-effects GLS regression                   Number of obs     =      4,360\nGroup variable: nr                              Number of groups  =        545\n\nR-squared:                                      Obs per group:\n     Within  = 0.1780                                         min =          8\n     Between = 0.2192                                         avg =        8.0\n     Overall = 0.2002                                         max =          8\n\n                                                Wald chi2(11)     =     976.27\ncorr(u_i, X) = 0 (assumed)                      Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0946036   .0109043     8.68   0.000     .0732315    .1159757\n       black |  -.1388124   .0488709    -2.84   0.005    -.2345977   -.0430271\n        hisp |   .0047758   .0426925     0.11   0.911    -.0788999    .0884515\n       exper |   .1168467   .0084197    13.88   0.000     .1003444     .133349\n     expersq |  -.0043009   .0006053    -7.11   0.000    -.0054872   -.0031146\n     married |   .0453033   .0183097     2.47   0.013      .009417    .0811896\n       union |   .0820871   .0192907     4.26   0.000      .044278    .1198963\n    mn_exper |  -.1672838    .051032    -3.28   0.001    -.2673046    -.067263\n  mn_expersq |   .0094254   .0032684     2.88   0.004     .0030195    .0158312\n  mn_married |   .0983604   .0450837     2.18   0.029     .0099979    .1867228\n    mn_union |   .1885894   .0504022     3.74   0.000     .0898029    .2873759\n       _cons |    .492309   .2210094     2.23   0.026     .0591386    .9254794\n-------------+----------------------------------------------------------------\n     sigma_u |  .32456727\n     sigma_e |  .35125535\n         rho |  .46057172   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\n\n ( 1)  mn_exper = 0\n ( 2)  mn_expersq = 0\n ( 3)  mn_married = 0\n ( 4)  mn_union = 0\n\n           chi2(  4) =   27.27\n         Prob &gt; chi2 =    0.0000\n\n\nComparing across models:\n\n\nCode\nqui:xtreg lwage educ black hisp exper expersq married union, fe\nest sto m1\nqui:xtreg lwage educ black hisp exper expersq married union, re\nest sto m3\nesttab m1 m2 m3, se b(4) noomit nonumber mtitle(FE CRE RE)\n\n\n\n------------------------------------------------------------\n                       FE             CRE              RE   \n------------------------------------------------------------\nexper              0.1168***       0.1168***       0.1121***\n                 (0.0084)        (0.0084)        (0.0083)   \n\nexpersq           -0.0043***      -0.0043***      -0.0041***\n                 (0.0006)        (0.0006)        (0.0006)   \n\nmarried            0.0453*         0.0453*         0.0628***\n                 (0.0183)        (0.0183)        (0.0168)   \n\nunion              0.0821***       0.0821***       0.1074***\n                 (0.0193)        (0.0193)        (0.0178)   \n\neduc                               0.0946***       0.1012***\n                                 (0.0109)        (0.0089)   \n\nblack                             -0.1388**       -0.1441** \n                                 (0.0489)        (0.0476)   \n\nhisp                               0.0048          0.0202   \n                                 (0.0427)        (0.0426)   \n\nmn_exper                          -0.1673**                 \n                                 (0.0510)                   \n\nmn_expersq                         0.0094**                 \n                                 (0.0033)                   \n\nmn_married                         0.0984*                  \n                                 (0.0451)                   \n\nmn_union                           0.1886***                \n                                 (0.0504)                   \n\n_cons              1.0649***       0.4923*        -0.1075   \n                 (0.0267)        (0.2210)        (0.1107)   \n------------------------------------------------------------\nN                    4360            4360            4360   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/11_advpanel.html#cre-implementation",
    "href": "rmethods/11_advpanel.html#cre-implementation",
    "title": "Advanced Panel Data",
    "section": "CRE Implementation",
    "text": "CRE Implementation\n\nAs shown above, in Stata, you could estimate the CRE panel models using the xtreg with RE option.\n\nYou just need to be careful when estimating the averages of all variables in the model.\nThis is particularly relevant for unbalance panel data.\nIn those cases, you could use cre (from fra install)\n\nYou could also extend this to using Multiple fixed effects (time and individual), but some equivalences are lost.\n\n\n\nCode\nfra install cre\ncre , abs(nr): xtreg lwage educ black hisp exper expersq married union, re \n\n\nchecking cre consistency and verifying not already installed...\nall files already exist and are up to date.\n\nRandom-effects GLS regression                   Number of obs     =      4,360\nGroup variable: nr                              Number of groups  =        545\n\nR-squared:                                      Obs per group:\n     Within  = 0.1780                                         min =          8\n     Between = 0.2192                                         avg =        8.0\n     Overall = 0.2002                                         max =          8\n\n                                                Wald chi2(11)     =     976.27\ncorr(u_i, X) = 0 (assumed)                      Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0946036   .0109043     8.68   0.000     .0732315    .1159757\n       black |  -.1388124   .0488709    -2.84   0.005    -.2345977   -.0430271\n        hisp |   .0047758   .0426925     0.11   0.911    -.0788999    .0884515\n       exper |   .1168467   .0084197    13.88   0.000     .1003444     .133349\n     expersq |  -.0043009   .0006053    -7.11   0.000    -.0054872   -.0031146\n     married |   .0453033   .0183097     2.47   0.013      .009417    .0811896\n       union |   .0820871   .0192907     4.26   0.000      .044278    .1198963\n    m1_exper |  -.1672838    .051032    -3.28   0.001    -.2673046    -.067263\n  m1_expersq |   .0094254   .0032684     2.88   0.004     .0030195    .0158312\n  m1_married |   .0983604   .0450837     2.18   0.029     .0099979    .1867228\n    m1_union |   .1885894   .0504022     3.74   0.000     .0898029    .2873759\n       _cons |  -.0330167   .1330654    -0.25   0.804    -.2938202    .2277867\n-------------+----------------------------------------------------------------\n     sigma_u |  .32456727\n     sigma_e |  .35125535\n         rho |  .46057172   (fraction of variance due to u_i)\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/11_advpanel.html#final-words",
    "href": "rmethods/11_advpanel.html#final-words",
    "title": "Advanced Panel Data",
    "section": "Final words",
    "text": "Final words\n\nAll estimation methodologies presented here could also be used in other contexts.\nExamples:\n\nGeronimus and Korenman (1992): Analysis of data siblings outcome accounting for family fixed effects.\nAshenfelder and Kruger (1994): Return to education using Twins Data.\n\nOne may also use the principles of Panel data with linked data in high dimensional data sets.\n\nEducation data and School FE\nHealth data and Hospital FE\nWages and Firm FE\netc.\n\nIn this cases, one may need to also considering explicit clustering in addition to “fixed effects”."
  },
  {
    "objectID": "rm-data/syllabus.html",
    "href": "rm-data/syllabus.html",
    "title": "Research Methods I",
    "section": "",
    "text": "Instructor: Fernando Rios-Avila\nEmail: friosavi@levy.org\nOffice Hours: Wednesdays 1:30pm to 4:00pm. Or by appointment. Other times can be arranged, but will be done remotely.\nCourse Website:\nClass Time: Wednesday, 9:30 am - 12:45 am"
  },
  {
    "objectID": "rm-data/syllabus.html#introduction-to-modern-research-tools",
    "href": "rm-data/syllabus.html#introduction-to-modern-research-tools",
    "title": "Research Methods I",
    "section": "1: Introduction to Modern Research Tools",
    "text": "1: Introduction to Modern Research Tools\n\nCourse overview and expectations.\nIntroduction to GitHub/Github-Desktop for version control and collaboration.\nGetting started with Quarto for reproducible research: RStudio and VSCode.\nOther Tools: Overleaf, Zotero\nData organization and management\nLab: Setting up GitHub, Quarto, VSCode, Zotero, and Overleaf"
  },
  {
    "objectID": "rm-data/syllabus.html#introduction-to-data-analysis",
    "href": "rm-data/syllabus.html#introduction-to-data-analysis",
    "title": "Research Methods I",
    "section": "2. Introduction to Data Analysis",
    "text": "2. Introduction to Data Analysis\n\nIntroduction to data analysis: The Process\nWhat is Data? What types of Data are there?\nData collection methods\nPreparing data for analysis\nTidy data principles\nData cleaning: Missing values, outliers, and errors\nReadings: Chapter 1 and 2"
  },
  {
    "objectID": "rm-data/syllabus.html#data-exploration",
    "href": "rm-data/syllabus.html#data-exploration",
    "title": "Research Methods I",
    "section": "3: Data Exploration",
    "text": "3: Data Exploration\n\nType of data vs type of analysis\nFrequencies, distributions, and summary statistics\nExploratory data analysis techniques and visualizations\nTheoretical distributions\nComparisons, correlations and conditional distributions\nLatent and observed variables\nReadings: Chapter 3 and 4"
  },
  {
    "objectID": "rm-data/syllabus.html#generalization-from-sample-to-population",
    "href": "rm-data/syllabus.html#generalization-from-sample-to-population",
    "title": "Research Methods I",
    "section": "4: Generalization: From Sample to Population",
    "text": "4: Generalization: From Sample to Population\n\nSampling and generalization\nRepetition and sampling variability\nConfidence intervals and standard errors: The Bootstrap method\nExternal validity\nHypothesis testing principles\nType I and Type II errors\nMultiple Hypothesis Testing and p-hacking\nReadings: Chapters 5 and 6"
  },
  {
    "objectID": "rm-data/syllabus.html#and-6-regression-analysis-i-simple-regression",
    "href": "rm-data/syllabus.html#and-6-regression-analysis-i-simple-regression",
    "title": "Research Methods I",
    "section": "5 and 6: Regression Analysis I: Simple Regression",
    "text": "5 and 6: Regression Analysis I: Simple Regression\n\nLinear and non-linear relationships\nLinear Regression: Estimation and interpretation\nCorrelations and coefficients: Searching for causality\nProperties and assumptions of the linear regression model\nTransformations and Semiparametric models\nExtreme values, Influential observations and measurement error\nGeneralizing Results: SE and CI\nTesting Hypotheses\nReadings: Chapters 7, 8, and 9"
  },
  {
    "objectID": "rm-data/syllabus.html#regression-analysis-ii-multiple-regression",
    "href": "rm-data/syllabus.html#regression-analysis-ii-multiple-regression",
    "title": "Research Methods I",
    "section": "7: Regression Analysis II: Multiple Regression",
    "text": "7: Regression Analysis II: Multiple Regression\n\nMultiple regression basics: Estimation and Inference\nProblems with multiple regression\nNon-linearities, interactions and qualitative variables\nReadings: Chapters 10"
  },
  {
    "objectID": "rm-data/syllabus.html#regression-analysis-iii-modeling-probabilities",
    "href": "rm-data/syllabus.html#regression-analysis-iii-modeling-probabilities",
    "title": "Research Methods I",
    "section": "8: Regression Analysis III: Modeling Probabilities",
    "text": "8: Regression Analysis III: Modeling Probabilities\n\nLinear Probability Model\nNon-linear models: Logit and Probit\nInterpretation and marginal effects\nGoodness of Fit and Predictive Power\nReadings: Chapter 11"
  },
  {
    "objectID": "rm-data/syllabus.html#time-series-analysis",
    "href": "rm-data/syllabus.html#time-series-analysis",
    "title": "Research Methods I",
    "section": "9: Time Series Analysis",
    "text": "9: Time Series Analysis\n\nIntroduction to time series data\nTrend and seasonality\nStationarity and autocorrelation\nSerial correlation\nReadings: Chapter 12"
  },
  {
    "objectID": "rm-data/syllabus.html#prediction",
    "href": "rm-data/syllabus.html#prediction",
    "title": "Research Methods I",
    "section": "10: Prediction",
    "text": "10: Prediction\n\nIntroduction to Prediction\nR2 vs AR2 and other measures of fit\nOverfitting and Cross-validation: Finding the right model\nExternal validity and generalization\nReadings: Chapter 13"
  },
  {
    "objectID": "rm-data/syllabus.html#model-building-for-prediction-lasso",
    "href": "rm-data/syllabus.html#model-building-for-prediction-lasso",
    "title": "Research Methods I",
    "section": "11: Model Building for Prediction: LASSO",
    "text": "11: Model Building for Prediction: LASSO\n\nThe Process of Prediction\nHow to choose \\(g(y)\\)\nWorking with \\(X's\\)\nIntroduction to LASSO: Prediction and Diagnosis\nReadings: Chapter 14"
  },
  {
    "objectID": "rm-data/syllabus.html#predicting-probabilities-and-classification",
    "href": "rm-data/syllabus.html#predicting-probabilities-and-classification",
    "title": "Research Methods I",
    "section": "12: Predicting Probabilities and Classification",
    "text": "12: Predicting Probabilities and Classification\n\nPredicting Probabilities and Classification\nClassification, confusion matrices, and ROC curves\nFinding the right threshold\nReadings: Chapter 17"
  },
  {
    "objectID": "rm-data/syllabus.html#forecasting-data",
    "href": "rm-data/syllabus.html#forecasting-data",
    "title": "Research Methods I",
    "section": "13: Forecasting Data",
    "text": "13: Forecasting Data\n\nIntroduction to Forecasting: Predicting the future\nTraining and Testing Data\nTrends, Seasonality, and Cycles\nForecasting with ARIMA\nVAR and External validity\nReadings: Chapter 18"
  },
  {
    "objectID": "rm-data/syllabus.html#term-paper-60-of-final-grade",
    "href": "rm-data/syllabus.html#term-paper-60-of-final-grade",
    "title": "Research Methods I",
    "section": "Term Paper (60% of final grade)",
    "text": "Term Paper (60% of final grade)\nThroughout the semester, students will work on a multi-part research project that applies the techniques learned in class to real-world data. This project will require students to propose a research question, collect and clean data, conduct exploratory and regression analyses, and present their findings.\n\nPart I: Research Proposal (5%, due Week 2)\n\nIntroduction (including research question and motivation)\nProposed data sources: Consider data from the textbook, Kaggle, or other sources\nExpected findings and relevance\nConclusion\nReferences (if any)\nSpecify which software (if other than Stata) you plan to use for your analysis\nCreate a GitHub repository for your project and submit the link with your proposal\n\n\n\nPart II: Data Collection and Cleaning (10%, due Week 4)\nWrite a report that includes the following:\n\nDescription of data sources, collection methods, and potential biases\nData overview and preprocessing steps\nDiscussion of issues encountered and solutions\nPreliminary analysis with descriptive statistics and visualizations\nInclude a data dictionary in your GitHub repository\n\n\n\nPeer Review Report (due Week 5)\n\nReview another student’s work and provide feedback on their data collection and cleaning process. Provide suggestions for improvement and identify any potential issues.\n\n\n\nInterim Progress Report (5%, due Week 7)\n\nBrief update on progress, challenges faced, and next steps\nAdditional visualizations or analyses\nIt should include a draft of literature review and methodology\n\nFor the literature review include summaries for 3-5 papers related to your research question.\nIt should also include a draft of the methodology section, including the model specification.\n\nAddress any feedback received from the peer review in Part II\n\n\n\nPart III: Data Analysis (15%, due Week 10)\nPresent a draft for data analysis that includes the following:\n\nModel specification discussion\nRegression results presentation\nInterpretation of results\nDiscuss Limitations and robustness checks\n\n\n\nPeer Review Report (due week 11)\n\nReview another student’s work and provide feedback on their data analysis. Provide suggestions for improvement and identify any potential issues.\n\n\n\nPart IV: Final Report (20%, due Week 13)\nComplete research paper should include:\n\nIntroduction\nLiterature Review\nData and Methodology\nRobustness Checks or Sensitivity/Sub-group Analysis\nConclusion\nReferences\nAppendices (if any)\n\n\n\nPresentaton (5%, Week 14)\n\n15-minute presentation of your research to the class\nSee here for an example for the kind of report expected at each stage, based on the first research proposal on the impact of remote work on urban housing prices.\n\n\n\nAdditional Requirements\n\nAll work should be submitted in Quarto format using GitHub.\nUnless Data used is confidential or too large, you should include all data in your GitHub repository.\nYour GitHub repository should include with all code, data (if possible), and the Quarto document for your report.\nIt should also include all papers used for the literature review.\nAt each stage, you should submit an email with the PDF and QMD files to the instructor, at or before the deadline. You should also push your work to GitHub to follow the progress of your project."
  },
  {
    "objectID": "rm-data/syllabus.html#resources",
    "href": "rm-data/syllabus.html#resources",
    "title": "Research Methods I",
    "section": "Resources",
    "text": "Resources\n\nTextbook: Békés, G., & Kézdi, G. (2021). Data Analysis for Business, Economics, and Policy. Cambridge University Press. There are additional resources available on the book’s website: Data Analysis\nSoftware: There are several statistical packages for analyzing data. In this course, we will be using the software Stata to cover all materials in class. Slides are self-replicable, thus you can copy and paste almost all code provided to replicate the results seen in class. The Institute will be providing you with licenses for Stata/BE for the length of the course.\n\nStata offers many free short webinars and video tutorials that may be useful if you never used Stata before, or even if you have some experience with it. Please see the resources page for more information.\nIf you decide to, you can also use R, Julia, or Python to study and work on the course materials and homework. The book for the class has a repository with all the code in Stata, R and Python. It could be of great advantage to you to learn other languages, as they are widely used in the industry and academia.\nAs with many other skills, the best way to learn is to simply work with the software, work on the book exercises, and ask any questions to me or your classmates when you find a problem you could not find a solution for.\nFor the additional software, please look into Quarto, GitHub, Zotero and VSCode."
  },
  {
    "objectID": "rm-data/slides/week11.html#motivation",
    "href": "rm-data/slides/week11.html#motivation",
    "title": "Model Building for Prediction",
    "section": "Motivation",
    "text": "Motivation\n\nYou want to predict apartment rental prices using location, size, amenities, and other features. But with so many variables available,\n\nhow should you specify the candidate models?\nWhich variables should they include, in what functional forms, and with what interactions?\nHow can you make sure the candidates include truly effective predictive models?\n\nYou want to predict hourly sales for a new shop, based on data from a similar existing shop.\n\nHow should you define your y variable, and how should you select predictor variables for regression models to find the best fit?\nFinally, how can you evaluate the prediction in a way that informs decision-makers about the uncertainty of your prediction?"
  },
  {
    "objectID": "rm-data/slides/week11.html#what-is-old-whats-the-new",
    "href": "rm-data/slides/week11.html#what-is-old-whats-the-new",
    "title": "Model Building for Prediction",
    "section": "What is old? whats the new?",
    "text": "What is old? whats the new?\n\nWe have learned about the basics of prediction.\n\nWe need to worry about the out-of-sample prediction error. Not the in-sample error.\n\nWe know we can use cross-validation to select the best model.\nAnd we know we can construct the best model by hand, because we have domain knowledge.\n\nBut what if we have a lot of variables? What if we have a lot of data?"
  },
  {
    "objectID": "rm-data/slides/week11.html#as-k-grows-large-kxk-grows-larger",
    "href": "rm-data/slides/week11.html#as-k-grows-large-kxk-grows-larger",
    "title": "Model Building for Prediction",
    "section": "As \\(K\\) grows large \\(KxK\\) grows larger",
    "text": "As \\(K\\) grows large \\(KxK\\) grows larger\n\n1 variable, 1 model\n2 variables, 3 models\n3 variables, 7 models\n10 variables, 1023 models!\n20 variables, 1,048,575 models!"
  },
  {
    "objectID": "rm-data/slides/week11.html#prediction-process-key-steps",
    "href": "rm-data/slides/week11.html#prediction-process-key-steps",
    "title": "Model Building for Prediction",
    "section": "Prediction process: key steps",
    "text": "Prediction process: key steps\n\nStart with defining your “question” - what you want to predict.\nBased on the question, the target variable is defined (operationalize in the data).\n\nFollowed by defining the sample to be used (as the target)\nDetermine the target variable and functional form (label engineering)\n\nDefine the list of predictors (feature engineering)\n\nBuild the model(s)\nevaluate the model\nmake the prediction"
  },
  {
    "objectID": "rm-data/slides/week11.html#sample-design",
    "href": "rm-data/slides/week11.html#sample-design",
    "title": "Model Building for Prediction",
    "section": "Sample design",
    "text": "Sample design\n\nIn a prediction exercise, we are interested in predicting target for units that look those in the live data.\n\nSelect a sample that is representative of the live/target data.\n\nBut, there is a trade-off:\n\nKeep observations close to what we’ll have in live data,\nOr aim for a large sample size.\n\nSample design is eventually a compromise"
  },
  {
    "objectID": "rm-data/slides/week11.html#sample-design-filtering",
    "href": "rm-data/slides/week11.html#sample-design-filtering",
    "title": "Model Building for Prediction",
    "section": "Sample design: filtering",
    "text": "Sample design: filtering\n\nBefore settling on a model, we need to design the sample.\nFiltering our data to match the business/ policy question.\nIt may involve dropping observations based on key predictor values.\n\nWe would be looking for 3-4 star hotels, not all of them."
  },
  {
    "objectID": "rm-data/slides/week11.html#sample-design-spotting-errors",
    "href": "rm-data/slides/week11.html#sample-design-spotting-errors",
    "title": "Model Building for Prediction",
    "section": "Sample design: Spotting errors",
    "text": "Sample design: Spotting errors\n\nFor prediction exercise, we should spend more time on finding and deleting errors.\n\nWe have no chance predicting extreme values, and certainly not errors.\nThey provide no information, and they may distort the model.\n\nKeeping an extreme value that is likely to be an error, will have a high cost - the quadratic errors in the loss function will tilt the curve and our prediction will be off for most observations.\n\nOLS is very sensitive to outliers.\n\nStronger focus on dropping observations we think are errors.\nIf data is missing, You may either drop the observation or try to understand why it is missing, and use that in the prediction."
  },
  {
    "objectID": "rm-data/slides/week11.html#case-study-of-used-cars-sample-design",
    "href": "rm-data/slides/week11.html#case-study-of-used-cars-sample-design",
    "title": "Model Building for Prediction",
    "section": "Case study of used cars: Sample design",
    "text": "Case study of used cars: Sample design\n\nDropping hybrid cars, manual gear, truck\nDrop cars without a clean title (i.e., cars that had to be removed from registration due to a major accident)\nDrop when suspect cars with clearly erroneous data on miles run,\nDrop cars in a fair (=bad) condition, cars that are new\nData cleaning resulted in 281 observations ( I kept Fair and new)"
  },
  {
    "objectID": "rm-data/slides/week11.html#label-engineering---defining-target",
    "href": "rm-data/slides/week11.html#label-engineering---defining-target",
    "title": "Model Building for Prediction",
    "section": "Label engineering - defining target",
    "text": "Label engineering - defining target\n\nWe need to define what will our target variable be.\nIn some cases, this requires no action, the business question may define it:\n\nthe price of the hotel is one such case.\n\nOften it requires thinking and decision-making about definition.\n\nHow to define default, injury, purchase\n\nBinary vs continuous.\nLog vs level"
  },
  {
    "objectID": "rm-data/slides/week11.html#label-engineering---log-vs-level",
    "href": "rm-data/slides/week11.html#label-engineering---log-vs-level",
    "title": "Model Building for Prediction",
    "section": "Label engineering - log vs level",
    "text": "Label engineering - log vs level\n\nWhen price is the target variable, its relation to predictor variables is often closer to linear when expressed in log price.\nLog differences approximate relative, or percentage, differences, and relative price differences are often more stable.\nThe related technical advantage is that the distribution of log prices is often close to normal, which makes linear regressions give better approximation to average differences.\n\nAlso, Log() is not the only transformation that can be used.\n\nChoosing the right functional form is important but not always easy.\n\nEconometrics vs prediction mindset"
  },
  {
    "objectID": "rm-data/slides/week11.html#label-engineering---log-vs-level-1",
    "href": "rm-data/slides/week11.html#label-engineering---log-vs-level-1",
    "title": "Model Building for Prediction",
    "section": "Label engineering - log vs level",
    "text": "Label engineering - log vs level\n\nWhen the target variable is expressed in log terms, we want to predict the value of the target variable (\\(\\hat y\\)) not its \\(log(y)\\).\nSimply doing this \\(e^{\\log y}\\) is not the same as obtaining \\(\\hat y\\).\n\nTechnical details:\n\\(\\hat{y}_j = e^{ \\widehat{\\log y}_j + \\hat e_j}\\)\n\nBut, because \\(\\hat e_j\\) is not observed, we need to approximate it via “Some” method.\n\nIf we assume that the error term is normally distributed: \\[\\hat{y}_j = e^{\\widehat{\\ln y}_j} e^{\\hat{\\sigma}^2/2}\\]"
  },
  {
    "objectID": "rm-data/slides/week11.html#used-cars-case-study-label-engineering---log",
    "href": "rm-data/slides/week11.html#used-cars-case-study-label-engineering---log",
    "title": "Model Building for Prediction",
    "section": "Used cars case study: Label engineering - log?",
    "text": "Used cars case study: Label engineering - log?\n\nBusiness case is about price itself, continuous\nBut model can have level or log price as target\nLook at some patterns\nCompare model performance\nLog vs level model - some coefficients easier interpreted\nWhen we have two cars of same age and type; the one with 10% more miles in the odometer is predicted to be sold for 0.5% less.\nSE version is 1300 dollar more costly."
  },
  {
    "objectID": "rm-data/slides/week11.html#used-cars-case-study-label-engineering---log-1",
    "href": "rm-data/slides/week11.html#used-cars-case-study-label-engineering---log-1",
    "title": "Model Building for Prediction",
    "section": "Used cars case study: Label engineering - log?",
    "text": "Used cars case study: Label engineering - log?\n\n\n\nModel\nPoint prediction\n80% PI: upper bound\n80% PI: lower bound\n\n\n\n\nin logs\n8.63\n8.18\n9.08\n\n\nRecalculated to level\n5,932\n3,783\n9,301\n\n\nIn levels\n6,073\n4,317\n7,829\n\n\n\nAsymetric for Log-model, Symetric for Linear Model\nPick what works better"
  },
  {
    "objectID": "rm-data/slides/week11.html#feature-engineering",
    "href": "rm-data/slides/week11.html#feature-engineering",
    "title": "Model Building for Prediction",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nRequires the most effort\nFeature engineering - defining the list and functional form of variables we will consider as predictor.\nImportantly, we use both domain knowledge - information about the actual market, product or the society - and statistics to make decisions."
  },
  {
    "objectID": "rm-data/slides/week11.html#feature-engineering---checklist",
    "href": "rm-data/slides/week11.html#feature-engineering---checklist",
    "title": "Model Building for Prediction",
    "section": "Feature engineering - checklist",
    "text": "Feature engineering - checklist\n\nWhat to do with missing values\nDealing with ordered categorical values - continuous or set of binaries\nHow to use text to create variables (Identify key words)\nSelecting functional form\nThinking interactions"
  },
  {
    "objectID": "rm-data/slides/week11.html#what-to-do-with-missing-values",
    "href": "rm-data/slides/week11.html#what-to-do-with-missing-values",
    "title": "Model Building for Prediction",
    "section": "What to do with missing values",
    "text": "What to do with missing values\n\nMissing at random: Observations with missing variables are not systematically different from rest, may replace with sample mean and add binary flag.\nMissing systematically, by nonrandom selection: Must analyze reasons, may simply mean =0, look at the source of the data / questionnaire.\nIf very few missing and it is random, do not do anything.\nFew cases, you may want to impute or look for proxies."
  },
  {
    "objectID": "rm-data/slides/week11.html#what-to-do-with-different-type-of-variables",
    "href": "rm-data/slides/week11.html#what-to-do-with-different-type-of-variables",
    "title": "Model Building for Prediction",
    "section": "What to do with different type of variables",
    "text": "What to do with different type of variables\n\nBinary (e.g, yes/no; male/female; 1/2) – create a 0/1 binary variable\nString / factor – check values, and create a set of binaries.\nContinuous – nothing to do. Make sure it is stored as number. Perhaps Winsorize.\nText – Natural Language Processing. Mining the text to get useful info.\n\nCounting words, looking for key words, etc."
  },
  {
    "objectID": "rm-data/slides/week11.html#case-study-predicting-airbnb-apartment-prices",
    "href": "rm-data/slides/week11.html#case-study-predicting-airbnb-apartment-prices",
    "title": "Model Building for Prediction",
    "section": "Case study: Predicting Airbnb Apartment Prices",
    "text": "Case study: Predicting Airbnb Apartment Prices\n\nLondon,UK\nhttp://insideairbnb.com\n50K observations\n94 variables, including many binaries for location and amenities\nKey variables: size, type, location, amenities\nQuantitative target: - price (in USD)\nIn reality: GBP"
  },
  {
    "objectID": "rm-data/slides/week11.html#case-study-predicting-airbnb-apartment-prices-1",
    "href": "rm-data/slides/week11.html#case-study-predicting-airbnb-apartment-prices-1",
    "title": "Model Building for Prediction",
    "section": "Case study: Predicting Airbnb Apartment Prices",
    "text": "Case study: Predicting Airbnb Apartment Prices\n\nKey issue is to look at variables and think functional form\nGuests to accommodate goes up to 16, but most apartments accommodate 1 through 7. Keep as is. Add variables for type. No need for complicated models\nRegarding other predictors, we have several binary variables, which we kept as they were: type of bed, type of property (apartment, house, room), cancellation policy.\nLook at possible need for interactions by domain knowledge / visualization"
  },
  {
    "objectID": "rm-data/slides/week11.html#graphical-way-of-finding-relationships",
    "href": "rm-data/slides/week11.html#graphical-way-of-finding-relationships",
    "title": "Model Building for Prediction",
    "section": "Graphical way of finding relationships",
    "text": "Graphical way of finding relationships"
  },
  {
    "objectID": "rm-data/slides/week11.html#graphical-way-of-finding-interactions",
    "href": "rm-data/slides/week11.html#graphical-way-of-finding-interactions",
    "title": "Model Building for Prediction",
    "section": "Graphical way of finding interactions",
    "text": "Graphical way of finding interactions"
  },
  {
    "objectID": "rm-data/slides/week11.html#model-building",
    "href": "rm-data/slides/week11.html#model-building",
    "title": "Model Building for Prediction",
    "section": "Model building",
    "text": "Model building\nTwo methods to build models:\n\nby hand - mix domain knowledge and statistics\nby smart algorithms = machine learning"
  },
  {
    "objectID": "rm-data/slides/week11.html#model-building-and-selection-build-model-by-hand",
    "href": "rm-data/slides/week11.html#model-building-and-selection-build-model-by-hand",
    "title": "Model Building for Prediction",
    "section": "Model building and selection: Build model by hand",
    "text": "Model building and selection: Build model by hand\n\nUse domain knowledge drives picking key variables\nDrop garbage - drop variables those that are useless. May be because of poor coverage or quality, or they may be irrelevant.\nLook at a pairwise correlations. Multi-collinearity is an issue for smaller datasets\nPrefer variables that are easier to update - cheaper operation of a prediction model used in production\nMatters when you have relatively many variables compared to size of observations"
  },
  {
    "objectID": "rm-data/slides/week11.html#selecting-variables-in-regressions-by-lasso",
    "href": "rm-data/slides/week11.html#selecting-variables-in-regressions-by-lasso",
    "title": "Model Building for Prediction",
    "section": "Selecting Variables in Regressions by LASSO",
    "text": "Selecting Variables in Regressions by LASSO\n\nKey question: which features to enter into model, how to select?\n\nBy hand – domain knowledge. Advantage: interpretation, external validity\nDisadvantage: with many features it’s very hard. Esp. with many possible interactions!\n\nThere is room for an automatic selection process.\nSome are computationally very intensive (compare every option?)\nAdvantage: no need to use outside info\nDisadvantage: may be sensitive to overfitting, hard to interpret"
  },
  {
    "objectID": "rm-data/slides/week11.html#lasso-idea",
    "href": "rm-data/slides/week11.html#lasso-idea",
    "title": "Model Building for Prediction",
    "section": "LASSO idea",
    "text": "LASSO idea\n\nLASSO (the acronym of Least Absolute Shrinkage and Selection Operator) is a method to select variables to include in a linear regression to produce good predictions and avoid overfitting.\nLASSO is a shrinkage method: it shrinks coefficients towards zero to reduce variance\n\nCost is in bias - LASSO is not unbiased\nUnlike OLS\n\nLASSO is a feature selection method as well"
  },
  {
    "objectID": "rm-data/slides/week11.html#lasso-process",
    "href": "rm-data/slides/week11.html#lasso-process",
    "title": "Model Building for Prediction",
    "section": "LASSO process",
    "text": "LASSO process\n\nIt starts with a large set of potential predictor variables that, typically, include many interactions, polynomials for nonlinear patterns, etc.\nLASSO modifies the way regression coefficients are estimated by adding a penalty term for too many coefficients.\nThe way its penalty works makes LASSO assign zero coefficients to variables whose inclusion does not improve the fit of the regression much.\nAssigning zero coefficients to some variables means not including them in the regression."
  },
  {
    "objectID": "rm-data/slides/week11.html#side-note-lasso-vs-bic",
    "href": "rm-data/slides/week11.html#side-note-lasso-vs-bic",
    "title": "Model Building for Prediction",
    "section": "Side note: LASSO vs BIC",
    "text": "Side note: LASSO vs BIC\n\nThe purpose is similar to the adjusted in-sample measures of fit, such as the BIC.\nTo find a regression that balances fitting the data and the number of variables.\nBut its result is different:\n\ninstead of producing a better measure of fit to help find the best one\nit alters coefficients to produce a better regression directly."
  },
  {
    "objectID": "rm-data/slides/week11.html#lasso",
    "href": "rm-data/slides/week11.html#lasso",
    "title": "Model Building for Prediction",
    "section": "LASSO",
    "text": "LASSO\nConsider the linear regression with i=1…n observations and k variables, denoted 1…k:\n\\[y^E = \\beta_0 + \\sum_{j=1}^k \\beta_jx_j\\]\nCoefficients are estimated by OLS: which minimizes the sum of squared residuals:\n\\[\\min_\\beta \\sum_{i=1}^N (y_i - (\\beta_0 + \\sum_{j=1}^k \\beta_jx_{ij}))^2\\]\nLASSO modifies this minimization by a penalty term:\n\\[\\min_\\beta \\sum_{i=1}^N (y_i - (\\beta_0 + \\sum_{j=1}^k \\beta_jx_{ij}))^2 + \\lambda \\sum_{j=1}^k |\\beta_j|\\]"
  },
  {
    "objectID": "rm-data/slides/week11.html#lasso-how-it-works",
    "href": "rm-data/slides/week11.html#lasso-how-it-works",
    "title": "Model Building for Prediction",
    "section": "LASSO: how it works",
    "text": "LASSO: how it works\n\n\\(\\lambda\\) — tuning parameter.\nweight for penalty term vs OLS fit –&gt; Strength of the variable selection\nMain effect of this constraint is to force many coefficients to zero.\nBest way to keep the sum of the absolute value of the coefficients low while maximizing fit –&gt; zero coefficients on variables whose inclusion improves fit only a little.\nThis adjustment gets rid of the weakest predictors."
  },
  {
    "objectID": "rm-data/slides/week11.html#lasso-how-it-works-1",
    "href": "rm-data/slides/week11.html#lasso-how-it-works-1",
    "title": "Model Building for Prediction",
    "section": "LASSO: how it works",
    "text": "LASSO: how it works\n\nThe value of the tuning parameter \\(\\lambda\\) drives the strength of this selection.\nLarger \\(\\lambda\\) values lead to more aggressive selection and thus fewer variables left in the regression.\nBut how can one specify a \\(\\lambda\\) value that leads to the best prediction?\nWe don’t need, the algorithm does\nThe LASSO algorithm can numerically solve for coefficients and the \\(\\lambda\\) parameter at once.\nThis makes it fast.\nUnlike OLS, we have no closed form solutions."
  },
  {
    "objectID": "rm-data/slides/week11.html#other-shrinkage-methods",
    "href": "rm-data/slides/week11.html#other-shrinkage-methods",
    "title": "Model Building for Prediction",
    "section": "Other shrinkage methods",
    "text": "Other shrinkage methods\n\nSo LASSO is a shrinkage method: it shrinks coefficients towards zero to reduce variance\nThere are other ways, other functional forms\nRidge regression has a quadratic penalty:\n\n\\[\\min_\\beta \\sum_{i=1}^N (y_i - (\\beta_0 + \\sum_{j=1}^k \\beta_jx_{ij}))^2 + \\lambda \\sum_{j=1}^k \\beta_j^2\\]\n\nNo coefficient is shrunk to zero. But close…"
  },
  {
    "objectID": "rm-data/slides/week11.html#lasso-and-ridge",
    "href": "rm-data/slides/week11.html#lasso-and-ridge",
    "title": "Model Building for Prediction",
    "section": "Lasso and Ridge",
    "text": "Lasso and Ridge\n\nLasso, Ridge regressions called regularization\nLASSO is “L1”, Ridge is “L2”\nBoth may help reduce overfitting\nLASSO also acts as feature selection model\nElastic net helps find a parameter between \\(|\\beta_j|\\) and \\(\\beta_j^2\\) via cross validation"
  },
  {
    "objectID": "rm-data/slides/week11.html#airbnb-pricing-model-building",
    "href": "rm-data/slides/week11.html#airbnb-pricing-model-building",
    "title": "Model Building for Prediction",
    "section": "Airbnb Pricing Model building",
    "text": "Airbnb Pricing Model building\n\nProcess: build many models that differ in terms of features:\n\nWhich predictors are included\nFunctional form of predictors\n\nHere: specified eight linear regression models for predicting price.\nData has 4393 observations. This is our original data.\n80% is our work set (3515 observations), the rest we will use for diagnostics."
  },
  {
    "objectID": "rm-data/slides/week11.html#versions-of-the-airbnb-apartment-price-prediction-models",
    "href": "rm-data/slides/week11.html#versions-of-the-airbnb-apartment-price-prediction-models",
    "title": "Model Building for Prediction",
    "section": "Versions of the Airbnb apartment price prediction models",
    "text": "Versions of the Airbnb apartment price prediction models\n\n\n\n\n\n\n\n\n\nMod\nPredictor variables\nN var\nN coeff\n\n\n\n\nM1\nguests accommodated, linearly\n1\n2\n\n\nM2\n= M1 + N beds, N days review, type: property, room, bed type\n6\n8\n\n\nM3\n= M2 + bathroom, cancellation, review score, N reviews (3 cat)+ F(miss)\n11\n15\n\n\nM4\n= M3 + N guest squared, square+cubic for days since 1st review\n11\n17\n\n\nM5\n= M4 + room type + N reviews interacted with property type\n11\n22\n\n\nM6\n=M5 + air conditioning, pets allowed - interacted with property type\n13\n28\n\n\nM7\n=M6 + all other amenities\n70\n72\n\n\nM8\n=M7 + all other amenities interacted with property type + bed type\n70\n293"
  },
  {
    "objectID": "rm-data/slides/week11.html#comparing-model-fit-measures",
    "href": "rm-data/slides/week11.html#comparing-model-fit-measures",
    "title": "Model Building for Prediction",
    "section": "Comparing model fit measures",
    "text": "Comparing model fit measures\n\n\n\n\n\n\n\n\n\n\n\nModel\nN predictors\nR-squared\nBIC\nTraining RMSE\nTest RMSE\n\n\n\n\n(1)\n1\n0.40\n36042\n40.48\n40.16\n\n\n(2)\n7\n0.48\n35598\n37.73\n37.38\n\n\n(3)\n14\n0.51\n35478\n36.78\n36.51\n\n\n(4)\n16\n0.57\n24076\n31.95\n32.24\n\n\n(5)\n21\n0.57\n24096\n31.82\n32.18\n\n\n(6)\n27\n0.58\n24113\n31.60\n32.19\n\n\n(7)\n71\n0.61\n24281\n30.42\n31.77\n\n\n(8)\n293\n0.66\n25675\n28.04\n51.41"
  },
  {
    "objectID": "rm-data/slides/week11.html#training-and-test-set-rmse-for-eight-models",
    "href": "rm-data/slides/week11.html#training-and-test-set-rmse-for-eight-models",
    "title": "Model Building for Prediction",
    "section": "Training and test set RMSE for eight models",
    "text": "Training and test set RMSE for eight models\n\nTraining RMSE falls with complexity\nTest RMSE falls then rises\nWe pick Model M7 based on lowest CV RMSE."
  },
  {
    "objectID": "rm-data/slides/week11.html#the-lasso-model",
    "href": "rm-data/slides/week11.html#the-lasso-model",
    "title": "Model Building for Prediction",
    "section": "The LASSO model",
    "text": "The LASSO model\n\nStart with M8 and appr 300 candidate variables in the regression.\nWe ran the LASSO algorithm with 5-fold cross-validation for selecting the optimal value for λ.\nLASSO regression just marginally better but: LASSO is automatic, a great advantage.\nHere: domain knowledge helped create M7. In other cases, LASSO could be great."
  },
  {
    "objectID": "rm-data/slides/week11.html#evaluating-the-prediction-using-a-holdout-set",
    "href": "rm-data/slides/week11.html#evaluating-the-prediction-using-a-holdout-set",
    "title": "Model Building for Prediction",
    "section": "Evaluating the Prediction Using a Holdout Set",
    "text": "Evaluating the Prediction Using a Holdout Set\n\nModel selection: selecting the best model using cross-validation\nOnce we have picked the best model, we advised going back and using the entire original data for the final estimate and to make a prediction.\nWhat part of the data should we use to evaluate that final prediction?\nThe solution is a random split before we do the analysis.\nWork set: We do all of the work using one part of the data: model building, selecting the best model and then making the prediction itself.\nHoldout set: another part of the data for evaluating the prediction itself. Don’t touch till the end."
  },
  {
    "objectID": "rm-data/slides/week11.html#the-holdout-set",
    "href": "rm-data/slides/week11.html#the-holdout-set",
    "title": "Model Building for Prediction",
    "section": "The holdout set",
    "text": "The holdout set\n\nTo do diagnostics and give a good estimate of how the model may work in the live data\n\nAdditional twist to the process\nThe holdout set.\n\nHoldout set is set is not used in any way for modelling – taken out in the beginning\n\nThis avoids cross-contamination\n\nUsed to give best guess for performance in live data\nUsed to do diagnostics of our model"
  },
  {
    "objectID": "rm-data/slides/week11.html#post-prediction-diagnostics",
    "href": "rm-data/slides/week11.html#post-prediction-diagnostics",
    "title": "Model Building for Prediction",
    "section": "Post-prediction diagnostics",
    "text": "Post-prediction diagnostics\n\nPost-prediction diagnostics - understand better how our model works\nWe look at prediction interval to learn about what precision we may expect to see of the estimates.\nWe look at how the model work for different classes of observations\nsuch as young and old cars."
  },
  {
    "objectID": "rm-data/slides/week11.html#cross-validation-and-holdout-set-procedure",
    "href": "rm-data/slides/week11.html#cross-validation-and-holdout-set-procedure",
    "title": "Model Building for Prediction",
    "section": "Cross-validation and holdout set procedure",
    "text": "Cross-validation and holdout set procedure\n\nStarting with the original data, split it into a larger work set and a smaller holdout set.\nFurther split the work set into training sets and test sets for k-fold cross-validation.\nBuild models and select the best model using that training-test split.\nRe-estimate the best model using all observations in the work set.\nTake the estimated best model and apply it to the holdout set.\nEvaluate the prediction using the holdout set."
  },
  {
    "objectID": "rm-data/slides/week11.html#illustration-of-the-uses-of-the-original-data-and-the-live-data",
    "href": "rm-data/slides/week11.html#illustration-of-the-uses-of-the-original-data-and-the-live-data",
    "title": "Model Building for Prediction",
    "section": "Illustration of the uses of the original data and the live data",
    "text": "Illustration of the uses of the original data and the live data"
  },
  {
    "objectID": "rm-data/slides/week11.html#post-prediction-diagnostics-1",
    "href": "rm-data/slides/week11.html#post-prediction-diagnostics-1",
    "title": "Model Building for Prediction",
    "section": "Post-prediction diagnostics",
    "text": "Post-prediction diagnostics\n\nPost-prediction diagnostics - understand better how our model works\nWe look at prediction interval to learn about what precision we may expect to see of the estimates.\nWe look at how the model work for different classes of observations\nsuch as young and old cars."
  },
  {
    "objectID": "rm-data/slides/week11.html#data-work-and-holdout",
    "href": "rm-data/slides/week11.html#data-work-and-holdout",
    "title": "Model Building for Prediction",
    "section": "Data work and holdout",
    "text": "Data work and holdout\n\nData has 4393 observations. This is our original data.\nrandom 20% holdout set with 878 observations.\nThe remaining 80% is our work set (3515 observations).\nWork set will be used for cross-validation with several folds of training and test sets."
  },
  {
    "objectID": "rm-data/slides/week11.html#diagnostics",
    "href": "rm-data/slides/week11.html#diagnostics",
    "title": "Model Building for Prediction",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nChose the OLS estimated M7.\nWhat can we say about model performance?\nAfter estimating the model on all observations in the work sample, we calculated its RMSE in the holdout sample. The RMSE for M7 is 41\nHigher than CV RMSE, could be other way around.\nLook at diagnostics on the holdout set."
  },
  {
    "objectID": "rm-data/slides/week11.html#diagnostics-prices",
    "href": "rm-data/slides/week11.html#diagnostics-prices",
    "title": "Model Building for Prediction",
    "section": "Diagnostics: prices",
    "text": "Diagnostics: prices\n\n\n\n\n\ny-y-hat plot\nhigher values not really caught."
  },
  {
    "objectID": "rm-data/slides/week11.html#diagnostics-variation-by-size",
    "href": "rm-data/slides/week11.html#diagnostics-variation-by-size",
    "title": "Model Building for Prediction",
    "section": "Diagnostics: variation by size",
    "text": "Diagnostics: variation by size\n\n\n\n\n\nThe model generates a very wide 80% PI for average apartment\nbar plot with PI bands\nwide intervals\nlinear and thus, hurts small numbers more"
  },
  {
    "objectID": "rm-data/slides/week11.html#prediction-with-big-data",
    "href": "rm-data/slides/week11.html#prediction-with-big-data",
    "title": "Model Building for Prediction",
    "section": "Prediction with Big Data",
    "text": "Prediction with Big Data\n\nThe principles of prediction are the same with Big Data as with moderate-sized data\nBig Data leads to smaller estimation error.\nThis reduction makes the total prediction error smaller\nThe magnitude of irreducible error, and problems with external validity, remain the same with Big Data"
  },
  {
    "objectID": "rm-data/slides/week11.html#prediction-with-big-data-1",
    "href": "rm-data/slides/week11.html#prediction-with-big-data-1",
    "title": "Model Building for Prediction",
    "section": "Prediction with Big Data",
    "text": "Prediction with Big Data\n\nAnother upside is that large number of rows sometimes comes with large number of variables\nRoom for more complex models\nConsideration: computing power (But there is AWS and Cloud Computing)\nWhen N is too large, we can take a random sample and select the best model with the help of usual cross-validation using that random sample"
  },
  {
    "objectID": "rm-data/slides/week11.html#summary",
    "href": "rm-data/slides/week11.html#summary",
    "title": "Model Building for Prediction",
    "section": "Summary",
    "text": "Summary\n\nOur aim was to build a prediction model for pricing apartments\nWe built a model, M7, with domain knowledge, and a horse race between models of various complexity\nPicked the winner by cross-validated RMSE\nThe model is useful for predication, but there is a great deal of uncertainty as suggested by diagnostics (on the holdout set)"
  },
  {
    "objectID": "rm-data/slides/week11.html#think-external-validity",
    "href": "rm-data/slides/week11.html#think-external-validity",
    "title": "Model Building for Prediction",
    "section": "Think external validity",
    "text": "Think external validity\n\nFuture dataset will look different\nThink about how much\nReally matters in prediction\nIf uncertain, pick simpler model"
  },
  {
    "objectID": "rm-data/slides/week11.html#main-takeaways",
    "href": "rm-data/slides/week11.html#main-takeaways",
    "title": "Model Building for Prediction",
    "section": "Main takeaways",
    "text": "Main takeaways\n\nWe can never evaluate all possible models to find the best one\nModel building is important to specify models that are likely among the best\nLASSO is an algorithm that can help in model building, by selecting the x variables and their functional forms\nExploratory data analysis and domain knowledge remain important alongside powerful algorithms, for assessing and improving the external validity of predictions"
  },
  {
    "objectID": "rm-data/slides/week11.html#stata-lasso",
    "href": "rm-data/slides/week11.html#stata-lasso",
    "title": "Model Building for Prediction",
    "section": "Stata: LASSO",
    "text": "Stata: LASSO\n\nLasso is one of the few machine learning algorithms that is available in Stata.\n\nStata also has a feature for elastic net and Ridge.\nfor now just focus on LASSO. help lasso\n\nThe syntax\n\n lasso model depvar [(alwaysvars)] othervars [if] [in] [weight] [, options]\n\nIt has various selection options (selection()) but we can use the default."
  },
  {
    "objectID": "rm-data/slides/week11.html#stata-lasso-1",
    "href": "rm-data/slides/week11.html#stata-lasso-1",
    "title": "Model Building for Prediction",
    "section": "Stata: LASSO",
    "text": "Stata: LASSO\n\nwebuse cattaneo2, clear\nlasso linear bweight c.mage##c.mage c.fage##c.fage c.mage#c.fage c.fedu##c.medu ///\n    i.(mmarried mhisp fhisp foreign alcohol msmoke fbaby prenatal1), nolog\nereturn display\n\n(Excerpt from Cattaneo (2010) Journal of Econometrics 155: 138–154)\n\nLasso linear model                          No. of obs        =      4,642\n                                            No. of covariates =         26\nSelection: Cross-validation                 No. of CV folds   =         10\n\n--------------------------------------------------------------------------\n         |                                No. of      Out-of-      CV mean\n         |                               nonzero       sample   prediction\n      ID |     Description      lambda     coef.    R-squared        error\n---------+----------------------------------------------------------------\n       1 |    first lambda    107.1305         0       0.0001     334929.8\n      37 |   lambda before    3.761556        10       0.0561     316156.2\n    * 38 | selected lambda     3.42739        11       0.0561     316154.9\n      39 |    lambda after     3.12291        11       0.0561     316156.8\n      65 |     last lambda    .2780062        19       0.0550     316532.1\n--------------------------------------------------------------------------\n* lambda selected by cross-validation.\n------------------------------------------------------------------------------\n     bweight | Coefficient\n-------------+----------------------------------------------------------------\n        mage |   .0026703\n             |\n      c.fedu#|\n      c.medu |   .3136748\n             |\n    mmarried |\nNot married  |  -140.4633\n     0.mhisp |  -34.00255\n   0.foreign |   63.71057\n   0.alcohol |   40.29426\n             |\n      msmoke |\n    0 daily  |   167.9734\n 6–10 daily  |  -36.54529\n  11+ daily  |   -78.1332\n             |\n       fbaby |\n         No  |   51.02337\n             |\n   prenatal1 |\n         No  |  -42.31267\n       _cons |   3137.903\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rm-data/slides/week11.html#stata-lasso-2",
    "href": "rm-data/slides/week11.html#stata-lasso-2",
    "title": "Model Building for Prediction",
    "section": "Stata: LASSO",
    "text": "Stata: LASSO\n\nqui: ssc install elasticregress\nwebuse cattaneo2, clear\nlassoregress bweight c.mage##c.mage c.fage##c.fage c.mage#c.fage c.fedu##c.medu ///\n    i.(mmarried mhisp fhisp foreign alcohol msmoke fbaby prenatal1), \nereturn display\n\n(Excerpt from Cattaneo (2010) Journal of Econometrics 155: 138–154)\n\nLASSO regression                       Number of observations     =      4,642\n                                       R-squared                  =     0.0597\n                                       alpha                      =     1.0000\n                                       lambda                     =     2.7733\n                                       Cross-validation MSE       =  3.165e+05\n                                       Number of folds            =         10\n                                       Number of lambda tested    =        100\n------------------------------------------------------------------------------\n     bweight | Coefficient\n-------------+----------------------------------------------------------------\n        mage |          0\n             |\n      c.mage#|\n      c.mage |          0\n             |\n        fage |          0\n             |\n      c.fage#|\n      c.fage |          0\n             |\n      c.mage#|\n      c.fage |          0\n             |\n        fedu |          0\n        medu |   .5584781\n             |\n      c.fedu#|\n      c.medu |   .3520153\n             |\n    mmarried |\nNot married  |          0  (empty)\n    Married  |   151.3538\n             |\n       mhisp |\n          0  |          0  (empty)\n          1  |   38.93988\n             |\n       fhisp |\n          0  |          0  (empty)\n          1  |          0\n             |\n     foreign |\n          0  |          0  (empty)\n          1  |  -72.23538\n             |\n     alcohol |\n          0  |          0  (empty)\n          1  |   -49.0195\n             |\n      msmoke |\n    0 daily  |          0  (empty)\n  1–5 daily  |  -144.4353\n 6–10 daily  |  -206.2044\n  11+ daily  |  -247.3819\n             |\n       fbaby |\n         No  |          0  (empty)\n        Yes  |  -50.38466\n             |\n   prenatal1 |\n         No  |          0  (empty)\n        Yes  |          0\n             |\n       _cons |     3256.7\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\n     bweight | Coefficient\n-------------+----------------------------------------------------------------\n        mage |          0\n             |\n      c.mage#|\n      c.mage |          0\n             |\n        fage |          0\n             |\n      c.fage#|\n      c.fage |          0\n             |\n      c.mage#|\n      c.fage |          0\n             |\n        fedu |          0\n        medu |   .5584781\n             |\n      c.fedu#|\n      c.medu |   .3520153\n             |\n    mmarried |\nNot married  |          0  (empty)\n    Married  |   151.3538\n             |\n       mhisp |\n          0  |          0  (empty)\n          1  |   38.93988\n             |\n       fhisp |\n          0  |          0  (empty)\n          1  |          0\n             |\n     foreign |\n          0  |          0  (empty)\n          1  |  -72.23538\n             |\n     alcohol |\n          0  |          0  (empty)\n          1  |   -49.0195\n             |\n      msmoke |\n    0 daily  |          0  (empty)\n  1–5 daily  |  -144.4353\n 6–10 daily  |  -206.2044\n  11+ daily  |  -247.3819\n             |\n       fbaby |\n         No  |          0  (empty)\n        Yes  |  -50.38466\n             |\n   prenatal1 |\n         No  |          0  (empty)\n        Yes  |          0\n             |\n       _cons |     3256.7\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rm-data/slides/week09.html#motivation",
    "href": "rm-data/slides/week09.html#motivation",
    "title": "Time series data",
    "section": "Motivation",
    "text": "Motivation\n\nYou are considering investing in a company stock, and you want to know how risky that investment is. You have downloaded data on daily stock prices for many years. How should you define returns? How should you assess whether and to what extent returns on the company stock move together with market returns?\nHeating and cooling are important uses of electricity. How does weather conditions affect electricity consumption? We are going to use monthly data on temperature and residential electricity consumption in Arizona. How to formulate a model which captures these factors?"
  },
  {
    "objectID": "rm-data/slides/week09.html#what-is-special-in-the-analysis-of-time-series-data",
    "href": "rm-data/slides/week09.html#what-is-special-in-the-analysis-of-time-series-data",
    "title": "Time series data",
    "section": "What is special in the analysis of time series data?",
    "text": "What is special in the analysis of time series data?\n\nDifferent\n\nWe have time series data if we observe one unit across many time periods.\nThere is a special notation: \\(y_t\\), \\(t = 1, 2, \\ldots, T\\)\nTime series data presents additional opportunities as well as additional challenges to compare variables.\n\n\n\nWith other features\n\nData wrangling novelties: frequency and aggregation\nSpecial nature of time series: serial correlation\nCoefficient interpretation."
  },
  {
    "objectID": "rm-data/slides/week09.html#data-preparation",
    "href": "rm-data/slides/week09.html#data-preparation",
    "title": "Time series data",
    "section": "Data preparation",
    "text": "Data preparation\n\nWhat is Frequency? of time series? = time elapsed between two observations of a variable\nPractical problems with frequency:\n\nNot all data is captured at the same frequency\nThere may be regular/irregular gaps between them: e.g. weekends for stock-exchange\nTwo variables have different frequencies\nExtreme values (spikes) in your variable\n\nLast but not least: in Stata you need to Declare your data as time series data with tsset command"
  },
  {
    "objectID": "rm-data/slides/week09.html#consequences",
    "href": "rm-data/slides/week09.html#consequences",
    "title": "Time series data",
    "section": "Consequences",
    "text": "Consequences\n\nRegressions: to condition \\(y_t\\) on values of \\(x_t\\) the two variables need to be on the same frequency. Otherwise, we need to adjust one of them.\nHow to adjust? Aggregation!\n\nFlow variables: sum up the values within the interval. e.g. daily sales \\(\\rightarrow\\) weakly sales is the sum of daily sales.\nStock variables: take the end-period value. e.g. daily stock prices uses the closing price on a given day\nOther kinds of variables: usually take the average value"
  },
  {
    "objectID": "rm-data/slides/week09.html#what-is-not-special-in-time-series",
    "href": "rm-data/slides/week09.html#what-is-not-special-in-time-series",
    "title": "Time series data",
    "section": "What is not special in time series",
    "text": "What is not special in time series\n\nTime series regressions are special for several reasons…but many aspects remain the same\n\nGeneralization, confidence intervals: Same difficulties as in cross-section\nTime series regression uncover patterns rather than evidence of causality: There is no “sample of time”\nPractical data issues, missing observations, extreme values etc, remain\nCoefficient interpretation is based on conditional comparison"
  },
  {
    "objectID": "rm-data/slides/week09.html#what-is-special-in-time-series",
    "href": "rm-data/slides/week09.html#what-is-special-in-time-series",
    "title": "Time series data",
    "section": "What is special in time series",
    "text": "What is special in time series\n\nOrdering matters - key difference to cross section. Complications…Past before future\nTrend - variables tend to have trends! Increasing or decreasing values over time\nSeasonality - variables may show some cyclical component, such 4 seasons, months, - every e.g. December value is expected to be different.\nTime series values are often not independent - correlated in time\n\nFailure to account for this may lead to wrong (biased) conclusions"
  },
  {
    "objectID": "rm-data/slides/week09.html#ts-variables-may-have-trends-trends",
    "href": "rm-data/slides/week09.html#ts-variables-may-have-trends-trends",
    "title": "Time series data",
    "section": "TS Variables may have trends Trends",
    "text": "TS Variables may have trends Trends\n\nTime series data “tends” to have trends. how to detect them?.\nDefine change (or fist difference): \\(\\Delta y_t = y_t - y_{t-1}\\)\n\n\\[\n\\begin{align}\n\\text{Positive trend}: & E[\\Delta y_t] &gt; 0 \\\\\n\\text{Negative trend}: & E[\\Delta y_t] &lt; 0\n\\end{align}\n\\]\n\nOne may also consider two types of trends: linear and exponential\n\n\\[\n\\begin{align}\n\\text{Linear trend}: & E[\\Delta y_t] = \\text{constant} \\\\\n\\text{Exponential trend}: & E[\\Delta \\ln(y_t)] = \\text{constant}\n\\end{align}\n\\]\n\nExponential trend, Changes are proportional"
  },
  {
    "objectID": "rm-data/slides/week09.html#but-they-may-also-show-seasonality",
    "href": "rm-data/slides/week09.html#but-they-may-also-show-seasonality",
    "title": "Time series data",
    "section": "But they may also show Seasonality",
    "text": "But they may also show Seasonality\n\nThere is seasonal variation, or seasonality, if its expected value changes periodically.\n\nFollows the seasons of the year, days of the week, hours of the day.\n\nSeasonality may be linear, when the seasonal differences are constant; it may be exponential, if relative differences (that may be approximated by log differences) are constant.\nImportant real life phenomenon - many economic activities follow seasonal variation over the year, through the week or day.\n\nThus they need to be accounted for in the analysis."
  },
  {
    "objectID": "rm-data/slides/week09.html#what-is-special-in-time-series-stationarity",
    "href": "rm-data/slides/week09.html#what-is-special-in-time-series-stationarity",
    "title": "Time series data",
    "section": "What is special in time series: Stationarity",
    "text": "What is special in time series: Stationarity\n\nTo learn something, we need consistency in patterns If the patterns change constantly, we cannot learn anything from the data\n\n\n\nStationary time series have the same expected value and same distribution, at all times.\nStationarity is a feature of the time series itself.\nStationarity means stability (in expectations).\nIf series are stationary, we can learn from them!"
  },
  {
    "objectID": "rm-data/slides/week09.html#what-is-special-in-time-series-non-stationarity",
    "href": "rm-data/slides/week09.html#what-is-special-in-time-series-non-stationarity",
    "title": "Time series data",
    "section": "What is special in time series: Non-stationarity",
    "text": "What is special in time series: Non-stationarity\n\nIn absence of stationarity, we cannot learn anything from the data Or what we learn may be misleading\n\n\n\nNon-stationary time series are those that are not stable for some reason.\nSeries that violate stationarity because the expected value is different at different times:\n\nHas a trends\nHas seasonality\nHas some unstable patterns\n\n\n\n\n\nAlthough, some of this issues can be address by “controlling” for them in the regression."
  },
  {
    "objectID": "rm-data/slides/week09.html#a-special-case-random-walk",
    "href": "rm-data/slides/week09.html#a-special-case-random-walk",
    "title": "Time series data",
    "section": "A Special case: Random walk",
    "text": "A Special case: Random walk\nSome non-stationary variables are not easy to deal with:\n\n\nAnother example of non-stationary time series is the random walk.\nRandom walk when \\(y_t\\) follows a random walk if its value in \\(t\\) is the same as in \\((t - 1)\\) plus some random term: \\(y_t = y_{t-1} + e_t\\).\nTime series variables that follow random walk change in completely random ways.\nWhatever the previous change was the next one may be anything. Wherever it starts, a random walk variable may end up anywhere after a long time."
  },
  {
    "objectID": "rm-data/slides/week09.html#section",
    "href": "rm-data/slides/week09.html#section",
    "title": "Time series data",
    "section": "",
    "text": "Code\nqui: {\n  clear\nset scheme white2\ncolor_style bay\nqui:set obs 101\ngen r = runiform(0,2*_pi)\n\ngen y = 0 \ngen x = 0\nreplace y = y[_n-1] + sin(r) if _n&gt;1\nreplace x = x[_n-1] + cos(r) if _n&gt;1\ngen n = _n\n}\n\n*scatter  y x , connect(l) name(m1, replace) \n*line y x n, name(m2, replace)"
  },
  {
    "objectID": "rm-data/slides/week09.html#what-to-fear-of-random-walk",
    "href": "rm-data/slides/week09.html#what-to-fear-of-random-walk",
    "title": "Time series data",
    "section": "What to fear of Random walk",
    "text": "What to fear of Random walk\n\n\nRandom walks are impossible to predict\nafter a change, they don’t revert back to some value or trend line but continue their journey from that point.\nSpread rising from one interval to another\nWhy is this important?\n\n\n\n\nTo identify patterns, we need stationarity\nFor stationary series, we need stability of patterns\nAvoid series with random walk when running regressions\n\nUsing them would only cause problems of Spurious Regressions"
  },
  {
    "objectID": "rm-data/slides/week09.html#how-to-detect-it-unit-root-test",
    "href": "rm-data/slides/week09.html#how-to-detect-it-unit-root-test",
    "title": "Time series data",
    "section": "How to detect it: Unit root test",
    "text": "How to detect it: Unit root test\n\n\nWe can test if a series is a random walk.\nPhillips-Perron test is based on this model: \\[y_t = \\alpha + \\rho y_{t-1} + e_t\\]\nThis model represents a random walk if \\(\\rho = 1\\), which is also called a unit root\nRandom walk test = testing if the series has a unit root.\nThe Phillips-Perron test has hypothesis\n\n\\[H_0: \\rho = 1 \\text{ vs } H_A: \\rho &lt; 1\\]"
  },
  {
    "objectID": "rm-data/slides/week09.html#section-1",
    "href": "rm-data/slides/week09.html#section-1",
    "title": "Time series data",
    "section": "",
    "text": "This test does not follow the usual t-distribution. Has its own distribution.\nin Stata, you can use it with the pperron command. (see helpfile)\nWhen the p-value is large (e.g., larger than 0.05), we don’t reject the null, concluding that the time series variable follows a random walk\nMany versions of unit root test.\n\n(Augmented) Dicky Fuller test is another popular one.\n\n\n\\[\\Delta y_t = \\alpha + \\delta y_{t-1} + e_t\\]\n\n\\(H_0: \\delta = 0\\) vs \\(H_A: \\delta &lt; 0\\)\nTests usually agree, but not always. see dfuller"
  },
  {
    "objectID": "rm-data/slides/week09.html#time-series-summary",
    "href": "rm-data/slides/week09.html#time-series-summary",
    "title": "Time series data",
    "section": "Time Series: Summary",
    "text": "Time Series: Summary\n\nStationary series are those where the expected value, variance, and auto-correlation does not change.\nExamples of non–stationarity:\n\nTrend - Expected value is different in later time periods than in earlier time periods\nSeasonality - Expected value is different in periodically recurring time periods\nRandom walk – Variance keeps increasing over time\n\nThis is the one we need to avoid\n\n\nWhy care? Regression with time series data variables that are not stationary are likely to give misleading results (Spurious)."
  },
  {
    "objectID": "rm-data/slides/week09.html#what-to-do-with-ts-cook-book",
    "href": "rm-data/slides/week09.html#what-to-do-with-ts-cook-book",
    "title": "Time series data",
    "section": "What to do With TS: Cook-book",
    "text": "What to do With TS: Cook-book\n\n\nCheck if your variable is stationary\n\nVisualize\nDo a unit-root test\n\n\n\n\n\nIf there is a good reason to believe your variable trending (or is Random Walk)\n\nTake differences \\(\\Delta y_t\\), or add a trend variable\nTake percentage changes or log differences\n\nIn extremely rare cases, difference your variable twice\n\n\n\n\n\n\nIf your variable has a seasonality\n\nUse seasonality dummies in your regression\nMay consider to work with seasonal changes."
  },
  {
    "objectID": "rm-data/slides/week09.html#microsoft-and-sp-500-stock-prices---data",
    "href": "rm-data/slides/week09.html#microsoft-and-sp-500-stock-prices---data",
    "title": "Time series data",
    "section": "Microsoft and S&P 500 stock prices - data",
    "text": "Microsoft and S&P 500 stock prices - data\n\nDaily price of Microsoft stock and value of S&P 500 stock market index\nThe data covers 21 years starting with December 31 1997 and ending with December 31 2018.\nMany decisions to make…\nLook at data first"
  },
  {
    "objectID": "rm-data/slides/week09.html#case-study-stock-price-and-stock-market-index-value",
    "href": "rm-data/slides/week09.html#case-study-stock-price-and-stock-market-index-value",
    "title": "Time series data",
    "section": "Case study: Stock price and stock market index value",
    "text": "Case study: Stock price and stock market index value\n\nCode\nqui {\nuse \"data_slides/stock-prices-daily.dta\", clear\nren *, low\n}\nline p_sp500 date, name(m1, replace)\nline p_msft date, name(m2, replace)"
  },
  {
    "objectID": "rm-data/slides/week09.html#time-series-comparisons---sp-500-case-study",
    "href": "rm-data/slides/week09.html#time-series-comparisons---sp-500-case-study",
    "title": "Time series data",
    "section": "Time series comparisons - S&P 500 case study",
    "text": "Time series comparisons - S&P 500 case study\nKey decisions:\n\nDaily price = closing price\nGaps will be overlooked\n\nFriday-Monday gap ignored\nHolidays (Christmas, 4 of July (when would be a weekday)\n\nAll values kept, extreme values part of process\nIn finance, portfolio managers often focus on monthly returns, Hence we choose monthly returns to analyze.\nTake the last day of each month"
  },
  {
    "objectID": "rm-data/slides/week09.html#microsoft-and-sp-500-stock-prices---ts-plot",
    "href": "rm-data/slides/week09.html#microsoft-and-sp-500-stock-prices---ts-plot",
    "title": "Time series data",
    "section": "Microsoft and S&P 500 stock prices - ts plot",
    "text": "Microsoft and S&P 500 stock prices - ts plot\n\nCode\nqui {\nuse \"data_slides/stock-prices-daily.dta\", clear\nren *, low\nsort ym date\nbysort ym: gen flag = _n == _N\nkeep if flag==1\n}\nline p_sp500 ym, name(m1, replace)\nline p_msft ym, name(m2, replace)"
  },
  {
    "objectID": "rm-data/slides/week09.html#microsoft-and-sp-500-stock-prices---decisions-2",
    "href": "rm-data/slides/week09.html#microsoft-and-sp-500-stock-prices---decisions-2",
    "title": "Time series data",
    "section": "Microsoft and S&P 500 stock prices - decisions 2",
    "text": "Microsoft and S&P 500 stock prices - decisions 2\n\nMonthly time series plot - easier to read\nAdditional decision needed: it is obviously non-stationary (Phillips-Perron test: very high p-value)\nUse returns:\n\nReturns: percent change of the closing prices: \\(100\\% \\frac{y_t - y_{t-1}}{y_t}\\).\nmonthly returns - take the closing price for the last day of a month\nAlternative measure: first difference of log prices."
  },
  {
    "objectID": "rm-data/slides/week09.html#microsoft-and-sp-500---index-returns-pct",
    "href": "rm-data/slides/week09.html#microsoft-and-sp-500---index-returns-pct",
    "title": "Time series data",
    "section": "Microsoft and S&P 500 - index returns (pct)",
    "text": "Microsoft and S&P 500 - index returns (pct)\n\nCode\nqui: {\nuse \"data_slides/stock-prices-daily.dta\", clear\nren *, low\nsort ym date\nbysort ym: gen flag = _n == _N\nkeep if flag==1\ntsset ym\ngen ret_sp500 = 100 * (p_sp500 - p_sp500[_n-1]) / p_sp500[_n-1]\ngen ret_msft = 100 * (p_msft - p_msft[_n-1]) / p_msft[_n-1]\n}\n\nline ret_sp500 ym, name(m1, replace)\nline ret_msft ym, name(m2, replace)"
  },
  {
    "objectID": "rm-data/slides/week09.html#unit-root-test-microsoft-and-sp-500-returns",
    "href": "rm-data/slides/week09.html#unit-root-test-microsoft-and-sp-500-returns",
    "title": "Time series data",
    "section": "Unit root test: Microsoft and S&P 500 returns",
    "text": "Unit root test: Microsoft and S&P 500 returns\n\n\nCode\npperron ret_sp500\npperron ret_msft\n\n\n\nPhillips–Perron test for unit root       Number of obs   = 251\nVariable: ret_sp500                      Newey–West lags =   4\n\nH0: Random walk without drift, d = 0\n\n                                       Dickey–Fuller\n                   Test      -------- critical value ---------\n              statistic           1%           5%          10%\n--------------------------------------------------------------\n Z(rho)        -230.845      -20.301      -14.000      -11.200\n Z(t)           -14.365       -3.460       -2.880       -2.570\n--------------------------------------------------------------\nMacKinnon approximate p-value for Z(t) = 0.0000.\n\nPhillips–Perron test for unit root       Number of obs   = 251\nVariable: ret_msft                       Newey–West lags =   4\n\nH0: Random walk without drift, d = 0\n\n                                       Dickey–Fuller\n                   Test      -------- critical value ---------\n              statistic           1%           5%          10%\n--------------------------------------------------------------\n Z(rho)        -284.851      -20.301      -14.000      -11.200\n Z(t)           -19.346       -3.460       -2.880       -2.570\n--------------------------------------------------------------\nMacKinnon approximate p-value for Z(t) = 0.0000."
  },
  {
    "objectID": "rm-data/slides/week09.html#time-series-regressions-the-same",
    "href": "rm-data/slides/week09.html#time-series-regressions-the-same",
    "title": "Time series data",
    "section": "Time series regressions: The same",
    "text": "Time series regressions: The same\n\nRegression in time series data is defined and estimated the same way as in other data.\n\n\\[y^E_t = \\beta_0 + \\beta_1 x_{1t} + \\beta_2 x_{2t} + \\ldots\\]\n\nInterpretations similar to cross-section\n\n\\(\\beta_0\\): We expect \\(y\\) to be \\(\\beta_0\\) when all explanatory variables are zero.\n\\(\\beta_1\\): Comparing time periods with different \\(x_1\\) but the same in terms of all other explanatory variables, we expect \\(y\\) to be higher by \\(\\beta_1\\) when \\(x_1\\) is higher by one unit."
  },
  {
    "objectID": "rm-data/slides/week09.html#time-series-regression-but-different",
    "href": "rm-data/slides/week09.html#time-series-regression-but-different",
    "title": "Time series data",
    "section": "Time series regression: But different",
    "text": "Time series regression: But different\n\nWith time series data, we often estimate regressions in changes\nWe use the \\(\\Delta\\) notation for changes\n\n\\[\\Delta x_t = x_t - x_{t-1}\\]\n\nThe regression in changes is \\[\\Delta y^E_t = \\alpha + \\beta \\Delta x_t\\]\n\n\\(\\alpha\\): \\(y\\) is expected to change by \\(\\alpha\\) when \\(x\\) doesn’t change\n\\(\\beta\\): \\(y\\) is expected to change by \\(\\beta\\) more when \\(x\\) increases by one unit more"
  },
  {
    "objectID": "rm-data/slides/week09.html#time-series-regression-growth",
    "href": "rm-data/slides/week09.html#time-series-regression-growth",
    "title": "Time series data",
    "section": "Time series regression: Growth",
    "text": "Time series regression: Growth\n\nWe often have variables in relative or percentage changes,\n\n\\[\\%\\Delta  (y_t)^E = \\alpha + \\beta \\%\\Delta(x_t)\\]\n\nWe can approximate relative differences by log differences, which are here log change: first taking logs of the variables and then taking the first difference\n\n\\[\\Delta \\ln(y_t) = \\ln(y_t) - \\ln(y_{t1})\\]"
  },
  {
    "objectID": "rm-data/slides/week09.html#returns-on-a-company-stock-and-market-returns",
    "href": "rm-data/slides/week09.html#returns-on-a-company-stock-and-market-returns",
    "title": "Time series data",
    "section": "Returns on a company stock and market returns",
    "text": "Returns on a company stock and market returns\n\\[\\%\\Delta(\\text{MSFT}_t) = \\alpha + \\beta \\%\\Delta (\\text{SP500}_t)\\]\n\n\\(\\alpha = 0.54\\); \\(\\beta = 1.26\\)\nIntercept: returns on the Microsoft stock tend to be 0.54 percent when the S%P500 index doesn’t change.\nSlope: returns on the Microsoft stock tend to be 1.26% higher when the returns on the S&P500 index are 1% higher. The 95% CI is \\([1.06, 1.46]\\).\nR-squared: 0.36\nFirst difference of log prices. Estimate is 1.24\nDaily returns (percent), beta is 1.10"
  },
  {
    "objectID": "rm-data/slides/week09.html#issues-to-deal-with-before-regression-a-laundry-list",
    "href": "rm-data/slides/week09.html#issues-to-deal-with-before-regression-a-laundry-list",
    "title": "Time series data",
    "section": "Issues to deal with, before Regression: a laundry list",
    "text": "Issues to deal with, before Regression: a laundry list\n\nHandling trend(s) and random walk (RW)\n\nAdd trend variable or Difference the series\n\nTransforming the series, such as taking first differences or percent change\nHandling seasonality and special events\n\nInclude dummies\n\nReconsidering standard errors. Specially if series have Unit Roots\nDealing with serial correlation - taking time-to-build into account with lags"
  },
  {
    "objectID": "rm-data/slides/week09.html#trend-rw---spurious-regression",
    "href": "rm-data/slides/week09.html#trend-rw---spurious-regression",
    "title": "Time series data",
    "section": "Trend & RW - Spurious regression",
    "text": "Trend & RW - Spurious regression\n\nTrends, seasonality, and random walks can present serious threats to uncovering meaningful patterns in time series data.\n\nExample: time series regression in levels \\(y^E_t = \\alpha + \\beta x_t\\).\nIf both \\(y\\) and \\(x\\) have a positive trend, the slope coefficient \\(\\beta\\) will be positive whether the two variables are related or not.\nAssociations between variables only because of the effect of trends are said to be spurious correlation.\n\nThink of trend and seasonality are confounders (omitted variables)\n\ntrend: global tendencies e.g. economic activity, fashion\nseasonality: e.g. weather, holidays, human habits (sleep)\n\nAnother reason for spurious correlation is small sample size…"
  },
  {
    "objectID": "rm-data/slides/week09.html#time-series-regressions-trends-and-seasonality",
    "href": "rm-data/slides/week09.html#time-series-regressions-trends-and-seasonality",
    "title": "Time series data",
    "section": "Time series regressions: Trends and seasonality",
    "text": "Time series regressions: Trends and seasonality\n\nTrend as confounder example\nA regression of the price of college education in the U.S. on the GDP of Germany over the past few decades\nPositive slope coefficient even though that two may not be related in any fundamental way.\nBut US GDP is correlated with both\n\nExamples https://tylervigen.com/spurious-correlations"
  },
  {
    "objectID": "rm-data/slides/week09.html#time-series-regressions-trends-and-seasonality-1",
    "href": "rm-data/slides/week09.html#time-series-regressions-trends-and-seasonality-1",
    "title": "Time series data",
    "section": "Time series regressions: Trends and seasonality",
    "text": "Time series regressions: Trends and seasonality\n\nIn a regression, we shall deal with trends\n\nReplacing variables in the regression with their first differences\n\nVariables in differences – no trends – likely to to be stationary.\nCould be log difference for exponential trends\n\nCould also add trends to model\n\nIn a regression, we shall deal with seasonality\n\nIncluding binary season variables in regressions.\nLook at pattern, figure out if quarters, months, weeks, days of week, etc.\nOr work with year-on-year (event to event) changes instead of first differences."
  },
  {
    "objectID": "rm-data/slides/week09.html#trend-rw---solution-first-differences",
    "href": "rm-data/slides/week09.html#trend-rw---solution-first-differences",
    "title": "Time series data",
    "section": "Trend & RW - solution: first differences",
    "text": "Trend & RW - solution: first differences\n\\[\\Delta y^E_t = \\alpha + \\beta \\Delta x_t\\]\n\nCoefficients have the same interpretation as before, but relate changes in variables.\nBecause variables denote changes…\n\n\\(\\alpha\\) is the average change in \\(y\\) when \\(x\\) doesn’t change.\nThe slope coefficient on \\(\\Delta x_t\\) shows how much more \\(y\\) is expected to change when \\(x\\) changes by one more unit.\nThe slope shows how \\(y\\) is expected to change when \\(x\\) changes, in addition to \\(\\alpha\\)."
  },
  {
    "objectID": "rm-data/slides/week09.html#seasonality-in-time-series-regressions",
    "href": "rm-data/slides/week09.html#seasonality-in-time-series-regressions",
    "title": "Time series data",
    "section": "Seasonality in time series regressions",
    "text": "Seasonality in time series regressions\n\nCapturing seasonality is also crucial.\nHigher the frequency – the more important.\n\nPeople behave differently on different hours and days\nWeather varies over months\nHolidays, ect\n\nHave seasonal dummies if seasonality is stable.\n\n\\[y^E_t = \\alpha + \\beta x_t + \\delta_{Jan} + \\delta_{Feb} + \\cdots + \\delta_{Nov}\\]\n\nPattern may vary over time. If it does, solutions must capture exact pattern – (difficult, not covering here)"
  },
  {
    "objectID": "rm-data/slides/week09.html#another-problem-serial-correlation",
    "href": "rm-data/slides/week09.html#another-problem-serial-correlation",
    "title": "Time series data",
    "section": "Another Problem: Serial correlation",
    "text": "Another Problem: Serial correlation\n\nSerial correlation means correlation of a variable with its previous values\n\nIt usually refers to the correlation of the residuals with their previous values.\n\nOrder serial correlation coefficient is defined as \\[\\rho_k = \\text{Corr}[x_t, x_{t-k}]\\]\nIf independent variables are serially correlated, usually not a problem.\nIf dependent variable is serially correlated (the error), it is a problem.\nSerial correlation makes the usual standard error estimates wrong.\n\nEven classical heteroskedasticity robust SE is wrong - sometimes very wrong\n\nMore precisely it is serial correlation in residuals, but think about is as serial correlation in \\(y_t\\) is okay"
  },
  {
    "objectID": "rm-data/slides/week09.html#standard-errors-in-time-series-regressions",
    "href": "rm-data/slides/week09.html#standard-errors-in-time-series-regressions",
    "title": "Time series data",
    "section": "Standard errors in time series regressions",
    "text": "Standard errors in time series regressions\n\nIn most time series, there will be some serial correlation\nSol1: Use the Newey-West SE: in Stata newey command.\n\nThis incorporates structure of serial correlation of the regression residuals\nFine if heteroskedasticity as well\nNeed to specify lags, based on frequency and seasonality\n\nSol2: Have lagged dependent variable in the regression (one lag is usually enough) \\[y_t = \\alpha + \\beta x_t + \\gamma_1 y_{t-1} + \\gamma_2 y_{t-2} \\ldots\\]\n\nThis will change Coefficients and adjust SEs"
  },
  {
    "objectID": "rm-data/slides/week09.html#electricity-consumption-and-temperature",
    "href": "rm-data/slides/week09.html#electricity-consumption-and-temperature",
    "title": "Time series data",
    "section": "Electricity consumption and temperature",
    "text": "Electricity consumption and temperature\n\nWe have access to monthly weather and electricity data for Phoenix, Arizona: Overall 204 month\nAlso access to “cooling degree days” and “heating degree days” data\nThe cooling degree days measure Number of degrees that a day’s average temperature is above 65F (18C). This is add up over the month.\nSimilar for heating degree days but below 65F (18C)\nAccess to Electricity consumption data over the month\nAs expected, this values will be seasonal."
  },
  {
    "objectID": "rm-data/slides/week09.html#cs-modelling-decisions",
    "href": "rm-data/slides/week09.html#cs-modelling-decisions",
    "title": "Time series data",
    "section": "CS: Modelling decisions",
    "text": "CS: Modelling decisions\n\nThere is an exponential trend in electricity –&gt; use log difference\nFor easier interpretation, take first difference (FD) of cooling and heating days\nNatural question: How much does electricity consumption change when temperature changes?\nAdd monthly dummies, January (December to January) is reference\nUse Newey-West standard errors"
  },
  {
    "objectID": "rm-data/slides/week09.html#model-estimates",
    "href": "rm-data/slides/week09.html#model-estimates",
    "title": "Time series data",
    "section": "Model estimates",
    "text": "Model estimates\n\n\n\nVariables\n(1)\n(2)\n\n\n\n\n\\(\\Delta\\)CD\n0.031**\n0.017**\n\n\n\n(0.001)\n(0.002)\n\n\n\\(\\Delta\\)HD\n0.037**\n0.014**\n\n\n\n(0.003)\n(0.003)\n\n\nmonth = 2, February\n\n-0.274**\n\n\nmonth = 3, March\n\n-0.122**\n\n\n…\n\n\n\n\nConstant\n0.001\n0.092**\n\n\n\n(0.002)\n(0.013)\n\n\nObservations\n203\n203"
  },
  {
    "objectID": "rm-data/slides/week09.html#model-results",
    "href": "rm-data/slides/week09.html#model-results",
    "title": "Time series data",
    "section": "Model results",
    "text": "Model results\n\nSimple (1) model:\n\nIn months when cooling degrees increase by one degree and heating degrees do not change, electricity consumption increases by 3.1 percent, on average.\nWhen heating degrees increase by one degree and cooling degrees do not change, electricity consumption increases by 3.7 percent, on average.\n\nModel (2) with monthly dummies.\n\nThe reference month is January: constant (when cooling and heating degrees stay the same), electricity consumption increases by about 9% from December to January.\nThe other season coefficients compare to this change:\n\nFebruary: the January to February change is 28 percentage points lower than in the reference month, December to January."
  },
  {
    "objectID": "rm-data/slides/week09.html#electricity-consumption-and-temperature-different-se-estimates",
    "href": "rm-data/slides/week09.html#electricity-consumption-and-temperature-different-se-estimates",
    "title": "Time series data",
    "section": "Electricity consumption and temperature – different SE estimates",
    "text": "Electricity consumption and temperature – different SE estimates\n\n\n\nVariables\n(1)\n(2)\n(3)\n\n\n\n\n\nSimple SE\nNewey–West SE\nLagged dep.var\n\n\n\\(\\Delta\\)CD\n0.017**\n0.017**\n0.017**\n\n\n\n(0.002)\n(0.002)\n(0.002)\n\n\n\\(\\Delta\\)HD\n0.014**\n0.014**\n0.014**\n\n\n\n(0.002)\n(0.003)\n(0.002)\n\n\nLag of \\(\\Delta\\) ln Q\n\n\n-0.002\n\n\n\n\n\n(0.062)\n\n\nMonth dummies\nYES\nYES\nYES\n\n\nObservations\n203\n203\n202\n\n\nR-squared\n0.951\n0.951\n\n\n\nStandard errors in parentheses\n\n\n\n\n\n** p&lt;0.01, * p&lt;0.05"
  },
  {
    "objectID": "rm-data/slides/week09.html#electricity-consumption-and-temperature-different-se-estimates-1",
    "href": "rm-data/slides/week09.html#electricity-consumption-and-temperature-different-se-estimates-1",
    "title": "Time series data",
    "section": "Electricity consumption and temperature – different SE estimates",
    "text": "Electricity consumption and temperature – different SE estimates\n\nTo correct for serial correlation, compare simple SE model with two correctly specified models\n\nwith Newey-West SE\nwith Lagged dependent variable\n\nSE marginally different, and with lagged values, coefficients are also similar up to 3 digits\n\nSimilar, not the same\nSometimes substantial difference (if strong serial correlation)"
  },
  {
    "objectID": "rm-data/slides/week09.html#propagation-effect-changes-and-lags---fdl-model",
    "href": "rm-data/slides/week09.html#propagation-effect-changes-and-lags---fdl-model",
    "title": "Time series data",
    "section": "Propagation effect: changes and lags - FDL model",
    "text": "Propagation effect: changes and lags - FDL model\nIn time series, we can analyze how an impact builds up across several periods (time-to-build):\n\\[\\Delta y^E_t = \\alpha + \\beta_0 \\Delta x_t + \\beta_1 \\Delta x_{t-1} + \\beta_2 \\Delta x_{t-2}\\]\n\n\\(\\beta_0\\) = how many units more \\(y\\) is expected to change within the same time period when \\(x\\) changes by one more unit (No change before or after).\n\\(\\beta_1\\) = how much more \\(y\\) is expected to change in the next time period after \\(x\\) changed by one more unit.\nCumulative effect: \\(\\beta_\\text{cumul} = \\beta_0 + \\beta_1 + \\beta_2\\)\n\\(\\beta_k\\) are the short-term effects, \\(\\beta_\\text{cumul}\\) is the cumulative effect/Long Term Effect."
  },
  {
    "objectID": "rm-data/slides/week09.html#testing-the-cumulative-effect",
    "href": "rm-data/slides/week09.html#testing-the-cumulative-effect",
    "title": "Time series data",
    "section": "Testing the cumulative effect",
    "text": "Testing the cumulative effect\n\nTo get a SE on the cumulative effect, do a trick and transformation, and estimate a different model\n\n\\[\n\\begin{aligned}\n\\Delta y^E_t &= \\alpha + \\beta_0 \\Delta x_t + \\beta_1 \\Delta x_{t-1} + \\beta_2 \\Delta x_{t-2} \\\\\n\\Delta y^E_t &= \\alpha + (\\beta_\\text{cumul} - \\beta_1 - \\beta_2) \\Delta x_t + \\beta_1 \\Delta x_{t-1} + \\beta_2 \\Delta x_{t-2} \\\\\n\\Delta y^E_t &= \\alpha + \\beta_\\text{cumul} \\Delta x_t + \\beta_1 (\\Delta x_{t-1}- \\Delta x_t) + \\beta_2 (\\Delta x_{t-2} - \\Delta x_t)\\\\\n\\Delta y^E_t &= \\alpha + \\beta_\\text{cumul} \\Delta x_{t} + \\delta_0 \\Delta(\\Delta x_t) + \\delta_1 \\Delta(\\Delta x_{t-1})\n\\end{aligned}\\]\n\n\\(\\beta_\\text{cumul}\\) is exactly the same as \\(\\beta_0 + \\beta_1 + \\beta_2\\)\nUsually estimate both. Separate and cumulative effect\nOften need a few lags"
  },
  {
    "objectID": "rm-data/slides/week09.html#propagation-effect-changes-and-lags---fdl-model-1",
    "href": "rm-data/slides/week09.html#propagation-effect-changes-and-lags---fdl-model-1",
    "title": "Time series data",
    "section": "Propagation effect: changes and lags - FDL model",
    "text": "Propagation effect: changes and lags - FDL model\nSee Case Study in Chapter 12"
  },
  {
    "objectID": "rm-data/slides/week09.html#summary-of-the-process",
    "href": "rm-data/slides/week09.html#summary-of-the-process",
    "title": "Time series data",
    "section": "Summary of the process",
    "text": "Summary of the process\n\nDecide on frequency; deal with gaps if necessary.\nPlot the series. Identify features and issues.\nHandle trends by transforming variables (Often: first difference d.).\nSpecify regression that handles seasonality, usually by including season dummies.\nInclude or don’t include lags of the right-hand-side variable(s).\nHandle serial correlation. newey or lagged dependent variable.\nInterpret coefficients in a way that pays attention to potential trend and seasonality.\nTime series econometrics very complicated beyond this.\nBut: These steps often good enough."
  },
  {
    "objectID": "rm-data/slides/week09.html#main-takeaways",
    "href": "rm-data/slides/week09.html#main-takeaways",
    "title": "Time series data",
    "section": "Main takeaways",
    "text": "Main takeaways\n\nRegressions with time series data allow for additional opportunities, but they pose additional challenges, too\nRegressions with time series data help uncover associations from changes and associations across time\nTrend, seasonality, and random walk-like non-stationarity are additional challenges\nDo not regress variables that have trend or seasonality; without dealing with them they produce spurious results"
  },
  {
    "objectID": "rm-data/slides/week07.html#motivation",
    "href": "rm-data/slides/week07.html#motivation",
    "title": "Multiple regression analysis",
    "section": "Motivation",
    "text": "Motivation\n\nWe are interested in finding evidence for or against labor market discrimination of women. Compare wages for men and women who share similarities in wage relevant factors such as experience and education.\nFind a good deal on a hotel to spend a night in a European city- analyzed the pattern of hotel price and distance and many other features to find hotels that are underpriced not only for their location but also those other features."
  },
  {
    "objectID": "rm-data/slides/week07.html#topics-to-cover",
    "href": "rm-data/slides/week07.html#topics-to-cover",
    "title": "Multiple regression analysis",
    "section": "Topics to cover",
    "text": "Topics to cover\n\nMultiple regression mechanics\nEstimation and interpreting coefficients\nNon-linear terms, interactions\nVariable selection, small sample problems\nMultiple regression and causality\nMultiple regression and prediction"
  },
  {
    "objectID": "rm-data/slides/week07.html#multivariate-regression",
    "href": "rm-data/slides/week07.html#multivariate-regression",
    "title": "Multiple regression analysis",
    "section": "Multivariate Regression",
    "text": "Multivariate Regression\n\nWhenever you start modeling an outcome \\(y\\), there will always be two factors that will determine that outcome:\n\nFactors that you can control (e.g., education, experience, etc.)\nFactors that you cannot control (e.g., errors)\n\n\nMultiple regression analysis uncovers average \\(y\\) as a function of more than one \\(x\\) variable: \\(y^E = f(x_1, x_2, ...)\\).\nIt can lead to better predictions \\(\\hat{y}\\) by considering more explanatory variables.\nIt may improve the interpretation of slope coefficients by comparing observations that are similar in terms of other \\(x's\\) variables."
  },
  {
    "objectID": "rm-data/slides/week07.html#multivariate-regression-1",
    "href": "rm-data/slides/week07.html#multivariate-regression-1",
    "title": "Multiple regression analysis",
    "section": "Multivariate Regression",
    "text": "Multivariate Regression\n\nMultiple linear regression specifies a linear function of the explanatory variables for the average \\(y\\): \\[y^E = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k\\]\nBut, now that we now we can have more than one \\(x\\) variable, what happens if we don’t?"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-ommited-variable-bias",
    "href": "rm-data/slides/week07.html#mr-ommited-variable-bias",
    "title": "Multiple regression analysis",
    "section": "MR: Ommited Variable Bias",
    "text": "MR: Ommited Variable Bias\nLets say we have to models: \\[\\begin{aligned}\ny &= \\alpha_0 + \\alpha_1 x_1 + \\varepsilon_1  \\\\\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon_2\n\\end{aligned}\n\\]\n\nHow do \\(\\alpha_1\\) and \\(\\beta_1\\) compare?\nLets Start by regressing \\(x_2\\) on \\(x_1\\): \\(x_2 = \\delta_0 + \\delta_1 x_1 + u\\)\nAnd plug this back into the second equation:\n\n\\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 (\\delta_0 + \\delta_1 x_1 + u) + \\varepsilon_2 \\\\\ny &= (\\beta_0 + \\beta_2\\delta_0) + (\\beta_1 + \\beta_2 \\delta_1) x  + (\\beta_2 u + \\varepsilon_2)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-ommited-variable-bias-1",
    "href": "rm-data/slides/week07.html#mr-ommited-variable-bias-1",
    "title": "Multiple regression analysis",
    "section": "MR: Ommited Variable Bias",
    "text": "MR: Ommited Variable Bias\n\nSo it turns out that:\n\n\\[\\alpha_1 = \\beta_1 + \\beta_2 \\delta_1 \\rightarrow \\beta_1-\\alpha_1 = -\\beta_2 \\delta_1\n\\]\n\nBy “ignoring” \\(x_2\\) in the first regression, we are actually estimating a biased coefficient for \\(x_1\\).\n\nAssuming the second model is the “true” model\n\nThis is what is known as the Omitted variable bias OBV\n\nNote: You could also have a bias because you are including a variable that should not be there. This is known as bad control."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-ommited-variable-bias-2",
    "href": "rm-data/slides/week07.html#mr-ommited-variable-bias-2",
    "title": "Multiple regression analysis",
    "section": "MR: Ommited Variable Bias",
    "text": "MR: Ommited Variable Bias\n\nOBV is a common problem in empirical research because we can never include all the variables that determine \\(y\\).\nHowever, mechanically, there are two cases where OBV is not a problem:\n\nWhen \\(x_1\\) and \\(x_2\\) are uncorrelated (\\(\\delta_1 = 0\\))\nWhen \\(y\\) and \\(x_2\\) are uncorrelated (\\(\\beta_2 = 0\\))"
  },
  {
    "objectID": "rm-data/slides/week07.html#simple-example",
    "href": "rm-data/slides/week07.html#simple-example",
    "title": "Multiple regression analysis",
    "section": "Simple example",
    "text": "Simple example\n\nTS regression: Regress month-to-month change in log quantity sold of Beer (\\(y\\)) on month-to-month change in log price (\\(x_1\\)).\n\n\\(\\beta = -0.5\\): sales tend to decrease by 0.5% when our price increases by 1%.\n\nRobustness: \\(x_2\\): change in ln average price charged by our competitors\n\nNew Results: \\(\\hat{\\beta}_1 = -3\\) and \\(\\hat{\\beta}_2 = 3\\)\n\nThere is a OBV (Model 1 is flatter than Model 2)\nPossibly the result of two things:\n\na positive association between the two price changes (\\(\\delta_1\\)) and\na positive association between competitor price and our own sales (\\(\\beta_2\\))."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-some-language",
    "href": "rm-data/slides/week07.html#mr-some-language",
    "title": "Multiple regression analysis",
    "section": "MR: Some language",
    "text": "MR: Some language\n\nSetup: Multiple regression with two explanatory variables (\\(x_1\\) and \\(x_2\\)),\nTechnicallity:: We measure differences in expected \\(y\\) across observations that differ in \\(x_1\\) but are similar in terms of \\(x_2\\).\nInterpretation: Difference in \\(y\\) by \\(x_1\\), conditional on \\(x_2\\). OR controlling for \\(x_2\\).\n\nWe condition on \\(x_2\\), or control for \\(x_2\\), when we include \\(x_2\\) in a multiple regression that focuses on average differences in \\(y\\) by \\(x_1\\).\n\nWhat we care is \\(x_1\\)’s effect on \\(y\\), but we control for \\(x_2\\) to get a better estimate of this effect.\nConfounding: \\(x_2\\) is a confounder if \\(x_2\\) is correlated with \\(x_1\\) and \\(y\\).\n\nThus, we have a problem if we omit \\(x_2\\) from the regression."
  },
  {
    "objectID": "rm-data/slides/week07.html#stata-multiple-regression",
    "href": "rm-data/slides/week07.html#stata-multiple-regression",
    "title": "Multiple regression analysis",
    "section": "Stata: Multiple regression",
    "text": "Stata: Multiple regression\nregress y x1 [x2 x3 ... ], robust\nestimates store m1\nesttab m1, star(* 0.10 ** 0.05 *** 0.01) label"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-standard-errors",
    "href": "rm-data/slides/week07.html#mr-standard-errors",
    "title": "Multiple regression analysis",
    "section": "MR: Standard Errors",
    "text": "MR: Standard Errors\n\\[\\text{SE}(\\hat{\\beta}_1) = \\frac{\\text{Std}[e]}{\\sqrt{n}\\text{Std}(x_1)\\color{blue}{\\sqrt{1 - R^2_1}}}\\]\n\nSame:\n\nthe SE is small if better the fit, large samples, or large the Std of \\(x_1\\).\n\nNew: \\(\\sqrt{1 - R^2_1}\\) term in the denominator.\n\nthe R-squared of the regression of \\(x_1\\) on \\(x_2\\)\n\nThe higher is \\(R^2_1\\), the larger the SE of \\(\\hat{\\beta}_1\\).\nNote: in practice, use robust SE"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-collinearity",
    "href": "rm-data/slides/week07.html#mr-collinearity",
    "title": "Multiple regression analysis",
    "section": "MR: Collinearity",
    "text": "MR: Collinearity\n\nPerfectly collinearity is when \\(x_i\\) is a linear function of \\(x_{-i}\\).\n\nConsequence: cannot calculate coefficients.\nOne will be dropped by software (but you should know which one).\n\nStrong but imperfect correlation between explanatory is sometimes called multicollinearity.\n\nConsequence: We can get the slope coefficients and their standard errors,\nBut, the standard errors may be large."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-collinearity-and-se",
    "href": "rm-data/slides/week07.html#mr-collinearity-and-se",
    "title": "Multiple regression analysis",
    "section": "MR: Collinearity and SE",
    "text": "MR: Collinearity and SE\n\nStrong multicollinearity is a problem because it increases the standard errors of the coefficients.\n\nIt is typically a problem when the sample size is small.\n\nNumerically, it could make the coefficient estimates unstable. (rare)\nMore often, you may need to either drop one of the variables, or\nCombine them into a single variable. (index)\n\nHow to know how strong is the multicollinearity problem ??\n\nEstimate \\(R^2\\) of the regression of \\(x_i\\) on all other \\(x\\) variables. For all cases!\n\nor use the estat vif command in Stata. (only with OLS)"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-collinearity-1",
    "href": "rm-data/slides/week07.html#mr-collinearity-1",
    "title": "Multiple regression analysis",
    "section": "MR: Collinearity",
    "text": "MR: Collinearity\n\nqui:frause oaxaca, clear\nqui:regress lnwage female educ exper tenure c.age c.age#c.age, robust\nestat vif\n\n\n\n\n\n    Variable |       VIF       1/VIF  \n-------------+----------------------\n      female |      1.11    0.900413\n        educ |      1.15    0.873284\n       exper |      2.48    0.403924\n      tenure |      1.82    0.549891\n         age |     54.62    0.018310\n c.age#c.age |     53.08    0.018839\n-------------+----------------------\n    Mean VIF |     19.04"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-testing-single-hypotheses",
    "href": "rm-data/slides/week07.html#mr-testing-single-hypotheses",
    "title": "Multiple regression analysis",
    "section": "MR: Testing Single hypotheses",
    "text": "MR: Testing Single hypotheses\n\nSame as before, but now we have more than one \\(x\\) variable to test.\n\n\\(H_0: \\beta_1 = 0\\)\n\nYou may want to be careful with multiple testing.\n\ntesting each coefficient separately with the same \\(\\alpha\\) level\n\nThere is also testing single hypotheses on combinations of coefficients.\n\n$H_0: _1-2*_2=0 $\n\nAs before, you just need to know the point estimate and the standard error to calculate the t-statistic."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-testing-joint-hypotheses",
    "href": "rm-data/slides/week07.html#mr-testing-joint-hypotheses",
    "title": "Multiple regression analysis",
    "section": "MR: Testing Joint hypotheses",
    "text": "MR: Testing Joint hypotheses\n\nTesting joint hypotheses: null hypotheses that contain statements about more than one regression coefficient: \\(H_0: \\beta_1 = \\beta_2 = 0\\) vs \\(H_1: H_0\\) is false\nThis kind of test is used to evaluate a subset of the coefficients (such as all geographical variables) are all zero.\nBut for doing this you need a new test statistic: the F-test.\n\nDifference with the t-test:\n\nIn contrast with the t-test, the F-test follows an F-distribution.\nThis distribution is not symmetric! And you need to know the degrees of freedom.\n\nHow many restrictions are you imposing? and how many coefficients did you estimate?\n\nAlso, all test are on-sided"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-testing-joint-hypotheses-1",
    "href": "rm-data/slides/week07.html#mr-testing-joint-hypotheses-1",
    "title": "Multiple regression analysis",
    "section": "MR: Testing Joint hypotheses",
    "text": "MR: Testing Joint hypotheses\n\nF-test"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-testing-hypotheses-in-stata",
    "href": "rm-data/slides/week07.html#mr-testing-hypotheses-in-stata",
    "title": "Multiple regression analysis",
    "section": "MR: Testing hypotheses in Stata",
    "text": "MR: Testing hypotheses in Stata\n\nRegressionJoint testSingle test Combined\n\n\n\nqui:webuse dui, clear\nregress  citations  fines i.taxes i.csize i.college, robust nohead\n** regress, coefleg to know \"names\" of variables\n\n------------------------------------------------------------------------------\n             |               Robust\n   citations | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       fines |  -7.690437   .3843873   -20.01   0.000    -8.445672   -6.935201\n             |\n       taxes |\n        Tax  |  -4.493918   .5819239    -7.72   0.000    -5.637269   -3.350566\n             |\n       csize |\n     Medium  |   5.492308    .531599    10.33   0.000     4.447834    6.536782\n      Large  |   11.23563   .5709191    19.68   0.000      10.1139    12.35736\n             |\n     college |\n    College  |   5.828441    .588277     9.91   0.000     4.672607    6.984274\n       _cons |   94.21955   3.948926    23.86   0.000     86.46079    101.9783\n------------------------------------------------------------------------------\n\n\n\n\n\ntest 1.taxes 1.college // &lt;- automatically test the joint hypothesis\n\n\n ( 1)  1.taxes = 0\n ( 2)  1.college = 0\n\n       F(  2,   494) =   66.74\n            Prob &gt; F =    0.0000\n\n\n\n\n\n** \"H0: 2*B_Taxes = B_fines\"\nlincom 2*1.taxes-fines\n\n\n ( 1)  - fines + 2*1.taxes = 0\n\n------------------------------------------------------------------------------\n   citations | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |  -1.297398   1.098122    -1.18   0.238    -3.454964    .8601678\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-non-linear-patterns",
    "href": "rm-data/slides/week07.html#mr-non-linear-patterns",
    "title": "Multiple regression analysis",
    "section": "MR: Non-linear patterns",
    "text": "MR: Non-linear patterns\n\nSurprise! you can use the same tools as with single regression\n\nUses splines, polynomials, other non-linear functions of \\(x\\) variables.\n\nNon-linear function of various \\(x_i\\) variables may be combined.\nAs show before, using non-linear functions will increase multicollinearity, but worry not about that type of collinearity.\nBe more careful with the interpretation of the coefficients."
  },
  {
    "objectID": "rm-data/slides/week07.html#cs-understanding-the-gender-difference-in-earnings",
    "href": "rm-data/slides/week07.html#cs-understanding-the-gender-difference-in-earnings",
    "title": "Multiple regression analysis",
    "section": "CS: Understanding the gender difference in earnings",
    "text": "CS: Understanding the gender difference in earnings\n\nIn the USA (2014), women tend to earn about 20% less than men\nAim 1: Find patterns to better understand the gender gap. Our focus is the interaction with age.\nAim 2: Think about if there is a causal link from being female to getting paid less."
  },
  {
    "objectID": "rm-data/slides/week07.html#cs-the-data",
    "href": "rm-data/slides/week07.html#cs-the-data",
    "title": "Multiple regression analysis",
    "section": "CS: The data",
    "text": "CS: The data\n\n2014 census data\nAge between 15 to 65\nExclude self-employed (earnings is difficult to measure)\nInclude those who reported 20 hours more as their usual weekly time worked\nEmployees with a graduate degree (higher than 4-year college)\nUse log hourly earnings (\\(\\ln w\\)) as dependent variable\nUse gender and add age as explanatory variables"
  },
  {
    "objectID": "rm-data/slides/week07.html#cs-the-model",
    "href": "rm-data/slides/week07.html#cs-the-model",
    "title": "Multiple regression analysis",
    "section": "CS: The model",
    "text": "CS: The model\nWe are quite familiar with the relation between earnings and gender: \\[\\ln w_E = \\alpha + \\beta\\text{female}, \\beta &lt; 0\\] Let’s include age as well: \\[\\ln w_E = \\beta_0 + \\beta_1\\text{female} + \\beta_2\\text{age}\\]\nWhat happens if we do not include age?"
  },
  {
    "objectID": "rm-data/slides/week07.html#cs-the-regression",
    "href": "rm-data/slides/week07.html#cs-the-regression",
    "title": "Multiple regression analysis",
    "section": "CS: The Regression",
    "text": "CS: The Regression\n\n\n\nVariables\nln wage\nln wage\nage\n\n\n\n\nfemale\n-0.195**\n-0.185**\n-1.484**\n\n\n\n(0.008)\n(0.008)\n(0.159)\n\n\nage\n\n0.007**\n\n\n\n\n\n(0.000)\n\n\n\nConstant\n3.514**\n3.198**\n44.630**\n\n\n\n(0.006)\n(0.018)\n(0.116)\n\n\nObservations\n18,241\n18,241\n18,241\n\n\nR-squared\n0.028\n0.046\n0.005\n\n\n\n\nNote: Robust standard errors in parentheses\n*** p&lt;0.01, ** p&lt;0.05, * p&lt;0.1\nSource: cps-earnings dataset. 2014 CPS Morg.\n\nWhat if Age is not linear? (has a non-linear effect on earnings)"
  },
  {
    "objectID": "rm-data/slides/week07.html#cs-adjustment-and-robustness",
    "href": "rm-data/slides/week07.html#cs-adjustment-and-robustness",
    "title": "Multiple regression analysis",
    "section": "CS: Adjustment and Robustness",
    "text": "CS: Adjustment and Robustness\n\n\n\nVariable\nModel 1\nModel 2\nModel 3\nModel 4\n\n\n\n\nfemale\n-0.195**\n-0.185**\n-0.183**\n-0.183**\n\n\n\n(0.008)\n(0.008)\n(0.008)\n(0.008)\n\n\nage\n\n0.007**\n0.063**\n0.572**\n\n\n\n\n(0.000)\n(0.003)\n(0.116)\n\n\nage2\n\n\n-0.001**\n-0.017**\n\n\n\n\n\n(0.000)\n(0.004)\n\n\nage3\n\n\n\n0.000**\n\n\n\n\n\n\n(0.000)\n\n\nObservations\n18,241\n18,241\n18,241\n18,241\n\n\nR-squared\n0.028\n0.046\n0.060\n0.062\n\n\n\n\nNote: Robust standard errors in parentheses, *** p&lt;0.01, ** p&lt;0.05, * p&lt;0.1\nSource: cps-earnings dataset. 2014 CPS Morg."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-qualitative-variables",
    "href": "rm-data/slides/week07.html#mr-qualitative-variables",
    "title": "Multiple regression analysis",
    "section": "MR: Qualitative variables",
    "text": "MR: Qualitative variables\n\nMR can also handle using qualitative variables as explanatory variables.\nTwo ways to include qualitative variables:\n\nCreate a dummy for each category.\nLet the software create the binary variables for you.\n\nYou can only include \\(k-1\\) dummies (dummy variable trap)\n\nLeft out category is the reference category (Base).\n\nStata Corner\n\ni. in front of a variable tells Stata to treat it as a categorical variable. (makes dummies on the background): reg lnwage i.educ\nBut, the categorical variable cannot be negative."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-qualitative-variables-1",
    "href": "rm-data/slides/week07.html#mr-qualitative-variables-1",
    "title": "Multiple regression analysis",
    "section": "MR: Qualitative variables",
    "text": "MR: Qualitative variables\nSay \\(X\\) is a qualitative variable with \\(3\\) categories: low, medium, and high. \\[y^E = \\beta_0 + \\beta_1 D^{med} + \\beta_2 D^{high}\\]\n\nlow is the reference category. Other values compared to this.\n\\(\\beta_0\\) shows average \\(y\\) in the reference category. (medium and high \\(=0\\))\n\\(\\beta_1\\): Average difference between \\(D_{medium}\\) and \\(D_low\\)\n\\(\\beta_2\\): Average difference between \\(D_{high}\\) and \\(D_low\\)"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-how-to-pick-a-reference-category",
    "href": "rm-data/slides/week07.html#mr-how-to-pick-a-reference-category",
    "title": "Multiple regression analysis",
    "section": "MR: How to pick a reference category?",
    "text": "MR: How to pick a reference category?\n\nChoose the category to which we want to compare the rest.\n\nHome country, the capital city, the lowest or highest value group.\n\nOr, chose a category with a large number of observations.\n\nImportant when inference is important, and SE are needed.\n\nFor prediction, it does not matter."
  },
  {
    "objectID": "rm-data/slides/week07.html#cs-gender-difference-in-earnings-and-education",
    "href": "rm-data/slides/week07.html#cs-gender-difference-in-earnings-and-education",
    "title": "Multiple regression analysis",
    "section": "CS: Gender difference in earnings and education",
    "text": "CS: Gender difference in earnings and education\n\n\n\n\n\n\n\n\n\nVariables\nln wage\nln wage\nln wage\n\n\n\n\nfemale\n-0.195**\n-0.182**\n-0.182**\n\n\n\n(0.008)\n(0.009)\n(0.009)\n\n\ned_Profess\n\n0.134**\n-0.002\n\n\n\n\n(0.015)\n(0.018)\n\n\ned_PhD\n\n0.136**\n\n\n\n\n\n(0.013)\n\n\n\ned_MA\n\n\n-0.136**\n\n\n\n\n\n(0.013)\n\n\nConstant\n3.514**\n3.473**\n3.609**\n\n\n\n(0.006)\n(0.007)\n(0.013)\n\n\nObservations\n18,241\n18,241\n18,241\n\n\nR-squared\n0.028\n0.038\n0.038"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-interactions",
    "href": "rm-data/slides/week07.html#mr-interactions",
    "title": "Multiple regression analysis",
    "section": "MR: Interactions",
    "text": "MR: Interactions\n\nOften data is made up of important groups: male and female workers or countries in different continents.\nand, Some of the patterns we are after may vary across these groups.\nThe strength of a relation may also be altered by a special variable.\n\nIn medicine, a moderator variable can reduce or amplify the effect of a drug on people.\nIn business, financial strength can affect how firms may weather a recession.\n\nMessage: different patterns for subsets of observations."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-interactions-and-parallel-lines",
    "href": "rm-data/slides/week07.html#mr-interactions-and-parallel-lines",
    "title": "Multiple regression analysis",
    "section": "MR: Interactions and parallel lines",
    "text": "MR: Interactions and parallel lines\n\nOption 1: Simply the Dummy to the model\n\n\\(y^E = \\beta_0 + \\beta_1 x_1 + \\beta_2 D\\)\n\nThis assumes \\(\\beta_1\\) is the same for both groups, only the intercepts are different.\nOption 2: Different slopes\n\n\\(y^E = \\beta_0 + \\beta_1 x_1 + \\beta_2 D + \\beta_3 x_1 \\times D\\)\n\n\nThis now assumes slopes are different for both groups.\nOption 3: Separate regressions\n\nBut Option 2 is better for testing if slopes are different."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-interaction-with-two-continuous-variable",
    "href": "rm-data/slides/week07.html#mr-interaction-with-two-continuous-variable",
    "title": "Multiple regression analysis",
    "section": "MR: Interaction with two continuous variable",
    "text": "MR: Interaction with two continuous variable\n\nInteractions can also be used with continuous variables, \\(x_1\\) and \\(x_2\\): \\[y_E = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2\\]\nExample:\n\n\\(y\\) is change in revenue \\(x_1\\) is change in global demand, \\(x_2\\) is firm’s financial health\nThe interaction can capture that drop in demand can cause financial problems in firms, but less so for firms with better balance sheet.\n\nPerhaps biggest challenge is to interpret a model with interactions."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-interaction-with-two-continuous-variable-1",
    "href": "rm-data/slides/week07.html#mr-interaction-with-two-continuous-variable-1",
    "title": "Multiple regression analysis",
    "section": "MR: Interaction with two continuous variable",
    "text": "MR: Interaction with two continuous variable\n\nThe interaction term \\(x_1 x_2\\) captures how this two variables affect each others effect on \\(y\\).\n\nTypically we assume this is zero.\n\nThe coefficient \\(\\beta_3\\) captures the magnitude of the interaction effect.\nHowever, if you are interested in the relationship betwee \\(x_1\\) or \\(x_2\\) with \\(y\\), you need extra care.\n\n\\(x_1\\) on \\(y\\) is \\(\\frac{dy}{dx_1}=\\beta_1 + \\beta_3 x_2\\).\n\\(x_2\\) on \\(y\\) is \\(\\frac{dy}{dx_2}=\\beta_2 + \\beta_3 x_1\\).\n\nSo you need to “fix” \\(x_2\\) to see the effect of \\(x_1\\) on \\(y\\). (typically at the mean)"
  },
  {
    "objectID": "rm-data/slides/week07.html#cs-interaction-between-gender-and-age",
    "href": "rm-data/slides/week07.html#cs-interaction-between-gender-and-age",
    "title": "Multiple regression analysis",
    "section": "CS: Interaction between gender and age",
    "text": "CS: Interaction between gender and age\n\n\n\nSeparate, Earning for men rises faster with age\nWith interaction, Same result!.\nFemale dummy is close to zero. Does this mean no gender gap?\n\nNo, Cumulative effect: \\(-0.036-0.003*age\\)\n\n\n\n\n\n\nVariables\nln wage (Women)\nln wage (Men)\nln wage (All)\n\n\n\n\nfemale\n\n\n-0.036\n\n\n\n\n\n(0.035)\n\n\nage\n0.006**\n0.009**\n0.009**\n\n\n\n(0.001)\n(0.001)\n(0.001)\n\n\nfemale × age\n\n\n-0.003**\n\n\n\n\n\n(0.001)\n\n\nConstant\n3.081**\n3.117**\n3.117**\n\n\n\n(0.023)\n(0.026)\n(0.026)\n\n\nObservations\n9,685\n8,556\n18,241\n\n\nR-squared\n0.011\n0.028\n0.047"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-stata-corner",
    "href": "rm-data/slides/week07.html#mr-stata-corner",
    "title": "Multiple regression analysis",
    "section": "MR: Stata corner",
    "text": "MR: Stata corner\n\nIn Stata you can use i. to create dummies for all categories of a variable.\nYou can also use # to create interactions between variables.\n\nUnless specified, Stata assume variables are categorical. You can use c. for continuous variables.\n\nYou can also use ## to create interactions plus the main effects.\n\nregress y i.x1##c.x2\nis equivalent to\nregress y i.x1 c.x2 i.x1#c.x2\nwhere i.x1 will create all dummies for x1\n\nIf dummies and interactions are created this way you can use margins to calculate effects of main variables.\n\nmargins, dydx(x1 x2)"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-causal-analysis",
    "href": "rm-data/slides/week07.html#mr-causal-analysis",
    "title": "Multiple regression analysis",
    "section": "MR: Causal analysis",
    "text": "MR: Causal analysis\n\nOne main reason to estimate multiple regressions is to get closer to a causal interpretation.\nBy conditioning on other observable variables, we can get closer to comparing similar objects – “apples to apples” – even in observational data.\nBut getting closer is not the same as getting there.\nIn principle, one could try conditioning on every potential confounder: variables that would affect \\(y\\) and the causal variable \\(x_1\\) at the same time.\nCeteris paribus = conditioning on every such relevant variable. (everything else constant)."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-causal-analysis-1",
    "href": "rm-data/slides/week07.html#mr-causal-analysis-1",
    "title": "Multiple regression analysis",
    "section": "MR: Causal analysis",
    "text": "MR: Causal analysis\n\nIn randomized experiments, we can use causal language: treated and untreated units similar - by random grouping.\nIn observational data, comparisons don’t uncover causal relations.\n\nCautious with language. Avoid use of “effect”, “increase”. But could use “associated with”, “linked to”.\nRegression, even with multiple \\(x\\) is just comparison. Conditional mean."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-causal-analysis-dont-overdo-it",
    "href": "rm-data/slides/week07.html#mr-causal-analysis-dont-overdo-it",
    "title": "Multiple regression analysis",
    "section": "MR: Causal analysis Don’t overdo it",
    "text": "MR: Causal analysis Don’t overdo it\n\nNot all variables should be included as control variables even if correlated both with the causal variable and the dependent variable.\nBad conditioning variables are variables that are correlated both with the causal variable and the dependent variable but are actually part of the causal mechanism.\n\nThis is the reason to exclude them.\n\nExample, should you control for visit to the doctor when estimating the effect of health spending on health?\n\nNo, because visiting the doctor is part of the causal mechanism."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-causal-analysis-2",
    "href": "rm-data/slides/week07.html#mr-causal-analysis-2",
    "title": "Multiple regression analysis",
    "section": "MR: Causal analysis",
    "text": "MR: Causal analysis\n\nA multiple regression on observational data is rarely capable of uncovering a causal relationship.\n\nCannot capture all potential confounder. (Not ceteris paribus)\nPotential Bad conditioning variables (bad controls)\nWe can never really know.\n\nMultiple regression can get us closer to uncovering a causal relationship\n\nCompare units that are the same in many respects - controls"
  },
  {
    "objectID": "rm-data/slides/week07.html#cs-understanding-the-gender-difference-in-earnings-1",
    "href": "rm-data/slides/week07.html#cs-understanding-the-gender-difference-in-earnings-1",
    "title": "Multiple regression analysis",
    "section": "CS: Understanding the gender difference in earnings",
    "text": "CS: Understanding the gender difference in earnings\n\n\n\n\n\n\n\n\n\n\nVariables\nln wage (Model 1)\nln wage (Model 2)\nln wage (Model 3)\nln wage (Model 4)\n\n\n\n\nfemale\n-0.224**\n-0.212**\n-0.151**\n-0.141**\n\n\n\n(0.012)\n(0.012)\n(0.012)\n(0.012)\n\n\nAge and education\n\nYES\nYES\nYES\n\n\nFamily circumstances\n\n\nYES\nYES\n\n\nDemographic background\n\n\n\nYES\n\n\nJob characteristics\n\n\n\nYES\n\n\nUnion member\n\n\n\nYES\n\n\nAge in polynomial\n\n\n\nYES\n\n\nHours in polynomial\n\n\n\nYES\n\n\nObservations\n9,816\n9,816\n9,816\n9,816\n\n\nR-squared\n0.036\n0.043\n0.182\n0.195\n\n\n\nMore and more confounders added"
  },
  {
    "objectID": "rm-data/slides/week07.html#regression-table-detour",
    "href": "rm-data/slides/week07.html#regression-table-detour",
    "title": "Multiple regression analysis",
    "section": "Regression table detour",
    "text": "Regression table detour\n\nRegression table with many \\(x\\) vars is hard to present\nIn presentation, suppress unimportant coefficients\nIn paper, you may present more, but mostly if you want to discuss them or for sanity check\nSanity check: do control variable coefficient make sense by and large?\nCheck \\(N\\) of observations: if the same sample, should be exactly the same.\n\\(R^2\\) is enough, no need for other stuff (unless other methods are used)"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-prediction-and-benchmarking",
    "href": "rm-data/slides/week07.html#mr-prediction-and-benchmarking",
    "title": "Multiple regression analysis",
    "section": "MR: Prediction and benchmarking",
    "text": "MR: Prediction and benchmarking\n\nSecond reason to estimate a multiple regression is to make a prediction\n\nfind the best guess for the dependent variable \\(y_j\\) for a particular observation \\(j\\) \\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots\\]\n\nA \\(\\hat{y} \\ vs \\ y\\) Scatter plot is a good way to visualize the fit of a prediction.\n\nAs well as identifying over or underpredictions.\n\nWe want the regression to produce as good a fit as possible.\n\nA common danger: Overfitting the data: finding patterns in the data that are not true in the population\n\nWe will discuss more about prediction in the next chapters."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-variable-selection",
    "href": "rm-data/slides/week07.html#mr-variable-selection",
    "title": "Multiple regression analysis",
    "section": "MR: Variable selection",
    "text": "MR: Variable selection\n\nHow should one decide which variables to include? and how?\nDepends on the purpose: prediction or causality.\n\nGeneral Advice:\n\nLot of judgment calls: theory, data, and context.\nNon-linear fit, use a non-parametric first and if non-linear, pick a model that is close - quadratic, piecewise spline.\nIf two or many variables strongly correlated, pick one of them.\nKeep it as simple as possible: Parsimony is a virtue."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-variable-selection-for-causal-questions",
    "href": "rm-data/slides/week07.html#mr-variable-selection-for-causal-questions",
    "title": "Multiple regression analysis",
    "section": "MR: Variable selection for causal questions",
    "text": "MR: Variable selection for causal questions\n\nCausal question: \\(x\\) impact on \\(y\\). Having \\(z\\) variables to condition on, to get closer to causality.\nOur aim is to focus on the coefficient on one variable. What matters are the estimated value of the coefficient and its confidence interval. Not prediction.\nKeep \\(z\\) – keep variables that help comparing units\nDrop \\(z\\) if they not matter, or if they are part of the causal mechanism. (affected by \\(x\\))\n\nFunctional form for \\(z\\) matters only for crucial confounders. (linear is fine)\n\nPresent the model you judge is best, and then report a few other solutions – robustness."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-variable-selection-process",
    "href": "rm-data/slides/week07.html#mr-variable-selection-process",
    "title": "Multiple regression analysis",
    "section": "MR: Variable selection – process",
    "text": "MR: Variable selection – process\n\nSelect control variables you want to include\nSelect functional form one by one\nFocus on key variables by domain knowledge (theory), add the rest linearly\nKey issue is sample size\n\nFor 20-40 obs, about 1-2 variables.\nFor 50-100 obs, about 2-4 variables\nFew hundred obs, 5-10 variables could work\nFew thousand obs, few dozen variables, including industry/country/profession etc dummmmies, interactions.\n10-100K obs - many variables, polynomials, interactions"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-variable-selection-for-prediction",
    "href": "rm-data/slides/week07.html#mr-variable-selection-for-prediction",
    "title": "Multiple regression analysis",
    "section": "MR: Variable selection for prediction",
    "text": "MR: Variable selection for prediction\n\nIf Prediction is the goal, keep whatever works\nBalance is needed to ensure it works beyond the data at hand\nOverfitting: building a model that captures some patterns that may fit the data in hand, but does not generalize well.\nFocus on functional form, interactions\nValue simplicity. Easier to explain, more robust.\nFormal way:\n\nBIC and AIC. Similar to R-squared but takes into account number of variables. The smaller, the better\n\nYou may use Adj R2, (although not perfect) to compare models."
  },
  {
    "objectID": "rm-data/slides/week07.html#summary-take-away",
    "href": "rm-data/slides/week07.html#summary-take-away",
    "title": "Multiple regression analysis",
    "section": "Summary take-away",
    "text": "Summary take-away\n\nMultiple regression are linear models with several \\(x\\) variables.\nMay include binary variables and interactions\nMultiple regression can take us closer to a causal interpretation and help make better predictions."
  },
  {
    "objectID": "rm-data/slides/week04.html#generalization",
    "href": "rm-data/slides/week04.html#generalization",
    "title": "Generalizing from Data",
    "section": "Generalization",
    "text": "Generalization\n\nSometimes we analyze a dataset with the goal of learning about patterns in that dataset alone.\n\nIn such cases there is no need to generalize our findings to other datasets.\nExample: We search for a good deal among offers of hotels, all we care about are the observations in our dataset.\n\nOften we analyze a dataset in order to learn about patterns that may be true in other situations.\n\nWe are interested in finding the relationship between our dataset and the situation we care about.\nExample: Will the treatment we are studying work in other settings?"
  },
  {
    "objectID": "rm-data/slides/week04.html#generalization-1",
    "href": "rm-data/slides/week04.html#generalization-1",
    "title": "Generalizing from Data",
    "section": "Generalization",
    "text": "Generalization\n\nGoal: Generalize the results from a single dataset to other situations.\nThe act of generalization is called inference: we infer something from our data about a more general sitatuation.\n\nFor this we want to test hypothesis based on our estimates (evidence)\n\nTwo Things to consider\n\nStatistical inference: the process of using data (in hand) to infer the properties of a population. Identify general pattern.\nExternal validity: the extent to which our data represents the general pattern we care about in other settings."
  },
  {
    "objectID": "rm-data/slides/week04.html#statistical-inference",
    "href": "rm-data/slides/week04.html#statistical-inference",
    "title": "Generalizing from Data",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nThere are several statistical methods to make inference.\nThe general pattern (A model) is an abstract thing that may or may not exist.\nIf we can assume that the general pattern exists, the tools of statistical inference can be very helpful.\n\nIf we find a positive relationship between two variables in our data, we can use statistical inference to say if the same relationship is likely in the population."
  },
  {
    "objectID": "rm-data/slides/week04.html#general-patterns-1-population-and-representative-sample",
    "href": "rm-data/slides/week04.html#general-patterns-1-population-and-representative-sample",
    "title": "Generalizing from Data",
    "section": "General patterns 1: Population and representative sample",
    "text": "General patterns 1: Population and representative sample\n\nThe cleanest example of representative data is a representative sample of a well-defined population.\nA sample is representative of a population if the distribution of all variables is very similar in the sample and the population. \\[f(y,x,z,...)_{sample} \\approx f(y,x,z,...)_{population}\\]\nRandom sampling is the best way to achieve a representative sample."
  },
  {
    "objectID": "rm-data/slides/week04.html#general-patterns-2-no-population-but-general-pattern",
    "href": "rm-data/slides/week04.html#general-patterns-2-no-population-but-general-pattern",
    "title": "Generalizing from Data",
    "section": "General patterns 2: No population but general pattern",
    "text": "General patterns 2: No population but general pattern\n\n“Representation” is less straightforward in other setups.\nThere isn’t a “population” from which a random sample was drawn on purpose.\n\nUsing the past to uncover a pattern of the future. (Time series)\nUse analogy to generalize patterns on Products A into Products B. (requires external Validity)\n\nInstead, we should think of our data as one that represents a general pattern (a model).\n\n\\(X\\beta\\) exists, and each year is a random realization.\n\\(X\\beta\\) exists, and each product is a random version.\n\nYou can think of a “general pattern” as the “true” model dictating the data."
  },
  {
    "objectID": "rm-data/slides/week04.html#external-validity",
    "href": "rm-data/slides/week04.html#external-validity",
    "title": "Generalizing from Data",
    "section": "External validity",
    "text": "External validity\n\nHow likely is that what we learn is relevant other situations we care about?\nAre our findings unique to our data? or can they happen “out there”?\n\nWith external validity, our data can tell what to expect.\nNo external validity: whatever we learn from our data, may turn out to be not relevant at all.\n\nThis has been a problem with RCTs in economics: the results are not always generalizable to other settings."
  },
  {
    "objectID": "rm-data/slides/week04.html#the-process-of-inference",
    "href": "rm-data/slides/week04.html#the-process-of-inference",
    "title": "Generalizing from Data",
    "section": "The process of inference",
    "text": "The process of inference\nThe process of inference:\n\nConsider a statistic we may care about, such as the mean.\nCompute its estimated value from a dataset.\nInfer the value in the population, that our data represents.\n\nIt is good practice to divide the inference problem into two:\n\nUse statistical inference to learn about the population the data represents.\nAssess external validity: Assess how the data in hand represents the population we care about."
  },
  {
    "objectID": "rm-data/slides/week04.html#stock-market-returns-inference",
    "href": "rm-data/slides/week04.html#stock-market-returns-inference",
    "title": "Generalizing from Data",
    "section": "Stock market returns: Inference",
    "text": "Stock market returns: Inference\n\nTask: Assess the likelihood of experiencing a loss of 5% on an investment portfolio from one day to the next\nData: day-to-day returns on the S&P 500, from 25 August 2006 to 26 August 2016: 2,519 days.\nFinding: 0.5% of the days in the dataset have a loss of 5% or more.\nInference problem:\n\nHow can we generalize this finding? What can we infer from this 0.5% chance for the next calendar year?"
  },
  {
    "objectID": "rm-data/slides/week04.html#repeated-samples-1",
    "href": "rm-data/slides/week04.html#repeated-samples-1",
    "title": "Generalizing from Data",
    "section": "Repeated samples",
    "text": "Repeated samples\n\nNormally, There is one sample. But, theoretical framework assumes you could obtain many (repeated) samples. (Frequentist approach)\nThe goal of statistical inference is learning the value of a statistic in the population represented by our data.\n\nBut, each repeated samples, would give a different value of the statistic.\n\nBecause of the different values, the statistic obtained with repeated samples will have a distribution\n\nThis is the sampling distribution.\n\nThe standard deviation of the sampling distribution is what is called the standard error of the statistic (typical error across random samples)."
  },
  {
    "objectID": "rm-data/slides/week04.html#repeated-samples-properties",
    "href": "rm-data/slides/week04.html#repeated-samples-properties",
    "title": "Generalizing from Data",
    "section": "Repeated samples properties",
    "text": "Repeated samples properties\nThe sampling distribution has three important properties:\n\nUnbiasedness: The average of the values in repeated samples is equal to its true value (=the value in the entire population / general pattern).\nAsymptotic normality: The sampling distribution is approximately normal. With large sample size, it is very very close.\nRoot-n convergence: The standard error (the standard deviation of the sampling distribution) is smaller the larger the samples, with a proportionality factor of the square root of the sample size."
  },
  {
    "objectID": "rm-data/slides/week04.html#repeated-samples-2",
    "href": "rm-data/slides/week04.html#repeated-samples-2",
    "title": "Generalizing from Data",
    "section": "Repeated samples",
    "text": "Repeated samples\n\nEasier concept:\n\nWhen data is sample from a well-defined population - many other samples could have turned out instead of what we have.\nExample: Mexican firms - random sample - population of firms.\n\nHarder concept:\n\nSome times there is no clear definition of population. (but there is a model).\nData of returns on an investment portfolio is as a particular realization of the history of returns that could have turned out differently.\n\nMultiverse: many possible histories of returns, we see only one."
  },
  {
    "objectID": "rm-data/slides/week04.html#case-study",
    "href": "rm-data/slides/week04.html#case-study",
    "title": "Generalizing from Data",
    "section": "Case study",
    "text": "Case study\nStock market returns: A simulation\n\nWe can not rerun history many many times…\nSo we will run a Simulation exercise - to better understand how repeated samples work.\nSuppose the 11-year dataset is the population - the fraction of days with 5%+ losses is 0.5% in the entire 11 years’ data. That’s the true value.\nWe assume we have only 500 days of daily returns in our dataset.\nTask: estimate the true value of the fraction in the 11-year period from the data we have using a simulation exercise."
  },
  {
    "objectID": "rm-data/slides/week04.html#stock-market-returns-a-simulation-1",
    "href": "rm-data/slides/week04.html#stock-market-returns-a-simulation-1",
    "title": "Generalizing from Data",
    "section": "Stock market returns: A simulation",
    "text": "Stock market returns: A simulation\n\nuse data_slides/sp500.dta, clear\ngen return = (value - value[_n-1])/value[_n-1]\ngen lost5 = return &lt; -0.05\nset seed 1\n\n** Simulation\ngen mn_lost5=.\nforvalues i = 1/1000 {\n    preserve\n      qui:sample 500, count\n      sum lost5 , meanonly\n    restore\n    qui:replace mn_lost5 = r(mean) in `i'\n}"
  },
  {
    "objectID": "rm-data/slides/week04.html#stock-market-returns-a-simulation-2",
    "href": "rm-data/slides/week04.html#stock-market-returns-a-simulation-2",
    "title": "Generalizing from Data",
    "section": "Stock market returns: A simulation",
    "text": "Stock market returns: A simulation\n\n\n(start=0, width=.002)"
  },
  {
    "objectID": "rm-data/slides/week04.html#the-standard-error-and-the-confidence-interval",
    "href": "rm-data/slides/week04.html#the-standard-error-and-the-confidence-interval",
    "title": "Generalizing from Data",
    "section": "The standard error and the confidence interval",
    "text": "The standard error and the confidence interval\n\nConfidence interval (CI) is a measure of statistical inference that allows some margin of error.\nThe CI defines a range where we can expect the true value in the population, with a probability.\nProbability tells how likely it is that the true value is in that range, if we were to draw many repeated samples."
  },
  {
    "objectID": "rm-data/slides/week04.html#if-ex-and-sex-are-known",
    "href": "rm-data/slides/week04.html#if-ex-and-sex-are-known",
    "title": "Generalizing from Data",
    "section": "If E(X) and SE(X) are known",
    "text": "If E(X) and SE(X) are known\n\nIf we know the true value of a statistic and its standard error, then we can calculate the CI.\n\nThe CI is centered around the true value of the statistic.\nThis CI is the range of values that we can expect the sample statistic to fall in, with a certain probability.\nfor example, 95% CI: the sample statistic will fall within the CI in 95% of the repeated samples.\nBut in 5% of the cases, the sample statistic will fall outside the CI."
  },
  {
    "objectID": "rm-data/slides/week04.html#if-ex-and-sex-are-not-known",
    "href": "rm-data/slides/week04.html#if-ex-and-sex-are-not-known",
    "title": "Generalizing from Data",
    "section": "If E(X) and SE(X) are not known",
    "text": "If E(X) and SE(X) are not known\n\nWhen we say “95% CI”, we mean that if we were to draw many repeated samples, the true value would fall within the CI in 95% of the cases.\nHowever, it also means that in 5% of the cases, the true value would fall outside the CI.\n\nThis means, some times (5% of the cases) we will be wrong."
  },
  {
    "objectID": "rm-data/slides/week04.html#confidence-interval",
    "href": "rm-data/slides/week04.html#confidence-interval",
    "title": "Generalizing from Data",
    "section": "Confidence interval",
    "text": "Confidence interval\n\nCI is almost always symmetric around the estimated value of the statistic in our dataset. (if we assume normality of the sampling distribution)\nHow to calculate the CI?\n\nGet estimated value.\nDefine probability, confidence level (Say 95%).\nCalculate CI with the use of SE. \\[95\\% CI= \\hat\\mu \\pm 1.96SE\\]\n\nUnder Normality, 90% CI is the ±1.645SE interval, the 99 % CI is the ±2.576SE.\nBut we commonly use the rule of 2: ±2SE."
  },
  {
    "objectID": "rm-data/slides/week04.html#calculating-the-standard-error",
    "href": "rm-data/slides/week04.html#calculating-the-standard-error",
    "title": "Generalizing from Data",
    "section": "Calculating the standard error",
    "text": "Calculating the standard error\n\nEstimating the sample mean \\(\\bar{x}\\) is easy. But how do we estimate the standard error?\n\nIn reality, we don’t get to observe the sampling distribution. Instead, we observe a single dataset.\nThat dataset is one of many potential samples that could have been drawn from the population.\n\nGood news: We can get a very good idea of how the sampling distribution would look like - good estimate of the standard error - even from a single sample.\nGetting SE – Option 1: Use a formula. \\(\\leftarrow\\) Theoretical approach.\nGetting SE – Option 2: Simulate \\(\\leftarrow\\), The bootstrap method."
  },
  {
    "objectID": "rm-data/slides/week04.html#calculating-the-standard-error-1",
    "href": "rm-data/slides/week04.html#calculating-the-standard-error-1",
    "title": "Generalizing from Data",
    "section": "Calculating the standard error",
    "text": "Calculating the standard error\nConsider the statistic of the sample mean.\n\nAssume the values of \\(x\\) are independent across observations in the dataset.\n\\(\\bar{x}\\) is the estimate of the true mean value of \\(x\\) in the population.\nAssume sampling distribution is approximately normal, with the true value as its mean.\n\nThe standard error formula for the estimated \\(\\bar{x}\\) is \\[SE (\\bar{x}) = \\frac{1}{\\sqrt{n}} Std[x]\\]\nwhere \\(Std[x]\\) is the standard deviation of the variable \\(x\\) in the data and \\(n\\) is the number of observations in the data."
  },
  {
    "objectID": "rm-data/slides/week04.html#the-standard-error-formula",
    "href": "rm-data/slides/week04.html#the-standard-error-formula",
    "title": "Generalizing from Data",
    "section": "The standard error formula",
    "text": "The standard error formula\n\nThe standard error is larger…\n\nthe larger the standard deviation of the variable.\nthe smaller the sample\n\nThe larger the standard error, the wider the confidence interval, and the less precise the estimate (wider CI)."
  },
  {
    "objectID": "rm-data/slides/week04.html#external-validity-2",
    "href": "rm-data/slides/week04.html#external-validity-2",
    "title": "Generalizing from Data",
    "section": "External validity",
    "text": "External validity\n\nIn statistical inference the CI represents the uncertainty about the true value of the statistic in the population that our data represents.\nBut What is the population, we care about? How close is our data to this?\nExternal validity: Can we generalize the pattern we found in our data to other situations?\nHigh external validity: if our data is close to the population.\nExternal validity is as important as statistical inference, but it is not a statistical question."
  },
  {
    "objectID": "rm-data/slides/week04.html#external-validity-3",
    "href": "rm-data/slides/week04.html#external-validity-3",
    "title": "Generalizing from Data",
    "section": "External validity",
    "text": "External validity\n\nThe three most important challenges to external validity are:\n\nTime: we have data on the past, but we care about the future.\nSpace: our data is on one country, but interested how a pattern would hold elsewhere in the world.\nSub-groups: our data is on 25-30 year old people. Would a pattern hold on younger / older people?"
  },
  {
    "objectID": "rm-data/slides/week04.html#external-validity-portafolio-example",
    "href": "rm-data/slides/week04.html#external-validity-portafolio-example",
    "title": "Generalizing from Data",
    "section": "External validity: Portafolio Example",
    "text": "External validity: Portafolio Example\n\nDaily 5%+ loss probability with a 95% CI [0.2, 0.8] in our sample. This captures uncertainty.\nExternal Validity: Would this data be representative of the events of one year in the future?\n\nProbably not, because the future is uncertain.\nOur data: 2006-2016 dataset includes the financial crisis and great recession of 2008-2009. uncertain if the future will have similar events.\n\nHence, the real CI is likely to be substantially wider."
  },
  {
    "objectID": "rm-data/slides/week04.html#external-validity-managers-example",
    "href": "rm-data/slides/week04.html#external-validity-managers-example",
    "title": "Generalizing from Data",
    "section": "External validity: Managers Example",
    "text": "External validity: Managers Example\n\nManager and firm size evidence in Mexico.\nHow to think about external validity?\n\nWould the same patterns hold in other countries? Develped countries? Emerging markets?\nWould the same patterns hold in other sectors? Other industries?\n\nOnly Mexico? only firms of a certain size?"
  },
  {
    "objectID": "rm-data/slides/week04.html#the-bootstrap-1",
    "href": "rm-data/slides/week04.html#the-bootstrap-1",
    "title": "Generalizing from Data",
    "section": "The bootstrap",
    "text": "The bootstrap\n\nBootstrap is a method to create synthetic samples that are similar but different.\nAn method that is very useful in general.\n\nThe method you use, when you don’t know…\n\nIt is essential for many advanced statistics applications such as machine learning.\nThe bootstrap is a method to estimate uncertainty in a statistic, that uses the data itself.\n\n\nto lift oneself by one’s bootstraps"
  },
  {
    "objectID": "rm-data/slides/week04.html#the-bootstrap-2",
    "href": "rm-data/slides/week04.html#the-bootstrap-2",
    "title": "Generalizing from Data",
    "section": "The bootstrap",
    "text": "The bootstrap\n\nThe bootstrap method takes the original dataset and draws many repeated samples (with replacement) of the size of that dataset.\nSay you have a dataset of 10 observations, named 1, 2, 3, …, 10.\n\nBootstrap sample 1: 2, 5, 5, 7, 8, 9, 9, 9, 10, 10.\nBootstrap sample 2: 1, 1, 2, 3, 4, 5, 6, 7, 8 , 9.\nAnd so on, repeated many times.\n\nEach new sample is called a bootstrap sample."
  },
  {
    "objectID": "rm-data/slides/week04.html#the-bootstrap-3",
    "href": "rm-data/slides/week04.html#the-bootstrap-3",
    "title": "Generalizing from Data",
    "section": "The bootstrap",
    "text": "The bootstrap\n\n\n\na Bsample is (almost) always the same size as the original dataset.\nSome Data is repeated, some is left out.\nTypically, we require between 500-10,000 bootstrap samples. (Stata’s default is 50)\nComputationally intensive, but feasible"
  },
  {
    "objectID": "rm-data/slides/week04.html#the-bootstrap-method-how-does-it-work",
    "href": "rm-data/slides/week04.html#the-bootstrap-method-how-does-it-work",
    "title": "Generalizing from Data",
    "section": "The bootstrap method: How does it work?",
    "text": "The bootstrap method: How does it work?\n\nFor each BSample, you estimate the statistic of interest. (e.g. mean)\nThe distribution of the statistic across these repeated bootstrap samples is a good approximation to the sampling distribution.\nIn this case, the bootstrap Standard Error is the standard deviation of the statistic across the bootstrap samples.\nAlso, the 95% CI is the 2.5th and 97.5th percentiles of the distribution of the statistic across the bootstrap samples. Or you can use the estimated SE."
  },
  {
    "objectID": "rm-data/slides/week04.html#stock-market-returns-the-bootstrap-standard-error",
    "href": "rm-data/slides/week04.html#stock-market-returns-the-bootstrap-standard-error",
    "title": "Generalizing from Data",
    "section": "Stock market returns: The Bootstrap standard error",
    "text": "Stock market returns: The Bootstrap standard error\nStata Corner\nBootstraping in Stata can be easy. Most commands have a built-in bootstrap option. Otherwise, we can program it!\n\ndisplay \"Bootstrap\"\nbootstrap mean=r(mean), nowarn reps(1000) seed(1) dots(100): sum lost5, meanonly\nest sto m1\ndisplay \"Formula\"\nmean lost5\nest sto m2"
  },
  {
    "objectID": "rm-data/slides/week04.html#stock-market-returns-the-bootstrap-standard-error-1",
    "href": "rm-data/slides/week04.html#stock-market-returns-the-bootstrap-standard-error-1",
    "title": "Generalizing from Data",
    "section": "Stock market returns: The Bootstrap standard error",
    "text": "Stock market returns: The Bootstrap standard error\nStata Corner\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean\n\n\n\n\n\nFormula\n0.0052\n[0.0023,0.0080]\n\n\n\n\nBootstrap\n\n\n0.0052\n[0.0024,0.0080]\n\n\nN\n2519\n\n2519"
  },
  {
    "objectID": "rm-data/slides/week04.html#generalization---summary",
    "href": "rm-data/slides/week04.html#generalization---summary",
    "title": "Generalizing from Data",
    "section": "Generalization - Summary",
    "text": "Generalization - Summary\n\nGeneralization is a key task - finding beyond the actual dataset.\nThis process is made up of discussing statistical inference and external validity.\nStatistical inference generalizes from our dataset to the population using a variety of statistical tools.\nExternal validity is the concept of discussing beyond the population for a general pattern we care about; an important but typically somewhat speculative process."
  },
  {
    "objectID": "rm-data/slides/week04.html#motivation",
    "href": "rm-data/slides/week04.html#motivation",
    "title": "Generalizing from Data",
    "section": "Motivation",
    "text": "Motivation\n\nThe internet allowed the emergence of specialized online retailers while brick-and-mortar shops also sell goods on the main street. How to measure price inflation in the age of these options?\nTo help answer this, we can collect and compare online and offline prices of the same products and test if they are the same."
  },
  {
    "objectID": "rm-data/slides/week04.html#the-logic-of-hypothesis-testing",
    "href": "rm-data/slides/week04.html#the-logic-of-hypothesis-testing",
    "title": "Generalizing from Data",
    "section": "The logic of hypothesis testing",
    "text": "The logic of hypothesis testing\n\nA hypothesis is a statement about the population parameter, of which we are not sure if true or not.\nHypothesis testing = analyze our data to make a decision on the hypothesis\nReject the hypothesis if there is enough evidence against it.\nDon’t reject it if there isn’t enough evidence against it.\n\nBut NEVER accept it as true.\n\nImportant asymmetry here: rejecting a hypothesis is a more conclusive decision than not rejecting it."
  },
  {
    "objectID": "rm-data/slides/week04.html#inference",
    "href": "rm-data/slides/week04.html#inference",
    "title": "Generalizing from Data",
    "section": "Inference",
    "text": "Inference\n\nTesting a hypothesis: making inference with a focus on a specific statement.\n\nHypothesis: It is cheaper to buy online than offline.\n\nCan answer questions about the population represented by our data.\nBut, It is an inference: have to assess external validity."
  },
  {
    "objectID": "rm-data/slides/week04.html#the-setup",
    "href": "rm-data/slides/week04.html#the-setup",
    "title": "Generalizing from Data",
    "section": "The setup",
    "text": "The setup\n\nDefine the the statistic we want to test, \\(s\\) (e.g. mean).\nWe are interested in the true value of \\(s\\), \\(s_{true}\\).\n\nThis implies the true value in the population.\n\nThe value of the statistic in our data is its estimated value, denoted by a hat on top \\(\\hat{s}\\)."
  },
  {
    "objectID": "rm-data/slides/week04.html#hypothesis-testing-h0-vs-ha",
    "href": "rm-data/slides/week04.html#hypothesis-testing-h0-vs-ha",
    "title": "Generalizing from Data",
    "section": "Hypothesis testing: H0 vs HA",
    "text": "Hypothesis testing: H0 vs HA\n\nNeed to formally state the question as two competing hypotheses of which only one can be true:\n\na null hypothesis \\(H_0\\) and an alternative hypothesis \\(H_a\\).\n\nThey are formulated in terms of the unknown true value of the statistic. (we now the sample value)\nTogether they cover all possibilities.\n\n\n\\(H_0\\): Online and offline prices are the same. \\(H_a\\): Online and offline prices are different."
  },
  {
    "objectID": "rm-data/slides/week04.html#the-null-is-protected",
    "href": "rm-data/slides/week04.html#the-null-is-protected",
    "title": "Generalizing from Data",
    "section": "The Null is protected",
    "text": "The Null is protected\n\nInnocent (H0) until proven guilty (Ha)\n\n\nTesting a hypothesis \\(H_0\\)= see if there is enough evidence in our data to reject the null.\nThe null is protected: We start assuming the Null is true\n\nIf we have strong evidence against it, we reject it\nIf not, we don’t reject it."
  },
  {
    "objectID": "rm-data/slides/week04.html#types-of-testing-h_0-vs-h_a",
    "href": "rm-data/slides/week04.html#types-of-testing-h_0-vs-h_a",
    "title": "Generalizing from Data",
    "section": "Types of testing: \\(H_0\\) vs \\(H_a\\)",
    "text": "Types of testing: \\(H_0\\) vs \\(H_a\\)\n\nThere are two types of Hypothesis:\n\nTwo-sided alternative: We are interested if the true value of the statistic is different from the hypothesized value.\n\n\n\\[H_0: \\theta = 42 \\ \\ vs  \\ \\ H_A: \\theta \\neq 42\\]\n\nOne-sided alternative: We are interested if the true value of the statistic is greater or smaller than the hypothesized value. \\[H_0: \\theta \\leq 42 \\ \\ vs  \\ \\ H_A: \\theta &gt; 42\\]"
  },
  {
    "objectID": "rm-data/slides/week04.html#the-logic-of-hypothesis-testing-1",
    "href": "rm-data/slides/week04.html#the-logic-of-hypothesis-testing-1",
    "title": "Generalizing from Data",
    "section": "The logic of hypothesis testing",
    "text": "The logic of hypothesis testing\n\n\\(H_A\\) is (often) what I want to prove\n\\(H_0\\) is what I wanna reject so that we can prove \\(H_A\\)\n\\(H_0\\) is not rejected\n\nnot enough evidence or\ntrue (ie \\(H_A\\) is false)\n\nI can never say \\(H_0\\) is true."
  },
  {
    "objectID": "rm-data/slides/week04.html#case-study---online-vs-offline-prices",
    "href": "rm-data/slides/week04.html#case-study---online-vs-offline-prices",
    "title": "Generalizing from Data",
    "section": "Case Study - online vs offline prices",
    "text": "Case Study - online vs offline prices\n\nQuestion: Do the online and offline prices of the same products differ?\nThis data includes 10 to 50 products in each retail store included in the survey (the largest retailers in the U.S. that sell their products both online and offline).\nThe products were selected by the data collectors in offline stores, and they were matched to the same products the same stores sold online.\nThe statistic of interest is the difference in average prices."
  },
  {
    "objectID": "rm-data/slides/week04.html#section",
    "href": "rm-data/slides/week04.html#section",
    "title": "Generalizing from Data",
    "section": "",
    "text": "Each product \\(i\\) has an off-line and on-line price.\nThe statistic with \\(n\\) observations (products) in the data, is: \\[s = \\bar{p}_\\text{diff} = \\frac{1}{n} \\sum_{i=1}^n (p_{i,\\text{online}} - p_{i,\\text{offline}})\\]\nThe average of the price differences is equal to the difference of the average prices \\[\\frac{1}{n} \\sum_{i=1}^n (p_{i,\\text{online}} - p_{i,\\text{offline}}) = \\frac{1}{n} \\sum_{i=1}^n p_{i,\\text{online}} - \\frac{1}{n} \\sum_{i=1}^n p_{i,\\text{offline}}\\]"
  },
  {
    "objectID": "rm-data/slides/week04.html#section-1",
    "href": "rm-data/slides/week04.html#section-1",
    "title": "Generalizing from Data",
    "section": "",
    "text": "Descriptive statistics of the difference:\n\nThe mean difference is USD -0.05: online prices are, on average, 5 cents lower in this dataset.\nSpread around this average: Std: USD 10\nExtreme values matter: Range: -380 — USD +415.\nOf the 6439 products, 64% have the same online and offline price, for 87%, the difference within ±1 dollars."
  },
  {
    "objectID": "rm-data/slides/week04.html#case-study---external-validity",
    "href": "rm-data/slides/week04.html#case-study---external-validity",
    "title": "Generalizing from Data",
    "section": "Case Study - External validity:",
    "text": "Case Study - External validity:\n\nThe products in the data may not represent all products sold at these stores.\n\nBias? Were the products selected randomly?\n\nStrictly: The findings refer to products sond online-offline by large retail stores. And those selected by the people collecing the data.\nMore broadly: price differences among all products in the U.S. sold both online and offline by the same retailers.\n\nMay not be representative of smaller retailers"
  },
  {
    "objectID": "rm-data/slides/week04.html#t-test",
    "href": "rm-data/slides/week04.html#t-test",
    "title": "Generalizing from Data",
    "section": "T-test",
    "text": "T-test\n\nThe t-test is the testing procedure based on the t-statistic\nWe compare the estimated value of the statistic \\(\\hat{s}\\) to zero. (\\(H_0\\))\nEvidence to reject the null is based on difference between \\(\\hat{s}\\) and zero.\n\nReject the null if difference large (its un unlikely to be zero).\nNot reject the null if the difference is small ( not enough evidence against it)."
  },
  {
    "objectID": "rm-data/slides/week04.html#t-test-1",
    "href": "rm-data/slides/week04.html#t-test-1",
    "title": "Generalizing from Data",
    "section": "T-test",
    "text": "T-test\n\nThe test statistic is a statistic that measures the (standardized) distance of the estimated value from what the true value would be if \\(H_0\\) was true.\nUses estimated value of \\(s\\) (\\(\\hat{s}\\)) and the standard error of estimate (SE (\\(\\hat{s}\\))).\nConsider \\(H_0: s_\\text{true} = 0, H_A: s_\\text{true} \\neq 0\\). The t-statistic for this hypotheses is: \\[t = \\frac{\\hat{s}}{\\text{SE}(\\hat{s})}\\]"
  },
  {
    "objectID": "rm-data/slides/week04.html#t-test-2",
    "href": "rm-data/slides/week04.html#t-test-2",
    "title": "Generalizing from Data",
    "section": "T-test",
    "text": "T-test\nWhen \\(\\hat{s}\\) is the average of a variable \\(x\\), the t-statistic is simply \\[t = \\frac{\\bar{x}}{\\text{SE}(\\bar{x})}\\]\nWhen \\(\\hat{s}\\) is the average of a variable \\(x\\) minus a number, the t-statistic is \\[t = \\frac{\\bar{x} - \\text{number}}{\\text{SE}(\\bar{x})}\\]\nWhen \\(\\hat{s}\\) is the difference between two averages, say, \\(\\bar{x}_A\\) and \\(\\bar{x}_B\\), the t-statistic is \\[t = \\frac{\\bar{x}_A - \\bar{x}_B}{\\text{SE}(\\bar{x}_A - \\bar{x}_B)}\\]"
  },
  {
    "objectID": "rm-data/slides/week04.html#t-test-3",
    "href": "rm-data/slides/week04.html#t-test-3",
    "title": "Generalizing from Data",
    "section": "T-test",
    "text": "T-test\n\nWhile we can use SE to calculate the t-statistic, SE may be more difficult to calculate in some situations.\n\nDifferent samples, different SE, etc\n\nSome times you may want to use Bootstrap to calculate SE.\nStata Corner: ttest command in Stata calculates the t-statistic for a difference in means."
  },
  {
    "objectID": "rm-data/slides/week04.html#making-a-decision",
    "href": "rm-data/slides/week04.html#making-a-decision",
    "title": "Generalizing from Data",
    "section": "Making a decision",
    "text": "Making a decision\n\nOnce you obtain your t-statistic (or other relevant statistic), you need to make a decision regarding the null hypothesis.\nIn hypothesis testing the decision is based on a clear rule specified in advance. A critical value.\n\nThis makes the decision straightforward + transparent\nHelps avoid personal bias:put more weight on the evidence that supports our prejudices."
  },
  {
    "objectID": "rm-data/slides/week04.html#making-a-decision-decision-rulecritical-value",
    "href": "rm-data/slides/week04.html#making-a-decision-decision-rulecritical-value",
    "title": "Generalizing from Data",
    "section": "Making a decision: decision rule/Critical value",
    "text": "Making a decision: decision rule/Critical value\n\nThe Critical value is a threshold that determines if the test statistic is large enough to reject the null.\n\nRecall, we start assuming the null is true.\nThen we need to test if our evidence (estimates) is different enough from the null to reject it.\nThe critical value is what determines how different is different enough.\n\nNull rejected if the test statistic is larger than the critical value"
  },
  {
    "objectID": "rm-data/slides/week04.html#making-a-decision-possible-outcomes",
    "href": "rm-data/slides/week04.html#making-a-decision-possible-outcomes",
    "title": "Generalizing from Data",
    "section": "Making a decision: Possible outcomes",
    "text": "Making a decision: Possible outcomes\n\n\n\nSome times we are right:\n\nReject the null when it is false,\nor do not reject the null when it is true.\n\nBut, We can be wrong:\n\nReject the null even though it is true\nor do not reject the null even though is false.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\n\nDo not reject \\(H_0\\)\nCorrect\nFalse negative (Type II)\n\n\nReject \\(H_0\\)\nFalse positive (TYPE I)\nCorrect"
  },
  {
    "objectID": "rm-data/slides/week04.html#making-a-decision-error-of-type-i-and-ii",
    "href": "rm-data/slides/week04.html#making-a-decision-error-of-type-i-and-ii",
    "title": "Generalizing from Data",
    "section": "Making a decision: Error of type I and II",
    "text": "Making a decision: Error of type I and II\n\nBoth types of errors are wrong but\nDuring Testing the null is protected: we only reject it if there is enough evidence against it.\nThe background assumption\n\nwrongly rejecting the null (a false positive) is a bigger mistake than wrongly accepting it (a false negative).\n\nDecision rule (critical value) is chosen in a way that makes false positives rare."
  },
  {
    "objectID": "rm-data/slides/week04.html#making-a-decision-critical-values",
    "href": "rm-data/slides/week04.html#making-a-decision-critical-values",
    "title": "Generalizing from Data",
    "section": "Making a decision: Critical values",
    "text": "Making a decision: Critical values\n\nA commonly applied critical value for a t-statistic is ±2 (or 1.96), a 95% confidence level, or a 5% level of significance (alpha).\nOther critical values can be set: 10% (1.65), 1% (2.58), etc.\nThat choice of 5% means that we tolerate a 5% chance for being wrong when rejecting the null (1/20)."
  },
  {
    "objectID": "rm-data/slides/week04.html#making-a-decision-in-a-picture",
    "href": "rm-data/slides/week04.html#making-a-decision-in-a-picture",
    "title": "Generalizing from Data",
    "section": "Making a decision: In a picture",
    "text": "Making a decision: In a picture"
  },
  {
    "objectID": "rm-data/slides/week04.html#false-negative-fn-and-false-positive-fp",
    "href": "rm-data/slides/week04.html#false-negative-fn-and-false-positive-fp",
    "title": "Generalizing from Data",
    "section": "False negative (FN) and False positive (FP)",
    "text": "False negative (FN) and False positive (FP)\n\n\n\nFixing the chance of FP affects the chance of FN at the same time.\nA FN arises when the t-statistic is within the critical values and we don’t reject the null even though the null is not true.\nThis can happen if Sample is small or The difference between true value and null is small"
  },
  {
    "objectID": "rm-data/slides/week04.html#size-and-power-of-the-test",
    "href": "rm-data/slides/week04.html#size-and-power-of-the-test",
    "title": "Generalizing from Data",
    "section": "Size and power of the test",
    "text": "Size and power of the test\nUnder the null:\n\nSize of the test: the probability of committing a false positive.\nLevel of significance: The maximum probability of false positives we tolerate.\n\nUnder the alternative:\n\nPower of the test: the probability of avoiding a false negative\nHighpower is more likely if:\n\nThe sample size is large\nThe null is far from the true value\nThe standard error is small"
  },
  {
    "objectID": "rm-data/slides/week04.html#recap",
    "href": "rm-data/slides/week04.html#recap",
    "title": "Generalizing from Data",
    "section": "Recap",
    "text": "Recap\n\nIn hypothesis testing we make decisions by a rule\n\nA false positive: decision to reject the null when it is true.\nA false negative: decision not to reject the nullwhen it is false.\n\nThe level of significance is the maximum probability of a false positive that we tolerate (\\(\\alpha\\)=5%).\nThe power of the test is the probability of avoiding a false negative.\n\nIn statistical testing we fix the level of significance of the test to be small (5%, 1%) and hope for high power (based on design).\n\nTests with more observations have more power in general."
  },
  {
    "objectID": "rm-data/slides/week04.html#the-p-value-1",
    "href": "rm-data/slides/week04.html#the-p-value-1",
    "title": "Generalizing from Data",
    "section": "The p-value",
    "text": "The p-value\n\nThe p-values are an alternative approach to do hypothesis testing.\n\nBefore we choose a critical value for a given “significance level” (5%, 1%, etc).\nThis approach suggests using the model significance.\n\nThe smallest significance level at which we can reject \\(H_0\\) in the data\nor largest probability of a false positive that we can tolerate.\n\n\nCalculatiion Will depend on the test statistic and sampling distribution.\nRemember, you can never be certain! (P is never zero)"
  },
  {
    "objectID": "rm-data/slides/week04.html#what-p-value-to-pick",
    "href": "rm-data/slides/week04.html#what-p-value-to-pick",
    "title": "Generalizing from Data",
    "section": "What p-value to pick?",
    "text": "What p-value to pick?\n\np-value is about a trade-off. Large (10-15%) or small (1%) depends on scenarios\nGuilty beyond reasonable doubt? (life or death scenario)\n\nPick a conservative value, like 1% or lower\n\nProof of concept? (a new idea, a new product)\n\nIt’s great if it works at 5%, but even 10-15% means it’s much more likely to be true"
  },
  {
    "objectID": "rm-data/slides/week04.html#case-study---comparing-online-and-offline-prices-testing-hypotheses",
    "href": "rm-data/slides/week04.html#case-study---comparing-online-and-offline-prices-testing-hypotheses",
    "title": "Generalizing from Data",
    "section": "Case Study - Comparing online and offline prices: Testing hypotheses",
    "text": "Case Study - Comparing online and offline prices: Testing hypotheses\n\nLet’s fix the level of significance at 5%.\n\nThe value of the statistic in the dataset is -0.054. Its standard error is 0.124.\nThe t-statistic is 0.44. This is well within ±2.\n\nDon’t reject the null hypothesis of zero difference.\nThe p-value of the test is 0.66.\nSo we don’t reject the null\nWe have not “proven” that online and offline prices are the same, but we have not found evidence that they are different."
  },
  {
    "objectID": "rm-data/slides/week04.html#multiple-testing-motivation",
    "href": "rm-data/slides/week04.html#multiple-testing-motivation",
    "title": "Generalizing from Data",
    "section": "Multiple testing: motivation",
    "text": "Multiple testing: motivation\n\nMedical dataset: data on 400 patients\nA particular heart disease binary variable and 100 feature of life style (sport, eating, health background, socio-economic factors)\nLook for a pattern – is the heart disease equally likely for poor vs rich, take vitamins vs not, etc.\nYou test one-by-one\nYou find that for half a dozen factors, there is a difference\nis there any problem with this procedure?"
  },
  {
    "objectID": "rm-data/slides/week04.html#multiple-testing",
    "href": "rm-data/slides/week04.html#multiple-testing",
    "title": "Generalizing from Data",
    "section": "Multiple testing",
    "text": "Multiple testing\n\nThe pre-set level of significance / p-value are defined for a single test\nbut, In many cases, you will consider doing many many tests.\n\nDifferent measures (mean, median, range, etc)\nDifferent products, retailers, countries\nDifferent measures of management quality\n\nFor multiple tests, you cannot use the same approach as for a single one.\nYou need to be even more conservative in rejecting the null."
  },
  {
    "objectID": "rm-data/slides/week04.html#multiple-testing-example",
    "href": "rm-data/slides/week04.html#multiple-testing-example",
    "title": "Generalizing from Data",
    "section": "Multiple testing: Example",
    "text": "Multiple testing: Example\n\nConsider 100 tests. The Nulls are true for all tests.\nSet \\(\\alpha\\)=5% for each test.\nIn the data, even if the null is true, you will reject 5% of the time. (false positives)\nHowever, if you do “use the evidence” from all tests, it would seem that the null is false in 99.4% of the cases. (by chance)\n\nThis is p-hacking. Choosing what works!"
  },
  {
    "objectID": "rm-data/slides/week04.html#multiple-testing-example-1",
    "href": "rm-data/slides/week04.html#multiple-testing-example-1",
    "title": "Generalizing from Data",
    "section": "Multiple testing: Example",
    "text": "Multiple testing: Example\n\nThere are various ways to deal with probabilities of false positives when testing multiple hypotheses.\n\nOften complicated.\n\nPossible Solution: If you have a few dozens of cases, just use a strict criteria (such as 0.1-0.5% instead than 1-5%) for rejecting null hypotheses.\nA very strict such adjustment is the Bonferroni correction that suggests dividing the single hypothesis value by the number of hypotheses.\nOther methods exists, but are similar in spirit.\nRisk: by being more conservative, you are more likely to obtain false negatives."
  },
  {
    "objectID": "rm-data/slides/week04.html#summary",
    "href": "rm-data/slides/week04.html#summary",
    "title": "Generalizing from Data",
    "section": "Summary",
    "text": "Summary\nTesting in statistics means making a decision about the value of a statistic in the general pattern represented by the data.\n\nHypothesis starts with explicitly stating \\(H_0\\) and \\(H_A\\).\nA statistical test rejects \\(H_0\\) if there is enough evidence against it; otherwise it does not reject it.\nTesting multiple hypotheses at the same time is a tricky business; it pays to be very conservative with rejecting the null."
  },
  {
    "objectID": "rm-data/slides/week02.html#motivation",
    "href": "rm-data/slides/week02.html#motivation",
    "title": "Origins of Data, and Data Preparation",
    "section": "Motivation",
    "text": "Motivation\n\nSuppose, you want to understand the extent and patterns of differences in online and offline prices. How would you go about collecting data?\n\nA super project, the Billion Prices Project at MIT did a variety of data collection approaches such as crowd-sourcing platforms, mobile phone apps and web scraping methods.\n\nInterested in understanding more about management practices?\n\nThe World Management Survey is a major effort by academics to survey practices around the world - asking the same questions in many countries the same way."
  },
  {
    "objectID": "rm-data/slides/week02.html#what-is-data-0s-and-1s",
    "href": "rm-data/slides/week02.html#what-is-data-0s-and-1s",
    "title": "Origins of Data, and Data Preparation",
    "section": "What is data ? 0s and 1s…",
    "text": "What is data ? 0s and 1s…\n\nData is a collection of numbers, characters, images, or other formats.\nThey provide information about something. (Prices? Management practices? Hotel characteristics? etc.)\nOf course, depending on how the data was collected, and what structure it has, it can be more or less useful for answering a particular question."
  },
  {
    "objectID": "rm-data/slides/week02.html#what-is-data-1",
    "href": "rm-data/slides/week02.html#what-is-data-1",
    "title": "Origins of Data, and Data Preparation",
    "section": "What is data ?",
    "text": "What is data ?\nAs Economist, we are more familiar with a specific data structure:\n\nData is most straightforward to analyze if it is in a data table form (2D Matrix form):\n\nA single file with rows and columns.\nEach row is an observation, and each column is a variable.\n\nHow do you find it in the real world?1\n\nStorage: Comma separated values .csv (.txt) is simplest, but other formats are possible\nStata (.dta), SPSS (.sav), R (.rda), Python (.pkl), etc.\n\nA Dataset is a collection of data tables that may be related to each other."
  },
  {
    "objectID": "rm-data/slides/week02.html#data-structures",
    "href": "rm-data/slides/week02.html#data-structures",
    "title": "Origins of Data, and Data Preparation",
    "section": "Data structures",
    "text": "Data structures\nAside from “format”, data can be structured in different ways:\n\nCross-sectional (xsec) data have information on many units observed at the same time\nTime series (tseries) data have information on a single unit observed many times\nMulti-dimensional (panel?) data have multiple dimensions (the observations)"
  },
  {
    "objectID": "rm-data/slides/week02.html#data-structures-panel-xt-data",
    "href": "rm-data/slides/week02.html#data-structures-panel-xt-data",
    "title": "Origins of Data, and Data Preparation",
    "section": "Data structures: Panel (xt) data",
    "text": "Data structures: Panel (xt) data\nMulti-dimensional: Panel data is of particular interest in economics:\n\nA common type of panel data has many units, each observed multiple times\n\ncountries observed repeatedly for several years\n\nIn xt data tables observations are identified by two ID variables: one for the cross-sectional units, one for time\nxt data is balanced if all cross-sectional units are observed at the very same time periods\nIt is called unbalanced if some cross-sectional units are observed more times than others"
  },
  {
    "objectID": "rm-data/slides/week02.html#data-quality-is-key",
    "href": "rm-data/slides/week02.html#data-quality-is-key",
    "title": "Origins of Data, and Data Preparation",
    "section": "Data quality is key",
    "text": "Data quality is key\n\nKeyword: Quality, Quality, Quality\nData quality is key for any analysis\n\nGarbage-in-garbage-out: If data is useless, then answers of our analysis are bound to be useless…\n… no matter how fancy a method we apply to it.\n\nData quality is generally a subjective notion: Different standards for different purposes\nFirst you have to specify what is your (research) question:\n\nWhat do you want to explore or understand?\n\nIf you have a clear (pseudo) answer, then you can decide on your data quality\n\n\n\nHowever, there are some objective measures to decide if you have your question"
  },
  {
    "objectID": "rm-data/slides/week02.html#dimensions-of-data-quality",
    "href": "rm-data/slides/week02.html#dimensions-of-data-quality",
    "title": "Origins of Data, and Data Preparation",
    "section": "Dimensions of data quality",
    "text": "Dimensions of data quality\n\nContent - what is the variable really capturing?\nValidity - how close the actual content of the variable to the intended content\nReliability - if we were to measure the same variable multiple times for the same observation it should give the same result\nComparability of measurement - how similarly the same variable is measured across different observations\nCoverage - what proportion of the population are represented in the data\nUnbiased selection - if coverage is incomplete, is it representative of the population?\n\n\nHow was the data collected??\nDoes the variable capture what it is supposed to capture? are labels correct? is the variable measured correctly? (is =1 a true or a false?)"
  },
  {
    "objectID": "rm-data/slides/week02.html#you-should-allways-know-your-data",
    "href": "rm-data/slides/week02.html#you-should-allways-know-your-data",
    "title": "Origins of Data, and Data Preparation",
    "section": "YOU should allways know your data",
    "text": "YOU should allways know your data\n\nHow data was born? How was it collected, and processed?\nAll details of measurement that may be relevant for their analysis\n\nWhen in doubt, ask the data source. Manuals, codebooks, etc.\n\nBecause of this, you may want to have a:\n\nREADME.txt that describes where dataset comes from\nVARIABLES.xls that provides basic information on your variables (cookbook)"
  },
  {
    "objectID": "rm-data/slides/week02.html#secondary-data-general-characteristics",
    "href": "rm-data/slides/week02.html#secondary-data-general-characteristics",
    "title": "Origins of Data, and Data Preparation",
    "section": "Secondary data: General characteristics",
    "text": "Secondary data: General characteristics\n\nType: Data, or information, collected by someone else, for different purposes\n\nTax records collects do not contain demographics and education.\nA Survey that collects Demographics, but not income data.\n\nData quality consequences\n\nMay not contain variables that we need\nValidity may be high or low\nPotential selection bias if low covarege\n\nFrequent advantages\n\nInexpensive?\nOften many observations\n\n\n\n\nSecondary data is data that was collected by someone else for a different purpose\nWe cant control how the data was collected, but we can control how we use it. You need to assume that the data is not perfect. And assume consequences of that."
  },
  {
    "objectID": "rm-data/slides/week02.html#how-to-collect-the-data",
    "href": "rm-data/slides/week02.html#how-to-collect-the-data",
    "title": "Origins of Data, and Data Preparation",
    "section": "How to Collect the Data",
    "text": "How to Collect the Data\n\nBy hand:\n\nMany data sources are available online: World Bank and IMF data, etc. (usually easy to download)\nFor the US, www.ipums.org has a lot of standardized data (CPS, ATUS, ACS, etc.)\n\nAutomated API:\n\nMany agencies also offer API (Application Programming Interface) to directly load data into a statistical software\nAPI widely used in many context. see here for a list from Berkeley"
  },
  {
    "objectID": "rm-data/slides/week02.html#how-to-collect-the-data-1",
    "href": "rm-data/slides/week02.html#how-to-collect-the-data-1",
    "title": "Origins of Data, and Data Preparation",
    "section": "How to Collect the Data",
    "text": "How to Collect the Data\n\nData from online platform (web scraping):\n\nhtml code includes data, that can be collected and analyzed\nR (rvest) and Python (beautiful soup) can be used for that purpose\nStata does not have good tools for web scraping\n\nNeed extensive cleaning\nCan be repeated (automated) if data is updated frequently\n\nMindful of the terms of service of the website\n\nData collection limited to what is on a site\n\n\n\nWeb scraping is a powerful tool that needs a know-how, and requires extensive cleaning\nWeb scraping is limited to what is on a site\nAlways be mindful of the terms of service of the website. Not all websites allow web scraping"
  },
  {
    "objectID": "rm-data/slides/week02.html#administrative-data-general-characteristics",
    "href": "rm-data/slides/week02.html#administrative-data-general-characteristics",
    "title": "Origins of Data, and Data Preparation",
    "section": "Administrative Data: General Characteristics",
    "text": "Administrative Data: General Characteristics\n\nBusiness transactions, Government records, taxes, social security\nMany advantages\n\nOften great coverage (Census), few missing values, high quality content (tax records)\nMany well defined and documented variables\n\nSome disadvantages\n\nDefined for a different purpose, not your research question\nOften not detailed/specific enough\nBiggest problem is very limited access: Need to apply for access"
  },
  {
    "objectID": "rm-data/slides/week02.html#survey-general-characteristics",
    "href": "rm-data/slides/week02.html#survey-general-characteristics",
    "title": "Origins of Data, and Data Preparation",
    "section": "Survey: General characteristics",
    "text": "Survey: General characteristics\n\nSurveys collect data by asking people (respondents) and recording their answers\nAnswers should be short(!) and easily(!) transformed into variables\nMajor advantage: you can ask what you want to know\nHow?\n\nself-administered surveys and interviews\nWeb, telephone, in person, mix - computer aided interview\n\nWhat could go wrong? (and assume House MD is wrong)\n\n\nChoice of data collection approach matters a great deal. Can be done efficiently, with good aids. And depending on design, can be cheap"
  },
  {
    "objectID": "rm-data/slides/week02.html#sampling",
    "href": "rm-data/slides/week02.html#sampling",
    "title": "Origins of Data, and Data Preparation",
    "section": "Sampling",
    "text": "Sampling\n\nPerhaps one can collect data on all observations we want (the population)\nbut, more often we don’t because it’s impractical or prohibitively expensive\nSampling is when we purposefully collect data on a subset/sample (\\(&lt;100%\\) coverage) of the population\nSampling is the process that selects that subset (How do we select the sample?)"
  },
  {
    "objectID": "rm-data/slides/week02.html#what-we-want-representative-samples",
    "href": "rm-data/slides/week02.html#what-we-want-representative-samples",
    "title": "Origins of Data, and Data Preparation",
    "section": "What We Want: Representative samples",
    "text": "What We Want: Representative samples\n\nA sample is good if it represents the population\n\nall important variables have very similar distributions in the sample and the population\nall patterns in the sample are very similar to the patterns in the population\n\nExamples\n\nThe age distribution of a sample of employees is the same as the age distribution of all employees\nThe income distribution in the CPS is the same as the income distribution in the US\n\nBut how can we tell?"
  },
  {
    "objectID": "rm-data/slides/week02.html#how-can-we-tell-if-a-sample-is-representative",
    "href": "rm-data/slides/week02.html#how-can-we-tell-if-a-sample-is-representative",
    "title": "Origins of Data, and Data Preparation",
    "section": "How can we tell if a sample is representative",
    "text": "How can we tell if a sample is representative\n\nNever for sure\n\nIf you knew the population, you wouldn’t need the sample\nWe know the patterns in the sample but not in the population\n\nBut, we could do Benchmarking\n\nWe may know a few distributions or patterns in the population\nThose should be similar in the sample\nExample: Using the Census to check the age distribution in the CPS\n\nOr, knowing the process of sampling\n\nRandom sampling is known to lead to representative samples with high likelihood"
  },
  {
    "objectID": "rm-data/slides/week02.html#sampling-random-sampling",
    "href": "rm-data/slides/week02.html#sampling-random-sampling",
    "title": "Origins of Data, and Data Preparation",
    "section": "Sampling: Random sampling",
    "text": "Sampling: Random sampling\n\nRandom sampling is a selection rule independent of any important variable\n\nits the most likely to produce representative samples\nAny other methods may lead to biased selection\n\nExamples\n\nGood: people with odd-numbered birth dates (a 50% sample)\nGood: the first half of a list of firms sorted by a random number generated by the computer\nBad: the first half of a list of people by alphabetical order\nBad: firms that were established in the most recent years"
  },
  {
    "objectID": "rm-data/slides/week02.html#random-sampling-is-best",
    "href": "rm-data/slides/week02.html#random-sampling-is-best",
    "title": "Origins of Data, and Data Preparation",
    "section": "Random sampling is best",
    "text": "Random sampling is best\n\nProvided sample is large enough (N \\(\\rightarrow\\) infinity)\nIn small samples (dozens) anything is possible\nIn a representative sample, size (N) matters, not coverage\n\nCPS ~ 70,000 households (0.02% of US); ATUS ~ 9,000 ppl (0.003% of US)\n\nLarger samples better (more power/precise estimates) but …"
  },
  {
    "objectID": "rm-data/slides/week02.html#sampling-clusterstratified-sampling",
    "href": "rm-data/slides/week02.html#sampling-clusterstratified-sampling",
    "title": "Origins of Data, and Data Preparation",
    "section": "Sampling: Cluster/Stratified sampling",
    "text": "Sampling: Cluster/Stratified sampling\n\nSome times, however, random sampling is prohibitively expensive.\nFurthermore, sometimes we want to oversample some groups (rare groups), to make sure we have enough observations\nBoth approaches may help to reduce costs of collection."
  },
  {
    "objectID": "rm-data/slides/week02.html#section",
    "href": "rm-data/slides/week02.html#section",
    "title": "Origins of Data, and Data Preparation",
    "section": "",
    "text": "Random samplingCluster samplingStratified sampling"
  },
  {
    "objectID": "rm-data/slides/week02.html#what-is-different-with-big-data",
    "href": "rm-data/slides/week02.html#what-is-different-with-big-data",
    "title": "Origins of Data, and Data Preparation",
    "section": "What is different with Big Data?",
    "text": "What is different with Big Data?\n\nmassive datasets that are (billions?) Not necessarily representative\noften automatically and continuously collected and stored (Transaction data, tweets, etc.)\n\nNot necessarily for analytic purposes\n\nComplex\n\ntext (video, music/noise), network, multidimensional, maps"
  },
  {
    "objectID": "rm-data/slides/week02.html#different-yet-the-same",
    "href": "rm-data/slides/week02.html#different-yet-the-same",
    "title": "Origins of Data, and Data Preparation",
    "section": "Different yet the same",
    "text": "Different yet the same\nDifferent:\n\nA particular source of uncertainty of the results of an analysis is greatly reduced\nRare or more nuanced patterns can be uncovered\nPractical challenges (storage, processing, etc.)\nSome challenges may be solved by working with a random subsample\n\nSame:\n\nNeed to represent entire population if incomplete coverage\nExample: Big Data with 75% coverage with a selection bias leads to biased results\nNon-big data from same population with 1% random sample leads to good results"
  },
  {
    "objectID": "rm-data/slides/week02.html#sample-selection-bias",
    "href": "rm-data/slides/week02.html#sample-selection-bias",
    "title": "Origins of Data, and Data Preparation",
    "section": "Sample selection bias",
    "text": "Sample selection bias\n\nThe sample you collect is different from the population\nThis difference is crucial in the story\nExample: Predicting presidential election\n\n1936: Literary Digest. FD Roosevelt vs Landon. 10m people asked. 2m replied. Biggest poll ever. Landon was predicted win 57%\n\n1948 Chicago Tribune. Dewey predicted beat Truman. Used phone registry"
  },
  {
    "objectID": "rm-data/slides/week02.html#legal-and-ethical-aspects",
    "href": "rm-data/slides/week02.html#legal-and-ethical-aspects",
    "title": "Origins of Data, and Data Preparation",
    "section": "Legal and ethical aspects",
    "text": "Legal and ethical aspects\nDuring Data collection, be aware of ethical and legal constraints, Special care with sensitive information\nMore with web scraping…\nAlways communicate with the source owner(s) and or with legal professional if you are planning to use seemingly sensitive data (names, addresses, etc.)"
  },
  {
    "objectID": "rm-data/slides/week02.html#ai-and-data-collection-wrangling",
    "href": "rm-data/slides/week02.html#ai-and-data-collection-wrangling",
    "title": "Origins of Data, and Data Preparation",
    "section": "AI and data collection, wrangling",
    "text": "AI and data collection, wrangling\n\nData collection and management often behind walls\nAI can help write code to web-scrape, etc. (Python is quite good at it)\nAI is great to give a first impression of your dataset, incl. quality, data structure\nAI is helpful to discuss sampling ideas\nAI needs context to do good, and will not have proper domain knowledge\nAI needs supervision\n\nLesson: AI is a tool, not a replacement"
  },
  {
    "objectID": "rm-data/slides/week02.html#main-takeaway",
    "href": "rm-data/slides/week02.html#main-takeaway",
    "title": "Origins of Data, and Data Preparation",
    "section": "Main takeaway",
    "text": "Main takeaway\n\nKnow your data\n\nHow it was born,\nWhat its main advantages are\nWhat its main disadvantages are\n\nData quality determines the results of your analysis\nData quality is determined by how the data was born, and how you are planning to use it"
  },
  {
    "objectID": "rm-data/slides/week02.html#motivation-1",
    "href": "rm-data/slides/week02.html#motivation-1",
    "title": "Origins of Data, and Data Preparation",
    "section": "Motivation",
    "text": "Motivation\n\nDoes immunization of infants against measles save lives in poor countries? Use data on immunization rates in various countries in various years from the World Bank. How should you store, organize and use the data to have all relevant information in an accessible format that lends itself to meaningful analysis?\nYou want to know, who has been the best manager in the top English football league. Have downloaded data on football games and on managers. To answer your question you need to combine this data. How should you do that? And are there issues with the data that you need to address?"
  },
  {
    "objectID": "rm-data/slides/week02.html#variable-types-qualitative-vs-quantitative",
    "href": "rm-data/slides/week02.html#variable-types-qualitative-vs-quantitative",
    "title": "Origins of Data, and Data Preparation",
    "section": "Variable types: Qualitative vs quantitative",
    "text": "Variable types: Qualitative vs quantitative\n\nData can be born (collected, generated) in different form, and our variables may capture the quality or the quantity of a phenomenon.\nQuantitative variables are born as numbers. Typically take many values. (age, height, steps…)\nQualitative variables, also called categorical variables, take on a few values, with each value having a specific interpretation (belonging a category). (Race, Gender, Brand, etc)"
  },
  {
    "objectID": "rm-data/slides/week02.html#variable-types-dummies-or-binary",
    "href": "rm-data/slides/week02.html#variable-types-dummies-or-binary",
    "title": "Origins of Data, and Data Preparation",
    "section": "Variable types: Dummies or binary",
    "text": "Variable types: Dummies or binary\n\nA special case is a binary variable, which can take on two values\n…yes/no answer to whether the observation belongs to some group. Best to represent these as 0 or 1 variables: 0 for no, 1 for yes.\n\nExample: is_female, is_head_of_household, is_pregnant, is_employed\n\nFlag - binary showing existence of some issue (such as missing value for another variable, presence in another dataset)\n\nExample: missing_age, missing_income, in_sample\n\nNote Some times Surveys do NOT use 0-1 for binary variables. Be careful.\n\nssc install fre\nfre categorical_variable\n* This program will tabulate the data and show you the labels if any"
  },
  {
    "objectID": "rm-data/slides/week02.html#variable-types---formal-definition",
    "href": "rm-data/slides/week02.html#variable-types---formal-definition",
    "title": "Origins of Data, and Data Preparation",
    "section": "Variable types - formal definition",
    "text": "Variable types - formal definition\n\nNominal qualitative variables take on values that cannot be unambiguously ordered: Color, brands, race\nOrdinal, or ordered variables take on values that are unambiguously ordered. Grade, satisfaction, income brackets\nInterval/continuous variables are ordered variables, with a comparable “change”: Age, Degree Celsius, Price in dollars."
  },
  {
    "objectID": "rm-data/slides/week02.html#data-wrangling",
    "href": "rm-data/slides/week02.html#data-wrangling",
    "title": "Origins of Data, and Data Preparation",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nData wrangling is the process of transforming raw data to a set of data tables that can be used for a variety of downstream purposes such as data analysis.\n\n\n\n\nUnderstanding and storing\n\nstart from raw data\nunderstand the structure and content\ncreate tidy data tables\nunderstand links between tables\n\n\n\n\nData cleaning\n\nunderstand features, variable types\nfilter duplicates\nlook for and manage missing observations\nunderstand limitations\n\n\n\n\nThis is a crucial because out there, Data is Messy"
  },
  {
    "objectID": "rm-data/slides/week02.html#the-tidy-data-approach",
    "href": "rm-data/slides/week02.html#the-tidy-data-approach",
    "title": "Origins of Data, and Data Preparation",
    "section": "The tidy data approach",
    "text": "The tidy data approach\nA useful concept of organizing and cleaning data is called the tidy data approach:\n\nEach observation forms a row.\nEach variable forms a column.\nEach type of observational unit forms a table. (One for Families, One for Members)\nEach observation has a unique identifier (ID) (Family ID and Person ID)\nCan be merged with other tables if needed.\n\nAdvantages:\n\nTidy tables are easy to work with, and make finding errors easy.\nEasy to understand and extend: New observations adds rows; new variables adds columns."
  },
  {
    "objectID": "rm-data/slides/week02.html#simple-tidy-data-table",
    "href": "rm-data/slides/week02.html#simple-tidy-data-table",
    "title": "Origins of Data, and Data Preparation",
    "section": "Simple tidy data table",
    "text": "Simple tidy data table\n\n\n\nhotel_id\nprice\ndistance\n\n\n\n\n21897\n81\n1.7\n\n\n21901\n85\n1.4\n\n\n21902\n83\n1.7\n\n\n\nSource: hotels-vienna data. Vienna, 2017 November week-end.\nEach Row a new observation, Each Column a new Variable"
  },
  {
    "objectID": "rm-data/slides/week02.html#tidy-data-table-of-multi-dimensional-data",
    "href": "rm-data/slides/week02.html#tidy-data-table-of-multi-dimensional-data",
    "title": "Origins of Data, and Data Preparation",
    "section": "Tidy data table of multi-dimensional data",
    "text": "Tidy data table of multi-dimensional data\n\nThe tidy approach - store xt data so that One row is one it observation (Cross-section unit i observed at time t). (Long format)\n\nThe next row then may be the same cross-sectional unit observed in the next time period.\n\nYou may want to use similar criteria with multi-dimensional data (ijt data)\n\nAlternative wide format : one row refers to one cross-sectional unit, and different time periods are represented in different columns. Not the best way to keep the data"
  },
  {
    "objectID": "rm-data/slides/week02.html#displaying-immunization-rates-across-countries---wide",
    "href": "rm-data/slides/week02.html#displaying-immunization-rates-across-countries---wide",
    "title": "Origins of Data, and Data Preparation",
    "section": "Displaying immunization rates across countries - WIDE",
    "text": "Displaying immunization rates across countries - WIDE\n\n\n\n\n\n\n\n\n\n\n\n\nCountry\nimm2015\nimm2016\nimm2017\ngdppc2015\ngdppc2016\ngdppc2017\n\n\n\n\nIndia\n87\n88\n88\n5743\n6145\n6516\n\n\nPakistan\n75\n75\n76\n4459\n4608\n4771\n\n\n\nSource: world-bank-vaccination data\nWide format of country-year panel data, each row is one country, different years are different variables.\nimm: rate of immunization against measles among 12–13-month-old infants.\ngdppc: GDP per capital, PPP, constant 2011 USD."
  },
  {
    "objectID": "rm-data/slides/week02.html#displaying-immunization-rates-across-countries---long",
    "href": "rm-data/slides/week02.html#displaying-immunization-rates-across-countries---long",
    "title": "Origins of Data, and Data Preparation",
    "section": "Displaying immunization rates across countries - LONG",
    "text": "Displaying immunization rates across countries - LONG\n\n\n\nCountry\nYear\nimm\ngdppc\n\n\n\n\nIndia\n2015\n87\n5743\n\n\nIndia\n2016\n88\n6145\n\n\nIndia\n2017\n88\n6516\n\n\nPakistan\n2015\n75\n4459\n\n\nPakistan\n2016\n75\n4608\n\n\nPakistan\n2017\n76\n4771\n\n\n\nNote: Tidy (long) format of country-year panel data, each row is one country in one year.\nimm: rate of immunization against measles among 12–13-month-old infants.\ngdppc: GDP per capital, PPP, constant 2011 USD. Source: world-bank-vaccination data."
  },
  {
    "objectID": "rm-data/slides/week02.html#stata-programming-corner",
    "href": "rm-data/slides/week02.html#stata-programming-corner",
    "title": "Origins of Data, and Data Preparation",
    "section": "Stata Programming corner",
    "text": "Stata Programming corner\n\nTransforming your data from Wide to Long format (or viceversa) can be done using reshape\n\n* From wide to long\nren *, low // &lt;- Make sure your variables are all lower case\nreshape long imm gdppc, /// &lt;- Make variable Long, and indicate what variables to \"make\" long\n    i(country) j(year) string // &lt;- also the dimension that was previously \"wide\" Year\n\n* From long to wide\nreshape wide imm gdppc, /// &lt;- Make variable Long, and indicate what variables to \"make\" long\n    i(country) j(year)  // &lt;- also the dimension that was previously \"wide\" Year"
  },
  {
    "objectID": "rm-data/slides/week02.html#stata-programming-corner-1",
    "href": "rm-data/slides/week02.html#stata-programming-corner-1",
    "title": "Origins of Data, and Data Preparation",
    "section": "Stata Programming corner",
    "text": "Stata Programming corner\n\nSome times, you may need to reshape only 1 variable, and keep the rest as they are.\n\nAdding head of household education to all family members.\n\nYou can do it two ways:\n\nCreate a smaller dataset and merge\nID head, and gen the new variable\nbysort hid: egen head_educ = max(educ*(is_head==1))"
  },
  {
    "objectID": "rm-data/slides/week02.html#a-complex-dataset-relational-database",
    "href": "rm-data/slides/week02.html#a-complex-dataset-relational-database",
    "title": "Origins of Data, and Data Preparation",
    "section": "A complex Dataset: Relational database",
    "text": "A complex Dataset: Relational database\n\nSome datasets cannot be stored in a single table.\n\nok, they could, but would be very inefficient.\n\nData like this are typically stored in a relational database.\n\nSmaller tidy datasets can be stored in a single table (single unit of observation),\nand that can be linked to other tables with a unique identifiers (ID or Keys).\n\nThis structure forces you to better understand your data.\nAfter understanding the data, you can merge/join/link/match tables as needed."
  },
  {
    "objectID": "rm-data/slides/week02.html#identifying-successful-futbol-managers",
    "href": "rm-data/slides/week02.html#identifying-successful-futbol-managers",
    "title": "Origins of Data, and Data Preparation",
    "section": "Identifying successful Futbol managers",
    "text": "Identifying successful Futbol managers\n\nReview the example, Specially if interested in Futbol\nIn short, Data can have different structures (all tidy)\nSome structures are more useful than others.\nUnderstanding those structures will allow you to work with the data"
  },
  {
    "objectID": "rm-data/slides/week02.html#american-time-use-survey",
    "href": "rm-data/slides/week02.html#american-time-use-survey",
    "title": "Origins of Data, and Data Preparation",
    "section": "American Time Use Survey",
    "text": "American Time Use Survey\n\nThe ATUS is a good example of a relational dataset\nIf downloaded RAW (census) you need to navigate through many files:\n\nATUS-ACT: contains all time activities, plus other info, for the individuals interviewed in the ATUS. Keys: tucaseid and tuactivity_n\nATUS-CPS: Data for all Family members, from CPS. Keys tucaseid tulineno\nATUS-RESP: Some aggregated TimeUse, and additional respondand information. Key tucaseid\nATUS-ROST: Basic demographics for all household members. Keys tucaseid tulineno\nATUS-SUM: Aggregated Time use data, by different types. Keys tucaseid"
  },
  {
    "objectID": "rm-data/slides/week02.html#american-time-use-survey-1",
    "href": "rm-data/slides/week02.html#american-time-use-survey-1",
    "title": "Origins of Data, and Data Preparation",
    "section": "American Time Use Survey",
    "text": "American Time Use Survey\n\nDepending on your goals, you may want to combine information from various datasets\n\nAggregate some data, combine others, transform.\n\nUnderstanding the data structure may also help you see how to best “merge the data”"
  },
  {
    "objectID": "rm-data/slides/week02.html#stata-corner-types-of-merging",
    "href": "rm-data/slides/week02.html#stata-corner-types-of-merging",
    "title": "Origins of Data, and Data Preparation",
    "section": "Stata-Corner: Types of Merging",
    "text": "Stata-Corner: Types of Merging\nThere are 4 3 types of merging, depending of the master or using dataset\n\n1:1 merging: Both master and using datasets are uniquely by the same variables. use atus-rost merge 1:1 tucaseid tulineno using atus-cps\n1:m merging: Each observation in the master file will be merge with many units in the using dataset. Master has unique ID. use atus-resp merge 1:m tucaseid using atus-act\nm:1 merging: Many observations in the master will be merge with one unit in the using. Using has a unique ID\nuse atus-act merge m:1 tucaseid using atus-resp\nm:m merging: its wrong…dont do it. Perhaps think joinby instead"
  },
  {
    "objectID": "rm-data/slides/week02.html#stata-corner",
    "href": "rm-data/slides/week02.html#stata-corner",
    "title": "Origins of Data, and Data Preparation",
    "section": "Stata-Corner",
    "text": "Stata-Corner\n\n\n\n\n\n\nImportant\n\n\n\nEvery time you do a merge, Stata will create a variable called _merge that will tell you what happened to the merge.\nIf this variable exist in your datasets (master or using) you will get an error.\nso make sure to drop (or rename) it before merging (after you have checked it)"
  },
  {
    "objectID": "rm-data/slides/week02.html#complex-data---tidy-data-summary",
    "href": "rm-data/slides/week02.html#complex-data---tidy-data-summary",
    "title": "Origins of Data, and Data Preparation",
    "section": "Complex data - tidy data: summary",
    "text": "Complex data - tidy data: summary\n\nCreating a tidy data is important so data tables are easy to understand, combine and extend in the future.\nIf relational data, IDs are essential (allow to link tables)\nOften raw data will not come in a tidy format, and you will need to work understanding the structure, relationships and find the individual ingredients.\nFor analysis work, may need to combine tidy data tables\nBut probably only need a fraction of all variables."
  },
  {
    "objectID": "rm-data/slides/week02.html#data-cleaning-1",
    "href": "rm-data/slides/week02.html#data-cleaning-1",
    "title": "Origins of Data, and Data Preparation",
    "section": "Data cleaning",
    "text": "Data cleaning\nWith most data, in addition to understand it, you need to “clean”, before using it (Very Important)\n\nEntity resolution:\n\nDealing with duplicates: Why are they there? What to do?\nAmbiguous identification: is it John or Jonh or Jon ?\nnon-entity rows: uh? what is this?\n\nMissing values\n\nwhy is it missing? is it missing at random? by design? endogenous?"
  },
  {
    "objectID": "rm-data/slides/week02.html#dealing-with-duplicates",
    "href": "rm-data/slides/week02.html#dealing-with-duplicates",
    "title": "Origins of Data, and Data Preparation",
    "section": "Dealing with duplicates",
    "text": "Dealing with duplicates\n\n** Duplicates**: Observations appearing more than once in the data.\n\nMay be the result of human error, or the features of data source (e.g., data scraped from classified ads. Some posts appear more than once).\n\nOften, easy spot\n\nduplicates report in Stata, or bysort ID: gen dup = _n\n\nbut one needs to investigate. Makes sense / an error? something else?\nDecision: what to keep. Sometimes no clear-cut way, but usually no big deal."
  },
  {
    "objectID": "rm-data/slides/week02.html#entity-identification-and-resolution",
    "href": "rm-data/slides/week02.html#entity-identification-and-resolution",
    "title": "Origins of Data, and Data Preparation",
    "section": "Entity identification and resolution",
    "text": "Entity identification and resolution\n\nYou need to have unique IDs\n\nyou can use isid to check if a variable(s) is a unique identifier\nIf not, check why not. Perhaps wrong ID?\n\nPossible cases:\n\nSame identifier, different entities\nDifferent identifiers, same entity (??)\n\nEntity resolution: process of identifying, merging and eliminating duplicate entities."
  },
  {
    "objectID": "rm-data/slides/week02.html#entity-resolution-example",
    "href": "rm-data/slides/week02.html#entity-resolution-example",
    "title": "Origins of Data, and Data Preparation",
    "section": "Entity resolution example",
    "text": "Entity resolution example\n\n\n\nTeam ID\nUnified name\nOriginal name\n\n\n\n\n19\nMan City\nManchester City\n\n\n19\nMan City\nMan City\n\n\n19\nMan City\nMan. City\n\n\n19\nMan City\nManchester City F.C.\n\n\n20\nMan United\nManchester United\n\n\n20\nMan United\nManchester United F.C.\n\n\n20\nMan United\nManchester United Football Club\n\n\n20\nMan United\nMan United"
  },
  {
    "objectID": "rm-data/slides/week02.html#getting-rid-of-non-entity-observations",
    "href": "rm-data/slides/week02.html#getting-rid-of-non-entity-observations",
    "title": "Origins of Data, and Data Preparation",
    "section": "Getting rid of non-entity observations",
    "text": "Getting rid of non-entity observations\n\nSometimes, data may contain rows that do not belong to an entity we want\n\nFor example, region-level data may contain country-level aggregates\n\nFind them and drop them (unless you have a good reason to keep them)\nCommon Case: a data table from the World Bank on countries often includes observations for larger regions (continents, low income countries, etc)"
  },
  {
    "objectID": "rm-data/slides/week02.html#missing-values",
    "href": "rm-data/slides/week02.html#missing-values",
    "title": "Origins of Data, and Data Preparation",
    "section": "Missing values",
    "text": "Missing values\n\nA frequent and important issue: missing values.\nMissing values mean that the value of a variable is available for some, but not all, observations.\nDifferent languages may encode missing values differently.\n\nIn Stata dot “.”, an empty space “” are missing for numbers and strings.\nBut, “.” is considered larger than any number, so be careful when coding.\n\nSurveys, may also have their own rules for coding missing\n\nbinary 0 for no, 1 for yes, 9 for missing\npercent 0-100, 9999 for missing\nnumeric, range is 1-100000, 9999999999 for missing"
  },
  {
    "objectID": "rm-data/slides/week02.html#missing-values-what-to-do",
    "href": "rm-data/slides/week02.html#missing-values-what-to-do",
    "title": "Origins of Data, and Data Preparation",
    "section": "Missing values: What to do?",
    "text": "Missing values: What to do?\nDepends on: Scope: How much missing? and Reason: Why missing?\n\nLook at content of data. This could be related to data quality (especially coverage)\nMissing by design may not be a problem. (Ever been pregnant? missing for men)\nMissing values should be counted, because they mean fewer observations with valid information. (compounding effect)\nBig problem: potential selection bias.\n\nIs data missing at random?\nIs the data still representative?"
  },
  {
    "objectID": "rm-data/slides/week02.html#missing-values-understanding-the-selection-process",
    "href": "rm-data/slides/week02.html#missing-values-understanding-the-selection-process",
    "title": "Origins of Data, and Data Preparation",
    "section": "Missing values: Understanding the selection process",
    "text": "Missing values: Understanding the selection process\n\nRandom: When missing data really means no information, it may be the result of errors in the data collection process. Rare.\nIn some other cases, missing just means “zero” or “no”. In these instances, we should simply recode (replace) the missing values as “zero” or as “no”. (how many children? missing means zero)\nOften, values are missing for a reason.\n\nSome survey respondents may not know the answer to a question or refuse to answer it,\nThey are likely to be different from those who provide valid answers."
  },
  {
    "objectID": "rm-data/slides/week02.html#missing-values-what-can-we-do",
    "href": "rm-data/slides/week02.html#missing-values-what-can-we-do",
    "title": "Origins of Data, and Data Preparation",
    "section": "Missing values: what can we do?",
    "text": "Missing values: what can we do?\nTwo basic options:\n\nRestrict the analysis to observations with non-missing values for all variables used in the analysis.\n\nDefault option in many statistical packages.\n\nImputation - Fill in some value for the missing values, such as the mean or median.\n\nThere are more advanced and better options, but should be used with caution.\nNot all imputation methods are created equal.\n\nBe conservative, impute if absolutely necessary, and document it."
  },
  {
    "objectID": "rm-data/slides/week02.html#missing-values-some-practical-advice",
    "href": "rm-data/slides/week02.html#missing-values-some-practical-advice",
    "title": "Origins of Data, and Data Preparation",
    "section": "Missing values: Some practical advice",
    "text": "Missing values: Some practical advice\n\nFor binary variables: zero if yes/no.\nFor qualitative nominal variables, missing as a new value: white, blue, red and missing.\nFor ordinal variables, missing could be recoded to a neutral variable.\nFor quantitative variables -&gt; mean or median? (try not to! There are good imputation methods)\nif impute, create a flag and use it analysis. At the very least for sensitivity analysis.\n(Bad) Imputation will have consequences, be conservative."
  },
  {
    "objectID": "rm-data/slides/week02.html#more-on-data-cleaning",
    "href": "rm-data/slides/week02.html#more-on-data-cleaning",
    "title": "Origins of Data, and Data Preparation",
    "section": "More on Data Cleaning",
    "text": "More on Data Cleaning\n\nConsider the data quality, and the data collection process.\n\nAre there outliers? data entry errors? bunching?\n\nUnderstand the data generating process.\n\nAre there missing values? why?\n\nThis is an iterative process, and you may need to go back and forth between data cleaning and data analysis.\nMore on this with EDA and Data Visualization"
  },
  {
    "objectID": "rm-data/slides/week02.html#data-wrangling-cooking-recipe",
    "href": "rm-data/slides/week02.html#data-wrangling-cooking-recipe",
    "title": "Origins of Data, and Data Preparation",
    "section": "Data wrangling: Cooking recipe",
    "text": "Data wrangling: Cooking recipe\n\nWrite a code: it can be repeated, commented, cleaned, and improved later\nUnderstand the structure of the dataset, recognize links. Draw a schema\nStart by looking into the data to spot issues. (summarize, tabulate, edit, browse)\n\ndo they have meaningful ranges? Correct them or set them as missing\n\nStore data in tidy datasets. one row is one observation, one column a variable\nHave a description of variables (Labels, make sure you know what they are)\nIdentify missing values and store them in an appropriate format. Make edits if needed.\nDocument every step of data cleaning &lt;- Very Important and goes to the code"
  },
  {
    "objectID": "rm-data/slides/week02.html#ai-and-data-wrangling-upside",
    "href": "rm-data/slides/week02.html#ai-and-data-wrangling-upside",
    "title": "Origins of Data, and Data Preparation",
    "section": "AI and data wrangling: Upside",
    "text": "AI and data wrangling: Upside\nIf given the right instructions, and information, AI can help you with data wrangling:\n\nunderstands your data structure\ncombines datasets\nsummarizes the data\nunderstands your variables\nfinds potential problems\n\nBut, it is not perfect. You need to understand the data and the process. Review and control."
  },
  {
    "objectID": "rm-data/slides/week01/template.html",
    "href": "rm-data/slides/week01/template.html",
    "title": "My Economic Analysis",
    "section": "",
    "text": "This is a simple template for a report."
  },
  {
    "objectID": "rm-data/slides/week01/template.html#method-1",
    "href": "rm-data/slides/week01/template.html#method-1",
    "title": "My Economic Analysis",
    "section": "Method 1",
    "text": "Method 1"
  },
  {
    "objectID": "rm-data/research-proposals/proposal9.html",
    "href": "rm-data/research-proposals/proposal9.html",
    "title": "To Boldly Go: Resource Allocation and Post-Scarcity Economics in Star Trek",
    "section": "",
    "text": "Introduction\nThe Star Trek universe presents a vision of a future where scarcity has been largely eliminated through advanced technology. This research proposal aims to analyze the economic system depicted in Star Trek, focusing on resource allocation mechanisms, the nature of work, and the implications of technologies like replicators for economic theory.\n\n\nBackground and Research Question\nStar Trek depicts a future where the Federation has moved beyond money-based economies, thanks to technologies like matter replicators that can produce most goods on demand (Saadia 2016). This post-scarcity scenario challenges many fundamental assumptions of economics, which is traditionally defined as the study of resource allocation under conditions of scarcity (Margolis 1998).\nHowever, even in this abundant future, some forms of scarcity persist, such as scarce antimatter for starship fuel, and limited spots in Starfleet Academy. This mix of abundance and residual scarcity provides a unique lens through which to examine economic principles (Krauss 2007).\nMoreover, the show depicts various alien civilizations with different economic systems, allowing for comparative analysis. The Ferengi, for instance, represent an extreme form of capitalism, contrasting sharply with the Federation’s post-scarcity economy.\nMain Research Question: How does the post-scarcity economy depicted in Star Trek challenge and inform traditional economic theories of resource allocation and value?\nSecondary Research Questions:\n\nWhat mechanisms replace market-based resource allocation in Star Trek’s moneyless economy?\nHow does the nature of work and human motivation change in a post-scarcity environment?\nWhat insights does Star Trek offer about the transition from a scarcity-based to a post-scarcity economy?\n\n\n\nPotential Data Sources\n\nTV Series and Films: Scripts and scenes from various Star Trek series and movies\nStar Trek Technical Manuals: Detailed information about technologies in the Star Trek universe\nFan Wikis: Comprehensive databases of Star Trek lore\nEconomic Literature: Theories on post-scarcity economics and resource allocation\nFuturist Predictions: Technological forecasts related to automation and resource abundance\n\n\n\nPotential Approach\nWe will employ a mixed-methods approach, combining qualitative content analysis with economic modeling. First, we will conduct a systematic review of Star Trek series and films, coding for depictions of economic activities, resource allocation decisions, and technologies with economic implications.\nUsing this data, we will construct a model of the Federation’s economy, estimating key parameters such as production possibilities, resource constraints, and mechanisms for allocating scarce resources (e.g., starship assignments). We will then compare this model with various real-world economic systems and theoretical post-scarcity models.\nTo analyze the nature of work and motivation, we will conduct a comparative analysis between the depiction of work in Star Trek and real-world theories of work motivation and job satisfaction. We will also develop a theoretical model of skill development and career progression in a post-scarcity environment.\nFor the transition to post-scarcity, we will use the limited information provided in Star Trek about Earth’s history to construct a speculative model of economic transition, comparing it with real-world technological transitions and economic development theories.\n\n\nExpected Findings\nWe anticipate finding that Star Trek’s post-scarcity economy challenges many fundamental economic assumptions, potentially offering insights into alternative resource allocation mechanisms beyond market-based systems. We expect to see that while replicator technology eliminates scarcity for most goods, the allocation of truly scarce resources (like starships) involves complex decision-making processes that may inform real-world resource allocation theories.\nRegarding work and motivation, we expect to find that Star Trek depicts a shift from extrinsic to intrinsic motivation, potentially offering insights into how work might be restructured in highly automated future economies.\nWe also anticipate that the study of various alien economies in Star Trek will provide a rich comparative framework for understanding different economic systems and their cultural underpinnings.\n\n\nConclusion\nThis research will provide a novel perspective on economic theory by examining a fictional post-scarcity society. While based on a speculative future, the findings may offer valuable insights into potential long-term economic developments, particularly as we move towards increasing automation and resource efficiency. Moreover, this study could demonstrate the potential of using science fiction scenarios as thought experiments for economic theory and policy planning.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/startrek-economics\nThis repository will contain all data collection scripts, economic models, visualization code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nKrauss, Lawrence M. 2007. The Physics of Star Trek. Basic Books.\n\n\nMargolis, Howard. 1998. “Star Trek: Where No Economy Has Gone Before.” Reason 30 (5): 58.\n\n\nSaadia, Manu. 2016. Trekonomics: The Economics of Star Trek. Pipertext."
  },
  {
    "objectID": "rm-data/research-proposals/proposal7.html",
    "href": "rm-data/research-proposals/proposal7.html",
    "title": "The Iron Bank Always Collects: A Study of Debt and Financial Institutions in Game of Thrones",
    "section": "",
    "text": "Introduction\nGeorge R.R. Martin’s “A Song of Ice and Fire” series and its television adaptation “Game of Thrones” present a rich, complex world with its own economic systems. This research proposal aims to analyze the role of debt and financial institutions in the series, focusing on the Iron Bank of Braavos, to draw parallels with real-world economic history and financial systems.\n\n\nBackground and Research Question\nThe Iron Bank of Braavos plays a crucial role in the political economy of the Game of Thrones world, financing wars and influencing the rise and fall of regimes (McCaffrey 2018). Its operations bear similarities to historical institutions like the Medici Bank and modern central banks (Graeber 2011). The series depicts various forms of debt, from personal loans to sovereign debt, and their consequences, providing a fictional lens through which to examine real-world financial dynamics.\nThe economic aspects of Game of Thrones have been subject to some academic scrutiny (Hudson 2017), but a comprehensive analysis of its financial institutions and their parallels to real-world economic history is lacking. This research aims to fill this gap, using the fictional world as a case study to explore broader economic principles.\nMain Research Question: How do the depictions of debt and financial institutions in Game of Thrones reflect real-world historical and contemporary economic phenomena?\nSecondary Research Questions:\n\nWhat parallels can be drawn between the Iron Bank of Braavos and historical financial institutions?\nHow does sovereign debt in Game of Thrones compare to real-world sovereign debt crises?\nWhat insights does the series offer about the relationship between financial power and political authority?\n\n\n\nPotential Data Sources\n\nBook Series: “A Song of Ice and Fire” by George R.R. Martin\nTV Series: Scripts and scenes from “Game of Thrones”\nFan Wikis: Detailed information about the economic aspects of the Game of Thrones world\nHistorical Economic Data: Information on medieval and early modern financial systems\nContemporary Economic Data: Modern sovereign debt and banking system data\n\n\n\nPotential Approach\nWe will employ a mixed-methods approach, combining qualitative textual analysis with quantitative historical comparisons. First, we will conduct a systematic review of all mentions and depictions of the Iron Bank and debt in both the book series and TV show, coding for types of transactions, terms of loans, and consequences of default.\nUsing this data, we will construct a model of the Game of Thrones financial system, estimating key parameters such as interest rates, default risks, and the Bank’s impact on political stability. We will then compare this model with historical data on institutions like the Medici Bank and the Bank of England.\nTo analyze sovereign debt, we will use comparative case studies, contrasting the Crown’s debt to the Iron Bank with real-world sovereign debt crises. We will also employ network analysis to map the relationships between financial and political power in the series.\n\n\nExpected Findings\nWe anticipate finding significant parallels between the Iron Bank and historical financial institutions, particularly in terms of their political influence and role in state formation. We expect to see that sovereign debt in Game of Thrones mirrors many aspects of real-world sovereign debt, including the consequences of default and the relationship between debt and political legitimacy.\nWe also anticipate finding that the series offers insights into the interplay between financial and political power, potentially highlighting how control over credit can be as important as military might in shaping political outcomes.\n\n\nConclusion\nThis research will provide a novel perspective on financial history and institutions by examining them through the lens of a popular fantasy series. While based on a fictional setting, the findings may offer insights into real-world financial dynamics, particularly regarding the role of debt in political economy. Moreover, this study could demonstrate the potential of using popular media as a tool for economic education and analysis.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/ironbank-economics\nThis repository will contain all data collection scripts, economic models, visualization code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nGraeber, David. 2011. Debt: The First 5000 Years. Melville House.\n\n\nHudson, John. 2017. “Winter Is Coming: The Medieval World of Game of Thrones.” History Today 67 (7).\n\n\nMcCaffrey, Matthew. 2018. “The Economics of Game of Thrones.” The Independent Review 22 (4): 593–605."
  },
  {
    "objectID": "rm-data/research-proposals/proposal5.html",
    "href": "rm-data/research-proposals/proposal5.html",
    "title": "The Impact of Financial Literacy Programs on Household Savings and Investment Behavior",
    "section": "",
    "text": "Introduction\nFinancial literacy is increasingly recognized as a critical life skill in today’s complex economic environment. This research proposal aims to evaluate the effectiveness of financial literacy programs in improving household savings and investment behavior, with a focus on long-term financial well-being.\n\n\nBackground and Research Question\nFinancial literacy has been linked to better financial decision-making, increased savings, and improved economic outcomes (Lusardi and Mitchell 2014). However, financial literacy levels remain low in many countries, even among developed economies (Klapper, Lusardi, and Van Oudheusden 2015). In response, many governments and organizations have implemented financial literacy programs, but their effectiveness remains debated (Fernandes, Lynch Jr, and Netemeyer 2014).\nPrevious studies have examined the short-term effects of financial education, but less is known about its long-term impact on savings and investment behavior. Moreover, the heterogeneity in program effectiveness across different demographic groups and the mechanisms through which financial literacy affects behavior are not well understood (Kaiser and Menkhoff 2017).\nMain Research Question: How do financial literacy programs impact household savings and investment behavior in the long term?\nSecondary Research Questions:\n\nHow does the effectiveness of financial literacy programs vary across different demographic groups?\nWhat are the mechanisms through which financial literacy affects savings and investment behavior?\nHow do different types of financial literacy programs (e.g., school-based, workplace-based, community-based) compare in terms of effectiveness?\n\n\n\nPotential Data Sources\n\nFinancial Literacy Program Data: Information from government agencies or NGOs implementing these programs\nHousehold Finance Data: National household finance surveys or panel studies\nDemographic Data: National statistical offices\nFinancial Market Participation Data: National securities depositories or financial regulators\nBehavioral Data: Custom surveys or experiments conducted as part of the study\n\n\n\nPotential Approach\nWe will use a combination of quasi-experimental methods and randomized controlled trials (RCTs) to evaluate the impact of financial literacy programs. For existing programs, we will employ a difference-in-differences approach, comparing changes in financial behavior between program participants and non-participants over time.\nFor new programs, we will conduct RCTs, randomly assigning individuals or households to treatment (financial literacy program) and control groups. We will collect data on financial knowledge, attitudes, and behaviors before the program, immediately after, and at several points in the future to assess long-term effects.\nTo understand mechanisms, we will use mediation analysis, examining how changes in financial knowledge and attitudes mediate the effect of the program on financial behaviors. We will also use heterogeneity analysis to examine how program effects vary across different demographic groups.\n\n\nExpected Findings\nWe anticipate finding positive effects of financial literacy programs on savings rates and investment diversification, with stronger effects for more intensive and longer-duration programs. We expect to see heterogeneity in program effectiveness, with potentially larger impacts for individuals with lower initial financial literacy levels.\nWe also anticipate that the effects of financial literacy programs will be mediated by changes in financial attitudes and self-efficacy, in addition to increases in financial knowledge. We expect to find that different types of programs (school-based, workplace-based, community-based) may be more effective for different demographic groups.\n\n\nConclusion\nThis research will provide valuable insights into the effectiveness of financial literacy programs and their long-term impact on household financial behavior. The findings will have important implications for policymakers designing financial education initiatives, for educators developing financial literacy curricula, and for individuals seeking to improve their financial well-being.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/financial-literacy-impact\nThis repository will contain all data processing scripts, econometric models, experimental designs, visualization code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nFernandes, Daniel, John G Lynch Jr, and Richard G Netemeyer. 2014. “Financial Literacy, Financial Education, and Downstream Financial Behaviors.” Management Science 60 (8): 1861–83.\n\n\nKaiser, Tim, and Lukas Menkhoff. 2017. “Does Financial Education Impact Financial Literacy and Financial Behavior, and If so, When?” The World Bank Economic Review 31 (3): 611–30.\n\n\nKlapper, Leora, Annamaria Lusardi, and Peter Van Oudheusden. 2015. “Financial Literacy Around the World: Insights from the Standard & Poor’s Ratings Services Global Financial Literacy Survey.” World Bank.\n\n\nLusardi, Annamaria, and Olivia S Mitchell. 2014. “The Economic Importance of Financial Literacy: Theory and Evidence.” Journal of Economic Literature 52 (1): 5–44."
  },
  {
    "objectID": "rm-data/research-proposals/proposal3.html",
    "href": "rm-data/research-proposals/proposal3.html",
    "title": "The Impact of Climate Change on Agricultural Productivity: A Global Analysis",
    "section": "",
    "text": "Introduction\nClimate change poses significant challenges to global food security through its impact on agricultural productivity. This research proposal aims to quantify the effects of climate change on agricultural yields across different regions and crops, providing insights for adaptation strategies and policy interventions.\n\n\nBackground and Research Question\nAgriculture is highly sensitive to climate conditions, and changes in temperature and precipitation patterns can significantly affect crop yields (Lobell, Schlenker, and Costa-Roberts 2011). While some regions may benefit from warmer temperatures or increased CO2 levels, others face threats from extreme weather events, water scarcity, and shifting growing seasons (Rosenzweig et al. 2014).\nPrevious studies have examined the impact of climate change on agriculture at local or regional levels, but a comprehensive global analysis is needed to understand the full scope of the challenge. Moreover, the heterogeneous effects across different crops and regions need to be quantified to inform targeted adaptation strategies (Challinor et al. 2014).\nMain Research Question: How does climate change impact agricultural productivity across different regions and crops globally?\nSecondary Research Questions:\n\nWhich regions and crops are most vulnerable to climate change impacts?\nHow do adaptation measures mitigate the negative effects of climate change on agricultural productivity?\n\n\n\nPotential Data Sources\n\nAgricultural Data: FAO’s FAOSTAT database for crop yields and production\nClimate Data: World Bank’s Climate Change Knowledge Portal\nSoil Data: FAO’s Harmonized World Soil Database\nSocioeconomic Data: World Bank’s World Development Indicators\nAdaptation Measures: UNFCCC’s database on climate change adaptation measures\n\n\n\nPotential Approach\nWe will use a panel data approach, combining time-series and cross-sectional data on crop yields, climate variables, and other relevant factors across countries and regions. The main econometric model will be a fixed-effects regression, allowing us to control for time-invariant country-specific factors.\nTo address potential non-linear relationships between climate variables and crop yields, we will use flexible functional forms, such as polynomial terms or semi-parametric methods. We will also interact climate variables with indicators for adaptation measures to assess their effectiveness in mitigating climate impacts.\nTo account for spatial correlation in agricultural productivity and climate patterns, we will employ spatial econometric techniques. This will allow us to capture spillover effects and improve the precision of our estimates.\n\n\nExpected Findings\nWe anticipate finding significant negative impacts of climate change on global agricultural productivity, with substantial heterogeneity across regions and crops. We expect that regions with already warm climates and limited adaptive capacity will be most vulnerable. We also anticipate that certain adaptation measures, such as drought-resistant crop varieties or improved irrigation systems, will show effectiveness in mitigating negative impacts.\n\n\nConclusion\nThis research will provide a comprehensive global assessment of climate change impacts on agriculture, informing policy decisions on climate adaptation and food security. The findings will be crucial for identifying vulnerable regions and crops, prioritizing adaptation efforts, and developing targeted strategies to ensure global food security in the face of climate change.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/climate-change-agriculture\nThis repository will contain all data processing scripts, econometric models, visualization code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nChallinor, Andrew J, James Watson, David B Lobell, S Mark Howden, Daniel R Smith, and Netra Chhetri. 2014. “A Meta-Analysis of Crop Yield Under Climate Change and Adaptation.” Nature Climate Change 4 (4): 287–91.\n\n\nLobell, David B, Wolfram Schlenker, and Justin Costa-Roberts. 2011. “Climate Trends and Global Crop Production Since 1980.” Science 333 (6042): 616–20.\n\n\nRosenzweig, Cynthia, Joshua Elliott, Delphine Deryng, Alex C Ruane, Christoph Müller, Almut Arneth, Kenneth J Boote, et al. 2014. “Assessing Agricultural Risks of Climate Change in the 21st Century in a Global Gridded Crop Model Intercomparison.” Proceedings of the National Academy of Sciences 111 (9): 3268–73."
  },
  {
    "objectID": "rm-data/research-proposals/proposal10.html",
    "href": "rm-data/research-proposals/proposal10.html",
    "title": "For the Horde: Resource Competition and Virtual Economies in World of Warcraft",
    "section": "",
    "text": "Introduction\nWorld of Warcraft (WoW), one of the most popular massively multiplayer online role-playing games (MMORPGs), has created a complex virtual economy that mirrors many aspects of real-world economic systems. This research proposal aims to analyze the economic dynamics within WoW, focusing on resource competition, market structures, and the intersection between virtual and real-world economies.\n\n\nBackground and Research Question\nWorld of Warcraft’s virtual economy involves millions of players engaging in production, trade, and consumption of virtual goods (Castronova 2005). The game features scarce resources, a player-driven auction house, and even experiences inflation and market crashes (Dibbell 2006). This virtual economy provides a unique laboratory for studying economic behavior and testing economic theories (Castronova 2008).\nMoreover, the existence of “gold farming” - the practice of playing the game to earn virtual currency which is then sold for real money - creates interesting intersections between the virtual and real economies (Heeks 2009). This phenomenon raises questions about the nature of value and the boundaries between virtual and real economic activity.\nThe game also presents interesting scenarios of resource competition, both between players (e.g., competition for rare spawns) and between factions (Horde vs. Alliance), which can be analyzed through the lens of game theory and resource economics.\nMain Research Question: How do the economic dynamics in World of Warcraft reflect and differ from real-world economic systems, and what insights can they provide for economic theory and policy?\nSecondary Research Questions:\n\nHow does resource competition in WoW compare to real-world resource competition, and what strategies emerge?\nWhat factors influence inflation and market stability in the WoW economy?\nHow does the intersection of virtual and real economies in WoW challenge traditional notions of economic value and activity?\n\n\n\nPotential Data Sources\n\nIn-game Economic Data: Auction house prices, resource spawn rates, etc. (potentially through API access or data scraping)\nPlayer Surveys: Custom surveys on economic behavior and decision-making in WoW\nWoW Forums and Wikis: Player discussions and documentation of economic strategies\nAcademic Literature: Existing studies on virtual economies and WoW\nReal-world Economic Data: For comparison with WoW economic trends\n\n\n\nPotential Approach\nWe will employ a mixed-methods approach, combining quantitative analysis of in-game economic data with qualitative analysis of player behavior and strategies. First, we will collect and analyze time-series data on prices, trade volumes, and resource availability in the WoW economy, using econometric techniques to identify trends and patterns.\nTo study resource competition, we will use game theory models to analyze player strategies around contested resources. We will also conduct surveys and interviews with players to understand their economic decision-making processes and strategies.\nFor the intersection of virtual and real economies, we will analyze the market for “gold farming” services, examining factors that influence exchange rates between virtual and real currencies. We will also explore the legal and ethical implications of this intersection.\nTo compare WoW’s economy with real-world economies, we will use comparative analysis, looking at factors such as inflation rates, market concentration, and responses to economic shocks.\n\n\nExpected Findings\nWe anticipate finding that the WoW economy exhibits many features of real-world economies, including market cycles, inflation, and emergent economic strategies. We expect to see that resource competition in WoW leads to complex strategies that may offer insights into real-world resource economics.\nRegarding the virtual-real economy intersection, we expect to find that the value of virtual currencies is influenced by both in-game factors and real-world economic conditions, challenging traditional notions of economic value.\nWe also anticipate finding that the controlled environment of WoW allows for clearer observation of certain economic phenomena, potentially offering insights that could inform real-world economic policy and theory.\n\n\nConclusion\nThis research will provide a novel perspective on economic dynamics by examining them in a virtual world. While based on a game environment, the findings may offer valuable insights into real-world economic phenomena, particularly in areas such as resource competition, market behavior, and the increasing digitalization of economic activity. Moreover, this study could demonstrate the potential of using virtual worlds as laboratories for economic research and policy experimentation.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/warcraft-economics\nThis repository will contain all data collection scripts, economic models, visualization code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nCastronova, Edward. 2005. Synthetic Worlds: The Business and Culture of Online Games. University of Chicago Press.\n\n\n———. 2008. “A Test of the Law of Demand in a Virtual World: Exploring the Petri Dish Approach to Social Science.” CESifo Working Paper Series.\n\n\nDibbell, Julian. 2006. Play Money: Or, How i Quit My Day Job and Made Millions Trading Virtual Loot. Basic Books.\n\n\nHeeks, Richard. 2009. “Understanding \"Gold Farming\" and Real-Money Trading as the Intersection of Real and Virtual Economies.” Journal of Virtual Worlds Research 2 (4)."
  },
  {
    "objectID": "rm-data/quizes/Quiz-grade.html",
    "href": "rm-data/quizes/Quiz-grade.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Lazaroes 4 4 Joe 4 5 Emi 4 5 Chenning 4 5 Brendon 4 5 Shane 4 5 Kailin 3 0\nq4 q5\nLazaroes q5=1 q4=0\nJoe q5=5 q4=5 Emi q5=0 q4=5 Chenning q5=5 q4=5 Brendon q5=5 q4=5 Shane q5=5 q4=4 Kailin q5=4 q4=4"
  },
  {
    "objectID": "rm-data/playlist.html",
    "href": "rm-data/playlist.html",
    "title": "Data Analysis in Economics",
    "section": "",
    "text": "Econometrics Masterclass\n            \n                Based on \"Data Analysis for Economic and Policy\"\n                \n                    Episodes created using NotebookLM by Google\n                \n            \n        \n\n        \n            \n                \n            \n\n            \n                \n                \n                \n                \n                \n                    \n                \n                \n                    0:00\n                    0:00"
  },
  {
    "objectID": "rm-data/index.html",
    "href": "rm-data/index.html",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "",
    "text": "The detailed class syllabus is available here.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#syllabus",
    "href": "rm-data/index.html#syllabus",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "",
    "text": "The detailed class syllabus is available here.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#instructor-information",
    "href": "rm-data/index.html#instructor-information",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Instructor Information",
    "text": "Instructor Information\n\nInstructor: Fernando Rios-Avila\nEmail: friosavi@levy.org\nOffice Hours: Wednesdays 1:30pm to 4:00pm, or by appointment. Other times can be arranged remotely.\nClass Time: Wednesday, 9:30 am - 12:45 pm",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#course-description",
    "href": "rm-data/index.html#course-description",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Course Description",
    "text": "Course Description\nThis course focuses on providing students with the tools and skills necessary to conduct data analysis for economics and policy research. Students will be exposed to the entire process of data analysis, from formulating questions and collecting data to cleaning, exploring, analyzing, and presenting results. The course covers exploratory data analysis, regression analysis, and introduces topics on prediction with machine learning. Students will gain hands-on experience using Stata, with Quarto for reproducible reporting, and GitHub for version control and collaboration.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#course-objectives",
    "href": "rm-data/index.html#course-objectives",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Course Objectives",
    "text": "Course Objectives\nBy the end of this course, students will be able to:\n\nApply advanced data analysis techniques to economic and policy questions.\nUse modern tools such as GitHub and Quarto for research collaboration and reproducibility.\nFormulate research questions and design appropriate data collection methods.\nClean, organize, and explore data using various techniques and visualizations.\nApply regression analysis techniques to analyze relationships between variables.\nUse machine learning methods for prediction and classification tasks.\nImplement data analysis techniques using Stata.\nEffectively communicate research findings through written reports and oral presentations.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#required-textbook",
    "href": "rm-data/index.html#required-textbook",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Required Textbook",
    "text": "Required Textbook\nBékés, G., & Kézdi, G. (2021). Data Analysis for Business, Economics, and Policy. Cambridge University Press.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#software-requirements",
    "href": "rm-data/index.html#software-requirements",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Software Requirements",
    "text": "Software Requirements\n\nStata: A student license will be provided.\nQuarto: Free and open-source software for reproducible research.\nVSCode: Free and open-source code editor.\nGitHub/GitHub-Desktop: Free platform for version control and collaboration.\nZotero: Free reference manager.\n\n\n\n\n\n\n\nImportant\n\n\n\nAll homework assignments are required to be submitted in Quarto format, using GitHub repositories to submit the assignments.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#audio-podcasts",
    "href": "rm-data/index.html#audio-podcasts",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Audio Podcasts",
    "text": "Audio Podcasts\nIf you are interested in listening to an audio-podcast like summary of the chapters, you can find them here.\nThe audiofiles were created using NotebookLM by Google. The audiofiles are generated using the text from the book. The audiofiles are not perfect, but they can be useful to listen to the content of the book while you are doing other activities.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#course-outline",
    "href": "rm-data/index.html#course-outline",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Course Outline",
    "text": "Course Outline\n\nPart I: Introduction to Modern Research Tools\n\nWeek 1: Course Overview and Tools Setup\n\nIntroduction to GitHub and Quarto\nData Organization and Management\nSlides\nHomework 1\n\n\n\n\nPart II: Data Analysis and Exploration\n\nWeek 2: Introduction to Data Analysis\n\nData Collection and Preparation\nTidy Data Principles\nReading: Békés & Kézdi (2021), Chapters 1-2\nSlides\nHomework 2\n\n\n\nWeek 3: Data Exploration\n\nExploratory Data Analysis Techniques\nData Cleaning and Tidy Data Principles\nReading: Békés & Kézdi (2021), Chapters 3-4\nSlides\nHomework 3\n\n\n\n\nPart III: Generalization and Regression Analysis\n\nWeek 4: Generalization: From Sample to Population\n\nSampling and Hypothesis Testing\nConfidence Intervals and Errors\nReading: Békés & Kézdi (2021), Chapters 5-6\nSlides\nHomework 4\n\n\n\nWeeks 5-6: Regression Analysis I: Simple Regression\n\nDate: October 9 and 16\nLinear Regression and Causality\nModel Assumptions and Transformations\nReading: Békés & Kézdi (2021), Chapters 7-9\nSlides\nHomework 5\nHomework 6\nAudio Summary v1. Chapter 7\n\n\n\nWeek 7: Regression Analysis II: Multiple Regression\n\nDate: October 18 (Make-up class)\nEstimation and Inference\nInteractions and Non-linearities\nReading: Békés & Kézdi (2021), Chapter 10\nSlides\nHomework 7\n\n\n\nWeek 8: Regression Analysis III: Modeling Probabilities\n\nDate: October 23\nLogit and Probit Models\nInterpretation and Predictive Power\nReading: Békés & Kézdi (2021), Chapter 11\nSlides\nHomework 8\n\n\n\n\nPart IV: Advanced Topics\n\nWeek 9: Time Series Analysis\n\nDate: October 30\nTrend, Seasonality, and Stationarity\nReading: Békés & Kézdi (2021), Chapter 12\nSlides\nHomework 9\n\n\n\nWeek 10: Prediction\n\nDate: November 6\nModel Fit and Cross-validation\nReading: Békés & Kézdi (2021), Chapter 13\nSlides\nHomework 10\n\n\n\nWeek 11: Model Building for Prediction: LASSO\n\nDate: November 13\nLASSO for Prediction and Diagnosis\nReading: Békés & Kézdi (2021), Chapter 14\nSlides\nHomework 11\n\n\n\nWeek 12: Predicting Probabilities and Classification\n\nDate: November 20\nClassification Techniques and ROC Curves\nReading: Békés & Kézdi (2021), Chapter 17\nSlides\nHomework 12\n\n\n\nWeek 13: Forecasting Data\n\nDate: November 27 (TBD)\nARIMA and Forecasting Techniques\nReading: Békés & Kézdi (2021), Chapter 18\nSlides\nHomework 13",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#grading-policy",
    "href": "rm-data/index.html#grading-policy",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Grading Policy",
    "text": "Grading Policy\n\nWeekly Quizzes: 10%\nWeekly Problem Sets: 30%\nTerm Paper: 60%\n\n\nTerm Paper Schedule (60% of final grade)\n\nPart I: Research Proposal (5%, due Week 2). See here for few examples of research proposals.\nPart II: Data Collection and Cleaning (10%, due Week 4)\nInterim Progress Report (5%, due Week 7): October 18\nPeer Review Report: (Week 8): October 23\nPart III: Data Analysis (15%, due Week 10): November 6\nPeer Review Report: (Week 11): November 13\nPart IV: (15%) Final Report: December 2\nPresentation (5%) due Last Date of Class: December 4 or 11\n\nComplete research paper should include:\n\nIntroduction\nLiterature Review\nData and Methodology\nRobustness Checks or Sensitivity/Sub-group Analysis\nConclusion\nReferences\nAppendices (if any)\nPresentation (5%)\n\n15-minute presentation of your research to the class\n\nSee here for an example for the kind of report expected at each stage, based on the first research proposal on the impact of remote work on urban housing prices.\n\n\n\nAdditional Requirements\n\nYou should submit a PDF of your report by the deadline, along with the Github repository link\nYour GitHub repository should include all code, data, and the Quarto document for your report\nAt each stage, you should submit your work to GitHub to follow the progress of your project",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#resources",
    "href": "rm-data/index.html#resources",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Resources",
    "text": "Resources\n\nTextbook: Békés, G., & Kézdi, G. (2021). Data Analysis for Business, Economics, and Policy. Cambridge University Press. Additional resources are available on the book’s website: Data Analysis",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#course-policies",
    "href": "rm-data/index.html#course-policies",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance: Attendance is highly recommended. Classes will not be recorded, and except for exceptional cases, there will be no online classes.\nLate Assignments: Late assignments will not be accepted unless prior arrangements have been made with the instructor.\nAcademic Integrity: All work submitted must be your own. Plagiarism will not be tolerated and will result in a failing grade for the assignment or course.\nAI Usage: The use of AI in the class is allowed. However, you must disclose any AI tools used in your assignments. AI is a tool you can use to generate ideas, edit your text, provide help with coding, etc. However, it is completely unacceptable to use AI to generate the entire assignment. You will have to be able to explain and defend your work in class.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/hw-material/report7.html",
    "href": "rm-data/hw-material/report7.html",
    "title": "The Impact of House Allegiances on Power Dynamics in Westeros",
    "section": "",
    "text": "This report examines the intricate power dynamics in Westeros, as depicted in George R.R. Martin’s “A Song of Ice and Fire” series and its television adaptation, “Game of Thrones.” We will explore how house allegiances shape the political landscape, analyze key alliances, and discuss their implications for the struggle for the Iron Throne."
  },
  {
    "objectID": "rm-data/hw-material/report7.html#house-allegiances-and-power-distribution",
    "href": "rm-data/hw-material/report7.html#house-allegiances-and-power-distribution",
    "title": "The Impact of House Allegiances on Power Dynamics in Westeros",
    "section": "House Allegiances and Power Distribution",
    "text": "House Allegiances and Power Distribution\nThe distribution of power in Westeros can be modeled using a modified version of the Lanchester equations, which originally described the relative strengths of military forces. In our context, we adapt this to represent the power dynamics between major houses:\n\\[\n\\frac{dR}{dt} = -\\alpha L, \\quad \\frac{dL}{dt} = -\\beta R\n\\tag{1}\\]\nWhere \\(R\\) and \\(L\\) represent the strength of rival houses (e.g., Stark and Lannister), and \\(\\alpha\\) and \\(\\beta\\) are coefficients representing the effectiveness of each house’s strategy and resources."
  },
  {
    "objectID": "rm-data/hw-material/report7.html#visualization-of-house-alliances",
    "href": "rm-data/hw-material/report7.html#visualization-of-house-alliances",
    "title": "The Impact of House Allegiances on Power Dynamics in Westeros",
    "section": "Visualization of House Alliances",
    "text": "Visualization of House Alliances\nTo better understand the complex web of alliances in Westeros, we’ve created a network graph representing the relationships between major houses throughout the series.\n\n\n\n\n\n\n\n\nFigure 1: Network of Major House Alliances in Westeros\n\n\n\n\n\nFigure Figure 1 illustrates the complex network of alliances between major houses in Westeros. The connections between houses play a crucial role in determining the balance of power, as discussed in Martin (2011)."
  },
  {
    "objectID": "rm-data/hw-material/report7.html#key-factors-influencing-house-power",
    "href": "rm-data/hw-material/report7.html#key-factors-influencing-house-power",
    "title": "The Impact of House Allegiances on Power Dynamics in Westeros",
    "section": "Key Factors Influencing House Power",
    "text": "Key Factors Influencing House Power\nSeveral factors contribute to a house’s overall power and influence in Westeros. Table Table 1 summarizes these key elements:\n\n\n\nTable 1: Key Factors Influencing House Power in Westeros\n\n\n\n\n\nFactor\nDescription\nImpact\n\n\n\n\nMilitary Strength\nSize and training of armies\nHigh\n\n\nEconomic Resources\nWealth and control over trade\nHigh\n\n\nPolitical Alliances\nRelationships with other houses\nMedium\n\n\nDragons\nPossession of dragons (Targaryen-specific)\nVery High\n\n\n\n\n\n\nAs seen in Table Table 1, military strength and economic resources are crucial for maintaining power. However, the reintroduction of dragons by House Targaryen significantly alters the balance, as noted in Equation Equation 11."
  },
  {
    "objectID": "rm-data/hw-material/report7.html#footnotes",
    "href": "rm-data/hw-material/report7.html#footnotes",
    "title": "The Impact of House Allegiances on Power Dynamics in Westeros",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe presence of dragons could be represented by an additional term in the Lanchester equations, significantly increasing the α coefficient for the house possessing them.↩︎"
  },
  {
    "objectID": "rm-data/hw-material/report5.html",
    "href": "rm-data/hw-material/report5.html",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "",
    "text": "This report examines the influence of Eric Ugland’s “The Good Guys” series on contemporary fantasy literature. We will explore the unique elements of Ugland’s work, its reception among readers, and its impact on the genre as a whole. The analysis will include quantitative data on book sales, a comparison with other works in the genre, and insights from literary critics."
  },
  {
    "objectID": "rm-data/hw-material/report5.html#uglands-narrative-formula",
    "href": "rm-data/hw-material/report5.html#uglands-narrative-formula",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "Ugland’s Narrative Formula",
    "text": "Ugland’s Narrative Formula\nOne of the key factors contributing to the success of “The Good Guys” series is Ugland’s innovative approach to character progression. This can be represented by the following equation:\n\\[\nP = (E \\times S) + (L \\times C)\n\\tag{1}\\]\nWhere P represents character progression, E is experience gained, S is skill level, L is luck factor, and C is character choices. This formula Equation 1 encapsulates Ugland’s balance between traditional RPG elements and character-driven storytelling."
  },
  {
    "objectID": "rm-data/hw-material/report5.html#sales-performance",
    "href": "rm-data/hw-material/report5.html#sales-performance",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "Sales Performance",
    "text": "Sales Performance\nThe series’ popularity can be visualized through its sales performance over time:\n\n\n\n\n\n\n\n\nFigure 1: Monthly sales of ‘The Good Guys’ series over two years\n\n\n\n\n\nAs shown in Figure 1, the series has experienced steady growth in sales, with periodic spikes coinciding with new book releases."
  },
  {
    "objectID": "rm-data/hw-material/report5.html#genre-comparison",
    "href": "rm-data/hw-material/report5.html#genre-comparison",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "Genre Comparison",
    "text": "Genre Comparison\nTo contextualize the success of “The Good Guys,” we can compare its key metrics with other popular fantasy series:\n\n\n\nTable 1: Comparison of popular LitRPG series\n\n\n\n\n\nSeries\nAvg. Rating\nBooks Published\nTotal Sales (millions)\n\n\n\n\nThe Good Guys\n4.6\n11\n2.5\n\n\nCradle\n4.7\n11\n3.0\n\n\nThe Land\n4.5\n8\n2.0\n\n\n\n\n\n\nThe data in Table 1 demonstrates that “The Good Guys” holds its own against other well-established series in the genre."
  },
  {
    "objectID": "rm-data/hw-material/report5.html#critical-reception",
    "href": "rm-data/hw-material/report5.html#critical-reception",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "Critical Reception",
    "text": "Critical Reception\nLiterary critics have praised Ugland’s work for its fresh take on the LitRPG genre. According to Johnson (2022), “Ugland’s ‘The Good Guys’ series represents a significant evolution in LitRPG storytelling, blending traditional fantasy elements with modern gaming concepts in a uniquely engaging way.”1\nThe series has also been noted for its contribution to the broader fantasy genre. Smith (2023) argues that “The Good Guys” has “pushed the boundaries of what readers expect from fantasy literature, potentially influencing the direction of the genre for years to come.”"
  },
  {
    "objectID": "rm-data/hw-material/report5.html#footnotes",
    "href": "rm-data/hw-material/report5.html#footnotes",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis praise is particularly noteworthy given the often-skeptical reception of LitRPG works by mainstream literary critics.↩︎"
  },
  {
    "objectID": "rm-data/hw-material/report3.html",
    "href": "rm-data/hw-material/report3.html",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "",
    "text": "Dungeons and Dragons (D&D) is a popular tabletop role-playing game that has captivated players for decades. This report explores the mathematical underpinnings of D&D, focusing on the probability distributions of dice rolls and their impact on gameplay. We will examine the statistical nature of character abilities, combat outcomes, and skill checks, providing insights into the game’s mechanics through equations, data visualization, and tabular analysis."
  },
  {
    "objectID": "rm-data/hw-material/report3.html#the-probability-of-adventure",
    "href": "rm-data/hw-material/report3.html#the-probability-of-adventure",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "The Probability of Adventure",
    "text": "The Probability of Adventure\nAt the heart of D&D lies the rolling of dice, particularly the iconic twenty-sided die (d20). The probability of rolling any number on a d20 is uniform, but the outcomes of these rolls can be modified by character abilities and situational modifiers. The probability of success for any given action can be expressed as:\n\\[\nP(success) = \\frac{21 - (DC - modifier)}{20}\n\\tag{1}\\]\nWhere DC is the Difficulty Class of the task, and the modifier is the character’s relevant skill or ability modifier. This equation (Equation 1) forms the foundation of many D&D mechanics1."
  },
  {
    "objectID": "rm-data/hw-material/report3.html#visualizing-character-ability-scores",
    "href": "rm-data/hw-material/report3.html#visualizing-character-ability-scores",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "Visualizing Character Ability Scores",
    "text": "Visualizing Character Ability Scores\nCharacter creation in D&D often involves rolling dice to determine ability scores. The most common method is rolling 4d6 and dropping the lowest die. Let’s visualize the distribution of these rolls:\n\n\n\n\n\n\n\n\nFigure 1: Distribution of D&D Ability Scores (4d6 drop lowest)\n\n\n\n\n\nFigure Figure 1 illustrates the distribution of ability scores using the 4d6 drop lowest method. This bell-shaped curve demonstrates why most characters have average abilities, with exceptional scores being rare."
  },
  {
    "objectID": "rm-data/hw-material/report3.html#combat-outcomes",
    "href": "rm-data/hw-material/report3.html#combat-outcomes",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "Combat Outcomes",
    "text": "Combat Outcomes\nCombat in D&D involves a complex interplay of attack rolls, damage calculations, and defensive abilities. Table Table 1 summarizes the average damage output for different weapon types:\n\n\n\nTable 1: Average Damage Output by Weapon Type\n\n\n\n\n\nWeapon Type\nAverage Damage\nCritical Hit Chance\n\n\n\n\nDagger\n2.5\n5%\n\n\nLongsword\n4.5\n5%\n\n\nGreataxe\n6.5\n5%\n\n\n\n\n\n\nAs shown in Table 1, weapon choice significantly impacts potential damage output, with larger weapons generally dealing more damage at the cost of other factors like weight and required strength."
  },
  {
    "objectID": "rm-data/hw-material/report3.html#the-role-of-randomness",
    "href": "rm-data/hw-material/report3.html#the-role-of-randomness",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "The Role of Randomness",
    "text": "The Role of Randomness\nWhile skill and strategy play crucial roles in D&D, the element of chance introduced by dice rolls adds excitement and unpredictability to the game. According to Tormey (2019), this balance between player agency and random chance is what makes D&D both challenging and engaging. The interplay between player decisions and dice rolls creates a unique narrative experience in each game session."
  },
  {
    "objectID": "rm-data/hw-material/report3.html#footnotes",
    "href": "rm-data/hw-material/report3.html#footnotes",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis simplified equation assumes a linear probability distribution and does not account for critical successes or failures, which are typically represented by rolling a natural 20 or 1, respectively.↩︎"
  },
  {
    "objectID": "rm-data/hw-material/report1.html",
    "href": "rm-data/hw-material/report1.html",
    "title": "The Impact of Resource Management in StarCraft: A Strategic Analysis",
    "section": "",
    "text": "This report examines the crucial role of resource management in the popular real-time strategy game StarCraft. We will explore how effective resource allocation influences gameplay dynamics and strategic decision-making. The analysis will include a mathematical model of resource gathering, a visualization of unit production rates, and a comparison of resource types across different races."
  },
  {
    "objectID": "rm-data/hw-material/report1.html#mathematical-model-of-resource-gathering",
    "href": "rm-data/hw-material/report1.html#mathematical-model-of-resource-gathering",
    "title": "The Impact of Resource Management in StarCraft: A Strategic Analysis",
    "section": "Mathematical Model of Resource Gathering",
    "text": "Mathematical Model of Resource Gathering\nIn StarCraft, the rate of resource accumulation can be modeled using a simple differential equation. If we denote the amount of resources as \\(R\\) and time as \\(t\\), we can express the rate of change of resources as:\n\\[\n\\frac{dR}{dt} = \\alpha N - \\beta P\n\\tag{1}\\]\nWhere \\(\\alpha\\) is the gathering rate per worker, \\(N\\) is the number of workers, \\(\\beta\\) is the consumption rate, and \\(P\\) is the production rate of units or structures. This model, as shown in Equation 1, forms the basis of the game’s economic system (Choi and Kim 2015)."
  },
  {
    "objectID": "rm-data/hw-material/report1.html#unit-production-rates",
    "href": "rm-data/hw-material/report1.html#unit-production-rates",
    "title": "The Impact of Resource Management in StarCraft: A Strategic Analysis",
    "section": "Unit Production Rates",
    "text": "Unit Production Rates\nTo illustrate the impact of resource management on unit production, we’ve created a visualization of unit production rates for different races in StarCraft.\n\n\n\n\n\n\n\n\nFigure 1: Unit Production Rates by Race in StarCraft\n\n\n\n\n\nAs shown in Figure 1, the Zerg race has the highest unit production rate, reflecting their swarm-based strategy. This aligns with the game’s design philosophy, where each race has unique strengths and weaknesses1."
  },
  {
    "objectID": "rm-data/hw-material/report1.html#resource-types-comparison",
    "href": "rm-data/hw-material/report1.html#resource-types-comparison",
    "title": "The Impact of Resource Management in StarCraft: A Strategic Analysis",
    "section": "Resource Types Comparison",
    "text": "Resource Types Comparison\nStarCraft features two primary resource types: minerals and vespene gas. Their availability and usage vary across races:\n\n\n\nTable 1: Resource Usage by Race\n\n\n\n\n\nRace\nMineral Usage\nGas Usage\nResource Dependency\n\n\n\n\nTerran\nHigh\nMedium\nBalanced\n\n\nProtoss\nMedium\nHigh\nGas-heavy\n\n\nZerg\nHigh\nLow\nMineral-heavy\n\n\n\n\n\n\nTable 1 illustrates how different races prioritize resources, influencing their strategic options and tech progression paths."
  },
  {
    "objectID": "rm-data/hw-material/report1.html#footnotes",
    "href": "rm-data/hw-material/report1.html#footnotes",
    "title": "The Impact of Resource Management in StarCraft: A Strategic Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis design approach contributes to StarCraft’s enduring popularity in esports and casual gaming circles.↩︎"
  },
  {
    "objectID": "rm-data/hw-material/pdf-template.html",
    "href": "rm-data/hw-material/pdf-template.html",
    "title": "Template for PDF",
    "section": "",
    "text": "Use this as the YAML template for your PDF files.\nMake sure you change “Your Title” to the title of your document and “author” to the name of the author.\n---\ntitle: \"Your Title\"\nformat: \n    pdf:\n        documentclass: article \n        number-sections: true\n        margin-top: 1in\n        margin-bottom: 1in\n        margin-left: 1in\n        margin-right: 1in\n        linestretch:  1.5\n        fontsize: 11pt\n    html: default \nauthor: \"author\"        \nbibliography: references.bib\n---"
  },
  {
    "objectID": "rm-data/example_proposals.html",
    "href": "rm-data/example_proposals.html",
    "title": "Example of Research Proposals",
    "section": "",
    "text": "This page contains few examples of research proposals. The goal is to give you an idea of what expect your research proposal to look like for the term paper.\n\nThe Impact of Remote Work on Urban Housing Prices link\nThe Effect of Social Media Sentiment on Stock Market Volatility link\nThe Impact of Climate Change on Agricultural Productivity: A Global Analysis link\nThe Economic Impact of Artificial Intelligence Adoption in Small and Medium Enterprises link\nThe Impact of Financial Literacy Programs on Household Savings and Investment Behavior link\nThe Economics of Springfield: A Case Study of Resource Allocation in The Simpson link\nThe Iron Bank Always Collects: A Study of Debt and Financial Institutions in Game of Thrones link\nThe Economics of the Matrix: Scarcity, Choice, and Human Capital in a Simulated Reality link\nTo Boldly Go: Resource Allocation and Post-Scarcity Economics in Star Trek link\nFor the Horde: Resource Competition and Virtual Economies in World of Warcraft link"
  },
  {
    "objectID": "quizes/quiz_2.html",
    "href": "quizes/quiz_2.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Name: ________________________________________________________\n\nConsider the following regression equation: \\(y=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+u\\). What does \\(\\beta_1\\) imply?\n\n\nIt measures the ceteris paribus effect of \\(x_1\\) on \\(x_2\\) .\nIt measures the ceteris paribus effect of \\(y\\) on \\(x_1\\)\nIt measures the ceteris paribus effect of \\(x_1\\) on \\(y\\)\nIt measures the ceteris paribus effect of \\(x_1\\) on \\(u\\)\n\n\nIn econometrics, the general partialling out result is usually called the _____.\n\n\nGauss-Markov assumption\nBest linear unbiased estimator\nFrisch-Waugh-Lovell theorem\nGauss-Markov theorem\n\n\nIf an independent variable in a multiple linear regression model is an exact linear combination of other independent variables, the model suffers from the problem of _____.\n\n\nperfect collinearity\nhomoskedasticity\nheteroskedasticty\nomitted variable bias\n\n\nThe term “linear” in a multiple linear regression model means that the equation is linear in parameters, not in terms of variables.\n\n\nTrue\nFalse\n\n\nThe coefficient of determination (R2) decreases when an independent variable is added to a multiple regression model.\n\n\nTrue\nFalse"
  },
  {
    "objectID": "quizes/quiz_2.html#quiz-2",
    "href": "quizes/quiz_2.html#quiz-2",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Name: ________________________________________________________\n\nConsider the following regression equation: \\(y=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+u\\). What does \\(\\beta_1\\) imply?\n\n\nIt measures the ceteris paribus effect of \\(x_1\\) on \\(x_2\\) .\nIt measures the ceteris paribus effect of \\(y\\) on \\(x_1\\)\nIt measures the ceteris paribus effect of \\(x_1\\) on \\(y\\)\nIt measures the ceteris paribus effect of \\(x_1\\) on \\(u\\)\n\n\nIn econometrics, the general partialling out result is usually called the _____.\n\n\nGauss-Markov assumption\nBest linear unbiased estimator\nFrisch-Waugh-Lovell theorem\nGauss-Markov theorem\n\n\nIf an independent variable in a multiple linear regression model is an exact linear combination of other independent variables, the model suffers from the problem of _____.\n\n\nperfect collinearity\nhomoskedasticity\nheteroskedasticty\nomitted variable bias\n\n\nThe term “linear” in a multiple linear regression model means that the equation is linear in parameters, not in terms of variables.\n\n\nTrue\nFalse\n\n\nThe coefficient of determination (R2) decreases when an independent variable is added to a multiple regression model.\n\n\nTrue\nFalse"
  },
  {
    "objectID": "quarto/table1.html",
    "href": "quarto/table1.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Male\nFemale\n\n\n\n\nN\n759 (46.1%)\n888 (53.9%)\n\n\nlog hourly wages\n3.440 (0.479)\n3.267 (0.570)\n\n\nyears of education\n11.800 (2.444)\n11.060 (2.260)\n\n\nyears of work experience\n14.077 (11.180)\n12.138 (8.327)\n\n\nyears of job tenure\n9.003 (9.061)\n6.605 (6.727)\n\n\nage of respondent\n38.516 (11.341)\n39.884 (10.727)\n\n\nMarital Status\n\n\n\n\nSingle\n297 (39.1%)\n268 (30.2%)\n\n\nMarried\n397 (52.3%)\n465 (52.4%)\n\n\nDivorced\n65 (8.6%)\n155 (17.5%)"
  },
  {
    "objectID": "quarto/regress.html",
    "href": "quarto/regress.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "(1)\n(2)\n(3)\n(4)\n\n\n\n\nyears of education\n0.0885***\n0.0794***\n0.0554***\n0.0854***\n\n\n\n(0.00519)\n(0.00522)\n(0.00613)\n(0.00876)\n\n\nyears of work\n0.0153***\n0.00399*\n-0.00483*\n0.00874*\n\n\nexperience\n(0.00126)\n(0.00188)\n(0.00203)\n(0.00356)\n\n\nyears of job tenure\n\n0.00407*\n-0.000553\n0.00150\n\n\n\n\n(0.00196)\n(0.00212)\n(0.00368)\n\n\nage of respondent\n\n0.0114***\n0.0249***\n0.00576*\n\n\n\n\n(0.00174)\n(0.00224)\n(0.00275)\n\n\nConstant\n2.136***\n1.915***\n1.903***\n1.965***\n\n\n\n(0.0654)\n(0.0727)\n(0.0784)\n(0.126)\n\n\nObservations\n1434\n1434\n751\n683"
  },
  {
    "objectID": "Projectos_Ecuador.html",
    "href": "Projectos_Ecuador.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Idea paper Migracion Pobreza Ecuador 5-7K\nComo identificar?\nComo impacto Migracion en Pobreza? 2007-2022 Que lugares recibieron mas migracion?\nMecanismos\nComo afecto pobreza? sector? servicios? Violencia\nAsset? wealth indirect?\n\nTiempos? 1 semestre… Research agenda.\n\nShift Share -&gt; Q3-Q4 ()\n\n\nBid -&gt; Mineria Ilegal Pobreza Colombia : Posible?"
  },
  {
    "objectID": "mathref/math_2.html#vectors",
    "href": "mathref/math_2.html#vectors",
    "title": "Math Refresher",
    "section": "Vectors",
    "text": "Vectors\nA vector is a list of numbers. We can think of a vector as a point in space, or as an arrow pointing from the origin to that point. For example, the vector\n\\[\\vec{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\]\nis a vector in \\(\\mathbb{R}^3\\) (three-dimensional space) that points from the origin to the point \\((1, 2, 3)\\).",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#section",
    "href": "mathref/math_2.html#section",
    "title": "Math Refresher",
    "section": "",
    "text": "We can add vectors together by adding their corresponding elements. For example,\n\\[\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 7 \\\\ 9 \\end{bmatrix}\\]\nWe can also multiply a vector by a scalar (a single number) by multiplying each element of the vector by that number. For example,\n\\[2 \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \\end{bmatrix}\n\\]",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#matrices",
    "href": "mathref/math_2.html#matrices",
    "title": "Math Refresher",
    "section": "Matrices",
    "text": "Matrices\nA matrix is a two-dimensional array of numbers. We can think of a matrix as a list of vectors. For example, the matrix:\n\\[A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} =\n\\begin{bmatrix} 1 \\\\ 4  \\end{bmatrix},\n\\begin{bmatrix} 2 \\\\ 5  \\end{bmatrix},\n\\begin{bmatrix} 3 \\\\ 6  \\end{bmatrix}\n\\]\nis a matrix that concatenates 3 \\(\\mathbb{R}^2\\) vectors together.\nMatrices can have different dimensions. For example, the matrix:\n\\[B = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\]\nis a square matrix that concatenates 3 \\(\\mathbb{R}^3\\) vectors together.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#matrix-dimensions",
    "href": "mathref/math_2.html#matrix-dimensions",
    "title": "Math Refresher",
    "section": "Matrix Dimensions",
    "text": "Matrix Dimensions\nMatrices are often denoted by their dimensions.\n\nFor example, the matrix \\(A\\) above is a \\(2 \\times 3\\) matrix, because it has 2 rows and 3 columns.\n\nThe matrix \\(B\\) above is a \\(3 \\times 3\\) matrix, because it has 3 rows and 3 columns.\n\nIn general, we can denote a matrix \\(M\\) with \\(r\\) rows and \\(c\\) columns as an \\(r \\times c\\) matrix.\n\nFor Notation, I will usually refer to this like \\(M_{r \\times c}\\). In this case we have \\(A_{2 \\times 3}\\) and \\(B_{3 \\times 3}\\).\nWe can denote the element in the \\(i\\)th row and \\(j\\)th column of \\(M\\) as \\(M_{ij}\\). For example, the element in the 2nd row and 3rd column of \\(B\\) is \\(B_{23} = 6\\).",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#stata-and-matrices",
    "href": "mathref/math_2.html#stata-and-matrices",
    "title": "Math Refresher",
    "section": "Stata and Matrices",
    "text": "Stata and Matrices\nStata has a powerful matrix algebra language called mata. We can define matrices in mata using the following syntax:\n\n\n\n\n\n\n. mata\n------------------------------------------------- mata (type end to exit) -----\n:  vv1 = (1\\2\\3)\n\n:  vv2 = (1,2,3)\n\n:  a = (1,2,3 \\ 4,5,6);a\n       1   2   3\n    +-------------+\n  1 |  1   2   3  |\n  2 |  4   5   6  |\n    +-------------+\n\n:  a = (1\\4),(2\\5),(3\\6);a\n       1   2   3\n    +-------------+\n  1 |  1   2   3  |\n  2 |  4   5   6  |\n    +-------------+\n\n:  a[1,3]\n  3\n\n: end\n-------------------------------------------------------------------------------\n\n.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#section-1",
    "href": "mathref/math_2.html#section-1",
    "title": "Math Refresher",
    "section": "",
    "text": ". mata\n------------------------------------------------- mata (type end to exit) -----\n:     (1\\2\\3)+(4\\5\\6)\n       1\n    +-----+\n  1 |  5  |\n  2 |  7  |\n  3 |  9  |\n    +-----+\n\n:     2*(1\\2\\3)\n       1\n    +-----+\n  1 |  2  |\n  2 |  4  |\n  3 |  6  |\n    +-----+\n\n: end\n-------------------------------------------------------------------------------\n\n.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#special-matrices",
    "href": "mathref/math_2.html#special-matrices",
    "title": "Math Refresher",
    "section": "Special Matrices",
    "text": "Special Matrices\n\nThere are a few special matrices that we will use often. The zero matrix is a matrix where all of the elements are 0. For example, the zero matrix with 2 rows and 3 columns is:\n\n\\[Zero=\\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\\]\nmata: J(2,3,0)\nA square matrix is a matrix where the number of rows is equal to the number of columns. For example, \\(B\\) is a square matrix.\nmata: J(3,3,0)",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#special-matrices-1",
    "href": "mathref/math_2.html#special-matrices-1",
    "title": "Math Refresher",
    "section": "Special Matrices",
    "text": "Special Matrices\nThe identity matrix is a square matrix where all of the elements are 0, except for the elements along the diagonal, which are 1. For example, the identity matrix with 3 rows and 3 columns is:\n\\[I_{3}=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\nmata: I(3)\nFor simplicitly, we will use the subscript to denote the size of the identity matrix. For example, \\(I_{3}\\) is a 3x3 identity matrix, and \\(I_{5}\\) is a 5x5 identity matrix.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#special-matrices-2",
    "href": "mathref/math_2.html#special-matrices-2",
    "title": "Math Refresher",
    "section": "Special Matrices",
    "text": "Special Matrices\nA \\(1\\times c\\) matrix is called a row vector. Wheras a \\(r \\times 1\\) matrix is called a column vector.\nA diagonal matrix is a square matrix where all of the elements off the diagonal are 0. For example, the following matrix is a diagonal matrix:\n\\[\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix}\\]\nThe identify matrix is a special case of a diagonal matrix.\nmata: diag( (1,2,3) ) or mata: diag( (1\\2\\3) )",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#matrix-operations",
    "href": "mathref/math_2.html#matrix-operations",
    "title": "Math Refresher",
    "section": "Matrix Operations",
    "text": "Matrix Operations\nWe can add matrices together by adding their corresponding elements. For example,\n\\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} + \\begin{bmatrix} 7 & 8 & 9 \\\\ 10 & 11 & 12 \\end{bmatrix} = \\begin{bmatrix} 8 & 10 & 12 \\\\ 14 & 16 & 18 \\end{bmatrix}\\]\nHowever, both matrices must have the same dimensions. For example, we cannot add the following matrices together:\n\\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}_{2\\times 3} + \\begin{bmatrix} 7 & 8 \\\\ 10 & 11 \\end{bmatrix}_{2\\times 2}\\]\nmata: a + b Will work if a and b have the same dimensions.\nmata will throw an error if you try to add matrices of different dimensions.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#matrix-scalar-multiplication",
    "href": "mathref/math_2.html#matrix-scalar-multiplication",
    "title": "Math Refresher",
    "section": "Matrix Scalar Multiplication",
    "text": "Matrix Scalar Multiplication\nWe can multiply a matrix by a scalar by multiplying each element of the matrix by that scalar. For example,\n\\[a \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} = \\begin{bmatrix} 1a & 2a & 3a \\\\ 4a & 5a & 6a \\end{bmatrix}\\]\n\n\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n: 2 * (1,2,3 \\ 4,5,6)\n        1    2    3\n    +----------------+\n  1 |   2    4    6  |\n  2 |   8   10   12  |\n    +----------------+\n\n: end\n-------------------------------------------------------------------------------\n\n.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#matrix-multiplication",
    "href": "mathref/math_2.html#matrix-multiplication",
    "title": "Math Refresher",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication\nWe can multiple two matrices together by taking the dot product of each row of the first matrix with each column of the second matrix. For example:\n\\[\\begin{aligned}\n\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}_{2\\times 3} \\begin{bmatrix} 7 & 8 \\\\ 10 & 11 \\\\ 13 & 14 \\end{bmatrix}_{3\\times 2} &= \\begin{bmatrix} 1*7 + 2*10 + 3*13 & 1*8 + 2*11 + 3*14 \\\\ 4*7 + 5*10 + 6*13 & 4*8 + 5*11 + 6*14 \\end{bmatrix}_{2\\times 2} \\\\\n&= \\begin{bmatrix} 66 & 72 \\\\ 156 & 171 \\end{bmatrix}\n\\end{aligned}\n\\]\nA good way of remembering this is to follow the flow: \\({\\rightarrow  \\times \\downarrow}\\)\nmata: a = (1,2,3 \\ 4,5,6) ; b = (7,8 \\ 10,11 \\ 13,14); a*b\n\n\n         1     2\n    +-------------+\n  1 |   66    72  |\n  2 |  156   171  |\n    +-------------+",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#matrix-multiplication-1",
    "href": "mathref/math_2.html#matrix-multiplication-1",
    "title": "Math Refresher",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication\nNote that the number of columns in the first matrix must be equal to the number of rows in the second matrix.\n\\[M_{a \\times b} \\times N_{b \\times c} = P_{a \\times c}\\]\nFor example, we cannot multiply the following matrices together:\n\\[\\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{pmatrix}_{2\\times 3} \\begin{pmatrix} 7 & 8 \\\\ 10 & 11 \\end{pmatrix}_{2\\times 2}\\]",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#section-2",
    "href": "mathref/math_2.html#section-2",
    "title": "Math Refresher",
    "section": "",
    "text": "Some properties of matrix multiplication:\n\nMatrix multiplication is not commutative. That is, \\(AB \\neq BA\\) in general.\n\n\n\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n: a*b\n         1     2\n    +-------------+\n  1 |   66    72  |\n  2 |  156   171  |\n    +-------------+\n\n: b*a\n[symmetric]\n         1     2     3\n    +-------------------+\n  1 |   39              |\n  2 |   54    75        |\n  3 |   69    96   123  |\n    +-------------------+\n\n: end\n-------------------------------------------------------------------------------\n\n.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#section-3",
    "href": "mathref/math_2.html#section-3",
    "title": "Math Refresher",
    "section": "",
    "text": "Matrix multiplication is associative. That is, \\(A(BC) = (AB)C\\).\n\nmata: c=(4,1\\2,4)\n\n\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:  c=(4,1\\2,4)\n\n:  a*(b*c) ; (a*b)*c\n         1     2\n    +-------------+\n  1 |  408   354  |\n  2 |  966   840  |\n    +-------------+\n         1     2\n    +-------------+\n  1 |  408   354  |\n  2 |  966   840  |\n    +-------------+\n\n: end\n-------------------------------------------------------------------------------\n\n.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#section-4",
    "href": "mathref/math_2.html#section-4",
    "title": "Math Refresher",
    "section": "",
    "text": "Any matrix multiplied by \\(I\\) is equal to itself. That is, \\(AI = IA = A\\).\n\n\n\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n: a*I(3)\n       1   2   3\n    +-------------+\n  1 |  1   2   3  |\n  2 |  4   5   6  |\n    +-------------+\n\n: b*I(2)\n        1    2\n    +-----------+\n  1 |   7    8  |\n  2 |  10   11  |\n  3 |  13   14  |\n    +-----------+\n\n: end\n-------------------------------------------------------------------------------\n\n.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#transpose",
    "href": "mathref/math_2.html#transpose",
    "title": "Math Refresher",
    "section": "Transpose",
    "text": "Transpose\nThe transpose of a matrix is a matrix where the rows and columns are swapped. For example, if the matrix \\(A\\) is defined as:\n\\[A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\]\nthen the transpose of \\(A\\), denoted \\(A^T\\), is:\n\\[A^T = \\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}\\]\nNote that if \\(A_{a\\times b}\\), then \\(A^T_{b\\times a}\\).\nmata: a_t = a'",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#section-5",
    "href": "mathref/math_2.html#section-5",
    "title": "Math Refresher",
    "section": "",
    "text": "Some properties of the transpose:\n\n\\((A^T)^T = A\\)\n\\((AB)^T = B^TA^T\\)\n\\((A+B)^T = A^T + B^T\\)\n\\((aA)^T = aA^T\\)\n\\((A^T)^{-1} = (A^{-1})^T\\)",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#inverse",
    "href": "mathref/math_2.html#inverse",
    "title": "Math Refresher",
    "section": "Inverse",
    "text": "Inverse\nThe inverse of a square matrix is a matrix that, when multiplied by the original matrix (\\(A A^{-1} = I\\)), results in the identity matrix. For example:\n\\[\nA = \\begin{bmatrix} 1 & 2 \\\\ 4   & 6 \\end{bmatrix} \\rightarrow\nA^{-1} = \\begin{bmatrix} -3 & 1 \\\\ 2 & -.5 \\end{bmatrix}\n\\]\n\n\n\n. mata\n------------------------------------------------- mata (type end to exit) -----\n: a = (1,2 \\ 4,6)\n\n: a_inv = luinv(a); a_inv\n         1     2\n    +-------------+\n  1 |   -3     1  |\n  2 |    2   -.5  |\n    +-------------+\n\n: a*a_inv\n[symmetric]\n       1   2\n    +---------+\n  1 |  1      |\n  2 |  0   1  |\n    +---------+\n\n: end\n-------------------------------------------------------------------------------\n\n.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#section-6",
    "href": "mathref/math_2.html#section-6",
    "title": "Math Refresher",
    "section": "",
    "text": "For a \\(2 \\times 2\\) matrix, the inverse is defined as:\n\\[\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\\]\nThus, if a matrix has determinant 0, then it is not invertible.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#determinant",
    "href": "mathref/math_2.html#determinant",
    "title": "Math Refresher",
    "section": "Determinant",
    "text": "Determinant\nThe determinant of a square matrix is a scalar value that is a function of the elements of the matrix. The determinant of a \\(2 \\times 2\\) matrix is defined as:\n\\[\\begin{vmatrix} a & b \\\\ c & d \\end{vmatrix} = ad - bc\\]\nThe determinant of a \\(3 \\times 3\\) matrix is defined as:\n\\[\\begin{vmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{vmatrix} = aei+dhc+gbf-ceg-fha-ibd\\]\nmata: det(a)",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#rank-and-linear-independence",
    "href": "mathref/math_2.html#rank-and-linear-independence",
    "title": "Math Refresher",
    "section": "Rank and linear independence",
    "text": "Rank and linear independence\n\nThe rank of a matrix is the number of linearly independent rows or columns in the matrix.\n\nIn a rectangular matrix, the rank cannot be larger than the smaller of the rows or columns.\n\nIf we consider each column, or rows, of a matrix as a vector, then the rank of the matrix is the number of linearly independent vectors in the matrix.\nIf a set of vectors are not linearly independent, then one of the vectors can be expressed as a linear combination of the other vectors. For example, the following vectors are not linearly independent:\n\n\\[a_1 \\vec x_1 + a_2 \\vec x_2 + a_3 \\vec x_3 = 0\\]\nmata: rank(a)",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#system-of-linear-equations",
    "href": "mathref/math_2.html#system-of-linear-equations",
    "title": "Math Refresher",
    "section": "System of linear equations",
    "text": "System of linear equations\nA system of linear equations is a set of equations that can be expressed in the form:\n\\[\\begin{aligned}\na_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &= b_1 \\\\\na_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &= b_2 \\\\\n\\vdots \\\\\na_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nn}x_n &= b_n \\\\\n\\end{aligned}\\]\nwhere \\(a_{ij}\\) and \\(b_i\\) are constants, and \\(x_i\\) are variables.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_2.html#section-7",
    "href": "mathref/math_2.html#section-7",
    "title": "Math Refresher",
    "section": "",
    "text": "This system of equations can be written in matrix form as:\n\\[A_{n\\times n}   X_{n\\times 1} =   b_{n\\times 1}\\]\nif the system has a unique solution, then the matrix \\(A\\) is invertible, and the solution is given by:\n\\[X = A^{-1}b\\]\nThus if there is no solution, then \\(A\\) is not invertible. If the determinant of \\(A\\) is 0, then \\(A\\) is not invertible.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Econometrics",
    "section": "",
    "text": "Greetings, brave students!\nGet ready for an incredible journey into the world of Econometrics, where knowledge converges. Picture yourselves as explorers navigating galaxies of data, uncovering economic secrets hidden among the stars. Armed with the tools of econometric analysis, you’ll master regression, causality, and inference, tackling challenges like multicollinearity and endogeneity.\nThis adventure isn’t for the faint-hearted. You’ll face tough challenges, but with powerful strategies, you’ll cut through confusion and reveal true causation. As you explore panel data and time series analysis, dive into econometric theories, discovering robustness and efficiency.\nTogether, as a team, engage in debates, share insights, and work on projects, unraveling econometric mysteries. When you complete this journey, you’ll be celebrated as heroes of economic analysis, ready to shape the future.\nSo, with passion and curiosity, let’s embark on this epic adventure. May your quest be filled with triumphs and insights as you rise to intellectual greatness. Embrace this journey and let econometrics reveal its wonders!\nOnward, courageous scholars, to limitless horizons!"
  },
  {
    "objectID": "imewld/chapter3.html",
    "href": "imewld/chapter3.html",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "Model:\n\\[colGPA = \\beta_0 + \\beta_1hsGPA + \\beta_2ACT + u\\]\n\nfrause gpa1, clear\nregress colgpa hsgpa act\nregress colgpa act\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(2, 138)       =     14.78\n       Model |  3.42365506         2  1.71182753   Prob &gt; F        =    0.0000\n    Residual |  15.9824444       138  .115814814   R-squared       =    0.1764\n-------------+----------------------------------   Adj R-squared   =    0.1645\n       Total |  19.4060994       140  .138614996   Root MSE        =    .34032\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .4534559   .0958129     4.73   0.000     .2640047    .6429071\n         act |    .009426   .0107772     0.87   0.383    -.0118838    .0307358\n       _cons |   1.286328   .3408221     3.77   0.000      .612419    1.960237\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(1, 139)       =      6.21\n       Model |  .829558811         1  .829558811   Prob &gt; F        =    0.0139\n    Residual |  18.5765406       139  .133644177   R-squared       =    0.0427\n-------------+----------------------------------   Adj R-squared   =    0.0359\n       Total |  19.4060994       140  .138614996   Root MSE        =    .36557\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         act |    .027064   .0108628     2.49   0.014     .0055862    .0485417\n       _cons |   2.402979   .2642027     9.10   0.000     1.880604    2.925355\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter3.html#example-3.1-determinants-of-college-gpa",
    "href": "imewld/chapter3.html#example-3.1-determinants-of-college-gpa",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "Model:\n\\[colGPA = \\beta_0 + \\beta_1hsGPA + \\beta_2ACT + u\\]\n\nfrause gpa1, clear\nregress colgpa hsgpa act\nregress colgpa act\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(2, 138)       =     14.78\n       Model |  3.42365506         2  1.71182753   Prob &gt; F        =    0.0000\n    Residual |  15.9824444       138  .115814814   R-squared       =    0.1764\n-------------+----------------------------------   Adj R-squared   =    0.1645\n       Total |  19.4060994       140  .138614996   Root MSE        =    .34032\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .4534559   .0958129     4.73   0.000     .2640047    .6429071\n         act |    .009426   .0107772     0.87   0.383    -.0118838    .0307358\n       _cons |   1.286328   .3408221     3.77   0.000      .612419    1.960237\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(1, 139)       =      6.21\n       Model |  .829558811         1  .829558811   Prob &gt; F        =    0.0139\n    Residual |  18.5765406       139  .133644177   R-squared       =    0.0427\n-------------+----------------------------------   Adj R-squared   =    0.0359\n       Total |  19.4060994       140  .138614996   Root MSE        =    .36557\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         act |    .027064   .0108628     2.49   0.014     .0055862    .0485417\n       _cons |   2.402979   .2642027     9.10   0.000     1.880604    2.925355\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter3.html#example-3.2-hourly-wage-equation",
    "href": "imewld/chapter3.html#example-3.2-hourly-wage-equation",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Example 3.2: Hourly Wage Equation",
    "text": "Example 3.2: Hourly Wage Equation\nModel:\n\\[log(wage)=\\beta_0 + \\beta_1educ + \\beta_2exper + \\beta_3tenure + u\\]\n\nfrause wage1, clear\ngen logwage = log(wage)\nreg logwage educ exper tenure\n\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(3, 522)       =     80.39\n       Model |  46.8741776         3  15.6247259   Prob &gt; F        =    0.0000\n    Residual |  101.455574       522  .194359337   R-squared       =    0.3160\n-------------+----------------------------------   Adj R-squared   =    0.3121\n       Total |  148.329751       525   .28253286   Root MSE        =    .44086\n\n------------------------------------------------------------------------------\n     logwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |    .092029   .0073299    12.56   0.000     .0776292    .1064288\n       exper |   .0041211   .0017233     2.39   0.017     .0007357    .0075065\n      tenure |   .0220672   .0030936     7.13   0.000     .0159897    .0281448\n       _cons |   .2843595   .1041904     2.73   0.007     .0796756    .4890435\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter3.html#partialling-out-interpretation-of-multiple-regression",
    "href": "imewld/chapter3.html#partialling-out-interpretation-of-multiple-regression",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Partialling Out Interpretation of Multiple Regression",
    "text": "Partialling Out Interpretation of Multiple Regression\nModel:\n\\[log(wage)=\\beta_0 + \\beta_1educ + \\beta_2exper + \\beta_3tenure + u\\]\nWe could estimate the same models with the followin:\n\\[educ=\\gamma_0 + \\gamma_1exper + \\gamma_2tenure + v\\]\n\\[log(wage)=\\beta_0 + \\beta_1 \\hat v + u\\]\n\nqui:reg logwage exper tenure\npredict logwage_res, resid\nqui:reg educ exper tenure\npredict educ_res, resid\nreg logwage_res educ_res\n\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =    158.24\n       Model |  30.6376772         1  30.6376772   Prob &gt; F        =    0.0000\n    Residual |  101.455574       524  .193617507   R-squared       =    0.2319\n-------------+----------------------------------   Adj R-squared   =    0.2305\n       Total |  132.093251       525  .251606192   Root MSE        =    .44002\n\n------------------------------------------------------------------------------\n logwage_res | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    educ_res |    .092029   .0073159    12.58   0.000     .0776568    .1064011\n       _cons |  -6.25e-10   .0191858    -0.00   1.000    -.0376905    .0376905\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter3.html#example-3.4-determinants-of-college-gpa",
    "href": "imewld/chapter3.html#example-3.4-determinants-of-college-gpa",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Example 3.4 : Determinants of College GPA",
    "text": "Example 3.4 : Determinants of College GPA\nSee example 3.1"
  },
  {
    "objectID": "imewld/chapter3.html#example-3.5-explaining-arrest-records",
    "href": "imewld/chapter3.html#example-3.5-explaining-arrest-records",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Example 3.5 : Explaining Arrest Records",
    "text": "Example 3.5 : Explaining Arrest Records\n\nfrause crime1, clear\nregress narr86 pcnv  ptime86 qemp86\n\n\n      Source |       SS           df       MS      Number of obs   =     2,725\n-------------+----------------------------------   F(3, 2721)      =     39.10\n       Model |  83.0741941         3   27.691398   Prob &gt; F        =    0.0000\n    Residual |  1927.27296     2,721  .708295833   R-squared       =    0.0413\n-------------+----------------------------------   Adj R-squared   =    0.0403\n       Total |  2010.34716     2,724  .738012906   Root MSE        =     .8416\n\n------------------------------------------------------------------------------\n      narr86 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        pcnv |  -.1499274   .0408653    -3.67   0.000    -.2300576   -.0697973\n     ptime86 |  -.0344199    .008591    -4.01   0.000    -.0512655   -.0175744\n      qemp86 |   -.104113   .0103877   -10.02   0.000    -.1244816   -.0837445\n       _cons |   .7117715   .0330066    21.56   0.000      .647051     .776492\n------------------------------------------------------------------------------\n\n\n\nregress narr86 pcnv avgsen ptime86 qemp86\n\n\n      Source |       SS           df       MS      Number of obs   =     2,725\n-------------+----------------------------------   F(4, 2720)      =     29.96\n       Model |  84.8242895         4  21.2060724   Prob &gt; F        =    0.0000\n    Residual |  1925.52287     2,720  .707912819   R-squared       =    0.0422\n-------------+----------------------------------   Adj R-squared   =    0.0408\n       Total |  2010.34716     2,724  .738012906   Root MSE        =    .84138\n\n------------------------------------------------------------------------------\n      narr86 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        pcnv |  -.1508319   .0408583    -3.69   0.000    -.2309484   -.0707154\n      avgsen |   .0074431   .0047338     1.57   0.116    -.0018392    .0167254\n     ptime86 |  -.0373908   .0087941    -4.25   0.000    -.0546345   -.0201471\n      qemp86 |   -.103341   .0103965    -9.94   0.000    -.1237268   -.0829552\n       _cons |   .7067565   .0331515    21.32   0.000     .6417519     .771761\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter3.html#variance-inflation-factors",
    "href": "imewld/chapter3.html#variance-inflation-factors",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Variance Inflation Factors",
    "text": "Variance Inflation Factors\n\nqui:regress narr86 pcnv avgsen ptime86 qemp86\nestat vif\n\n\n    Variable |       VIF       1/VIF  \n-------------+----------------------\n     ptime86 |      1.13    0.883693\n      qemp86 |      1.08    0.927081\n      avgsen |      1.06    0.942363\n        pcnv |      1.00    0.996771\n-------------+----------------------\n    Mean VIF |      1.07\n\n\n\nqui:regress pcnv avgsen ptime86 qemp86\ndisplay \"VIF for pcnv:   \" 1/(1-e(r2))\nqui:regress avgsen ptime86 qemp86 pcnv\ndisplay \"VIF for avgsen: \" 1/(1-e(r2))\n\nVIF for pcnv: 1.003239\nVIF for avgsen: 1.0611622"
  },
  {
    "objectID": "imewld/chapter3.html#example-3.7-evaluating-a-job-training-program",
    "href": "imewld/chapter3.html#example-3.7-evaluating-a-job-training-program",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Example 3.7 : Evaluating a Job Training Program",
    "text": "Example 3.7 : Evaluating a Job Training Program\n\nfrause jtrain98, clear\nregress earn98 train\n\n\n      Source |       SS           df       MS      Number of obs   =     1,130\n-------------+----------------------------------   F(1, 1128)      =     17.91\n       Model |  1054.41369         1  1054.41369   Prob &gt; F        =    0.0000\n    Residual |  66408.4778     1,128   58.872764   R-squared       =    0.0156\n-------------+----------------------------------   Adj R-squared   =    0.0148\n       Total |  67462.8915     1,129   59.754554   Root MSE        =    7.6729\n\n------------------------------------------------------------------------------\n      earn98 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       train |  -2.050053   .4844142    -4.23   0.000    -3.000507   -1.099599\n       _cons |    10.6099    .279429    37.97   0.000     10.06164    11.15816\n------------------------------------------------------------------------------\n\n\n\nregress earn98 train earn96 educ age married  \n\n\n      Source |       SS           df       MS      Number of obs   =     1,130\n-------------+----------------------------------   F(5, 1124)      =    152.99\n       Model |  27320.1797         5  5464.03593   Prob &gt; F        =    0.0000\n    Residual |  40142.7118     1,124  35.7141564   R-squared       =    0.4050\n-------------+----------------------------------   Adj R-squared   =    0.4023\n       Total |  67462.8915     1,129   59.754554   Root MSE        =    5.9761\n\n------------------------------------------------------------------------------\n      earn98 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       train |   2.410547   .4352625     5.54   0.000     1.556528    3.264565\n      earn96 |   .3725384   .0186262    20.00   0.000     .3359923    .4090845\n        educ |   .3628329    .064047     5.67   0.000     .2371678    .4884979\n         age |   -.181046    .018875    -9.59   0.000    -.2180803   -.1440118\n     married |   2.481719   .4262625     5.82   0.000      1.64536    3.318079\n       _cons |   4.667042   1.145283     4.08   0.000     2.419908    6.914176\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "fantasy_world_data_dictionary.html",
    "href": "fantasy_world_data_dictionary.html",
    "title": "Fantasy World Data Dictionary",
    "section": "",
    "text": "Field\nDescription\n\n\n\n\ncharacter_id\nUnique identifier for each character\n\n\nname\nCharacter’s full name\n\n\nrace\nCharacter’s race\n\n\nclass\nCharacter’s class\n\n\nlevel\nCharacter’s current level (1-120)\n\n\nregion_id\nID of the region where the character resides\n\n\nprofession_primary\nCharacter’s primary profession\n\n\nprofession_secondary\nCharacter’s secondary profession\n\n\ngold\nAmount of gold the character possesses\n\n\nachievement_points\nCharacter’s total achievement points\n\n\nhousehold_id\nID of the household the character belongs to\n\n\nguild_id\nID of the guild the character belongs to\n\n\ngender\nCharacter’s gender\n\n\nage\nCharacter’s age in years\n\n\nbirth_date\nCharacter’s date of birth\n\n\neducation_level\nCharacter’s level of education\n\n\nrelationship_status\nCharacter’s relationship status\n\n\nnum_children\nNumber of children the character has\n\n\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nhousehold_id\nUnique identifier for each household\n\n\nregion_id\nID of the region where the household is located\n\n\nsize\nNumber of characters in the household\n\n\ntotal_income\nTotal income of the household\n\n\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nguild_id\nUnique identifier for each guild\n\n\nname\nName of the guild\n\n\nregion_id\nID of the region where the guild is based\n\n\nlevel\nGuild’s current level (1-25)\n\n\nmember_count\nNumber of members in the guild\n\n\nbank_gold\nAmount of gold in the guild bank\n\n\nachievement_points\nGuild’s total achievement points\n\n\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nregion_id\nUnique identifier for each region\n\n\nregion_name\nName of the region\n\n\navg_item_price\nAverage price of items in the region\n\n\ninflation_rate\nCurrent inflation rate in the region\n\n\nunemployment_rate\nCurrent unemployment rate in the region\n\n\ngdp_growth\nGDP growth rate of the region\n\n\ntrade_volume\nTotal trade volume in the region\n\n\n\n\n\n\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\nquest_id\nUnique identifier for each quest\n\n\nname\nName of the quest\n\n\nregion_id\nID of the region where the quest is available\n\n\nmin_level\nMinimum level required to start the quest\n\n\ndifficulty\nDifficulty level of the quest\n\n\nreward_gold\nAmount of gold rewarded for completing the quest\n\n\nreward_xp\nAmount of experience points rewarded for completing the quest\n\n\nrequired_participants\nNumber of participants required for the quest\n\n\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\ncharacter_id\nID of the character performing the activity\n\n\nactivity_type\nType of activity performed\n\n\nduration_hours\nDuration of the activity in hours\n\n\ngold_earned\nAmount of gold earned from the activity\n\n\nxp_earned\nAmount of experience points earned from the activity"
  },
  {
    "objectID": "fantasy_world_data_dictionary.html#characters",
    "href": "fantasy_world_data_dictionary.html#characters",
    "title": "Fantasy World Data Dictionary",
    "section": "",
    "text": "Field\nDescription\n\n\n\n\ncharacter_id\nUnique identifier for each character\n\n\nname\nCharacter’s full name\n\n\nrace\nCharacter’s race\n\n\nclass\nCharacter’s class\n\n\nlevel\nCharacter’s current level (1-120)\n\n\nregion_id\nID of the region where the character resides\n\n\nprofession_primary\nCharacter’s primary profession\n\n\nprofession_secondary\nCharacter’s secondary profession\n\n\ngold\nAmount of gold the character possesses\n\n\nachievement_points\nCharacter’s total achievement points\n\n\nhousehold_id\nID of the household the character belongs to\n\n\nguild_id\nID of the guild the character belongs to\n\n\ngender\nCharacter’s gender\n\n\nage\nCharacter’s age in years\n\n\nbirth_date\nCharacter’s date of birth\n\n\neducation_level\nCharacter’s level of education\n\n\nrelationship_status\nCharacter’s relationship status\n\n\nnum_children\nNumber of children the character has"
  },
  {
    "objectID": "fantasy_world_data_dictionary.html#households",
    "href": "fantasy_world_data_dictionary.html#households",
    "title": "Fantasy World Data Dictionary",
    "section": "",
    "text": "Field\nDescription\n\n\n\n\nhousehold_id\nUnique identifier for each household\n\n\nregion_id\nID of the region where the household is located\n\n\nsize\nNumber of characters in the household\n\n\ntotal_income\nTotal income of the household"
  },
  {
    "objectID": "fantasy_world_data_dictionary.html#guilds",
    "href": "fantasy_world_data_dictionary.html#guilds",
    "title": "Fantasy World Data Dictionary",
    "section": "",
    "text": "Field\nDescription\n\n\n\n\nguild_id\nUnique identifier for each guild\n\n\nname\nName of the guild\n\n\nregion_id\nID of the region where the guild is based\n\n\nlevel\nGuild’s current level (1-25)\n\n\nmember_count\nNumber of members in the guild\n\n\nbank_gold\nAmount of gold in the guild bank\n\n\nachievement_points\nGuild’s total achievement points"
  },
  {
    "objectID": "fantasy_world_data_dictionary.html#economic-data",
    "href": "fantasy_world_data_dictionary.html#economic-data",
    "title": "Fantasy World Data Dictionary",
    "section": "",
    "text": "Field\nDescription\n\n\n\n\nregion_id\nUnique identifier for each region\n\n\nregion_name\nName of the region\n\n\navg_item_price\nAverage price of items in the region\n\n\ninflation_rate\nCurrent inflation rate in the region\n\n\nunemployment_rate\nCurrent unemployment rate in the region\n\n\ngdp_growth\nGDP growth rate of the region\n\n\ntrade_volume\nTotal trade volume in the region"
  },
  {
    "objectID": "fantasy_world_data_dictionary.html#quests",
    "href": "fantasy_world_data_dictionary.html#quests",
    "title": "Fantasy World Data Dictionary",
    "section": "",
    "text": "Field\nDescription\n\n\n\n\nquest_id\nUnique identifier for each quest\n\n\nname\nName of the quest\n\n\nregion_id\nID of the region where the quest is available\n\n\nmin_level\nMinimum level required to start the quest\n\n\ndifficulty\nDifficulty level of the quest\n\n\nreward_gold\nAmount of gold rewarded for completing the quest\n\n\nreward_xp\nAmount of experience points rewarded for completing the quest\n\n\nrequired_participants\nNumber of participants required for the quest"
  },
  {
    "objectID": "fantasy_world_data_dictionary.html#character-activities",
    "href": "fantasy_world_data_dictionary.html#character-activities",
    "title": "Fantasy World Data Dictionary",
    "section": "",
    "text": "Field\nDescription\n\n\n\n\ncharacter_id\nID of the character performing the activity\n\n\nactivity_type\nType of activity performed\n\n\nduration_hours\nDuration of the activity in hours\n\n\ngold_earned\nAmount of gold earned from the activity\n\n\nxp_earned\nAmount of experience points earned from the activity"
  },
  {
    "objectID": "adv_class/syllabus.html",
    "href": "adv_class/syllabus.html",
    "title": "ECON 629 - Applied Econometric Methods for Empirical Research and Policy Evaluation",
    "section": "",
    "text": "Instructor: Fernando Rios-Avila\nOffice: Room 307, Blithewood\nOffice hours: By appointment\nEmail: friosavi@levy.org\nWebsite: https://riosavila.github.io/."
  },
  {
    "objectID": "adv_class/syllabus.html#course-description",
    "href": "adv_class/syllabus.html#course-description",
    "title": "ECON 629 - Applied Econometric Methods for Empirical Research and Policy Evaluation",
    "section": "Course Description",
    "text": "Course Description\nThis course is designed to introduce students to econometric methods used in empirical research, with emphasis on the analysis and identification of causal effects and Policy Evaluation. It aims to provide an overview and general guidelines on practical implementation, discussing the intuition behind the empirical methods.\nThe course is divided in two sections. The first section focuses on a review of advanced estimation methods for the analysis of data under the linearity assumption: ordinary least squares, parametric and semiparametric model estimation, quantile regression, and RIF regressions. We also review the use of maximum likelihood estimation (MLE), and Nonlinear Least Squares (NLS) as methodologies that allows the estimation of both linear and nonlinear models.\nIn the second part of the course, we re-introduce the problem of identification of causal effects, the role of experimental designs, and the idea of potential outcome models, discussing the main limitations on the identification of causal effects. We follow by discussing various methods focused on the identification of causal effects including, fixed effects / Panel data models, matching and reweighting, instrumental variables, Regression Discontinuity, Differences in Differences and extensions, and Synthetic controls. In all cases, we discuss limitations and interpretation of treatment effects.\nBy the end of the course students should have a good understanding of the estimation of causal effects, as well as the main methods used to address this problem, and critically evaluate and interpret the output of such analyses."
  },
  {
    "objectID": "adv_class/syllabus.html#course-outline",
    "href": "adv_class/syllabus.html#course-outline",
    "title": "ECON 629 - Applied Econometric Methods for Empirical Research and Policy Evaluation",
    "section": "Course outline",
    "text": "Course outline\nThis course syllabus provides a general plan for the course; deviations may be necessary:\n\nPart I: Advanced Estimation Methods\n\nIntroduction: General Overview of the course\nLinear Regression:\n\n\nEstimation, Assumptions, Inference\nDeviation from Main Assumptions\nAlternative Approaches: Robust-Standard errors, Cluster Standard errors, Bootstrap Standard error.\nModel Selection, and Shrinkage Methods\n\nMHE – Chapter 3\nCI – Chapter 2\nMMA - Chapter 11\nISL – Chapter 3, Chapter 6.1 – 6.2\nMacKinnon, et al (2023) “Cluster-Robust Inference: A Guide to Empirical Practice” Journal of Econometrics\n\n\n\nParametric, non-parametric and Semi-parametric Regressions:\n\n\nDifference between Parametric and Non-Parametric models\nDistributions, densities and histograms\nAdvantages and disadvantages of non-parametric models (bias vs variance)\nEstimation: Kernel vs Splines; Semi - Parametric vs non-parametric\n\nMMA Chapter 9\nISL Chapter 7\nHenderson and Parmeter 2016 Teaching Nonparametric Econometrics to Undergraduates\nRacine, 2008 Nonparametric Econometrics: A Primer\nRios-Avila, 2020 Smooth Varying-coefficient Models in Stata\nVerardi and Debarsy 2012 Robinson’s square root of N consistent semiparametric regression estimator in Stata\n\n\n\nGoing Beyond the Mean: [conditional] Quantile Regression\n\n\nDifference between Linear Regression and CQuantile Regression\nEstimation and Statistical Inference\nQuantile Regression: Modeling Heteroskedasticity\nQuantile Regression: As a semiparametric model with unobserved Heterogeneity\n\nMHE Chapter 7\nKoenker and Hallok (2001) Quantile Regression\nMachado and Santos Silva (2019) Quantiles via moments\nWenz (2018) What does QR does and Doesn’t do\nCastro et al (2019) Smoothed GMM for quantile models\n\n\n\nGoing Beyond the Mean II: RIF – regressions\n\n\nInfluence Functions and Recentered Influence functions\nUsage with distributional Statistics\nRIF-Regressions: Unconditional Quantile Regressions, Partially Conditional Quantile Regressions\n\nRios-Avila and Maroto (2022) Moving Beyond Linear Regression: Implementing and Interpreting Quantile Regression Models With Fixed Effects\nBorah and Basu (2013) Highlighting Differences Between Conditional and Unconditional Quantile Regression Approaches Through An Application To Assess Medication Adherence\nFirpo Fortin and Lemieux (2009) Unconditional Quantile Regressions\nRios-Avila (2020) Recentered influence functions (RIFs) in Stata: RIF regression and RIF decomposition\n\n\n\nMaximum Likelihood Estimation\n\n\nLinear vs Non-Linear models\nNon-Linear Least Squares\nMLE for Logit/probit/poisson\nMLE for heckman/tobit\nMLE for other NL models\n\nMMA Chapter 4\nEA Chapter 7, 14 (Greene)\nRios-Avila (2018) Standard-error correction in two-stage optimization models: A quasi–maximum likelihood estimation approach\nKapteyn and Ypma (2007) Measurement Error and Misclassification: A comparison of Survey and Administrative Data.\nRansom (1987) An Empirical Model of Discrete and Continuous Choice in Family Labor Supply\n\n\n\n\nPart II: Causal Inference\n\nPotential Outcomes Causal Model\n\n\nAssumptions, Implications and Imputations\nRandomized Control Trials\nRegression in Experimental settings\n\nMHE Chapter 1\nCI Chapter 4\nIEP Chapter 3 – 4\nMM Chapter 1-2\nTE Chapter 10 - 11\n\n\n\nPanel Data and Fixed Effects\n\n\nRandom Effects vs Fixed effects\nCorrelated Random Effects Model\n2 or more FE: Frisch-Waugh-Lovell Theorem\n\nTE Chapter 16\nMHE Chapter 5\nCI Chapter 8\nRios-Avila (2015) Feasible fitting of linear models with N fixed effects\nCorreia (2016) A Feasible Estimator for Linear Models with Multi-Way Fixed Effects\nWooldridge (2019) Correlated random effects models with unbalanced panels\nMachado & Santos Silva (2019) Quantiles via moments\n\n\n\nInstrumental Variables - IV\n\n\nTheory, assumptions and Limitations\nRandomization and Natural Experiments\nHeterogeneity and Local Average Treatment Effects\nLeading Examples and Limitations\n\nCI Chapter 7\nMHE Chapter 4\nMM Chapter 3\nTE Chapter 19\n\n\n\nMatching and Reweighting\n\n\nStratification, Exact Matching, Approximate Matching\nCurse of Dimensionality\nWeighting and Re-weighting Analysis\n\nCI Chapter 5\nTE Chapter 14\nIEP Chapter 8\nAustin (2011) An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies\nKing and Nielsen (2019) Why Propensity Scores Should Not Be Used for Matching\nCaliendo et al (2008) Some Practical Guidance for the Implementation Of Propensity Score Matching\n\n\n\nRegression Discontinuity\n\n\nIdentification, Assumptions and Limitations\nLeveraging threshold: Estimation – Parametric and non-parametric\nSharp vs Fuzzy RD\n\nCI Chapter 6\nMHE Chapter 6\nTE Chapter 20\nMM Chapter 4\nCattaneo & Titiunik 2022 Regression Discontinuity Designs\nLee & Lemieux 2010 Regression Discontinuity Designs in Economics\n\n\n\nDifferences in Differences\n\n\nAssumptions and Identification of Canonical 2x2 DID\nAllowing for Covariates\nGeneralized DID, TWFE, and Limitations\nEvent Studies\nTiming and cohort Heterogeneity\n\nCI Chapter 9\nMHE Chapter 5\nTE Chapter 18\nMM Chapter 5\nRoth et al (2022) What’s Trending in Difference-in-Differences? A Synthesis of the Recent Econometrics Literature\nChaisemartin & D’Haultfeuille 2022 Two-Way Fixed Effects and Differences-in-Differences with Heterogeneous Treatment Effects: A Survey\n\n\n\nSynthetic Control\n\n\nIdentification and Estimation\nInternal vs external Validity\nExact Statistics and Extensions\n\nCI Chapter 10\nTE Chapter 21\nAbadie (2020), “Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects,” JEL"
  },
  {
    "objectID": "adv_class/syllabus.html#suggested-readings",
    "href": "adv_class/syllabus.html#suggested-readings",
    "title": "ECON 629 - Applied Econometric Methods for Empirical Research and Policy Evaluation",
    "section": "Suggested Readings",
    "text": "Suggested Readings\nMHE - Mostly Harmless Econometrics: An Empiricist’s Companion\nby Joshua D. Angrist and Jorn-Steffen Pischke\nCI - Causal Inference: The Mixtape\nBy Scott Cunningham\nTE - The Effect: An Introduction to Research Design and Causality\nBy Nick Huntington-Klein\nMM - Mastering Metrics: The path from Cause to Effect\nby Joshua D. Angrist and Jorn-Steffen Pischke\nMMA - Microeconometrics: Methods and Applications\nBy Colin Cameron and Pravin Trivedi (Selected Chapters)\nEA - Econometric Analysis\nBy William Greene (Selected Chapters)\nISL - An Introduction to Statistical Learning By Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani (Selected Chapters)"
  },
  {
    "objectID": "adv_class/syllabus.html#further-readings",
    "href": "adv_class/syllabus.html#further-readings",
    "title": "ECON 629 - Applied Econometric Methods for Empirical Research and Policy Evaluation",
    "section": "Further Readings",
    "text": "Further Readings\n\nPotential Outcomes Causal Model\nBlattman, Christopher, Nathan Fiala, and Sebastian Martinez. 2014. “Generating Skilled Self-Employment in Developing Countries: Experimental Evidence from Uganda.” Quarterly Journal of Economics 129 (2): 697–752. doi: 10.1093/qje/qjt057.\nSchultz, Paul. 2004. “School Subsidies for the Poor: Evaluating the Mexican Progresa Poverty Program.” Journal of Development Economics 74 (1): 199–250\nManning, Willard G., et al. “Health Insurance and the Demand for Medical Care: Evidence from a Randomized Experiment.” The American Economic Review, vol. 77, no. 3, 1987, pp. 251–77. JSTOR, http://www.jstor.org/stable/1804094. Accessed 18 Jan. 2023.\n\n\nPanel Data and Fixed Effects\nCornwell, C. And Rupert, P. (1997), Unobservable Individual Effects, Marriage And The Earnings Of Young Men. Economic Inquiry, 35: 285-294. Https://Doi.Org/10.1111/J.1465-7295.1997.Tb01910.X\nFreeman, R. B. (1984). Longitudinal Analyses of the Effects of Trade Unions. Journal of Labor Economics, 2(1), 1–26. http://www.jstor.org/stable/2535015Instrumental Variables - IV\nAshenfelter, O., & Rouse, C. (1998). Income, Schooling, and Ability: Evidence from a New Sample of Identical Twins. The Quarterly Journal of Economics, 113(1), 253–284. http://www.jstor.org/stable/2586991\nSandra E. Black, Paul J. Devereux, Kjell G. Salvanes, From the Cradle to the Labor Market? The Effect of Birth Weight on Adult Outcomes, The Quarterly Journal of Economics, Volume 122, Issue 1, February 2007, Pages 409–439, https://doi.org/10.1162/qjec.122.1.409\n\n\nInstrumental Variables - IV\nCygan-Rehm, K., & Wunder, C. (2018). Do working hours affect health? Evidence from statutory workweek regulations in Germany. Labour Economics, 53, 162-171. https://doi.org/10.1016/j.labeco.2018.05.003\nBulman, George, Robert Fairlie, Sarena Goodman, and Adam Isen. 2021. “Parental Resources and College Attendance: Evidence from Lottery Wins.” American Economic Review, 111 (4): 1201-40.\nTsai, A. C., & Venkataramani, A. S. (2015). The causal effect of education on HIV stigma in Uganda: Evidence from a natural experiment. Social Science & Medicine, 142, 37-46. https://doi.org/10.1016/j.socscimed.2015.08.009\nKoppel, Stephen, Bergin, Tiffany, Ropac, René, Randolph, Imani, & Joseph, Hannah. (2022). Examining the causal effect of pretrial detention on case outcomes: a judge fixed effect instrumental variable approach. Journal of Experimental Criminology.\n\n\nMatching and Reweighting\nJyotsna Jalan & Martin Ravallion (2003) Estimating the Benefit Incidence of an Antipoverty Program by Propensity-Score Matching, Journal of Business & Economic Statistics, 21:1, 19-30, DOI: 10.1198/073500102288618720 Arceneaux, K., Gerber, A., & Green, D. (2006). Comparing experimental and matching methods using a largescale voter mobilization experiment. Political Analysis, 14, 1-26.\nRen Mu & Dominique van de Walle (2011) Rural Roads and Local Market Development in Vietnam, The Journal of Development Studies, 47:5, 709-734, DOI: 10.1080/00220381003599436\n\n\nRegression Discontinuity\nCard, D., Dobkin, C., & Maestas, N. (2008). The Impact of Nearly Universal Insurance Coverage on Health Care Utilization: Evidence from Medicare. The American Economic Review, 98(5), 2242–2258. http://www.jstor.org/stable/29730170\nCarpenter, Christopher, and Carlos Dobkin. 2009. “The Effect of Alcohol Consumption on Mortality: Regression Discontinuity Evidence from the Minimum Drinking Age.” American Economic Journal: Applied Economics, 1 (1): 164-82.\nBleemer, Zachary, and Aashish Mehta. 2022. “Will Studying Economics Make You Rich? A Regression Discontinuity Analysis of the Returns to College Major.” American Economic Journal: Applied Economics, 14 (2): 1-22.\n\n\nDifferences in Differences\nNollenberger, Natalia and Rodriguez-Planas, Núria, (2015), Full-time universal childcare in a context of low maternal employment: Quasi-experimental evidence from Spain, Labour Economics, 36, issue C, p. 124-136\nGousse, Marion and Leturcq, Marion, (2022), More or Less unmarried. The impact of legal settings of cohabitation on labour market outcomes, European Economic Review 149 (2022)\nHotchkiss, J. L., Moore, R. E., & Rios-Avila, F. (2014). Reevaluation of the Employment Impact of the 1996 Summer Olympic Games. Southern Economic Journal, 140619071514006. doi:10.4284/0038-4038-2013.063\n\n\nSynthetic Control\nAbadie, A., Diamond, A., & Hainmueller, J. (2010). Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program. Journal of the American Statistical Association, 105(490), 493–505. doi:10.1198/jasa.2009.ap08746\nPeri, G., & Yasenov, V. (2018). The Labor Market Effects of a Refugee Wave: Synthetic Control Method Meets the Mariel Boatlift. Journal of Human Resources, 0217_8561R1.\nViana, J. H., Barbosa, A. V., & Sampaio, B. (2018). Does the World Cup get the economic ball rolling? Evidence from a synthetic control approach. EconomiA, 19(3), 330-349. https://doi.org/10.1016/j.econ.2018.05.001"
  },
  {
    "objectID": "adv_class/syllabus.html#course-website",
    "href": "adv_class/syllabus.html#course-website",
    "title": "ECON 629 - Applied Econometric Methods for Empirical Research and Policy Evaluation",
    "section": "Course website",
    "text": "Course website\nWe will be using Github for all materials, presentations and announcements. The course website is located at https://riosavila.github.io/."
  },
  {
    "objectID": "adv_class/syllabus.html#attendance",
    "href": "adv_class/syllabus.html#attendance",
    "title": "ECON 629 - Applied Econometric Methods for Empirical Research and Policy Evaluation",
    "section": "Attendance:",
    "text": "Attendance:\nClass attendance is highly recommended, but not compulsory. Material for homeworks will come from class lectures."
  },
  {
    "objectID": "adv_class/syllabus.html#course-software",
    "href": "adv_class/syllabus.html#course-software",
    "title": "ECON 629 - Applied Econometric Methods for Empirical Research and Policy Evaluation",
    "section": "Course Software",
    "text": "Course Software\nThere are several statistical packages for analyzing data. In this course, I will be using Stata to cover all materials in class. The Institute will be providing you with licenses for Stata/BE for length of the course. However, you may also use other-software of your liking. Two of the books used for the course, Causal Effect and The Effect, provides codes for most of the material we cover here using Stata, Python and R. Julia is also a viable option.\nStata offers many free short webinars and video tutorials that may be useful if you never used Stata before, or even if you have some experience with it. See the resources for links to the webinars offered by Stata.\nThere are also a series of Cheat sheets to remember basics of Data processing, analysis, and visualization here stata-cheat-sheets"
  },
  {
    "objectID": "adv_class/syllabus.html#additional-information",
    "href": "adv_class/syllabus.html#additional-information",
    "title": "ECON 629 - Applied Econometric Methods for Empirical Research and Policy Evaluation",
    "section": "Additional Information:",
    "text": "Additional Information:\nAll students are responsible for knowing Bard’s Policy on Academic Honesty as publish in Bard College Student Handbook."
  },
  {
    "objectID": "adv_class/syllabus.html#grading",
    "href": "adv_class/syllabus.html#grading",
    "title": "ECON 629 - Applied Econometric Methods for Empirical Research and Policy Evaluation",
    "section": "Grading",
    "text": "Grading\n\nPresentations 20%: Students will have to do a series of presentations during the second half of the semester based on the suggested material (further reading) or other papers the students may be interested in.\n\nThe main requirement. The paper should implement any of the methodologies we will be covering in class. The presentation should emphasize the Research question, assumptions used, methodology, and results. If possible, provide comments and suggestions to extend the paper.\n\nHomework 40%: Homeworks will be provided for you to practice and implement the different methodologies discussed in class. They can be carried out individually or in groups. This will include making a brief description of the results.\nPaper Project 40: Write a term paper that can be of two types:\n\nPaper Replication: You can choose to write a replication paper on a methodological paper, or applied empirical paper.\n\nIn either case, the replication will have to extend the analysis of the original paper to a different setup (empirical paper), different software (if replication paper), or other extensions to the original analysis/methodology.\n\nOriginal Research: A 20-25 pages paper where students answer a research question of their choice, using any of the methodologies presented in class. Standard structure of the paper applies.\n\n\n10% of the grade will be based on a presentation of the paper."
  },
  {
    "objectID": "adv_class/index.html",
    "href": "adv_class/index.html",
    "title": "Advance Econometrics: Causal Effects",
    "section": "",
    "text": "Find the syllabus here-HTML and here-PDF\nAll reading materials are available here\nIntroduction: Even the longest journey starts somewhere",
    "crumbs": [
      "Home",
      "Courses",
      "Advance Econometrics: Causal Effects"
    ]
  },
  {
    "objectID": "adv_class/index.html#syllabus",
    "href": "adv_class/index.html#syllabus",
    "title": "Advance Econometrics: Causal Effects",
    "section": "",
    "text": "Find the syllabus here-HTML and here-PDF\nAll reading materials are available here\nIntroduction: Even the longest journey starts somewhere",
    "crumbs": [
      "Home",
      "Courses",
      "Advance Econometrics: Causal Effects"
    ]
  },
  {
    "objectID": "adv_class/index.html#part-i-the-tools",
    "href": "adv_class/index.html#part-i-the-tools",
    "title": "Advance Econometrics: Causal Effects",
    "section": "Part I: The tools",
    "text": "Part I: The tools\n\n1. Unveiling the Tapestry of Truth: The Grand Saga of Linear Regression and its Statistical Inference\nLinear Regression Model: Statistical Inference and Extensions: html or pdf\n\n\n2. Unleashing the Power of Infinite Flexibility: Exploring the Cosmos of Semi- and Non-Parametric Regression\nSemi- and Non-Parametric Regression: How Flexible is Flexible Enough?: html or pdf\n\n\nHomework 1: html or pdf\n\n3. Beyond the Ordinary: Embarking on the Quest of Conditional Quantile Regressions\nConditional Quantile Regressions: Because No One is Average html or pdf\n\n\n4. Ascending the Ladder of Equality: Studying the Mysteries of Unconditional Quantile Regressions\nUnconditional Quantile Regressions: When We Care About Everyone html or pdf\n\n\n5. Breaking the Chains of Linearity: A Transcendent Expedition into NLS, IRLS, and MLE\nNLS, IRLS, and MLE: Going Truly Nonlinear html or pdf\n\n\n\nHomework 2: html or pdf",
    "crumbs": [
      "Home",
      "Courses",
      "Advance Econometrics: Causal Effects"
    ]
  },
  {
    "objectID": "adv_class/index.html#part-ii-the-methods",
    "href": "adv_class/index.html#part-ii-the-methods",
    "title": "Advance Econometrics: Causal Effects",
    "section": "Part II: The methods",
    "text": "Part II: The methods\n\n6. Chronicles of Fate: Exploring the Mystical What-Ifs in the Web of Causal Models\nPotential outcomes and Causal Models html or pdf\n\n\n7. The Mirror’s Embrace: Taming Unobservables with the Power of Fixed Effects\nPanel Data and Fixed Effects (Many FE) html or pdf\n\n\n8. The Summoner’s Call: Using Instrumental Variables to estimate LATE’s\nInstrumental Variables html or pdf\n\n\n9. The Blade of Equivalence: Unleashing the Art of Matching in the Realm of Divergence\nMatching and Re-weighting html or pdf\n\n\n10. Beyond the Discontinuity Veil: Crossing thresholds to Illuminate Secrets\nRegression Discontinuity Design html or pdf\n\n\n11. Twofold Wisdom: Unraveling Truths with the Dual Forces of Differences in Differences\nDifferences in Differences html\n\n\n12. Cosmic Recreations: Forging New Realities through Synthetic Control\nSynthetic Control html or pdf",
    "crumbs": [
      "Home",
      "Courses",
      "Advance Econometrics: Causal Effects"
    ]
  },
  {
    "objectID": "adv_class/Homework1.html",
    "href": "adv_class/Homework1.html",
    "title": "Homework I",
    "section": "",
    "text": "Consider the dataset hhprice.dta, available from frause repository. The dataset contains information on 10k house values with data on house characteristics and location.\n\nUsing data for all houses with information on latitude and longitude, estimate a log linear model on the determinants of TownHouse prices (type_h==1). Interpret the Results and discuss significance of the coefficients.\n\nBe mindful of using data with sufficient variation, specially if using categorical variables. (for example, if using categorical variables, avoid using groups with fewer than 10 observations).\n\nConsider the code here, adapt the code to estimate robust Standard errors and robust standard errors clustered by postcode for your model.\n\nIs there any reason to believe errors are related to postcode?\nDoes any of your conclusions (regarding significance) change when using clustered standard errors?"
  },
  {
    "objectID": "adv_class/Homework1.html#part-i-ols-and-standard-errors",
    "href": "adv_class/Homework1.html#part-i-ols-and-standard-errors",
    "title": "Homework I",
    "section": "",
    "text": "Consider the dataset hhprice.dta, available from frause repository. The dataset contains information on 10k house values with data on house characteristics and location.\n\nUsing data for all houses with information on latitude and longitude, estimate a log linear model on the determinants of TownHouse prices (type_h==1). Interpret the Results and discuss significance of the coefficients.\n\nBe mindful of using data with sufficient variation, specially if using categorical variables. (for example, if using categorical variables, avoid using groups with fewer than 10 observations).\n\nConsider the code here, adapt the code to estimate robust Standard errors and robust standard errors clustered by postcode for your model.\n\nIs there any reason to believe errors are related to postcode?\nDoes any of your conclusions (regarding significance) change when using clustered standard errors?"
  },
  {
    "objectID": "adv_class/Homework1.html#part-ii-model-selection-and-cross-validation",
    "href": "adv_class/Homework1.html#part-ii-model-selection-and-cross-validation",
    "title": "Homework I",
    "section": "Part II: Model selection and Cross validation",
    "text": "Part II: Model selection and Cross validation\nAs explained in class, some times models are selected based on their capability to make outsample predictions. This is what methods like Lasso and Ridge use to select among a myriad of possible models.\nOne way to evaluate the predictive power of a model is to use cross validation.\nUsing the same dataset as in Part I, but now consider two models: - The same one you propose in Part I - A model where all variables are interacted with each other (over parametrized model)\n\nReport the number of parameters used, the goodness of fit (R2 and adjusted R2) of the models, and comment on these results.\nImplement a simple one round 5-fold Crossvalidation procedure to evaluate the predictive power of the two models.\nProcess:\n\nSplit the data into 5 random groups of equal size\nUsing the first 4 groups, estimate the two models, and predict the values for the 5th group\nRepeat the process for all possible combinations of 4 groups (there are 5 possible combinations)\nFor each model, calculate the MSE (mean squared error) of the predictions.\n\n\nThe MSE is estimated as:\n\\[MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]\nwhere \\(y_i\\) is the actual value of the dependent variable, and \\(\\hat{y}_i\\) is the predicted value of the dependent variable using the different models.\n\nWhat do you conclude?\nRepeat the process again, and see if your conclusions change."
  },
  {
    "objectID": "adv_class/Homework1.html#part-iii-non-parametric-models",
    "href": "adv_class/Homework1.html#part-iii-non-parametric-models",
    "title": "Homework I",
    "section": "Part III: Non-parametric models",
    "text": "Part III: Non-parametric models\n\nIn real-estate, Location of a house is a very important determinant of house prices. One way to measure this location effect is to use the distance to the city center, as the center may offer the most amenities and services.\nConsider your model from Part I. If you have not done so, add the distance (log distance) to the city center as a regressor.\nFollowing the code here, adapt it to estimate Robinson (1988) Root-N-Consistent Semiparametric regression\nThe model:\n\n\\[lprice = x\\beta + \\theta (distance) + e\\]\n\nCompare the results of other coefficients with the OLS model. What do you conclude? Does modeling distance non-parametrically matter?\nBased on your results, Is the effect of distance on house prices linear?\nA second way to explore the role of distance on housing price-determination is by looking at the model coefficients across houses at different distances from the city center.\nClassify houses into 5 groups based on their distance to the city center, and estimate the model for each group. (this is a rough approach to the Smooth varying coefficient model)\n\nWhat do you conclude? Does distance affect how other amenities impact house prices?"
  },
  {
    "objectID": "adv_class/12did.html#why-differences-in-diffences",
    "href": "adv_class/12did.html#why-differences-in-diffences",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Why differences in diffences?",
    "text": "Why differences in diffences?\n\nDifferences-in-Differences (DID) is one of the most popular tools in applied economics for analyzing causal effects of an intervention or policy treatment.\nUnder reasonable assumptions, DID allows you to identify these effects by comparing changes in the treatment group with changes in the control group.\n\nThis method of identification even allows for controlling for self-selection in the treatment.\n\nIn recent years, several advances in DID have been developed, revealing issues with the traditional DID design, particularly with the application of TWFE.\nToday, I will briefly present the problems and solutions that have been proposed, and how they are implemented in Stata.\nSo lets Start with the basics: 2x2 DID"
  },
  {
    "objectID": "adv_class/12did.html#x2-did-cannonic-design",
    "href": "adv_class/12did.html#x2-did-cannonic-design",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "2x2 DID: Cannonic Design",
    "text": "2x2 DID: Cannonic Design\n\nIn the 2x2 DID design, we have 2 groups:\n\nControl (\\(D=1\\)) y treated (\\(D=0\\)),\n\nWhich are observed for two periods of time:\n\nBefore (\\(T=0\\)) and after (\\(T=1\\)) the treatment.\n\nFor all groups and periods of time, we observe the realized outcome \\(Y\\), but cannot observe all potential outcomes.\nThis setup is valid both for panel data and repeat cross-sections, but will focus on panel data."
  },
  {
    "objectID": "adv_class/12did.html#potential-outcomes-and-treatment-effects",
    "href": "adv_class/12did.html#potential-outcomes-and-treatment-effects",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Potential outcomes and Treatment Effects",
    "text": "Potential outcomes and Treatment Effects\n\nPotential outcomes are the outcomes we would observe for each observation in the data if it were assigned to either the treatement or control group.\n\n\\(Y_{i,t}(D=0)\\): Potential outcome for individual \\(i\\) at time \\(t\\) if he was never treated.\n\\(Y_{j,s}(D=1)\\): Potential outcome for individual \\(j\\) at time \\(s\\) if he was treated.\n\nHowever, we are only able to observe one of this outcomes: \\[Y_{i,t} = Y_{i,t}(1)D_i + Y_{i,t}(0)(1-D_i)\\]\nWhere \\(D_i\\) is a dummy indicates the effective treatment status of individual \\(i\\).\nNow, we are interested in Treatment Effects for \\(i\\), or any summary measures, after the treatment is applied:\n\n\\[TE = Y_{i,1}(1)-Y_{i,1}(0) \\text{ or } ATT =E(Y_{i,1}(1)-Y_{i,1}(0)|D=1)\\]"
  },
  {
    "objectID": "adv_class/12did.html#the-did-estimator-assumptions",
    "href": "adv_class/12did.html#the-did-estimator-assumptions",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "The DID Estimator: Assumptions",
    "text": "The DID Estimator: Assumptions\n\nBecause we can’t observe both potential outcomes, we need to make assumptions to identify the treatment effect.\n\n\nStable Unit Treatment Value Assumption (SUTVA): The treatment status of one unit does not affect the potential outcomes of other units. (No spillovers)\nParallel Trends: In the absence of treatment, in average, both groups would have followed the same trend (changes) over time.\n\n\\[\\color{blue}{E(Y_{i,1}(0) - Y_{i,0}(0)|D_i=1)} = \\color{green}{E(Y_{i,1}(0)-Y_{i,0}(0)|D_i=0)}\\]\n\nNo Anticipation: Before the treatment takes place (\\(T=0\\)), the potential outcomes do not depend on the treatment status.\n\n\\[Y_{i,0}(0) = Y_{i,0}(1)\\]"
  },
  {
    "objectID": "adv_class/12did.html#the-did-estimator",
    "href": "adv_class/12did.html#the-did-estimator",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "The DID Estimator",
    "text": "The DID Estimator\n\nLets take a second look at the ATT definition: \\[\\begin{aligned}\nATT &=E(Y_{i,1}(1)|D=1)-\\color{red}{E(Y_{i,1}(0)|D=1)} \\\\\n&=E(Y_{i,1}|D=1)-\\color{red}{E(Y_{i,1}(0)|D=1)}\n\\end{aligned}\n\\]\nThe problem is to estimate the red component, to obtain an estimator for the ATT.\nHowever, under the PTA and No Anticipation, the observed part can be written as: \\[E(Y_{i,1}(0)|D=1) = E(Y_{i,1}-Y_{i,0}|D=0) + E(Y_{i,0}|D=1)\n\\]\nWhich gives the usual ATT estimator:\n\n\\[ATT =\\underbrace{E(Y_{i,1} - Y_{i,0}|D=1)}_{\\Delta treat} - \\underbrace{E(Y_{i,1}-Y_{i,0}|D=0)}_{\\Delta control}\\]"
  },
  {
    "objectID": "adv_class/12did.html#graphically",
    "href": "adv_class/12did.html#graphically",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Graphically",
    "text": "Graphically\n\n\n\n\n\nchecking did_imputation consistency and verifying not already installed...\ninstalling into c:\\ado\\plus\\...\ninstallation complete."
  },
  {
    "objectID": "adv_class/12did.html#how-to-estimate-it",
    "href": "adv_class/12did.html#how-to-estimate-it",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "How to estimate it?",
    "text": "How to estimate it?\n\nDifference-in-Means:\n\n\\[\\widehat{ATT}=\\bar Y^{D=1,T=1} - \\bar Y^{D=1,T=0} - [ \\bar Y^{D=0,T=1} - \\bar Y^{D=0,T=0} ]\\]\n\nRegression:\n\n\\[Y_{i,t} = \\beta_0 + \\beta_1 D_i + \\beta_2 t + \\beta_3 (D_i \\times t) + \\epsilon_{i,t}\\]\n\nWith Panel Data: Fixed Effects\n\n\\[\\begin{aligned}\nY_{i,t} &= \\beta_0 + \\sum \\delta_i  + \\beta_2 t + \\beta_3 (D_i \\times t) + \\epsilon_{i,t} \\\\\n\\Delta Y_{i} &= \\beta_2 + \\beta_3 D_i + \\epsilon_{i} \\text{ for } t=0 \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/12did.html#extension-adding-controls",
    "href": "adv_class/12did.html#extension-adding-controls",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Extension: Adding Controls",
    "text": "Extension: Adding Controls\n\nThe DID estimator presented above relies on Unconditional Parallel Trends. It may work if both control and treated groups are similar in all aspects, except for the treatment. But they may not.\nIn this case, we can add controls to the model, to account for differences in the groups.\n\n\\[Y_{i,t} = \\beta_0 + \\beta_1 D_i + \\beta_2 t + \\beta_3 (D_i \\times t) +  X_{it} \\gamma + \\epsilon_{i,t}\\]\n\nBut this would be wrong. Why?"
  },
  {
    "objectID": "adv_class/12did.html#problems-of-adding-controls",
    "href": "adv_class/12did.html#problems-of-adding-controls",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Problems of Adding Controls",
    "text": "Problems of Adding Controls\n\nSimply adding controls imposes assumption of homogenous treatement effects. As described in Sant’Anna and Zhao (2020), \\(\\gamma\\) may not be the same as the ATT.\n\nSolution: Interactions between controls and all group\\time dummies: \\[y_{i,t} = \\beta_0 + \\sum \\delta_i  + \\beta_2 t + \\beta_3 (D_i \\times t) +  X_{it} \\gamma + \\sum \\gamma_x (X-\\bar X_{D\\times T})*D*T + \\epsilon_{i,t}\\]\n\nControlling for time varying variables may introduce biases (bad controls)\n\nSolution: use only pre-treatment variables as controls. (panel) or variables that should not be affected by the treatment (cross-sections)\nAlso: If using Panel Data, one can add as controls all variables history. (see for example Caetano and Callaway (2022))"
  },
  {
    "objectID": "adv_class/12did.html#potential-solution-santanna_doubly_2020",
    "href": "adv_class/12did.html#potential-solution-santanna_doubly_2020",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Potential Solution: Sant’Anna and Zhao (2020)",
    "text": "Potential Solution: Sant’Anna and Zhao (2020)\n\nDID is a straighforward estimator, that relies strongly on the Unconditional Parallel Trends Assumption. (UPTA)\nOne solution to the problem is to use the Conditional Parallel Trends Assumption (CPTA), which can be less restrictive.\nThis states that PTA holds, only after conditioning on a set of variables \\(X\\). (looking at subgroups).\n\n\\[\\color{blue}{E(Y_{i,1}(0) - Y_{i,0}(0)|D_i=1,X)} = \\color{green}{E(Y_{i,1}(0)-Y_{i,0}(0)|D_i=0,X)}\\]\n\nSo, if you can “control” for individual characteristics, you could estimate the treatment effect, and still report Average Treatment Effects for the population.\n\n\\[ATT = E\\big[ [E(Y_{i,1}(0) - Y_{i,0}(0)|D_i=1,X ] \\big]\\]"
  },
  {
    "objectID": "adv_class/12did.html#added-assumption",
    "href": "adv_class/12did.html#added-assumption",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Added Assumption",
    "text": "Added Assumption\n\nWhen accouting for covariates, one needs to add an additional assumption to the data:\nThere is an overlap in the distribution of \\(X\\) between the treated and control groups. (common support)\n\n\\[0 &lt;&lt; Pr(D=1|X) &lt;&lt; 1\\]\n\nThis is important, because if there is no overlap, the treatment effect cannot be identified for all groups, and may create distorted estimates."
  },
  {
    "objectID": "adv_class/12did.html#potential-solution-drdid-teffects",
    "href": "adv_class/12did.html#potential-solution-drdid-teffects",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Potential Solution: drdid, teffects",
    "text": "Potential Solution: drdid, teffects\n\nSant’Anna and Zhao (2020) propose various alternatives to estimate the treatment effect with controls, under CPTA. They contrast the implementation of methods that use linear regressions, Inverse Probability Weighting (IPW) and Doubly Robust Estimators (DR).\nSimilar estimates could be obtained using teffects in Stata.\nBut how does this work? Two Cases:\n\nPanel Data: This simplifies the problem, because we can use the Data structure to estimate the model.\nCrossection: This is more complicated because requires careful consideration of idenfication assumptions, and groups of interest."
  },
  {
    "objectID": "adv_class/12did.html#panel-data",
    "href": "adv_class/12did.html#panel-data",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Panel Data",
    "text": "Panel Data\n\nStep 1: Estimate the changes in the outcome \\(\\Delta Y_i = Y_{i,1} - Y_{i,0}\\). (the First D)\n\nBecause of this, we are implicitly reducing our sample size: from \\(N\\) to \\(N/2\\) (because of the unestimated fixed effects)\n\nStep 2: Estimate the treatment effect:\n\nSimplest case, no controls: \\[\\Delta Y_i = \\beta_0 + \\beta_1 D_i + \\varepsilon_i\\]\n\nIn other words, the model is now a simple linear regression, with 1 period."
  },
  {
    "objectID": "adv_class/12did.html#panel-data-adding-controls",
    "href": "adv_class/12did.html#panel-data-adding-controls",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Panel Data: Adding controls",
    "text": "Panel Data: Adding controls\n\nBecause you have now only 1 period, and two groups. All treatment effect estimators can be used.\n\nregression outcome, IPW, doubly robust, matching, etc.\nWe use them to estimate the second D of DID\n\nEmpirically: With Transformed data (\\(\\Delta y\\)) as dependent variable, any of the methods in teffects can be used to estimate the treatment effect.\nWith untransformed data (RC), you can use drdid to estimate the treatment effect, and their Standard errors.\n\nYou need to make sure you have only 2 periods of data."
  },
  {
    "objectID": "adv_class/12did.html#panel-data-methods",
    "href": "adv_class/12did.html#panel-data-methods",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Panel Data: Methods",
    "text": "Panel Data: Methods\n\nRegression: reg (drdid) or ra (teffects)\n\nusing \\(\\Delta y\\) as dependent variable, estimate a linear model with covariates (No FE) for the untreated, and predict outcomes. This would become your “Potential outcome change under no-treatment”\n\n\n\\[ ATT = E(\\Delta Y_{i}|D=1) - E({\\hat\\gamma X|D=1})\\]\n\nInverse Probability Weighting: stdipw (drdid) or ipw (teffects)\n\nEstimate the likelihood of being treated, and use it to identify a propensity score and IPW weights \\(p(D=1|X) = \\hat p(X)\\)\n\n\n\\[ ATT = E(\\Delta Y_{i}|D=1) - E\\left({\\Delta Y_{i} \\frac{p(X)}{1-p(X)}|D=0}\\right)\\]"
  },
  {
    "objectID": "adv_class/12did.html#panel-data-methods-1",
    "href": "adv_class/12did.html#panel-data-methods-1",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Panel Data: Methods",
    "text": "Panel Data: Methods\n\nDoubly Robust Estimation: dript and dripw (drdid) or ipwra aipw (teffects)\n\nEstimate the likelihood of being treated, and use it to identify a propensity score and IPW weights \\(p(D=1|X) = \\hat p(X)\\)\nEstimate a weighted regression with \\(\\Delta y\\) as dependent variable or estimate a weighted regression correction. (see here slide 17-18) for more details.\nCombinations of modeling outcome and modeling likelihood and pscores makes the estimator doubly robust.\n\nFor Repeated crossection, the math becomes more complicated to follow and identify the “group of interest”\n\nSee here slide 53-54 for more details."
  },
  {
    "objectID": "adv_class/12did.html#example",
    "href": "adv_class/12did.html#example",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Example",
    "text": "Example\nSome Data Preparation\n\nqui:frause lalonde, clear\nkeep  if treated==0 | sample==2\nbysort id (year):gen dy = re[2]-re[1]\n\n(5,574 observations deleted)\n\n\nUsing teffects:\n\nteffects ra (dy educ black married nodegree hisp re74) (experimental) if year==1975 , atet\n\n\nIteration 0:  EE criterion =  4.433e-21  \nIteration 1:  EE criterion =  8.994e-26  \n\nTreatment-effects estimation                    Number of obs     =     16,417\nEstimator      : regression adjustment\nOutcome model  : linear\nTreatment model: none\n------------------------------------------------------------------------------\n             |               Robust\n          dy | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATET         |\nexperimental |\n   (1 vs 0)  |  -1055.483   346.1459    -3.05   0.002    -1733.917   -377.0497\n-------------+----------------------------------------------------------------\nPOmean       |\nexperimental |\n          0  |   3118.849   185.0719    16.85   0.000     2756.114    3481.583\n------------------------------------------------------------------------------\n\n\nUsing drdid:\n\ndrdid re educ black married nodegree hisp re74 , ivar(id) time(year) tr(experimental) reg\n\n\nDoubly robust difference-in-differences                 Number of obs = 32,834\nOutcome model  : regression adjustment\nTreatment model: none\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATET         |\nexperimental |\n   (1 vs 0)  |  -1055.483   346.1451    -3.05   0.002    -1733.915   -377.0511\n------------------------------------------------------------------------------\n\n\nby hand:\n\nqui:reg dy educ black married nodegree hisp re74 if exper==0\npredict dy_hat, xb\ngen att_i = dy-dy_hat\ntabstat att_i, by(exper) \n\n\nSummary for variables: att_i\nGroup variable: experimental \n\nexperimental |      Mean\n-------------+----------\n           0 | -9.61e-06\n           1 | -1055.483\n-------------+----------\n       Total | -27.32414\n------------------------"
  },
  {
    "objectID": "adv_class/12did.html#highlights-i-identification",
    "href": "adv_class/12did.html#highlights-i-identification",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Highlights I: Identification",
    "text": "Highlights I: Identification\n\nThe identification relies on the non-anticipation assumption and the parallel trends assumption.\n\nNeither of these assumptions can be tested, but they can be supported by the data.\n\nFor Implementation, If one has panel data, teffects and drdid can be used to estimate the treatment effect, and their standard errors. (after preparing the data)\nOtherwise, drdid can be used for repeated crossection as well. (or GMM)"
  },
  {
    "objectID": "adv_class/12did.html#highlighs-ii-overlapping",
    "href": "adv_class/12did.html#highlighs-ii-overlapping",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Highlighs II: Overlapping",
    "text": "Highlighs II: Overlapping\n\nThe overlapping assumption is important, and has different implications for the different methods.\nWhen the estimation method is reg or ra, the overlapping assumption is less biding, because the model is extrapolates to created “potential outcomes” for the treated group.\n\nHowever, extrapolation may not be accurate, or create incorrect extrapolations (base category dummies)\n\nWith doubly robust methods, the overlapping assumption is more relevant. dript is particularly sensitive to this assumption, and may not even converge if there is weak overlap.\nFor most practical purposes, dripw may be the most stable, and preferred method."
  },
  {
    "objectID": "adv_class/12did.html#highlighs-iii-controls",
    "href": "adv_class/12did.html#highlighs-iii-controls",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Highlighs III: Controls",
    "text": "Highlighs III: Controls\n\nWith panel data, drdid automatically constrains model specification to use pre-treatment information only.\n\nAdded work is required to include post-treatment information.\n\nWith Repeated crossection, drdid would use both pre-treatment and post-treatment information."
  },
  {
    "objectID": "adv_class/12did.html#how-gxt-did-was-suppoused-to-work",
    "href": "adv_class/12did.html#how-gxt-did-was-suppoused-to-work",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "How GxT DID was suppoused to work",
    "text": "How GxT DID was suppoused to work\n\nWhile the 2x2 DID design is simple to understand, it does not reflect the type of information that is available in most cases. (G&gt;2 and T&gt;2)\nUntil few years ago, when this kind of data was available, the standard approach was to use a generalized DID design, which extended the use of Fixed Effects:\n\n\\[\n\\begin{aligned}\n2\\times2: Y_{i,t} &= \\beta_0 + \\beta_1 T  + \\beta_2 D_i + \\beta_3 (D_i \\times T) + \\epsilon_{i,t} \\\\\nG\\times T: Y_{i,t} &= \\sum \\gamma_t + \\sum \\delta_i  + \\theta^{fe} PT_{i,t} + \\epsilon_{i,t} \\\\\n\\end{aligned}\n\\]\nwhere \\(PT_{i,t}\\) assume the value of 1 for the treated group After the treatment is applied, and 0 otherwise, and \\(\\theta^{fe}\\) representing the treatment effect.\n\nLittle that we know that this approach only works if the treatment effect is homogeneous across all groups and periods."
  },
  {
    "objectID": "adv_class/12did.html#why-it-doesnt-work",
    "href": "adv_class/12did.html#why-it-doesnt-work",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Why it doesn’t work?",
    "text": "Why it doesn’t work?\nThere are at least two-ways that have been used to explain why the Standard TWFE does not work:\n\nThe “Bad Controls” Problem: Goodman-Bacon (2021) shows that the TWFE estimator for ATT is a kind of weighted average of all possible DID one could estimate. Some of these would not be good DID designs.\nNegative Weights: Chaisemartin and D’Haultfœuille (2020) and Borusyak, Jaravel, and Spiess (2023) show that because TWFE ATT estimator is a weighted average of all group-Specific ATT, some may include negative weights.\nThis may produce negative ATT even if all group-specific ATT are positive.\nHowever, when the treatment effect is homogeneous, Neither of these situations would be a problem."
  },
  {
    "objectID": "adv_class/12did.html#the-bad-controls-problem",
    "href": "adv_class/12did.html#the-bad-controls-problem",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "The “Bad Controls” Problem",
    "text": "The “Bad Controls” Problem\n\nAll CasesEarly Treated vs Later Treated (Good)Early Treated vs Later Treated (bad)"
  },
  {
    "objectID": "adv_class/12did.html#the-bad-controls-problem-1",
    "href": "adv_class/12did.html#the-bad-controls-problem-1",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "The “Bad Controls” Problem",
    "text": "The “Bad Controls” Problem\n\nAll CasesEarly Treated vs Later Treated (Good)Early Treated vs Later Treated (bad)"
  },
  {
    "objectID": "adv_class/12did.html#negative-weights",
    "href": "adv_class/12did.html#negative-weights",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Negative Weights",
    "text": "Negative Weights\n\nTo understand the idea of Negative Weights, lets consider the following example:\n\n\\[Y_{i,t} = \\sum \\delta_i + \\sum \\gamma_t  + \\theta PT + \\epsilon_{i,t}\\]\nIf the panel data is balanced, and we apply FWL, we can use partialling out the Fixed Effects:\n\\[\\widetilde{PT}_{it}=PT_{it}-\\overline{PT}_{i}-\\overline{PT}_{t}+\\overline{PT}\\]\nand estimate \\(\\theta^{fe}\\) as:\n\\[\\hat\\theta^{fe} = \\frac{\\sum \\widetilde{PT_{it}} Y_{it}}{\\sum \\widetilde{PT}_{it}^2}\n= w_{it} Y_{it}\n\\]"
  },
  {
    "objectID": "adv_class/12did.html#section",
    "href": "adv_class/12did.html#section",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "",
    "text": "\\[\\widetilde{PT}_{it}=PT_{it}-\\overline{PT}_{i}-\\overline{PT}_{t}+\\overline{PT}\\]\n\n\\(\\overline{PT}_{i}\\) is the share of periods a unit is observed to be treated. (larger for early treated)\n\\(\\overline{PT}_{t}\\) is the share of units treated at time \\(t\\). (increasing)\n\\(\\overline{PT}\\) Share of treated “unit-periods”\n\nThus,\n\nUnits that are treated early, (high \\(\\overline{PT}_{i}\\))\nbut are analyzed at later periods (\\(\\overline{PT}_{t}\\) increasing in \\(t\\))\n\nWill be more likely to have a negative weight \\(w_{it}\\), thus contaminating the ATT estimate."
  },
  {
    "objectID": "adv_class/12did.html#graphically-1",
    "href": "adv_class/12did.html#graphically-1",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Graphically",
    "text": "Graphically"
  },
  {
    "objectID": "adv_class/12did.html#summarizing-the-problem",
    "href": "adv_class/12did.html#summarizing-the-problem",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Summarizing the Problem",
    "text": "Summarizing the Problem\n\nBoth Negative Weights and Bad Controls are capturing the same problem.\n\n\nBy using already treated units as controls (bad controls), we are implicitly applying negative weights to units that are already treated.\nThis would not be a problem on its own, if the treatment effect is homogeneous across all groups and periods.\nHowever, if the treatment effect is heterogeneous, Parallel Trends between already treated and late treated units may not hold."
  },
  {
    "objectID": "adv_class/12did.html#gxt-did-generalized-did",
    "href": "adv_class/12did.html#gxt-did-generalized-did",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "GxT DID: Generalized DID",
    "text": "GxT DID: Generalized DID\nSetup\n\nIn the Generalized DID\n\nOne has access to multiple periods of data: \\(t = 1,2,...,T\\).\nAnd units can be treated at any point before, during or after the available Data \\(G = -k,..1,2,..,T+l\\).\n\nThis is the cohort or group.\n\nWe also assume that once a unit is treated, it remains treated. (no reversals/no cohort-change)\n\nFrom our perspective, units treated at or before \\(t=1\\) will be considered as Allways treated.\n\nThis units cannot be used for analysis.\n\nFor any practical purpose, if we do not observe a unit being treated in the window of time observed, we assume its Never Treated.\n\nFor Notation we say these units belong to \\(g=\\infty\\) (or that they could be treated at some point in the far future)"
  },
  {
    "objectID": "adv_class/12did.html#gxt-did-generalized-did-1",
    "href": "adv_class/12did.html#gxt-did-generalized-did-1",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "GxT DID: Generalized DID",
    "text": "GxT DID: Generalized DID\nPotential Outcomes\n\nIn contrast with previous setup, with the GDID, we believe observations have multiple potential outcomes, for each period of time.\n\n\\(Y_{i,t}(G)\\) is the potenital outcome for individual \\(i\\) at time \\(t\\), if this unit would be treated at time \\(G\\).\nThus we state that depending on “when” a unit is treated, the potential outcomes could be different.\nThis is what it means allowing for heterogeneity."
  },
  {
    "objectID": "adv_class/12did.html#gxt-did-generalized-did-2",
    "href": "adv_class/12did.html#gxt-did-generalized-did-2",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "GxT DID: Generalized DID",
    "text": "GxT DID: Generalized DID\nParallel Trends Assumption\n\nPTA assumption is also slightly modified. Because we can differentiate between never-treated and not-yet-treated, one could impose those PTA assumptions.\nNever Treated \\[E(Y_{i,t}(\\infty) - Y_{i,t-s}(\\infty)|G=g) = E(Y_{i,t}(\\infty) - Y_{i,t-s}(\\infty)|G=\\infty) \\forall s&gt;0 \\]\nNot Yet Treated\n\\[E(Y_{i,t}(\\infty) - Y_{i,t-s}(\\infty)|G=g) = E(Y_{i,t}(\\infty) - Y_{i,t-s}(\\infty)|G=g') \\forall s&gt;0 \\]\nWhich suggests PTA hold for all pre- and post-treatment periods.\nSome methods only rely on Post-treatment PTA."
  },
  {
    "objectID": "adv_class/12did.html#gxt-did-generalized-did-3",
    "href": "adv_class/12did.html#gxt-did-generalized-did-3",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "GxT DID: Generalized DID",
    "text": "GxT DID: Generalized DID\nNo Anticipation\n\nAs before, we also require no anticipation.\n\nThat the before treatment takes place (or is announced), the potential outcomes do not depend on the treatment status.\n\n\n\\[Y_{i,t}(G) = Y_{i,t}(\\infty) \\text{ if } t&lt;G \\]\n\nAlso important.\n\n\\(E(Y_{i,t}(G)|g=G) = E(Y_{i,t}|g=G)\\) if \\(t&gt;G\\)\n\\(E(Y_{i,t}(\\infty)|g=\\infty) = E(Y_{i,t}|g=\\infty)\\)"
  },
  {
    "objectID": "adv_class/12did.html#general-overview",
    "href": "adv_class/12did.html#general-overview",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "General Overview",
    "text": "General Overview\nThe root of the problem with TWFE:\n\nIf treatment effects are heterogenous, the TWFE estimator of treatment effects produces incorrect estimates because of “negative weights” or “bad controls”\n\nTo solve the problem we need to do (at least) one of three things:\n\nAvoid using bad controls (Borusyak, Jaravel, and Spiess (2023) and Gardner (2022))\nAllow for heterogeneity in the treatment effect. (Wooldridge (2021) and Sun and Abraham (2021))\nUse only good DID designs. (Callaway and Sant’Anna (2021))"
  },
  {
    "objectID": "adv_class/12did.html#avoid-using-bad-controls-did_imputation-and-did2s",
    "href": "adv_class/12did.html#avoid-using-bad-controls-did_imputation-and-did2s",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Avoid using bad controls: did_imputation and did2s",
    "text": "Avoid using bad controls: did_imputation and did2s\nBorusyak, Jaravel, and Spiess (2023) and Gardner (2022)\n\nBoth methods are based on the idea of “imputing” the missing potential outcomes \\(Y_{it}(0)\\) for the treated group, using a method similar to the one used for 2x2 DID. A two-stage approach.\n\n\nWe know that estimating the following model would be incorrect:\n\n\\[y_{it} = \\delta_i + \\gamma_t + \\theta D_{it} + \\epsilon_{it}\\]\n\nHowever, what this authors suggest is to identify \\(\\delta_i\\) and \\(\\gamma_t\\) using pre-treatment data only:\n\n\\[y_{it} = \\delta_i + \\gamma_t + \\epsilon_{it} \\text{ if } t&lt;g \\]\nThis helps identifying the fixed effects, without any contamination. (its a model for the potential outcome of no treatment)"
  },
  {
    "objectID": "adv_class/12did.html#avoid-using-bad-controls-did_imputation-and-did2s-1",
    "href": "adv_class/12did.html#avoid-using-bad-controls-did_imputation-and-did2s-1",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Avoid using bad controls: did_imputation and did2s",
    "text": "Avoid using bad controls: did_imputation and did2s\nBorusyak, Jaravel, and Spiess (2023) and Gardner (2022)\n\nUse the previous model to re-estimate 1:\n\n\\[y_{it} = \\hat \\delta_i + \\hat \\gamma_t + \\theta D_{it} + \\epsilon_{it}\\]\n\nIn fact, one could use many other specifications to identify Treatment effects, including by group, by calendar, or dynamic effects.\n\n\\[y_{it} = \\hat \\delta_i + \\hat \\gamma_t + \\theta D_{it}*(other Heterogeneity) + \\epsilon_{it}\\]"
  },
  {
    "objectID": "adv_class/12did.html#avoid-using-bad-controls-did_imputation-and-did2s-2",
    "href": "adv_class/12did.html#avoid-using-bad-controls-did_imputation-and-did2s-2",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Avoid using bad controls: did_imputation and did2s",
    "text": "Avoid using bad controls: did_imputation and did2s\nBorusyak, Jaravel, and Spiess (2023) and Gardner (2022)\n\nThe identification of TE using the imputation method relies on the same assumptions as the traditional DID design.\nHowever, It also requires the Parallel Trends Assumption (PTA) to hold for all pre-treatment periods.\n\nThis why how we can use pre-treatment information to predict the potential outcomes for the treated group.\n\nAt the extreme, one could even identify TE without access to non-treated units!\n\n\nThis also means that the method is sensitive to problems with long PTA. Although it can be somewhat relaxed (based on model specification).\nHowever, its more efficient than other models because it uses all-pretreatment data for estimation.\n\nData requirements are similar to the traditional DID design."
  },
  {
    "objectID": "adv_class/12did.html#example-1",
    "href": "adv_class/12did.html#example-1",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Example",
    "text": "Example\n\nThere are two implementations of the method in Stata: did_imputation and did2s. In addition to the original GMM estimator described in Gardner (2022).\nHowever, the most flexible and Robust implementation is did_imputation.\n\n\nuse http://pped.org/bacon_example.dta, clear\n**Estimate model for Pre-treatment data\nqui:reghdfe asmrs if post==0, abs(fe1 = stfips fe2 = year)\n** Extra polate\nbysort stfips (fe1):replace fe1=fe1[1]\nbysort year (fe2):replace fe2=fe2[1]\n\n** get TE for i\ngen te = asmrs-fe1-fe2-_b[_cons]\nsum te if post==0\nsum te if post==1\n\n(843 real changes made)\n(1107 real changes made)\n(264 missing values generated)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          te |        510    2.92e-09    9.204828  -36.51952   51.26879\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          te |        843   -5.530065    15.66787  -69.98216   48.63423\n\n\nusing did_imputation:\n\n** Gvar\negen gvar = csgvar(post), ivar(stfips) tvar(year)\n** event\ngen event = year - gvar if gvar!=0\n** Gvar = . (missing implies never treated)\nclonevar gvar2=gvar if gvar!=0\ndid_imputation asmrs stfips year gvar2, autosample\ndid_imputation asmrs stfips year gvar2, autosample horizon(1/10) pretrends(5)\n\n(165 missing values generated)\n(165 missing values generated)\nWarning: part of the sample was dropped for the following coefficients because FE could not be imputed: tau.\n\n                                                         Number of obs = 1,353\n------------------------------------------------------------------------------\n       asmrs | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         tau |  -5.530065   2.978178    -1.86   0.063    -11.36719    .3070564\n------------------------------------------------------------------------------\nWarning: part of the sample was dropped for the following coefficients because FE could not be imputed: tau1 tau2 tau3 tau4 tau5 tau6 tau7 tau8 tau9 tau10.\n\n                                                           Number of obs = 870\n------------------------------------------------------------------------------\n       asmrs | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        tau1 |   .1501093   1.857768     0.08   0.936     -3.49105    3.791268\n        tau2 |   1.089884    2.60167     0.42   0.675    -4.009296    6.189064\n        tau3 |    1.02914   2.788825     0.37   0.712    -4.436857    6.495136\n        tau4 |   .3781413   2.549497     0.15   0.882     -4.61878    5.375063\n        tau5 |  -1.302828   2.063713    -0.63   0.528    -5.347632    2.741976\n        tau6 |  -1.137572    3.37171    -0.34   0.736    -7.746001    5.470858\n        tau7 |  -5.499892   2.053929    -2.68   0.007    -9.525519   -1.474265\n        tau8 |  -4.864576   3.374892    -1.44   0.149    -11.47924     1.75009\n        tau9 |  -4.645944   2.361134    -1.97   0.049    -9.273682   -.0182059\n       tau10 |  -6.772627   3.287879    -2.06   0.039    -13.21675   -.3285015\n        pre1 |   1.556381   3.517751     0.44   0.658    -5.338285    8.451046\n        pre2 |   1.200269   2.631038     0.46   0.648     -3.95647    6.357009\n        pre3 |  -.4639667   2.211242    -0.21   0.834    -4.797921    3.869987\n        pre4 |   1.921326   1.945554     0.99   0.323    -1.891888    5.734541\n        pre5 |  -1.315265   1.903228    -0.69   0.490    -5.045522    2.414992\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/12did.html#allow-for-heterogeneity-jwdid-and-eventstudyinteract",
    "href": "adv_class/12did.html#allow-for-heterogeneity-jwdid-and-eventstudyinteract",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Allow for Heterogeneity: jwdid and eventstudyinteract",
    "text": "Allow for Heterogeneity: jwdid and eventstudyinteract\nWooldridge (2021) and Sun and Abraham (2021)\n\nThe work by Wooldridge (2021) suggested that the GDID-TWFE estimator was not a problem perse.\nThe problem was that the GDID-TWFE estimator was simply misspecified.\nInstead of modeling: \\[Y_{i,t} = \\delta_i + \\gamma_t  + \\theta^{fe} PT_{i,t} + \\epsilon_{i,t}\\]\nOne should allow for a full set of interactions between the group and time dummies:\n\n\\[Y_{i,t} = \\delta_i + \\gamma_t  + \\sum_{g=2}^T \\sum_{t=g}^T \\theta_{g,t} \\mathbb{1}(G=g,T=t) + \\epsilon_{i,t}\\]\n\nIn this framework, each \\(\\theta_{g,t}\\) represents the ATT for each group at a particular period."
  },
  {
    "objectID": "adv_class/12did.html#allow-for-heterogeneity-jwdid-and-eventstudyinteract-1",
    "href": "adv_class/12did.html#allow-for-heterogeneity-jwdid-and-eventstudyinteract-1",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Allow for Heterogeneity: jwdid and eventstudyinteract",
    "text": "Allow for Heterogeneity: jwdid and eventstudyinteract\nWooldridge (2021) and Sun and Abraham (2021)\n\nIn the basic setup, this approach is basically the same as the method proposed by Borusyak, Jaravel, and Spiess (2023) and Gardner (2022).\nWooldridge, however, was not the first approach that aim to “allow for heterogeneity” in the treatment effect. Early attempts were done by using a dynamic events structure, using both leads ands lags of the treatment variable. \\[Y_{i,t} = \\delta_i + \\gamma_t  + \\sum_{e=-k}^{-2} \\theta_e \\mathbb{1}(t-G_i=e) + \\sum_{e=0}^L \\theta_e \\mathbb{1}(t-G_i=e) + \\epsilon_{i,t}\\]\nThis not only allows for heterogenous effects across time, but also allows you to analyze pre-treatments effects.\nHowever Sun and Abraham (2021) showed that this approach could also be wrong, if dynamic effects are also heterogenous across groups."
  },
  {
    "objectID": "adv_class/12did.html#allow-for-heterogeneity-jwdid-and-eventstudyinteract-2",
    "href": "adv_class/12did.html#allow-for-heterogeneity-jwdid-and-eventstudyinteract-2",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Allow for Heterogeneity: jwdid and eventstudyinteract",
    "text": "Allow for Heterogeneity: jwdid and eventstudyinteract\nWooldridge (2021) and Sun and Abraham (2021)\n\nAs solution, Sun and Abraham (2021) propose to use a full set of interactions between the group dummies and the event-study dummies. This is similar to Wooldridge (2021). \\[Y_{i,t} = \\delta_i + \\gamma_t  + \\sum_{g=2}^T \\sum_{e=-k}^{-2} \\theta_{g,e} \\mathbb{1}(t-G_i=e, G=g) + \\sum_{g=2}^T \\sum_{e=0}^L \\theta_{g,e} \\mathbb{1}(t-G_i=e, G=g) + \\epsilon_{i,t}\\]\nIn fact, if we write the “event” as “time”, it would look very similar to the model proposed by Wooldridge (2021).\n\n\\[Y_{i,t} = \\delta_i + \\gamma_t  + \\sum_{g=2}^T \\sum_{t=1}^{g-2} \\theta_{g,t} \\mathbb{1}(T=t, G=g)\n+ \\sum_{g=2}^T \\sum_{t=g}^{T} \\theta_{g,t} \\mathbb{1}(T=t, G=g) + \\epsilon_{i,t}\n\\]\n\nThus, both approaches are identical if we allow for Full interactions before and after treatment."
  },
  {
    "objectID": "adv_class/12did.html#allow-for-heterogeneity-jwdid-and-eventstudyinteract-3",
    "href": "adv_class/12did.html#allow-for-heterogeneity-jwdid-and-eventstudyinteract-3",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Allow for Heterogeneity: jwdid and eventstudyinteract",
    "text": "Allow for Heterogeneity: jwdid and eventstudyinteract\nAssumptions and Limitations\n\nThe model proposed by Wooldridge (2021) follows the same assumptions as Borusyak, Jaravel, and Spiess (2023). (Long PTA)\nSun and Abraham (2021) and the modified Wooldridge (2021), however, only requires PTA to hold “after” treatment takes place.\nBoth methods require careful consideration of covariates (time constant), and they require additional work for adding it into a model (variable shifting).\n\nThe limitations of sample size are somewhat more evident in this framework.\n\nWooldridge’s approach, however, could also be used beyond the linear case as shown in Wooldridge (2023)."
  },
  {
    "objectID": "adv_class/12did.html#implementation",
    "href": "adv_class/12did.html#implementation",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Implementation",
    "text": "Implementation\nThere are various commands that implement Sun and Abraham (2021) estimator, including her original command eventsudyinteract, as well as xtevent.\nFor Wooldridge (2021), there is now the official Stata18 command xthdidregress twfe, the one I developed: jwdid and a newer one wooldid.\nWill focus on jwdid.\nBase Estimation:\n\nqui:ssc install frause, replace\nfrause mpdta, clear\n** \njwdid lemp, ivar(countyreal) tvar(year) gvar(first_treat) \nestat simple\n\n(Written by R.              )\nWARNING: Singleton observations not dropped; statistical significance is biased (link)\n(MWFE estimator converged in 2 iterations)\n\nHDFE Linear regression                            Number of obs   =      2,500\nAbsorbing 2 HDFE groups                           F(   7,    499) =       3.82\nStatistics robust to heteroskedasticity           Prob &gt; F        =     0.0005\n                                                  R-squared       =     0.9933\n                                                  Adj R-squared   =     0.9915\n                                                  Within R-sq.    =     0.0101\nNumber of clusters (countyreal) =        500      Root MSE        =     0.1389\n\n                                        (Std. err. adjusted for 500 clusters in countyreal)\n-------------------------------------------------------------------------------------------\n                          |               Robust\n                     lemp | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n--------------------------+----------------------------------------------------------------\nfirst_treat#year#c.__tr__ |\n               2004 2004  |  -.0193724   .0223818    -0.87   0.387    -.0633465    .0246018\n               2004 2005  |  -.0783191   .0304878    -2.57   0.010    -.1382195   -.0184187\n               2004 2006  |  -.1360781   .0354555    -3.84   0.000    -.2057386   -.0664177\n               2004 2007  |  -.1047075   .0338743    -3.09   0.002    -.1712613   -.0381536\n               2006 2006  |   .0025139   .0199328     0.13   0.900    -.0366487    .0416765\n               2006 2007  |  -.0391927   .0240087    -1.63   0.103    -.0863634     .007978\n               2007 2007  |   -.043106   .0184311    -2.34   0.020    -.0793182   -.0068938\n                          |\n                    _cons |    5.77807    .001544  3742.17   0.000     5.775036    5.781103\n-------------------------------------------------------------------------------------------\n\nAbsorbed degrees of freedom:\n-----------------------------------------------------+\n Absorbed FE | Categories  - Redundant  = Num. Coefs |\n-------------+---------------------------------------|\n  countyreal |       500         500           0    *|\n        year |         5           1           4     |\n-----------------------------------------------------+\n* = FE nested within cluster; treated as redundant for DoF computation\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      simple |  -.0477099    .013265    -3.60   0.000    -.0737088   -.0217111\n------------------------------------------------------------------------------\n\n\nAdding controls\n\njwdid lemp lpop, ivar(countyreal) tvar(year) gvar(first_treat) \nestat simple\n\nWARNING: Singleton observations not dropped; statistical significance is biased (link)\n(MWFE estimator converged in 2 iterations)\n\nHDFE Linear regression                            Number of obs   =      2,500\nAbsorbing 2 HDFE groups                           F(  18,    499) =       3.62\nStatistics robust to heteroskedasticity           Prob &gt; F        =     0.0000\n                                                  R-squared       =     0.9933\n                                                  Adj R-squared   =     0.9916\n                                                  Within R-sq.    =     0.0212\nNumber of clusters (countyreal) =        500      Root MSE        =     0.1385\n\n                                                  (Std. err. adjusted for 500 clusters in countyreal)\n-----------------------------------------------------------------------------------------------------\n                                    |               Robust\n                               lemp | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n------------------------------------+----------------------------------------------------------------\n          first_treat#year#c.__tr__ |\n                         2004 2004  |   -.021248   .0216933    -0.98   0.328    -.0638695    .0213735\n                         2004 2005  |    -.08185   .0273307    -2.99   0.003    -.1355474   -.0281526\n                         2004 2006  |  -.1378704   .0307448    -4.48   0.000    -.1982756   -.0774651\n                         2004 2007  |  -.1095395   .0322696    -3.39   0.001    -.1729405   -.0461384\n                         2006 2006  |   .0025368   .0188523     0.13   0.893    -.0345029    .0395765\n                         2006 2007  |  -.0450935   .0219516    -2.05   0.040    -.0882223   -.0019646\n                         2007 2007  |  -.0459545    .017946    -2.56   0.011    -.0812136   -.0106954\n                                    |\nfirst_treat#year#c.__tr__#c._x_lpop |\n                         2004 2004  |   .0046278   .0175555     0.26   0.792     -.029864    .0391196\n                         2004 2005  |   .0251131   .0178749     1.40   0.161    -.0100063    .0602325\n                         2004 2006  |   .0507346   .0210361     2.41   0.016     .0094043    .0920648\n                         2004 2007  |   .0112497   .0265741     0.42   0.672    -.0409613    .0634607\n                         2006 2006  |   .0389352   .0164453     2.37   0.018     .0066246    .0712457\n                         2006 2007  |   .0380597   .0224407     1.70   0.091    -.0060301    .0821495\n                         2007 2007  |  -.0198351    .016172    -1.23   0.221    -.0516088    .0119385\n                                    |\n                        year#c.lpop |\n                              2004  |   .0110137   .0075431     1.46   0.145    -.0038064    .0258338\n                              2005  |   .0207333    .008093     2.56   0.011     .0048328    .0366339\n                              2006  |   .0105354   .0108004     0.98   0.330    -.0106845    .0317552\n                              2007  |    .020921   .0117917     1.77   0.077    -.0022465    .0440884\n                                    |\n                              _cons |   5.736532   .0215065   266.73   0.000     5.694277    5.778786\n-----------------------------------------------------------------------------------------------------\n\nAbsorbed degrees of freedom:\n-----------------------------------------------------+\n Absorbed FE | Categories  - Redundant  = Num. Coefs |\n-------------+---------------------------------------|\n  countyreal |       500         500           0    *|\n        year |         5           1           4     |\n-----------------------------------------------------+\n* = FE nested within cluster; treated as redundant for DoF computation\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      simple |   -.050627   .0124796    -4.06   0.000    -.0750866   -.0261675\n------------------------------------------------------------------------------\n\n\nCompared to did_imputation\n\ngen first2=first_treat if first_treat&gt;0\ndid_imputation lemp countyreal year first2\n\n(1,545 missing values generated)\n\n                                                         Number of obs = 2,500\n------------------------------------------------------------------------------\n        lemp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         tau |  -.0477101   .0132225    -3.61   0.000    -.0736257   -.0217944\n------------------------------------------------------------------------------\n\n\n\nqui:jwdid lemp, ivar(countyreal) tvar(year) gvar(first_treat) \nestat simple\n\nWARNING: Singleton observations not dropped; statistical significance is biased (link)\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      simple |  -.0477099    .013265    -3.60   0.000    -.0737088   -.0217111\n------------------------------------------------------------------------------\n\n\nEstimating an effect similar to Sun and Abraham (2021)\n\nqui:jwdid lemp, ivar(countyreal) tvar(year) gvar(first_treat) ///\n    never //&lt;- request full interaction\nestat event\n\nWARNING: Singleton observations not dropped; statistical significance is biased (link)\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   __event__ |\n         -4  |   .0033064   .0245551     0.13   0.893    -.0448207    .0514335\n         -3  |   .0250218   .0181543     1.38   0.168      -.01056    .0606037\n         -2  |   .0244587   .0142668     1.71   0.086    -.0035037    .0524211\n         -1  |          0  (omitted)\n          0  |  -.0199318   .0118575    -1.68   0.093    -.0431722    .0033085\n          1  |  -.0509574   .0168707    -3.02   0.003    -.0840233   -.0178914\n          2  |  -.1372587   .0365895    -3.75   0.000    -.2089728   -.0655447\n          3  |  -.1008114   .0345043    -2.92   0.003    -.1684385   -.0331842\n------------------------------------------------------------------------------\n\n\nIt is also possible to add further restrictions to “event/calendar/group” aggregates\n\n** requires latest JWDID\nqui:net install jwdid, from(https://friosavila.github.io/stpackages) replace\nprogram drop _all\ngen subsample = inrange(__event__,2,6)\nestat event, other(subsample)\n\noption other() not allowed"
  },
  {
    "objectID": "adv_class/12did.html#using-good-did-designs-csdid-and-csdid2",
    "href": "adv_class/12did.html#using-good-did-designs-csdid-and-csdid2",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Using Good DID Designs: csdid and csdid2",
    "text": "Using Good DID Designs: csdid and csdid2\nCallaway and Sant’Anna (2021)\n\nA third, and last, approach has been proposed by Callaway and Sant’Anna (2021).\nIn contrast with previous methods (which focus on global estimators), this approach suggests deconstructing the estimation into smaller, but well define pieces: The ATTGTs for 2x2 DIDs\n\nMakes it easy to use using GOOD DID designs only\nand, once all ATTGT’s are estimated, they can be aggregated in various ways\n\nThis approach takes full advantage of the fact we know quite well (drdid + others) how to estimate 2x2 DID.\nand (or but) forces you to utilize time constant or pretreatment controls only.\nHowever, one now needs to estimate as many as periods and cohorts are available in the data."
  },
  {
    "objectID": "adv_class/12did.html#using-good-did-designs-csdid-and-csdid2-1",
    "href": "adv_class/12did.html#using-good-did-designs-csdid-and-csdid2-1",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Using Good DID Designs: csdid and csdid2",
    "text": "Using Good DID Designs: csdid and csdid2\nCallaway and Sant’Anna (2021)\n\nThe proposed estimator starts with the assumption that one is interested in the ATTGT:\n\n\\[ATT(g,t) = E(y_{i,t}(G) - y_{i,t}(\\infty) |G_i =g)\n\\]\n\nAs in the simpler 2x2 case, however, we cannot observe the \\(y_{i,t}(\\infty)\\) after they are treated.\nWhat Callaway and Sant’Anna (2021) does is to impute this piece applyint PTA and no Anticipation.\n\\[{E(y_{i,t}(\\infty)|G_i =g)} \\approx E(y_{i,G-k}|G_i =g) + E(y_{i,t}-y_{i,G-k}| G_i \\in Control)\n\\]\nThus, the estimator for the ATT(g,t) becomes: \\[\\widehat{ATT(g,t)} = E(y_{i,t}- y_{i,G-k}|G_i =g)- E(y_{i,t}- y_{i,G-k}|G_i \\in Control)\n\\]"
  },
  {
    "objectID": "adv_class/12did.html#using-good-did-designs-csdid-and-csdid2-2",
    "href": "adv_class/12did.html#using-good-did-designs-csdid-and-csdid2-2",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Using Good DID Designs: csdid and csdid2",
    "text": "Using Good DID Designs: csdid and csdid2\nCallaway and Sant’Anna (2021)\n\\[\\widehat{ATT(g,t)} = E(y_{i,t}- y_{i,G-k}|G_i =g)- E(y_{i,t}- y_{i,G-k}|G_i \\in Control)\\]\n\nWith no anticipation and PTA, ANY period before treatment (\\(G-k\\)) could be used to construct the ATT(g,t).\nBecause of this, it only relies on Post-treatment PTA. (Violations of PTA before treatment have no impact on the estimates)\nDepending on the analysis of interest, one could choose different “control groups”\n\nNever treated: Most common\nNot-yet-treated: Include observations that up to time \\(t\\) have not been treated.\nFor pre-treatment ATT(g,t)’s, the Not-yet treated cound include all cohorts not treated until \\(t\\) R or those not treat until \\(t\\) nor \\(g\\) Stata."
  },
  {
    "objectID": "adv_class/12did.html#using-good-did-designs-csdid-and-csdid2-3",
    "href": "adv_class/12did.html#using-good-did-designs-csdid-and-csdid2-3",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Using Good DID Designs: csdid and csdid2",
    "text": "Using Good DID Designs: csdid and csdid2\nCallaway and Sant’Anna (2021)\n\nThis approach is relatively easy to implement. The main difficulty is keeping track of ALL the ATT(g,t)s that are estimated (and their VCV)\nHowever, once all ATT(g,t)’s are obtained, aggregation is straight forward:\n\n\\[\nAGG(ATT(g,t)) = \\frac{\\sum_{g,t \\in G\\times T}   ATT(g,t) * w_{g,t} * sel_{g,t} }\n{\\sum_{g,t \\in G\\times T} w_{g,t} * sel_{g,t} }\n\\]\n\nwhere \\(w_{g,t}\\) represents the size of cohort \\(g\\) at time \\(t\\) used in the estimation of ATT(g,t).\nand \\(sel_{g,t}\\) is an indicator for whether that ATTGT will be used in the aggregation."
  },
  {
    "objectID": "adv_class/12did.html#using-good-did-designs-csdid-and-csdid2-4",
    "href": "adv_class/12did.html#using-good-did-designs-csdid-and-csdid2-4",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Using Good DID Designs: csdid and csdid2",
    "text": "Using Good DID Designs: csdid and csdid2\nCallaway and Sant’Anna (2021)\nTypical Aggregations:\n\nSimple: \\(sel_{g,t}=1\\) if \\(t&gt;=g\\)\nGroup/cohort: \\(sel_{g,t}=1\\) if \\(t&gt;=g\\) and \\(g=G\\)\nCalendar: \\(sel_{g,t}=1\\) if \\(t&gt;=g\\) and \\(t=T\\)\nEvent: \\(sel_{g,t}=1\\) if \\(t-g=e\\)\nCevent: \\(sel_{g,t}=1\\) if \\(t-g \\in [ll,\\dots,uu]\\)\n\nand may be possible to combine some of these restrictions\n\nAlso, because the method is based on 2x2 DID, it allows to easily implement various methodologies, including the Doubly Robust."
  },
  {
    "objectID": "adv_class/12did.html#map-how-callaway_2021-relates-to-wooldridge_2021-and-borusyak2023revisiting",
    "href": "adv_class/12did.html#map-how-callaway_2021-relates-to-wooldridge_2021-and-borusyak2023revisiting",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Map: How Callaway and Sant’Anna (2021) relates to Wooldridge (2021) and Borusyak, Jaravel, and Spiess (2023)",
    "text": "Map: How Callaway and Sant’Anna (2021) relates to Wooldridge (2021) and Borusyak, Jaravel, and Spiess (2023)\nRelations: Simple case of no covariates\n\njwdid = did_imputation: jwdid can be applied to nonlinear models, but did_imputation is more flexible for linear models.\njwdid = did_imputation = csdid, notyet: When using the not-yet treated as control, csdid will produce the same estimates as the others if there is only 1 pretreatment period.\njwdid, never = csdid = eventinteract: If jwdid uses full interactions for pre and post treatment periods, the results will be the same as csdid or eventinteract\n\nThus the main differences across models is:\n\nHow they use “pre-treatment” information for the estimation of ATTs (long vs short)\nWhether they use only “never-treated” or “not-yet-treated” units for estimation\nAnd how are covariates treated"
  },
  {
    "objectID": "adv_class/12did.html#example-2",
    "href": "adv_class/12did.html#example-2",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Example",
    "text": "Example\n\ncsdid is the older command, with all functions well documented (helpfile).\nHowever, it can be slow in large tasks, because of the bottle neck of using the Full Dataset for every task.\ncsdid2 works almost identically to csdid, its faster, but some functions are not yet documented (helpfile is still missing). Will use this one here\n\n\nqui:net install csdid2, from(https://friosavila.github.io/stpackages) replace\nuse http://pped.org/bacon_example.dta, clear\n\n** create Gvar: still needs csdid\negen gvar = csgvar(post), ivar(stfips) tvar(year)\n\ncsdid2 asmrs, ///\n    ivar(stfips) /// &lt;- required for panel Data. Otherwise RC\n    tvar(year)   /// &lt;- Time variable. Should be continuously (month 11, 12, 13, ...)\n    gvar(gvar)   \n** Default uses \"never treated\", and produces Long gaps.\n\nProducing Long Gaps by default\nUsing method reg\n----+--- 1 ---+--- 2 ---+--- 3 ---+--- 4 ---+--- 5 \n..................................................    50\n..................................................   100\n..................................................   150\n..................................................   200\n..................................................   250\n..................................................   300\n..................................................   350\n..................................\n\n\n\ncsdid2 is creating the full set of ATT(g,t) estimations for the data. But will produce no result\nTo obtain results and plotting, one must use post estimation commands:\n\n\nestat event, /// request event-type estimates\n    noavg    /// asks not to produce Averages\n    revent(-10/10) // requests to limit the output to events between -10/10\n** Its also possible to restrict to specific groups/cohorts rgroup( list of numbers)\n** or restrict to specific years rcalendar( list of numbers)\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        tm10 |  -1.456481   6.604164    -0.22   0.825     -14.4004    11.48744\n         tm9 |  -7.234872   4.564612    -1.58   0.113    -16.18135    1.711603\n         tm8 |  -4.652236   4.995688    -0.93   0.352    -14.44361    5.139134\n         tm7 |  -6.442625   6.308106    -1.02   0.307    -18.80629    5.921036\n         tm6 |  -4.424961   4.217086    -1.05   0.294     -12.6903    3.840376\n         tm5 |  -6.785855   5.032378    -1.35   0.178    -16.64913    3.077424\n         tm4 |  -1.428164   3.663803    -0.39   0.697    -8.609087    5.752758\n         tm3 |  -2.313338    3.60909    -0.64   0.522    -9.387025    4.760349\n         tm2 |  -.8035454   3.215177    -0.25   0.803    -7.105177    5.498086\n         tp0 |  -.7577979   2.763696    -0.27   0.784    -6.174542    4.658946\n         tp1 |  -2.687627   3.036269    -0.89   0.376    -8.638605     3.26335\n         tp2 |  -3.590762   4.207388    -0.85   0.393    -11.83709    4.655566\n         tp3 |  -3.341711   2.549732    -1.31   0.190    -8.339094    1.655672\n         tp4 |  -4.882915   2.892191    -1.69   0.091    -10.55151    .7856753\n         tp5 |  -6.205122   2.776364    -2.23   0.025     -11.6467   -.7635485\n         tp6 |  -6.301267    3.82955    -1.65   0.100    -13.80705    1.204514\n         tp7 |  -10.83263     3.3192    -3.26   0.001    -17.33814   -4.327119\n         tp8 |  -9.945774   3.590409    -2.77   0.006    -16.98285   -2.908703\n         tp9 |  -9.285554    3.65841    -2.54   0.011    -16.45591   -2.115202\n        tp10 |  -11.17722   3.836158    -2.91   0.004    -18.69596   -3.658494\n------------------------------------------------------------------------------\n\n\nestat plot, will produce a figure from the last estimation\n\nestat plot, xsize(10) ysize(8)\ngraph export figev.png, replace width(800)\n\n \nAdvantage of csdid2 over other methods. Uniform Confidence Intervals using Wildbootstrap SE."
  },
  {
    "objectID": "adv_class/12did.html#conclusions",
    "href": "adv_class/12did.html#conclusions",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Conclusions",
    "text": "Conclusions\n\nIn this workshop I aimed to provide a brief overview of the problems with TWFE and the potential solutions.\n3 main solutions were presented:\n\nAvoid using bad controls (Borusyak, Jaravel, and Spiess (2023) and Gardner (2022))\nAllow for heterogeneity in the treatment effect. (Wooldridge (2021) and Sun and Abraham (2021))\nUse only good DID designs. (Callaway and Sant’Anna (2021))"
  },
  {
    "objectID": "adv_class/12did.html#conclusions-1",
    "href": "adv_class/12did.html#conclusions-1",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Conclusions",
    "text": "Conclusions\n\nAll methods have their own advantages and disadvantages.\n\nis the most robust based on the assumptions, but is the least efficient because of the sample size requirements. It also allows for flexiblity of DR estimators, and ensures you use pre-treatment controls only.\n\n\nand 2) are more efficient, (more data is used), but rely strongy on Long PTA assumption.\n\n\ncan be applied in non-linear settings, using nonlinear models. But aggregations are slower to obtain.\n\n\nand 2) can also be adapted to consider -dose-response- effects. As well as Treatment Reversals."
  },
  {
    "objectID": "adv_class/12did.html#suggested-readings",
    "href": "adv_class/12did.html#suggested-readings",
    "title": "Diff in Diff: From 2x2 to GxT DID",
    "section": "Suggested readings",
    "text": "Suggested readings\n\n\nBorusyak, Kirill, Xavier Jaravel, and Jann Spiess. 2023. “Revisiting Event Study Designs: Robust and Efficient Estimation.” https://arxiv.org/abs/2108.12419.\n\n\nCaetano, Carolina, and Brantly Callaway. 2022. “Difference-in-Differences with Time-Varying Covariates in the Parallel Trends Assumption.” https://doi.org/10.48550/ARXIV.2202.02903.\n\n\nCallaway, Brantly, and Pedro H. C. Sant’Anna. 2021. “Difference-in-Differences with Multiple Time Periods.” Journal of Econometrics, Themed Issue: Treatment Effect 1, 225 (2): 200–230. https://doi.org/10.1016/j.jeconom.2020.12.001.\n\n\nChaisemartin, Clément de, and Xavier D’Haultfœuille. 2020. “Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects.” American Economic Review 110 (9): 2964–96. https://doi.org/10.1257/aer.20181169.\n\n\n———. 2023. “Two-Way Fixed Effects and Differences-in-Differences with Heterogeneous Treatment Effects: A Survey.” The Econometrics Journal 26 (3): C1–30. https://doi.org/10.1093/ectj/utac017.\n\n\nGardner, John. 2022. “Two-Stage Differences in Differences.” https://arxiv.org/abs/2207.05943.\n\n\nGoodman-Bacon, Andrew. 2021. “Difference-in-Differences with Variation in Treatment Timing.” Journal of Econometrics 225 (2): 254–77. https://doi.org/https://doi.org/10.1016/j.jeconom.2021.03.014.\n\n\nRoth, Jonathan, Pedro H. C. Sant’Anna, Alyssa Bilinski, and John Poe. 2023. “What’s Trending in Difference-in-Differences? A Synthesis of the Recent Econometrics Literature.” Journal of Econometrics 235 (2): 2218–44. https://doi.org/10.1016/j.jeconom.2023.03.008.\n\n\nSant’Anna, Pedro H. C., and Jun Zhao. 2020. “Doubly Robust Difference-in-Differences Estimators.” Journal of Econometrics 219 (1): 101–22. https://doi.org/10.1016/j.jeconom.2020.06.003.\n\n\nSun, Liyang, and Sarah Abraham. 2021. “Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects.” Journal of Econometrics, Themed Issue: Treatment Effect 1, 225 (2): 175–99. https://doi.org/10.1016/j.jeconom.2020.09.006.\n\n\nWooldridge, Jeffrey M. 2021. “Two-Way Fixed Effects, the Two-Way Mundlak Regression, and Difference-in-Differences Estimators.” SSRN Journal. https://doi.org/10.2139/ssrn.3906345.\n\n\n———. 2023. “Simple Approaches to Nonlinear Difference-in-Differences with Panel Data.” The Econometrics Journal 26 (3): C31–66. https://doi.org/10.1093/ectj/utad016."
  },
  {
    "objectID": "adv_class/10matching.html#recap-potential-outcomes-and-identification",
    "href": "adv_class/10matching.html#recap-potential-outcomes-and-identification",
    "title": "Matching and Re-weighting",
    "section": "Recap: Potential outcomes and Identification",
    "text": "Recap: Potential outcomes and Identification\nTo identify treatment effects one could just compare potential outcomes in two states:\n\nwith treatment\nwithout treatment\n\nMathematically, average treatment effects would be: \\[\nATE = E(Y_i(1)-Y_i(0))\n\\]\nthe problem: with real data, we are only able to see one outcome. The counter factual is not observed:\n\\[\nY_i = Y_i(1)*D + Y_i(0)*(1-D)\n\\]\nand simple differences may not capture ATE, because of selection bias and heterogeneity in effects."
  },
  {
    "objectID": "adv_class/10matching.html#recap-gold-standard---rct",
    "href": "adv_class/10matching.html#recap-gold-standard---rct",
    "title": "Matching and Re-weighting",
    "section": "Recap: Gold Standard - RCT",
    "text": "Recap: Gold Standard - RCT\nThe easiest, but most expensive, way to deal with the problem is using Randomized Control Trials.\nEffectively, you randomize Treatment, so that potential outcomes are independent of treatment:\n\\[\nY(1),Y(0) \\perp D\n\\]\nIn other words, the distribution of potential outcomes is the same for those treated or untreated units.\n\\[\n\\begin{aligned}\nE(Y,D=1)&=E(Y(1),D=1)=E(Y(1),D=0) \\\\\nE(Y,D=0)&=E(Y(0),D=1)=E(Y(0),D=0) \\\\\nATT&=E(Y,D=1) - E(Y,D=0)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/10matching.html#when-unconditional-fails",
    "href": "adv_class/10matching.html#when-unconditional-fails",
    "title": "Matching and Re-weighting",
    "section": "When unconditional fails",
    "text": "When unconditional fails\nMore often than not, specially if we didn’t construct the data, it would be impossible to find that unconditional independence assumption holds.\nFor example, treatment (say having health insurance) may vary by age, gender, race, location, etc.\nThis is similar to the selection bias: Outcomes across treated and untreated groups will be different because:\n\nComposition: Characteristics of people among the treated could be different than those among the untreated For example, they could be older, more educated, mostly men, etc.\nOther factors: There could be factors we cannot control for, that also affect outcomes."
  },
  {
    "objectID": "adv_class/10matching.html#there-is-conditional",
    "href": "adv_class/10matching.html#there-is-conditional",
    "title": "Matching and Re-weighting",
    "section": "There is conditional",
    "text": "There is conditional\nWhen unconditional independence assumption fails, we can call on Conditional independence assumption:\n\\[\nY(1),Y(0) \\perp D | X\n\\]\nIn other words, If we can look into specific groups (given \\(X\\)), it may be possible to impose the Independence assumption.\nThis relaxes the independence condition, but assumes selection is due to observable characteristics only. (it still needs to be as good as randomized given \\(X\\))\nImplications:\n\\[\n\\begin{aligned}\nE(Y|D=1,X) =E(Y(1)|D=1,X)=E(Y(1)|D=0,X)  \\\\\nE(Y|D=0,X) =E(Y(0)|D=1,X)=E(Y(0)|D=0,X)  \n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/10matching.html#intuition",
    "href": "adv_class/10matching.html#intuition",
    "title": "Matching and Re-weighting",
    "section": "Intuition",
    "text": "Intuition\nMatching is a methodology that falls within quasi-experimental designs. You cannot or could not decide the assignment rules, so now are using data as given.\nThe idea is to construct an artificial control and use it as a counter-factual, so that both treated and control groups “look similar” in terms of observables.\nOnce a group of synthetic controls has been constructed, treatment effects can be calculated for the whole population:\n\\[\n\\begin{aligned}\nATE(X) &= E(Y|D=1,X) -E(Y|D=0,X) \\\\\nATE &= \\int ATE(X) dFx\n\\end{aligned}\n\\]\nHow can we do this?\nwe just need to find observational twins!"
  },
  {
    "objectID": "adv_class/10matching.html#matching-twins",
    "href": "adv_class/10matching.html#matching-twins",
    "title": "Matching and Re-weighting",
    "section": "Matching Twins",
    "text": "Matching Twins\n\nMatching on Observables"
  },
  {
    "objectID": "adv_class/10matching.html#subclassification-or-stratification",
    "href": "adv_class/10matching.html#subclassification-or-stratification",
    "title": "Matching and Re-weighting",
    "section": "Subclassification or stratification",
    "text": "Subclassification or stratification\nConsider the following dataset:\n\n\nCode\nfrause titanic, clear\nexpand freq\ndrop if freq==0\ngen class1=class==1\ntab survived class1 , nofreq col\n\n\n\n\n\n(Data downloaded from R base)\n(8 zero counts ignored; observations not deleted)\n(2,177 observations created)\n(8 observations deleted)\n\n           |        class1\n  Survived |         0          1 |     Total\n-----------+----------------------+----------\n        No |     72.92      37.54 |     67.70 \n       Yes |     27.08      62.46 |     32.30 \n-----------+----------------------+----------\n     Total |    100.00     100.00 |    100.00 \n\n\nIf we assume full Independence assumption we would believe that being in first class increased chance of survival in 35.4%. but is that the case?\nWhat if the composition of individuals differs across classes (women and children)\n\n\nCode\ntab age class1, nofreq col\ntab sex class1, nofreq col\n\n\n\n           |        class1\n       Age |         0          1 |     Total\n-----------+----------------------+----------\n     Child |      5.49       1.85 |      4.95 \n     Adult |     94.51      98.15 |     95.05 \n-----------+----------------------+----------\n     Total |    100.00     100.00 |    100.00 \n\n           |        class1\n       Sex |         0          1 |     Total\n-----------+----------------------+----------\n      Male |     82.68      55.38 |     78.65 \n    Female |     17.32      44.62 |     21.35 \n-----------+----------------------+----------\n     Total |    100.00     100.00 |    100.00 \n\n\nThere were fewer children, but more women in first class. Perhaps that explains the difference in survival rates"
  },
  {
    "objectID": "adv_class/10matching.html#section",
    "href": "adv_class/10matching.html#section",
    "title": "Matching and Re-weighting",
    "section": "",
    "text": "A better approach would be to look into the survival probabilities stratifying the data:\n\n\nCode\ngen surv=survived==2\nbysort age sex class1:egen sr_mean=mean(survived==2)\ntable (age sex) (class1), stat(mean surv) nototal\n\n\n\n-----------------------------------\n             |         class1      \n             |         0          1\n-------------+---------------------\nAge          |                     \n  Child      |                     \n    Sex      |                     \n      Male   |  .4067797          1\n      Female |  .6136364          1\n  Adult      |                     \n    Sex      |                     \n      Male   |  .1883378   .3257143\n      Female |  .6263345   .9722222\n-----------------------------------\n\n\nSo even within each group, the survival probability is larger in first class. What about Average?\n\n\nCode\nbysort age sex:egen sr_mean_class1=max(sr_mean*(class1==1))\nbysort age sex:egen sr_mean_class0=max(sr_mean*(class1==0))\ngen teff = sr_mean_class1-sr_mean_class0\nsum teff if class1==1 // ATT\nsum teff if class1==0 // ATU\nsum teff  // ATE\n\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        teff |        325    .2375421    .1125033   .1373765   .5932204\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        teff |      1,876    .1887847    .1089261   .1373765   .5932204\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        teff |      2,201    .1959842    .1107948   .1373765   .5932204"
  },
  {
    "objectID": "adv_class/10matching.html#what-did-we-do",
    "href": "adv_class/10matching.html#what-did-we-do",
    "title": "Matching and Re-weighting",
    "section": "What did we do?",
    "text": "What did we do?\nThe procedure above is a simple stratification approach, aka matching, to analyze the true impact of the treatment (being a 1st class passenger).\n\nStratified the sample in groups by age and gender.\n\nIdentify the shares of each group by class1\n\nPredict probability of survival per strata and class1\nObtain the Strata level Effects\nAggregate as needed.\n\nHere, we could estimate ATE, ATT or ATU!\n\n\nWhere could things go wrong?"
  },
  {
    "objectID": "adv_class/10matching.html#overlapping",
    "href": "adv_class/10matching.html#overlapping",
    "title": "Matching and Re-weighting",
    "section": "Overlapping",
    "text": "Overlapping\nThe procedure describe above works well whenever there is data overlapping.\n\nFor every combination of X, you see data on the control and treated group \\(0&lt;P(D|X)&lt;1\\)\n\nWhen this fails, you wont be able to estimate ATE’s, although ATT’s or ATU’s might still be possible:\n\nfor ATT: \\(P(D|X)&lt;1\\)\nfor ATU: \\(0&lt;P(D|X)\\)\n\nFor example:\n\n\nCode\nfrause hhprice, clear\nkeep price rooms type_h\ntab rooms type_h\n\n\n\n           |    =0 if house, =1\n Number of |       TownHouse\n     rooms |         0          1 |     Total\n-----------+----------------------+----------\n         1 |        37         72 |       109 \n         2 |     1,134        751 |     1,885 \n         3 |     4,634        648 |     5,282 \n         4 |     2,465        115 |     2,580 \n         5 |       465          2 |       467 \n         6 |        46          0 |        46 \n         7 |         7          0 |         7 \n-----------+----------------------+----------\n     Total |     8,788      1,588 |    10,376 \n\n\nWould not be able to estimate ATE nor ATU. Only ATT for townhouses."
  },
  {
    "objectID": "adv_class/10matching.html#curse-of-dimensionality",
    "href": "adv_class/10matching.html#curse-of-dimensionality",
    "title": "Matching and Re-weighting",
    "section": "Curse of dimensionality",
    "text": "Curse of dimensionality\nThere is a second problem in terms of stratification. How would we deal with Multiple dimensions? Would it be possible to find “twins” for every observation?\nThe answer is, probably no. Too many groups to track, to many micro cells to make use of:\n\n\nCode\nfrause oaxaca, clear\ndrop if lnwage==.\negen strata=group(educ isco)\nbysort strata:egen flag=mean(female)\nlist educ isco female if (flag==0 | flag==1) & educ == 10, sep(0)\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n\n      +----------------------+\n      | educ   isco   female |\n      |----------------------|\n 158. |   10      1        0 |\n 159. |   10      1        0 |\n 197. |   10      7        0 |\n 198. |   10      7        0 |\n 199. |   10      9        1 |\n 200. |   10      9        1 |\n      +----------------------+"
  },
  {
    "objectID": "adv_class/10matching.html#alternative-matching-as-a-weighted",
    "href": "adv_class/10matching.html#alternative-matching-as-a-weighted",
    "title": "Matching and Re-weighting",
    "section": "Alternative: Matching as a weighted",
    "text": "Alternative: Matching as a weighted\nThe problem of curse of dimensional states that as the number of desired characteristics to match increase, fewer “twins” will be available in the data. At the end…no one will be like you!\nThe alternative, is to look into People that are sufficiently close so they can be used for matching.\n\\[\n\\begin{aligned}\nATT_i &= Y_i -  \\sum_{j \\in C} w(x_j,x_i) Y_j \\\\\nATT   &= \\frac{1}{N_T}\\sum(ATT_i)  \\\\\nATT   &=E(Y|D=1) - E_i\\left( \\sum_{j \\in C} w(x_j,x_i) Y_j  \\Big| D=0 \\right)\n\\end{aligned}\n\\]\nDepending how \\(w(.)\\) is defined, we would be facing different kinds of matching estimators."
  },
  {
    "objectID": "adv_class/10matching.html#matching-on-covariates",
    "href": "adv_class/10matching.html#matching-on-covariates",
    "title": "Matching and Re-weighting",
    "section": "Matching on covariates",
    "text": "Matching on covariates\nThe first decision to take is whether one should find matches based on covariates, or based on scores (propensity scores).\nUsing covariates implies that will aim to find the closest “twin” possible, based on multiple dimensions: \\[\n\\begin{aligned}\nEclidean=d(x_i,x_j) &=\\sqrt{ (x_i-x_j)'(x_i-x_j)} \\\\\nWEclidean=d(x_i,x_j) &=\\sqrt{ (x_i-x_j)'W (x_i-x_j)} \\\\\nMaha =d(x_i,x_j) &=\\sqrt{(x_i-x_j)'S^{-1}(x_i-x_j)}\n\\end{aligned}\n\\]\nDistance measures are used to identify the closest matches to a given observation, and thus the weight assigned to that observation.\nHas the advantage of looking at individuals who are indeed close to each other, but becomes more difficult as the dimensionality of X’s increase. (you will not find close matches)"
  },
  {
    "objectID": "adv_class/10matching.html#matching-on-scores",
    "href": "adv_class/10matching.html#matching-on-scores",
    "title": "Matching and Re-weighting",
    "section": "Matching on Scores",
    "text": "Matching on Scores\nA second approach is to match individuals based on some summary index that condenses the information in \\(X\\) into a single scalar \\(h(x)\\), reducing the dimensionality problem fron K to 1.\nFew candidates:\n\nPropensity Score: \\(P(D|X)\\) based on a logit/probit/binomial model. Most common approach!\nPredicted Mean: \\(X\\beta\\) if there is information on outcome to be predicted\nPCA: Using Principal components to reduce dimensionality before Matching\n\nSince there is only 1 dimension to consider, multiple distance measures are possible:\n\nnearest neighbors, kernel weight matching, radious matching.\n\nBut one has to be careful with the approach. King and Nielsen (2019) Argue about the risks of PSM"
  },
  {
    "objectID": "adv_class/10matching.html#vs-k-matching-with-and-without-replacement",
    "href": "adv_class/10matching.html#vs-k-matching-with-and-without-replacement",
    "title": "Matching and Re-weighting",
    "section": "1 vs K matching; With and without replacement",
    "text": "1 vs K matching; With and without replacement\nTwo additional questions remain regarding matching. How many “twins” to use, and if twins will be obtained with/without replacement.\n\nFewer matches reduce bias (choosing only the closest observation), but increase variance.\nMore matches increase bias, but reduce variance. (because of less optimal matches)\nwith replacement: control units may be used more than once. This will improve matching quality reducing bias. But by using the same units multiple times, it will increase variance.\nwithout replacement: Control units are used once, potentially reducing matching quality, but reducing variance. It will be order dependent.\n\nsee Caliendo and Kopeing (2008)"
  },
  {
    "objectID": "adv_class/10matching.html#what-about-se-and-statistical-inference",
    "href": "adv_class/10matching.html#what-about-se-and-statistical-inference",
    "title": "Matching and Re-weighting",
    "section": "What about SE? and Statistical inference?",
    "text": "What about SE? and Statistical inference?\nWell….this is one of the few cases where Bootstrapping WON’T work!\nStandard errors are more cumbersome. So we will just rely on software results"
  },
  {
    "objectID": "adv_class/10matching.html#other-considerations",
    "href": "adv_class/10matching.html#other-considerations",
    "title": "Matching and Re-weighting",
    "section": "Other considerations",
    "text": "Other considerations\nOnce you have chosen your matching method, find your “statistical twins”, and estimate your differences you are done! (or are you)\nNot yet…common practice: Evaluate the balance of your data\n\nMatching aims to reduce or eliminate differences in characteristics between treatment and control units. Thus, one should evaluate the differences (before and after match) of your characteristis\n\n\nCheck for overlapping condition.\n\n\neither variable by variable or with pscore\n\n\nAssess Matching Quality: Have differences across groups vanished?\n\n\nCheck Standardized differences \\(\\frac{\\mu_1 - \\mu_2}{\\sqrt{0.5*(V_1 + V_2)}}\\)\nt-tests\nPR2 of regression with matched data"
  },
  {
    "objectID": "adv_class/10matching.html#implementation",
    "href": "adv_class/10matching.html#implementation",
    "title": "Matching and Re-weighting",
    "section": "Implementation",
    "text": "Implementation\nIn Stata, there are at least two approaches that can be used for matching:\n\npsmatch2 (from ssc)\nteffects (Official Stata command)\n\nWe will use this to answer a simple question:\n\nWhat is the impact of Traing Jobs on Earnings?"
  },
  {
    "objectID": "adv_class/10matching.html#example",
    "href": "adv_class/10matching.html#example",
    "title": "Matching and Re-weighting",
    "section": "Example",
    "text": "Example\nThis file contains information on experimental and observed data for the analysis of training on earnings program:\n\n\nCode\nuse https://friosavila.github.io/playingwithstata/drdid/lalonde.dta, clear\nkeep if year==1978 \ndrop if dwincl==0\nlabel define sample 1 \"exper\"  2 \"CPS\" 3 \"PSID\"\nlabel values sample sample\ntab sample treated,m\n\n\n(19,204 observations deleted)\n(277 observations deleted)\n\n           |             treated\n    sample |         0          1          . |     Total\n-----------+---------------------------------+----------\n     exper |       260        185          0 |       445 \n       CPS |         0          0     15,992 |    15,992 \n      PSID |         0          0      2,490 |     2,490 \n-----------+---------------------------------+----------\n     Total |       260        185     18,482 |    18,927 \n\n\nFirst Experimental design - RCT\n\n\nCode\nreg re treated\ntabstat age educ black married nodegree , by(treated)\nlogit treated age educ black hisp married nodegree \n\n\n\n      Source |       SS           df       MS      Number of obs   =       445\n-------------+----------------------------------   F(1, 443)       =      8.04\n       Model |   348013183         1   348013183   Prob &gt; F        =    0.0048\n    Residual |  1.9178e+10       443  43290369.3   R-squared       =    0.0178\n-------------+----------------------------------   Adj R-squared   =    0.0156\n       Total |  1.9526e+10       444  43976681.9   Root MSE        =    6579.5\n\n------------------------------------------------------------------------------\n          re | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     treated |   1794.342   632.8534     2.84   0.005     550.5745     3038.11\n       _cons |   4554.801   408.0459    11.16   0.000     3752.855    5356.747\n------------------------------------------------------------------------------\n\nSummary statistics: Mean\nGroup variable: treated \n\n treated |       age      educ     black   married  nodegree\n---------+--------------------------------------------------\n       0 |  25.05385  10.08846  .8269231  .1538462  .8346154\n       1 |  25.81622  10.34595  .8432432  .1891892  .7081081\n---------+--------------------------------------------------\n   Total |  25.37079  10.19551  .8337079  .1685393  .7820225\n------------------------------------------------------------\n\nIteration 0:  Log likelihood =     -302.1  \nIteration 1:  Log likelihood = -294.72908  \nIteration 2:  Log likelihood = -294.71464  \nIteration 3:  Log likelihood = -294.71464  \n\nLogistic regression                                     Number of obs =    445\n                                                        LR chi2(6)    =  14.77\n                                                        Prob &gt; chi2   = 0.0221\nLog likelihood = -294.71464                             Pseudo R2     = 0.0244\n\n------------------------------------------------------------------------------\n     treated | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0059171   .0142668     0.41   0.678    -.0220452    .0338794\n        educ |  -.0639597    .071354    -0.90   0.370     -.203811    .0758916\n       black |  -.2543689   .3639735    -0.70   0.485    -.9677438    .4590061\n        hisp |  -.8291587   .5042305    -1.64   0.100    -1.817432     .159115\n     married |   .2342415   .2661824     0.88   0.379    -.2874665    .7559495\n    nodegree |  -.8385524   .3093833    -2.71   0.007    -1.444933   -.2321722\n       _cons |   1.053028   1.047384     1.01   0.315    -.9998064    3.105862\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/10matching.html#section-1",
    "href": "adv_class/10matching.html#section-1",
    "title": "Matching and Re-weighting",
    "section": "",
    "text": "Then using PScore Matching CPS\n\n\nCode\nkeep if treated == 1 | sample ==2\nreplace treated=0 if treated==.\nreg re treated\ntabstat age educ black hisp married nodegree , by(treated)\n\n\n(2,750 observations deleted)\n(15,992 real changes made)\n\n      Source |       SS           df       MS      Number of obs   =    16,177\n-------------+----------------------------------   F(1, 16175)     =    142.43\n       Model |  1.3206e+10         1  1.3206e+10   Prob &gt; F        =    0.0000\n    Residual |  1.4997e+12    16,175  92717515.8   R-squared       =    0.0087\n-------------+----------------------------------   Adj R-squared   =    0.0087\n       Total |  1.5129e+12    16,176  93528158.4   Root MSE        =      9629\n\n------------------------------------------------------------------------------\n          re | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     treated |  -8497.516   712.0207   -11.93   0.000    -9893.156   -7101.877\n       _cons |   14846.66   76.14292   194.98   0.000     14697.41    14995.91\n------------------------------------------------------------------------------\n\nSummary statistics: Mean\nGroup variable: treated \n\n treated |       age      educ     black      hisp   married  nodegree\n---------+------------------------------------------------------------\n       0 |  33.22524  12.02751  .0735368   .072036  .7117309  .2958354\n       1 |  25.81622  10.34595  .8432432  .0594595  .1891892  .7081081\n---------+------------------------------------------------------------\n   Total |  33.14051  12.00828  .0823391  .0718922  .7057551  .3005502\n----------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/10matching.html#section-2",
    "href": "adv_class/10matching.html#section-2",
    "title": "Matching and Re-weighting",
    "section": "",
    "text": "We need to do trimming\n\n\nCode\nbysort educ black hisp married:egen n11=sum(treated==1)\nbysort age  black hisp married:egen n22=sum(treated==1)\ndrop if n11==0 | n22 ==0\ntabstat age educ black hisp married nodegree , by(treated)\nreg re treated\n\n\n(13,536 observations deleted)\n\nSummary statistics: Mean\nGroup variable: treated \n\n treated |       age      educ     black      hisp   married  nodegree\n---------+------------------------------------------------------------\n       0 |  24.24145  11.69788   .252443  .0260586  .3346906  .2569218\n       1 |  25.81622  10.34595  .8432432  .0594595  .1891892  .7081081\n---------+------------------------------------------------------------\n   Total |  24.35176  11.60318  .2938281  .0283983  .3244983  .2885271\n----------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     2,641\n-------------+----------------------------------   F(1, 2639)      =     73.89\n       Model |  5.7607e+09         1  5.7607e+09   Prob &gt; F        =    0.0000\n    Residual |  2.0575e+11     2,639  77964783.1   R-squared       =    0.0272\n-------------+----------------------------------   Adj R-squared   =    0.0269\n       Total |  2.1151e+11     2,640  80117339.3   Root MSE        =    8829.8\n\n------------------------------------------------------------------------------\n          re | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     treated |  -5786.584   673.1834    -8.60   0.000    -7106.605   -4466.564\n       _cons |   12135.73   178.1702    68.11   0.000     11786.36     12485.1\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/10matching.html#section-3",
    "href": "adv_class/10matching.html#section-3",
    "title": "Matching and Re-weighting",
    "section": "",
    "text": "Lets do some matching\n\n\nCode\nteffects nnmatch (re age educ black   married nodegree  ) (treated)\ntebalance summarize\nteffects nnmatch (re age educ black   married nodegree  ) (treated), nn(2)\ntebalance summarize\nteffects psmatch (re) (treated age educ black   married nodegree  )\ntebalance summarize\nteffects psmatch (re) (treated age educ black   married nodegree  ) ,  nn(2)\ntebalance summarize\n\n\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : nearest-neighbor matching     Matches: requested =          1\nOutcome model  : matching                                     min =          1\nDistance metric: Mahalanobis                                  max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |  -3685.665   1188.666    -3.10   0.002    -6015.407   -1355.923\n------------------------------------------------------------------------------\n(refitting the model using the generate() option)\n\nCovariate balance summary\n\n                         Raw      Matched\n-----------------------------------------\nNumber of obs =        2,641        5,282\nTreated obs   =          185        2,641\nControl obs   =        2,456        2,641\n-----------------------------------------\n\n-----------------------------------------------------------------\n                |Standardized differences          Variance ratio\n                |        Raw     Matched           Raw    Matched\n----------------+------------------------------------------------\n            age |   .2342346    -.015417      1.305844   .8410946\n           educ |  -.7684118   -.0812288      1.881909   .8598207\n          black |   1.473105           0      .7039609          1\n        married |  -.3351313   -.0008087      .6923501    .999393\n       nodegree |   1.010393           0      1.088086          1\n-----------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : nearest-neighbor matching     Matches: requested =          2\nOutcome model  : matching                                     min =          2\nDistance metric: Mahalanobis                                  max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |  -5166.888   1107.653    -4.66   0.000    -7337.848   -2995.929\n------------------------------------------------------------------------------\n(refitting the model using the generate() option)\n\nCovariate balance summary\n\n                         Raw      Matched\n-----------------------------------------\nNumber of obs =        2,641        5,282\nTreated obs   =          185        2,641\nControl obs   =        2,456        2,641\n-----------------------------------------\n\n-----------------------------------------------------------------\n                |Standardized differences          Variance ratio\n                |        Raw     Matched           Raw    Matched\n----------------+------------------------------------------------\n            age |   .2342346   -.0209048      1.305844   .7345997\n           educ |  -.7684118   -.0385284      1.881909   .8978301\n          black |   1.473105    .0074673      .7039609   1.006716\n        married |  -.3351313    -.004586      .6923501   .9965432\n       nodegree |   1.010393    .0016705      1.088086   1.001557\n-----------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : propensity-score matching     Matches: requested =          1\nOutcome model  : matching                                     min =          1\nTreatment model: logit                                        max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |  -4278.549   1135.847    -3.77   0.000    -6504.768   -2052.331\n------------------------------------------------------------------------------\n(refitting the model using the generate() option)\n\nCovariate balance summary\n\n                         Raw      Matched\n-----------------------------------------\nNumber of obs =        2,641        5,282\nTreated obs   =          185        2,641\nControl obs   =        2,456        2,641\n-----------------------------------------\n\n-----------------------------------------------------------------\n                |Standardized differences          Variance ratio\n                |        Raw     Matched           Raw    Matched\n----------------+------------------------------------------------\n            age |   .2342346    .0014058      1.305844   .9313458\n           educ |  -.7684118   -.1308249      1.881909   .9665937\n          black |   1.473105   -.0926638      .7039609     .90999\n        married |  -.3351313   -.0973289      .6923501   .9197524\n       nodegree |   1.010393    .0821105      1.088086    1.07103\n-----------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : propensity-score matching     Matches: requested =          2\nOutcome model  : matching                                     min =          2\nTreatment model: logit                                        max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |  -4380.078   1158.019    -3.78   0.000    -6649.754   -2110.403\n------------------------------------------------------------------------------\n(refitting the model using the generate() option)\n\nCovariate balance summary\n\n                         Raw      Matched\n-----------------------------------------\nNumber of obs =        2,641        5,282\nTreated obs   =          185        2,641\nControl obs   =        2,456        2,641\n-----------------------------------------\n\n-----------------------------------------------------------------\n                |Standardized differences          Variance ratio\n                |        Raw     Matched           Raw    Matched\n----------------+------------------------------------------------\n            age |   .2342346     -.06133      1.305844   .8834346\n           educ |  -.7684118   -.1321518      1.881909   1.021302\n          black |   1.473105   -.0698339      .7039609    .933348\n        married |  -.3351313   -.0414439      .6923501   .9674741\n       nodegree |   1.010393    .0939209      1.088086   1.080951\n-----------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/10matching.html#section-4",
    "href": "adv_class/10matching.html#section-4",
    "title": "Matching and Re-weighting",
    "section": "",
    "text": "A missing variable? Earnings in previous year. May capture information of Need to do treatment (selection)\n\n\nCode\ntabstat age educ black hisp married nodegree re74, by(treated)\ngen dre = re-re74\nteffects nnmatch (dre age educ black   married nodegree  ) (treated)\n\nteffects nnmatch (dre age educ black   married nodegree  ) (treated), nn(2)\n\nteffects psmatch (dre) (treated age educ black   married nodegree  )\n\nteffects psmatch (dre) (treated age educ black   married nodegree  ) ,  nn(2)\n\n\n\nSummary statistics: Mean\nGroup variable: treated \n\n treated |       age      educ     black      hisp   married  nodegree\n---------+------------------------------------------------------------\n       0 |  24.24145  11.69788   .252443  .0260586  .3346906  .2569218\n       1 |  25.81622  10.34595  .8432432  .0594595  .1891892  .7081081\n---------+------------------------------------------------------------\n   Total |  24.35176  11.60318  .2938281  .0283983  .3244983  .2885271\n----------------------------------------------------------------------\n\n treated |      re74\n---------+----------\n       0 |  9347.406\n       1 |  2095.574\n---------+----------\n   Total |  8839.421\n--------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : nearest-neighbor matching     Matches: requested =          1\nOutcome model  : matching                                     min =          1\nDistance metric: Mahalanobis                                  max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |   2616.653   1803.172     1.45   0.147    -917.4997    6150.806\n------------------------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : nearest-neighbor matching     Matches: requested =          2\nOutcome model  : matching                                     min =          2\nDistance metric: Mahalanobis                                  max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |   730.2925    1674.91     0.44   0.663     -2552.47    4013.055\n------------------------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : propensity-score matching     Matches: requested =          1\nOutcome model  : matching                                     min =          1\nTreatment model: logit                                        max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |   2162.311    1740.12     1.24   0.214    -1248.262    5572.884\n------------------------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : propensity-score matching     Matches: requested =          2\nOutcome model  : matching                                     min =          2\nTreatment model: logit                                        max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |    1833.03   1739.496     1.05   0.292    -1576.318    5242.379\n------------------------------------------------------------------------------\n\n\nIn this case, Matching alone could not get the right answer. Who were the most likely to “go to the training?”\nSo instead we change the question: How much the change in earnings compare across groups."
  },
  {
    "objectID": "adv_class/10matching.html#wait-what-about-reweighting",
    "href": "adv_class/10matching.html#wait-what-about-reweighting",
    "title": "Matching and Re-weighting",
    "section": "Wait: What about Reweighting?",
    "text": "Wait: What about Reweighting?\nAn alternative method to Matching is to do Re-weighting.\nWe have seen this!\nYour control group has a distribution \\(g(x)\\) and your treatment \\(f(x)\\). We can use some weighting factors \\(h(x)\\) that reshapes \\(g(x)\\rightarrow \\hat f(x)\\).\nHow? Using Propensity scores\nWhy does it work? Just as matching, your goal is to compare distributions of outcomes, forcing differences in observed characteristics to be the same.\nIPW, does this by reweighting the distribution! (rather than matching)"
  },
  {
    "objectID": "adv_class/10matching.html#inverse-probability-weightingipw",
    "href": "adv_class/10matching.html#inverse-probability-weightingipw",
    "title": "Matching and Re-weighting",
    "section": "Inverse Probability Weighting:IPW",
    "text": "Inverse Probability Weighting:IPW\ns1: Estimate Pscore\n\\[\np(D=1|X)=F(X\\beta)\n\\] S2: Estimate IPW\nFor ATT: \\(W(D=1,x)=1 \\ \\&  \\ W(D=0,X)=\\frac{\\hat p(x)}{1-\\hat p(x)}\\)\nFor ATU: \\(W(D=0,x)=1 \\ \\&  \\ W(D=1,X)=\\frac{1-\\hat p(x)}{\\hat p(x)}\\)\nFor ATE: \\(W(D=0,x)=\\frac{1}{1-\\hat p(x)} \\ \\&  \\ W(D=1,X)=\\frac{1}{\\hat p(x)}\\)\ns3: Estimate Treatment effect:\n\\[\nTE = \\sum_{i \\in D=1} w_i^s(1) Y_i - \\sum_{i \\in D=0} w_i^s(0) Y_i\n\\]"
  },
  {
    "objectID": "adv_class/10matching.html#even-better-go-dr",
    "href": "adv_class/10matching.html#even-better-go-dr",
    "title": "Matching and Re-weighting",
    "section": "Even Better: Go DR!",
    "text": "Even Better: Go DR!\nAn interesting advantage of IPW approach is that you can gain efficiency using Doubly Robust Methods. Namely, instead of comparing outcomes directly, you could compare predicted outcomes!\n\\[\n\\begin{aligned}\nATT &= \\frac{1}{N_t}\\sum(Y_1-X'\\hat\\beta_w^0) \\\\\nATU &= \\frac{1}{N_c}\\sum(X'\\hat\\beta_w^1-Y_0) \\\\\nATE &= \\frac{1}{N}\\sum(X'\\hat\\beta_w^1-X'\\hat\\beta_w^0)\n\\end{aligned}\n\\] where \\(\\hat \\beta^k_w\\) can be modeled using weighted least squares"
  },
  {
    "objectID": "adv_class/10matching.html#comparing-to-matching",
    "href": "adv_class/10matching.html#comparing-to-matching",
    "title": "Matching and Re-weighting",
    "section": "Comparing to Matching",
    "text": "Comparing to Matching\nteffects ipw (re) (treated age educ black   married nodegree) , iter(3) nolog\nteffects ipwra (re age educ black   married nodegree) (treated age educ black married nodegree), iter(3) nolog\nteffects ipw (dre) (treated age educ black   married nodegree), iter(3) nolog\nteffects ipwra (dre age educ black   married nodegree) (treated age educ black   married nodegree), iter(3) nolog\n\nTreatment-effects estimation                    Number of obs     =      2,641\nEstimator      : inverse-probability weights\nOutcome model  : weighted mean\nTreatment model: logit\n------------------------------------------------------------------------------\n             |               Robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |  -4833.352   1088.667    -4.44   0.000    -6967.101   -2699.603\n-------------+----------------------------------------------------------------\nPOmean       |\n     treated |\n          0  |   11979.19   179.1903    66.85   0.000     11627.99     12330.4\n------------------------------------------------------------------------------\n\nTreatment-effects estimation                    Number of obs     =      2,641\nEstimator      : IPW regression adjustment\nOutcome model  : linear\nTreatment model: logit\n------------------------------------------------------------------------------\n             |               Robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |   -4835.38   1012.598    -4.78   0.000    -6820.036   -2850.724\n-------------+----------------------------------------------------------------\nPOmean       |\n     treated |\n          0  |   11976.52   179.0958    66.87   0.000     11625.49    12327.54\n------------------------------------------------------------------------------\nWarning: Convergence not achieved.\n\nTreatment-effects estimation                    Number of obs     =      2,641\nEstimator      : inverse-probability weights\nOutcome model  : weighted mean\nTreatment model: logit\n------------------------------------------------------------------------------\n             |               Robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |    1475.71   1792.427     0.82   0.410    -2037.382    4988.802\n-------------+----------------------------------------------------------------\nPOmean       |\n     treated |\n          0  |   2746.475   161.2845    17.03   0.000     2430.363    3062.587\n------------------------------------------------------------------------------\nWarning: Convergence not achieved.\n\nTreatment-effects estimation                    Number of obs     =      2,641\nEstimator      : IPW regression adjustment\nOutcome model  : linear\nTreatment model: logit\n------------------------------------------------------------------------------\n             |               Robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |   1286.605   1516.493     0.85   0.396    -1685.666    4258.875\n-------------+----------------------------------------------------------------\nPOmean       |\n     treated |\n          0  |   2754.756   161.4406    17.06   0.000     2438.338    3071.173\n------------------------------------------------------------------------------\nWarning: Convergence not achieved."
  },
  {
    "objectID": "adv_class/08panel_FE.html#re-cap-potential-outcome-model",
    "href": "adv_class/08panel_FE.html#re-cap-potential-outcome-model",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Re-Cap: Potential outcome Model",
    "text": "Re-Cap: Potential outcome Model\nIn the ideal world, where we can see all possible outcomes and scenarios of your potential treatments, it will be very simple to estimate treatment effects:\n\\[\n\\delta_i = Y_i(1)-Y_i(0)\n\\]\nWhy does this work??\nOne way to understand this it to imagine that potential outcomes are a function of all observed and unobserved individual characteristics, plust the treatment Status.\n\\[y_i(D)=y_i(X,u,D)\n\\]\nSo when comparing a person with himself (clones or parallel worlds), we know (or at least expect) that everything else is the same, except for the Treatment Status.\nDifferences between the two states are explained only by the treatment!"
  },
  {
    "objectID": "adv_class/08panel_FE.html#the-problem-and-first-solution-rct",
    "href": "adv_class/08panel_FE.html#the-problem-and-first-solution-rct",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "The Problem and first Solution RCT",
    "text": "The Problem and first Solution RCT\nWe do not observe both States at the same time. People will either be treated or untreated, not both.\nSo what can we do?\n\nWe need to find good counterfactuals!\n\nOne way to do so is via RCT, for example using a lottery!\n\nWhy does it work?\n\nPotential outcomes will be unrelated to treatment, because treatmet is assigned at random.\nHere, it also means that \\(X's\\) and \\(u's\\) will be similar across groups (because of random assigment)\nBut…you cannot estimatate individual effects, but at least estimate aggregate effects (ATE = ATT = ATU)"
  },
  {
    "objectID": "adv_class/08panel_FE.html#other-solutions",
    "href": "adv_class/08panel_FE.html#other-solutions",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Other Solutions?",
    "text": "Other Solutions?\nSo RCTs can be very expensive, and difficult to implement after the fact. In those situations, however, you can use observed data to try answering the same questions!.\nOne option? Something we have done before…Regression Analysis!\n\\[y_i = a_0 + \\delta D_i + X_i\\beta + e_i\\]\nThe idea is that you directly control for all confounding factors that could be related to \\(y_i\\) and \\(d_i\\).\nIn other words, you add controls until \\(D_i\\) is exogenous! \\(E(e_i|D)=0\\)"
  },
  {
    "objectID": "adv_class/08panel_FE.html#implications-to-the-po-model",
    "href": "adv_class/08panel_FE.html#implications-to-the-po-model",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Implications to the PO model?",
    "text": "Implications to the PO model?\n\nAssumes all individuals have the same outcome structure (\\(\\beta s\\)), except for the TE\nThe treatment is effect is homogenous (no heterogeneity)\nand that functional form is correct (for extrapolation)\n\nHowever, explicitly controlling for covariates, balances characteristics (FWL):\n\\[\n\\begin{aligned}\nD_i &= X\\gamma + v_i \\\\\ny_i &= a_0 + \\delta v_i + u_i \\\\\n\\delta &=\\frac{1}{N} \\sum \\frac{D_i - X\\gamma}{var(v_i)} y_i\n\\end{aligned}\n\\]\n\nTreated units will get positive weights, and controls negative weights, with exceptions because of the LPM.\nWeights will “balance Samples” to estimate ATE."
  },
  {
    "objectID": "adv_class/08panel_FE.html#controlling-for-unobservables",
    "href": "adv_class/08panel_FE.html#controlling-for-unobservables",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Controlling for Unobservables",
    "text": "Controlling for Unobservables"
  },
  {
    "objectID": "adv_class/08panel_FE.html#what-if-you-can-see-xs",
    "href": "adv_class/08panel_FE.html#what-if-you-can-see-xs",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "What if you can See X’s",
    "text": "What if you can See X’s\nSome times, you may have situations where some covariates cannot be observed (Z_i): \\[\ny_i = \\delta D_i +X_i \\beta + Z_i \\gamma + e_i\n\\]\nIf \\(Z_i\\) is unrelated to \\(D_i\\), you are on the clear. If its unrelated to \\(Y_i\\) you are also ok. But what if that doesn happen?\n\nThen you have a problem!\n\nYou no longer can use regression, because the potential outcomes will no longer be independent of the treatment.\nyou are dooomed!\n(when would this happen)"
  },
  {
    "objectID": "adv_class/08panel_FE.html#having-access-to-more-data",
    "href": "adv_class/08panel_FE.html#having-access-to-more-data",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Having access to More Data",
    "text": "Having access to More Data\nSolution?: Say you have access to panel data: Same individuals across time:\n\\[\ny_{it} = \\delta D_{it} +X_{it} \\beta + Z_{it} \\gamma + e_{it}\n\\]\nIf we can’t measure \\(Z_{it}\\), and you estimate this using Pool OLS (just simple OLS), you still need the assumption that:\n\\[E(Z_{it}\\gamma + e_{it}|D_it)=0\n\\]\nBut that doesnt solve the problem if \\(Z_{it}\\) is related to \\(D_it\\).\nOne option, in cases like this, is assuming that individual unobservables are fixed across time:\n\\[\ny_{it} = \\delta D_{it} +X_{it} \\beta + Z_{i} \\gamma + e_{it}\n\\]\nin which case, it may be possible to estimate Treatment effects"
  },
  {
    "objectID": "adv_class/08panel_FE.html#fixed-effects",
    "href": "adv_class/08panel_FE.html#fixed-effects",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nWith panel data and assuming unobservables are fixed across time, estimating TE is “Easy”. Just add Dummies for each individual!\n\\[\ny_{it}= \\delta D_{it} +X_{it} \\beta + \\sum d_i \\gamma_i + e_{it}\n\\]\nHere \\(d_i \\gamma_i\\) is our proxy for ALL unobserved factors. OLS can be used to estimate ATEs\nThis happens because we can estimate potential outcome under the same assumptions as before.\nyou could, in fact, consider adding fixed effects for all dimensions you consider important to account for:\n\nCity, school, region, age, industry, etc\n\nThe only limitation…how many dummies can your computer handle? What happens internally?"
  },
  {
    "objectID": "adv_class/08panel_FE.html#fixed-effects-estimation---the-variation-within",
    "href": "adv_class/08panel_FE.html#fixed-effects-estimation---the-variation-within",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Fixed Effects: Estimation - The variation within",
    "text": "Fixed Effects: Estimation - The variation within\nThe obvious approach is using dummies. But that can take you only so far (why?), and may create other problems! (excluded variables)\nThe alternative is using the within estimator. Say we take the means across individuals, and use that to substract information from the original regression:\n\\[\n\\begin{aligned}\ny_{it} &= \\delta D_{it} +X_{it} \\beta + Z_{i} \\gamma + e_{it} \\\\\n\\bar y_i &= \\delta \\bar D_i + \\bar X_i \\beta +  Z_{i} \\gamma + \\bar e_i \\\\\ny_{it}-\\bar y_i = \\tilde y_{it} &=\\delta \\tilde D_{it} + \\tilde X_{it}\\beta+\\tilde e_{it}\n\\end{aligned}\n\\]\nLast equation is easier to estimate (no dummies!) however you need within variation. IF unobserved factors are fixed, they will be “absorbed”.\nAlso, the SE will have to be adjusted for degrees of freedom. (but nothing else)\nThis is nothing else but the use of FWL and regression on residuals."
  },
  {
    "objectID": "adv_class/08panel_FE.html#dont-forget-random-effects",
    "href": "adv_class/08panel_FE.html#dont-forget-random-effects",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Dont Forget Random Effects",
    "text": "Dont Forget Random Effects\nThis approach is more efficient than Fixed effects because you don’t estimate fixed effects, just the distribution.\nSo how does this affect the estimation:\n\nErrors have two components. One time fixed \\(e_i\\), and one time variant \\(u_{it}\\). Then total errors will be correlated with themselves across time \\[corr(v_{it}, v_{is}) =  corr(e_i+u_{it}, e_i+u_{is}) = \\sigma^2_e\n\\]\nApply FGLS to eliminating this source of auto-correlation! \\[y_{it}-\\lambda \\bar y_i = (X_{it}-\\lambda \\bar X_it) + v_{it}\\]\n\nBut, you need the assumption that unobservables \\(e_i\\) are unrelated to \\(X's\\). (because we are not directly controlling for them).\nThe advantage, however, is that you do no need within variation!"
  },
  {
    "objectID": "adv_class/08panel_FE.html#fe-vs-re",
    "href": "adv_class/08panel_FE.html#fe-vs-re",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "FE vs RE",
    "text": "FE vs RE\nSo there are two ways to Analyze data Panel data.\n\nFE: Uses only within variation, is more consistent, but less efficient (too many dummies)\nRE: Uses all variation in data, is less consistent (stronger assumptions), but more efficient!\n\nHow to choose?\nThe Standard approach is to apply a Hausan Test:\n\nH0:\\(\\beta^{FE} = \\beta^{RE}\\) using Chi2\n\nIf they are not different (H0 cannot be rejected), then choose RE (efficient). If they are different then choose FE (consistent)"
  },
  {
    "objectID": "adv_class/08panel_FE.html#more-fixed-effects-twfe---nwfe",
    "href": "adv_class/08panel_FE.html#more-fixed-effects-twfe---nwfe",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "More Fixed effects: TWFE - NWFE?",
    "text": "More Fixed effects: TWFE - NWFE?\nWith multiple sets of fixed effects (individual, time, cohort, region, etc), you can still use dummies to add them to the model.\nBut, you can apply something similar to the previous approach:\n\\[\n\\begin{aligned}\ny_{it} &= \\delta D_{it}+x_{it}\\beta + \\gamma_i + \\gamma_t + e_{it} \\\\\n\\bar y_i &= \\bar D_i +\\bar x_{i}\\beta + \\gamma_i + E(\\gamma_t|i) + \\bar e_i \\\\\n\\bar y_t &= \\bar D_t +\\bar x_{t}\\beta + E(\\gamma_i|t) + \\gamma_t + \\bar e_t \\\\\n\\bar y &= \\bar D +\\bar x\\beta + E(\\gamma_i) + E(\\gamma_t)+ \\bar e \\\\\n\\tilde y_{it} &= y_{it}-\\bar y_i - \\bar y_t + \\bar y\n\\end{aligned}\n\\]\nSo one can estimate the following:\n\\[\n\\tilde y_{it} = \\delta \\tilde D_{it} + \\tilde X_{it} \\beta + \\tilde e_{it}\n\\]\nThis eliminates FE for both time and individual (if panel is balanced)"
  },
  {
    "objectID": "adv_class/08panel_FE.html#second-option",
    "href": "adv_class/08panel_FE.html#second-option",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Second Option:",
    "text": "Second Option:\nAlternatively, you can just run regressions of residuals:\n\\[\nw_{it} = \\gamma^w_i+\\gamma^w_t+rw_{it}\n\\]\nand make regressions using the residuals. (Demeaning also works, but its an iterative process)"
  },
  {
    "objectID": "adv_class/08panel_FE.html#stata-example",
    "href": "adv_class/08panel_FE.html#stata-example",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Stata Example",
    "text": "Stata Example\n\n\n\n\n\n\n\nCode\n#delimit;\nfrause school93_98, clear;\nxtset schid year;\nqui:reg math4 lunch lenrol lrexpp                     ; est sto m1;\nqui:xtreg math4 lunch lenrol lrexpp                   ; est sto m2;\nqui:xtreg math4 lunch lenrol lrexpp, fe               ; est sto m3;\nqui:reghdfe math4 lunch lenrol lrexpp, abs(schid)     ; est sto m4;\nqui:reghdfe math4 lunch lenrol lrexpp, abs(schid year); est sto m5;\nesttab m1 m2 m3 m4 m5, mtitle(ols re fe refe1 refe2) compress se b(3);\nhausman m3 m2;\n\n\n\nPanel variable: schid (strongly balanced)\n Time variable: year, 1993 to 1998\n         Delta: 1 unit\n\n---------------------------------------------------------------------------\n                 (1)          (2)          (3)          (4)          (5)   \n                 ols           re           fe        refe1        refe2   \n---------------------------------------------------------------------------\nlunch         -0.413***    -0.370***     0.057        0.057       -0.062*  \n             (0.007)      (0.011)      (0.031)      (0.031)      (0.026)   \n\nlenrol        -0.121        0.936        8.766***     8.766***     0.297   \n             (0.425)      (0.616)      (1.704)      (1.704)      (1.468)   \n\nlrexpp        28.887***    39.161***    46.450***    46.450***     2.799*  \n             (0.860)      (0.878)      (1.006)      (1.006)      (1.265)   \n\n_cons       -162.292***  -254.864***  -377.338***  -377.423***    37.398*  \n             (7.960)      (8.681)     (14.913)     (14.918)     (15.847)   \n---------------------------------------------------------------------------\nN               9369         9369         9369         9328         9328   \n---------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n                 ---- Coefficients ----\n             |      (b)          (B)            (b-B)     sqrt(diag(V_b-V_B))\n             |       m3           m2         Difference       Std. err.\n-------------+----------------------------------------------------------------\n       lunch |     .056932    -.3703211        .4272531        .0287753\n      lenrol |    8.766051     .9357725        7.830279        1.588902\n      lrexpp |    46.44966     39.16107        7.288595        .4915896\n------------------------------------------------------------------------------\n                          b = Consistent under H0 and Ha; obtained from xtreg.\n           B = Inconsistent under Ha, efficient under H0; obtained from xtreg.\n\nTest of H0: Difference in coefficients not systematic\n\n    chi2(3) = (b-B)'[(V_b-V_B)^(-1)](b-B)\n            = 627.26\nProb &gt; chi2 = 0.0000"
  },
  {
    "objectID": "adv_class/08panel_FE.html#correlated-random-effects",
    "href": "adv_class/08panel_FE.html#correlated-random-effects",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Correlated Random Effects",
    "text": "Correlated Random Effects\nRandom effects model may produce inconsistent results, because it assumes unobserved factors are uncorrelated to characteristics.\nFixed effects controls for individual effects explicitly, or via demeaning.\nA 3rd approach is known as CRE model. A more explicit modeling of the unobserved but fixed components.\n\nCall the unobserved component \\(a_i\\), and say we suspect it may be related with individual characteristics.\nBecause \\(a_i\\) is constant over time, it may be reasonable assuming its correlated with individual average characteristics: \\[a_i = a + \\bar X_i \\gamma + r_i\n\\qquad(1)\\]\n\nBy construction, \\(r_i\\) and \\(X_{it} \\&  \\bar X_i\\) will be uncorrelated. So lets just add that to the main model"
  },
  {
    "objectID": "adv_class/08panel_FE.html#cre",
    "href": "adv_class/08panel_FE.html#cre",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "CRE",
    "text": "CRE\nLets add Equation 1 to our main equation. \\[\ny_i = \\beta X_{it} +  \\theta Z_{i} + \\bar X_i \\gamma + r_i + e_{it}\n\\]\nThis equation can now be estimated using RE, because it already allows controls for the correlation of unobserved factors and the individual effects.\nYou can also estimate the model using pool OLS, clustering errors at individual level.\nResult:\n\nyou now have a model that allows for time variant and time fixed components, that is consistent as FE (same coefficients).\n\nUses:\n\nSimpler way to test for FE vs RE (are the \\(\\gamma 's\\) significant?)\nthere is no need for within variation for any variable! (just overall variation)"
  },
  {
    "objectID": "adv_class/08panel_FE.html#cre-in-stata",
    "href": "adv_class/08panel_FE.html#cre-in-stata",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "cre in Stata",
    "text": "cre in Stata\n\n\nCode\n#delimit cr\nfrause school93_98, clear\nreghdfe math4 lunch lenrol lrexpp, abs(schid year) cluster(schid)\n** Experimental\ncre, abs(schid year): reg math4 lunch lenrol lrexpp, cluster(schid)\n\n\n(dropped 41 singleton observations)\n(MWFE estimator converged in 5 iterations)\n\nHDFE Linear regression                            Number of obs   =      9,328\nAbsorbing 2 HDFE groups                           F(   3,   1734) =       2.59\nStatistics robust to heteroskedasticity           Prob &gt; F        =     0.0515\n                                                  R-squared       =     0.7548\n                                                  Adj R-squared   =     0.6985\n                                                  Within R-sq.    =     0.0014\nNumber of clusters (schid)   =      1,735         Root MSE        =    11.5739\n\n                              (Std. err. adjusted for 1,735 clusters in schid)\n------------------------------------------------------------------------------\n             |               Robust\n       math4 | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       lunch |  -.0620863   .0324188    -1.92   0.056    -.1256705    .0014978\n      lenrol |   .2966956   1.484868     0.20   0.842    -2.615625    3.209017\n      lrexpp |   2.798777   1.410581     1.98   0.047     .0321579    5.565397\n       _cons |   37.39798    16.8327     2.22   0.026     4.383449     70.4125\n------------------------------------------------------------------------------\n\nAbsorbed degrees of freedom:\n-----------------------------------------------------+\n Absorbed FE | Categories  - Redundant  = Num. Coefs |\n-------------+---------------------------------------|\n       schid |      1735        1735           0    *|\n        year |         6           1           5     |\n-----------------------------------------------------+\n* = FE nested within cluster; treated as redundant for DoF computation\n\nLinear regression                               Number of obs     =      9,328\n                                                F(9, 1734)        =     821.90\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.4595\n                                                Root MSE          =     15.505\n\n                              (Std. err. adjusted for 1,735 clusters in schid)\n------------------------------------------------------------------------------\n             |               Robust\n       math4 | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       lunch |  -.0620863   .0327488    -1.90   0.058    -.1263177     .002145\n      lenrol |   .2966948   1.509296     0.20   0.844    -2.663538    3.256927\n      lrexpp |   2.798777   1.435068     1.95   0.051    -.0158696    5.613423\n    m1_lunch |  -.3799049   .0343728   -11.05   0.000    -.4473213   -.3124884\n    m2_lunch |  -2.750266   .5058811    -5.44   0.000    -3.742467   -1.758065\n   m1_lenrol |  -2.394415   1.633657    -1.47   0.143    -5.598561     .809731\n   m2_lenrol |  -273.8924   11.25589   -24.33   0.000    -295.9689   -251.8158\n   m1_lrexpp |   5.667779   2.276245     2.49   0.013     1.203305    10.13225\n   m2_lrexpp |    73.4047   3.276595    22.40   0.000      66.9782    79.83119\n       _cons |   37.39799   17.10933     2.19   0.029     3.840901    70.95507\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/08panel_FE.html#caveats-not-everything-is-solved-using-fe",
    "href": "adv_class/08panel_FE.html#caveats-not-everything-is-solved-using-fe",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Caveats: Not everything is solved using FE",
    "text": "Caveats: Not everything is solved using FE\n\nWhile FE allows you do control for unobserve but time fixed factors, it will Not help you if those factors are time varying.\nif \\(e_{it}\\) is different across treated and control groups \\(D_{it}=0,1\\) then TE cannot be estimated.\nThis could happen if cases of reverse causality or\nBecause it depends strongly on within variation, it will be more sensitive to measurement errors. Specifically:\n\n\\[\\beta^{fe} = \\beta * \\left(1-\\frac{\\sigma_v^2}{(\\sigma^2_v+\\sigma^2_x)(1-\\rho_x)}\\right)\n\\]\nIn other words. when \\(X\\) has strong autocorrelation (Stable treatment), the measurement error effect is far larger!"
  },
  {
    "objectID": "adv_class/08panel_FE.html#caveats-fe-makes-things-harder-to-analyze",
    "href": "adv_class/08panel_FE.html#caveats-fe-makes-things-harder-to-analyze",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Caveats: FE makes things harder to analyze",
    "text": "Caveats: FE makes things harder to analyze\n\nWhen using a single FE, OLS using within variation to identify the slope coefficients. How does a change in X’s (compared to the average) affect changes in the outcomes (respect to averages)\nWhen using Two Fixed effects (individuals and time) identification becomes tricky: \\[\\tilde y_{it} = y_{it}-\\bar y_i - \\bar y_t + \\bar y  \\]\n\nWe are looking for variation across time but also across individuals.\n\nwe are using changes in outcome that are different from the average changes in the sample.\n\n\nwith Multiple FE, same story…we are trying to exploit variation across multiple dimensions! Difficult to understand"
  },
  {
    "objectID": "adv_class/08panel_FE.html#caveats-some-times-the-variation-may-be-wrong",
    "href": "adv_class/08panel_FE.html#caveats-some-times-the-variation-may-be-wrong",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Caveats: Some times, the variation may be wrong:",
    "text": "Caveats: Some times, the variation may be wrong:\nConsider:\n\\[y_{it} = a_i + a_t + \\delta D_{it} + e_{it}\\]\nIf \\(D_it\\) changes only for some people at the same time, we are good.\n\nThe variation comes from comparing individuals (before and after) (time variation), who were treated and untreated (individual effects)\n\nBut if \\(D_{it}\\) changes at different times for different people, we have a problem.\n\nWho is being compared???\n\nThose before and after (fine) to those with Status change (D=0 -&gt; D=1) to those whos status do not change! (D=0 to D=0) or (D=1 to D=1)\n\n\nWe will discuss this problem again when talking about DID"
  },
  {
    "objectID": "adv_class/08panel_FE.html#income-schooling-and-ability",
    "href": "adv_class/08panel_FE.html#income-schooling-and-ability",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Income, Schooling, and Ability:",
    "text": "Income, Schooling, and Ability:\nEvidence from a New Sample of Identical Twins\nby\nOrley Ashenfelter and Cecilia Rouse"
  },
  {
    "objectID": "adv_class/08panel_FE.html#motivation",
    "href": "adv_class/08panel_FE.html#motivation",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Motivation:",
    "text": "Motivation:\nIn search of Returns to Education\nThis paper aims to identify returns of education abstracting from the impact of innate ability.\nIn their framework, ability is mostly explained by genetics, thus to control for it, the authors use a sample of identical twins, to “absorb” unobserved genetics using FE.\nThey address some of the problems inherited to FE estimation"
  },
  {
    "objectID": "adv_class/08panel_FE.html#the-model",
    "href": "adv_class/08panel_FE.html#the-model",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "The model",
    "text": "The model\n\nThe theoretical model described states that all individuals have an optimal level of Schooling, such that maximizes the his/her returns.\nHowever, Total schooling may be affected by measurement or optimization errors.\nSchooling will be directly affected by returns to education, but also by the ability of students.\n\nIn their framework, for the twins setup, (log)earnings will be determined by:\n\\[\n\\begin{aligned}\ny_{1j}=A_j + b_j S_{1j} + \\gamma X_j + e_{1j} \\\\\ny_{2j}=A_j + b_j S_{2j} + \\gamma X_j + e_{2j}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/08panel_FE.html#the-model-1",
    "href": "adv_class/08panel_FE.html#the-model-1",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "The model",
    "text": "The model\nBecause ability is related to schooling, they suggest using the following:\n\\[\ny_{ij}=\\lambda(0.5(S_{1j}+S_{2j}) + b_j S_{1j} + \\gamma X_j + v_j+ e_{ij}\n\\]\nWhich is the equivalent to CRE. Or estimate the fixed effects equivalent:\n\\[y_{1j} - y_{2j} = b(S_{2j}-S_{1j}) + e_{2j} - e_{2j}\n\\]\nThe later is a First difference, rather than FE estimator, but they both identical when T=2.\n\nAn additional model the authors use is one where returns to education could be related to ability.\nOr where ability is measured/proxied by parents education. (which is fixed across twins)"
  },
  {
    "objectID": "adv_class/08panel_FE.html#data",
    "href": "adv_class/08panel_FE.html#data",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "adv_class/08panel_FE.html#ols",
    "href": "adv_class/08panel_FE.html#ols",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "OLS",
    "text": "OLS"
  },
  {
    "objectID": "adv_class/08panel_FE.html#fe-re-cre",
    "href": "adv_class/08panel_FE.html#fe-re-cre",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "FE-RE-CRE?",
    "text": "FE-RE-CRE?"
  },
  {
    "objectID": "adv_class/08panel_FE.html#heterogeneity",
    "href": "adv_class/08panel_FE.html#heterogeneity",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Heterogeneity",
    "text": "Heterogeneity"
  },
  {
    "objectID": "adv_class/06nlreg.html#sowhat-is-non-linear",
    "href": "adv_class/06nlreg.html#sowhat-is-non-linear",
    "title": "NLS, IRLS, and MLE",
    "section": "So…what is non-linear?",
    "text": "So…what is non-linear?\nOptions:\n\\[\n\\begin{aligned}\ny = b_0 + b_1 x^{b_2}+e \\\\\ny = exp(b_0+b_1 x)+e \\\\\ny = exp(b_0+b_1 x+e) \\\\\ny = h(x\\beta)+e \\\\\ny = h(x\\beta+e)\n\\end{aligned}\n\\]\nAll of them are Nonlinear, but some of them are linearizable.\n\nA Linearizable model is one you can apply a transformation and make it linear.\n\nFor models #2 and #4, you could apply (logs) or \\(h^{-1}()\\) (if functions), and use OLS. For the others you need other methods."
  },
  {
    "objectID": "adv_class/06nlreg.html#how-do-you-do-nl",
    "href": "adv_class/06nlreg.html#how-do-you-do-nl",
    "title": "NLS, IRLS, and MLE",
    "section": "How do you do, NL?",
    "text": "How do you do, NL?\nNonlinear models are tricky. In contrast with our good old OLS, there no “close form” solution we can plug in:\n\\[\n\\beta = (X'X)^{-1}X'y\n\\]\nWe already saw this! For Quantile regressions, we never did it by-hand (requires linear programming). Because, Qregressions are also nolinear.\nIn this section, we will cover some of the few methodologies that are available for the estimation of Nonlinear models. We start with the first, an extension to OLS, we will call NLS.\n\\[\ny = h(x,\\beta) + e\n\\]\nWhat makes this model NLS, is that the error adds to the outcome (or CEF)! However, the CEF is modeled as a nolinear function of \\(X's\\) and \\(\\beta's\\). (but we still aim to MIN SSR)"
  },
  {
    "objectID": "adv_class/06nlreg.html#some-assumptions",
    "href": "adv_class/06nlreg.html#some-assumptions",
    "title": "NLS, IRLS, and MLE",
    "section": "Some Assumptions",
    "text": "Some Assumptions\nFor identification and estimation NLS requires similar assumptions as OLS:\n\nFunctional form: \\(E(y|X)\\) is given by \\(h(x,\\beta)\\), which is continuous and differentiable.\nThere is a unique solution! (like no-multicolinearity). if \\(\\beta\\) Minimizes the errors, then there is no other \\(\\beta_0\\) that will give the same solution.\nThe expected value of the error is zero \\(E(e)=0\\), and \\(E(e|h(x,\\beta))=0\\) . Similar to No endogeneity, but constrained by functional form.\nData is well behaved. (no extreme distributions, so that mean and variance exists)\n\nUnder this assumptions, its possible to Estimate the coefficients of interest."
  },
  {
    "objectID": "adv_class/06nlreg.html#but-how",
    "href": "adv_class/06nlreg.html#but-how",
    "title": "NLS, IRLS, and MLE",
    "section": "But How?",
    "text": "But How?\nNLS aims to choose \\(\\beta's\\) to minimize the sum of squared residuals:\n\\[\nSSR(\\beta) = \\sum(y-h(x,\\beta))^2 = [y-h(x,\\beta)]'[y-h(x,\\beta)]\n\\]\nThe FOC of this model are a non-linear system of equations.\n\\[\n\\frac{\\partial SSR(\\beta)}{\\partial \\beta}=-2 \\Bigg[\\frac{\\partial h(x,\\beta)}{\\partial \\beta}\\Bigg]' [y-h(x,\\beta)] =-2\\color{red}{\\tilde X'} e\n\\]\nSo how? Lets Start with a 2nd Order Taylor Expansion:\n\\[\n\\begin{aligned}\nSSR(\\beta) &\\simeq  SSR(\\beta_0) + g(\\beta_0)' (\\beta-\\beta_0)+\\frac{1}{2}(\\beta-\\beta_0)'H(\\beta_0)(\\beta-\\beta_0) \\\\\ng(\\beta) &=-2 \\tilde X'e ;H(\\beta)=2 \\tilde X'\\tilde X ; \\tilde X=\n\\Bigg[\\frac{\\partial h(x,\\beta)}{\\partial \\beta}\\Bigg]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/06nlreg.html#foc-again..",
    "href": "adv_class/06nlreg.html#foc-again..",
    "title": "NLS, IRLS, and MLE",
    "section": "FOC again..",
    "text": "FOC again..\n\\[\n\\begin{aligned}\ng(\\beta_0)+H(\\beta_0)(\\beta-\\beta_0)&=0 \\\\\n\\beta-\\beta_0 &= -H(\\beta_0)^{-1}g(\\beta_0) \\\\\n\\beta_t &=\\beta_{t-1}-H(\\beta_{t-1})^{-1}g(\\beta_{t-1})\n\\end{aligned}\n\\]\nThis simply says, In order to solve the system, you need to use a recursive system, so that \\(\\beta's\\) are updated until there is no longer a change.\nThis Iterative process is also known as a Newton Raphson method to solve nonlinear equations (if a solution exists).\nWhy does this work?\n\nYou change \\(\\beta\\) in the direction that should minimize SSR. (that direction is \\(g(.,.)\\).\nThat get to that change the “fastest” way possible using the Hessian\n\nThis is the most basic numerical optimization method."
  },
  {
    "objectID": "adv_class/06nlreg.html#small-example",
    "href": "adv_class/06nlreg.html#small-example",
    "title": "NLS, IRLS, and MLE",
    "section": "Small Example",
    "text": "Small Example\nConsider the function \\(y = x^4 -18 x^2 + 15x\\) , find the Minimum.\nS1. Gradient: \\(\\frac{\\partial y}{\\partial x}=4x^3-36*x+15\\)\nS2. Hessian: \\(\\frac{\\partial y ^2}{\\partial^2 x}=12*x^2-36\\)\nSolution:\n\\[x_t = x_{t-1} - \\frac{dy/dx}{dy^2/d^2x}\\]\nmata:\n     // function to obtain the Value, the gradient and Hessian \n     real matrix  fgh_x(real matrix x, real scalar g){\n        real matrix y\n        if (g==0)      y =    -x:^4 :- 18*x:^2 :+ 15*x\n        else if (g==1) y =  -4*x:^3 :- 36*x    :+ 15\n        else if (g==2) y = -12*x:^2 :- 36   \n        return(y)\n     }\n    // Some initial values\n     x = -2 , 2 ,0 \n     xt = x,fgh_x(x,0),fgh_x(x,1)\n    for(i=1;i&lt;8;i++) {\n         x = x :- fgh_x(x,1):/fgh_x(x,2)\n         xt = xt \\ x,fgh_x(x,0),fgh_x(x,1)\n    }\n    xt[,(1,4,7)] \n    xt[,(2,5,8)] \n    xt[,(3,6,9)] \n    end\n\n        +----------------------------------------------+\n      1 |            -2            -86             55  |\n      2 |  -6.583333333    999.5046779   -889.2939815  |\n      3 |  -4.746265374    30.78669521   -241.8115913  |\n      4 |  -3.714313214   -113.7119057   -56.25720696  |\n      5 |  -3.280073929   -127.1074326   -8.077091068  |\n      6 |  -3.193322943   -127.4662895   -.2936080906  |\n      7 |  -3.189923432   -127.4667891   -.0004426933  |\n      8 |  -3.189918291   -127.4667891   -1.01177e-09  |\n        +----------------------------------------------+\n                      1              2              3\n        +----------------------------------------------+\n      1 |             2            -26            -25  |\n      2 |   4.083333333    39.13430748    140.3356481  |\n      3 |    3.22806275   -30.56155349    33.34042084  |\n      4 |   2.853639205   -37.46140286    5.220655055  |\n      5 |   2.769051829    -37.6890608    .2425933896  |\n      6 |   2.764720715   -37.68958705    .0006229957  |\n      7 |   2.764709535   -37.68958705    4.14680e-09  |\n      8 |   2.764709535   -37.68958705    1.42109e-14  |\n        +----------------------------------------------+\n                     1             2             3\n        +-------------------------------------------+\n      1 |            0             0            15  |\n      2 |  .4166666667   3.155140818   .2893518519  |\n      3 |  .4251979252   3.156376126   .0003663956  |\n      4 |  .4252087555   3.156376128   5.98494e-10  |\n      5 |  .4252087556   3.156376128   1.77636e-15  |\n      6 |  .4252087556   3.156376128             0  |\n      7 |  .4252087556   3.156376128             0  |\n      8 |  .4252087556   3.156376128             0  |\n        +-------------------------------------------+"
  },
  {
    "objectID": "adv_class/06nlreg.html#why-so-many-solutions",
    "href": "adv_class/06nlreg.html#why-so-many-solutions",
    "title": "NLS, IRLS, and MLE",
    "section": "Why So many Solutions?",
    "text": "Why So many Solutions?\n\nStata"
  },
  {
    "objectID": "adv_class/06nlreg.html#nls",
    "href": "adv_class/06nlreg.html#nls",
    "title": "NLS, IRLS, and MLE",
    "section": "NLS",
    "text": "NLS\nThe same principle (as above) can be used for Regression:\n\\[\ny = b_0 + b_1 x ^{b_2} + e = h(x,b) + e\n\\]\nPseudo Regressors and gradients\n\\[\n\\begin{aligned}\n\\tilde X(b) &= \\frac{\\partial h(x,b)}{\\partial b_0},\n\\frac{\\partial h(x,b)}{\\partial b_1},\n\\frac{\\partial h(x,b)}{\\partial b_2} \\\\\n\\tilde X(b) &= 1, x^{b_2},{b_1} x^{b_2}\\ log\\ b_2 \\\\\n\\beta_t &=\\beta_{t-1}-(\\tilde X ' \\tilde X) ^{-1} \\tilde X' \\hat e \\\\\n\\hat e&=y-h(x,b_{t-1})\n\\end{aligned}\n\\]\nIt turns out that:\n\\[\n\\begin{aligned}\nb^* \\sim N(b, (\\tilde X' \\tilde X)^{-1}\\tilde X' \\Omega \\tilde X (\\tilde X' \\tilde X)^{-1} )\\\\\n\\Omega = f(\\hat e)\n\\end{aligned}\n\\]\nSo…you can 🥪 just like with regular OLS!"
  },
  {
    "objectID": "adv_class/06nlreg.html#nls-in-stata-long-example",
    "href": "adv_class/06nlreg.html#nls-in-stata-long-example",
    "title": "NLS, IRLS, and MLE",
    "section": "NLS in Stata: Long Example",
    "text": "NLS in Stata: Long Example\nclear\nset seed 101\nset obs 100\n** Generating fake data\ngen x = runiform()\ngen y = 1+0.5*x^0.5+rnormal()*.1\n*** Load data in Mata...to make things quick\nmata\nx=st_data(.,\"x\")\ny=st_data(.,\"y\")\nend\n\nmata\n// Initial data\nb=1\\1\\1\nb0=999\\999\\999\nbb=b'\n// and a loop to see when data converges\nwhile (sum(abs(b0:-b))&gt; 0.000001 ) { \n    b0=b    \n    // residuals\n    e=y:-(b[1]:+b[2]*x:^b[3])\n    // pseudo regressors\n    sx=J(100,1,1),x:^b[3],b[2]*x:^b[3]:*ln(x)\n    // gradient and Hessian\n    g=-cross(sx,e)\n    H=cross(sx,sx)\n    // updating B\n    b=b-invsym(H)*g\n    // Storing results\n    bb=bb\\b'\n}\n/// Now STD ERR (for fun 😃 )\nvcv = e'e / (100-3) * invsym(H)\nb , diagonal(vcv):^0.5\n\nend\n: b , diagonal(vcv):^0.5\n                 1             2\n    +-----------------------------+\n  1 |   1.06407084   .0527603468  |\n  2 |  .4228891942   .0477125017  |\n  3 |  .5788053878    .159931095  |\n    +-----------------------------+"
  },
  {
    "objectID": "adv_class/06nlreg.html#nls-in-stata-short-example-nl-function",
    "href": "adv_class/06nlreg.html#nls-in-stata-short-example-nl-function",
    "title": "NLS, IRLS, and MLE",
    "section": "NLS in Stata: Short Example: NL function",
    "text": "NLS in Stata: Short Example: NL function\n** Stata has the function -nl- (nonlinear)\n** it can be used to estimate this types of models\n** see help nl\n** Be careful about Initial values\nnl ( y = {b0=1} + {b1=1} * x ^ {b2=1})\n\nIteration 0:  residual SS =   .854919\nIteration 1:  residual SS =  .7742535\nIteration 2:  residual SS =   .766106\nIteration 3:  residual SS =  .7660948\nIteration 4:  residual SS =  .7660947\nIteration 5:  residual SS =  .7660947\nIteration 6:  residual SS =  .7660947\n\n\n      Source |      SS            df       MS\n-------------+----------------------------------    Number of obs =        100\n       Model |  1.2706842          2  .635342093    R-squared     =     0.6239\n    Residual |  .76609469         97  .007897883    Adj R-squared =     0.6161\n-------------+----------------------------------    Root MSE      =     .08887\n       Total |  2.0367789         99  .020573524    Res. dev.     =  -203.3743\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         /b0 |   1.064071   .0527608    20.17   0.000     .9593554    1.168786\n         /b1 |   .4228891   .0477129     8.86   0.000     .3281923     .517586\n         /b2 |   .5788057   .1599306     3.62   0.000     .2613878    .8962236\n------------------------------------------------------------------------------\nNote: Parameter b0 is used as a constant term during estimation.\nSo, you could now estimate many nonlinear models! (logits, probits, poissons, etc) or can you?"
  },
  {
    "objectID": "adv_class/06nlreg.html#nls-for-logit",
    "href": "adv_class/06nlreg.html#nls-for-logit",
    "title": "NLS, IRLS, and MLE",
    "section": "NLS for logit",
    "text": "NLS for logit\nThe model (as NLS)\n\\[\nP(y=1|x) = \\frac{exp(x\\beta)}{1+exp(x\\beta)}+e\n\\]\nThis guaranties the predicted value is between 0 and 1. But, still assumes errors are homoskedastic!\nfrause oaxaca, clear\nnl (lfp = logistic({b0}+{b1:female age educ}))\n\n      Source |      SS            df       MS\n-------------+----------------------------------    Number of obs =      1,647\n       Model |  1272.8283          4  318.207079    R-squared     =     0.8876\n    Residual |  161.17168       1643  .098095973    Adj R-squared =     0.8873\n-------------+----------------------------------    Root MSE      =   .3132028\n       Total |       1434       1647  .870673953    Res. dev.     =   845.9593\n\n------------------------------------------------------------------------------\n         lfp | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         /b0 |   3.432866   .9601508     3.58   0.000     1.549617    5.316114\n  /b1_female |  -3.056149   .8625563    -3.54   0.000    -4.747975   -1.364324\n     /b1_age |  -.0205121   .0054815    -3.74   0.000    -.0312635   -.0097607\n    /b1_educ |   .1522987   .0329513     4.62   0.000     .0876679    .2169296\n------------------------------------------------------------------------------\nlogit lfp female age educ\n\n\nLogistic regression                                     Number of obs =  1,647\n                                                        LR chi2(3)    = 251.69\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -508.42172                             Pseudo R2     = 0.1984\n\n------------------------------------------------------------------------------\n         lfp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |   -3.21864    .365329    -8.81   0.000    -3.934672   -2.502609\n         age |  -.0233149   .0072746    -3.20   0.001    -.0375729   -.0090569\n        educ |   .1719904   .0411498     4.18   0.000     .0913383    .2526425\n       _cons |   3.507185   .6550208     5.35   0.000     2.223367    4.791002\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/06nlreg.html#glm-and-interative-reweighted-ls-irls",
    "href": "adv_class/06nlreg.html#glm-and-interative-reweighted-ls-irls",
    "title": "NLS, IRLS, and MLE",
    "section": "GLM and Interative Reweighted LS (IRLS)",
    "text": "GLM and Interative Reweighted LS (IRLS)\nGeneralized Linear Model are a natural extension to LR models. It changes how LR models are estimated.\n\nPuts more emphasis in modeling the CEF (conditional mean) of the distribution\nAllows for different transformations that relate the index \\(xb\\) to \\(E(y|x)\\) (links)\nConsiders data can come from different distributions. ( family )\n\n\\[\nE(y|X) = \\eta ^{-1}(x\\beta) ; Var(E(y|X)) = \\sigma^2(x)\n\\]\nFor example:\nLogit model: Family \\(\\rightarrow\\) binomial, Link function logistic function is logistic \\(p(y|x) = \\frac {e^{x\\beta} }{1+e^{x\\beta}} \\rightarrow x\\beta=log \\left( \\frac{p}{1-p} \\right)\\)\nFamily: Binomial, so variance is given by \\(Var(y|X) = p(y|x)(1-p(y|x))\\)"
  },
  {
    "objectID": "adv_class/06nlreg.html#how-does-this-change-estimation-nls",
    "href": "adv_class/06nlreg.html#how-does-this-change-estimation-nls",
    "title": "NLS, IRLS, and MLE",
    "section": "How does this change Estimation? NLS??",
    "text": "How does this change Estimation? NLS??\n\nFusion"
  },
  {
    "objectID": "adv_class/06nlreg.html#how-does-this-change-estimation-nls-1",
    "href": "adv_class/06nlreg.html#how-does-this-change-estimation-nls-1",
    "title": "NLS, IRLS, and MLE",
    "section": "How does this change Estimation? NLS??",
    "text": "How does this change Estimation? NLS??\nRecall GLS?\n\nHeteroskedasticity was addressed by either using “weights” to estimate the model, or by transforming the data!\nHere, when we choose a family, we are also choosing a particular source of heteroskedasticy. (Logit, probit, poisson, have very specific heteroskedastic errors)\nThus, you just need to apply NLS to transformed data!\n\n\\[\n\\begin{aligned}\nSSR(\\beta) = \\sum \\left( \\frac{y-h(x,\\beta)}{\\sigma(x,\\beta)} \\right)^2 \\\\\\\n\\tilde X = \\frac{1}{\\sigma(x,\\beta)} \\frac{\\partial h(x,\\beta)}{\\partial \\beta} ; \\tilde y = \\frac{y}{\\sigma(x,\\beta)} ; \\tilde e=\\frac{1}{\\sigma(x,\\beta)}(y-h(x,\\beta))\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/06nlreg.html#how-does-this-change-estimation-nls-2",
    "href": "adv_class/06nlreg.html#how-does-this-change-estimation-nls-2",
    "title": "NLS, IRLS, and MLE",
    "section": "How does this change Estimation? NLS??",
    "text": "How does this change Estimation? NLS??\nThen, just apply the iterative process we saw before, until it converges!\n\\[\n\\beta_t =\\beta_{t-1}-(\\tilde X'\\tilde X)^{-1} (\\tilde X' \\tilde e)\n\\]\nThen simply derive Standard errors from here.\nfrause oaxaca, clear\nnl (lfp = logistic({b0}+{b1:female age educ}))   \nlogit lfp female age educ\n\n** IRSL\ngen one =1\nmata\ny = st_data(.,\"lfp\")\nx = st_data(.,\"female age educ one\")\nb = 0\\0\\0\\0\n end\n\n\nmata:\nb0=999\\999\\999\\999\nbb=b'\nwhile (sum(abs(b0:-b))&gt; 0.000001 )  {\n    b0=b    \n    yh = logistic(x*b)\n    err = y:-yh\n    se = sqrt(yh:*(1:-yh))\n    wsx =  yh:*(1:-yh):*x:/se\n    werr=  err:/se\n    g = -cross(wsx,werr)\n    h = cross(wsx,wsx)\n    b = b:-invsym(h)*g;b'\n    }\nb,diagonal(cross(werr,werr)/1643*invsym(h)):^.5\nend\n    +-------------------------------+\n  1 |  -3.218640936    .3458501096  |\n  2 |  -.0233149268    .0068867539  |\n  3 |   .1719904055    .0389557282  |\n  4 |   3.507185231    .6200958697  |\n  +-------------------------------+"
  },
  {
    "objectID": "adv_class/06nlreg.html#glm-in-stata",
    "href": "adv_class/06nlreg.html#glm-in-stata",
    "title": "NLS, IRLS, and MLE",
    "section": "GLM in Stata",
    "text": "GLM in Stata\nThis one is easy as 🥧\nhelp glm\n* see for advanced options if interested\n* syntax\nglm y x1 x2 x3 , family( family ) link(function) method\nfrause oaxaca\nglm lfp female age educ, family(binomial)  link(probit)\nglm lfp female age educ, family(binomial)  link(identity)\nglm lfp female age educ, family(binomial)  link(logit)\n\n\nGeneralized linear models                         Number of obs   =      1,647\nOptimization     : ML                             Residual df     =      1,643\n                                                  Scale parameter =          1\nDeviance         =   1016.84343                   (1/df) Deviance =   .6188944\nPearson          =  1472.464769                   (1/df) Pearson  =    .896205\n\nVariance function: V(u) = u*(1-u)                 [Bernoulli]\nLink function    : g(u) = ln(u/(1-u))             [Logit]\n\n                                                  AIC             =   .6222486\nLog likelihood   = -508.4217152                   BIC             =  -11152.38\n\n------------------------------------------------------------------------------\n             |                 OIM\n         lfp | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -3.218641    .365329    -8.81   0.000    -3.934672   -2.502609\n         age |  -.0233149   .0072746    -3.20   0.001    -.0375729   -.0090569\n        educ |   .1719904   .0411498     4.18   0.000     .0913383    .2526425\n       _cons |   3.507185   .6550209     5.35   0.000     2.223368    4.791002\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/06nlreg.html#what-is-mle",
    "href": "adv_class/06nlreg.html#what-is-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "What is MLE",
    "text": "What is MLE\nMLE is an estimation method that allows you to find asymptotically efficient estimators of the parameters of interest \\(\\beta's\\).\nHow?\n\nUnder the assumption that you know something about the data conditional distribution, MLE will find the set of parameters that maximizes the probability (likelihood)that data “comes” from the chosen distribution.\n\nok….but HOW???\nLets do this by example"
  },
  {
    "objectID": "adv_class/06nlreg.html#poisson-regression-via-mle",
    "href": "adv_class/06nlreg.html#poisson-regression-via-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "Poisson Regression via MLE",
    "text": "Poisson Regression via MLE\nConsider variable \\(y={1,1,2,2,3,3,3,3,4,5}\\)\nAnd say, we assume it comes from a poisson distribution:\n\\[\np(y|\\lambda) = \\frac{e^{-\\lambda} \\lambda ^y}{y!}\n\\]\nThis function depends on the value of \\(\\lambda\\) .\n\nWhen \\(\\lambda\\) is known, this is the probability \\(y=\\#\\), assuming a poisson distribution.\nWhen \\(\\lambda\\) is unknown, this function (now \\(f(y|\\lambda)\\) gives the likelihood that we observe \\(y=\\#\\) , for any given \\(\\lambda\\)."
  },
  {
    "objectID": "adv_class/06nlreg.html#the-likelihood-of-lambda",
    "href": "adv_class/06nlreg.html#the-likelihood-of-lambda",
    "title": "NLS, IRLS, and MLE",
    "section": "The Likelihood of \\(\\lambda\\)",
    "text": "The Likelihood of \\(\\lambda\\)"
  },
  {
    "objectID": "adv_class/06nlreg.html#section",
    "href": "adv_class/06nlreg.html#section",
    "title": "NLS, IRLS, and MLE",
    "section": "",
    "text": "Previous figure only considers the likelihood function of a single observation. And every curve suggests its own parameter. (care to guess?)\nWhat if we want one that Maximizes the likelihood of ALL observations at once!. For this we need to impose an additional assumption: Independent distribution.\nThis means that JOINT density is defined as:\n\n\\[\nL(\\lambda|y_1,y_2,...,y_{10})=\\prod f(y_i|\\lambda)  \n\\]"
  },
  {
    "objectID": "adv_class/06nlreg.html#the-likelihood-of-lambda-1",
    "href": "adv_class/06nlreg.html#the-likelihood-of-lambda-1",
    "title": "NLS, IRLS, and MLE",
    "section": "The Likelihood of \\(\\lambda\\)",
    "text": "The Likelihood of \\(\\lambda\\)"
  },
  {
    "objectID": "adv_class/06nlreg.html#what-mle-does",
    "href": "adv_class/06nlreg.html#what-mle-does",
    "title": "NLS, IRLS, and MLE",
    "section": "What MLE does",
    "text": "What MLE does\nWhat MLE does, then, is to find the parameter \\(\\lambda\\) that maximizes the Joint probability of observing the dataset \\(Y\\). Simple as that….\nWith one more difference. Because products are Hard, MLE aims to maximize the Log-Likelihood of observing the data:\n\\[\n\\begin{aligned}\nLnL(\\lambda|y_1,y_2,...,y_{10}) &=\\sum ln f(y_i|\\lambda) \\\\\\\n&= \\sum (-\\lambda + y_i ln(\\lambda) - ln(y_i!))\n\\end{aligned}\n\\]\nAnd just two more changes."
  },
  {
    "objectID": "adv_class/06nlreg.html#section-1",
    "href": "adv_class/06nlreg.html#section-1",
    "title": "NLS, IRLS, and MLE",
    "section": "",
    "text": "When you have \\(X's\\), you can further modify this model, to allow for covariates. For example:\n\n\\[\\lambda = e^{x\\beta}\n\\]\nWhich guaranties the mean, or more specifically conditional mean \\(\\lambda = E(y|X)\\) , is positive.\n\nWe divide LnL by \\(N\\). (Number of observations)\n\n\\[\n\\begin{aligned}\n\\beta = \\min_\\beta LnL(\\beta|Y,X) \\\\\n\\beta = \\min_\\beta \\frac{1}{N}\\sum -e^{x'\\beta}+y_i  x_i'\\beta - ln(y_i!)\n\\end{aligned}\n\\]\nWhich arrives to the Same solution"
  },
  {
    "objectID": "adv_class/06nlreg.html#how-to-solve-this",
    "href": "adv_class/06nlreg.html#how-to-solve-this",
    "title": "NLS, IRLS, and MLE",
    "section": "How to solve this?",
    "text": "How to solve this?\nWe actually already covered this…We use Iterative methods!\n\\[\n\\begin{aligned}\nLL &=  \\sum ln \\ f(y|x,\\beta) \\\\\nFOC&: \\\\\n\\frac{\\partial LL}{\\partial \\beta}&= \\sum \\frac{\\partial ln \\ f(y|x,\\beta)}{\\partial \\beta} = \\sum g_i=n E[g_i] =0 \\\\\nSOC&: \\\\\n\\frac{\\partial^2 LL}{\\partial \\beta \\partial \\beta'}&=\n\\sum\\frac{\\partial^2 ln f(y|x,\\beta)}{\\partial \\beta \\partial \\beta'} =\\sum H_i = n E(H_i)\n\\end{aligned}\n\\]\nThus, \\(\\beta's\\) can be estimated using the iterative process (or other more efficient process)\n\\[\n\\beta_t = \\beta_{t-1} - E(H)^{-1} E(g_i)\n\\]"
  },
  {
    "objectID": "adv_class/06nlreg.html#why-do-we-like-mle",
    "href": "adv_class/06nlreg.html#why-do-we-like-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "Why do we like MLE?",
    "text": "Why do we like MLE?\nProperties of MLE:\n\nThe estimates are consistent \\(plim \\ \\hat \\theta = \\theta\\)\nIts MLE estimates are asymptically normal \\(\\hat \\theta\\sim N(\\theta,\\sigma^2_\\theta)\\)\nAnd asymptotically efficient (smallest variance)\n\nWait…What about Variances? How do you estimate them!"
  },
  {
    "objectID": "adv_class/06nlreg.html#regularity-conditions-and-mle",
    "href": "adv_class/06nlreg.html#regularity-conditions-and-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "Regularity Conditions and MLE ⚠️",
    "text": "Regularity Conditions and MLE ⚠️\nThe variance of estimated coefficients has three estimators for its variance:\n\nWe can 🥪(Sandwich) the variance:\n\n\\[\nVar\\left(N^{-1/2}(\\hat\\beta - \\beta) \\right) = H^{-1}g'gH^{-1} = A^{-1}BA^{-1}\n\\]\n\nOr you can use one of the following: \\[\n\\begin{aligned}\nVar\\left(N^{-1/2}(\\hat\\beta - \\beta) \\right) &= -H^{-1} \\\\\nVar\\left(N^{-1/2}(\\hat\\beta - \\beta) \\right) &= \\left(\\frac{1}{N}\\sum g_i g_i'\\right)^{-1}\n\\end{aligned}\n\\]\n\nBut if all is well (Regularity conditions), They are all equivalent. Otherwise Opt1 is similar to HC1, and Option 2a is closer to non-robust Standard Errors"
  },
  {
    "objectID": "adv_class/06nlreg.html#regularity-conditions",
    "href": "adv_class/06nlreg.html#regularity-conditions",
    "title": "NLS, IRLS, and MLE",
    "section": "Regularity Conditions ⚠️",
    "text": "Regularity Conditions ⚠️\n\nThe LogLikelihood function is build on a well behaved distribution function. Which implies FOC:\n\n\\[\n\\begin{aligned}\n\\int f(y|\\theta) dy &=1 \\\\\n\\int \\frac{\\partial f(y|\\theta)}{\\partial \\theta}dy&=\\int  \\frac{\\partial ln f(y|\\theta)}{\\partial \\theta} f(y|\\theta)=E(g_i)=0\n\\end{aligned}\n\\]\n\nOrder of Diff and Integration is interchangeable. We obtain SOC: \\[\\begin{aligned}\n\\int \\left( \\frac{\\partial^2 ln f(y|\\theta)}{\\partial \\theta \\partial \\theta'}f(y|\\theta) +\n\\frac{\\partial ln f(y|\\theta)}{\\partial \\theta} \\frac{\\partial ln f(y|\\theta)}{\\partial \\theta'} f(y|\\theta) \\right)dy &= 0 \\\\\nE(H_i) = -E(g_ig_i')\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/06nlreg.html#lr-as-mle",
    "href": "adv_class/06nlreg.html#lr-as-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "LR as MLE",
    "text": "LR as MLE\nUnder the assumption of normality, LR can be estimated using ML:\n\\[y_i = x_i'\\beta + e_i \\ | \\ e_i\\sim N(0,\\sigma^2) \\rightarrow y|x \\sim N(x'\\beta, \\sigma^2)\n\\]\nHow does the MLE look? \\[\n\\begin{aligned}\nL_i = f(y_i|x_i,\\beta,\\sigma^2) = \\frac{1}{ \\sqrt{2\\pi \\sigma^2 }} e^{-\\frac{1}{2} \\frac{ (y_i-x_i'\\beta)^2}{\\sigma^2} } \\\\\nLL_i = -\\frac{1}{2}\\frac{(y_i-x_i'\\beta)^2}{\\sigma^2} - \\frac{1}{2}ln(2\\pi)-\\frac{1}{2}ln(\\sigma^2)\n\\end{aligned}\n\\]\nTotal LL?\n\\[\nLL = -\\frac{1}{2\\sigma^2} \\sum (y_i-x_i'\\beta)^2-\\frac{N ln(2\\pi)}{2}-\\frac{N}{2} ln(\\sigma^2)\n\\]"
  },
  {
    "objectID": "adv_class/06nlreg.html#lr-as-mle-pii",
    "href": "adv_class/06nlreg.html#lr-as-mle-pii",
    "title": "NLS, IRLS, and MLE",
    "section": "LR as MLE PII",
    "text": "LR as MLE PII\nFOC:\n\\[\n\\begin{aligned}\n\\frac{\\partial LL}{\\partial \\beta} = -\\frac{1}{\\sigma^2}\\sum x'(y_i-x'\\beta)=0 \\rightarrow \\hat\\beta=(X'X)^{-1}X'y \\\\\n\\frac{\\partial LL}{\\partial \\sigma^2} = \\frac{\\sum e^2}{2\\sigma^4}-\\frac{N}{2\\sigma^2}=0 \\rightarrow \\hat \\sigma^2 = \\frac{\\sum e^2}{N}\n\\end{aligned}\n\\]\nSOC:\n\\[\n\\begin{aligned}\n\\frac{\\partial^2 LL}{\\partial \\beta \\partial \\beta'} = - \\frac{X'X}{\\sigma^2} ;\n\\frac{\\partial^2 LL}{\\partial \\beta \\partial \\sigma} = - \\frac{X'y-X'X\\beta }{\\sigma^2} \\\\\n\\frac{\\partial^2 LL}{\\partial \\sigma \\partial \\beta'}=0;\n\\frac{\\partial^2 LL}{\\partial \\sigma ^2}=-\\frac{\\sum e^2}{\\sigma^6}+\\frac{N}{2\\sigma^4}=-\\frac{N}{2\\sigma^4}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/06nlreg.html#soc",
    "href": "adv_class/06nlreg.html#soc",
    "title": "NLS, IRLS, and MLE",
    "section": "SOC",
    "text": "SOC\n\\[\n\\begin{aligned}\nH = \\Bigg[\\begin{matrix} -\\frac{X'X}{\\sigma^2}& 0 \\\\\n0 & -\\frac{N}{2\\sigma^4}\n\\end{matrix} \\Bigg] \\\\\n& \\rightarrow Var(\\beta,\\sigma^2)=-H^{-1}=\n\\Bigg[\\begin{matrix}\n\\sigma^2 (X'X)^{-1} & 0 \\\\\n0 & \\frac{2\\sigma^4}{N}\n\\end{matrix} \\Bigg]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/06nlreg.html#other-considerations",
    "href": "adv_class/06nlreg.html#other-considerations",
    "title": "NLS, IRLS, and MLE",
    "section": "Other considerations",
    "text": "Other considerations\nMLE is consistent if the model is correctly Specified.\nThis means that one correctly specifies the conditional distribution of \\(y\\).\n\nOften, its possible to especify the CEF, but specify the variance correctly, may be difficult\n\nUsually, this would be grounds of inconsistency. However if the distribution function is part of the exponential family (normal, bernulli, poisson, etc), one only needs to correctly specify the CEF correctly!\n\nIn this case, the model is no longer estimated using MLE but QMLE\n\nIn this cases, use Robust!"
  },
  {
    "objectID": "adv_class/06nlreg.html#mle-in-stata",
    "href": "adv_class/06nlreg.html#mle-in-stata",
    "title": "NLS, IRLS, and MLE",
    "section": "MLE in Stata",
    "text": "MLE in Stata\nMany native commands in Stata actually estimate models using MLE on the background.\n\nlogit; probit; poisson; ologit; mlogit, etc\n\nSome multiple equation models as well.\n\nmovestay , craggit , ky_fit\n\nAre just a few user written commands that also rely on MLE.\nSo how do you estimate this models?\nyou do it by hand!"
  },
  {
    "objectID": "adv_class/06nlreg.html#this-is-the-way",
    "href": "adv_class/06nlreg.html#this-is-the-way",
    "title": "NLS, IRLS, and MLE",
    "section": "This is the way",
    "text": "This is the way"
  },
  {
    "objectID": "adv_class/06nlreg.html#programming-mle",
    "href": "adv_class/06nlreg.html#programming-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "Programming MLE",
    "text": "Programming MLE\nStata has one feature that would allow you to estimate almost any model via MLE. the -ml- suit.\nThis has many levels of programming (lf, d0, d1, d2, etc), but we will concentrate on the easiest one : lf (linear function)\n\nThis only requires you to provide the individual level likelihood function!\n\nBut how do we start?"
  },
  {
    "objectID": "adv_class/06nlreg.html#section-2",
    "href": "adv_class/06nlreg.html#section-2",
    "title": "NLS, IRLS, and MLE",
    "section": "",
    "text": "You need three pieces of information:\n\nIdentify distribution or objective function to maximize.\nIdentify the parameters that the distribution depends on, and how will they be affected by characteristics\nIf more than one equation exists, Identify possible connections across variables\nWrap it all in a program\n\nFor details on a few examples see Rios-Avila & Canavire-Bacarreza 2018"
  },
  {
    "objectID": "adv_class/06nlreg.html#programming-mle-pi",
    "href": "adv_class/06nlreg.html#programming-mle-pi",
    "title": "NLS, IRLS, and MLE",
    "section": "Programming MLE PI",
    "text": "Programming MLE PI\nOLS via MLE:\n\n\\(y\\) distributes as normal distribution, which depends on the mean \\(\\mu\\) and variance \\(\\sigma^2\\). We assume that only the mean depends on \\(X\\).\n\n** Define Program\ncapture program drop my_ols\nprogram define my_ols\n  args lnf ///   &lt;- Stores the LL for obs i\n       xb  ///   &lt;- Captures the Linear combination of X's\n       lnsigma // &lt;- captures the Log standard error\n  ** Start creating all aux variables and lnf     \n  qui {\n    tempvar sigma // Temporary variable\n    gen double `sigma' = exp(`lnsigma')\n    tempvar y\n    local y $ML_y1\n    replace `lnf'      = log( normalden(`y',`xb',`sigma' ) )\n  }     \nend\nNow Just Call on the program\n* load some data\nfrause oaxaca, clear\nml model lf   /// Ask to use -ml- to estimate a model with method -lf-\n   my_ols     /// your ML program\n   (xb: lnwage = age educ female  ) /// 1st Eq (only one in this case)\n   (lnsig: = female ) // Empty, (no other Y, but could add X's)\n   // I could haave added weights, or IF conditions\nml check     // checks if the code is correct\nml maximize  // maximizes\nml display   // shows results\n\n* Short version \nml model lf  my_ols /// Model and method\n   (xb: lnwage = age educ female) (lnsig: = female ) /// model Parms\n   , robust maximize // Other SE options, and maximize\n   \n\nml display\n\n                                                        Number of obs =  1,434\n                                                        Wald chi2(3)  = 393.05\nLog pseudolikelihood = -870.41117                       Prob &gt; chi2   = 0.0000\n\n------------------------------------------------------------------------------\n             |               Robust\n      lnwage | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nxb           |\n         age |   .0177272   .0012221    14.51   0.000      .015332    .0201224\n        educ |   .0685501   .0056712    12.09   0.000     .0574347    .0796655\n      female |  -.1487184   .0247333    -6.01   0.000    -.1971948   -.1002421\n       _cons |   1.949072   .0888074    21.95   0.000     1.775012    2.123131\n-------------+----------------------------------------------------------------\nlnsig        |\n      female |   .3440266   .0691673     4.97   0.000     .2084611     .479592\n       _cons |  -.9758137   .0488944   -19.96   0.000    -1.071645   -.8799825\n------------------------------------------------------------------------------\n\n. reg  lnwage age educ female\n\n      Source |       SS           df       MS      Number of obs   =     1,434\n-------------+----------------------------------   F(3, 1430)      =    167.12\n       Model |  104.907056         3  34.9690188   Prob &gt; F        =    0.0000\n    Residual |  299.212747     1,430  .209239683   R-squared       =    0.2596\n-------------+----------------------------------   Adj R-squared   =    0.2580\n       Total |  404.119804     1,433  .282009633   Root MSE        =    .45743\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0161424   .0010978    14.70   0.000      .013989    .0182959\n        educ |   .0719322   .0050365    14.28   0.000     .0620524     .081812\n      female |  -.1453936   .0243888    -5.96   0.000    -.1932352    -.097552\n       _cons |   1.970021   .0725757    27.14   0.000     1.827654    2.112387\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/06nlreg.html#other-considerations-1",
    "href": "adv_class/06nlreg.html#other-considerations-1",
    "title": "NLS, IRLS, and MLE",
    "section": "Other Considerations",
    "text": "Other Considerations\nWith MLE, as with logit probit tobit, etc, you cannot interpret the models directly!\nThen what?\n\nIdentify what is the Statistic of interest (most of the time its the expected value, conditional mean, or predicted mean). But others may be something related to other parameters (we care about \\(\\sigma\\) not \\(ln \\ \\sigma\\)).\nSee margins, and option expression\nIdentify if you are interested in Average effects, or effects at the average.\nSome times, you may need to use your own predictions!\n\nsee example for probit"
  },
  {
    "objectID": "adv_class/06nlreg.html#margins-in-action",
    "href": "adv_class/06nlreg.html#margins-in-action",
    "title": "NLS, IRLS, and MLE",
    "section": "Margins in action",
    "text": "Margins in action\n**Estimate Logit model\nlogit lfp educ female age married divorced\n** use margins to estimate predicted probability\nmargins\n** or use expression\nmargins, expression(exp(xb())/(1+exp(xb())))\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _cons |    .870674   .0071023   122.59   0.000     .8567537    .8845942\n------------------------------------------------------------------------------\n\n** marginal effects\nmargins, dydx(educ) \n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0161004   .0037127     4.34   0.000     .0088236    .0233772\n------------------------------------------------------------------------------\n\n\n** Marginal effect of the marginal effect?\nmargins, expression( logistic(xb())*(1-logistic(xb()))*_b[educ] )\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _cons |   .0161004   .0037127     4.34   0.000     .0088236    .0233772\n------------------------------------------------------------------------------\nmargins, dydx(*) expression( logistic(xb())*(1-logistic(xb()))*_b[educ] )\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n\n-------------+----------------------------------------------------------------\n        educ |  -.0010759   .0004973    -2.16   0.031    -.0020507   -.0001012\n    1.female |   .0246023   .0054734     4.49   0.000     .0138747    .0353299\n         age |   5.41e-06   .0000502     0.11   0.914     -.000093    .0001039\n   1.married |    .021062    .004861     4.33   0.000     .0115346    .0305893\n  1.divorced |   .0037604   .0013899     2.71   0.007     .0010362    .0064846\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/06nlreg.html#going-beyond",
    "href": "adv_class/06nlreg.html#going-beyond",
    "title": "NLS, IRLS, and MLE",
    "section": "Going Beyond?",
    "text": "Going Beyond?\nMLE can also be used in more Advanced models.\n\nMulti equation model that may or may not depend on each other.\nFor example, Oaxaca-Blinder Decomposition, requires using Multiple Equations\nIV - CF, or IV TSLS are also multi equation models but which depend on each other.\nOne could also estimate nonlinear versions of Standard models. For example Nonlinear Tobit model (See Ransom 1987)\nEstimation of latent groups of finite mixture models . This combine models that are otherwise unobserved (See Kapteyn and Ypma 2007)\n\nBasically, if you can figure out \\(f(.)\\)’s and how are they connected, you can estimate any model via MLE (with few exceptions)"
  },
  {
    "objectID": "adv_class/04cqreg.html#introduction",
    "href": "adv_class/04cqreg.html#introduction",
    "title": "Conditional Quantile Regressions",
    "section": "Introduction",
    "text": "Introduction\nQuestion: What are quantiles? and why do we care??\n\nQuantiles provide a better characterization of distributions.\n\nIt provides you with more information than standard summary statistics (means and variance)\n\nHow so? In general, there are 3 ways you can use to know “everything” about a distribution.\n\nYou either have access to every single \\(y_i\\)\nOr you know the distribution function \\(f(y)\\) (pdf)\nOr you know the cumulative distribution function \\(F(y)=\\int_\\infty^y f(t) dt = P(Y\\leq y)\\)\n\nHowever, there is an additional way. Quantile:\n\n\\[\nQ(\\theta) = F^{-1}(p)\n\\]"
  },
  {
    "objectID": "adv_class/04cqreg.html#qtheta-f-1theta",
    "href": "adv_class/04cqreg.html#qtheta-f-1theta",
    "title": "Conditional Quantile Regressions",
    "section": "\\(Q(\\theta) = F^{-1}(\\theta)\\)",
    "text": "\\(Q(\\theta) = F^{-1}(\\theta)\\)"
  },
  {
    "objectID": "adv_class/04cqreg.html#other-advantages-yes",
    "href": "adv_class/04cqreg.html#other-advantages-yes",
    "title": "Conditional Quantile Regressions",
    "section": "Other advantages? Yes!",
    "text": "Other advantages? Yes!\n\nQuantiles are far more stable in the presence of outliers. Because of this, they are particularly useful as measures of central tendency (perhaps superior to the mean) (🤔?)\n\nSimple “test”. In the small town of Troy-NY one of the residents wins the 2B$ lottery. How much has welfare increase for the average resident?\n\nScaled IQR can be used as an alternative measure of dispersion.\n\n\\[\nse2 = \\frac{Q_{75}-Q_{25}}{1.34898}\n\\]\n\nThey are also “function-transformation” resistant: \\(exp(Q_{log(y)} (.10)) = Q_y(.10)\\)"
  },
  {
    "objectID": "adv_class/04cqreg.html#section",
    "href": "adv_class/04cqreg.html#section",
    "title": "Conditional Quantile Regressions",
    "section": "",
    "text": "And are also very easy to estimate:\n\n\nSort data by \\(y\\) \\(\\rightarrow\\) Obtain weighted ranks \\(\\rightarrow\\) choose the lowest value so that \\(\\theta\\) % of the data is less of equal to that number\n\n\\[\nF^{-1} (tau) = inf(x: F(x)\\geq t)\n\\]\n\nThis “just” requires obtaining an approximation for \\(F(\\theta)\\), which can be approximated using nonparametric methods!\n\n\\[\\hat F(x) = \\frac{1}{N}\\sum (K_F(x,x_i,h)) =\n\\frac{1}{N}\\sum 1(x_i&lt;x)\n\\]\n\nthen we simply “invert” the function for whichever quantile we are interested in."
  },
  {
    "objectID": "adv_class/04cqreg.html#technical-note",
    "href": "adv_class/04cqreg.html#technical-note",
    "title": "Conditional Quantile Regressions",
    "section": "Technical Note",
    "text": "Technical Note\n\nThere are many empirical ways to estimate quantiles, even when using the empirical distribution function.\nSo do not be suprised about small differences in the estimates.\nWhen using Smooth functions, the choice of the kernel is also important. (and bandwidth)"
  },
  {
    "objectID": "adv_class/04cqreg.html#statistical-inference",
    "href": "adv_class/04cqreg.html#statistical-inference",
    "title": "Conditional Quantile Regressions",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nAs with the mean, sampling quantiles are measured with sampling error.\nHowever their standard errors are not as intuitive to obtain ( but can be derived using the delta Method)\n\n\\[Q_y(\\tau) = F_y^{-1}(\\tau) \\rightarrow F_y(Q_y(\\tau)) = \\tau \\ || \\ \\frac{\\partial}{\\partial \\tau}\n\\]\nThis gives us: \\[f_y(Q_y(\\tau)) \\frac{dQ}{d\\tau}  =1 \\rightarrow \\frac{dQ}{d\\tau} =  \\frac{1}{f(Q_y(\\tau))}\n\\]"
  },
  {
    "objectID": "adv_class/04cqreg.html#section-1",
    "href": "adv_class/04cqreg.html#section-1",
    "title": "Conditional Quantile Regressions",
    "section": "",
    "text": "So we have:\n\\[\n\\begin{aligned}\n\\hat Q_y(\\tau) &\\simeq Q_y(\\tau) + \\frac{1}{f(Q_y(\\tau))}(\\hat \\tau-\\tau) \\\\\n\\hat Q_y(\\tau) - Q_y(\\tau) &\\simeq \\frac{1}{f(Q_y(\\tau)}(\\hat \\tau-\\tau) \\\\\nVar(\\hat Q_y(\\tau)) &= \\frac{Var(\\hat \\tau - \\tau) }{f^2(Q_y(\\tau))} = \\frac{N^{-1} \\tau(1-\\tau)}{f^2(Q_y(\\tau))}\n\\end{aligned}\n\\]\nLets understand this elements"
  },
  {
    "objectID": "adv_class/04cqreg.html#section-3",
    "href": "adv_class/04cqreg.html#section-3",
    "title": "Conditional Quantile Regressions",
    "section": "",
    "text": "Quantile SE\n\\[\nVar(\\hat Q_y(\\tau)) = \\frac{Var(\\hat \\tau - \\tau) }{f^2(Q_y(\\tau))} = \\frac{1}{f^2(Q_y(\\tau))}\\frac{\\tau(1-\\tau)}{N}\n\\]\n\nThe variance of a quantile depends on the distribution of \\(\\tau\\). This follows a Bernoulli distribution: Is \\(y\\geq Q_y\\) or \\(y&lt;Q_y\\).\n\nLargest near the center of the distribution (50%-50%) but smaller (more precise) near the tails of the distribution.\n\nBut also depends on the density of the distribution.\n\nMore precise estimates when the density is high (center), but less precise near tails of the distribution.\nAnd, because \\(f()\\) is unknown, there is another source of variation.\n\nAnd as usual, it depends on the sample size (N)\nOf course, you also have the alternative method. Bootstrap!"
  },
  {
    "objectID": "adv_class/04cqreg.html#section-4",
    "href": "adv_class/04cqreg.html#section-4",
    "title": "Conditional Quantile Regressions",
    "section": "",
    "text": "Example\nUsing Bootstrap\n\nfrause wage2, clear\nbootstrap q10=r(r1) q25=r(r2) q50=r(r3) ///\n          q75=r(r4) q90=r(r5), reps(1000) nodots: ///\n          _pctile wage  , p(10 25 50 75 90)\n\n\n\n\n\nwarning: _pctile does not set e(sample), so no observations will be excluded\n         from the resampling because of missing values or other reasons. To\n         exclude observations, press Break, save the data, drop any\n         observations that are to be excluded, and rerun bootstrap.\n\nBootstrap results                                        Number of obs =   935\n                                                         Replications  = 1,000\n\n      Command: _pctile wage, p(10 25 50 75 90)\n          q10: r(r1)\n          q25: r(r2)\n          q50: r(r3)\n          q75: r(r4)\n          q90: r(r5)\n\n------------------------------------------------------------------------------\n             |   Observed   Bootstrap                         Normal-based\n             | coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         q10 |        500   9.149192    54.65   0.000     482.0679    517.9321\n         q25 |        668   14.21654    46.99   0.000     640.1361    695.8639\n         q50 |        905   15.19745    59.55   0.000     875.2135    934.7865\n         q75 |       1160   20.87954    55.56   0.000     1119.077    1200.923\n         q90 |       1444   32.84186    43.97   0.000     1379.631    1508.369\n------------------------------------------------------------------------------\n\n\nAnalytical SE\n\nsort wage\ngen w1 = _n\ngen w0 = _n-1\nby wage:gen p=0.5*(w1[_N]+w0[1])/935\nkdensity wage, at(wage) gen(fwage) nodraw\ngen se = sqrt(p*(1-p)/935)/fwage\ntabstat wage se if inlist(wage,500,668,905,1160,1444), by(wage)\n\n\nSummary statistics: Mean\nGroup variable: wage (monthly earnings)\n\n    wage |      wage        se\n---------+--------------------\n     500 |       500  13.27634\n     668 |       668  14.78419\n     905 |       905  14.69217\n    1160 |      1160   19.3574\n    1444 |      1444  29.32711\n---------+--------------------\n   Total |  730.7619  15.88035\n------------------------------"
  },
  {
    "objectID": "adv_class/04cqreg.html#from-q_y-to-q_yx",
    "href": "adv_class/04cqreg.html#from-q_y-to-q_yx",
    "title": "Conditional Quantile Regressions",
    "section": "From \\(Q_Y\\) to \\(Q_{Y|X}\\)",
    "text": "From \\(Q_Y\\) to \\(Q_{Y|X}\\)\n\nThe approaches used earlier to identify a particular quantile are not the only ones.\nJust like we can use OLS to estimate Means, we could also use a similar method to estimate the median.\nWe only need to change the loss function \\(L()\\) from an \\(L^2\\) to a \\(|L|\\).\n\nConsider this:\n\\[\\begin{aligned}\nmedian(Y) &= min_\\mu \\frac{1}{N}\\sum |y-\\mu| \\\\\n&=\\frac{2}{N}\\sum (y-u)(0.5-I([y-u]&lt;0)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/04cqreg.html#section-6",
    "href": "adv_class/04cqreg.html#section-6",
    "title": "Conditional Quantile Regressions",
    "section": "",
    "text": "Q and Loss functions\n\nWhy does it matter?Objective function\n\n\n\nThe loss function for Quantiles does not penalize “errors” as much as \\(L^2\\) does.\n\nThis is why its more robust to outliers (almost not affected by them).\n\nHowever, the loss function is no longer differentiable (is discontinuous). So requires other methods to find the solution. Even if it may not look like that:"
  },
  {
    "objectID": "adv_class/04cqreg.html#section-7",
    "href": "adv_class/04cqreg.html#section-7",
    "title": "Conditional Quantile Regressions",
    "section": "",
    "text": "From \\(\\beta\\) to \\(X\\beta\\)\nKoenker and Bassett (1978) extended this approximation in two ways:\n\nAllowing for Covariates (\\(X's\\)) variation\nAllowing to identify other quantiles in the distribution:\n\n\\[ \\beta(\\tau) = \\underset {b}{min} \\ N^{-1} \\sum \\rho_\\tau(y_i-X_i'b) \\\\\n\\rho_\\tau (u) = u (\\tau-I(u&lt;0))\n\\]\n\nThis implicitly states that you want to find a combination of \\(X's\\) such that \\(\\tau\\) proportion of \\(y_i\\) are lower than the \\(X_i'\\beta(\\tau)\\), for every combination of \\(X's\\).\n\nOr as close as possible."
  },
  {
    "objectID": "adv_class/04cqreg.html#interpretation-why-is-it-so-different-from-ols",
    "href": "adv_class/04cqreg.html#interpretation-why-is-it-so-different-from-ols",
    "title": "Conditional Quantile Regressions",
    "section": "Interpretation: Why is it so different from OLS?",
    "text": "Interpretation: Why is it so different from OLS?\n\nIndividualConditionalUnconditionalConclusion\n\n\n\nIn Rios-Avila and Maroto(2022) we stress that OLS can be interpreted at different “levels”. Consider the following:\n\n\\[\ny_i = b_0 + b_1 x_1 + b_2 x_2  + e\n\\]\nIf the errors are exogenous, and there is no heteroskedasticty, you can “obtain” marginal effects at many levels\n\n\n\\[ Ind: \\frac{dy_i}{dx_{1i}}=b_1\n\\]\nIf \\(x_1i\\) changes, and everything else is fixed, then \\(y_i\\) changes by \\(b_1\\) units for that individual.\n\n\n\\[\\begin{aligned}\nE(y_i|X=x)&=b_0 + b_1 x_1 + b_2 x_2 \\\\\n\\frac{dE(y|x)}{dx_1} &=b_1 \\\\\n\\end{aligned}\n\\]\nIf \\(x_1\\) changes for a group of individuals with the same characterisitcs, everything else is fixed, then everyone in that group will experience a change in \\(Y\\) by \\(b_1\\) units.\n\n\n\\[\\begin{aligned}\nE(y_i) &= b_0 + b_1 E(x_1) + b_2 E(x_2) \\\\\n\\frac{dE(y)}{dE(x_1)} &=b_1\n\\end{aligned}\n\\]\nIf \\(E(x_1)\\) changes for everyone, then the overall average change in \\(Y\\) is \\(b_1\\) units.\n\n\n\nSo in OLS, assuming a linear model in parameters, Nothing changes. The effect is the same! (although magnitude of the “experiment” changes)"
  },
  {
    "objectID": "adv_class/04cqreg.html#but-cqreg",
    "href": "adv_class/04cqreg.html#but-cqreg",
    "title": "Conditional Quantile Regressions",
    "section": "But CQreg?",
    "text": "But CQreg?\nFor quantile regressions, things are not that simple.\n\nThere is no “individual” level quantile effect, because we do not observe individual ranks \\(\\tau\\).\n\nIf we could observe them, and we assume they are fixed, then one can obtain individual level effects.\n\nBecause \\(\\tau\\) is unobserved, all Qregression coefficients, should be interpreted as effects on Conditional Distributions (thus the name CQREG).\n\nIn other words, effects are just expected changes in some points in the distribution.\n\nYou cannot use it for unconditional effects either (not easily), because\n\n\\[\nE(Q_{Y|X}(\\tau)) \\neq Q_Y(\\tau)\n\\]\nand you cannot “simply” average the CQREG effects to get unconditional quantiles."
  },
  {
    "objectID": "adv_class/04cqreg.html#section-9",
    "href": "adv_class/04cqreg.html#section-9",
    "title": "Conditional Quantile Regressions",
    "section": "",
    "text": "what does it mean?\n\nThis means that CQREG interpretation are percentile \\(\\tau\\) and covariate \\(X\\) specific.\n\nFixed rank. If you happen to be on the top of the distribution (and stay there), the quantile effect is given by the \\(\\beta(\\tau)\\)\nRank is not fixed: What we see is the effect of a change in \\(X\\) on the conditional distribution of \\(Y\\) (measured by the quantile)\n\n\nSo this must be kept in mind, whenever one interpret results"
  },
  {
    "objectID": "adv_class/04cqreg.html#section-10",
    "href": "adv_class/04cqreg.html#section-10",
    "title": "Conditional Quantile Regressions",
    "section": "",
    "text": "Visualizing Differences in Interpretation\n\nFixed RankVarying Rank"
  },
  {
    "objectID": "adv_class/04cqreg.html#example-wages",
    "href": "adv_class/04cqreg.html#example-wages",
    "title": "Conditional Quantile Regressions",
    "section": "Example: Wages…",
    "text": "Example: Wages…\n\nfrause oaxaca, clear\nqui:qreg  lnwage educ exper tenure female, nolog q(10)\nest sto m1\nqui:qreg  lnwage educ exper tenure female, nolog q(50)\nest sto m2\nqui:qreg  lnwage educ exper tenure female, nolog q(90)\nest sto m3\n* ssc install estout\nesttab m1 m2 m3, se nogaps mtitle(q10 q50 q90)\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                      q10             q50             q90   \n------------------------------------------------------------\neduc                0.103***       0.0694***       0.0639***\n                 (0.0166)       (0.00433)       (0.00902)   \nexper              0.0200***      0.00758***      0.00402   \n                (0.00493)       (0.00128)       (0.00267)   \ntenure           0.000669         0.00657***      0.00774*  \n                (0.00603)       (0.00157)       (0.00327)   \nfemale             -0.151         -0.0689**       -0.0543   \n                 (0.0806)        (0.0210)        (0.0437)   \n_cons               1.462***        2.474***        2.984***\n                  (0.219)        (0.0570)         (0.119)   \n------------------------------------------------------------\nN                    1434            1434            1434   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "adv_class/04cqreg.html#example-wages-1",
    "href": "adv_class/04cqreg.html#example-wages-1",
    "title": "Conditional Quantile Regressions",
    "section": "Example: Wages…",
    "text": "Example: Wages…\nqregplot educ exper tenure female, cons q(5/95)"
  },
  {
    "objectID": "adv_class/04cqreg.html#random-coefficents",
    "href": "adv_class/04cqreg.html#random-coefficents",
    "title": "Conditional Quantile Regressions",
    "section": "Random coefficents",
    "text": "Random coefficents\nOne approach to both understanding, and simulating QREG is by also understanding the intuition behind the data generating process.\n\\[\n\\begin{aligned}\ny &= b_0(\\tau)+b_1(\\tau)x_1 + +b_2(\\tau)x_2+...+b_k(\\tau) x_k \\\\\n\\tau &\\sim runiform(0,1)\n\\end{aligned}\n\\]\nwhere all coefficients are a function (preferably monotonically increasing or decreasing) of \\(\\tau\\) .\n\nWe want them to be monotonically increasing or decreasing because we want that \\[\nX \\beta(\\tau_1 ) \\geq\nX \\beta(\\tau_2 ) \\  \\text{ if } \\ \\tau_1 &gt; \\tau_2\n\\]"
  },
  {
    "objectID": "adv_class/04cqreg.html#section-11",
    "href": "adv_class/04cqreg.html#section-11",
    "title": "Conditional Quantile Regressions",
    "section": "",
    "text": "In this specification the unobserved component \\(\\tau\\) is similar to luck. If you are lucky and get a high \\(\\tau\\) then you will have better outcomes than anyone of your peers.\nAlso notice: \\(\\tau\\) is the only random factor, and should be uncorrelated with \\(X\\) (you do not make your luck!)"
  },
  {
    "objectID": "adv_class/04cqreg.html#svc-model-with-a-latent-running-variable",
    "href": "adv_class/04cqreg.html#svc-model-with-a-latent-running-variable",
    "title": "Conditional Quantile Regressions",
    "section": "SVC model with a latent running variable",
    "text": "SVC model with a latent running variable\n\nAnother way of thinking about Qreg is to align it to the semiparametric method we introduced ealier. SVC model.\nIn SVC, there is an observed running variable \\(z\\), and we focus on analyzing how the “local” effects of \\(X\\) on \\(Y\\) change as a function of \\(z\\).\nThe difference with Qreg is that the running variable is unknown \\(\\tau\\).\n\nGiven the outcome, and characteristics we can identify something like a “latent” component."
  },
  {
    "objectID": "adv_class/04cqreg.html#section-12",
    "href": "adv_class/04cqreg.html#section-12",
    "title": "Conditional Quantile Regressions",
    "section": "",
    "text": "There are a few (recent) papers that focus on estimation and identification of these models. The general intuition is that the qreg model is identified by the following moment condition:\n\n\\[\nE\\Big( 1[x\\beta(\\tau) - y &gt; 0 ] - \\tau \\Big) = 0\n\\]\nbut substitute the indicator function with a smooth function. CDF\n\\[\nE\\Big( F(x\\beta(\\tau) - y) - \\tau \\Big) = 0\n\\]\nBeing differentiable, this problem is relatively easier to solve (given good initial values)"
  },
  {
    "objectID": "adv_class/04cqreg.html#example-with-sivqr",
    "href": "adv_class/04cqreg.html#example-with-sivqr",
    "title": "Conditional Quantile Regressions",
    "section": "Example (with sivqr)",
    "text": "Example (with sivqr)"
  },
  {
    "objectID": "adv_class/04cqreg.html#scale-and-location-model",
    "href": "adv_class/04cqreg.html#scale-and-location-model",
    "title": "Conditional Quantile Regressions",
    "section": "Scale and Location Model",
    "text": "Scale and Location Model\nAnother approach that can be used to understand Quantile regressions (and elaborate the interpretation) is to assume that the coefficients are in fact capturing two components:\n\\[y = Xb + Xg(\\tau)\n\\]\n\nLocation: \\(Xb\\) which indicates what is the average/typical relationship between X and Y.\nScale: \\(Xg(\\tau)\\) which indicates how far one could be from the average effect, given a relative rank \\(\\tau\\)\n\nEstimation of this model is not standard. But can be manually implemented:\n\nEstimate OLS and get residuals\nEstimate QREG using those residuals\n\nRequires additional care for the estimation of SE"
  },
  {
    "objectID": "adv_class/04cqreg.html#scale-and-location-2-heteroskedasticity",
    "href": "adv_class/04cqreg.html#scale-and-location-2-heteroskedasticity",
    "title": "Conditional Quantile Regressions",
    "section": "Scale and Location 2: Heteroskedasticity",
    "text": "Scale and Location 2: Heteroskedasticity\nA second approach that is useful to understand and interpret CQreg is to consider a parametric version of the LS model:\n\\[y = Xb + \\gamma (X) * e \\text{ with } \\gamma(X)&gt;&gt;0\\]\n\nThis shows the relationship between a quantile regressions and heteroskedasticity in the error term.\nIf we assume Heteroskedasticity is parametric (\\(\\gamma(x) = X \\gamma\\)), it constrains the relationship across all quantile coefficients:\n\n\\[y = X(b+\\gamma F^{-1}(\\tau)) \\rightarrow b(\\tau)=b+\\gamma\\times q(\\tau)\n\\]\n\nMaking it more efficient, albeit imposing constrains of the relationship."
  },
  {
    "objectID": "adv_class/04cqreg.html#section-13",
    "href": "adv_class/04cqreg.html#section-13",
    "title": "Conditional Quantile Regressions",
    "section": "",
    "text": "Example (with mmqreg)\n\n\nCode\nqui:frause oaxaca, clear\nqui:mmqreg lnwage educ exper tenure female, robust\nqregplot educ exper tenure female, cons q(5(5)95)"
  },
  {
    "objectID": "adv_class/04cqreg.html#estimation-and-statistical-inference",
    "href": "adv_class/04cqreg.html#estimation-and-statistical-inference",
    "title": "Conditional Quantile Regressions",
    "section": "Estimation and Statistical Inference",
    "text": "Estimation and Statistical Inference\nAs hinted previously, there are many approaches that can be used for the estimation of Conditional Quantile regressions.\n\nOfficial: qreg, sqreg, bsqreg, iqreg\nCContributed: qreg2, qrprocess, mmqreg, smqreg, sivqr\nFor Standard errors, however, there are main 3 options. Under the assumption of iid error. Non iid error (robust), and assuming clustered standard errors.\n\n\\[\n\\begin{aligned}\niid: \\Sigma_\\beta &=\\frac{\\tau(1-\\tau)}{N f^2_y(F^{-1}(\\tau))}(X'X)^{-1} \\\\\nniid: \\Sigma_\\beta &= \\tau(1-\\tau) (X'f(0|x)X)^{-1} \\ (X'X) \\ (X'f(0|x)X)^{-1} \\\\\nalt: \\Sigma_\\beta &= (IF_\\beta \\ ' IF_\\beta) N^{-2}\n\\end{aligned}\n\\]\nOr simply Bootstrap"
  },
  {
    "objectID": "adv_class/04cqreg.html#problems-and-considerations",
    "href": "adv_class/04cqreg.html#problems-and-considerations",
    "title": "Conditional Quantile Regressions",
    "section": "Problems and Considerations",
    "text": "Problems and Considerations\n\nUnless otherwise specified, quantile regressions are linear in variables (and parameters?)\nWith few exceptions, quantile regressions are quantile specific. Comparisons across quantiles require joint estimation (to construct VCV matrix)\nBecause they are “local” estimators, there is risk of crossing quantiles. (Violation of Monotonicity)\nNon-linear effects will be present if either the location or scale components are nonlinear.\nQuantile regressions are very sensitive to measurement errors in both dependent and independent variables\nThey can be difficult to interpret (see references)\nImplementation of fixed effects is not straightforward"
  },
  {
    "objectID": "adv_class/04cqreg.html#the-problem",
    "href": "adv_class/04cqreg.html#the-problem",
    "title": "Conditional Quantile Regressions",
    "section": "The problem",
    "text": "The problem\n\nThere are two problems related to Estimating Quantile Regressions with (multiple) Fixed Effects\n\nFirst: As with nonlinear models, Adding many fixed effects creates an incidental parameter problem.\nSecond: For Conditional Quantile Regressions, it can be difficult to interpret the role of fixed effects."
  },
  {
    "objectID": "adv_class/04cqreg.html#simulating-some-data",
    "href": "adv_class/04cqreg.html#simulating-some-data",
    "title": "Conditional Quantile Regressions",
    "section": "Simulating some data",
    "text": "Simulating some data\n\nclear\nset obs 1000\ngen id = _n\ngen vi = rnormal()\ngen ui = rnormal()+vi\n\ngen toexp = 1+rpoisson(5)\nexpand toexp\ngen err = rnormal()\ngen x1 = rnormal()+vi+err\ngen x2 = rnormal()+vi+err\ngen y = 1+x1+x2+ui+rnormal()*exp(0.2*x1-0.2*x2+0.3*ui)\n\nNumber of observations (_N) was 0, now 1,000.\n(5,019 observations created)"
  },
  {
    "objectID": "adv_class/04cqreg.html#benchmark",
    "href": "adv_class/04cqreg.html#benchmark",
    "title": "Conditional Quantile Regressions",
    "section": "Benchmark",
    "text": "Benchmark\nAssume you observe those fixed effects:\n\n\nCode\nset line 255\nqui:qreg y x1 x2 ui, q(10)\nest sto m10\nqui:qreg y x1 x2 ui, q(50)\nest sto m20\nqui:qreg y x1 x2 ui, q(90)\nest sto m30\nesttab m10 m20 m30, se nogaps mtitle(q10 q50 q90)\n\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                      q10             q50             q90   \n------------------------------------------------------------\nx1                  0.780***        0.988***        1.219***\n                 (0.0190)        (0.0139)        (0.0224)   \nx2                  1.219***        1.000***        0.763***\n                 (0.0189)        (0.0138)        (0.0223)   \nui                  0.654***        1.000***        1.352***\n                 (0.0180)        (0.0132)        (0.0212)   \n_cons              -0.458***        0.970***        2.420***\n                 (0.0236)        (0.0173)        (0.0279)   \n------------------------------------------------------------\nN                    6019            6019            6019   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "adv_class/04cqreg.html#ignoring-fixed-effects",
    "href": "adv_class/04cqreg.html#ignoring-fixed-effects",
    "title": "Conditional Quantile Regressions",
    "section": "Ignoring Fixed Effects",
    "text": "Ignoring Fixed Effects\n\n\nCode\nqui:qreg y x1 x2  , q(10)\nest sto m1\nqui:qreg y x1 x2  , q(50)\nest sto m2\nqui:qreg y x1 x2  , q(90)\nest sto m3\nesttab m1 m2 m3 m10 m20 m30, se nogaps mtitle(q10 q50 q90)\n\n\n\n------------------------------------------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)             (5)             (6)   \n                      q10             q50             q90             m10             m20             m30   \n------------------------------------------------------------------------------------------------------------\nx1                  0.981***        1.171***        1.470***        0.780***        0.988***        1.219***\n                 (0.0235)        (0.0217)        (0.0415)        (0.0190)        (0.0139)        (0.0224)   \nx2                  1.339***        1.260***        1.139***        1.219***        1.000***        0.763***\n                 (0.0234)        (0.0215)        (0.0412)        (0.0189)        (0.0138)        (0.0223)   \nui                                                                  0.654***        1.000***        1.352***\n                                                                 (0.0180)        (0.0132)        (0.0212)   \n_cons              -1.133***        0.813***        3.275***       -0.458***        0.970***        2.420***\n                 (0.0300)        (0.0276)        (0.0529)        (0.0236)        (0.0173)        (0.0279)   \n------------------------------------------------------------------------------------------------------------\nN                    6019            6019            6019            6019            6019            6019   \n------------------------------------------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "adv_class/04cqreg.html#solution-1-correlated-random-effects",
    "href": "adv_class/04cqreg.html#solution-1-correlated-random-effects",
    "title": "Conditional Quantile Regressions",
    "section": "Solution 1: Correlated Random Effects",
    "text": "Solution 1: Correlated Random Effects\nIdea: Include “PAnel average” of all indep variables as regressors.\nThey should control (at least partially) for the unobserved effects.\n\\[Q_p(y|X) = X\\beta + \\alpha \\bar X + \\epsilon\\]\n\n\nCode\nbysort id: egen x1p = mean(x1)\nbysort id: egen x2p = mean(x2)\nqui:qreg y x1 x2 x1p x2p , q(10)\nest sto m1\nqui:qreg y x1 x2  x1p x2p , q(50)\nest sto m2\nqui:qreg y x1 x2  x1p x2p , q(90)\nest sto m3\nesttab m1 m2 m3 m10 m20 m30, se nogaps mtitle(q10 q50 q90)\n\n\n\n------------------------------------------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)             (5)             (6)   \n                      q10             q50             q90             m10             m20             m30   \n------------------------------------------------------------------------------------------------------------\nx1                  0.825***        0.972***        1.195***        0.780***        0.988***        1.219***\n                 (0.0250)        (0.0246)        (0.0460)        (0.0190)        (0.0139)        (0.0224)   \nx2                  1.176***        1.023***        0.774***        1.219***        1.000***        0.763***\n                 (0.0252)        (0.0248)        (0.0463)        (0.0189)        (0.0138)        (0.0223)   \nx1p                 0.310***        0.413***        0.663***                                                \n                 (0.0575)        (0.0565)         (0.106)                                                   \nx2p                 0.283***        0.406***        0.551***                                                \n                 (0.0564)        (0.0555)         (0.104)                                                   \nui                                                                  0.654***        1.000***        1.352***\n                                                                 (0.0180)        (0.0132)        (0.0212)   \n_cons              -0.951***        0.861***        3.146***       -0.458***        0.970***        2.420***\n                 (0.0278)        (0.0273)        (0.0511)        (0.0236)        (0.0173)        (0.0279)   \n------------------------------------------------------------------------------------------------------------\nN                    6019            6019            6019            6019            6019            6019   \n------------------------------------------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "adv_class/04cqreg.html#solution-2-fe-are-fixed",
    "href": "adv_class/04cqreg.html#solution-2-fe-are-fixed",
    "title": "Conditional Quantile Regressions",
    "section": "Solution 2: FE are Fixed",
    "text": "Solution 2: FE are Fixed\n\nCanay (2011) proposes make the “simplifying” that “Fixed effects” are constant across quantiles.\nThus a two step procedure is proposed:\n\nFirst: Estimate the fixed effects using OLS\nSecond: Estimate the quantile regression using outcome after taking FE “off”\n\n\n\\[\\begin{aligned}\ny &= X\\beta + \\alpha_i + \\epsilon \\\\\nQ_\\tau(y - \\hat \\alpha_i |X) &= X\\beta(\\tau) +  \\epsilon_\\tau \\\\\n\\end{aligned}\n\\]\n\n\nCode\nqui:reghdfe y x1 x2, absorb(fe = id)\ngen y_fe = y - fe\nqui:qreg y_fe x1 x2   , q(10)\nest sto m1\nqui:qreg y_fe x1 x2   , q(50)\nest sto m2\nqui:qreg y_fe x1 x2 , q(90)\nest sto m3\nesttab m1 m2 m3 m10 m20 m30, se nogaps mtitle(q10 q50 q90)\n\n\n(4 missing values generated)\n\n------------------------------------------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)             (5)             (6)   \n                      q10             q50             q90             m10             m20             m30   \n------------------------------------------------------------------------------------------------------------\nx1                  0.727***        0.992***        1.277***        0.780***        0.988***        1.219***\n                 (0.0213)        (0.0120)        (0.0218)        (0.0190)        (0.0139)        (0.0224)   \nx2                  1.117***        1.004***        0.853***        1.219***        1.000***        0.763***\n                 (0.0212)        (0.0119)        (0.0216)        (0.0189)        (0.0138)        (0.0223)   \nui                                                                  0.654***        1.000***        1.352***\n                                                                 (0.0180)        (0.0132)        (0.0212)   \n_cons              -0.296***        1.021***        2.336***       -0.458***        0.970***        2.420***\n                 (0.0272)        (0.0153)        (0.0277)        (0.0236)        (0.0173)        (0.0279)   \n------------------------------------------------------------------------------------------------------------\nN                    6015            6015            6015            6019            6019            6019   \n------------------------------------------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "adv_class/04cqreg.html#solution-3-modified-canay2011",
    "href": "adv_class/04cqreg.html#solution-3-modified-canay2011",
    "title": "Conditional Quantile Regressions",
    "section": "Solution 3: Modified Canay(2011)",
    "text": "Solution 3: Modified Canay(2011)\n\nSame as before, but rather than “removing” fixed effects, we control for them in the model:\n\n\\[\\begin{aligned}\ny &= X\\beta + \\alpha_i + \\epsilon \\\\\nQ_\\tau(y |X) &= X\\beta(\\tau) + \\gamma \\hat \\alpha_i + \\epsilon_\\tau \\\\\n\\end{aligned}\n\\]\n\n\nCode\nqui:qreg y x1 x2 fe  , q(10)\nest sto m1\nqui:qreg y x1 x2 fe  , q(50)\nest sto m2\nqui:qreg y x1 x2 fe, q(90)\nest sto m3\nesttab m1 m2 m3 m10 m20 m30, se nogaps mtitle(q10 q50 q90)\n\n\n\n------------------------------------------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)             (5)             (6)   \n                      q10             q50             q90             m10             m20             m30   \n------------------------------------------------------------------------------------------------------------\nx1                  0.800***        0.993***        1.171***        0.780***        0.988***        1.219***\n                 (0.0182)        (0.0137)        (0.0206)        (0.0190)        (0.0139)        (0.0224)   \nx2                  1.193***        1.006***        0.831***        1.219***        1.000***        0.763***\n                 (0.0181)        (0.0137)        (0.0205)        (0.0189)        (0.0138)        (0.0223)   \nfe                  0.691***        0.991***        1.279***                                                \n                 (0.0159)        (0.0120)        (0.0180)                                                   \nui                                                                  0.654***        1.000***        1.352***\n                                                                 (0.0180)        (0.0132)        (0.0212)   \n_cons              -0.334***        1.017***        2.354***       -0.458***        0.970***        2.420***\n                 (0.0227)        (0.0171)        (0.0257)        (0.0236)        (0.0173)        (0.0279)   \n------------------------------------------------------------------------------------------------------------\nN                    6015            6015            6015            6019            6019            6019   \n------------------------------------------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "adv_class/04cqreg.html#solution-4-ls-model",
    "href": "adv_class/04cqreg.html#solution-4-ls-model",
    "title": "Conditional Quantile Regressions",
    "section": "Solution 4: LS model",
    "text": "Solution 4: LS model\n\nMachado and Silva (2019) propose a different approach. They suggest modeling the quantile regression using linear models for a scale and location model.\nThis simplifies the task of estimating multiple equations:\n\n\\[\\begin{aligned}\ny &= X\\beta + \\epsilon \\\\\n\\hat\\epsilon^2 &= X\\gamma + \\nu \\\\\nQ\\left( \\frac{\\hat\\epsilon}{X\\gamma}\\right) &= q(\\tau) \\\\\n\\beta(\\tau) &= \\beta + \\gamma \\times q(\\tau)\n\\end {aligned}\n\\]\n\nqui:mmqreg y x1 x2   , q(10)  abs(id) robust\nest sto m1\nqui:mmqreg y x1 x2   , q(50)  abs(id) robust\nest sto m2\nqui:mmqreg y x1 x2 , q(90) abs(id) robust\nest sto m3\nesttab m1 m2 m3 m10 m20 m30, se nogaps mtitle(q10 q50 q90)\n\n\n------------------------------------------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)             (5)             (6)   \n                      q10             q50             q90             m10             m20             m30   \n------------------------------------------------------------------------------------------------------------\nmain                                                                                                        \nx1                  0.780***        0.993***        1.214***        0.780***        0.988***        1.219***\n                 (0.0183)        (0.0147)        (0.0203)        (0.0190)        (0.0139)        (0.0224)   \nx2                  1.231***        0.992***        0.745***        1.219***        1.000***        0.763***\n                 (0.0197)        (0.0162)        (0.0233)        (0.0189)        (0.0138)        (0.0223)   \nui                                                                  0.654***        1.000***        1.352***\n                                                                 (0.0180)        (0.0132)        (0.0212)   \n_cons              -0.346***        1.017***        2.431***       -0.458***        0.970***        2.420***\n                 (0.0220)        (0.0207)        (0.0251)        (0.0236)        (0.0173)        (0.0279)   \n------------------------------------------------------------------------------------------------------------\nN                    6019            6019            6019            6019            6019            6019   \n------------------------------------------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "adv_class/02OLS.html#introduction",
    "href": "adv_class/02OLS.html#introduction",
    "title": "Linear Regression Model",
    "section": "Introduction",
    "text": "Introduction\n\nLinear Regression is the most basic, and still most useful, tool for analyzing data.\nThe goal is to find what the relationship between the outcome \\(y\\) and explanatory variables \\(X's\\) is.\nSay that we start with a very simple “model” that states tries to describe the population function as the following:\n\n\\[\ny = h(X,\\varepsilon)\n\\]\nHere, \\(X\\) represents a set of observed covariates and \\(\\varepsilon\\) the set of unobserved characteristics, with no no pre-defined relationship between these components.\n\nFor now, we will make standard exogeneity assumptions for the identification of the model"
  },
  {
    "objectID": "adv_class/02OLS.html#estimation",
    "href": "adv_class/02OLS.html#estimation",
    "title": "Linear Regression Model",
    "section": "Estimation",
    "text": "Estimation\n\nThe functional form is unknowable. However, under the small assumption of Exogeneity of \\(X\\), we could instead consider the Conditional Expectation function (CEF):\n\n\\[\nE(y_i|X_i=x) = \\int y f_{y|x}(y)dy\n\\]\n\nThis implies a fully non-parametric estimation of the Linear function.\nWith this, the outcome \\(y\\) can be decomposed into factors determined by observed characteristics (CEF) and on the error \\(\\varepsilon\\).\n\n\\[\ny = E(y|X) + \\varepsilon\n\\]"
  },
  {
    "objectID": "adv_class/02OLS.html#section",
    "href": "adv_class/02OLS.html#section",
    "title": "Linear Regression Model",
    "section": "",
    "text": "The CEF is a convenient abstract, but to estimate it, we require assumptions. (Recall the assumptions for unbiased OLS?)\nNamely, we need to impose a linearity assumption, namely:\n\n\\[\nE(y_i|X_i=x) = \\beta_0 + \\beta_1 x_1 +\\beta_2 x_2 + ... +\n\\beta_k x_k = X_i'\\beta\n\\]\n\nAnd the solution for \\(\\beta\\) is given by:\n\n\\[\n\\beta = \\underset{b}{arg} \\ E(L(y_i-X'_i b))\n\\]\nWhere the loss function \\(L(x)=x^2\\). (Square loss function)\n\nThis implies the following condition: \\(E[X_i (y_i-X_i'b)]=0 \\rightarrow \\beta = E[X_i'X_i]^{-1}E[X_i'y_i]\\)"
  },
  {
    "objectID": "adv_class/02OLS.html#mata-ols-estimator",
    "href": "adv_class/02OLS.html#mata-ols-estimator",
    "title": "Linear Regression Model",
    "section": "Mata: OLS Estimator",
    "text": "Mata: OLS Estimator\nThe estimator using Sample equivalents become:\n\\[\n\\hat \\beta =\n\\left(\\frac{1}{N} \\sum_i X_i'X_i \\right)^{-1}\n\\frac{1}{N} \\sum_i X_i'y_i=(X'X)^{-1}X'y\n\\]\n\n\nCode\nfrause oaxaca, clear\nkeep if lnwage !=.\nmata:\n  y = st_data(.,\"lnwage\")\n  n = rows(y)\n  x = st_data(.,\"female age educ\"),J(n,1,1)\n  exx = cross(x,x)/n\n  exy = cross(x,y)/n\n  b   = invsym(exx)*exy\n  b\nend  \n\n\n\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:   y = st_data(.,\"lnwage\")\n\n:   n = rows(y)\n\n:   x = st_data(.,\"female age educ\"),J(n,1,1)\n\n:   exx = cross(x,x)/n\n\n:   exy = cross(x,y)/n\n\n:   b = invsym(exx)*exy\n\n:   b\n                 1\n    +---------------+\n  1 |  -.145393595  |\n  2 |  .0161424301  |\n  3 |  .0719321873  |\n  4 |  1.970020725  |\n    +---------------+\n\n: end\n-------------------------------------------------------------------------------\n\n."
  },
  {
    "objectID": "adv_class/02OLS.html#inference---distribution-of-betas",
    "href": "adv_class/02OLS.html#inference---distribution-of-betas",
    "title": "Linear Regression Model",
    "section": "Inference - Distribution of \\(\\beta's\\)",
    "text": "Inference - Distribution of \\(\\beta's\\)\nGiven the model and OLS estimator:\n\\[\\begin{aligned}\ny &= X\\beta + \\varepsilon \\\\\n\\hat \\beta &= (X'X)^{-1}X'y\n\\end{aligned}\n\\]\nIf we substitute \\(y\\) in the second equation, we get:\n\\[\\begin{aligned}\n\\hat \\beta &= (X'X)^{-1}X'( X\\beta + \\varepsilon) \\\\\n\\hat \\beta &= \\beta + (X'X)^{-1}X'\\varepsilon) \\\\\n\\hat \\beta - \\beta &=  (X'X)^{-1}X'\\varepsilon) \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/02OLS.html#section-1",
    "href": "adv_class/02OLS.html#section-1",
    "title": "Linear Regression Model",
    "section": "",
    "text": "Finally: \\[\n\\sqrt N (\\hat\\beta - \\beta) = {\\sqrt N}\\Big[\\frac{1}{N}\\sum (X_iX_i')\\Big]^{-1} \\frac{1}{N}\\sum(X_i\\varepsilon_i)\n\\]\n\nHere \\(\\varepsilon\\) is the true population error. \\(\\hat\\beta\\) is unbiased if the second term has an expectation of Zero. (the error is independent from \\(X\\)).\nThe first term is assumed fixed \\(E(X_i X_i')\\). And, because \\(E(X_i\\varepsilon)=0\\), and \\(\\frac{1}{\\sqrt N} \\sum(X_i\\varepsilon)\\) is normalized, by CLT we have that:\n\n\\[\n\\sqrt N (\\hat\\beta-\\beta)\\sim N(0,E(X_iX_i')^{-1} \\ E(X_iX_i'\\varepsilon_i ^2) \\ E(X_iX_i')^{-1})\n\\]\n\nFrom here, the main question is : How do we estimate \\(E(X_iX'\\varepsilon_i^2)\\)?"
  },
  {
    "objectID": "adv_class/02OLS.html#inference-estimating-se",
    "href": "adv_class/02OLS.html#inference-estimating-se",
    "title": "Linear Regression Model",
    "section": "Inference: Estimating SE",
    "text": "Inference: Estimating SE\n\nLets First Rewrite the last expression:\n\n\\[\nVar(\\hat\\beta)=(X'X)^{-1} X'\\Omega X (X'X)^{-1}\n\\]\nwhere:\n\\[\n\\Omega=\n\\left(\n\\begin{matrix}\n\\sigma_1^2 & \\sigma_{12} &  ... & \\sigma_{1N}\\\\\n\\sigma_{21} & \\sigma_{2}^2 & ... & \\sigma_{2N} \\\\\n...&...&...&...\\\\\n\\sigma_{N1} & \\sigma_{N2} & ... & \\sigma_{NN}^2 \\\\\n\\end{matrix}\n\\right)\n\\]\nIn other words, the variance of \\(\\hat\\beta\\) allows for arbitrary relationship among the errors, as well as heteroskedasticity. This, however is impossible to estimate!, thus we require assumptions"
  },
  {
    "objectID": "adv_class/02OLS.html#homoskedasticity-and-independent-samples",
    "href": "adv_class/02OLS.html#homoskedasticity-and-independent-samples",
    "title": "Linear Regression Model",
    "section": "Homoskedasticity and independent samples",
    "text": "Homoskedasticity and independent samples\nWith homoskedastic errors \\(\\sigma^2 = \\sigma_i^2 \\ \\forall i \\in 1,...,N\\) .\nWith independent samples \\(\\sigma_{ij}=0 \\ \\forall \\ i\\neq j\\) . \\[\n\\Omega_00=\n\\left(\n\\begin{matrix}\n\\sigma_1^2 & \\sigma_{12} &  ... & \\sigma_{1N}\\\\\n\\sigma_{21} & \\sigma_{2}^2 & ... & \\sigma_{2N} \\\\\n...&...&...&...\\\\\n\\sigma_{N1} & \\sigma_{N2} & ... & \\sigma_{NN}^2 \\\\\n\\end{matrix}\n\\right)=I(N)*\\sigma^2\n\\]\nThus \\[\n\\begin{aligned}\nVar(\\hat\\beta)_{00} &=(X'X)^{-1} X'I(N)\\sigma^2 X (X'X)^{-1} =\\sigma^2 (X'X)^{-1} \\\\\n\\sigma^2 &= E(\\varepsilon^2)\n\\end{aligned}\n\\]\n\n\nCode\nmata: e=err = y:-x*b\nmata: var_b_000 = mean(err:^2) * invsym(x'x)\nmata: b,sqrt(diagonal(var_b_000))\n\n\n                 1             2\n    +-----------------------------+\n  1 |  -.145393595   .0243547399  |\n  2 |  .0161424301   .0010962465  |\n  3 |  .0719321873    .005029506  |\n  4 |  1.970020725   .0724744138  |\n    +-----------------------------+"
  },
  {
    "objectID": "adv_class/02OLS.html#section-2",
    "href": "adv_class/02OLS.html#section-2",
    "title": "Linear Regression Model",
    "section": "",
    "text": "But, \\(\\sigma^2\\) is not known, so we have to use \\(\\hat\\sigma^2\\) instead, which depends on the sample residuals: \\[\n\\hat\\sigma^2 = \\frac{1}{N-k-1}\\sum \\hat e^2\n\\]\nWhere we account for the fact true errors are not observed, but rather residuals are estimated, adjusting the degrees of freedom.\n\n\nCode\nmata:\n    N = rows(y); k = cols(x)\n    var_b_00 = sum(err:^2)/(N-k) * invsym(x'x)\n    b,sqrt(diagonal(var_b_00))\nend\n\n\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     N = rows(y); k = cols(x)\n\n:     var_b_00 = sum(err:^2)/(N-k) * invsym(x'x)\n\n:     b,sqrt(diagonal(var_b_00))\n                 1             2\n    +-----------------------------+\n  1 |  -.145393595   .0243887787  |\n  2 |  .0161424301   .0010977786  |\n  3 |  .0719321873   .0050365354  |\n  4 |  1.970020725   .0725757058  |\n    +-----------------------------+\n\n: end\n-------------------------------------------------------------------------------\n\n."
  },
  {
    "objectID": "adv_class/02OLS.html#lifting-assumptions-heteroscedasticity",
    "href": "adv_class/02OLS.html#lifting-assumptions-heteroscedasticity",
    "title": "Linear Regression Model",
    "section": "Lifting Assumptions: Heteroscedasticity",
    "text": "Lifting Assumptions: Heteroscedasticity\n\nWe start by lifting this assumption, which implies the following:\n\n\\[\n\\sigma^2_i \\neq \\sigma^2_j \\  \\forall \\ i\\neq j\n\\]\nBut to estimate this, we need an approximation for \\(\\sigma^2_i = E(\\varepsilon_i^2) = \\varepsilon_i^2\\).\n\nWith this, we can obtain what is known as th White or Eicker-White or Heteroskedasiticy Robust Standard errors.\n\n\\[\n\\begin{aligned}\nVar(\\hat\\beta)_{0} &= (X'X)^{-1} (X \\cdot \\hat e)'(X \\cdot  \\hat e) (X'X)^{-1} \\\\\n&=(X'X)^{-1} \\sum(X_iX_i'\\hat e^2) (X'X)^{-1}\n\\end{aligned}\n\\]\nWhich imposes NO penalty to the fact that we are using residuals not errors. If we account for that however, we obtain what is known as HC1, SE, the standard in Stata. (when you type robust)\n\\[\nVar(\\hat\\beta)_{1}=\\frac{N}{N-K-1}Var(\\hat\\beta)_{0}\n\\]\n\n\nCode\nmata:\n    ixx = invsym(x'x)\n    var_b_0 = ixx * (x:*e)'(x:*e) * ixx\n    var_b_1 = N/(N-k)*var_b_0\n    b,sqrt(diagonal(var_b_0)),sqrt(diagonal(var_b_1))\nend\n\n\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     ixx = invsym(x'x)\n\n:     var_b_0 = ixx * (x:*e)'(x:*e) * ixx\n\n:     var_b_1 = N/(N-k)*var_b_0\n\n:     b,sqrt(diagonal(var_b_0)),sqrt(diagonal(var_b_1))\n                 1             2             3\n    +-------------------------------------------+\n  1 |  -.145393595   .0243162137   .0243501986  |\n  2 |  .0161424301   .0013544849   .0013563779  |\n  3 |  .0719321873    .005690214   .0056981668  |\n  4 |  1.970020725   .0875757052   .0876981032  |\n    +-------------------------------------------+\n\n: end\n-------------------------------------------------------------------------------\n\n."
  },
  {
    "objectID": "adv_class/02OLS.html#but-error-is-not-the-same-as-residual",
    "href": "adv_class/02OLS.html#but-error-is-not-the-same-as-residual",
    "title": "Linear Regression Model",
    "section": "But error is not the same as residual!",
    "text": "But error is not the same as residual!\nA residual is model dependent, and should not be confused with the model error \\(\\hat \\varepsilon \\neq \\varepsilon\\). Because of this, additional corrections are needed to obtained unbiased \\(var(\\hat\\beta)\\) estimates. (Degrees of freedom). But other options exists.\nRedefine the Variance Formula:\n\\[\nVar(\\hat\\beta)=(X'X)^{-1} (\\sum X_iX_i \\psi_i )  (X'X)^{-1}\n\\]\nFrom here Mackinnon and White (1985) suggest few other options: \\[\n\\begin{matrix}\nHC0: \\psi_i = \\hat e^2 &\nHC1: \\psi_i = \\frac{N}{N-K}  \\hat e^2 \\\\\nHC2: \\psi_i =   \\hat e^2 \\frac{1}{1-h_{ii}} &\nHC3: \\psi_i =   \\hat e^2 \\frac{1}{(1-h_{ii})^2}\n\\end{matrix}\n\\]\nWhere \\(h_{ii}\\) is the ith diagonal element of \\(X(X'X)^{-1}X'\\) and allows you to see how dependent a model is to a single observation.\nHC2 and HC3 Standard errors are better than HC1 SE, specially when Samples are small.\n\nNOTE: this \\(h_{ii}\\) element is also used to measure the degrees of freedom of a model. Sum it up, and you will see!."
  },
  {
    "objectID": "adv_class/02OLS.html#coding-robust-se",
    "href": "adv_class/02OLS.html#coding-robust-se",
    "title": "Linear Regression Model",
    "section": "Coding Robust SE",
    "text": "Coding Robust SE\n\n\nCode\nmata:\n    // h = diagonal(X invsym(X'x) X') Wrong Way, too many calculations\n    h = rowsum(x*invsym(x'x):*x)\n    psi0 = e:^2           ;   psi1 = e:^2*N/(N-k)\n    psi2 = e:^2:/(1:-h)   ;   psi3 = e:^2:/((1:-h):^2)\n    var_b_0 = ixx * cross(x,psi0,x) * ixx\n    var_b_1 = ixx * cross(x,psi1,x) * ixx\n    var_b_2 = ixx * cross(x,psi2,x) * ixx\n    var_b_3 = ixx * cross(x,psi3,x) * ixx\n    b,sqrt(diagonal(var_b_0)),sqrt(diagonal(var_b_1)),\n    sqrt(diagonal(var_b_2)),sqrt(diagonal(var_b_3))\nend  \n\n\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     h = rowsum(x*invsym(x'x):*x)\n\n:     psi0 = e:^2 ; psi1 = e:^2*N/(N-k)\n\n:     psi2 = e:^2:/(1:-h) ; psi3 = e:^2:/((1:-h):^2)\n\n:     var_b_0 = ixx * cross(x,psi0,x) * ixx\n\n:     var_b_1 = ixx * cross(x,psi1,x) * ixx\n\n:     var_b_2 = ixx * cross(x,psi2,x) * ixx\n\n:     var_b_3 = ixx * cross(x,psi3,x) * ixx\n\n:     b,sqrt(diagonal(var_b_0)),sqrt(diagonal(var_b_1)),\n&gt;     sqrt(diagonal(var_b_2)),sqrt(diagonal(var_b_3))\n                 1             2             3             4             5\n    +-----------------------------------------------------------------------+\n  1 |  -.145393595   .0243162137   .0243501986   .0243568124   .0243975204  |\n  2 |  .0161424301   .0013544849   .0013563779   .0013573922   .0013603079  |\n  3 |  .0719321873    .005690214   .0056981668   .0057079191    .005725691  |\n  4 |  1.970020725   .0875757052   .0876981032   .0878131672   .0880514838  |\n    +-----------------------------------------------------------------------+\n\n: end\n-------------------------------------------------------------------------------\n\n. \n\n\nOr in Stata:\nregress y x1 x2 x3, vce(robust)\nregress y x1 x2 x3, vce(hc2)\nregress y x1 x2 x3, vce(hc3)"
  },
  {
    "objectID": "adv_class/02OLS.html#gls-and-weighted-least-squares",
    "href": "adv_class/02OLS.html#gls-and-weighted-least-squares",
    "title": "Linear Regression Model",
    "section": "GLS and Weighted Least Squares",
    "text": "GLS and Weighted Least Squares\n\nGLS is a generalization of OLS, that could be used to address heteroskedasticity.\nThere are two ways to do this:\n\nTransform/weight the data to make it homoskedastic (WLS)\nModify the variance covariance matrix of the errors (GLS)\n\nCall \\(\\hat h(x)\\) the predicted error variance. The GLS estimator for \\(V_{gls}(\\beta)\\) is given by:\n\n\\[V_{gls}(\\beta)=(X'X)^{-1} \\sum(X_iX_i'\\hat h(x)) (X'X)^{-1}\n\\]\n\nThat way, Heteroskedasticity is addressed, but without changing the model estimates \\(\\beta's\\)"
  },
  {
    "objectID": "adv_class/02OLS.html#lifting-even-more-assumptions-correlation",
    "href": "adv_class/02OLS.html#lifting-even-more-assumptions-correlation",
    "title": "Linear Regression Model",
    "section": "Lifting Even more Assumptions: Correlation",
    "text": "Lifting Even more Assumptions: Correlation\n\nOne assumption we barely consider last semester was the possibility that errors could be correlated across observations. (except for time series and serial correlation)\nFor example, families may share similar unobserved factors, So would people interviewed from the same classroom, cohort, city, etc. There could be many dimensions to consider possible correlations!\nIn that situation, we may be missmeasuring the magnitude of the errors (probably downward), because the \\(\\Omega\\) is no longer diagonal: \\(\\sigma_{ij} \\neq 0\\) for some \\(i\\neq j\\).\n\nBut, estimate all parameters in an NxN matrix is unfeasible. We need assumptions!"
  },
  {
    "objectID": "adv_class/02OLS.html#section-3",
    "href": "adv_class/02OLS.html#section-3",
    "title": "Linear Regression Model",
    "section": "",
    "text": "New Assumptions\n\nSay we have \\(G\\) groups \\(g=(1…G)\\) . We can rewrite the expression for \\(\\hat\\beta\\) as follows:\n\n\\[\n\\begin{aligned}\n\\hat\\beta-\\beta &= (X'X)^{-1}\\sum_{g=1}^G X'_g \\varepsilon_g \\\\\n&=(X'X)^{-1}\\sum_{g=1}^G s_g\n\\end{aligned}\n\\]\n\nWe can assume that individuals are correlated within groups \\(E(s_g's_g) =\\Sigma_g\\) , but they are uncorrelated across groups \\(E(s_g s_g')=0 \\ \\forall \\ g \\neq g'\\) .\nThese groups are typically known as “clusters”"
  },
  {
    "objectID": "adv_class/02OLS.html#section-4",
    "href": "adv_class/02OLS.html#section-4",
    "title": "Linear Regression Model",
    "section": "",
    "text": "Addressing Correlation\n\nThe idea of correcting for clusters is pretty simple. We just need to come up with an estimator for \\(\\Sigma_g\\) for every cluster, so that:\n\n\\[\n\\begin{aligned}\nVar(\\hat\\beta) &= (X'X)^{-1} \\left( \\sum_{g=1}^N \\Sigma_g \\right) (X'X)^{-1} \\\\\n\\Sigma_g &= E( X_g' \\Omega_g X_g)\n\\end{aligned}\n\\]\n\nHere \\(\\Omega_g\\) should be an approximation of the variance covariance matrix among the errors of ALL individuals that belong to the same cluster. But how do we approximate it?\nAs with the EW - HC standard errors, there are many ways to estimate Clustered Standard errors. See MacKinnon et al (2023) for reference. We will refer only to the simpler ones CV0 and CV1."
  },
  {
    "objectID": "adv_class/02OLS.html#section-5",
    "href": "adv_class/02OLS.html#section-5",
    "title": "Linear Regression Model",
    "section": "",
    "text": "Recall we approximate \\(\\sigma^2_i\\) with \\(\\varepsilon_i^2\\). Then we can approximate \\(\\sigma_{ij}\\) with \\(\\varepsilon_j \\varepsilon_i\\). More specifically:\n\n\\[\n\\Omega_g \\simeq \\varepsilon \\varepsilon' \\ or \\ \\Sigma_g = X'_g \\varepsilon \\varepsilon' X_g = (X'_g \\varepsilon) (\\varepsilon' X_g)\n\\]\n\nChange \\(\\varepsilon\\) with \\(\\hat\\varepsilon\\), do that for every group, and done! (almost)."
  },
  {
    "objectID": "adv_class/02OLS.html#section-6",
    "href": "adv_class/02OLS.html#section-6",
    "title": "Linear Regression Model",
    "section": "",
    "text": "As mentioned earlier, there are many CCSE (clustered consistent SE).\n\n\\[\n\\begin{aligned}\nCV_0 &= (X'X)^{-1} \\sum_{g=1}^G \\hat \\Sigma_g (X'X)^{-1} \\\\\nCV_1 &= \\frac{G(N-1)}{(G-1)(N-k-1)}(X'X)^{-1} \\sum_{g=1}^G \\hat \\Sigma_g (X'X)^{-1}\n\\end{aligned}\n\\]\n\nSimilar to HC. CV0 does not correct for degrees of freedom. CV1, however, accounts for Degrees of freedom in the model, and clusters.\n\n\n\nCode\nsort isco\nmata:\n    // 1st Sort Data (easier in Stata rather than Mata) and reload\n    y   = st_data(.,\"lnwage\")\n    x   = st_data(.,\"educ exper female\"),J(1434,1,1) \n    cvar= st_data(.,\"isco\")\n    ixx = invsym(cross(x,x)); xy = cross(x,y)\n    b   = ixx * xy\n    e   = y:-x*b\n    // Set the panel info\n    info = panelsetup(cvar,1); g=rows(info); n=rows(y)\n    // get X_g'e for all groups: \n    s_xg_e = panelsum(x:*e,info)\n    // Sum Sigma_g\n    sigma_g = s_xg_e's_xg_e\n    cv0 = ixx*sigma_g*ixx\n    cv1 =g/(g-1)*(n-1)/(n-k)*ixx*sigma_g*ixx\n    b,sqrt(diagonal(cv0)),sqrt(diagonal(cv1))\nend    \n\n\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     y = st_data(.,\"lnwage\")\n\n:     x = st_data(.,\"educ exper female\"),J(1434,1,1)\n\n:     cvar= st_data(.,\"isco\")\n\n:     ixx = invsym(cross(x,x)); xy = cross(x,y)\n\n:     b = ixx * xy\n\n:     e = y:-x*b\n\n:     info = panelsetup(cvar,1); g=rows(info); n=rows(y)\n\n:     s_xg_e = panelsum(x:*e,info)\n\n:     sigma_g = s_xg_e's_xg_e\n\n:     cv0 = ixx*sigma_g*ixx\n\n:     cv1 =g/(g-1)*(n-1)/(n-k)*ixx*sigma_g*ixx\n\n:     b,sqrt(diagonal(cv0)),sqrt(diagonal(cv1))\n                  1              2              3\n    +----------------------------------------------+\n  1 |   .0858251775    .0140570765    .0149254126  |\n  2 |   .0147342796    .0014534593    .0015432426  |\n  3 |  -.0949227416    .0525121234    .0557559112  |\n  4 |   2.218849962    .1947497649    .2067798804  |\n    +----------------------------------------------+\n\n: end\n-------------------------------------------------------------------------------\n\n. \n\n\nor compare it to\n\n\nCode\nreg lnwage educ exper female, cluster(isco)\n\n\n\nLinear regression                               Number of obs     =      1,434\n                                                F(3, 8)           =      59.13\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.2217\n                                                Root MSE          =     .46897\n\n                                   (Std. err. adjusted for 9 clusters in isco)\n------------------------------------------------------------------------------\n             |               Robust\n      lnwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0858252   .0149254     5.75   0.000     .0514071    .1202432\n       exper |   .0147343   .0015432     9.55   0.000     .0111756     .018293\n      female |  -.0949227   .0557559    -1.70   0.127    -.2234961    .0336506\n       _cons |    2.21885   .2067799    10.73   0.000     1.742015    2.695685\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/02OLS.html#visualizing-the-difference",
    "href": "adv_class/02OLS.html#visualizing-the-difference",
    "title": "Linear Regression Model",
    "section": "Visualizing the difference",
    "text": "Visualizing the difference\n\n\nCode\nclear\nset scheme white2\ncolor_style tableau\nset seed 1\nset obs 50\ngen r1=runiformint(1,4)\ngen r2=runiformint(1,4)\ngen id=_n\nsort r1  r2\nqui:mata:\nr1=st_data(.,\"r1\")\nr2=st_data(.,\"r2\")\nrr1=J(rows(r1)*rows(r2),4,0)\nk=0\nfor(i=1;i&lt;=50;i++){\n    for(j=1;j&lt;=50;j++){\n        if ((r1[i]==r1[j]) | (r2[i]==r2[j])) {\n            k++\n            rr1[k,]=(51-i,j,(r1[i]==r1[j]),(r2[i]==r2[j]) )         \n        }\n    }   \n}\nrr1=rr1[1..k,]\nend\ngetmata rr1*=rr1, replace force\n\ntwo (scatter rr11 rr12 if rr13==1,  ms(s) msize(2.1))  ///\n    (scatter rr11 rr12 if 51-rr11 == rr12, ms(s) msize(2.1) color(gs1)  ) ///\n    , aspect(1) legend(off)  xtitle(\"\") ytitle(\"\") yscale(off) xscale(off) xsize(6) ysize(6)"
  },
  {
    "objectID": "adv_class/02OLS.html#visualizing-multi-way-clustering",
    "href": "adv_class/02OLS.html#visualizing-multi-way-clustering",
    "title": "Linear Regression Model",
    "section": "Visualizing Multi-way Clustering",
    "text": "Visualizing Multi-way Clustering\n\nFirst ClusterSecond ClusterCombining Clusters\n\n\n\n\nCode\ntwo (scatter rr11 rr12 if rr13==1,  ms(s) msize(2.1))  ///\n    , aspect(1) legend(off)  xtitle(\"\") ytitle(\"\") yscale(off) xscale(off) name(m1, replace) \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntwo (scatter rr11 rr12 if rr14==1,  ms(s) msize(2.1))  ///\n    , aspect(1) legend(off)  xtitle(\"\") ytitle(\"\") yscale(off) xscale(off) name(m2, replace)    \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntwo (scatter rr11 rr12 if rr14==1 | rr13==1,  ms(s) msize(2.1))  ///\n    , aspect(1) legend(off)  xtitle(\"\") ytitle(\"\") yscale(off) xscale(off) name(m3, replace)"
  },
  {
    "objectID": "adv_class/02OLS.html#beware-of-over-clustering",
    "href": "adv_class/02OLS.html#beware-of-over-clustering",
    "title": "Linear Regression Model",
    "section": "Beware of over-clustering",
    "text": "Beware of over-clustering\nWhile clustering helps address a problem of “intragroup” correlation, it can/should be done with care. It is important to be aware about some unintended problems of over-clustering.\n\nCV0 and CV1 work well when you have a large number of Clusters. How many? MHE(2009) says…42 (this is like having large enough samples for Asymptotic variance). If # clusters are small, you would do better with other approaches (including CV2 and CV3).\nWhen you cluster your standard errors, you will “most-likely” generate larger standard errors in your model. Standard recommendation (MHE) is to cluster at the level that makes sense (based on data) and produces largest SE (to be conservative)."
  },
  {
    "objectID": "adv_class/02OLS.html#role-of-clusters",
    "href": "adv_class/02OLS.html#role-of-clusters",
    "title": "Linear Regression Model",
    "section": "Role of clusters",
    "text": "Role of clusters\n\nStandard Errors"
  },
  {
    "objectID": "adv_class/02OLS.html#section-7",
    "href": "adv_class/02OLS.html#section-7",
    "title": "Linear Regression Model",
    "section": "",
    "text": "You may also consider that clustering does not work well when sample sizes within cluster are to diverse (micro vs macro clusters)\nAnd there is the case where clustering is required among multiple dimensions (see vcemway). Where the unobserved correlation could be present in different dimensions."
  },
  {
    "objectID": "adv_class/02OLS.html#section-8",
    "href": "adv_class/02OLS.html#section-8",
    "title": "Linear Regression Model",
    "section": "",
    "text": "So what to cluster and how?\n\nMackinnon et al (2023) provides a guide on how and when to cluster your standard errors. (some are quite advanced)\nGeneral practice, At least use Robust SE (HC2 or HC3 if sample is small), but use clustered SE for robustness.\nYou may want to cluster SE based on some theoretical expectations. Choose -broader- groups for conservative analysis.\nIn treatment-causal effect analysis, you may want to cluster at the “treatment” level.\n\n\nBut…Beyond hc0/1 and CV0/1 there is not much out there for correcting Standard errors in nonlinear models."
  },
  {
    "objectID": "adv_class/02OLS.html#if-you-cant-sandwich-you-can-re-sample",
    "href": "adv_class/02OLS.html#if-you-cant-sandwich-you-can-re-sample",
    "title": "Linear Regression Model",
    "section": "If you can’t Sandwich 🥪, you can re-Sample",
    "text": "If you can’t Sandwich 🥪, you can re-Sample\n\nThe discussion above refered to the estimation of SE using \\(Math\\). In other words, it was based on the asymptotic properties of the data. Which may not work in small samples.\nAn alternative, often used by practitioners, is using re-sampling methods to obtain approximations to the coefficient distributions of interest.\n\nBut… How does it work?🤔\nFirst ask yourself, how does Asymptotic theory work (and econometrics)? 😱\n\nNote: I recommend reading the -simulation- chapter in The effect, and simulation methods chapter in CT."
  },
  {
    "objectID": "adv_class/02OLS.html#a-brief-reviewagain",
    "href": "adv_class/02OLS.html#a-brief-reviewagain",
    "title": "Linear Regression Model",
    "section": "A Brief Review…again 😇",
    "text": "A Brief Review…again 😇\nIf I were to summarize most of the methodologies (ok all) we used last semester, and this one, the properties that have been derived and proofed are based on the assumption that we “could” always get more data (frequentist approach).\nThere is population (or super population) from where we can get samples of data (and never repeat data).\n\nWe get a sample (\\(y,X\\)) (of size N)\nEstimate our model : method(\\(y,X\\))\\(\\rightarrow\\) \\(\\beta's\\)\nRepeat to infinitum\nCollect all \\(\\beta's\\) and summarize. (Mean and Standard deviations)\n\nDone.\nThe distributions you get from the above exercise should be the same as what your estimation method produces. (in average) (if not, there there is something wrong with the estimation method)"
  },
  {
    "objectID": "adv_class/02OLS.html#section-9",
    "href": "adv_class/02OLS.html#section-9",
    "title": "Linear Regression Model",
    "section": "",
    "text": "But we only get 1 Sample!\nThe truth is we do not have access to multiple samples. Getting more data, is in fact, very expensive. So what to do ?\n\nRely on Asymptotic theory\nlearn Bayesian Econometrics 🥺\nor-resample? and do Bootstrap!"
  },
  {
    "objectID": "adv_class/02OLS.html#section-10",
    "href": "adv_class/02OLS.html#section-10",
    "title": "Linear Regression Model",
    "section": "",
    "text": "Basic idea of Bootstrapping\n\nIn the ideal scenario, you get multiple samples from your population, Estimate parameters, and done.\nIf not possible you do the next best thing. You get your sample (assume is your mini-population),\n\nDraw subsamples of same size (with replacement) (\\(y_i^s,X_i^s\\))\nestimate your model and obtain parameters \\(\\beta^s_i\\)\nSummarize those parameters…and done, you get \\(Var(\\hat\\beta)\\) for 🆓. (or is it?)"
  },
  {
    "objectID": "adv_class/02OLS.html#section-11",
    "href": "adv_class/02OLS.html#section-11",
    "title": "Linear Regression Model",
    "section": "",
    "text": "Bootstrapping\n\n👢Bootstrapping is a methodology that allows you to obtain empirical estimations of standard errors making use of the data in hand, and without even knowing about Asymptotic theory (other than how to get means and variances).\n\n\nBootstrap Sample\nAnd of course, it comes in different flavors."
  },
  {
    "objectID": "adv_class/02OLS.html#section-12",
    "href": "adv_class/02OLS.html#section-12",
    "title": "Linear Regression Model",
    "section": "",
    "text": "Standard Bootstrap:\n\nNon-parametric Bootstrap: You draw subsamples from the main sample. Each observation has the same pr of being selected.\n\nEasiest to implement ( see bootstrap:)\nWorks in almost all cases, but you may have situations when some covariates are rare.\nCan allow for “clusters” using “block bootstrapping”."
  },
  {
    "objectID": "adv_class/02OLS.html#section-13",
    "href": "adv_class/02OLS.html#section-13",
    "title": "Linear Regression Model",
    "section": "",
    "text": "Standard Bootstrap:\n\nParametric Bootstrap: You estimate your model, make assumptions of your model error.\n\nYou need to implement it on your own. \\(y^s=x\\hat b+\\tilde e\\) for \\(\\tilde e \\sim f(\\hat \\theta)\\)\nIt will not work well if the assumptions of the error modeling are wrong."
  },
  {
    "objectID": "adv_class/02OLS.html#section-14",
    "href": "adv_class/02OLS.html#section-14",
    "title": "Linear Regression Model",
    "section": "",
    "text": "Standard Bootstrap:\n\nResidual bootstrap: Estimate your model, obtain residuals. Re-sample residuals\n\nAgain, implement it on your own. \\(y^s = x\\hat b+\\tilde e\\) for \\(\\tilde e \\sim {\\hat e_1 , ... , \\hat e_N}\\)\nIt depends even more on the assumptions of the error modeling."
  },
  {
    "objectID": "adv_class/02OLS.html#section-15",
    "href": "adv_class/02OLS.html#section-15",
    "title": "Linear Regression Model",
    "section": "",
    "text": "Wild Bootstrap\nThen there are the more advanced (but faster) Bootstrap methods: WildBootstrap\n\nUWild bootstrap: Estimate your model, obtain residuals, and re-sample residual weights.\n\nAgain…on your own: \\(y^s = x\\hat b +\\hat e * v\\) , where \\(v \\sim ff()\\) where \\(ff()\\) is a “good” distribution function. \\(E(v)=0 \\ \\& \\ Var(v)=1\\)\nRe-estimate the model and obtain \\(\\hat \\beta's\\). Repeat and summarize.\nActually quite flexible, and works well under heteroskedasticity!\nIt can also allow clustered standard errors. The error \\(v\\) no longer changes by individual, but by group. It also works well with weights."
  },
  {
    "objectID": "adv_class/02OLS.html#section-16",
    "href": "adv_class/02OLS.html#section-16",
    "title": "Linear Regression Model",
    "section": "",
    "text": "Wild Bootstrap:\n\nUWild bootstrap-2 : Estimate your model, obtain Influence functions 😱 , and re-sample residual weights.\n\nThis is an extension to the previous option. But with advantages\n\nyou do not need to re-estimate the model. Just look into how the the mean of IF’s change.\nit can be applied to linear and nonlinear model (if you know how to build the IF’s)\n\nWorks well with clustered and weights."
  },
  {
    "objectID": "adv_class/02OLS.html#section-17",
    "href": "adv_class/02OLS.html#section-17",
    "title": "Linear Regression Model",
    "section": "",
    "text": "Wild Bootstrap:\n\nCWild bootstrap: Similar UWild Bootstrap, Obtain Influence functions under the Null (imposing restrictions), and use that to test the NULL.\n\nNo, you do not need to do it on your own. see bootest in Stata.\nWorks pretty well with small samples and small # clusters. Probably the way to go if you really care about Standard errors."
  },
  {
    "objectID": "adv_class/02OLS.html#how-to-bootstrap-in-stata",
    "href": "adv_class/02OLS.html#how-to-bootstrap-in-stata",
    "title": "Linear Regression Model",
    "section": "How to Bootstrap? in Stata",
    "text": "How to Bootstrap? in Stata\nI have a few notes on Bootstrapping here Bootstrapping in Stata. But let me give you the highlights for the most general case.\n\nMost (if not all commands) in Stata allow you to obtain bootstrap standard errors, by default. see:help [cmd]\nthey usually have the following syntax:\n[cmd] y x1 x2 x3, vce(bootstrap, options)\nregress lnwage educ exper female, vce(bootstrap, reps(100))\nHowever, you can also Bootstrap that commands that do not have their own bootstrap option.\nbootstrap:[cmd] y x1 x2 x3, \nbootstrap, reps(100):regress lnwage educ exper female\nbootstrap, reps(100) cluster(isco):regress lnwage educ exper female"
  },
  {
    "objectID": "adv_class/02OLS.html#section-18",
    "href": "adv_class/02OLS.html#section-18",
    "title": "Linear Regression Model",
    "section": "",
    "text": "This last command may allow you to bootstrap multiple models at the same time, although it does require a bit of programming. (and a do file)\n\n\n\nCode\nfrause oaxaca, clear\ngen tchild = kids6 + kids714\ncapture program drop bs_wages_children\nprogram bs_wages_children, eclass // eclass is for things like equations\n    ** Estimate first model\n    reg lnwage educ exper female\n    matrix b1 = e(b)\n    matrix coleq b1 = lnwage\n    ** Estimate second model\n    reg tchild educ exper female\n    matrix b2 = e(b)\n    matrix coleq b2 = tchild\n    ** Put things together and post\n    matrix b = b1 , b2\n    ereturn post b\nend\nbootstrap: bs_wages_children\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n(running bs_wages_children on estimation sample)\n\nwarning: bs_wages_children does not set e(sample), so no observations will be\n         excluded from the resampling because of missing values or other\n         reasons. To exclude observations, press Break, save the data, drop\n         any observations that are to be excluded, and rerun bootstrap.\n\nBootstrap replications (50): .........10.........20.........30.........40......\n&gt; ...50 done\n\nBootstrap results                                        Number of obs = 1,647\n                                                         Replications  =    50\n\n------------------------------------------------------------------------------\n             |   Observed   Bootstrap                         Normal-based\n             | coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nlnwage       |\n        educ |   .0858252   .0062606    13.71   0.000     .0735547    .0980957\n       exper |   .0147343    .001283    11.48   0.000     .0122196     .017249\n      female |  -.0949227   .0305222    -3.11   0.002    -.1547452   -.0351003\n       _cons |    2.21885   .0855954    25.92   0.000     2.051086    2.386614\n-------------+----------------------------------------------------------------\ntchild       |\n        educ |   .0177854   .0087606     2.03   0.042      .000615    .0349558\n       exper |  -.0047747   .0017462    -2.73   0.006    -.0081972   -.0013522\n      female |  -.1306332   .0395711    -3.30   0.001    -.2081911   -.0530753\n       _cons |   .4163459   .1201824     3.46   0.001     .1807927    .6518991\n------------------------------------------------------------------------------\n\n\nWhy does it matter? because you may want to test coefficients individually, or across models. This is only possible if the FULL system is estimated jointly"
  },
  {
    "objectID": "adv_class/02OLS.html#section-19",
    "href": "adv_class/02OLS.html#section-19",
    "title": "Linear Regression Model",
    "section": "",
    "text": "What about Wild Bootstrap?\n\nWildbootstrap is available using boottest (ssc install bootest)\nAnd in Stata18+, you have wildbootstrap (although is meant for clustered SE)\n\n\n\nCode\nfrause oaxaca, clear\nregress lnwage educ exper female, robust\nboottest educ, nograph\nboottest exper, nograph\nboottest female, nograph\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\nLinear regression                               Number of obs     =      1,434\n                                                F(3, 1430)        =      97.11\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.2217\n                                                Root MSE          =     .46897\n\n------------------------------------------------------------------------------\n             |               Robust\n      lnwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0858252   .0060342    14.22   0.000     .0739883     .097662\n       exper |   .0147343    .001354    10.88   0.000     .0120783    .0173903\n      female |  -.0949227   .0254309    -3.73   0.000    -.1448086   -.0450369\n       _cons |    2.21885   .0830438    26.72   0.000     2.055949    2.381751\n------------------------------------------------------------------------------\n\nWild bootstrap-t, null imposed, 999 replications, Wald test, Rademacher weights\n&gt; :\n  educ\n\n                         t(1430) =    14.2231\n                        Prob&gt;|t| =     0.0000\n\n95% confidence set for null hypothesis expression: [.07399, .09766]\n\nWild bootstrap-t, null imposed, 999 replications, Wald test, Rademacher weights\n&gt; :\n  exper\n\n                         t(1430) =    10.8822\n                        Prob&gt;|t| =     0.0000\n\n95% confidence set for null hypothesis expression: [.01208, .01739]\n\nWild bootstrap-t, null imposed, 999 replications, Wald test, Rademacher weights\n&gt; :\n  female\n\n                         t(1430) =    -3.7326\n                        Prob&gt;|t| =     0.0000\n\n95% confidence set for null hypothesis expression: [−.1433, −.04703]"
  },
  {
    "objectID": "adv_class/02OLS.html#final-words-on-bootstrap",
    "href": "adv_class/02OLS.html#final-words-on-bootstrap",
    "title": "Linear Regression Model",
    "section": "Final words on Bootstrap:",
    "text": "Final words on Bootstrap:\nSo bootstrap (and its many flavors) are convenient approaches to estimate standard errors and elaborate statistical Inference, but its not infallible.\n\nIf the re-sampling process does not simulate the true sampling design, we may miss important information when constructing SE.\nWhen the parameters are estimated using “hard” cutoffs or restricted distributions, it may not produce good approximations for SE.\nYou usually require MANY repetitions (standard = 50, but you probably want 999 or more). The more the better, but has some computational costs. (specially simple bs)\nSome methods play better with weighted samples, clusters, and other survey designs than others. And some require more know-how than others.\n\nSo choose your 🔫weapon wisely!"
  },
  {
    "objectID": "adv_class/02OLS.html#variance-of-nonlinear-functions",
    "href": "adv_class/02OLS.html#variance-of-nonlinear-functions",
    "title": "Linear Regression Model",
    "section": "Variance of nonlinear functions",
    "text": "Variance of nonlinear functions\n\n\nSome times (perhaps not with simple OLS) you many need to estimate Standard errors for transformations of your main coefficient of interest, or combinations of those coefficients.\nSay that you estimated \\(\\theta \\sim N(\\mu_\\theta, \\sigma^2_\\theta)\\) but are interested in the distribution of \\(g(\\theta)\\). How do you do this?\nTwo options:\n\nyou re estimate \\(g(\\theta\\)) instead, or\nyou make an approximation, using the Delta Method\n\nHow does it work?"
  },
  {
    "objectID": "adv_class/02OLS.html#section-20",
    "href": "adv_class/02OLS.html#section-20",
    "title": "Linear Regression Model",
    "section": "",
    "text": "The Delta method uses the linear approximations to approximate the otherwise not known distributions.\nFurther, It relies on the fact that linear transformations a normal distribution, is on itself normal. For example:\n\n\\[\ng(\\hat \\theta) \\simeq g(\\theta) + g'(\\hat\\theta) (\\hat \\theta-\\theta)\n\\]\n\nThis states that the nonlinear function \\(g(\\theta)\\) can be “locally” approximated as a linear function in the neighborhood of \\(g(\\theta)\\).\nPredictions above or below are approximated using the slope of the function. \\(g'(\\theta)\\).\nSo, if we take the variance, we get:\n\n\\[\nVar(g(\\hat \\theta)) \\simeq  Var \\left(g(\\theta)+ g'(\\hat\\theta) (\\hat \\theta-\\theta)\\right)\n=g'(\\hat\\theta)^2 Var(\\theta)\n\\]"
  },
  {
    "objectID": "adv_class/02OLS.html#delta-method-visualization",
    "href": "adv_class/02OLS.html#delta-method-visualization",
    "title": "Linear Regression Model",
    "section": "Delta Method: Visualization",
    "text": "Delta Method: Visualization"
  },
  {
    "objectID": "adv_class/02OLS.html#section-21",
    "href": "adv_class/02OLS.html#section-21",
    "title": "Linear Regression Model",
    "section": "",
    "text": "It can go multivariate as well:\n\\[\n\\begin{aligned}\ng(\\hat \\theta, \\hat \\gamma)-g(\\theta,\\gamma) &\\simeq N(0,\\nabla g ' \\Sigma \\nabla g) \\\\\n\\nabla g ' &=   [\\begin{matrix}\n    dg/d\\theta & dg/d\\gamma\n  \\end{matrix}]\n\\end{aligned}  \n\\]\nAlthough you need to get the partial derivatives of \\(g(\\theta,\\gamma)\\)"
  },
  {
    "objectID": "adv_class/02OLS.html#section-22",
    "href": "adv_class/02OLS.html#section-22",
    "title": "Linear Regression Model",
    "section": "",
    "text": "Example\n\nSay that you obtain the mean standard error for averages wages for men and women, along with the correlation between the two.\nhowever, you are insterested in estimating the wage ratio, and its variance. How do you do this?\nNeed to obtain the Gradients \\(g\\)\n\n\\[\nR = \\frac{ \\mu_f}{\\mu_m};\ng = \\begin{bmatrix}\n    \\frac{ \\partial R}{\\partial \\mu_f} \\\\\n    \\frac{ \\partial R}{\\partial \\mu_m}\n    \\end{bmatrix} =\n\\begin{bmatrix}    \n\\frac{1}{\\mu_m} \\\\  -\\frac{\\mu_f}{\\mu_m^2}\n\\end{bmatrix}\n\\]\nThen the variance of \\(R\\) is:\n\\[Var(R) = g' \\Sigma_\\mu g\\]"
  },
  {
    "objectID": "adv_class/02OLS.html#section-23",
    "href": "adv_class/02OLS.html#section-23",
    "title": "Linear Regression Model",
    "section": "",
    "text": "Example in Stata\n\n\nCode\nfrause oaxaca, clear\ngen wage = exp(lnwage)\nmean wage, over(female)\nmata:\n  mu = st_matrix(\"e(b)\")\n  vcv = st_matrix(\"e(V)\")\n  dg = 1/mu[2] \\ -mu[1]/mu[2]^2\n  var_r = dg'*vcv*dg\n  sqrt(var_r)\nend\nnlcom _b[ c.wage@0.female]/_b[ c.wage@1.female] \n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 missing values generated)\n\nMean estimation                           Number of obs = 1,434\n\n---------------------------------------------------------------\n              |       Mean   Std. err.     [95% conf. interval]\n--------------+------------------------------------------------\nc.wage@female |\n           0  |   34.33619   .5175882      33.32088    35.35151\n           1  |   30.25354   .6805642      28.91853    31.58855\n---------------------------------------------------------------\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:   mu = st_matrix(\"e(b)\")\n\n:   vcv = st_matrix(\"e(V)\")\n\n:   dg = 1/mu[2] \\ -mu[1]/mu[2]^2\n\n:   var_r = dg'*vcv*dg\n\n:   sqrt(var_r)\n  .0307332119\n\n: end\n-------------------------------------------------------------------------------\n\n. \n\n       _nl_1: _b[ c.wage@0.female]/_b[ c.wage@1.female]\n\n------------------------------------------------------------------------------\n        Mean | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _nl_1 |   1.134948   .0307332    36.93   0.000     1.074712    1.195184\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/02OLS.html#so-why-do-we-care",
    "href": "adv_class/02OLS.html#so-why-do-we-care",
    "title": "Linear Regression Model",
    "section": "So why do we care:",
    "text": "So why do we care:\nTwo reasons:\n\nNonlinear models need this kind of approximations to do statistical inference (probit/logit)\nRecall that when using Robust Standard errors Joint hypothesis Should be done with Care…\n\nConsider a linear set of restrictions imposed by the \\(H_0: R\\beta = r\\).\n\nEstimate the Variance of \\(R\\beta\\)\n\n\\[\nVar(R\\beta)  = \\nabla (R\\beta)' Var(\\beta) R \\nabla (R\\beta)'= R' Var(\\beta) R\n\\]\n\nEstimate the F value for the Linear Hypothesis (Wald Test)\n\n\\[\n(R\\hat \\beta-r)' Var(R\\beta)^{-1} (R\\hat \\beta-r)/Q \\sim F(Q,N-K)\n\\]"
  },
  {
    "objectID": "adv_class/02OLS.html#what-happens-when-k-is-too-big",
    "href": "adv_class/02OLS.html#what-happens-when-k-is-too-big",
    "title": "Linear Regression Model",
    "section": "What happens when K is too big?",
    "text": "What happens when K is too big?\n\nHow many variables (max) can you use in a model?\n\n\\[max \\ k = rank(X'X)\\]\n\nWhat happens when you add too many variables in a model?\n\nIncrease Multicolinearity and coefficient variance (too much noise)\nR2 overly large (without explaining much)\nFar more difficult to interpret (too many factors)\nMay introduce endogeneity (when it wasnt a problem before)\n\nHow can you solve the problem?\n\nYou select only a few of the variables, based on theory, and contribution to the model\n\nWhat if you can’t choose?"
  },
  {
    "objectID": "adv_class/02OLS.html#ml-we-let-the-choose-for-you",
    "href": "adv_class/02OLS.html#ml-we-let-the-choose-for-you",
    "title": "Linear Regression Model",
    "section": "ML: We let the 💻Choose for you",
    "text": "ML: We let the 💻Choose for you\n\nBefore we start. The methodology we will discuss are usually meant to get models with “good” predictive power, and some times better interpretability, not so much stat-inference (although its possible)\n\nWhen you do not know how to choose, you could try select a subset of variables from your model such that you maximize out-of-sample predictive power\nThis is typically achieved using the following:\n\\[\nAR^2 = 1-\\frac{SSR}{SST}\\frac{n-1}{n-k-1} \\\\\nAIC = n^{-1}(SSR + 2k\\hat\\sigma^2) \\\\\nBIC = n^{-1}(SSR + ln(n) k\\hat\\sigma^2)\n\\]\nOr using a method known as cross-validation (Comparing predictive power using data not used for model estimation)\nHowever, we can always try to estimate a model with all variables!"
  },
  {
    "objectID": "adv_class/02OLS.html#ridge-and-lasso-and-elasticnet",
    "href": "adv_class/02OLS.html#ridge-and-lasso-and-elasticnet",
    "title": "Linear Regression Model",
    "section": "Ridge and Lasso and ElasticNet",
    "text": "Ridge and Lasso and ElasticNet\n\nRecall that when using OLS to obtain \\(\\beta's\\), we try to minimize the following:\n\n\\[\nSSR = \\sum_i(y_i - X_i \\beta)^2\n\\]\n\nThis has the restrictions of mentioned before (\\(k &lt; N\\)). In addition to letting coefficents vary “too much”\nAn alternative is to Impose additional restrictions so that coefficients do not vary as much. This is known as Regularization."
  },
  {
    "objectID": "adv_class/02OLS.html#section-24",
    "href": "adv_class/02OLS.html#section-24",
    "title": "Linear Regression Model",
    "section": "",
    "text": "Ridge Regression\n\nOne such approach is Ridge regression, which minimizes the following:\n\n\\[\nrSS = \\sum_i(y_i - X_i \\beta)^2+ \\lambda \\sum_{k=1}^K\\beta_k^2\n\\]\n\nThis essentially aims to find parameters that reduces SSR, but also “controls” for how large \\(\\beta's\\) can be, using a shrinkage penalty that depends on \\(\\lambda\\).\nIf \\(\\lambda = 0\\) you get Standard OLS, and if \\(\\lambda \\rightarrow \\infty\\) , you get a situation where all betas (but the constant) are zero. For intermediate values, you may have better models than OLS, because you can balance Bias (when \\(\\beta's\\) are zero) with increase variance (when all \\(\\beta's\\) vary as they “please”)"
  },
  {
    "objectID": "adv_class/02OLS.html#section-25",
    "href": "adv_class/02OLS.html#section-25",
    "title": "Linear Regression Model",
    "section": "",
    "text": "We usually start with Ridge, because is relatively Easy to implement, since it has a close form Solution:\n\n\\[\n\\beta = (X'X + \\lambda I)^{-1}{X'y}\n\\]\n\n\nCode\nset linesize 255\nfrause oaxaca, clear\nkeep if lnwage!=.\ngen male = 1-female\nmata:\n    y = st_data(.,\"lnwage\")\n    x = st_data(.,\"educ exper female male\")\n    // Standardization. Need men and SD\n    mn_x = mean(x)\n    sd_x = diagonal(sqrt(variance(x)))'\n    // Centering and addinc constant\n    x = (x:-mn_x):/sd_x; x = x,J(1434,1,1)\n    i0 = I(5);i0[5,5]=0\n    // SD errors as Column, including a 1 for Constant\n    sd_x=sd_x'\\1\n    xx = (cross(x,x)) ; xy = (cross(x,y))\n    bb0 = invsym(xx)*xy \n    bb1 = invsym(xx:+i0*1)*xy \n    bb10 = invsym(xx:+i0*10)*xy \n    bb100 = invsym(xx:+i0*100)*xy \n    bb1000 = invsym(xx:+i0*1000)*xy \n    bb10000 = invsym(xx:+i0*10000)*xy\n    bb100000 = invsym(xx:+i0*100000)*xy\n    // \n    bb0:/sd_x,bb1:/sd_x,bb10:/sd_x,bb100:/sd_x,bb1000:/sd_x,bb10000:/sd_x, bb100000:/sd_x\nend \n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n\n. mata:\n------------------------------------------------- mata (type end to exit) -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n:     y = st_data(.,\"lnwage\")\n\n:     x = st_data(.,\"educ exper female male\")\n\n:     mn_x = mean(x)\n\n:     sd_x = diagonal(sqrt(variance(x)))'\n\n:     x = (x:-mn_x):/sd_x; x = x,J(1434,1,1)\n\n:     i0 = I(5);i0[5,5]=0\n\n:     sd_x=sd_x'\\1\n\n:     xx = (cross(x,x)) ; xy = (cross(x,y))\n\n:     bb0 = invsym(xx)*xy\n\n:     bb1 = invsym(xx:+i0*1)*xy\n\n:     bb10 = invsym(xx:+i0*10)*xy\n\n:     bb100 = invsym(xx:+i0*100)*xy\n\n:     bb1000 = invsym(xx:+i0*1000)*xy\n\n:     bb10000 = invsym(xx:+i0*10000)*xy\n\n:     bb100000 = invsym(xx:+i0*100000)*xy\n\n:     bb0:/sd_x,bb1:/sd_x,bb10:/sd_x,bb100:/sd_x,bb1000:/sd_x,bb10000:/sd_x, bb100000:/sd_x\n                  1              2              3              4              5              6              7\n    +----------------------------------------------------------------------------------------------------------+\n  1 |   .0858251775    .0857575755    .0851540495    .0795662808     .048317346    .0100245225    .0011301355  |\n  2 |   .0147342796    .0147216463     .014608921    .0135704573    .0079519909    .0015717189    .0001752401  |\n  3 |  -.0949227416   -.0474767326   -.0476121478   -.0487120452   -.0481537918   -.0183303744   -.0024025378  |\n  4 |             0    .0474767326    .0476121478    .0487120452    .0481537918    .0183303744    .0024025378  |\n  5 |   3.357604356    3.357604356    3.357604356    3.357604356    3.357604356    3.357604356    3.357604356  |\n    +----------------------------------------------------------------------------------------------------------+\n\n: end\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n."
  },
  {
    "objectID": "adv_class/02OLS.html#lasso-and-elastic-net",
    "href": "adv_class/02OLS.html#lasso-and-elastic-net",
    "title": "Linear Regression Model",
    "section": "Lasso and Elastic Net",
    "text": "Lasso and Elastic Net\n\nRidge is a relatively easy model to understand and estimate, since it has a close form solution. It has the slight disadvantage that you still estimate a coefficient for “every” variable (tho some are very small)\nAnother approach, that overcomes this advantage is known as Lasso.\n\n\\[\nLSS = \\sum_i(y_i - X_i \\beta)^2+ \\lambda \\sum_{k=1}^K |\\beta_k|\n\\]\n\nand the one known as Elastic net\n\n\\[\neSS = \\sum_i(y_i - X_i \\beta)^2+ \\lambda_L \\sum_{k=1}^K |\\beta_k| +\n\\lambda_r \\sum_{k=1}^K \\beta_k^2\n\\]"
  },
  {
    "objectID": "adv_class/02OLS.html#lasso-vs-ridge",
    "href": "adv_class/02OLS.html#lasso-vs-ridge",
    "title": "Linear Regression Model",
    "section": "Lasso vs Ridge",
    "text": "Lasso vs Ridge"
  },
  {
    "objectID": "adv_class/02OLS.html#considerations",
    "href": "adv_class/02OLS.html#considerations",
    "title": "Linear Regression Model",
    "section": "Considerations:",
    "text": "Considerations:\nAs with many methodologies, the benefits from this approaches is not free.\n\nYou need to choose tuning parameters “wisely” using approaches such as AIC, BIC, or cross validation.\nThe model you get may improve prediction, but inference is not as straight forward.\nIt also requires working with Standardized coefficients. (so the same penalty can be used for all variables in the model.\n\nNevertheless, they can be used as starting point for model selection.\nif interested, look into Stata introduction to Lasso regression. help Lasso intro"
  },
  {
    "objectID": "adv_class/02OLS.html#brief-example",
    "href": "adv_class/02OLS.html#brief-example",
    "title": "Linear Regression Model",
    "section": "Brief Example:",
    "text": "Brief Example:\n\n\nCode\nqui {\nfrause oaxaca, clear\nkeep if lnwage!=.\nqui:reg lnwage i.age\npredict p_ols\nqui:elasticnet linear lnwage i.age, selection(cv, alllambdas)  alpha(0)\npredict p_ridge\nqui:lasso linear lnwage i.age, selection(cv, alllambdas)  \npredict p_lasso\nqui:elasticnet linear lnwage i.age, selection(cv, alllambdas)   \npredict p_elastic\n}"
  },
  {
    "objectID": "adv_class/02OLS.html#shrinking-coefficients",
    "href": "adv_class/02OLS.html#shrinking-coefficients",
    "title": "Linear Regression Model",
    "section": "Shrinking Coefficients",
    "text": "Shrinking Coefficients\n\nLasso vs Ridge"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site was created with the only purpose of sharing the class slides with all Students.\nIt will also contain the homeworks and class assigments."
  },
  {
    "objectID": "adv_class/01Introduction.html#introduction",
    "href": "adv_class/01Introduction.html#introduction",
    "title": "Introduction",
    "section": "Introduction",
    "text": "Introduction\nFirst of all, thank you for joining me this semester, we I expect we all will earn something new.\nWhy? well, while I have learned and implemented many of the methodologies we will see here today, there are a few I have yet to dive into.\nNevertheless, I hope you will enjoy, and learn as much as you can from this course, which has the purpose of:\n\nExpose you to a large set of empirical econometric analysis techniques\nExpose you to the application of some of this techniques to the analysis of Causal effects\n\nBut first, lets lay out the Rules of the game…"
  },
  {
    "objectID": "adv_class/01Introduction.html#grades",
    "href": "adv_class/01Introduction.html#grades",
    "title": "Introduction",
    "section": "Grades",
    "text": "Grades\nYour grade will consist on 4 elements\n\nParticipation 10%: As before, active participation is encourage, so reading the material before class is highly recommended.\nPresentations 15%: Students will have two presentations during the semester (second half), based on suggested material (Syllabus) or other papers the students may be interested in.\nThe main requirement. The paper should implement any of the methodologies we will be covering in class.\nThe presentation should emphasize the Research question, assumptions used, methodology, and results. If possible also provide criticism to the paper.\nHomework 25%: Homeworks will be provided for you to practice and implement the different methodologies discussed in class. They can be carried out individually or in groups (of 2). This will include making a brief description of the results."
  },
  {
    "objectID": "adv_class/01Introduction.html#grades-1",
    "href": "adv_class/01Introduction.html#grades-1",
    "title": "Introduction",
    "section": "Grades",
    "text": "Grades\n\nPaper Project 50:\n\n\nWrite a term paper that can be of two types:\n\n\nPaper Replication: You can choose to write a replication paper on a methodological paper, or applied empirical paper.\nIn either case, the replication will have to extend the analysis of the original paper to a different setup (empirical paper), different software (if replication paper), or other extensions to the original analysis/methodology.\nResearch: A 20-25 pages paper where students answer a research question of their choice, using any of the methodologies presented in class. Standard structure of the paper applies.\n\n\nPresentation of the paper in class"
  },
  {
    "objectID": "adv_class/01Introduction.html#course-content",
    "href": "adv_class/01Introduction.html#course-content",
    "title": "Introduction",
    "section": "Course Content",
    "text": "Course Content\nThe course content will consist of two parts:\n\nPart I: I will review and cover many of empirical methodologies that expand on the Linear Regression analysis we cover in Econometrics 1. This include:\n\nLinear Regression: OLS (again), but with SE emphasis, and allowing for (too)many variables.\nSemi- and non-parametric regressions: When you need things to be Flexible (but not too flexible)\nCQuantile regressions: When you are interested in people beyond the mean (distributions)\nUQuantile-Regressions, RIF-Regressions: When you are interested in the whole distribution\nNonlinear Models: MLE and GMM: When your models are nonlinear (in coefficients)"
  },
  {
    "objectID": "adv_class/01Introduction.html#course-content-1",
    "href": "adv_class/01Introduction.html#course-content-1",
    "title": "Introduction",
    "section": "Course Content",
    "text": "Course Content\n\n\n\n\n\\[MLE: Y|x \\sim f(\\theta)\\] \\[GMM: E(y-m(x))= 0\\]"
  },
  {
    "objectID": "adv_class/01Introduction.html#course-content-2",
    "href": "adv_class/01Introduction.html#course-content-2",
    "title": "Introduction",
    "section": "Course Content",
    "text": "Course Content\nThe second part of the course aims to introduce Methodologies that have the goal of identifying Causal effects.\nWhat do we mean with that? We specifically focus on cases when:\n\nA change in T(reatment) (\\(0 \\rightarrow 1\\)) has an impact in Y from \\(y(0) \\rightarrow y(1)\\), because we manage use a design that makes everything else (\\(X's \\ \\& \\ \\varepsilon's\\)) constant.\n\n\nParallel WorldsIdeally you want to observe the same unit under two different Status! (Multiverse!)"
  },
  {
    "objectID": "adv_class/01Introduction.html#course-content-3",
    "href": "adv_class/01Introduction.html#course-content-3",
    "title": "Introduction",
    "section": "Course Content",
    "text": "Course Content\nHowever, as we have discussed last semester, achieving this is hard. There are many factors that we may not be able to control. Thus we need to come-up with “cleaver” strategies to achieve something Similar.\n\nRandomization: When Treatment is generated at “random”. You can’t see All worlds, but its the closest.\nPanel Data and Fixed Effects: Some things are fixed, and you may be able to get “rid” of them if you see them often: Panel data, family effect, twins, etc.\nInstrumental Variables: Searching for Exogenous Variation to “simulate” random assignment. Even tho it may only capture “local” effects\nMatching and Re-weighting: If true twins do not exist, find people who are “observational twins”. Same life, same characteristics, different treatment and outcome. (or instead of people, distributions)"
  },
  {
    "objectID": "adv_class/01Introduction.html#course-content-4",
    "href": "adv_class/01Introduction.html#course-content-4",
    "title": "Introduction",
    "section": "Course Content",
    "text": "Course Content\n\nRDD-Regression Discontinuity Design: Use Jumps, and assignment rules. Use the fact that some times treatment is assigned around a threshold.\nDifferences in Differences: Use changes across groups and time. DiD: Accounts for group differences and trends (but be aware of TWFE - when we absorb too much)\nSynthetic Control: Similar to Matching, you can create “synthetic” units, combining the information of multiple units. at the same time. And like DID, you can use that to identify effects."
  },
  {
    "objectID": "adv_class/01Introduction.html#reading",
    "href": "adv_class/01Introduction.html#reading",
    "title": "Introduction",
    "section": "Reading",
    "text": "Reading\n\nI have assigned many readings! But you do not need to read them all (but, you may benefit from it).\nAt the very least, read one paper/chapter of the assigned readings.\nThe main books: Casual Inference:The mixtape, The Effect, Mostly Harmless Econometrics, are all available online.\nOtherwise, I ll provide the corresponding -pdfs- on the class website.\n\n\n\n\nte"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#introduction",
    "href": "adv_class/03par_spar_npar.html#introduction",
    "title": "Semi- and Non- Parametric regression",
    "section": "Introduction",
    "text": "Introduction\nWhat exactly do we mean with non parametric??\n\nFirst of all, everything we have done in the last class, concerned to the analysis of parametric relationships between \\(y\\) and \\(X's\\) .\nWhy parametric? Because we assume that the relationship between those variables is linear, so we just need to estimate the parameters of that relationship. (\\(\\beta's\\)). Even tho the CEF was on itself non-parametric.\nThis was just a matter of convince. Instead of trying to estimate all possible conditional means (impossible task?) we impose functional form conditions, to identify the relationship of interest.\nWhen we covered MLE (last semester) we even imposed functional forms assumptions on relationships and distribution!"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section",
    "href": "adv_class/03par_spar_npar.html#section",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "So what about non-parametric?\n\nNon-parametric is on the other side of the spectrum. There are no “single” parameters to estimate, but rather it tries to be as flexible as possible, to identify all possible relationships in the data.\nIn terms of distributions, it may no longer assumes data distributes as normal, poisson, exponential, etc. Instead, it simply assumes it distributes, however it does. 🤔 But isnt that a problem?\nYes it can be.\n\nOn the one hand Parametric modeling is very “strict” regarding functional forms. (linear quadratic, logs, etc).\nOn the other, Non-parametric can be too flexible. Making the problem almost impossible to solve.\n\nHowever, the benefits of letting your data “speak” for itself, would allow you to avoid some problems with parametric models."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-1",
    "href": "adv_class/03par_spar_npar.html#section-1",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "Ok but what about Semi-parametric!\n\nSemi-parametric models try to establish a mid point between parametric and non-parametric models, attempting to draw from the benefit of both.\n\nIt also helps that it has a smaller computational burden (we will see what do I mean with this).\n\nWhat about an example? Say we are trying to explain “wages” as a function of age and education. (assume exogeneity)"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-2",
    "href": "adv_class/03par_spar_npar.html#section-2",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "\\[\\begin{aligned}\n\\text{Theoretical framework :}wage &= g(age, education, \\varepsilon) \\\\ \\\\\n\\text{Parametric model: }wage &= b_0 + b_1 age + b_2 education +\\varepsilon \\\\ \\\\\n\\text{Non-parametric model: }wage &= g(age,education) +\\varepsilon\n\\\\ \\\\\n\\text{Semi-parametric model: } wage &= b_0 + g_1(age) + g_2(education) +\\varepsilon \\\\\nwage &= g_0(age)+b1 education+\\varepsilon \\\\\nwage &= g_0(age)+g_1 (age)education+\\varepsilon\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#step1-estimation-of-density-functions",
    "href": "adv_class/03par_spar_npar.html#step1-estimation-of-density-functions",
    "title": "Semi- and Non- Parametric regression",
    "section": "Step1: Estimation of Density functions",
    "text": "Step1: Estimation of Density functions\n\nThe first step towards learning non-parmetric analysis, is learning to use the most basic task of all.\nEstimating distributions (PDFs) : why? in economics, and other social sciences, we care about distributions!\nDistribution of income, how many live under poverty, how much is concentrated among the rich, how skew the distribution is, what is the level of inequality, etc, etc\nThe parametric approach to estimating distribution, is by using some predefined functional form (say normal), and use the data to estimate the parameters that define that distribution:\n\n\\[\n\\hat f(x) = \\frac{1}{\\sqrt{2\\pi\\hat\\sigma^2}}exp \\left(-\\frac{1}{2}\\left(\\frac{x-\\hat \\mu}{\\hat \\sigma}\\right)^2 \\right)\n\\]"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-4",
    "href": "adv_class/03par_spar_npar.html#section-4",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "Which can be done rather easy in Stata\n\nfrause oaxaca, clear\ndrop if lnwage==.\nsum lnwage\ngen f = normalden(lnwage, r(mean), r(sd))\nhistogram wages\n\nBut as you can see, it does not fit well."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#histogram",
    "href": "adv_class/03par_spar_npar.html#histogram",
    "title": "Semi- and Non- Parametric regression",
    "section": "Histogram",
    "text": "Histogram\nHistograms are a type of non-parametric estimator that imposes no functional form restrictions to estimate probability density functions (PDFs).\nConstruction histograms, is in fact, a fairly Straight forward task:\n\nYou select the width of bins, \\(h\\) , and starting value \\(x_0\\)\n\n\\[\\text{if } \\ x_i \\in [x_0 + m * h, x_0 + (m+1)h ) \\rightarrow\nbin(x_i)=m+1\n\\]\n\nAnd the Histogram estimator for density, is given by:\n\n\\[\\hat f (x) = \\frac{1}{nh} \\sum_i 1(bin(x)=bin(x_i))\n\\]\nSimple yet powerful, but sensitive to “h”"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-5",
    "href": "adv_class/03par_spar_npar.html#section-5",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "Histograms with Varying h"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#kernel-density",
    "href": "adv_class/03par_spar_npar.html#kernel-density",
    "title": "Semi- and Non- Parametric regression",
    "section": "Kernel density",
    "text": "Kernel density\nAn alternative to Histograms is known as the kernel density estimator.\n\\[\n\\hat f(x) = \\frac{1}{nh}\\sum_i K\\left(\\frac{X_i-x}{h}\\right)\n\\]\nwhere \\(K\\) is what is known as a kernel function."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-6",
    "href": "adv_class/03par_spar_npar.html#section-6",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "Kernel function\nA Kernel function is such that has the following properties:\n\\[\n\\int K(z)dz = 1 ; \\int zK(z)dz = 0 ; \\int z^2K(z)dz &lt; \\infty\n\\]\nIs a well behaved pdf on its own, that is symmetric, with defined second moment.\n\nas with the histogram estimator, the Kden is just an average of functions, that has the advantage of being smooth.\nAlthough it also depends strongly, on the choice of bandwidth."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-7",
    "href": "adv_class/03par_spar_npar.html#section-7",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "Kernel density: Visualization"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-8",
    "href": "adv_class/03par_spar_npar.html#section-8",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "Code in Stata\nhistogram var [if] [weight]\nkdensity var [if] [weight]\n\n\nCode\nfrause oaxaca, clear\nkeep if lnwage!=.\nmata:\n    y = st_data(.,\"lnwage\")\n    fden = J(rows(y),1,0)\n    for(i=1; i&lt;=rows(y); i++) {\n       fden[i] =mean(normalden(y, y[i], 0.08))\n    }\n    tg = fden,y\nend\ngetmata tg* = tg\n\n\n\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     y = st_data(.,\"lnwage\")\n\n:     fden = J(rows(y),1,0)\n\n:     for(i=1; i&lt;=rows(y); i++) {\n&gt;        fden[i] =mean(normalden(y, y[i], 0.08))\n&gt;     }\n\n:     tg = fden,y\n\n: end\n-------------------------------------------------------------------------------\n\n. \n\n\n\n\nCode\nscatter tg1 tg2, name(mx, replace)"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#trade-off-bias-vs-variance",
    "href": "adv_class/03par_spar_npar.html#trade-off-bias-vs-variance",
    "title": "Semi- and Non- Parametric regression",
    "section": "Trade off: Bias vs variance",
    "text": "Trade off: Bias vs variance\nWhile these estimators are “flexible”, there is one parameter that needs attention: The bandwidth \\(h\\)\nThis parameter needs to be calibrated to balance two problems in Non-parametric analysis. Bias vs Variance:\n\nwhen \\(h\\rightarrow 0\\) , the bias of your estimator goes to zero ( in average). Intuitively \\(\\hat f(x)\\) is constructed based on information that comes from \\(x\\) alone.\nBut the variance increases! Because things will vary for every \\(x\\).\nwhen \\(h \\rightarrow \\infty\\) , the bias increases, because you start using data that is very different to \\(x\\) to estimate \\(\\hat f(x)\\).\nBut variance decreases. Since the “function” is now very smooth (a line?)\n\nThus, special attention is needed to choose the right h, which minimizes the problems (bias and variance)."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#kdensity-bias-vs-variance",
    "href": "adv_class/03par_spar_npar.html#kdensity-bias-vs-variance",
    "title": "Semi- and Non- Parametric regression",
    "section": "Kdensity, Bias vs Variance",
    "text": "Kdensity, Bias vs Variance"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-9",
    "href": "adv_class/03par_spar_npar.html#section-9",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "Other Considerations\n\nAs shown above, one needs to choose the bandwidth \\(h\\) carefully, balancing the bias-variance trade off. Common approach is to simply use rule-of-thumb approaches to select this parameter:\n\n\\[\nh = 1.059 \\sigma n ^ {-1/5} \\\\ h = 1.3643 * d * n ^ {-1/5} * min(\\sigma,iqr\\sigma)\n\\]\nBut other approaches may work better."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-10",
    "href": "adv_class/03par_spar_npar.html#section-10",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "Other Considerations\n\nA second consideration is the choice of Kernel function! (see help kdensity -&gt; kernel)\n\nAlthough, except in few cases, the choice of bandwidth matters more than the kernel function.\n\nThis method works well when your data is smooth and continuous. But not so much for discrete data.\n\nNevertheless, it is still possible to use it with discrete data, and kernel weights!\n\nCan be “easily” extended to multiple dimensions \\(f(x,y,z,...)\\), including mixture of continuous and discrete data. You just multiple Kernels!\n\nBut, beware of Curse of dimensionality.\nBut still better than just Subsampling!"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#kfunctions",
    "href": "adv_class/03par_spar_npar.html#kfunctions",
    "title": "Semi- and Non- Parametric regression",
    "section": "Kfunctions",
    "text": "Kfunctions"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#np---regression",
    "href": "adv_class/03par_spar_npar.html#np---regression",
    "title": "Semi- and Non- Parametric regression",
    "section": "NP - Regression",
    "text": "NP - Regression\n\nAs hinted from the beginning, the idea of non-parametric regressions is related to estimate a model that is as flexible as it can be.\nThis relates to the CEF, where we want to estimate a conditional mean for every combination of X’s. In other words, you aim to estimate models that are valid “locally”. A very difficult task.\n\nYou have a limited sample size\nYou may not see all possible X’s combinations\nand for some, you may have micro-samples (n=1) Can you really do something with this?\n\nYes, make your model flexible, but not overly flexible! but how?\n\nKernel regression ; Spline regression\nPolynomial regression; Smoothed Spline regression."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#univariate-case",
    "href": "adv_class/03par_spar_npar.html#univariate-case",
    "title": "Semi- and Non- Parametric regression",
    "section": "Univariate case",
    "text": "Univariate case\n\nConsider a univariate case \\(y,X\\) where you only have 1 indep variable, which are related as follows:\n\n\\[\ny = m(x) + e\n\\]\nwhich imposes the simplifying assumption that error is additive.\n\nIn the parametric case, you could model this as a linear relationship:\n\n\\[\ny =b_0 + b_1 x + b_2 x^2 +b_3 x^3 +...+e\n\\]\n(this is, in fact, starting to become less parametric)"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-11",
    "href": "adv_class/03par_spar_npar.html#section-11",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "But in the full (unconstrained) model it would just be the conditional mean:\n\n\\[\nE(y|X) = \\hat m(x) = \\frac{\\sum y_i 1(x_i=x)}{\\sum 1(x_i=x)}\n\\]\nProblems? Impossible to do out of sample predictions, and if \\(n&lt;42\\) inference would be extremely unreliable."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-12",
    "href": "adv_class/03par_spar_npar.html#section-12",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "Local Constant Regression\nWe can improve over the Unconstrained mean using the following connection:\n\n\\(1(x_i=x)\\) is a non-smooth indicator that shows if an observation is in-sample .\nWe can substitute this with a smooth indicator function\n\n\\[\nK_w(x_i,x) = \\frac{K\\left(\\frac{x_i-x}{h}\\right)}{K(0)}\n\\]\nObservations where \\(x_i=x\\) will have a weight of 1, but depending on \\(h\\), less weight is given the farther \\(x_i\\) is to \\(x\\)."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-13",
    "href": "adv_class/03par_spar_npar.html#section-13",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "This gives what is known as the Nadaraya-Watson or Local constant estimator:\n\\[\n\\hat m(x) = \\frac{\\sum y_i K_w(x_i,x)}{\\sum K_w(x_i,x)} = \\sum y_i w_i\n\\]\nWhich, on its core, is simply a weighted regression, with weights given by \\(\\frac{K_w(x_i,x)}{\\sum K_w(x_i,x)}\\)\nKernel Regressions “borrows” info from neighboring observations to obtain a smooth estimator."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-14",
    "href": "adv_class/03par_spar_npar.html#section-14",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "Visuals"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-15",
    "href": "adv_class/03par_spar_npar.html#section-15",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "Implementation\n\n\nCode\nwebuse motorcycle, clear\nscatter accel time \n\n\n(Motorcycle data from Fan & Gijbels (1996))\n\n\n\n\n\n\n\n\n\n\n\nCode\nmata:\n    y = st_data(.,\"accel\")\n    x = st_data(.,\"time\")\n    yh = J(133,12,0)\n    for(k=1;k&lt;=12;k=k++) {\n        for(i=1;i&lt;=133;i++){\n            h = k/2\n            yh[i,k]=mean(y, normalden(x, x[i], h))\n        }\n    }    \nend\ngetmata yh* = yh, replace\ncolor_style viridis\ntwo scatter accel time || line yh* time, xsize(7) ysize(5)\n\n\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     y = st_data(.,\"accel\")\n\n:     x = st_data(.,\"time\")\n\n:     yh = J(133,12,0)\n\n:     for(k=1;k&lt;=12;k=k++) {\n&gt;         for(i=1;i&lt;=133;i++){\n&gt;             h = k/2\n&gt;             yh[i,k]=mean(y, normalden(x, x[i], h))\n&gt;         }\n&gt;     }\n\n: end\n-------------------------------------------------------------------------------\n\n."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-16",
    "href": "adv_class/03par_spar_npar.html#section-16",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "Considerations\n\nLocal Constant estimator is simple to estimate with a single variable. Multiple variables is just as easy:\n\n\\[\n\\hat m(x,z) = \\frac{\\sum y_i K_h(x_i,x) \\times K_h(z_i,z)}{\\sum K_h(x_i,x) \\times K_h(z_i,z)}\n\\]\nThe problem, however, lies on the curse of dimensionality.\nMore dimensions, less data per \\((x,z)\\) point, unless you “increase” bandwidths.\n\nAs before, it all depends on the Bandwidth \\(h\\), which determines the “flexibility” of the model.\nThe local constant tends to have considerable bias (specially near limits of the distribution, or when \\(g\\) has too much curvature)"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-17",
    "href": "adv_class/03par_spar_npar.html#section-17",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "Choosing h\nThe quality of the NPK regression depends strongly on the choice of \\(h\\). And as with density estimation, the choice translates into a tradeoff between bias and variance of the estimation.\nThere are various approaches to choose \\(h\\). Some which depend strongly on the dimensionality of the model.\nFor Example, Stata command lpoly estimates local constant models, using the following:\n\nBut that is not the only approach.\nAn alternative (used for regularization) is using Cross-Validaton. (a method to evaluate the predictive power of a model)"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#cross-validation-intuition",
    "href": "adv_class/03par_spar_npar.html#cross-validation-intuition",
    "title": "Semi- and Non- Parametric regression",
    "section": "Cross Validation: Intuition",
    "text": "Cross Validation: Intuition\n\nSeparate your data in two parts: Training and testing Sample.\nEstimate your model in the TrainS, and evaluate predictive power in TestS.\nTo obtain a full view of predictive power, Repeat the process rotating the training set\n\n\\[\nmse = \\frac{1}{N}\\sum(y_i - g_{-k}(x))^2\n\\]\nThis should give you a better idea of the predictive power of the model."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-18",
    "href": "adv_class/03par_spar_npar.html#section-18",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "Cross-validation in Stata\n\nfrause oaxaca, clear\nssc install cv_kfold\n\nqui:reg lnwage educ exper tenure female age\n\ncv_kfold\nk-fold Cross validation\nNumber of Folds     :          5\nNumber of Repetions :          1\nAvg Root Mean SE    :    0.45838\n\nqui:reg lnwage c.(educ exper tenure female age)\n               ##c.(educ exper tenure female age)\n\ncv_kfold\nk-fold Cross validation\nNumber of Folds     :          5\nNumber of Repetions :          1\nAvg Root Mean SE    :    0.42768\n\n. qui:reg lnwage c.(educ exper tenure female age)\n                 ##c.(educ exper tenure female age)\n                 ##c.(educ exper tenure female age)\n\n. cv_kfold\nk-fold Cross validation\nNumber of Folds     :          5\nNumber of Repetions :          1\nAvg Root Mean SE    :    0.43038\n\nssc install cv_regress\n* Does lOOCV for regression\ncv_regress\n\nLeave-One-Out Cross-Validation Results \n-----------------------------------------\n         Method          |    Value\n-------------------------+---------------\nRoot Mean Squared Errors |       0.4244\nLog Mean Squared Errors  |      -1.7144\nMean Absolute Errors     |       0.2895\nPseudo-R2                |      0.36344\n-----------------------------------------"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#loocv",
    "href": "adv_class/03par_spar_npar.html#loocv",
    "title": "Semi- and Non- Parametric regression",
    "section": "LOOCV",
    "text": "LOOCV\nBecause the “choice” of “folds” and Repetitions, and the randomness, may produce different results every-time, one also has the option of using the “leave-one-out” approach.\nThis means, leave one observation out, and use the rest to make the predictions.\n\\[\nCV(h) = n^{-1}\\sum_{i=1}^n(y_i - \\hat g_{-i}(x_i))^2\n\\]\nThis is not as bad as it looks, since we can use the shortcut\n\\[\nCV(h) = n^{-1}\\sum_{i=1}^n\\left(\\frac{y_i - \\hat g(x_i)}{1-w_i/\\Sigma w_j}\\right)^2\n\\]\nIn Stata, the command npregress kernel uses this type of cross-validation to determine “optimal” \\(h\\)\nlpoly y x, kernel(gaussian) nodraw\ndisplay r(bwidth)\n.23992564\nnpregress kernel y x, estimator(constant) noderiv\n. Bandwidth\n-------------------------\n             |      Mean \n-------------+-----------\n           x |  .4064052 \n-------------------------"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#extending-from-constant-to-polynomial",
    "href": "adv_class/03par_spar_npar.html#extending-from-constant-to-polynomial",
    "title": "Semi- and Non- Parametric regression",
    "section": "Extending from constant to Polynomial",
    "text": "Extending from constant to Polynomial\nAn alternative way to understanding the simple NW (local constant) regressions, is to understand it as a local regression model with anything but a constant:\n\\[\n\\hat m(x)=min\\sum(y_i - \\beta_0)^2 w(x,h)_i\n\\]\nThis means that you could extend the analogy and include “centered” polynomials to the model.\n\\[\n\\begin{aligned}\nmin &\\sum(y_i - \\beta_0 - \\beta_1 (x_i -x) -\\beta_2 (x_i - x) ^2 - ...-\\beta_k(x_i-x)^k)^2 w(x,h)_i \\\\\n\\hat m(x) &= \\hat \\beta_0\n\\end{aligned}\n\\]\nThis is called the local polynomial regression.\n\nBecause its more flexible, it shows less bias when the true function shows a lot of variation.\nBecause of added polynomials, it requires more information (larger \\(h\\))\nIt can be used to easily obtain local marginal effects.\nAnd can also be used with multinomial models (local planes)\n\n\\[min \\sum (y_i - \\beta_0 - \\beta_1 (x_i-x) - \\beta_2 (z_i-z))^2 w(x,z,h)\n\\]"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#local-constant-to-local-polynomial",
    "href": "adv_class/03par_spar_npar.html#local-constant-to-local-polynomial",
    "title": "Semi- and Non- Parametric regression",
    "section": "Local Constant to Local Polynomial",
    "text": "Local Constant to Local Polynomial\n\n\nwebuse motorcycle\ntwo scatter accel time || ///\nlpoly accel time , degree(0) n(100) || ///\nlpoly accel time , degree(1) n(100) || ///\nlpoly accel time , degree(2) n(100) || ///\nlpoly accel time , degree(3) n(100) , ///\nlegend(order(2 \"LConstant\" 3 \"Local Linear\" ///\n4 \"Local Cubic\" 5 \"Local Quartic\"))"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#statistical-inference",
    "href": "adv_class/03par_spar_npar.html#statistical-inference",
    "title": "Semi- and Non- Parametric regression",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nFor Statistical Inference, since each regression is just a linear model, standard errors can be obtained using the criteria as in Lecture 1. (Robust, Clustered, bootstrapped).\n\nWith perhaps one caveat. Local estimation and standard errors may need to be estimated “globally”, rather than locally.\n\nThe estimation of marginal effects becomes a bit more problematic.\n\nLocal marginal effects are straightforward (when local linear or higher local polynomial is used)\nGlobal marginal effects, can be obtained averaging all local marginal effects.\nHowever, asymptotic standard errors are difficult to obtain (consider the multiple correlated components), but bootstrapping is still possible."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#stata-example",
    "href": "adv_class/03par_spar_npar.html#stata-example",
    "title": "Semi- and Non- Parametric regression",
    "section": "Stata Example",
    "text": "Stata Example\nfrause oaxaca\nnpregress kernel lnwage age exper\n\nComputing mean function\n  \nMinimizing cross-validation function:\nIteration 6:   Cross-validation criterion = -1.5912075  \n  \nComputing optimal derivative bandwidth\nIteration 3:   Cross-validation criterion =  .00196371  \n\nBandwidth\n------------------------------------\n             |      Mean     Effect \n-------------+----------------------\n         age |  2.843778   15.10978 \n       exper |  3.113587   16.54335 \n------------------------------------\n\nLocal-linear regression                    Number of obs      =          1,434\nKernel   : epanechnikov                    E(Kernel obs)      =          1,434\nBandwidth: cross-validation                R-squared          =         0.3099\n------------------------------------------------------------------------------\n      lnwage |   Estimate\n-------------+----------------------------------------------------------------\nMean         |\n      lnwage |   3.339269\n-------------+----------------------------------------------------------------\nEffect       |\n         age |   .0169326\n       exper |  -.0010196\n------------------------------------------------------------------------------\nNote: Effect estimates are averages of derivatives.\nNote: You may compute standard errors using vce(bootstrap) or reps()."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#other-types-of-non-parametric-models",
    "href": "adv_class/03par_spar_npar.html#other-types-of-non-parametric-models",
    "title": "Semi- and Non- Parametric regression",
    "section": "Other types of “non-parametric” models",
    "text": "Other types of “non-parametric” models\nWe have explored the basic version of non-parametric modeling. But its not the only one.\nThere are at least two others that are easy to implement.\n\nNonparametric Series Regression (we will see this)\nSmoothing series/splines: Which borrows from Series regression and Ridge Regression."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#non-parametric-series",
    "href": "adv_class/03par_spar_npar.html#non-parametric-series",
    "title": "Semi- and Non- Parametric regression",
    "section": "Non-parametric series",
    "text": "Non-parametric series\nThis approach assumes that model flexibility can achieve by using “basis” functions in combination with Interactions, but using “global” regressions (OLS)\nBut what are “basis” functions? They are a collection of terms that approximates smooth functions arbitrarily well.\n\\[\n\\begin{aligned}\ny &= m(x,z)+e  \\\\\nm(x,z)  &= B(x)+ B(z)+B(x)*B(z)  \\\\\nB(x)  &= (x, x^2, x^3,...) \\\\\nB(x)  & = fracPoly \\\\\nB(x)  &= (x, max(0,x-c_1), max(0,x-c_2), ... \\\\\nB(x)  &= (x,x^2,max(0,x-c_1)^2, max(0,x-c_2)^2, ... \\\\\nB(x)  &= B-splines\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section-19",
    "href": "adv_class/03par_spar_npar.html#section-19",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "Polynomials can be used, but there may be problems with high order polynomials. (Runge’s phenomenon,multiple-co-linearity). They are “global” estimators.\nFractional polynomials: More flexible than polynomials, without producing “waves” on the predictions\nNatural Splines, are better at capturing smooth transitions (depending on degree). Require choosing Knots appropriately.\nB-splines are similar to N-splines, but with better stat properties. Also require choosing knots\n\nExcept for correctly estimating the Bases functions (fracpoly and Bsplines are not straight forward), estimation requires simple OLS."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#np-series---tuning",
    "href": "adv_class/03par_spar_npar.html#np-series---tuning",
    "title": "Semi- and Non- Parametric regression",
    "section": "NP series - tuning",
    "text": "NP series - tuning\n\nWhile NP-series are easy to estimate, we also need to address problems of over-fitting.\nWith Polynomial: What degree of polynomial is correct? What about the degree of the interactions?\nFractional Polynomials: How many terms are needed, what would their “degrees” be.\nNsplines, Bsplines: How to choose degree? and where to set the knots?\n\nThese questions are similar to the choosing \\(h\\) in kernel regressions. However, model choice is simple…Cross validation.\n\nEstimate a model under different specifications (cut offs), and compare the out-of-sample predictive power. (see Stata: cv_kfold or cv_regress)\n\nOne more problem left. Statistical Inference"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#np-series---se-and-mfx",
    "href": "adv_class/03par_spar_npar.html#np-series---se-and-mfx",
    "title": "Semi- and Non- Parametric regression",
    "section": "NP series - SE and Mfx",
    "text": "NP series - SE and Mfx\nLecture 1 applies here. Once the model has been chosen, you can estimate SE using appropriate methods. There is only one caveat\n\nStandard SE estimation ignores the uncertainty of choosing cut-offs or polynomial degrees. In principle, cut-offs uncertainty can be modeled. But requires non-linear estimation.\n\nMarginal effects are somewhat easier for some basis. Just take derivatives:\n\\[\ny = b_0 + b_1 x + b_2 x^2 +b_3 max(0,x-c)^2 + e \\\\\n\\frac{dy}{dx}=b_1 + 2 b_2 x + 2 b_3 (x-c) 1(x&gt;c)\n\\]\n\nBut keeping track of derivatives in a multivariate model is difficult, and often, the functions are hard to track down. so how to implement it?"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#np-series-implementation-marginal-effects",
    "href": "adv_class/03par_spar_npar.html#np-series-implementation-marginal-effects",
    "title": "Semi- and Non- Parametric regression",
    "section": "NP series: Implementation marginal effects",
    "text": "NP series: Implementation marginal effects\nAs always, it all depends on how are the models estimated.\n\nStata command npregress series allows you to estimate this type of models using polynomials, splines and B-splines. And also allows estimates marginal effects for you. (can be slow)\nfp estimates fractional polynomials, but does not estimate marginal effects for you.\nIn Stata, you can use the package f_able to estimate those marginal effects, however. see here for details. and SSC for the latest update.\n\nfrause oaxaca, clear\ndrop agesq\nf_spline age = age, nk(1) degree(3)\nf_spline exper = exper, nk(1) degree(3)\nqui:regress lnwage c.(age*)##c.(exper*)\nf_able age? exper?, auto \nmargins, dydx(age exper) noestimcheck \n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0360234   .0033909    10.62   0.000     .0293775    .0426694\n       exper |   .0082594   .0050073     1.65   0.099    -.0015547    .0180735\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#semiparametric-regressions",
    "href": "adv_class/03par_spar_npar.html#semiparametric-regressions",
    "title": "Semi- and Non- Parametric regression",
    "section": "Semiparametric Regressions",
    "text": "Semiparametric Regressions\n\nFull non-parametric estimations are powerful to identify very flexible functional forms. To avoid overfitting, however, one must choose tuning parameters appropriately (\\(h\\) and \\(cutoffs\\) ).\nA disadvantage: Curse of dimensionality. More variables need more data to provide good results. But, the more data you have, the more difficult to estimate (computing time).\nIt also becomes extremly difficult to interpret. (too much flexibility)\nAn alternative, Use the best of both worlds: Semiparametric regression\n\nFlexibility when needed with the structure of standard regressions, to avoid the downfalls of fully nonparametric models"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#partially-linear-model",
    "href": "adv_class/03par_spar_npar.html#partially-linear-model",
    "title": "Semi- and Non- Parametric regression",
    "section": "Partially Linear model",
    "text": "Partially Linear model\n\\[\ny = x\\beta +g(z) +e\n\\]\nThis model assumes that only a smaller set of covariates need to be estimated non-parametrically in the model.\nEstimators:\n\nnpregress series: Use Basis to estimate \\(g(z)\\)\nYatchew 1997: For a single z, sort variables by it, and estimate: \\(\\Delta y=\\Delta X\\beta+ \\Delta g(z) + \\Delta e\\). This works because \\(\\Delta g(z)\\rightarrow 0\\)\nEstimate \\(g(z)\\) regressing \\(y-x\\hat \\beta\\) on \\(z\\). See plreg\nRobinson 1988: Application of FWL. Estimate \\(y = g_y(z)+e_y\\) and \\(x = g_x(z)+e_x\\) and estimate \\(\\beta = (e_x ' e_x)^{-1} e_x ' e_y\\) . For \\(g(z)\\) same as before. See semipar.\nOther methods available see semi_stata"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#generalized-additive-model",
    "href": "adv_class/03par_spar_npar.html#generalized-additive-model",
    "title": "Semi- and Non- Parametric regression",
    "section": "Generalized Additive model",
    "text": "Generalized Additive model\n\\[\ny = g(x) +g(z)+e\n\\]\nThis model assumes the effect of X and Z (or any other variables) are additive separable, and may have a nonlinear effect on y.\n\nnpregress series: with non-interaction option. Fractional polynomials mfp, cubic splines mvrs (see mvrs) , or manual implementation.\nKernel regression possible. (as in Robinson 1988), but requires an iterative method. (back fitting algorithm)\n\n\\(g(x) = smooth (y-g(z)|x)\\), center \\(g(x)\\) , and \\(g(z) = smooth (y-g(x)|z)\\), center \\(g(z)\\) until convergence\n\nIn general, it can be easy to apply, but extra work required for marginal effects."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#smooth-coefficient-model",
    "href": "adv_class/03par_spar_npar.html#smooth-coefficient-model",
    "title": "Semi- and Non- Parametric regression",
    "section": "Smooth Coefficient model",
    "text": "Smooth Coefficient model\n\\[\ny = g_0(z)+g_1(z)x + e\n\\]\nThis model assumes that \\(X's\\) have a locally linear effect on \\(y\\), but that effect varies across values of \\(z\\), in a non-parametric way.\n\nfp or manual implementation of basis functions, with interaction. May allow for multiple variables in \\(z\\)\nOne can also use Local Kernel regressions. locally weighted regression where All X variables are considered fixed, or interacted with polynomials of Z. Choice of bandwidth problematic, but doable (LOOCV).\nvc_pack can estimate this models with a single z, as well as test it. Overall marginal effects still difficult to obtain."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#example",
    "href": "adv_class/03par_spar_npar.html#example",
    "title": "Semi- and Non- Parametric regression",
    "section": "Example",
    "text": "Example\n\nfrause oaxaca\nvc_bw lnwage educ exper tenure female married divorced, vcoeff(age)\nvc_reg lnwage educ exper tenure female married divorced, vcoeff(age) k(20)\nssc install addplot\nvc_graph educ exper tenure female married divorced, rarea\naddplot grph1:, legend(off) title(Education)\naddplot grph2:, legend(off) title(Experience)\naddplot grph3:, legend(off) title(Tenure)\naddplot grph4:, legend(off) title(Female)\naddplot grph5:, legend(off) title(Married)\naddplot grph6:, legend(off) title(Divorced)\n\ngraph combine grph1 grph2 grph3 grph4 grph5 grph6"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#example-1",
    "href": "adv_class/03par_spar_npar.html#example-1",
    "title": "Semi- and Non- Parametric regression",
    "section": "Example",
    "text": "Example\n\nWage Profile across years"
  },
  {
    "objectID": "adv_class/05uqreg.html#introduction",
    "href": "adv_class/05uqreg.html#introduction",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Introduction",
    "text": "Introduction\nAs we saw last class, conditional quantile regressions have only one purpose:\n\nAnalyze relationships between conditional distributions.\n\nThis is a very useful tool!. As it allows you to move beyond Average relationships.\n\nHow do people (who are not all average) would be affected by changes in \\(Xs\\)\n\nThere is a limitation, however. The effects you may estimate, will depend strongly on model specification.\n\nThis is similar to OVB. Changing covariates could drastically change the conditional distributions and associated coefficients\n\nWhat if, you are interested in distributional effects across the whole population! Not only a subsample?"
  },
  {
    "objectID": "adv_class/05uqreg.html#eqyx-is-not-qy",
    "href": "adv_class/05uqreg.html#eqyx-is-not-qy",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "\\(E(q(y|X))\\) is not \\(Q(y)\\)",
    "text": "\\(E(q(y|X))\\) is not \\(Q(y)\\)\n\nCommon mistake when analyzing QRegressions: Make interpretations as if the average effects on the \\(qth\\) conditional quantiles would be the same as the effect on the “overall” \\(qth\\) quantile.\nExcept for few cases (when Quantile regressions are not relevant), CQ effects do not translate directly into Changes into the unconditional quantile.\n\nHowever, as a policy maker, this would be the most relevant estimand you may be interested in :\n\nHow does improving education affect inequality?\nWould eliminating Unionization would increase wage inequality?\nIs there heterogeneity in consumption expenditure?\n\nHowever, going from Conditional to unconditional statistics (not only Q) is not always straight forward."
  },
  {
    "objectID": "adv_class/05uqreg.html#waitwhat-do-we-mean-unconditional",
    "href": "adv_class/05uqreg.html#waitwhat-do-we-mean-unconditional",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "⌚Wait…What do we mean unconditional?",
    "text": "⌚Wait…What do we mean unconditional?\nOne of the questions I read a lot regarding UQR is what do we mean unconditional?\n\nThis is perhaps a someone poor choice of words.\nAnytime we estimate ANY statistic, we condition on something.\n\nWe condition on all individual characteristics (including errors)\nWe condition on groups characteristics (CQREG and CEF)\nor, We condition on all characteristics (distributions). We happen to call this, unconditional statistics.\n\nThis, however, does make a big difference in interpretation."
  },
  {
    "objectID": "adv_class/05uqreg.html#from-condition-on-individuals",
    "href": "adv_class/05uqreg.html#from-condition-on-individuals",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "From Condition on Individuals,",
    "text": "From Condition on Individuals,\nto conditioning on Distributions\n\\[\\begin{aligned}\ny_i &= b_0 + b_1 x_i + e_i + x_i e_i \\\\\n\\frac{dy_i}{dx_i}&=b_1 + e_i \\\\\nE(y_i|x_i=x) &= b_0 + b_1 x  \\\\\n\\frac{dE(y_i|x)}{dx}&=b_1 \\\\\nE(E(y_i|x_i=x))=E(y_i) &= b_0 + b_1 E(x_i)    \\\\\n\\frac{dE(y_i)}{dE(x_i)}&=b_1\n\\end{aligned}\n\\]\nSame effects, but different interpretations (specially last one)"
  },
  {
    "objectID": "adv_class/05uqreg.html#how-are-unconditional-effects-estimated",
    "href": "adv_class/05uqreg.html#how-are-unconditional-effects-estimated",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "How are Unconditional effects Estimated?",
    "text": "How are Unconditional effects Estimated?\nConsider any distributional statistic \\(v\\), which takes as arguments, all observations, density distributions \\(f()\\), or cumulative distributions \\(F()\\).\n\\[\nv = v(F_y) \\ or \\ v(f_y) \\ or \\ v(y_1, y_2, ...,y_n)\n\\]\nAnd to simplify notation, lets say this function is defined as follows:\n\\[\nv(f_y) = \\int_{-\\infty}^\\infty h(y,\\theta) f(y)dy\n\\]\nThis simply considers distributional statistics \\(v\\) that can be estimated by simply integrating a transformation of \\(h(y,\\theta)\\) given a set of parameters \\(\\theta\\).\nBut for now, lets consider only the Identify function \\(h(y,\\theta)=y\\)\nbut…What about Controls??"
  },
  {
    "objectID": "adv_class/05uqreg.html#introducing-controls",
    "href": "adv_class/05uqreg.html#introducing-controls",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Introducing controls",
    "text": "Introducing controls\nAssume there is a joint distribution of function \\(f(y,x)\\), then\n\\[\n\\begin{aligned}\nf(y,x)&=f(y|x)f(x) \\\\\nf(y) &= \\int f(y|x) f(x) dx\n\\end{aligned}\n\\]\nAnd all together:\n\\[\n\\begin{aligned}\nv(f_y) &= \\int y \\int f(y|x) f(x) dx \\ dy \\\\\nv(f_y) &= \\iint y f(y|x) dy \\ f(x) dx  \\\\\nv(f_y) &= \\int  E(y|X) f(x) dx  \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/05uqreg.html#or-a-bit-more-general",
    "href": "adv_class/05uqreg.html#or-a-bit-more-general",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Or a bit more General",
    "text": "Or a bit more General\n\\[\nv(f_y) = \\iint h(y,\\theta) f(y|x)f(x)dxdy\n\\]\nSo, the statistic \\(v\\) will change if:\n- We change the function \\(h\\) or its parameters \\(\\theta\\).\n- Assume some shocks that change the conditional \\(f(y|x)\\)\n- or the distribution of characteristics change!\nNote: \\[f(y|x) \\sim \\beta \\text{ and }\nf(x) \\sim x\n\\]"
  },
  {
    "objectID": "adv_class/05uqreg.html#againhow-are-unconditional-effects-estimated",
    "href": "adv_class/05uqreg.html#againhow-are-unconditional-effects-estimated",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Again…How are Unconditional effects Estimated?",
    "text": "Again…How are Unconditional effects Estimated?\nIn an ideal scenario, you simple get the data under two regimes (before and after changes in \\(x\\)), and do the following:\n\\[\\Delta v = v(f'_y)-v(f_y)\n\\]\nThat is, just estimate the statistic in two scenarios (\\(f'\\) and \\(f\\)), and calculate the difference. (impossible!)\nBut there are (at least) three alternatives:\n\nUsing Reweighting approaches to “reshape” the data: \\(f(x)\\) (non-parametric)\nIdentify \\(f(y|x)\\) so one can simulate how \\(\\Delta X\\) affect y\nFocus on the statistic \\(v\\) and indirectly identify the effects of interest. (RIF!)"
  },
  {
    "objectID": "adv_class/05uqreg.html#op1-re-weighting",
    "href": "adv_class/05uqreg.html#op1-re-weighting",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Op1: Re-weighting",
    "text": "Op1: Re-weighting\nConsider the following\n\nThere is a policy such that you plan to improve education in a country.\nEvery single person will have at least 7 years of education, and will have free access to two additional years of education if they want to.\nIn other words, characteristics change from \\(f(x) \\rightarrow g(x)\\) . But you do not see this!\n\n\\[ v(g_y) = \\iint h(y,\\theta) f(y|x) \\color{red}{g(x)}dxdy \\]"
  },
  {
    "objectID": "adv_class/05uqreg.html#section",
    "href": "adv_class/05uqreg.html#section",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "",
    "text": "but perhaps, we could see this:\n\\[\\hat v(g_y) = \\iint h(y,\\theta) f(y|x) \\color{red}{\\hat w(x)}f(x) dxdy\n\\]\nif we can come up with a set of weights \\(\\color{red}{\\hat w(x)}\\) such that \\(f(x)\\hat w(x)=g(x)\\)\n\\[\n\\hat w(x) = \\frac{\\hat g(x)}{\\hat f(x)}\n\\]"
  },
  {
    "objectID": "adv_class/05uqreg.html#section-1",
    "href": "adv_class/05uqreg.html#section-1",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "",
    "text": "Simple, yet hard. Estimation of multivariate densities can be a difficult task.\n\\[\nf(x) = h(x|s=0) ; g(x) = h(x|s=1)\n\\]\nThis makes things “easier”.\n\\[\n\\begin{aligned}\nh(x|s=k)  &= \\frac{h(x)p(s=k|x)}{p(s=k)} \\\\\n\\hat w(x)  \n         &= \\frac{h(x)p(s=1|x)}{h(x)p(s=0|x)}\\frac{p(s=0)}{p(s=1)} \\\\\n         &=\\frac{p(s|x)}{1-p(s|x)} \\frac{1-p(s)}{p(s)}\n\\end{aligned}\n\\]\nEasier to estimate conditional probabilities, (logit probit or other) than Densities"
  },
  {
    "objectID": "adv_class/05uqreg.html#example",
    "href": "adv_class/05uqreg.html#example",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Example",
    "text": "Example\nGoal: Evaluate the impact of an increase in Fines on # of citations. (using reweighting)\n\nwebuse dui, clear\n** Create Fake Sample\ngen id = _n\nexpand 2\nbysort id:gen smp = _n ==2\n** Now you have two of ever person. So lets do some Policy\n** Fines increase lower fines more than higher ones, up to 12\n** Here we have a simulation of a policy that increases fines\nreplace fines = 0.1*(12-fines)+fines if smp==1\n\n\n\n\n(Fictional data on monthly drunk driving citations)\n(500 observations created)\n(498 real changes made)\n\n\nEstimation of Logit (or Probit) to estimate \\(p(s|x)\\)\nAnd estimate IPW weights\n\n** Estimate logit \nqui:logit smp c.fines##c.fines taxes i.csize college\npredict pr_smp\ngen wgt = pr_smp / (1-pr_smp) \nreplace wgt = 1 if smp==1\n\n(option pr assumed; Pr(smp))\n(500 real changes made)\n\n\nHave the IPW weights helped simulate the policy?\n\nset scheme white2\ncolor_style tableau\nxi:tabstat fines i.csize college  taxes [w=wgt],  by(smp)\n\ni.csize           _Icsize_1-3         (naturally coded; _Icsize_1 omitted)\n(analytic weights assumed)\n\nSummary statistics: Mean\nGroup variable: smp \n\n     smp |     fines  _Icsiz~2  _Icsiz~3   college     taxes\n---------+--------------------------------------------------\n       0 |  10.10573  .2919004  .3571831  .2483835  .7047717\n       1 |  10.10568       .29      .358      .248      .704\n---------+--------------------------------------------------\n   Total |  10.10571  .2909501  .3575916  .2481917  .7043858\n------------------------------------------------------------\n\n\nI can now compare the distribution of fines before and after the policy\n\n\nCode\ntwo (kdensity citations if smp==0 ) ///\n    (kdensity citations if smp==0 [w=wgt]) /// \n    , legend(order(1 \"Before Policy\" 2 \"After Policy\"))\n\n\n(analytic weights assumed)\n(analytic weights assumed)\n(analytic weights assumed)\n\n\n\nSeems to be a contraction of # citations:\n\n\nCode\ndisplay \"Before Policy\"\ntabstat citations if smp ==0, stats(p10 p25 p50 mean p75 p90  )\ndisplay \"After Policy\"\ntabstat citations if smp ==0 [w=wgt],  stats(p10 p25 p50 mean p75 p90  )\n\n\nBefore Policy\n\n    Variable |       p10       p25       p50      Mean       p75       p90\n-------------+------------------------------------------------------------\n   citations |      11.5        15        20    22.018        27      34.5\n--------------------------------------------------------------------------\nAfter Policy\n(analytic weights assumed)\n\n    Variable |       p10       p25       p50      Mean       p75       p90\n-------------+------------------------------------------------------------\n   citations |        11        14        19  20.29751        25        32\n--------------------------------------------------------------------------\n\n\n\nIncreasing fines may reduce citations in about 1.3., but have almost no effect at the bottom of the distribution.\n\nWhat about Standard errors? Bootstrap! (logit and estimation, probably clustering at individual level)\nEasy to extend to other Statistics, but, can only provide results “within” support."
  },
  {
    "objectID": "adv_class/05uqreg.html#op2-model-conditional-distribution",
    "href": "adv_class/05uqreg.html#op2-model-conditional-distribution",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Op2: Model Conditional Distribution",
    "text": "Op2: Model Conditional Distribution\nSay that you are interested in the same Policy, but do not trust re-weighting. Instead you want to model the Outcome, using some parametric or nonparametric analysis\n\nDefine your model. Should be feasible enough to accommodate changes in the conditional distribution. (one “model” for each \\(X's\\) combination?)\nUse the model to make predictions of your outcome (quite a few times). and summarize all results.\n\nOptions for flexible mode?\n\nYou can use Heteroskedastic OLS \\(y\\sim N(x\\beta,x\\gamma)\\) and predict from here\nYou can use CQregressions to simulate the results.\n\nOne of this is similar to what we do in simulation analysis, and imputation. The other is similar to the work of Machado Mata (2005) and Melly(2005). Where you invert the whole distribution “globally”"
  },
  {
    "objectID": "adv_class/05uqreg.html#section-2",
    "href": "adv_class/05uqreg.html#section-2",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "",
    "text": "Recipe\n\nModel \\(Y=G(X,\\theta)\\)\nCreate a “policy” \\(X'=H(X)\\)\nPredict \\(Y'=G(X',\\theta)\\) and identify effect:\n\\[\\Delta V(Y) = V(Y')-V(Y)\\]\nRepeat many times, and summarize results."
  },
  {
    "objectID": "adv_class/05uqreg.html#example-1-hetregress",
    "href": "adv_class/05uqreg.html#example-1-hetregress",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Example #1: Hetregress",
    "text": "Example #1: Hetregress\n** Example for OPT2\nwebuse dui, clear\n** Modeling OLS with heteroskedastic errors\n    hetregress citations fines i.csize college taxes ,  het(fines i.csize college taxes )\n    \n    \n------------------------------------------------------------------------------\n   citations | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\ncitations    |\n       fines |   -6.18443   .3018298   -20.49   0.000    -6.776006   -5.592855\n             |\n       csize |\n     Medium  |   4.683941   .5028377     9.32   0.000     3.698397    5.669484\n      Large  |   9.655742   .5261904    18.35   0.000     8.624428    10.68706\n             |\n     college |   4.495635   .5283579     8.51   0.000     3.460072    5.531197\n       taxes |  -3.640864   .4938209    -7.37   0.000    -4.608735   -2.672993\n       _cons |   79.48011   3.118008    25.49   0.000     73.36892    85.59129\n-------------+----------------------------------------------------------------\nlnsigma2     |\n       fines |  -.5261208    .082495    -6.38   0.000     -.687808   -.3644337\n             |\n       csize |\n     Medium  |    .331204   .1681709     1.97   0.049     .0015952    .6608129\n      Large  |   .5578834   .1662309     3.36   0.001     .2320768    .8836899\n             |\n     college |   .3186815   .1539424     2.07   0.038     .0169599    .6204032\n       taxes |  -.3988692   .1437708    -2.77   0.006    -.6806548   -.1170836\n       _cons |   8.257714   .8201063    10.07   0.000     6.650335    9.865093\n------------------------------------------------------------------------------\nLR test of lnsigma2=0: chi2(5) = 75.42                    Prob &gt; chi2 = 0.0000\n\n\n**  make Policy\nclonevar fines_copy = fines\nreplace fines = 0.1*(12-fines)+fines \n\npredict xb, xb\npredict xbs, sigma\n\n** Simulate results\ncapture program drop sim1\nprogram sim1, eclass\n    capture drop cit_hat \n    gen cit_hat = rnormal(xb,xbs)   \n    qui:sum citations, d \n    local lp10 = r(p10)\n    local lp25 = r(p25)\n    local lp50 = r(p50) \n    local lpmn = r(mean)\n    local lp75 = r(p75)\n    local lp90 = r(p90)\n    qui:sum cit_hat, d \n    matrix b = r(p10)-`lp10',r(p25)-`lp25', r(p50)-`lp50' , r(mean) -`lpmn',r(p75)-`lp75',r(p90)-`lp90'\n    matrix colname b = p10 p25 p50 mean p75 p90\n    ereturn post b\nend\n\nsimulate, reps(1000): sim1\nsum\n\n-------------+---------------------------------------------------------\n      _b_p10 |      1,000    -1.08147    .3913698   -2.31713   .1689796\n      _b_p25 |      1,000   -.3262908    .3230118  -1.817808   .6465259\n      _b_p50 |      1,000   -.2085465     .316455   -1.09237   .7785921\n     _b_mean |      1,000   -1.675626    .2234377  -2.400322   -1.03909\n      _b_p75 |      1,000   -1.541725    .4210822  -2.857586  -.2505198\n-------------+---------------------------------------------------------\n      _b_p90 |      1,000   -3.543298    .6079578  -5.464802  -1.682991\nEffects larger than Reweigthing. Statistical inference here may be flawed. (first stage error not carried over)"
  },
  {
    "objectID": "adv_class/05uqreg.html#example-2-qregress",
    "href": "adv_class/05uqreg.html#example-2-qregress",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Example #2: Qregress",
    "text": "Example #2: Qregress\nwebuse dui, clear\ngen id = _n\n** Expand to 99 quantiles\nexpand 99 \nbysor id:gen q=_n\n** make policy\ngen fines_policy=0.1*(12-fines)+fines \ngen fines_copy  =fines \n** Estimate 99 quantiles (in theory one should do more..but choose at random)\nssc install qrprocess // Faster than qreg\n** Save Cit hat (prediction)\n** cit policy (with policy)\ngen cit_hat=.\ngen cit_pol=.\n\nforvalues  i = 1 / 99 {\n    if `i'==1   _dots 0 0\n    _dots `i' 0\n    qui {\n        local i100=`i'/100\n        capture drop aux\n        qrprocess citations c.fines##c.fines  (i.csize college taxes) if q==1, q(`i100')\n        ** predicts the values as if they were in q100\n        predict aux\n        replace cit_hat=aux if q==`i'\n        drop aux\n        replace fines = fines_policy\n        predict aux\n        replace cit_pol=aux if q==`i'   \n        replace fines = fines_copy \n    }   \n}\n\n tabstat citations cit_hat cit_pol, stats(p10 p25 p50 mean p75 p90)\n \n   Stats |  citati~s   cit_hat   cit_pol\n---------+------------------------------\n     p10 |      11.5  10.70744  9.911633\n     p25 |        15  15.42857  14.27302\n     p50 |        20  21.15557  19.68303\n    Mean |    22.018   22.0002  20.31824\n     p75 |        27  27.65936  25.56173\n     p90 |      34.5  34.03413  31.39192\n----------------------------------------\nVery demanding (computationally) and may only capture effects to the extend that we have good coverage of the distribution.\nStandard Errors…Bootstrapping. Perhaps use random quantile assignment, and may have problems near boundaries."
  },
  {
    "objectID": "adv_class/05uqreg.html#opt-1-and-2-comments",
    "href": "adv_class/05uqreg.html#opt-1-and-2-comments",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Opt 1 and 2: Comments",
    "text": "Opt 1 and 2: Comments\n\nThe first option allow you to estimate effects of changes in \\(f(x)\\) on the unconditional distribution of \\(y\\), and in consequence, the distributional statistics of interest.\nThe second option allows you to estiamte those effects by modeling the conditional distribution of \\(y\\) or \\(E(y|x)\\).\n\nThey have limitations:\n\nThey both are limited to a single experiment. A different policy requires a change in the setup.\nReweighing is simple to apply, but has limitation on the type of policies. They all need to be within the support of \\(X\\)\nModeling the conditional distribution is a more direct approach, but more computationally intensive, specially for obtaining Standard errors."
  },
  {
    "objectID": "adv_class/05uqreg.html#opt-3.-local-approximation-rif-regression",
    "href": "adv_class/05uqreg.html#opt-3.-local-approximation-rif-regression",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Opt 3. Local Approximation: RIF regression",
    "text": "Opt 3. Local Approximation: RIF regression\nThe third approach was first introduced by Firpo, Fortin and Lemieux 2009, as a computationally simple way to analyze how changes in \\(X's\\) affect the unconditional quantiles of \\(y\\).\nThis strategy was later extended to analyze the effects on a myriad of distributional statistics and rank dependent indices, as well as an approach to estimate distributional treatment effects.\nSee Rios-Avila (2020).\nIn contrast with other approaches, it can be used to analyze multiple types of policies without re-estimating the model. However the identification and interpretation needs particular attention.\nIt also allows you to easily make Statistical inference. (except for quantiles…)"
  },
  {
    "objectID": "adv_class/05uqreg.html#opt-3.-from-ground-up",
    "href": "adv_class/05uqreg.html#opt-3.-from-ground-up",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Opt 3. From ground up",
    "text": "Opt 3. From ground up\nReconsider the Original question. How do you capture the effect of changes of distribution of \\(x\\) on the distribution of \\(y\\).\n\\[\n\\Delta v=v(G_y) - v(F_y)\n\\]\nNow, assume that \\(G_y\\) is just marginally different from \\(F_y\\) (different in a very particular way)\n\\[\nG_y(y_i) = (1-\\epsilon)F_y+ \\epsilon 1(y&gt;y_i)\n\\]\nThis function puts just a bit more weight on observation \\(y_i\\). Think of it as “dropping” a new person in the pool.\nIf this is the case, the \\(\\Delta v(y_i)\\) Captures how would the Statistic \\(v\\) changes if the distribution puts just a bit extra weight on 1 observation. (this would be very small)"
  },
  {
    "objectID": "adv_class/05uqreg.html#opt-3.-one-more-thing",
    "href": "adv_class/05uqreg.html#opt-3.-one-more-thing",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Opt 3. One more thing",
    "text": "Opt 3. One more thing\nLets Rescale it:\n\\[\nIF(v,F_y,y_i) =lim_{\\epsilon \\rightarrow 0} \\frac{v(G_y(y_i))-v(F_y)}{\\epsilon}\n\\]\nThe influence function is a measure of direction of change, we should expect the statistic \\(v\\) will have as we change \\(F_y \\rightarrow G_y\\) .\nFrom here the RIF is just \\(RIF(v,F_y,y_i) = v + IF(v,F_y,y_i)\\)\nWhich has some properties:\n\\[\n\\begin{aligned}\n\\int IF(v,F_y,y_i)f_ydy=0 &;\n\\int RIF(v,F_y,y_i)f_y dy=v \\\\\nv(F_y) \\sim N \\left(v(F_y),\\frac{\\sigma^2_{IF}}{N} \\right) &;\n\\int IF^2f_ydy =\\sigma^2_{IF}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/05uqreg.html#opt-3.-rif-regression",
    "href": "adv_class/05uqreg.html#opt-3.-rif-regression",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Opt 3. RIF Regression",
    "text": "Opt 3. RIF Regression\nFirst:\n\\[\nv(F_y) = \\iint RIF(v,F_Y,y_i) f(y|x)f(x)dy = \\int E(RIF(.)|x) f(x)\n\\]\nFrom here is similar to Opt 3. Use some econometric model to estimate \\(E(RIF(.)|X)\\), and use that to make predictions on how \\(v(F_y)\\) would change, when there is a distributional change in \\(X\\).\nRIF-OLS: Unconditional effect!\n\\[\nRIF(v,F_y,y_i) = X\\beta+e  \\ \\rightarrow\\\nE(RIF) = v(F_y) = \\bar X \\beta \\\\\n\\frac{dv(F_y)}{d\\bar X}=\\beta\n\\]\nLogic. When \\(F_x\\) changes, it will change the distribution of \\(F_y\\), which will affect how the statistic \\(v\\) will change. But, we can only consider changes in means! (and Var)"
  },
  {
    "objectID": "adv_class/05uqreg.html#why-it-works-and-why-it-may-not",
    "href": "adv_class/05uqreg.html#why-it-works-and-why-it-may-not",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Why it works, and why it may not",
    "text": "Why it works, and why it may not\nRIF regressions works by using a linear approximation of the statistic \\(v\\) with the changes in \\(F_y\\) which are caused by changes in \\(F_x\\), proxied by changes in \\(\\bar X\\).\n\nChanges at the individual \\(x_i\\) are not interesting (in a population of 1million, what happens to person 99 may not be large enough to matter)\n\nDepending on the model specification, however, we may only be able to identify changes in first and second moments of the distribution of \\(x\\). (Mean and variance).\n-\nHowever, as any linear approximation to a non-linear function, the approximations are BAD when the changes in \\(F_x\\) are too large. The most relevant example…Dummies and treatment!"
  },
  {
    "objectID": "adv_class/05uqreg.html#rif-reg-and-dummies",
    "href": "adv_class/05uqreg.html#rif-reg-and-dummies",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "RIF-Reg and dummies",
    "text": "RIF-Reg and dummies\nDummies are a challenge. At individual or conditional level, we usually consider changes from 0 to 1 (off or on).\n\nFor unconditional effects this is not correct (too large of a change) (No-one treated vs All treated). Thus you need to change the question…Not on and off changes, but Changes in proportion of treated!\n\nVery important. a 1% increase in pop treated is different if current treatment is 10% or 90%.\n\nHowever, its possible to restructure RIF regressions to be partially conditional (Rios-Avila and Maroto 2023) (Combines CQREG with UQREG)\nSimilar problems are experienced if the change in continuous variables is large!\n\nMinor point. How do you construct RIFs? (analytically and Empirically)"
  },
  {
    "objectID": "adv_class/05uqreg.html#example-1",
    "href": "adv_class/05uqreg.html#example-1",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Example",
    "text": "Example\nwebuse dui, clear\n**  Consider the policy change\ngen change_fines= 0.1*(12-fines)\n**  consider average change in fines.Since we are only considering this effect\nsum change_fines\n\nrifhdreg citations fines i.csize college taxes, rif(q(10)) \nest sto m1\nrifhdreg citations fines i.csize college taxes, rif(q(50)) \nest sto m2\nrifhdreg citations fines i.csize college taxes, rif(q(90)) \nest sto m3\n** This are Rescaled to show true effect\nrifhdreg citations fines i.csize college taxes, rif(q(10)) scale(.21048)\nest sto m4\nrifhdreg citations fines i.csize college taxes, rif(q(50)) scale(.21048)\nest sto m5\nrifhdreg citations fines i.csize collegetaxes, rif(q(90)) scale(.21048)\nest sto m6\n\n. esttab m1 m2 m3 m4 m5 m6, se mtitle(q10 q50 q90 r-q10 r-q50 r-q90) compress nogaps\n\n----------------------------------------------------------------------------------------\n                 (1)          (2)          (3)          (4)          (5)          (6)   \n                 q10          q50          q90        r-q10        r-q50        r-q90   \n----------------------------------------------------------------------------------------\nfines         -4.476***    -6.700***    -9.887***    -0.942***    -1.410***    -2.081***\n             (0.491)      (0.493)      (0.978)      (0.103)      (0.104)      (0.206)   \n1.csize            0            0            0            0            0            0   \n                 (.)          (.)          (.)          (.)          (.)          (.)   \n2.csize        4.603***     7.325***     6.370***     0.969***     1.542***     1.341***\n             (0.963)      (0.966)      (1.917)      (0.203)      (0.203)      (0.404)   \n3.csize        6.504***     13.54***     12.97***     1.369***     2.851***     2.729***\n             (0.914)      (0.917)      (1.820)      (0.192)      (0.193)      (0.383)   \ncollege        2.922**      5.948***     9.973***     0.615**      1.252***     2.099***\n             (0.890)      (0.892)      (1.771)      (0.187)      (0.188)      (0.373)   \ntaxes         -3.279***    -3.303***    -8.319***    -0.690***    -0.695***    -1.751***\n             (0.842)      (0.844)      (1.676)      (0.177)      (0.178)      (0.353)   \n_cons          53.71***     81.04***     129.2***     11.30***     17.06***     27.20***\n             (4.964)      (4.977)      (9.880)      (1.045)      (1.048)      (2.080)   \n----------------------------------------------------------------------------------------\nN                500          500          500          500          500          500   \n----------------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/05uqreg.html#how-do-they-compare",
    "href": "adv_class/05uqreg.html#how-do-they-compare",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "How Do they Compare",
    "text": "How Do they Compare"
  },
  {
    "objectID": "adv_class/05uqreg.html#other-considerations",
    "href": "adv_class/05uqreg.html#other-considerations",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Other Considerations",
    "text": "Other Considerations\nRIF Regressions are useful, but again, one must use them with care.\n\nOnly Small changes! Larger changes may be meaningless\n\nExcept for Stata (see rif and rifhdreg), the applications of RIF regressions outside Mean, Variance and Quantiles are non-existent. (paper?)\n\nFor most Common Statistics, RIF’s automatically provide correct Standard errors (which can be Robustized!). In fact, a simple LR can be considered as a special case of RIF’s\n\n\\[\n\\begin{aligned}\nRIF(mean,y_i,F_y) &= y_i \\\\\nRIF(variance,y_i,F_y) &= (y_i-\\bar y)^2 \\\\\nRIF(Q,y_i,F_Y) &= Q_y(\\tau) + \\frac{\\tau-1(y_i \\leq Q_y(\\tau))}{f_Y(y_i)}\n\\end{aligned}\n\\]\nExcept for quantile related functions! (\\(f_y\\) also needs estimation, thus errors!)"
  },
  {
    "objectID": "adv_class/05uqreg.html#section-3",
    "href": "adv_class/05uqreg.html#section-3",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "",
    "text": "Other Considerations\n\nAccounting for “local” unconditional effects beyond means require Center Polynomials:\n\n\\[\nRIF(.,y) = b_0 + b_1 x + b_2 (x-\\bar x)^2+\\varepsilon\n\\]\n\nQuantile treatment effects (on and off) are possible using PC-RIF (When you condition the distribution on just 1 variable)\n\n\\[\nRIF(.,F_{Y|D},y) = b_0 + b_1 D+b_2 x + b_3 (x-\\bar x)^2+\\varepsilon\n\\]"
  },
  {
    "objectID": "adv_class/05uqreg.html#final-words-on-rif",
    "href": "adv_class/05uqreg.html#final-words-on-rif",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Final words on RIF",
    "text": "Final words on RIF\nBecause this implementation uses LR, you can add Multiple Fixed effects as well. (with limitations)\nAnd you can skip LR all together, and model RIF using Other approaches! (which may be even better than OLS)."
  },
  {
    "objectID": "adv_class/07po_ci.html#introduction-what-if",
    "href": "adv_class/07po_ci.html#introduction-what-if",
    "title": "Potential outcomes and Causal Models",
    "section": "Introduction: What if?",
    "text": "Introduction: What if?\nFrom here on, the whole purpose of the methodologies we will dicusess is the analysis of causal effects of some:\n\nPolicy, treatment, experiment, or otherwise event\n\nBut, How is it different from what we did before?\nIt isn’t.\nUnder Exogeneity assumption \\(E(e|X)=0\\), one can make causal effect claims.\n\n\n\n\n\n\nWe seek the truth\n\n\nHow much of the change in outcome is caused by the program alone?\n\n\n\nBut this is not as easy."
  },
  {
    "objectID": "adv_class/07po_ci.html#exampleswhere-is-the-challenge",
    "href": "adv_class/07po_ci.html#exampleswhere-is-the-challenge",
    "title": "Potential outcomes and Causal Models",
    "section": "Examples…Where is the challenge?",
    "text": "Examples…Where is the challenge?\nA few examples for Causal effect questions:\n\nDo minimum wages increase unemployment ?\nDo Conditional cash transfers improve health outcomes in children?\nDo Covid Vaccines help reduce the Spread of Covid?\n\nThese questions are, however, difficult to answer.\n\nHow do you make sure the “treatment” is the Only factor that explains the difference in outcome across groups??\n\nTo do this we need strategies that rule out any other explanation that could “take away” the connection we seek.\n\nWe need to close all back doors, block all alternative explanations, or nuisanse factors"
  },
  {
    "objectID": "adv_class/07po_ci.html#potential-outcomes",
    "href": "adv_class/07po_ci.html#potential-outcomes",
    "title": "Potential outcomes and Causal Models",
    "section": "Potential Outcomes",
    "text": "Potential Outcomes\n\nChoice"
  },
  {
    "objectID": "adv_class/07po_ci.html#the-path-not-taken",
    "href": "adv_class/07po_ci.html#the-path-not-taken",
    "title": "Potential outcomes and Causal Models",
    "section": "The Path not taken",
    "text": "The Path not taken\nThe way we express the \\(TE_i\\), compares counterfactuals. Two possible States of the world, where an allknowing researcher can perfectly identify TE.\nUnfortunately, the same person cannot take both paths, and we cannot see both options. A person is either Treated or Untreated. Thus, the first approach to Casual effects is impossible.\n…\nBut it does provide us with a clue of how to go ahead and analyze Causal effects. We “simply” need to estimate the counterfactual!\nBut before going deeper into how to estimate the counterfactuals And treatment effects some notation"
  },
  {
    "objectID": "adv_class/07po_ci.html#potential-outcomes-notation",
    "href": "adv_class/07po_ci.html#potential-outcomes-notation",
    "title": "Potential outcomes and Causal Models",
    "section": "Potential Outcomes: Notation",
    "text": "Potential Outcomes: Notation\n\n\\(i\\) will represent a unit. Person, city, country, school, classroom, etc\n\\(D\\) will indicate the treatment Status of a unit. \\(D=1\\) means is treated, and \\(D=0\\) is untreated.\nEach unit has two potential outcomes \\(Y_i(D=1)\\) and \\(Y_i(D=0)\\)\nAll units have only one effective or realized outcome: \\(Y_i\\), which is what we observe, and depends on their treatment status:\n\n\\[Y_i=(1-D_i)*Y_i(0)+D_i*Y_i(1)\\]\n\nUnit Specific causal effect is the difference between potential outcomes: \\[\\delta_i = Y_i(1)-Y_i(0)\\]\n\\(\\pi\\) is the proportion of treated units"
  },
  {
    "objectID": "adv_class/07po_ci.html#parameters-of-interest",
    "href": "adv_class/07po_ci.html#parameters-of-interest",
    "title": "Potential outcomes and Causal Models",
    "section": "Parameters of Interest",
    "text": "Parameters of Interest\nAssume we can see the true USCE (unit specific casual effect) for all units. In addition to their Treatment Status.\nThere are three parameters one might be interested in analyzing:\n\\[\n\\begin{aligned}\nATE &= E(\\delta_i) \\\\\nATT &= E(\\delta_i|D_i=1) \\\\\nATU &= E(\\delta_i|D_i=0)\n\\end{aligned}\n\\]\nIn general, this three estimands may tell very different stories.\nLets Put some numbers here"
  },
  {
    "objectID": "adv_class/07po_ci.html#simulating-effects-stata",
    "href": "adv_class/07po_ci.html#simulating-effects-stata",
    "title": "Potential outcomes and Causal Models",
    "section": "Simulating effects Stata",
    "text": "Simulating effects Stata\n\n\nCode\nclear\nset linesize 255\nset seed 101\nset obs 1000\ngen y0 = rnormal(5)\ngen t  = rnormal(0.0,0.5)\ngen y1 = y0+t\n** Assume only those with t&gt;0 take treatment\ngen trt =(t&gt;0)\ngen y = y0 * (1-trt) +  y1 * (trt)\nformat y0 y1 y t %4.3f \nlist in 1/10, sep(0) clean\n** For Everyone 100 obs\ntabstat t, by(trt)\n\n\n\n\n\nNumber of observations (_N) was 0, now 1,000.\n\n          y0        t      y1   trt       y  \n  1.   5.254   -0.801   4.453     0   5.254  \n  2.   4.997    0.039   5.035     1   5.035  \n  3.   2.608   -0.490   2.118     0   2.608  \n  4.   6.280   -0.579   5.700     0   6.280  \n  5.   5.761    0.090   5.850     1   5.850  \n  6.   6.132    0.211   6.342     1   6.342  \n  7.   4.928    0.153   5.081     1   5.081  \n  8.   4.377   -0.073   4.304     0   4.377  \n  9.   3.142    0.427   3.569     1   3.569  \n 10.   6.050   -0.332   5.718     0   6.050  \n\nSummary for variables: t\nGroup variable: trt \n\n     trt |      Mean\n---------+----------\n       0 | -.3984745\n       1 |  .3748617\n---------+----------\n   Total | -.0187664\n--------------------"
  },
  {
    "objectID": "adv_class/07po_ci.html#but-there-is-only-1",
    "href": "adv_class/07po_ci.html#but-there-is-only-1",
    "title": "Potential outcomes and Causal Models",
    "section": "But there is only 1",
    "text": "But there is only 1\nBut, we never see potential outcomes, nor unit specific effects.\nThe most naive estimator is to just estimate the mean difference in “post-treatment” outcome after treatment was in place. But that would be very biased!\n\n\nCode\ngen yy0 = y if trt==0\ngen yy1 = y if trt==1\nlist y yy1 yy0 trt in 1/10\nreg  y trt\n\n\n(491 missing values generated)\n(509 missing values generated)\n\n     +-----------------------------------+\n     |     y        yy1        yy0   trt |\n     |-----------------------------------|\n  1. | 5.254          .   5.254051     0 |\n  2. | 5.035   5.035442          .     1 |\n  3. | 2.608          .   2.608155     0 |\n  4. | 6.280          .    6.27964     0 |\n  5. | 5.850   5.850478          .     1 |\n     |-----------------------------------|\n  6. | 6.342    6.34237          .     1 |\n  7. | 5.081   5.080995          .     1 |\n  8. | 4.377          .   4.376629     0 |\n  9. | 3.569   3.568727          .     1 |\n 10. | 6.050          .   6.049989     0 |\n     +-----------------------------------+\n\n      Source |       SS           df       MS      Number of obs   =     1,000\n-------------+----------------------------------   F(1, 998)       =     49.49\n       Model |  49.4759442         1  49.4759442   Prob &gt; F        =    0.0000\n    Residual |  997.714103       998   .99971353   R-squared       =    0.0472\n-------------+----------------------------------   Adj R-squared   =    0.0463\n       Total |  1047.19005       999  1.04823829   Root MSE        =    .99986\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         trt |   .4449359   .0632467     7.03   0.000      .320824    .5690477\n       _cons |   4.988793   .0443179   112.57   0.000     4.901826     5.07576\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/07po_ci.html#bias-direction",
    "href": "adv_class/07po_ci.html#bias-direction",
    "title": "Potential outcomes and Causal Models",
    "section": "Bias Direction",
    "text": "Bias Direction\n\\[\n\\begin{aligned}\nE(Y_i|D_i=1)-E(Y_i|D_i=0) &= ATE  \\\\\n&  +E(Y(0)|D=1)-E(Y(0)|D=0) \\\\\n& +(1-\\pi)(ATT-ATU)\n\\end{aligned}\n\\]\nIntuition: Simple Difference will be biases because\n\nThere could be a selection bias (one group baseline outcome is different from the other)\nTreatment Heterogeneity. Some groups are affected differently from others\n\nFrom these two problems, the second one is easier to handle (either concentrate on ATT or ATU).\nThe first one, however, requires using strategies to be able to account for selection bias."
  },
  {
    "objectID": "adv_class/07po_ci.html#independence-assumption",
    "href": "adv_class/07po_ci.html#independence-assumption",
    "title": "Potential outcomes and Causal Models",
    "section": "Independence assumption",
    "text": "Independence assumption\nWhile the Simple mean estimator is most likely to be biased, under Independence assumption, it may still work:\n\\[\nY(1),Y(0) \\perp D\n\\]\nThis means that Treatment Status should NOT depend on the potential outcomes.\nIn other words, there shouldnt be any differneces in the potential outcomes before or after treatment takes place.\nThis eliminates the selection bias. And group Heterogeneity.\n\\[\n\\begin{aligned}\nATT - ATU &= E(Y|D=1) -  E(Y(0)|D=1) \\\\\n&- E(Y(1)|D=0) -  E(Y|D=0)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/07po_ci.html#sutva",
    "href": "adv_class/07po_ci.html#sutva",
    "title": "Potential outcomes and Causal Models",
    "section": "SUTVA",
    "text": "SUTVA\nStable Unit Treatment value assumption\nThis is a strong assumption that is still required to estimate of treatment effects.\nIt assumes that:\n\nTreatment is Homogenous. Same intensity, quality, type of treatment among all treated.\nThere are no spill overs. Your Treatment Status effects you and you only, and you are only affected by your treatment status. No externalities nor Spillovers.\nAlso, there are no general equibrium effects\nAnd NO Anticipation.\n\nThis assumptions namely guaranties that when a unit is not treated, its/his/her outcome will not change."
  },
  {
    "objectID": "adv_class/07po_ci.html#narrowing-down-the-problem",
    "href": "adv_class/07po_ci.html#narrowing-down-the-problem",
    "title": "Potential outcomes and Causal Models",
    "section": "Narrowing down the problem",
    "text": "Narrowing down the problem\n\nIndividual level effects are impossible to identify. We only observe one outcome at a time. (not both)\nIt is possible to identify causal effects on groups (treated, not-treated, kind-of-treated). But..\nSimple Mean difference will not identify causal effects, unless Independence and Sutva assumptions hold\n\nThis however, suggests a path. Constructing good counterfactuals can help idenfiying the Causal effects.\nGoal:\n\nIdentify a control/comparison group that is statistically identical to the treated group, except for the Treatment Status"
  },
  {
    "objectID": "adv_class/07po_ci.html#the-gold-standard-randomized-control-trials",
    "href": "adv_class/07po_ci.html#the-gold-standard-randomized-control-trials",
    "title": "Potential outcomes and Causal Models",
    "section": "The Gold Standard: Randomized Control Trials",
    "text": "The Gold Standard: Randomized Control Trials\n\n\n\n\n\n\nTo keep in mind\n\n\nSearching for good controls doesnt require having access to perfect “clones”. However, in average, we need groups (T vs UT) that are very similar to each other.\n\n\n\nIn general, research designed is guided by the rules of program or treatment assignment on participants.\nWhen researchers have control on the assingment rules, the best approach is to design a randomized control trial.\nIn an RCT, randomized assigment, eliminates any selection-bias problems (although SUTVA remains as an assumption)"
  },
  {
    "objectID": "adv_class/07po_ci.html#rct-and-selection-problems",
    "href": "adv_class/07po_ci.html#rct-and-selection-problems",
    "title": "Potential outcomes and Causal Models",
    "section": "RCT and Selection Problems",
    "text": "RCT and Selection Problems\nConsider the example of the Health Impacts of Hospitals.\n\nHospitals (or health care) should improve health of individuals. but\nOnly unhealthy people will use Health care services. Selection bias\nThus It may look that Hospitals Hurt people’s health becuase those who used it have lower health than those who dont: \\[\nE(Y|D=1)-E(Y|D=0) = ATT + E(Y(0)|D=1)-E(Y(0)|D=0)\n\\]\n\nSo even if Health services help those in need (ATT). If the selection bias is large, Naive estimations may suggest Hospitals are harmful.\nThis is a problem caused because Treatment(going to the Hospital) is affected by pre-conditions or potential treatment outcomes."
  },
  {
    "objectID": "adv_class/07po_ci.html#stata-example",
    "href": "adv_class/07po_ci.html#stata-example",
    "title": "Potential outcomes and Causal Models",
    "section": "Stata Example",
    "text": "Stata Example\n\n\nCode\ngen trt2=rnormal()&gt;0\ngen yrct = y0 * (1-trt2) +  y1 * (trt2)\ntabstat t y0 y1 yrct, by(trt2)\nreg yrct trt2\n\n\n\nSummary statistics: Mean\nGroup variable: trt2 \n\n    trt2 |         t        y0        y1      yrct\n---------+----------------------------------------\n       0 | -.0097507  5.061626  5.051875  5.061626\n       1 |  -.027891  4.984309  4.956418  4.956418\n---------+----------------------------------------\n   Total | -.0187664  5.023199  5.004433  5.009338\n--------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     1,000\n-------------+----------------------------------   F(1, 998)       =      2.64\n       Model |  2.76706003         1  2.76706003   Prob &gt; F        =    0.1046\n    Residual |  1046.22695       998   1.0483236   R-squared       =    0.0026\n-------------+----------------------------------   Adj R-squared   =    0.0016\n       Total |  1048.99401       999  1.05004406   Root MSE        =    1.0239\n\n------------------------------------------------------------------------------\n        yrct | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        trt2 |  -.1052076   .0647568    -1.62   0.105    -.2322827    .0218675\n       _cons |   5.061626   .0456524   110.87   0.000      4.97204    5.151212\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/07po_ci.html#internal-vs-external-validity",
    "href": "adv_class/07po_ci.html#internal-vs-external-validity",
    "title": "Potential outcomes and Causal Models",
    "section": "Internal vs External Validity",
    "text": "Internal vs External Validity\nRCTs are not the only strategy that allows you to control the Rules of Treatment. In Experimental Economics this is done quite often\n\nyou select your sample (of students), and randomly assigned treatments of interest (Experimental design).\n\nThere are, however, further considerations to be taken:\n\n\n\n\n\n\nInternal Validity\n\n\nThe estimated Impact is net of all other confunding factors (Random assigment)\n\n\n\n\n\n\n\n\n\nExternal Validity\n\n\nThe estimated impact can be generalized to the population in general. (Random Sample)\n\n\n\n\n\n\n\n\n\nHow to Randomize\n\n\nDepends mostly on how reasonable is to mantain SUTVA assumption"
  },
  {
    "objectID": "adv_class/07po_ci.html#examples",
    "href": "adv_class/07po_ci.html#examples",
    "title": "Potential outcomes and Causal Models",
    "section": "Examples",
    "text": "Examples\n\n\n\n\n\n\nCCT and Education in Mexico\n\n\nProgresa/Prospera is a CCT program for poor mothers based on children school enrollment to suport education attainment.\nEligibility was based on Census data on Poverty levels and Baseline Data collection. But during phase-in period, only 2/3 if localities were selected to receive the Transfer\n\n\n\n\n\n\n\n\n\nWater and Sanitation Intervention in Bolivia\n\n\nIn 2012 IADB and Bolivian Goverment implemented a random assigment of water/sanitation interventions in Small Rural Communities.\nFrom 369 eliginle communities, 182 were selected at random for the program implementation, via public lotteries, constraining on Community Size"
  },
  {
    "objectID": "adv_class/07po_ci.html#how-to-analyze-the-data",
    "href": "adv_class/07po_ci.html#how-to-analyze-the-data",
    "title": "Potential outcomes and Causal Models",
    "section": "How to Analyze the Data",
    "text": "How to Analyze the Data\n\nVerify Data Balance: Even if treatment was assigned at random, it is important to verify of groups remain comparable. (Thus avoid compossition effects)\nMean Difference: Because of Random Assigment, one could use simple mean differences to estimate ATE\nConsider using controls: Under RA, controls will not affect the outcome, but may improve precision: \\(y_i = \\alpha_0 + \\tau D_i + x_i\\beta + \\varepsilon_i\\).\n\nBut controls should not be affected by the treatment itself (thus should be pre-treatment)\n\nMay consider falsification tests\nAnd Other Robustness test: Outliers, or distributional impacts may be of interest"
  },
  {
    "objectID": "adv_class/07po_ci.html#background",
    "href": "adv_class/07po_ci.html#background",
    "title": "Potential outcomes and Causal Models",
    "section": "Background",
    "text": "Background\nProgresa is a Cash Transfer Program designed to increase School Enrollment among the poor, minimizing desincentives to work.\nThis program provided Grants to families whos children attended school for atleast 85% of the school year, covering between 50/75% of school cost.\nWhile there were 495 localities that were eligible to benefit from the program, only 314 were randomly selected to start reciving resources for the first 2 years. With the unselected localities being treated a couple of years later.\n\nThe program continued beyond the original scope of the policy, now its known as Progresa/Oportunidades"
  },
  {
    "objectID": "adv_class/07po_ci.html#method",
    "href": "adv_class/07po_ci.html#method",
    "title": "Potential outcomes and Causal Models",
    "section": "Method",
    "text": "Method\nGoal. Estimate the impact of Progresa (P) on Enrollment (S) \\[\nS_i = a_0 +a_1 P_i + a_2 E_i + a_3 P_iE_i + \\delta Enrolled_c + \\beta X + e_i\n\\]\n\\(P_i=1\\) if the comunity is eligible\n\\(E_i=1\\) if the child is Poor\n\\(P_i * E_i\\) the impact on Poor Chilren in eligible communities.\nModel can be estimated Separately (5 years), or using pooled data\nThis is a kind of DIDID model. However, we could consider it as a simple mean comparison between those Effectively treated and those untreated."
  },
  {
    "objectID": "adv_class/07po_ci.html#differences-in-characteristics",
    "href": "adv_class/07po_ci.html#differences-in-characteristics",
    "title": "Potential outcomes and Causal Models",
    "section": "Differences in Characteristics",
    "text": "Differences in Characteristics"
  },
  {
    "objectID": "adv_class/07po_ci.html#raw-differences",
    "href": "adv_class/07po_ci.html#raw-differences",
    "title": "Potential outcomes and Causal Models",
    "section": "Raw Differences",
    "text": "Raw Differences\n\nPoor HH"
  },
  {
    "objectID": "adv_class/07po_ci.html#with-controls",
    "href": "adv_class/07po_ci.html#with-controls",
    "title": "Potential outcomes and Causal Models",
    "section": "With Controls",
    "text": "With Controls"
  },
  {
    "objectID": "adv_class/07po_ci.html#other-outcomes",
    "href": "adv_class/07po_ci.html#other-outcomes",
    "title": "Potential outcomes and Causal Models",
    "section": "Other Outcomes",
    "text": "Other Outcomes"
  },
  {
    "objectID": "adv_class/09iv.html#recap-pos-and-rcts",
    "href": "adv_class/09iv.html#recap-pos-and-rcts",
    "title": "Instrumental Variables",
    "section": "Recap: PO’s and RCT’s",
    "text": "Recap: PO’s and RCT’s\n\nQuick Recap. The goal of the methodologies we are covering is to identify treatment effects.\nIn the PO framework, that is done by simply comparing a group with itself, in two different States (treated vs untreated)\nSince this is impossible, the next best solution is using RCT. Individuals are randomized, and assuming every body follows directions, we can identify treatment effects of the experiments.\nBut only if the RCT is well executed! Sometimes even that may fail"
  },
  {
    "objectID": "adv_class/09iv.html#instrumental-variables",
    "href": "adv_class/09iv.html#instrumental-variables",
    "title": "Instrumental Variables",
    "section": "Instrumental Variables",
    "text": "Instrumental Variables\n\nWhile here discussed 3rd, the second best approach to identify Treatment effects is by using Instrumental variables.\nIn fact with a Good-Enough instrument, one should be able to identify ANY causal effect. Assuming such IV exists.\n\nbut how?\n\nIf the instrument is good, it may create an exogenous variation, which will allow us to identify Treatment effects by looking ONLY at those affected by the treatment!\nUsing the external variation, we can Estimate TE comparing two groups who are identical in every aspect, except being expose to the Instrument, because they were exposed to the instrument. The randomization comes Because of the IV!"
  },
  {
    "objectID": "adv_class/09iv.html#cannonical-iv",
    "href": "adv_class/09iv.html#cannonical-iv",
    "title": "Instrumental Variables",
    "section": "Cannonical IV",
    "text": "Cannonical IV\nAs we have mentioned, the estimation of TE require that we identify two groups of individuals with mostly similar (if not identical) characteristics. This include unobserved characteristics.\nIf the latter is not true, we have a problem of confunders or Endogeneity. But why?\nConsider the following diagram\n\n\n\n\n\n\nHere the effect of \\(D\\) on \\(Y\\) is direct, because there is nothing else that would get people confuse why treatment affects outcome\n\n\n\n\n\n\nflowchart LR\n  e(error) --&gt; Y(Outcome)  \n  D(Treatment) --&gt; Y(Outcome)  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere the effect of \\(D\\) on \\(Y\\) is not as clear, because there is an additional factor \\(v\\) that affects \\(D\\) and \\(Y\\) (is in the way)\n\n\n\n\n\n\nflowchart LR\n  e( error ) --&gt; Y(Outcome)  \n  D( Treatment ) --&gt; Y(Outcome)\n  v(unobserved) -.-&gt; D(Treatment)  \n  v(unobserved) -.-&gt; Y(Outcome)"
  },
  {
    "objectID": "adv_class/09iv.html#cannonical-iv-1",
    "href": "adv_class/09iv.html#cannonical-iv-1",
    "title": "Instrumental Variables",
    "section": "Cannonical IV",
    "text": "Cannonical IV\nHere is where a good instrument comes into play.\n\nNot everything in \\(D\\) is affected by \\(v\\). Some may, but some may be trully exogenous. What if we have an instrument that helps you ID this:\n\n\n\n\n\n\nflowchart LR\ne( error ) --&gt; Y(Outcome)  \n  subgraph Treatment\n    D1(Exog) \n    D2(Endog)\n  end\n  \n  D1( Exog ) --&gt; Y(Outcome)\n  D2( Endog ) --&gt; Y(Outcome)\n  Z(Instrument) --&gt; D1  \n  v(unobserved) -.-&gt; D2\n  \n  v(unobserved) -.-&gt; Y(Outcome)  \n\n\n\n\n\n\n\nBy Isolating those affected by the Instrument Alone, we do not need to worry about endogeneity anymore."
  },
  {
    "objectID": "adv_class/09iv.html#properties",
    "href": "adv_class/09iv.html#properties",
    "title": "Instrumental Variables",
    "section": "Properties",
    "text": "Properties\nInstrumental variables should have at the very list 2 Properties\n\nThe instrumental variable \\(Z\\) should not be correlated with the model error (Validity).\nBut, it should explain the treatment Itself \\(D\\) (Relevance).\n\nFailure of (1) may reintroduce problems of endogeneity. Faiture of (2) will make the instrument Irrelevant."
  },
  {
    "objectID": "adv_class/09iv.html#how-does-it-work",
    "href": "adv_class/09iv.html#how-does-it-work",
    "title": "Instrumental Variables",
    "section": "How does it work",
    "text": "How does it work\nConsider the following.\n\nPeople who study more, tend to earn higher wages\nPeople with high ability tend to study more.\nPeople with high ability, also earn higher wages.\n\nDoes Studying more generate higher wages?\nInstrument. We create a lottery that provides some people resources to pay for their education. This gives them a chance to study more (regardless of ability). \\[Z \\rightarrow D\\]"
  },
  {
    "objectID": "adv_class/09iv.html#section",
    "href": "adv_class/09iv.html#section",
    "title": "Instrumental Variables",
    "section": "",
    "text": "So, we know the instrument was Random. We can analyze how much outcome increases among those benefited by the Lottery.\n\\[E(W|Z=1)-E(W|Z=0)\\]\n\nThis is often called the reduced form effect.\nIn principle, \\(Z\\) only affects wages because of education. So looking at this differences should be similar to a treatment effect of Lotteries.\nThese are also known as Intention to treatment effect. Which will bias towards zero, because not everyone will effectively make use of the opporunities"
  },
  {
    "objectID": "adv_class/09iv.html#section-1",
    "href": "adv_class/09iv.html#section-1",
    "title": "Instrumental Variables",
    "section": "",
    "text": "In othe words, not everyone will Study more…So we can see if the lotery had that effect.\n\\[E(S|Z=1)-E(S|Z=0)\\]\nThis is the equivalent to the first stage. Where we measure the impact of the “instrument/lottery” on Education (to see, say, relevance)\nFinally, the TE is given by the Ratio of thes two\n\\[TE=\\frac{E(W|Z=1)-E(W|Z=0)}{E(S|Z=1)-E(S|Z=0)}\n\\]\nThis is also known as the Wald Estimator. How much of the changes in wages is due to changes in the “# treated”"
  },
  {
    "objectID": "adv_class/09iv.html#some-commnents",
    "href": "adv_class/09iv.html#some-commnents",
    "title": "Instrumental Variables",
    "section": "Some commnents",
    "text": "Some commnents\n\nThis was an example of a binary instrument, which was assigned at random.\nIn fact, this particular scenario is typical byproduct of “failed” RCTs!\n\nPartially failed RCTs: Not every body selected WAS treated\n\n\nConsider the following:\n\nWe make the RCT above giving bouchers to People so they Study more.\nBut, not everybody uses the bouchers:\n\nSome use them and study more.\nSome decide to not use them.\n\n\nComparing Wages among those who receive will only provide you the “intention to treat” effect. (Reduced form)\nBecause of imperfect compliance we need to “readjust/inflate” our TE estimate."
  },
  {
    "objectID": "adv_class/09iv.html#more-comments",
    "href": "adv_class/09iv.html#more-comments",
    "title": "Instrumental Variables",
    "section": "More Comments",
    "text": "More Comments\n\nIn this scenario the Reduced form and second stage can be estimated by just comparing means, because the treatment was randonmized.\n\nIn other words, something you really want is an instrument that is as good as random.\n\nThe effect we capture is a LOCAL treatment effect (LATE).\n\nHowever, it could be an ATE if:\n\nThe effect is homogenous for everyone.\nThe people affected by the treatment is a representative of the population\n\nIt all boils down to identifying who is or might be affected by the treatment.\nFor now, lets assume effects are Homogenous (So we get ATEs)"
  },
  {
    "objectID": "adv_class/09iv.html#who-are-affected-and-who-are-not",
    "href": "adv_class/09iv.html#who-are-affected-and-who-are-not",
    "title": "Instrumental Variables",
    "section": "Who Are Affected and who are not?",
    "text": "Who Are Affected and who are not?\nEven if we are able to identify ATEs, its important to understand who can be affected by the instrument, because the population is generally selected in 3 groups\n\nnever takers & always takers: These are the individuals who would have never done anything different than their normal.\n\nPerhaps their likelihood was already too low (or high) to be affected.\n\nCompliers: These are the ones who, given they receive “instrument”, they comply and follow up. We use their variation for analysis.\nDefiers: These are the ones who, given Z, will do the oposite. We cannot differentiate them from Compliers, so they will affect how treatment is estimated.\n\nWe do not want to have defiers!"
  },
  {
    "objectID": "adv_class/09iv.html#extension-1-continuous-instrument",
    "href": "adv_class/09iv.html#extension-1-continuous-instrument",
    "title": "Instrumental Variables",
    "section": "Extension 1: Continuous Instrument",
    "text": "Extension 1: Continuous Instrument\nThe Wald estimator is for the simplest case of binary treatment. However, if the treatment is continuous, one could modify the IV estimator as follows:\n\\[\n\\delta_{IV} = \\frac{cov(y,z)}{cov(d,z)}\n\\]\nThe logic remains. We are trying to see how variation in the outcome related to Z reates to changes in treatment because of Z.\nThe treatment here is very small (Small changes in d). The intuition is that we are averaging the variation in the outcome across all Zs to estimate the effect."
  },
  {
    "objectID": "adv_class/09iv.html#extension-2-controls",
    "href": "adv_class/09iv.html#extension-2-controls",
    "title": "Instrumental Variables",
    "section": "Extension 2: Controls",
    "text": "Extension 2: Controls\nAdding controls to the model is also straight forward, and you have quite a few options for it\n\nAdding exogenous controls may help improving model precision, even if instrument was randomized. The easiest way to do this is by applying the 2sls procedure (among others)\n\n\\[\n\\begin{aligned}\n1st: d = z\\gamma_z + x\\gamma_x + e_1  \\\\\n2nd: y = x\\beta_x + \\delta \\hat d+ e_2\n\\end{aligned}\n\\]\n\nThe 1st stage “randomizes” instrument to measure the effect on treatment.\nThe 2nd stage uses predicted values of the first to see what the impact on the outcome will be.\nThis works because \\(\\hat d\\) is exogenous, “carrying over” exogenous changes in the treatment."
  },
  {
    "objectID": "adv_class/09iv.html#extension-2-controls-1",
    "href": "adv_class/09iv.html#extension-2-controls-1",
    "title": "Instrumental Variables",
    "section": "Extension 2: Controls",
    "text": "Extension 2: Controls\nOne can also think of the approach as a pseudo Wald Estimator, with continuous variables:\n\\[\n\\begin{aligned}\n1st: d &= \\gamma_z * z + x\\gamma_x + e_1  \\\\\nrd:  y &= \\beta_z  * z+ x\\beta_x + e_2 \\\\\nATE &=\\frac{\\beta_z}{\\gamma_z}=\\frac{cov(y,\\tilde z)}{cov(d,\\tilde z)}\n\\end{aligned}\n\\]\nThis compares average changes in the outcome to average changes in the treatment."
  },
  {
    "objectID": "adv_class/09iv.html#extension-3-multiple-endogenous-variables",
    "href": "adv_class/09iv.html#extension-3-multiple-endogenous-variables",
    "title": "Instrumental Variables",
    "section": "Extension 3: Multiple Endogenous Variables",
    "text": "Extension 3: Multiple Endogenous Variables\nAlthough less common in Causal Analysis perspective, in other frameworks one may to consider more than 1 instrument or using instrument interactions. In these cases one still has two alternatives\n\n2SLS: One can model more than one endogenous variable at the same time, simply substituting the predicted values in the main regression. When using interactions, or polynomials, each will need its own first stage regression.\nControl function approach. In contrast with the “prediction subtstitution” approach, this method suggests using a “residual inclusion approach”. This controls for endogeneity directly. If there is only one endogenous variable (with interactions of polynomials) only one model is needed.\n\nIn the first case, you need at least 1 instrument per regression. Even if its just a transformation of the original variable\nIn the second case, you need at least 1 instrument per endogenous variable."
  },
  {
    "objectID": "adv_class/09iv.html#instrument-validity",
    "href": "adv_class/09iv.html#instrument-validity",
    "title": "Instrumental Variables",
    "section": "Instrument Validity",
    "text": "Instrument Validity\nAs mentioned earlier, Instruments require to fullfill two conditions:\n\nRelevant. They need to be Strongly related to the endogenous variable\nExogenous. instruments should not and cannot be endogenous. In fact, you want instruments that are as good as random, thus not defined by the “system” in anyway."
  },
  {
    "objectID": "adv_class/09iv.html#iv-validity-exogeneity",
    "href": "adv_class/09iv.html#iv-validity-exogeneity",
    "title": "Instrumental Variables",
    "section": "IV Validity: Exogeneity",
    "text": "IV Validity: Exogeneity\nUnfortunately, for most cases, this assumption is not testable, because we do not observe the model unobservables, thus dont know if \\(z\\) is related to those unobserved components.\nWhile most efforts for these are done through model design, or argumentation, there are at least 2 options to verify the exogeneity\n\nIf truly exogenous, the instrument should be as good as random. Thus controls shouldnt be affected by the instrument. (Balance test)\nOtherwise, one could test for exogeneity only by comparing Estimates across different IV’s. Different results may suggest instruments are invalid.\n\nRun a regression of Residuals from the main model against all exogenous variables plus other instruments.\n\n\nNote: Unless the instrument was randomized, assumed is going to be slighly endogenous."
  },
  {
    "objectID": "adv_class/09iv.html#iv-validity-strength",
    "href": "adv_class/09iv.html#iv-validity-strength",
    "title": "Instrumental Variables",
    "section": "IV Validity: Strength",
    "text": "IV Validity: Strength\nThe only thing we could probably do is try to analyze model strength. How much does the instrument affect treatment take up? is the effect marginal? or a large effect?\nWeaker instruments may create larger problems on the analysis because:\n\nWith weaker instruments, the precision of the estimator drops substantially.\nWith weaker instruments, any “endogeneity” problem (even due to randomness) will generate a bias\n\n\nStock and Yogo (2005) suggest and F~13.9 (or higher) for a 5% bias\nLee, et al (2020) suggest you need even higher F’s if you want to avoid problems with CI\n\n\nWith weak instruments, distribution of beta coefficients will no longer be normal!"
  },
  {
    "objectID": "adv_class/09iv.html#iv-strength",
    "href": "adv_class/09iv.html#iv-strength",
    "title": "Instrumental Variables",
    "section": "IV Strength",
    "text": "IV Strength\n\n\nCode\ncapture program drop simx\nprogram simx, eclass\n    clear\n    set obs 500\n    gen z=rnormal()&gt;0\n    gen u1=rnormal()\n    gen u2=rnormal()\n    gen u3=rnormal()\n    forvalues i = 1/5 {\n        gen d`i' = ((-0.5+z) + (u1 + u2)*0.5*`i')&gt;0\n        gen y`i' = 1 + d`i'+u3+u2\n    }\n    forvalues i = 1/5 {\n        reg d`i' z\n        matrix b`i' =(_b[z]/_se[z])^2\n        ivregress  2sls y`i' (d`i'=z)\n        matrix b`i' = b`i',_b[d`i'],_se[d`i'],_b[d`i']/_se[d`i']\n        matrix colname b`i'=f_stat beta beta_se beta_t\n        matrix coleq b`i'=md`i'\n    }\n    matrix b=b1\n    forvalues i = 2/5 {\n        matrix b=b,b`i'\n    }\n    ereturn post b\nend\n\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nmd1_b_f_stat |        494    189.9262    38.63693   101.8865   337.4117\n  md1_b_beta |        494    .9912474    .2289997   .1758122    1.64053\nmd1_b_beta~e |        494    .2440663    .0235266   .1867024   .3293121\n-------------+---------------------------------------------------------\nmd2_b_f_stat |        494    43.33591    13.76347   12.07967   101.6624\n  md2_b_beta |        494    .9717824    .4421832  -.8390987   2.327243\nmd2_b_beta~e |        494    .4711093    .0945389   .3032212   .9873207\n-------------+---------------------------------------------------------\nmd3_b_f_stat |        494    19.25043     8.81203   2.051613   61.55846\n  md3_b_beta |        494    .9354647    .6820819  -1.964163   2.996998\nmd3_b_beta~e |        494    .7430147    .2493365    .383426    2.16554\n-------------+---------------------------------------------------------\nmd4_b_f_stat |        494    11.19082    6.428525   .0483399   38.44057\n  md4_b_beta |        494    .8711808    .9987467  -4.383311   5.634943\nmd4_b_beta~e |        494    1.135114    1.244907   .4609722   25.16503\n-------------+---------------------------------------------------------\nmd5_b_f_stat |        494    7.522731    5.144063   .0544148   28.23535\n  md5_b_beta |        494    .7696057    1.443986  -6.293723   8.951643\nmd5_b_beta~e |        494     1.69479    1.866909    .530196    18.3837"
  },
  {
    "objectID": "adv_class/09iv.html#iv-strength-bias-distribution",
    "href": "adv_class/09iv.html#iv-strength-bias-distribution",
    "title": "Instrumental Variables",
    "section": "IV Strength: Bias distribution",
    "text": "IV Strength: Bias distribution\n\n\nCode\nuse resources/simiv.dta, clear\nforvalues i = 1/5 {\n  qui:sum md`i'_b_beta\n  gen new`i'=(md`i'_b_beta-1)/r(sd)\n}\nset scheme white2\ncolor_style tableau\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new1, name(m1, replace) , legend(off)\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new2, name(m2, replace) , legend(off)\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new3, name(m3, replace) , legend(off)\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new4, name(m4, replace) , legend(off)\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new5, name(m5, replace) , legend(off)\ngraph combine m1 m2 m3 m4 m5, col(3) xcommon ycommon\ngraph export resources/cmb.png, width(1500)  replace"
  },
  {
    "objectID": "adv_class/09iv.html#iv-strength-solution-weakiv",
    "href": "adv_class/09iv.html#iv-strength-solution-weakiv",
    "title": "Instrumental Variables",
    "section": "IV Strength Solution: weakiv",
    "text": "IV Strength Solution: weakiv\n\nWeak IV’s are a problem in the sense that it may induce bias on the estimated coefficients, but also that it may affect how Standard Errors are estimated.\n\nThe distribution of the Statistic is no longer normal\n\nOne solution, in this case, is at least adjusting SE and CI So they better reflect the problem.\nIn Stata, this can be done with weakiv (ssc install weakiv)\nAt the end, however, if you weak instruments, you may be able to correct of potential biases, but you may need to get more data, or better instruments"
  },
  {
    "objectID": "adv_class/09iv.html#late-local-average-treatement-effect",
    "href": "adv_class/09iv.html#late-local-average-treatement-effect",
    "title": "Instrumental Variables",
    "section": "LATE: Local Average Treatement Effect",
    "text": "LATE: Local Average Treatement Effect\nUp to this point, we imposed the assumption that TE were homogenous. Thus, IV could identify Treatment effects for everyone. (Average Treatment effect)\nHowever, not everyone may be affected by the instrument, only by the compliers.\nTwo ways of thinking about it:\n\nNot everybody is affected by the instrument. (you have the always and never takers)\nthe instrument was never suppoused to affect certain groups!\n\nSo, IV will identify TE for the compliers only.\nBecause of this, using different instruments may actually identify different effects, based on which population was affected.\nOverid tests may fail in this case."
  },
  {
    "objectID": "adv_class/09iv.html#simulation-example",
    "href": "adv_class/09iv.html#simulation-example",
    "title": "Instrumental Variables",
    "section": "Simulation Example:",
    "text": "Simulation Example:\n\n\nCode\nclear\nset obs 10000\ngen sex = rnormal()&gt;0\ngen z1 = rnormal()&gt;0\ngen z2 = rnormal()&gt;0\ngen e_1 =rnormal()\ngen e_2 =rnormal()\ngen e_3 =rnormal()\ngen D =(z1*(sex==0) + z2*(sex==1) + (e_1 + e_2)*.5)&gt;0\ngen Ds =(  (e_1 + e_2)*.5)&gt;0\ngen y = 1 + D*(sex==0) +2*D*(sex==1)+e_3+e_2\n\n\nNumber of observations (_N) was 0, now 10,000.\n\n\n\n\nCode\n%%echo\nivregress 2sls y (D=z1)\nivregress 2sls y (D=z2)\nivregress 2sls y (D=z1 z2)\n\n\n\n. ivregress 2sls y (D=z1)\n\nInstrumental variables 2SLS regression            Number of obs   =     10,000\n                                                  Wald chi2(1)    =      68.61\n                                                  Prob &gt; chi2     =     0.0000\n                                                  R-squared       =     0.2706\n                                                  Root MSE        =     1.5526\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           D |   1.170842   .1413543     8.28   0.000     .8937923    1.447891\n       _cons |   1.233464   .1015275    12.15   0.000     1.034474    1.432455\n------------------------------------------------------------------------------\nEndogenous: D\nExogenous:  z1\n\n. ivregress 2sls y (D=z2)\n\nInstrumental variables 2SLS regression            Number of obs   =     10,000\n                                                  Wald chi2(1)    =     211.38\n                                                  Prob &gt; chi2     =     0.0000\n                                                  R-squared       =     0.3558\n                                                  Root MSE        =     1.4591\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           D |   1.948051   .1339876    14.54   0.000      1.68544    2.210662\n       _cons |   .6818014   .0962172     7.09   0.000     .4932192    .8703836\n------------------------------------------------------------------------------\nEndogenous: D\nExogenous:  z2\n\n. ivregress 2sls y (D=z1 z2)\n\nInstrumental variables 2SLS regression            Number of obs   =     10,000\n                                                  Wald chi2(1)    =     257.90\n                                                  Prob &gt; chi2     =     0.0000\n                                                  R-squared       =     0.3223\n                                                  Root MSE        =     1.4966\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           D |   1.556085   .0968958    16.06   0.000     1.366172    1.745997\n       _cons |   .9600188   .0703862    13.64   0.000     .8220643    1.097973\n------------------------------------------------------------------------------\nEndogenous: D\nExogenous:  z1 z2\n\n."
  },
  {
    "objectID": "adv_class/09iv.html#canonical-designs",
    "href": "adv_class/09iv.html#canonical-designs",
    "title": "Instrumental Variables",
    "section": "Canonical Designs",
    "text": "Canonical Designs\n\nThe general message about using IV’s is, and has always been, that they are hard to come by.\nApplied research spends a quite good amount of time explaining why a particular instrument IS valid. (exogenous and relevant)\nRelevance is generally easy to test, but exogeneity is difficult. Little can be done other than relying in other papers, and circumstances.\nThere are also those “clever” IVs, that tend to be case specific\n\nScott Cunningham talks about Instruments being “weird”, because you wouldnt expect them to be in the context of the research\n\nThere are, however, some designs that are used quite often, because they apply to different circumstances."
  },
  {
    "objectID": "adv_class/09iv.html#cd-lotteries",
    "href": "adv_class/09iv.html#cd-lotteries",
    "title": "Instrumental Variables",
    "section": "CD: Lotteries",
    "text": "CD: Lotteries\n\nIn RCT, Lotteries are commonly used to decide who gets or doesnt get treatment among participants. Once treatment is assigned, however, not everyone will effectively taking up the treatment.\n\nFurthermore, some people may still end up being effectively treated because of other factors.\n\nThis is a case of imperfect compliance.\nIn cases like this, the lottery itself (which is randomized) can be used as instrument to identify the effect of being effectively treated.\n\nExamples:\n\nVietnam Draft Lottery\nOregon Medicaid Expansion Lottery"
  },
  {
    "objectID": "adv_class/09iv.html#cd-judge-fixed-effects",
    "href": "adv_class/09iv.html#cd-judge-fixed-effects",
    "title": "Instrumental Variables",
    "section": "CD: Judge Fixed Effects",
    "text": "CD: Judge Fixed Effects\nThis design is also partially based on a kind of randomized assigment.\n\nIndividuals are “allocated” to work, or be judge, under different officers “judges”, at random.\nJudges are consistent among each other, with only difference being the severity of the judgment.\nThen Judge fixed effect can be used as an instrument on the judgment (treatment), and the final impact on the outcome of interest.\n\nThe idea here is that “judgment-severity” varies by judge. This difference in taste creates exogenous variation on some treatment, which is analyzed on some treatment.\nExample:\n\nTeachers Grading? Driving test officers? Performance tests?"
  },
  {
    "objectID": "adv_class/09iv.html#cd-shift-share-bartik-instrument",
    "href": "adv_class/09iv.html#cd-shift-share-bartik-instrument",
    "title": "Instrumental Variables",
    "section": "CD: Shift-Share Bartik Instrument",
    "text": "CD: Shift-Share Bartik Instrument\nOriginally used in a study of regional labor market effects, this kind of instruments have also been used widely in other areas, such as imigration and trade.\nThe instrument was developed to analyze how changes in economic growth would affect market outcomes. (reverse Causality)\nTo do this, Bartik (1991) suggests, that it could be possible to create an instrument, making use of only exogenous variations, to first predict Potential local growth.\n\nEstimate industry shares by local region, based on some Ex ante information.\nEstimate national growth by industry (which should be exogenous to local growth)\nEstimate Potential Local growth using Shares x growth\n\nThis last one should represent the instrument to be used on actual local growth\nThis instrument depends strongly on the assumption that Shares are exogenous, and states are small compare to the national experience."
  },
  {
    "objectID": "adv_class/11rdd.html#re-cap-potential-outcome-model",
    "href": "adv_class/11rdd.html#re-cap-potential-outcome-model",
    "title": "Regression Discontinuity Design",
    "section": "Re-Cap: Potential outcome Model",
    "text": "Re-Cap: Potential outcome Model\nIn the ideal world, where we can see all possible outcomes and scenarios of your potential treatments, it will be very simple to estimate treatment effects:\n\\[\n\\delta_i = Y_i(1)-Y_i(0)\n\\]\nThis works because all observed and unobserved individual characteristics are kept fixed, except for the treatment Status.\n\\[y_i(D)=y_i(X,u,D)\\]\nSo when comparing a person with himself (clones or parallel worlds), we know (or at least expect) that everything else is the same, and that differences between the two states are explained only by the treatment."
  },
  {
    "objectID": "adv_class/11rdd.html#the-problem",
    "href": "adv_class/11rdd.html#the-problem",
    "title": "Regression Discontinuity Design",
    "section": "The Problem",
    "text": "The Problem\nWe do not observe both ALL States at the same time. People will either be treated or untreated, not both.\nSo what can we do?\n\nWe need to find good counterfactuals!\n\nThis means finding people are very similar to the ones treated, so they can be used as the examples of the “what if” question.\nBut there is a problem. Even in the best scenarios, we can never be asure about how to control for unobservables…or can we?"
  },
  {
    "objectID": "adv_class/11rdd.html#section",
    "href": "adv_class/11rdd.html#section",
    "title": "Regression Discontinuity Design",
    "section": "",
    "text": "You can always RCT But it can be expensive\nYou can IV the problem but its hard to justify\nYou can add FE, but you have time varying errors\n\nThen what?\n\nYou could RDD the problem (if you have the right data!)"
  },
  {
    "objectID": "adv_class/11rdd.html#what-is-rdd",
    "href": "adv_class/11rdd.html#what-is-rdd",
    "title": "Regression Discontinuity Design",
    "section": "What is RDD?",
    "text": "What is RDD?\nRDD or Regression Discontinuity design is methodology that is known for its clean identification and with a relatively easy visualization tool to understand the identification, and solve the problem of unobserved distributions. (see here for a recent paper on how to make graphs on this).\nIn fact, the treatment Status has a very clear Rule!\nConsider the following problem:\n\nYou want to study the role of college on earnings.\nYou have data on people who are applying to go to school. They all take some standardized tests. Their grade will determine if they get into College or not.\nPeople with high skills will get a higher grades in the GRE, go to college, and probably get higher salaries.\nBut, there is a problem. How can you figure out if wages are due to College or skill?"
  },
  {
    "objectID": "adv_class/11rdd.html#possible-solution",
    "href": "adv_class/11rdd.html#possible-solution",
    "title": "Regression Discontinuity Design",
    "section": "Possible Solution",
    "text": "Possible Solution\nSay that we actually have access to the grades, which range from 100 to 200. And assume that you say, every one with grades higher than 170 will go to college.\nCan you estimate the effect now?\n\nYou can’t compare people with more than 170 to those with less than 170. Because skill or ability will be different across groups.\nHowever, what if you compare individuals with 170-172 vs 167-169?\n\nThese individuals are so close togeter they problably have very similar characteristics as well!\nyou have a Localized randomization.\n\n\nIn this case, your analysis is those individuals just above the thresholds to those just below (counterfactual)\nUnless you think grades near the threshold are as good as random, then you have a design to identify treatment effects!"
  },
  {
    "objectID": "adv_class/11rdd.html#rdd-how-it-works.",
    "href": "adv_class/11rdd.html#rdd-how-it-works.",
    "title": "Regression Discontinuity Design",
    "section": "RDD: How it works.",
    "text": "RDD: How it works.\nSelection and Index:\n\nThe first thing you need to see if you can use and RDD is to see if you have access to a variable that “ranks” units.\n\n\nThis variable should be smooth and preferibly continuous.\n\nage, distance from boarder, test score, poverty index\n\n\n\nAssignment into treatment is a function of this index only, with a clear threshold for the index to have an impact the treatment.\n\n\nThose under the Threshold are not treated. Those above are.\nThis is called a Sharp RDD design."
  },
  {
    "objectID": "adv_class/11rdd.html#section-1",
    "href": "adv_class/11rdd.html#section-1",
    "title": "Regression Discontinuity Design",
    "section": "",
    "text": "The threshold should be unique to the Treatment of interest (nothing else happens around that point)\n\n\nIn the College Case, we assume 170 triggers acceptance to School. But if it also triggers Scholarships??\n\n\nPerhaps the Most important: The score cannot be manipulated\n\nOnly then we have true local randomization.\n\nYou want the potential outcomes to be smooth functions of Z. (so we do not mix treatment effects with Nonsmooth changes in outcomes)"
  },
  {
    "objectID": "adv_class/11rdd.html#sharp-rdd",
    "href": "adv_class/11rdd.html#sharp-rdd",
    "title": "Regression Discontinuity Design",
    "section": "Sharp RDD",
    "text": "Sharp RDD\n\n\nCode\nclear\nssc install synth\nset scheme white2\ncolor_style bay\nset seed 1\nqui:set obs 100\ngen e=rnormal()\ngen z=runiform()+e\ngen t = z&gt;0\ngen y = 1 + z + e + rnormal() + (z&gt;0)\ntwo line t z, sort title(\"Treatment\") ylabel(0 \"Not Treated\" 1 \"Treated\") name(m1, replace) ytitle(\"\")\ngraph export resources\\rdd1.png,  width(1000) height(1000) replace\ntwo scatter y z, sort title(\"Outcome\") pstyle(p1) || lfit y z, lw(1) pstyle(p2) ///\n    || lfit y z if z&lt;0, lw(0.5) || lfit y z if z&gt;0, lw(0.5) , legend(off) name(m2, replace)\ngraph export resources\\rdd2.png,  width(1000) height(1000)  replace"
  },
  {
    "objectID": "adv_class/11rdd.html#how-it-works-p2",
    "href": "adv_class/11rdd.html#how-it-works-p2",
    "title": "Regression Discontinuity Design",
    "section": "How it works : p2",
    "text": "How it works : p2\nRecall that in an RCT (or under randomization) treatment effects are estimated by comparing those treated and those not treated.\n\\[E(y|D=1)-E(y|D=0)\\]\nUnder SRDD, you can also think about the same experiment, except that we would need to compare individuals AT the theshold.\n\\[\\begin{aligned}\n\\lim_{z\\downarrow c} E(y|Z=z) &- \\lim_{z\\uparrow c} E(y|Z=z) \\\\\nE(y(1)|Z=c) &-E(y(0)|Z=c)\n\\end{aligned}\n\\]\n\nIn this case, the overlapping assumption is violated. So we need to attemp obtaining effects for groups AT the limit when \\(Z=c\\)."
  },
  {
    "objectID": "adv_class/11rdd.html#estimation",
    "href": "adv_class/11rdd.html#estimation",
    "title": "Regression Discontinuity Design",
    "section": "Estimation",
    "text": "Estimation\nThe most simple way to proceed is to estimate the model using a parametric approach (OLS)\n\\[y = a_0 + \\delta D_{z&gt;c} + f(z-c) + e\\]\nThe idea here is to identify a “jump” in the outcome (treatment effect) at the point where \\(z\\) crosses the threshold.\nBut to identify the jump only, we also need to model the trend observe before and after that threshold (\\(f(z-c)\\)), which can be modelled as flexible as possible. (this include interactions with the jump)\nAlternatively, we could use smaller bandwidths (nonparametric)"
  },
  {
    "objectID": "adv_class/11rdd.html#example",
    "href": "adv_class/11rdd.html#example",
    "title": "Regression Discontinuity Design",
    "section": "Example",
    "text": "Example\n\n\nCode\n qui: {\nclear\nset seed 1\nset obs 200\ngen e=rnormal()\ngen z=runiform()+e\nsum z\nreplace z=(z-r(mean))/r(sd)\ngen t = z&gt;0\ngen y = 1 + 0.5*z + e + rnormal() + (z&gt;0)\n\nqui:reg y t z \npredict yh1\nlocal b1:display %3.2f _b[t]\nqui:reg y t c.z##c.z  \npredict yh2\nlocal b2:display %3.2f _b[t]\nqui:reg y t c.z##c.z##c.z\npredict yh3\nlocal b3:display %3.2f _b[t]\nsort z\n}\ntwo (scatter y z, sort title(\"Sharp RDD\") pstyle(p1) color(%20)) ///\n    (line yh1 z if z&lt;0, pstyle(p2) lw(0.5)) (line yh1 z if z&gt;0, pstyle(p2) lw(0.5)) ///\n    (line yh2 z if z&lt;0, pstyle(p3) lw(0.5)) (line yh2 z if z&gt;0, pstyle(p3) lw(0.5)) ///\n    (line yh3 z if z&lt;0, pstyle(p4) lw(0.5)) (line yh3 z if z&gt;0, pstyle(p4) lw(0.5)) , ///\n    legend(order(2 \"Linear ATT: `b1'\" 3 \"Quadratic ATT: `b2'\" 4 \"Cubic ATT: `b3'\")) name(m1, replace)"
  },
  {
    "objectID": "adv_class/11rdd.html#example-1",
    "href": "adv_class/11rdd.html#example-1",
    "title": "Regression Discontinuity Design",
    "section": "Example",
    "text": "Example\n\n\nCode\nqui: {\nqui:reg y t c.z#t \npredict yh11\nlocal b1:display %3.2f _b[t]\nqui:reg y t (c.z##c.z)#t  \npredict yh21\nlocal b2:display %3.2f _b[t]\nqui:reg y t (c.z##c.z##c.z)#t\npredict yh31\nlocal b3:display %3.2f _b[t]\n}\ntwo (scatter y z, sort title(\"Sharp RDD\") pstyle(p1) color(%20)) ///\n    (line yh11 z if z&lt;0, pstyle(p2) lw(0.5)) (line yh11 z if z&gt;0, pstyle(p2) lw(0.5)) ///\n    (line yh21 z if z&lt;0, pstyle(p3) lw(0.5)) (line yh21 z if z&gt;0, pstyle(p3) lw(0.5)) ///\n    (line yh31 z if z&lt;0, pstyle(p4) lw(0.5)) (line yh31 z if z&gt;0, pstyle(p4) lw(0.5)) , ///\n    legend(order(2 \"Linear ATT: `b1'\" 3 \"Quadratic ATT: `b2'\" 4 \"Cubic ATT: `b3'\")) name(m2, replace)"
  },
  {
    "objectID": "adv_class/11rdd.html#fuzzy-rd-imperfect-compliance",
    "href": "adv_class/11rdd.html#fuzzy-rd-imperfect-compliance",
    "title": "Regression Discontinuity Design",
    "section": "Fuzzy RD: Imperfect compliance",
    "text": "Fuzzy RD: Imperfect compliance\nWhile the Idea Scenario happens when there is perfect compliance (above the threshold you are treated), this doesnt happen all the time.\nIn the education example:\n\nSome people with low grades may be “legacy” or have “contacts” (or took a second exam later) and manage to go to college\nSome decided not to go, even after entering to college\n\nSounds Familiar? (Never takers vs always takers)\nWhen this happens, you can still do RDD, but you need more steps"
  },
  {
    "objectID": "adv_class/11rdd.html#fuzzy-rd",
    "href": "adv_class/11rdd.html#fuzzy-rd",
    "title": "Regression Discontinuity Design",
    "section": "Fuzzy RD",
    "text": "Fuzzy RD\n\nEstimate the impact of Discontinuity on Treatment\nEstimate the impact of Discontinuity on Outcome\nEstimate the ratio between (1) and (2)\n\nSounds Familiar?\n\nIts a kind of wald/IV estimator.\n\nThe instrument is the discontinuity\nThe the endogenous variable is the treament\n\n\nYou still need to estimate the effect as close to the Discontinuity as possible\nivregress may still do most of this for you"
  },
  {
    "objectID": "adv_class/11rdd.html#example-2",
    "href": "adv_class/11rdd.html#example-2",
    "title": "Regression Discontinuity Design",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\n\n\nTreatment\n\n\n\n\n\n\n\nOutcome\n\n\n\n\n\nEffect:\n\nCode\nqui: gen dz = z&gt;0\nqui: reg y dz c.z##c.z#i.dz\nlocal b1 = _b[dz]\nqui: reg t dz c.z##c.z#i.dz\nlocal b2 = _b[dz]\n\ndisplay \"There is a \" %3.2f `b1' \" effect on the outcome\"\ndisplay \"and a \" %3.2f `b2' \" effect on the treatment\"\ndisplay \"which imply a LATE of \" %3.2f `=`b1'/`b2''\n\nThere is a 0.45 effect on the outcome and a 0.44 effect on the treatment which imply a LATE of 1.03"
  },
  {
    "objectID": "adv_class/11rdd.html#things-to-consider",
    "href": "adv_class/11rdd.html#things-to-consider",
    "title": "Regression Discontinuity Design",
    "section": "Things to consider",
    "text": "Things to consider\nTheoretical:\n\nYou need to identify “jumps” caused by a running variable. (depends on knowing how things works)\nThe potential outcomes have to be smooth functions of the running variable\n\nEmpirical:\n\nThe running variable shouldnt be manipulated. (random)\n\nImplies Assignment rules are not known, are exogenous, and there is no random heaping\n\nControls should be balanced around the threshold"
  },
  {
    "objectID": "adv_class/11rdd.html#testing-empirical-assumptions",
    "href": "adv_class/11rdd.html#testing-empirical-assumptions",
    "title": "Regression Discontinuity Design",
    "section": "Testing Empirical Assumptions",
    "text": "Testing Empirical Assumptions\nManipulation of running variable may cause a non-smooth density in the running variable:\n\nIf there is no manipulation, you may expect density round threshold to be smooth.\n\nIn Stata: ssc install rddensity. In r install.packages(c(‘rdd’,‘rddensity’))\n\n\n\n\nCode\nset linesize 100\nqui:ssc install  lpdensity, replace\nqui:ssc install  rddensity, replace\nrddensity z, c(0) plot\ngraph export resources\\frdd3.png, width(1000) replace"
  },
  {
    "objectID": "adv_class/11rdd.html#section-2",
    "href": "adv_class/11rdd.html#section-2",
    "title": "Regression Discontinuity Design",
    "section": "",
    "text": "In cases of nonrandom heaping, it may be possible to avoid the problem by restricting the data.\nThis is an example of measurement error, when individuals may “round-up/down” answers. And may occure near threshold.\nThis does not necessarily mean there is manipulation.\nPossible Solution? Estimate RDD excluding observations around (excluding) threshold."
  },
  {
    "objectID": "adv_class/11rdd.html#covariate-balance-and-placebo-tests",
    "href": "adv_class/11rdd.html#covariate-balance-and-placebo-tests",
    "title": "Regression Discontinuity Design",
    "section": "Covariate balance and Placebo tests",
    "text": "Covariate balance and Placebo tests\n\nIf treatment is locally randomized, then covariates should not be affected by discontinuity.\nAlternatively, one could estimate effects on variables you know CANNOT be affected by the treatment\nOne could also implement a placebo test, checking the impact on a different threholds.\n\nNo effect should be observed on the outcome, (but some on the treatment)"
  },
  {
    "objectID": "adv_class/11rdd.html#example-3",
    "href": "adv_class/11rdd.html#example-3",
    "title": "Regression Discontinuity Design",
    "section": "Example",
    "text": "Example\nImpact of Scores on Scholarship recipiency\n\n\nCode\nuse resources\\fuzzy, clear\ncolor_style bay\nssc install rdrobust\nssc install rddensity\nqui:rdplot d x1, graph_options(legend( pos(6)))\n\n\nchecking rdrobust consistency and verifying not already installed...\nall files already exist and are up to date.\nchecking rddensity consistency and verifying not already installed...\nall files already exist and are up to date."
  },
  {
    "objectID": "adv_class/11rdd.html#manipulation-test",
    "href": "adv_class/11rdd.html#manipulation-test",
    "title": "Regression Discontinuity Design",
    "section": "Manipulation test",
    "text": "Manipulation test\n\n\nCode\nqui:rddensity x1, plot"
  },
  {
    "objectID": "adv_class/11rdd.html#intention-to-treat",
    "href": "adv_class/11rdd.html#intention-to-treat",
    "title": "Regression Discontinuity Design",
    "section": "Intention to treat",
    "text": "Intention to treat\nImpact on Enrollment\n\n\nCode\nqui:rdplot y x1, graph_options(legend( pos(6)))"
  },
  {
    "objectID": "adv_class/11rdd.html#estimation-of-the-effect",
    "href": "adv_class/11rdd.html#estimation-of-the-effect",
    "title": "Regression Discontinuity Design",
    "section": "Estimation of the effect:",
    "text": "Estimation of the effect:\n\n\nCode\ngen dx1 = x1&gt;0\nivregress 2sls y (d = dx1) c.x1##c.x1#dx1 \n\n\n\nInstrumental variables 2SLS regression            Number of obs   =     23,132\n                                                  Wald chi2(5)    =    1645.56\n                                                  Prob &gt; chi2     =     0.0000\n                                                  R-squared       =     0.2845\n                                                  Root MSE        =     .39876\n\n-------------------------------------------------------------------------------\n            y | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n--------------+----------------------------------------------------------------\n            d |   .4432824   .0215111    20.61   0.000     .4011214    .4854434\n              |\n     dx1#c.x1 |\n           0  |   .0002949   .0019179     0.15   0.878    -.0034642    .0040539\n           1  |  -.0034238    .000779    -4.39   0.000    -.0049507   -.0018969\n              |\ndx1#c.x1#c.x1 |\n           0  |   .0000741   .0000703     1.05   0.292    -.0000637    .0002119\n           1  |   .0000616   .0000164     3.76   0.000     .0000295    .0000936\n              |\n        _cons |   .5103635    .010876    46.93   0.000     .4890469    .5316801\n-------------------------------------------------------------------------------\nEndogenous: d\nExogenous:  0b.dx1#c.x1 1.dx1#c.x1 0b.dx1#c.x1#c.x1 1.dx1#c.x1#c.x1 dx1"
  },
  {
    "objectID": "adv_class/11rdd.html#some-sensitivity",
    "href": "adv_class/11rdd.html#some-sensitivity",
    "title": "Regression Discontinuity Design",
    "section": "Some Sensitivity",
    "text": "Some Sensitivity\n\n\nCode\nqui:{   \nmatrix b1 = 0,0\nforvalues i = 1/15 {\n    ivregress 2sls y (d = dx1) c.x1#dx1  if abs(x1)&lt;`i'\n    matrix b1=b1\\[_b[d],_se[d]]\n}\n\nmatrix b2 = 0,0\nforvalues i = 1/15 {\n    ivregress 2sls y (d = dx1) c.x1##c.x1#dx1  if abs(x1)&lt;`i'\n    matrix b2=b2\\[_b[d],_se[d]]\n}\n\nmatrix b3 = 0,0\nforvalues i = 1/15 {\n    ivregress 2sls y (d = dx1) c.x1##c.x1##c.x1#dx1  if abs(x1)&lt;`i'\n    matrix b3=b3\\[_b[d],_se[d]]\n}\n\nlbsvmat b1\nlbsvmat b2\nlbsvmat b3\nforvalues i = 1/3 {\n  gen ll`i'=b`i'1-b`i'2*1.96\n  gen ul`i'=b`i'1+b`i'2*1.96\n}\ngen z = _n-1 if b11!=.\nreplace z=. in 1\n\n}\ngen z1 = z + 0.25\ngen z2 = z + 0.5\ntwo (rspike ll1 ul1 z, lw(1) pstyle(p1) color(%50) ) (scatter b11 z, pstyle(p1) ) ///\n    (rspike ll2 ul2 z1, lw(1) pstyle(p2) color(%50) ) (scatter b21 z1, pstyle(p2) ) ///\n    (rspike ll3 ul3 z2, lw(1) pstyle(p3) color(%50) ) (scatter b31 z2, pstyle(p3) ), ///\n    legend(order(1 \"Linear\" 3 \"Quadratic\" 5 \"Cubic\")) xtitle(\"Bandwidth\")\n\n\n(23,117 missing values generated)\n(23,117 missing values generated)"
  },
  {
    "objectID": "adv_class/11rdd.html#falsification-test",
    "href": "adv_class/11rdd.html#falsification-test",
    "title": "Regression Discontinuity Design",
    "section": "Falsification test",
    "text": "Falsification test\n\n\nCode\nqui {\nivregress 2sls icfes_female (d = dx1) c.x1#dx1    if abs(x1)&lt;20\nest sto m1\nivregress 2sls icfes_age (d = dx1) c.x1#dx1       if abs(x1)&lt;20\nest sto m2\nivregress 2sls icfes_urm (d = dx1) c.x1#dx1       if abs(x1)&lt;20\nest sto m3\nivregress 2sls icfes_famsize (d = dx1) c.x1#dx1       if abs(x1)&lt;20\nest sto m4\n}\nesttab m1 m2 m3 m4, keep(d)\n\n\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n             icfes_female       icfes_age       icfes_urm    icfes_fams~e   \n----------------------------------------------------------------------------\nd                  0.0199           0.128         0.00791          0.0439   \n                   (0.80)          (1.05)          (0.65)          (0.63)   \n----------------------------------------------------------------------------\nN                   14841           14799           14841           14801   \n----------------------------------------------------------------------------\nt statistics in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "adv_class/13SC.html#introduction",
    "href": "adv_class/13SC.html#introduction",
    "title": "Synthetic Control",
    "section": "Introduction",
    "text": "Introduction\n\nOne more last time. What is the Goal of Causal Analysis?\n\n\n\n\n\n\n\nImportant\n\n\nThe goal of Causal Analysis is to identify how a treatment affects the outcome by itself, once all other factors are kept constant or controlled for.\n\n\n\nFrom a theoretical point of view, that is very easy. You simply compare two Potential outcomes:\n\\[\nTE_i = y_i(1)- y_i(0)\n\\]\nand aggregate those outcomes as needed:\n\\[\nATT=E(TE_i|D=1);ATU=E(TE_i|D=0);ATE=E(TE_i);ATX=E(TE_i|X)\n\\]"
  },
  {
    "objectID": "adv_class/13SC.html#section",
    "href": "adv_class/13SC.html#section",
    "title": "Synthetic Control",
    "section": "",
    "text": "Unfortunately, we only observe one outcome. You are either treated or untreated…So how do we fix this?\nYou need to find counterfactuals so both observed (\\(X\\)) and unobserved (\\(e\\)) are the same (or close) between treated and contro group.\n\nRCT: Gold Standard, You randomize treatment and compare means. If correctly done, \\(X's\\) and \\(e's\\) will be comparable across groups, and ATE’s can be identified.\nReg + FE: For other cases, we just work with observational data. First method, Regression (OLS?). Adding covariates controls for their presence, working as a pseudo balancing approach.\nYou could also add fixed effects, to control for factors that are fixed (across time), but you do not observe. (requires Panel data).\nIt works if Treatment occurs at the same time for everyone treated. and if Unobserved are “fixed”"
  },
  {
    "objectID": "adv_class/13SC.html#section-1",
    "href": "adv_class/13SC.html#section-1",
    "title": "Synthetic Control",
    "section": "",
    "text": "Instrumental variables: 2nd Best to RCT. It uses IV to generate a small randomization process that can be used for estimating ATT. Technically it compares the effect among those potentially affected by the random instrument. Requires Randome instrument, and no-defiers. Its a Local ATE\nMatching and Reweigthing. Similar to Regression, but better to balance characteristics. The goal is to find units with similar characteristics for all treated units. You can estimate ATE, ATT or ATU. Depends on how well Matching is done\nRDD. If you have data where treatment depends on a single variable and a threshold, you can use this to identify TE for those “Near” the threshold. They Key assumption, treatment assigment is as good at random at the threshold."
  },
  {
    "objectID": "adv_class/13SC.html#section-2",
    "href": "adv_class/13SC.html#section-2",
    "title": "Synthetic Control",
    "section": "",
    "text": "DD. Differences in differences uses variation across time and across individual to identify treatment effects. Under PTA, and SUTVA\nDif. within individuals eliminates common time trends, Dif across time, eliminates individual fixed effects. DD provide you with ATT’s for the treated, after treatment.\n\\[ATT=(Y_{g=1,t=1}-Y_{g=1,t=0}) - [(Y_{g=0,t=1}-Y_{g=0,t=0})]\\]\nCan be generalized to Many periods and many groups, but requires stronger assumptions (no anticipation and no change in treatment status), and further aggregation.\nOr combined with Matching for even better results."
  },
  {
    "objectID": "adv_class/13SC.html#synthetic-control-special-case",
    "href": "adv_class/13SC.html#synthetic-control-special-case",
    "title": "Synthetic Control",
    "section": "Synthetic Control: Special case",
    "text": "Synthetic Control: Special case\nAs previous Cases, Synthetic control aims to identify treatment constructing appropriate “counterfactuals”.\nIt is said that Synthethic controls may be even MORE credible methodology, because the treated group is by construction Exogenous…but how?\n\nThe treated group is a Case Study.\nAn isolated event or unit that is affected by a treatment, and should not affect other units !\n\nIn this sense, the treatment is exogenous, because it affected a single unit.\nBut what about the counterfactual?"
  },
  {
    "objectID": "adv_class/13SC.html#section-3",
    "href": "adv_class/13SC.html#section-3",
    "title": "Synthetic Control",
    "section": "",
    "text": "In other methods (in particular Matching), our “conterfactual” mean to look for observations that had the same characteristics as the treated observation.\nSome times, we needed to settle to use a single “bad” control, because we couldnt find one better. (people are very different).\n\nUsing Stricter criteria would make it unfeasible.\nMore relax and we have lots of biases.\n\nSC is different. You have MANY controls, so why settle with only one?\nSC is like Dr Frankenstein, where you “build” a single comparison group by averaging information of all controls.\nYou build the synthetic control getting “weighted averages”.\nBut…we assume you can see all units across time (panel data)"
  },
  {
    "objectID": "adv_class/13SC.html#this-is-a-very-popular-method",
    "href": "adv_class/13SC.html#this-is-a-very-popular-method",
    "title": "Synthetic Control",
    "section": "This is a very popular method",
    "text": "This is a very popular method\nWhere has this method been used:\n\neffects of right-to-carry laws (Donohue et al., 2019),\nlegalized prostitution (Cunningham and Shah, 2018),\nimmigration policy (Bohn et al., 2014),\ncorporate political connections (Acemoglu et al., 2016),\ntaxation (Kleven et al., 2013),\norganized crime (Pinotti, 2015)\n\nJust to name a few."
  },
  {
    "objectID": "adv_class/13SC.html#assumptions",
    "href": "adv_class/13SC.html#assumptions",
    "title": "Synthetic Control",
    "section": "Assumptions:",
    "text": "Assumptions:\n\nThe Donor Pool should be a good match for the treated unit. Thus, the synthethic control should be Zero before treatment.\n\nThis is similar to PTA, but stronger. Before treatment, there should be no difference between Treated and synthetic control\n\nSUTVA. Only the treated group is affected by treatment. The control group should be unaffected (no spill over effects).\nThere should be NO other “event” in the period of analysis. (Thus we only capture treatment impact)"
  },
  {
    "objectID": "adv_class/13SC.html#how-does-it-work.",
    "href": "adv_class/13SC.html#how-does-it-work.",
    "title": "Synthetic Control",
    "section": "How does it work.",
    "text": "How does it work.\nRecall, we want to estimate TE for the single untreated unit:\n\\[ATT_{1t} = Y_{1t}- Y(0)_{1t}\n\\]\nbut we do not observe \\(Y(0)_{1t}\\). We only know that before treatment\n\\[ATT_{1t} = Y_{1t}- Y(0)_{1t}=0\\]\nWe could construct a synthetic control:\n\\[\\hat Y_{1t}(0) = \\sum_{i = 2}^N w_i Y_{it}\n\\]\nAt the very least, the weights \\(w\\) should be such that before treatment (\\(G\\)):\n\\[Y_{1t} = \\sum_{i \\neq 1} w_i Y_{it} \\ \\forall \\ t&lt;G \\]"
  },
  {
    "objectID": "adv_class/13SC.html#section-4",
    "href": "adv_class/13SC.html#section-4",
    "title": "Synthetic Control",
    "section": "",
    "text": "At the very least, the weights \\(w\\) should be such that before treatment (\\(G\\)):\n\\[Y_{1t} = \\sum_{i \\neq 1} w_i Y_{it} \\ \\forall \\ t&lt;G \\]\nHavent we seen seen something like this Before? OLS:\n\\[y = x\\beta + e\\] \\[y^t_{1} = \\color{red}{a_0} +  y_i^t w + e\\]\n\\[\n\\begin{bmatrix}\ny^1_1 \\\\ y^1_2 \\\\ ... \\\\ y^1_{G-1} \\\\\n\\end{bmatrix} = \\color{red}{a_0} +\n\\begin{bmatrix}\ny^2_1 & y^3_1 & ... & y^k_1  \\\\\ny^2_2 & y^3_2 & ... & y^k_2 \\\\\n...  & ... & ... & ...\\\\\ny^2_{G-1} & y^3_{G-1} & ... & y^k_{G-1}\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nw_2 \\\\ w_3 \\\\ ... \\\\ w_k\n\\end{bmatrix}\n+ e\n\\]"
  },
  {
    "objectID": "adv_class/13SC.html#section-5",
    "href": "adv_class/13SC.html#section-5",
    "title": "Synthetic Control",
    "section": "",
    "text": "\\[y^t_{1} = \\color{red}{a_0} +  y_i^t w + e\\]\n\nIn this Specification, each row (observation) is a “pre-treatment” period of observed data.\nand each control unit (from the many controls) will be a variable.\n\nOLS can help you find the weights, which can then be used for obtaining the “Synthetic” control"
  },
  {
    "objectID": "adv_class/13SC.html#small-example",
    "href": "adv_class/13SC.html#small-example",
    "title": "Synthetic Control",
    "section": "Small Example",
    "text": "Small Example\n\n\nCode\nqui:frause smoking, clear\ncolor_style tableau\nbysort year:egen mean_cig=mean(cigsale) if state!=3\ntwo (line cigsale year if state ==3) (line mean_cig year if state==1), ///\n    legend(order(1 \"California\" 2 \"Avg Other States\") pos(6) col(2)) xline(1988)\n\n\n\n\n\n(31 missing values generated)"
  },
  {
    "objectID": "adv_class/13SC.html#section-6",
    "href": "adv_class/13SC.html#section-6",
    "title": "Synthetic Control",
    "section": "",
    "text": "Code\ndrop mean_cig\nqui:reshape wide cigsale lnincome beer age15to24 retprice , i(year) j(state)\nren cigsale1 mcigsale\n\n\n\nNow we have…38 variables, (other States but California)\nAnd 31 periods (only 19 Before treatment)\nCan we estimate the weights using OLS?\n\n…\n\nNop. N&lt;K !"
  },
  {
    "objectID": "adv_class/13SC.html#section-7",
    "href": "adv_class/13SC.html#section-7",
    "title": "Synthetic Control",
    "section": "",
    "text": "Code\nqui:reg mcigsale cigsale* if year&lt;=1988, nocons\npredict mcigh1\nqui: lasso linear  mcigsale cig* if year&lt;=1988, nocons\npredict mcigh2\n two (line mcigsale year, lw(0.5)  ) ///\n  (line mcigh1 mcigh2   year, lw(0.5)  ) , ///\n  legend(order(1 \"California\" 2 \"OLS Synthetic\" 3 \"LASSO Synthetic\")) xline(1988)\n\n\n(option xb assumed; fitted values)\n(options xb penalized assumed; linear prediction with penalized coefficients)\n\n\n\n\nOLS Not appropriate (specially if N&lt;K)\nLasso Better, because of regularization, but not great.\nWe are not controlling for other factors either (controls)\nOther Details we cover next"
  },
  {
    "objectID": "adv_class/13SC.html#allowing-for-covariates",
    "href": "adv_class/13SC.html#allowing-for-covariates",
    "title": "Synthetic Control",
    "section": "Allowing for Covariates:",
    "text": "Allowing for Covariates:\n\nAs with other methodologies, one should also considered controlling for covariates.\nSpecifically, more covariates can be allowed by Stacking them:\n\n\\[\\begin{bmatrix} y^t_{1} \\\\ x^t_{1} \\\\ z^t_{1} \\end{bmatrix}\n= \\color{red}{(a_0=0)} +\n\\begin{bmatrix} y^t_{2} & y^t_{3} ... & y^t_{k} \\\\\n                x^t_{2} & x^t_{3} ... & x^t_{k} \\\\\n                z^t_{2} & z^t_{3} ... & z^t_{k} \\end{bmatrix}\n\\begin{bmatrix} w_2 \\\\ w_3 \\\\ ... \\\\ w_k \\end{bmatrix}\n+ e\n\\]"
  },
  {
    "objectID": "adv_class/13SC.html#section-8",
    "href": "adv_class/13SC.html#section-8",
    "title": "Synthetic Control",
    "section": "",
    "text": "Code\nqui:frause smoking, clear\nren (cigsale lnincome beer age15to24 retprice) ///\n      (var1    var2     var3 var4      var5)\nqui: reshape long var, i(state year)    j(new)\nqui: reshape wide var, i(year new)  j(state)\nlabel define new 1 \"cigsale\" ///\n                 2 \"lnincome\" ///\n                 3 \"beer\" ///\n                 4 \"age15to24\" /// \n                 5 \"retprice\", modify\nlabel values new new    \nren var3 cal_out\nqui:reg cal_out var* if year&lt;=1988, nocons\npredict mcigh1\nqui: lasso linear   cal_out var* if year&lt;=1988, nocons\npredict mcigh2\n two (line cal_out year if new==1, lw(0.5)  ) ///\n  (line mcigh1 mcigh2   year if new==1, lw(0.5)  ) , ///\n  legend(order(1 \"California\" 2 \"OLS Synthetic\" 3 \"LASSO Synthetic\")) xline(1988)\n\n\n(option xb assumed; fitted values)\n(32 missing values generated)\n(options xb penalized assumed; linear prediction with penalized coefficients)"
  },
  {
    "objectID": "adv_class/13SC.html#what-else-to-keep-in-mind",
    "href": "adv_class/13SC.html#what-else-to-keep-in-mind",
    "title": "Synthetic Control",
    "section": "What else to keep in mind",
    "text": "What else to keep in mind\n\nWith More Variables, the goal is still to be able to choose \\(w's\\) that best explain the observed outcomes (and characteristics) of the “treated unit”.\n\n\\[w = \\min_w \\sum_{m=1}^K \\left[ v_m \\left( X_{1t}-\\sum_{j=2}^J w_j X_{jt}  \\right)^2 \\right]\n\\]\nHowever, we also need to impose restrictions on Weights:\n\n\\(w_j \\geq 0\\) Weights cannot be negative.\n\\(\\sum w_j =1\\) They should sum up to 1.\n\\(v_m\\) can be used to increase, or reduce the relative importance of factors in the model. (lower bound at 0) The constant is zero.\n\nThis is a maximization problem with constrains. Restrictions ensure the prediction is based on a “convex” set, avoiding extrapolation."
  },
  {
    "objectID": "adv_class/13SC.html#is-it-noise-or-causal",
    "href": "adv_class/13SC.html#is-it-noise-or-causal",
    "title": "Synthetic Control",
    "section": "Is it noise? or Causal?",
    "text": "Is it noise? or Causal?\n\nWhen using SC, you essentially have a sample \\(n=1\\) to estimate an effect. How do you know that effect is significant? and not just noise?\n\nYou can do a randomization experiment! and answer:\n\n\n\n“how unusual is this estimate under the null hypothesis of no policy effect?”.\n\n\nHow does this work?"
  },
  {
    "objectID": "adv_class/13SC.html#randomization",
    "href": "adv_class/13SC.html#randomization",
    "title": "Synthetic Control",
    "section": "Randomization",
    "text": "Randomization\n\nExcluding the treated unit, estimate the pseudo effect of every other unit in the dataset. These are placebos, and you should expect the effect to be zero for them…but you may see some positive and negative effects.\n\nThis may be consider the sampling distribution of the estimated effect.\n\nCalculate the pre- and post- treament Root mean squared prediction error for all units (treated and placebos).\n\nPre-RMSPE provides a statistic of how well the model fits before treatment.\nPost-RMSPE provides a statistic of how unusual is the outcome after the “treatment date”. The largest it is, the more unpredictable (or stronger treatment effect) it would be."
  },
  {
    "objectID": "adv_class/13SC.html#section-9",
    "href": "adv_class/13SC.html#section-9",
    "title": "Synthetic Control",
    "section": "",
    "text": "\\[\n  \\begin{aligned}\n  RMSPE_i^{pre} &= \\sqrt{ \\frac{1}{g-1}\\sum_{t=1}^{g-1}(y_{i,t}-\\sum_{j\\neq i}w_j^i y_{j,t})^2 } \\\\\n  RMSPE_i^{post} &= \\sqrt{ \\frac{1}{T-g+1}\\sum_{t=g}^{T}(y_{i,t}-\\sum_{j\\neq i}w_j^i y_{j,t})^2 }\n  \\end{aligned}\n  \\]\n\nEstimate the ratio between Pre and Post RMSPE, and rank them. \\[Ratio_i = \\frac{RMSPE_i^{post}}{RMSPE_i^{pre}}\n\\]\nThe p-value for the treatment is proportional to the Rank:\n\\[pvalue_i = \\frac{rank(i)}{Tot}\\]"
  },
  {
    "objectID": "adv_class/13SC.html#lets-continue-the-example",
    "href": "adv_class/13SC.html#lets-continue-the-example",
    "title": "Synthetic Control",
    "section": "Lets continue the example:",
    "text": "Lets continue the example:\n\n\nCode\nqui:frause smoking, clear\nxtset state year\ntempfile sc3\n** For California\nsynth cigsale cigsale(1970) cigsale(1975) cigsale(1980) cigsale(1985) cigsale(1988), trunit(3) trperiod(1989) keep(`sc3') replace\n** Same Specification for All other States excluding California\nforvalues i =1/39{\n    if `i'!=3 {\n        local pool\n        foreach j of local stl {\n            if `j'!=3 & `j'!=`i' local pool `pool' `j'\n        }\n        tempfile sc`i'\n        synth cigsale cigsale(1970) cigsale(1975) cigsale(1980) cigsale(1985) cigsale(1988), ///\n        trunit(`i') trperiod(1989) keep(`sc`i'') replace counit(`pool')\n    }\n}\n** Some data cleaning and prepration\nforvalues i =1/39{\n    use `sc`i'' , clear\n    gen tef`i' = _Y_treated - _Y_synthetic\n    egen sef`i'a =mean( (_Y_treated - _Y_synthetic)^2) if _time&lt;=1988\n    egen sef`i'b =mean( (_Y_treated - _Y_synthetic)^2) if _time&gt;1988\n  replace sef`i'a=sqrt(sef`i'a[1])\n    replace sef`i'b=sqrt(sef`i'b[_N])\n    drop if _time==.\n    keep tef`i' sef`i'* _time\n    save `sc`i'', replace\n}\n**\n** Merging all together, and getting ready to plot\n** \n\nuse `sc1', clear\nforvalues i = 2/39 {\n    merge 1:1 _time using `sc`i'', nogen\n}\nglobal toplot\nglobal toplot2\n\nforvalues i = 1/39 {\n    global toplot $toplot (line tef`i' _time, color(gs11) )\n  if (sef`i'a[1])&lt;(2*sef3a[1]) {\n        global toplot2 $toplot2 (line tef`i' _time, color(gs11) )\n    }\n}\n\n\nAll Cases\n\n\nCode\ntwo $toplot (line tef3 _time, lw(1) color(navy*.8)), xline(1989) legend(off)"
  },
  {
    "objectID": "adv_class/13SC.html#section-10",
    "href": "adv_class/13SC.html#section-10",
    "title": "Synthetic Control",
    "section": "",
    "text": "Good Cases Restricts to States with Good RMSEP (less than 2 California)\n\n\nCode\ntwo $toplot2 (line tef3 _time, lw(1) color(navy*.8)), xline(1989) legend(off)"
  },
  {
    "objectID": "adv_class/13SC.html#rmse-ratio",
    "href": "adv_class/13SC.html#rmse-ratio",
    "title": "Synthetic Control",
    "section": "RMSE Ratio",
    "text": "RMSE Ratio\n\n\nCode\nforvalues i = 1/39 {\n  if (sef`i'a[1])&lt;(2*sef3a[1]) {\n        matrix rt=nullmat(rt)\\[`i',sef`i'b[1]/sef`i'a[1]]\n    }\n}\nsvmat rt\negen rnk=rank(rt2)\n \ntwo bar rt2 rnk || bar rt2 rnk if rt1==3 , ///\n legend(order( 2 \"California\")) ///\n  ytitle(RMSE ratio)  xtitle(RMSE rank)\n\n\nnumber of observations will be reset to 32\nPress any key to continue, or Break to abort\nNumber of observations (_N) was 31, now 32."
  },
  {
    "objectID": "adv_class/13SC.html#p-values",
    "href": "adv_class/13SC.html#p-values",
    "title": "Synthetic Control",
    "section": "p-values",
    "text": "p-values\n\n\nCode\n gen rnk2=0\nforvalues i = 1/39 {\n    if   (sef`i'a[1])&lt;(2*sef3a[1]) {\n        local t = `t'+1\n        qui:replace rnk2=rnk2+(tef`i'&lt;=tef3)    \n    }\n} \nqui: gen pv=rnk2*100/`t'\n \ntwo bar pv _time if _time&gt;1988 & rnk2&lt;32, ylabel(0(2)15) xlabel(1989/2000)"
  },
  {
    "objectID": "adv_class/13SC.html#other-falsification-tests",
    "href": "adv_class/13SC.html#other-falsification-tests",
    "title": "Synthetic Control",
    "section": "Other Falsification Tests",
    "text": "Other Falsification Tests\n\nChange of treatment Year.\n\nIn the manual implementation you may want to change the treatment year (to an earler point). One should see no effect between false treatment date and the true to be zero.\nUsing synth (Stata) you may want to drop some of the controls, so only “pre-false” treatment data is used.\n\nLook also into synth_runner\n\n\nChange of Outcome.\n\nOne can estimate the effect on alternative outcomes. No effect should be estimated."
  },
  {
    "objectID": "adv_class/13SC.html#conclusions",
    "href": "adv_class/13SC.html#conclusions",
    "title": "Synthetic Control",
    "section": "Conclusions",
    "text": "Conclusions\n\nThe basic methodology presented here differs from other strategies because one uses a single treated unit, with pletora of treated groups.\nInstead of comparing single units with the treated group, it aims to compare a weighted average “synthetic control” to do so.\nIt will work better than matching because you are focusing on getting the best “weighted” group for a single unit.\nBut this methodology is still under development, with extensions toward using dissagregated data, or a combination with DD approaches.\nThis may change how much more one can do with the method"
  },
  {
    "objectID": "adv_class/Homework2.html",
    "href": "adv_class/Homework2.html",
    "title": "Homework II",
    "section": "",
    "text": "Consider the dataset atus_2020_22.dta. This dataset is a subsample of the American Time Use survey for the years 2020 through 2022. The dataset contains information on time dedicated by individuals 15 years old or older to household production activities. It also contains information on the average MET value of their activities during the day (x100) (avgmet). So a person with an avgmet value of 100 is spending the same ammount of energy as if they were sitting down all day.\n\nPropose a model to analyze the determinants of AvgMet activities, justify the variables you include in the model.\nEstimate the model using Linear Regression, and for Conditional quanttiles at 0.1, 0.25, 0.5, 0.75, and 0.9. Interpret the results of the variables you consider most important or interesting."
  },
  {
    "objectID": "adv_class/Homework2.html#part-i-conditional-quantile-regressions",
    "href": "adv_class/Homework2.html#part-i-conditional-quantile-regressions",
    "title": "Homework II",
    "section": "",
    "text": "Consider the dataset atus_2020_22.dta. This dataset is a subsample of the American Time Use survey for the years 2020 through 2022. The dataset contains information on time dedicated by individuals 15 years old or older to household production activities. It also contains information on the average MET value of their activities during the day (x100) (avgmet). So a person with an avgmet value of 100 is spending the same ammount of energy as if they were sitting down all day.\n\nPropose a model to analyze the determinants of AvgMet activities, justify the variables you include in the model.\nEstimate the model using Linear Regression, and for Conditional quanttiles at 0.1, 0.25, 0.5, 0.75, and 0.9. Interpret the results of the variables you consider most important or interesting."
  },
  {
    "objectID": "adv_class/Homework2.html#part-ii-unconditional-quantile-regressions",
    "href": "adv_class/Homework2.html#part-ii-unconditional-quantile-regressions",
    "title": "Homework II",
    "section": "Part II: Unconditional Quantile Regressions",
    "text": "Part II: Unconditional Quantile Regressions\nConsider the dataset hprice2.dta, available with frause. Propose a model to analyze the determinants of housing prices, making sure you also include data on crime and nox (nitrogen oxide concentration).\n\nEstimate unconditional quantile regressions at 0.1, 0.25, 0.5, 0.75, and 0.9. And interpret the results of the variables you consider most important.\nSay that there is a policy that promises to reduce average crime by 50%, and average NOX by 50%. How would this affect the distribution of housing prices?"
  },
  {
    "objectID": "adv_class/Homework2.html#part-iii-maximum-likelihood-estimation",
    "href": "adv_class/Homework2.html#part-iii-maximum-likelihood-estimation",
    "title": "Homework II",
    "section": "Part III: Maximum Likelihood Estimation",
    "text": "Part III: Maximum Likelihood Estimation\nMaximum likelihood estimation is a method to estimate the parameters of a model by maximizing the likelihood of the data given the parameters.\nIn this part, you will estimate the parameters of a Poisson model using maximum likelihood estimation, when the data is censored.\n\nThe Poisson Model\nThe Poisson model is a model for count data. The likelihood function for a single observation in a Poisson model is given by:\n\\[\nP(y=y_i) = \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}\n\\]\nWhere \\(y_i\\) is the count of events, and \\(\\lambda\\) is the mean of the distribution. If we assume \\(\\lambda\\) depends on a set of covariates, then we can parameterize \\(\\lambda = \\exp x\\beta\\).\nFor the purpose of programming the likelihood function, it is easier to work with the log-likelihood function, which is given by:\n\\[\n\\begin{aligned}\nL_i(y,X\\beta) = - \\exp(x\\beta) + y_i (x\\beta) - log(y_i!)\n\\end{aligned}\n\\]\nWhich corresponds to line 16 in the do-file mypoisson.do.\n\n\nCensored Data\nIf data is censored, the likelihood function needs to be modified to account for the fact that we do not observe the exact number of events. Specifically:\nif \\(y_i\\) is the number of events, and \\(c\\) is the censoring threshold, then the likelihood function is given by:\n\\[\\begin{aligned}\n\\text{if }y&gt;c &\\rightarrow P(y=y_i) = \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!}  \\\\\n\\text{if }y \\leq c &\\rightarrow P(y \\leq c) = \\sum_{k=0}^c \\frac{e^{-\\lambda} \\lambda^{k }}{k!}\n\\end{aligned}\n\\]\n\n\nExercise\nAssume that you have data (\\(yc\\)) that is censored. You observe the exact number of events if more than 3 occured. If they are less than that, they are recoded as 3.\n\nModify the program mypoissonc in mypoisson.do to account for this censoring, and estimate the parameters of the model.\nUsing the simulated y,yc, x1 and x2, estimate the parameters of the model using the standard Poisson model (y), and the censored poisson model (yc). Compare the results, and discuss the implications of the censoring on the estimation of the parameters."
  },
  {
    "objectID": "adv_class/PO_RCT.html",
    "href": "adv_class/PO_RCT.html",
    "title": "RCT Implementation",
    "section": "",
    "text": "In this excecise, I’ll provide some simple examples of how to implement a basic RCT analysis.\nBefore we start, however, we need to create some data.\n\nEstimate two wages models. For men and women, using an heteroskedastic regression model.\n\nThe goal: estimate two potential outcomes for wages. One following Womens wage structure, and the other using Men wage structure.\n\n\nCode\nset linesize 255 \nfrause oaxaca, clear\nset seed 101\nqui:hetreg lnwage age agesq married divorced kids6 kids714 if female==0, het(age agesq married divorced kids6 kids714)\nest sto m1\npredict xb1, xb\npredict s1, sigma\nqui:hetreg lnwage age agesq married divorced kids6 kids714 if female==1, het(age agesq married divorced kids6 kids714)\nest sto m2\npredict xb0, xb\npredict s0, sigma\n\n\n\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n\nHere, as I did for UQregression, I obtain the predicted mean and predicted standard errors. and simulate two wages. I will use wage differences between simulated men and women predicted log wages as the treated effect\n\\[\n\\begin{aligned}\nTE= ln\\hat w_1 - ln\\hat w_0  \\\\\nln\\hat w_1 \\sim N(xb_1,s^2_1) \\\\\nln\\hat w_0 \\sim N(xb_0,s_0^2)\n\\end{aligned}\n\\]\n\n\nCode\ngen lnwage1 = rnormal(xb0,s0)\ngen lnwage0 = rnormal(xb1,s1)\ngen teff=lnwage1-lnwage0\n\n\nI will create a randomzed treatment, and assume those treated get the treatment effect\n\\[\nY_i = Y_i(0) + trt_i * \\delta_i\n\\]\n\n\nCode\ngen trt = runiform()&lt;.5\nreplace lnwage = lnwage0 + trt * teff\n\n\n(1,647 real changes made)"
  },
  {
    "objectID": "adv_class/PO_RCT.html#randomized-control-trial",
    "href": "adv_class/PO_RCT.html#randomized-control-trial",
    "title": "RCT Implementation",
    "section": "",
    "text": "In this excecise, I’ll provide some simple examples of how to implement a basic RCT analysis.\nBefore we start, however, we need to create some data.\n\nEstimate two wages models. For men and women, using an heteroskedastic regression model.\n\nThe goal: estimate two potential outcomes for wages. One following Womens wage structure, and the other using Men wage structure.\n\n\nCode\nset linesize 255 \nfrause oaxaca, clear\nset seed 101\nqui:hetreg lnwage age agesq married divorced kids6 kids714 if female==0, het(age agesq married divorced kids6 kids714)\nest sto m1\npredict xb1, xb\npredict s1, sigma\nqui:hetreg lnwage age agesq married divorced kids6 kids714 if female==1, het(age agesq married divorced kids6 kids714)\nest sto m2\npredict xb0, xb\npredict s0, sigma\n\n\n\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n\nHere, as I did for UQregression, I obtain the predicted mean and predicted standard errors. and simulate two wages. I will use wage differences between simulated men and women predicted log wages as the treated effect\n\\[\n\\begin{aligned}\nTE= ln\\hat w_1 - ln\\hat w_0  \\\\\nln\\hat w_1 \\sim N(xb_1,s^2_1) \\\\\nln\\hat w_0 \\sim N(xb_0,s_0^2)\n\\end{aligned}\n\\]\n\n\nCode\ngen lnwage1 = rnormal(xb0,s0)\ngen lnwage0 = rnormal(xb1,s1)\ngen teff=lnwage1-lnwage0\n\n\nI will create a randomzed treatment, and assume those treated get the treatment effect\n\\[\nY_i = Y_i(0) + trt_i * \\delta_i\n\\]\n\n\nCode\ngen trt = runiform()&lt;.5\nreplace lnwage = lnwage0 + trt * teff\n\n\n(1,647 real changes made)"
  },
  {
    "objectID": "adv_class/PO_RCT.html#visual-exploration",
    "href": "adv_class/PO_RCT.html#visual-exploration",
    "title": "RCT Implementation",
    "section": "Visual Exploration",
    "text": "Visual Exploration\nNow that we have a randomized treatment, we could start exploring the data:\n\n\nCode\ntwo (kdensity lnwage if trt == 1) (kdensity lnwage if trt == 0) , ///\n    legend(order(1 \"Treated\" 2 \"Untreated\"))\n\n\n\n\n\nLog wage distribution between Treated and untreated\n\n\n\n\nIn order to estimate the treatment effects, we could simple estimate a regression model of the outcome. Compare it to the treatment effect"
  },
  {
    "objectID": "adv_class/PO_RCT.html#estimation-of-ate-effect",
    "href": "adv_class/PO_RCT.html#estimation-of-ate-effect",
    "title": "RCT Implementation",
    "section": "Estimation of ATE Effect",
    "text": "Estimation of ATE Effect\n\n\nCode\n** True Effect\nsum teff\n** Simple Regression\nset linesize 255\nreg lnwage  trt, robust\nest sto m0\n\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        teff |      1,647   -.2121816    .6613419  -3.024343   2.704082\n\nLinear regression                               Number of obs     =      1,647\n                                                F(1, 1645)        =      79.85\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.0464\n                                                Root MSE          =      .5017\n\n------------------------------------------------------------------------------\n             |               Robust\n      lnwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         trt |  -.2213292   .0247679    -8.94   0.000    -.2699092   -.1727492\n       _cons |   3.465982   .0150123   230.88   0.000     3.436537    3.495427\n------------------------------------------------------------------------------\n\n\nBecause treatment is randomized, we could also add other controls to the model, and improve on precision\n\nCode\nqui:reg lnwage  trt age agesq , robust\nest sto m1\nqui:reg lnwage  trt age agesq married divorced , robust\nest sto m2\nqui:reg lnwage  trt age agesq married divorced kids6 kids714 , robust\nest sto m3\n\nesttab m0 m1 m2 m3, se nonum mtitle(\"m0\" \"m1\" \"m2\" \"m3\") keep(trt) md\n\n\n\n\n\n\n\n\n\n\n\n\nm0\nm1\nm2\nm3\n\n\n\n\ntrt\n-0.221***\n-0.214***\n-0.213***\n-0.212***\n\n\n\n(0.0248)\n(0.0225)\n(0.0225)\n(0.0224)\n\n\nN\n1647\n1647\n1647\n1647\n\n\n\nStandard errors in parentheses * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "adv_class/PO_RCT.html#falsification",
    "href": "adv_class/PO_RCT.html#falsification",
    "title": "RCT Implementation",
    "section": "Falsification",
    "text": "Falsification\nWe could just use other outcomes that shouldnt be affected by the treatment. You expect they have no impact on outcome\n\nCode\nqui:reg exper  trt age agesq married divorced kids6 kids714 , robust\nest sto m0\nqui:reg tenure trt age agesq married divorced kids6 kids714 , robust\nest sto m1\nesttab m0 m1 , se nonum mtitle(\"m0\" \"m1\") keep(trt) md\n\n\n\n\n\n\n\n\n\n\nm0\nm1\n\n\n\n\ntrt\n-0.104\n-0.458\n\n\n\n(0.366)\n(0.335)\n\n\nN\n1434\n1434\n\n\n\nStandard errors in parentheses * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "adv_class/PO_RCT.html#balance-test",
    "href": "adv_class/PO_RCT.html#balance-test",
    "title": "RCT Implementation",
    "section": "Balance test",
    "text": "Balance test\nYou should also try to create balance tables, where you compare and test if characteristics are similar across treated and control groups:\n\n\nCode\ntabstat age agesq married divorced kids6 kids714 , by(trt)\nsureg age agesq married divorced kids6 kids714 =trt, \n\n\n\nSummary statistics: Mean\nGroup variable: trt \n\n     trt |       age     agesq   married  divorced     kids6   kids714\n---------+------------------------------------------------------------\n       0 |  39.14475   1649.63    .53076  .1206273  .2979493  .3365501\n       1 |   39.3643  1675.521  .5158924  .1466993  .2713936  .3215159\n---------+------------------------------------------------------------\n   Total |  39.25379  1662.489  .5233758  .1335762  .2847602  .3290832\n----------------------------------------------------------------------\n\nSeemingly unrelated regression\n------------------------------------------------------------------------------\nEquation             Obs   Params         RMSE  \"R-squared\"      chi2   P&gt;chi2\n------------------------------------------------------------------------------\nage                1,647        1     11.02798      0.0001       0.16   0.6862\nagesq              1,647        1     893.7224      0.0002       0.35   0.5566\nmarried            1,647        1     .4993979      0.0002       0.36   0.5458\ndivorced           1,647        1     .3399466      0.0015       2.42   0.1197\nkids6              1,647        1     .6626276      0.0004       0.66   0.4161\nkids714            1,647        1     .7071256      0.0001       0.19   0.6662\n------------------------------------------------------------------------------\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nage          |\n         trt |   .2195505   .5434865     0.40   0.686    -.8456636    1.284765\n       _cons |   39.14475   .3830175   102.20   0.000     38.39405    39.89545\n-------------+----------------------------------------------------------------\nagesq        |\n         trt |   25.89111   44.04489     0.59   0.557    -60.43528    112.2175\n       _cons |    1649.63   31.04026    53.14   0.000     1588.792    1710.467\n-------------+----------------------------------------------------------------\nmarried      |\n         trt |  -.0148675   .0246116    -0.60   0.546    -.0631054    .0333703\n       _cons |     .53076   .0173448    30.60   0.000     .4967648    .5647552\n-------------+----------------------------------------------------------------\ndivorced     |\n         trt |    .026072   .0167534     1.56   0.120    -.0067641    .0589081\n       _cons |   .1206273   .0118068    10.22   0.000     .0974863    .1437682\n-------------+----------------------------------------------------------------\nkids6        |\n         trt |  -.0265557    .032656    -0.81   0.416    -.0905602    .0374488\n       _cons |   .2979493    .023014    12.95   0.000     .2528427     .343056\n-------------+----------------------------------------------------------------\nkids714      |\n         trt |  -.0150342   .0348489    -0.43   0.666    -.0833368    .0532685\n       _cons |   .3365501   .0245595    13.70   0.000     .2884143    .3846858\n------------------------------------------------------------------------------\n\n\nHere, the goal is just to see if trt is not-significant across groups"
  },
  {
    "objectID": "data_resources.html",
    "href": "data_resources.html",
    "title": "Data-Set Handbook",
    "section": "",
    "text": "Source: L.E. Papke (1995), “Participation in and Contributions to 401(k) Pension Plans: Evidence from Plan Data,” Journal of Human Resources 30, 311-325.\nProfessor Papke, of Michigan State University, kindly provided these data. She gathered them from the Internal Revenue Service’s Form 5500 tapes. Used in Text: pages 62, 76, 133, 169, 212-213, 656-657\nNotes: This data set is used in a variety of ways in the text. One additional possibility is to investigate whether the coefficients from the regression of prate on mrate, log(totemp) differ by whether the plan is a sole plan. The Chow test (see Section 7.4), and the less restrictive version that allows different intercepts, can be used.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: 401K.DES\nDataset title (if known): Not provided\nSuggested title: 401(k) Plan Characteristics\n\nThe dataset appears to contain information about 401(k) plans, including the participation rate, match rate, total participants, total eligible employees, age of the plan, total number of employees, and whether the 401(k) plan is the firm’s sole plan. The dataset also includes the logarithm of the total number of employees.\nPotential research ideas:\n\nExamine the relationship between the 401(k) plan match rate and the participation rate. This could provide insights into the effectiveness of employer contributions in encouraging employee participation in retirement savings plans.\nInvestigate the impact of the age of the 401(k) plan on the participation rate and total participants. This could reveal how the maturity of a plan affects employee engagement over time.\nAnalyze the differences in 401(k) plan characteristics between firms with the plan as their sole retirement offering and those with multiple retirement plans. This could shed light on the factors that influence an employer’s decision to offer a single or multiple retirement savings options.\nExplore the relationship between the total number of employees and the participation rate or total participants. This could help understand how the size of a firm affects the adoption and utilization of 401(k) plans.\nInvestigate the factors that influence the age of a 401(k) plan, such as the firm’s industry, size, or other organizational characteristics. This could provide insights into the decision-making process for establishing and maintaining retirement savings plans.\n\n\n\n\n\n\n\nSource: A. Abadie (2003), “Semiparametric Instrumental Variable Estimation of Treatment Response Models,” Journal of Econometrics 113, 231-263. Professor Abadie, now at MIT, kindly provided these data. He obtained them from the 1991 Survey of Income and Program Participation (SIPP). Used in Text: pages 160-161, 178, 217-218, 258-259, 276, 292, 528\nNotes: This data set can also be used to illustrate the binary response models, probit and logit, in Chapter 17, where, say, pira (an indicator for having an individual retirement account) is the dependent variable, and e401k [the 401(k) eligibility indicator] is the key explanatory variable.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: 401KSUBS.DES\nDataset title (if known): Not provided\nSuggested title: 401(k) Participation and Household Characteristics\n\nThe dataset contains information on 9,275 individuals and their characteristics related to 401(k) plan eligibility and participation. The variables include annual income, marital status, gender, age, family size, net total financial assets, 401(k) participation, individual retirement account (IRA) ownership, as well as the squared terms of income and age. This dataset provides insights into the demographic and financial factors associated with 401(k) plan eligibility and participation.\nPotential research ideas:\n\nExamining the relationship between household income and 401(k) plan participation: This study could investigate how income levels and the squared term of income (incsq) influence the likelihood of 401(k) plan participation, providing insights into the role of financial resources in retirement savings decisions.\nAnalyzing the impact of marital status and gender on 401(k) plan eligibility and participation: This research could explore how marital status (marr) and gender (male) affect an individual’s access to and engagement with 401(k) plans, potentially identifying any disparities in retirement savings opportunities.\nInvestigating the influence of age and family size on 401(k) plan participation: This study could examine how age, the squared term of age (agesq), and family size (fsize) are associated with 401(k) plan participation, shedding light on the life-cycle factors that shape retirement savings behavior.\nExploring the relationship between net total financial assets (nettfa) and 401(k) plan participation: This research could analyze how an individual’s overall financial resources, as measured by net total financial assets, are linked to their decision to participate in a 401(k) plan, providing insights into the role of wealth in retirement savings.\nComparing 401(k) plan participation and individual retirement account (IRA) ownership: This study could investigate the interplay between 401(k) plan participation (p401k) and IRA ownership (pira), examining how these two retirement savings vehicles are utilized by individuals with different demographic and financial characteristics."
  },
  {
    "objectID": "data_resources.html#k",
    "href": "data_resources.html#k",
    "title": "Data-Set Handbook",
    "section": "",
    "text": "Source: L.E. Papke (1995), “Participation in and Contributions to 401(k) Pension Plans: Evidence from Plan Data,” Journal of Human Resources 30, 311-325.\nProfessor Papke, of Michigan State University, kindly provided these data. She gathered them from the Internal Revenue Service’s Form 5500 tapes. Used in Text: pages 62, 76, 133, 169, 212-213, 656-657\nNotes: This data set is used in a variety of ways in the text. One additional possibility is to investigate whether the coefficients from the regression of prate on mrate, log(totemp) differ by whether the plan is a sole plan. The Chow test (see Section 7.4), and the less restrictive version that allows different intercepts, can be used.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: 401K.DES\nDataset title (if known): Not provided\nSuggested title: 401(k) Plan Characteristics\n\nThe dataset appears to contain information about 401(k) plans, including the participation rate, match rate, total participants, total eligible employees, age of the plan, total number of employees, and whether the 401(k) plan is the firm’s sole plan. The dataset also includes the logarithm of the total number of employees.\nPotential research ideas:\n\nExamine the relationship between the 401(k) plan match rate and the participation rate. This could provide insights into the effectiveness of employer contributions in encouraging employee participation in retirement savings plans.\nInvestigate the impact of the age of the 401(k) plan on the participation rate and total participants. This could reveal how the maturity of a plan affects employee engagement over time.\nAnalyze the differences in 401(k) plan characteristics between firms with the plan as their sole retirement offering and those with multiple retirement plans. This could shed light on the factors that influence an employer’s decision to offer a single or multiple retirement savings options.\nExplore the relationship between the total number of employees and the participation rate or total participants. This could help understand how the size of a firm affects the adoption and utilization of 401(k) plans.\nInvestigate the factors that influence the age of a 401(k) plan, such as the firm’s industry, size, or other organizational characteristics. This could provide insights into the decision-making process for establishing and maintaining retirement savings plans."
  },
  {
    "objectID": "data_resources.html#ksubs",
    "href": "data_resources.html#ksubs",
    "title": "Data-Set Handbook",
    "section": "",
    "text": "Source: A. Abadie (2003), “Semiparametric Instrumental Variable Estimation of Treatment Response Models,” Journal of Econometrics 113, 231-263. Professor Abadie, now at MIT, kindly provided these data. He obtained them from the 1991 Survey of Income and Program Participation (SIPP). Used in Text: pages 160-161, 178, 217-218, 258-259, 276, 292, 528\nNotes: This data set can also be used to illustrate the binary response models, probit and logit, in Chapter 17, where, say, pira (an indicator for having an individual retirement account) is the dependent variable, and e401k [the 401(k) eligibility indicator] is the key explanatory variable.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: 401KSUBS.DES\nDataset title (if known): Not provided\nSuggested title: 401(k) Participation and Household Characteristics\n\nThe dataset contains information on 9,275 individuals and their characteristics related to 401(k) plan eligibility and participation. The variables include annual income, marital status, gender, age, family size, net total financial assets, 401(k) participation, individual retirement account (IRA) ownership, as well as the squared terms of income and age. This dataset provides insights into the demographic and financial factors associated with 401(k) plan eligibility and participation.\nPotential research ideas:\n\nExamining the relationship between household income and 401(k) plan participation: This study could investigate how income levels and the squared term of income (incsq) influence the likelihood of 401(k) plan participation, providing insights into the role of financial resources in retirement savings decisions.\nAnalyzing the impact of marital status and gender on 401(k) plan eligibility and participation: This research could explore how marital status (marr) and gender (male) affect an individual’s access to and engagement with 401(k) plans, potentially identifying any disparities in retirement savings opportunities.\nInvestigating the influence of age and family size on 401(k) plan participation: This study could examine how age, the squared term of age (agesq), and family size (fsize) are associated with 401(k) plan participation, shedding light on the life-cycle factors that shape retirement savings behavior.\nExploring the relationship between net total financial assets (nettfa) and 401(k) plan participation: This research could analyze how an individual’s overall financial resources, as measured by net total financial assets, are linked to their decision to participate in a 401(k) plan, providing insights into the role of wealth in retirement savings.\nComparing 401(k) plan participation and individual retirement account (IRA) ownership: This study could investigate the interplay between 401(k) plan participation (p401k) and IRA ownership (pira), examining how these two retirement savings vehicles are utilized by individuals with different demographic and financial characteristics."
  },
  {
    "objectID": "data_resources.html#admnrev",
    "href": "data_resources.html#admnrev",
    "title": "Data-Set Handbook",
    "section": "ADMNREV",
    "text": "ADMNREV\nSource: Data from the National Highway Traffic Safety Administration: “A Digest of State Alcohol-Highway Safety Related Legislation,” U.S. Department of Transportation, NHTSA. I used the third (1985), eighth (1990), and 13th (1995) editions. Used in Text: not used\nNotes: This is not so much a data set as a summary of so-called “administrative per se” laws at the state level, for three different years. It could be supplemented with drunk-driving fatalities for a nice econometric analysis. In addition, the data for 2000 or later years can be added, forming the basis for a term project. Many other explanatory variables could be included. Unemployment rates, state-level tax rates on alcohol, and membership in MADD are just a few possibilities.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: ADMNREV.DES\nDataset title (if known): Not provided\nSuggested title: Administrative Revocation Laws and Suspension Days\n\nThe dataset contains information on administrative revocation laws and the number of days suspended for first and second offenses across different states and years. The variables include the state postal code, the year (1985, 1990, or 1995), a binary indicator for whether an administrative revocation law was in place, and the number of days suspended for first and second offenses.\nPotential research ideas:\n\nExamine the relationship between the presence of administrative revocation laws and the number of days suspended for first and second offenses. This could provide insights into the effectiveness of these laws in deterring drunk driving.\nInvestigate the variation in suspension days across states and over time, and explore potential factors (e.g., demographic, economic, or policy changes) that may influence these differences.\nAnalyze the impact of administrative revocation laws on traffic safety outcomes, such as the incidence of alcohol-related crashes or fatalities, to assess the broader societal implications of these policies.\nExplore the differences in suspension days between first and second offenses, and investigate whether these differences are consistent across states or if there are notable variations that could inform policy decisions.\nConduct a comparative analysis of administrative revocation laws and their implementation across different states, identifying best practices or areas for improvement in the design and enforcement of these policies."
  },
  {
    "objectID": "data_resources.html#affairs",
    "href": "data_resources.html#affairs",
    "title": "Data-Set Handbook",
    "section": "AFFAIRS",
    "text": "AFFAIRS\nSource: R.C. Fair (1978), “A Theory of Extramarital Affairs,” Journal of Political Economy 86, 45-61, 1978.\nI collected the data from Professor Fair’s web cite at the Yale University Department of Economics. He originally obtained the data from a survey by Psychology Today. Used in Text: not used\nNotes: This is an interesting data set for problem sets starting in Chapter 7. Even though naffairs (number of extramarital affairs a woman reports) is a count variable, a linear model can be used to model its conditional mean as an approximation. Or, you could ask the students to estimate a linear probability model for the binary indicator affair, equal to one of the woman reports having any extramarital affairs. One possibility is to test whether putting the single marriage rating variable, ratemarr, is enough, against the alternative that a full set of dummy variables is needed; see pages 239-240 for a similar example. This is also a good data set to illustrate Poisson regression (using naffairs) in Section 17.3 or probit and logit (using affair) in Section 17.1.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: AFFAIRS.DES\nDataset title (if known): Not provided\nSuggested title: Marital Affairs and Relationship Satisfaction\n\nThe dataset contains information on 601 individuals, including their gender, age, years married, whether they have children, religious beliefs, education level, occupation, marital satisfaction, number of affairs within the last year, and whether they had at least one affair. The variables provide insights into the relationship between various demographic and personal characteristics and marital affairs and satisfaction.\nPotential research ideas:\n\nInvestigate the relationship between gender, age, and the likelihood of having an affair. This could provide insights into the factors that contribute to infidelity in marriages.\nExamine the association between religious beliefs and marital satisfaction, as well as the likelihood of engaging in affairs. This could shed light on the role of religious and spiritual beliefs in shaping relationship dynamics.\nExplore the impact of education level and occupation on marital satisfaction and the prevalence of affairs. This could help identify socioeconomic factors that influence relationship outcomes.\nAnalyze the relationship between the number of children and the likelihood of having an affair. This could offer insights into how the presence of children affects the dynamics of a marriage.\nInvestigate the interplay between marital satisfaction, as measured by the “ratemarr” variable, and the occurrence of affairs. This could provide a deeper understanding of the factors that contribute to relationship stability and the decision to engage in infidelity."
  },
  {
    "objectID": "data_resources.html#airfare",
    "href": "data_resources.html#airfare",
    "title": "Data-Set Handbook",
    "section": "AIRFARE",
    "text": "AIRFARE\nSource: Jiyoung Kwon, a former doctoral student in economics at MSU, kindly provided these data, which she obtained from the Domestic Airline Fares Consumer Report by the U.S. Department of Transportation.\nUsed in Text: pages 476, 488, 557, 557\nNotes: This data set nicely illustrates the different estimates obtained when applying pooled OLS, random effects, and fixed effects.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: AIRFARE.DES\nDataset title (if known): Not provided\nSuggested title: Airline Fare and Passenger Data\n\nThe dataset contains information about airline fares and passenger volumes for various routes over the years 1997 to 2000. The variables include the year, origin and destination airports, a unique route identifier, the distance of the route, the average number of passengers per day, the average one-way fare, the market share of the largest carrier, and several derived variables such as the logarithm of distance, fare, and passengers.\nPotential research ideas:\n\nAnalyze the relationship between route distance and average fare, and investigate how this relationship has changed over the years.\nExamine the impact of market concentration (as measured by the largest carrier’s market share) on average fares and passenger volumes.\nExplore the factors that influence the average number of passengers per day on a given route, such as distance, fare, and market concentration.\nInvestigate the role of the economic conditions (as indicated by the year dummies) in shaping the trends in airline fares and passenger volumes.\nDevelop a predictive model to forecast average fares or passenger volumes based on the available variables, and assess the model’s performance over time."
  },
  {
    "objectID": "data_resources.html#alcohol",
    "href": "data_resources.html#alcohol",
    "title": "Data-Set Handbook",
    "section": "ALCOHOL",
    "text": "ALCOHOL\nSource: Terza, J.V. (2002), “Alcohol Abuse and Employment: A Second Look,” Journal of Applied Econometrics 17, 393-404. I obtained these data from the Journal of Applied Econometrics data archive at http://qed.econ.queensu.ca/jae/. Used in Text: page 600\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: alcohol.des\nDataset title (if known): Not provided\nSuggested title: Alcohol Consumption and Socioeconomic Factors\n\nThe dataset appears to contain information on alcohol consumption, employment status, health status, demographic characteristics, and various state-level factors that may influence alcohol consumption. The variables include measures of alcohol abuse, employment status, unemployment rate, age, education, marital status, family size, race, health status, geographic location, state-level taxes on alcohol and cigarettes, and state-level per-capita ethanol consumption. The dataset also includes information on whether the individual’s parents or the individual themselves were alcoholics, and whether the individual lived with an alcoholic.\nPotential research ideas:\nExamine the relationship between individual characteristics (e.g., age, education, marital status, family size, race) and the likelihood of alcohol abuse. This could provide insights into the socioeconomic factors that contribute to problematic alcohol consumption.\nInvestigate the impact of state-level policies, such as alcohol and cigarette taxes, on individual alcohol consumption and abuse. This could inform policymakers on the effectiveness of such measures in reducing harmful drinking behaviors.\nAnalyze the role of parental and personal history of alcoholism in an individual’s own alcohol consumption and abuse. This could shed light on the intergenerational transmission of alcohol-related issues and the importance of early intervention and support.\nExplore the relationship between an individual’s health status and their alcohol consumption, both in terms of the impact of alcohol on health and the potential use of alcohol as a coping mechanism for poor health.\nInvestigate the interplay between employment status, unemployment rate, and alcohol consumption. This could help identify vulnerable populations and inform policies aimed at addressing the intersection of economic and substance abuse issues."
  },
  {
    "objectID": "data_resources.html#apple",
    "href": "data_resources.html#apple",
    "title": "Data-Set Handbook",
    "section": "APPLE",
    "text": "APPLE\nSource: These data were used in the doctoral dissertation of Jeffrey Blend, Department of Agricultural Economics, Michigan State University, 1998. The thesis was supervised by Professor Eileen van Ravensway. Drs. Blend and van Ravensway kindly provided the data, which were obtained from a telephone survey conducted by the Institute for Public Policy and Social Research at MSU. Used in Text: pages 195, 217, 259-260, 598\nNotes: This data set is close to a true experimental data set because the price pairs facing a family were randomly determined. In other words, the family head was presented with prices for the eco-labeled and regular apples, and then asked how much of each kind of apple the family would buy at the given prices. As predicted by basic economics, the own price effect is negative (and strong) and the cross price effect is positive (and strong). While the main dependent variable, ecolbs, piles up at zero, estimating a linear model is still worthwhile. Interestingly, because the survey design induces a strong positive correlation between the prices of eco-labeled and regular apples, there is an omitted variable problem if either of the price variables is dropped from the demand equation. A good exam question is to show a simple regression of ecolbs on ecoprc and then a multiple regression on both prices, and ask students to decide whether the price variables must be positively or negatively correlated.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: APPLE.DES\nDataset title (if known): Not provided\nSuggested title: Apple Purchasing Behavior\n\nThe dataset appears to contain information about apple purchasing behavior of households. It includes variables related to the respondent’s demographics (education, gender, age, household size, and family income), the prices of regular and eco-labeled apples, the quantity of regular and eco-labeled apples purchased, and the number of household members in different age groups. The data was collected from 660 respondents across different states and during different times of the year.\nPotential research ideas:\n\nInvestigating the impact of household demographics on the purchase of regular and eco-labeled apples: This study could explore how factors such as education, income, household size, and age composition influence the demand for different types of apples.\nAnalyzing the relationship between apple prices and purchasing behavior: Researchers could examine how changes in the prices of regular and eco-labeled apples affect the quantity of each type of apple purchased, and whether price sensitivity varies across different demographic groups.\nExploring the role of seasonality in apple purchasing: The dataset includes a variable indicating whether the respondent was interviewed in November, which could be used to investigate how the timing of the interview affects apple purchasing patterns.\nExamining the impact of eco-labeling on consumer choice: Researchers could investigate how the availability of eco-labeled apples and their price relative to regular apples influence consumers’ decisions to purchase eco-labeled products.\nDeveloping a predictive model for apple purchasing behavior: Using the available variables, researchers could build a model to predict the quantity of regular and eco-labeled apples purchased by households, which could have practical applications for apple producers and retailers."
  },
  {
    "objectID": "data_resources.html#approval",
    "href": "data_resources.html#approval",
    "title": "Data-Set Handbook",
    "section": "APPROVAL",
    "text": "APPROVAL\nSource: Harbridge, L., J. Krosnick, and J.M. Wooldridge (2016), “Presidential Approval and Gas Prices: Sociotropic or Pocketbook Influence?” In New Explorations in Political Psychology, ed. J. Krosnick. New York: Psychology Press (Taylor and Francis Group) Professor Harbridge kindly provided the data, of which I have used a subset. Used in Text: 365, 393, 424\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: approval.des\nDataset title (if known): Not provided\nSuggested title: Approval Ratings and Economic Indicators\n\nThe dataset contains monthly data on various economic indicators and the Gallup approval rate from 1978 to 2004. The variables include the S&P 500 index, Consumer Price Index (CPI), CPI for food, average gas price, unemployment rate, and dummy variables for events such as Hurricane Katrina, the 9/11 attacks, and the Iraq invasion. The dataset allows for the analysis of the relationship between economic conditions and public approval ratings.\nPotential research ideas:\n\nInvestigate the impact of changes in gas prices on the Gallup approval rate. This could involve analyzing the relationship between real gas prices and approval ratings, and exploring whether the effect varies depending on the magnitude or direction of gas price changes.\nExamine the relationship between the unemployment rate and approval ratings. This could include analyzing whether changes in the unemployment rate lead to changes in approval ratings, and whether the effect is symmetric (i.e., does an increase in unemployment have the same magnitude of impact as a decrease).\nExplore the impact of major events, such as Hurricane Katrina, the 9/11 attacks, and the Iraq invasion, on approval ratings. This could involve assessing the short-term and long-term effects of these events on public approval.\nInvestigate the relationship between inflation (as measured by the CPI and CPI for food) and approval ratings. This could include analyzing whether changes in inflation, particularly in food prices, have a significant impact on public approval.\nAnalyze the dynamic relationship between the S&P 500 index and approval ratings. This could involve examining whether changes in the stock market lead to changes in approval ratings, and whether the effect is asymmetric (i.e., do positive and negative stock market movements have different impacts)."
  },
  {
    "objectID": "data_resources.html#athlet1",
    "href": "data_resources.html#athlet1",
    "title": "Data-Set Handbook",
    "section": "ATHLET1",
    "text": "ATHLET1\nSources:: Peterson’s Guide to Four Year Colleges, 1994 and 1995 (24th and 25th editions). Princeton University Press. Princeton, NJ. The Official 1995 College Basketball Records Book, 1994, NCAA. 1995 Information Please Sports Almanac (6th edition). Houghton Mifflin. New York, NY. Used in Text: page 661\nNotes: These data were collected by Patrick Tulloch, an MSU economics major, for a term project. The “athletic success” variables are for the year prior to the enrollment and academic data. Updating these data to get a longer stretch of years, and including appearances in the “Sweet 16” NCAA basketball tournaments, would make for a more convincing analysis. With the growing popularity of women’s sports, especially basketball, an analysis that includes success in women’s athletics would be interesting.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: ATHLET1.DES\nDataset title (if known): Not provided\nSuggested title: College Athletics Dataset\n\nThe dataset appears to contain information about various characteristics of colleges and their athletic programs, including the number of applications, academic performance of the student body, student-faculty ratio, and performance in men’s basketball (e.g., conference championships, Final Four appearances, and bowl game appearances). The data covers 118 observations, likely representing different colleges or universities, over the years 1992 and 1993.\nPotential research ideas:\n\nExamine the relationship between academic performance (e.g., average SAT scores, percentage of students in the top 25% of their high school class) and athletic success (e.g., conference championships, Final Four appearances, bowl game appearances). This could provide insights into the balance between academic and athletic priorities at these institutions.\nInvestigate the impact of athletic success on the number of applications received by the college or university. This could help understand the potential marketing and recruitment benefits of a successful athletic program.\nAnalyze the changes in various metrics (e.g., applications, academic performance, student-faculty ratio) between 1992 and 1993 to identify any trends or patterns in the data.\nExplore the relationship between a college’s athletic success and its student-faculty ratio, as this could provide insights into the allocation of resources and priorities within the institution.\nInvestigate the potential differences in the characteristics and athletic performance of colleges with and without a history of bowl game appearances or Final Four appearances in men’s basketball. This could reveal insights into the factors that contribute to sustained athletic success."
  },
  {
    "objectID": "data_resources.html#athlet2",
    "href": "data_resources.html#athlet2",
    "title": "Data-Set Handbook",
    "section": "ATHLET2",
    "text": "ATHLET2\nSources:: Peterson’s Guide to Four Year Colleges, 1995 (25th edition). Princeton University Press. 1995 Information Please Sports Almanac (6th edition). Houghton Mifflin. New York, NY Used in Text: page 661\nNotes: These data were collected by Paul Anderson, an MSU economics major, for a term project. The score from football outcomes for natural rivals (Michigan-Michigan State, California-Stanford, Florida-Florida State, to name a few) is matched with application and academic data. The application and tuition data are for Fall 1994. Football records and scores are from 1993 football season. Extended these data to obtain a long stretch of panel data and other “natural” rivals could be very interesting.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: ATHLET2.DES\nDataset title (if known): Not provided\nSuggested title: College Basketball Game Outcomes and University Characteristics\n\nThe dataset appears to contain information about college basketball games played in 1993 and 1994. The variables include the difference in scores between the home and visiting teams, the difference in in-state and out-of-state tuition, whether the home and visiting teams are private or public schools, the difference in the number of applications, the winning records of the home and visiting teams, the difference in winning records, and the difference in whether the home and visiting teams are private or public schools.\nPotential research ideas:\n\nInvestigate the relationship between the difference in in-state and out-of-state tuition and the outcome of college basketball games. This could provide insights into the potential impact of financial factors on athletic performance.\nExplore the differences in game outcomes between private and public universities. This could shed light on the potential advantages or disadvantages of different types of institutions in college sports.\nAnalyze the impact of the difference in the number of applications on the outcome of college basketball games. This could help understand the relationship between institutional popularity and athletic success.\nExamine the influence of the home team’s winning record on the game outcome. This could provide insights into the potential advantages of home-court advantage in college basketball.\nInvestigate the relationship between the difference in winning records and the game outcome. This could help understand the role of team performance in predicting the results of college basketball games."
  },
  {
    "objectID": "data_resources.html#attend",
    "href": "data_resources.html#attend",
    "title": "Data-Set Handbook",
    "section": "ATTEND",
    "text": "ATTEND\nSource: These data were collected by Professors Ronald Fisher and Carl Liedholm during a term in which they both taught principles of microeconomics at Michigan State University. Professors Fisher and Liedholm kindly gave me permission to use a random subset of their data, and their research assistant at the time, Jeffrey Guilfoyle, who completed his Ph.D. in economics at MSU, provided helpful hints. Used in Text: pages 110, 146, 193-194, 216\nNotes: The attendance figures were obtained by requiring students to slide their ID cards through a magnetic card reader, under the supervision of a teaching assistant. You might have the students use final, rather than the standardized variable, so that they can see the statistical significance of each variable remains exactly the same. The standardized variable is used only so that the coefficients measure effects in terms of standard deviations from the average score.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: ATTEND.DES\nDataset title (if known): Not provided\nSuggested title: Student Attendance and Academic Performance\n\nThe dataset contains information about 680 students, including their class attendance, academic performance, and demographic characteristics. The variables include the number of classes attended, term GPA, cumulative GPA prior to the term, ACT score, final exam score, percentage of classes attended, percentage of homework turned in, and whether the student is a freshman or sophomore. Additionally, the dataset includes the number of classes skipped and a standardized final exam score.\nPotential research ideas:\n\nInvestigate the relationship between class attendance and academic performance: Analyze the impact of class attendance, as measured by the number of classes attended and the percentage of classes attended, on student GPA and final exam scores. This could provide insights into the importance of class attendance for academic success.\nExplore the influence of prior academic achievement on current performance: Examine the relationship between the students’ cumulative GPA prior to the term and their term GPA and final exam scores. This could help identify the role of previous academic performance in predicting current outcomes.\nAssess the impact of demographic factors on academic performance: Investigate whether the students’ freshman or sophomore status has any significant effect on their academic performance, as measured by GPA and final exam scores. This could provide insights into the challenges and opportunities faced by students at different stages of their academic journey.\nAnalyze the relationship between homework completion and academic performance: Explore the association between the percentage of homework turned in and student GPA and final exam scores. This could shed light on the importance of consistent homework completion for academic success.\nDevelop a predictive model for student academic performance: Utilize the available variables, such as class attendance, prior GPA, ACT score, and homework completion, to create a predictive model that can estimate a student’s final exam score or term GPA. This could help identify the key factors that contribute to academic performance and inform interventions to support student success."
  },
  {
    "objectID": "data_resources.html#audit",
    "href": "data_resources.html#audit",
    "title": "Data-Set Handbook",
    "section": "AUDIT",
    "text": "AUDIT\nSource: These data come from a 1988 Urban Institute audit study in the Washington, D.C. area. I obtained them from the article “The Urban Institute Audit Studies: Their Methods and Findings,” by James J. Heckman and Peter Siegelman. In Fix, M. and Struyk, R., eds., Clear and Convincing Evidence: Measurement of Discrimination in America. Washington, D.C.: Urban Institute Press, 1993, 187-258. Used in Text: pages 732, 738, 741\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: AUDIT.DES\nDataset title (if known): Not provided\nSuggested title: Job Offer Outcomes for White and Black Applicants\n\nThe dataset appears to contain information on the job offer outcomes for white and black applicants. The variables include a binary indicator for whether a white applicant received a job offer (w), a binary indicator for whether a black applicant received a job offer (b), and the difference between the black and white offer rates (y). The dataset contains 241 observations.\nPotential research ideas:\n\nInvestigate the relationship between race and job offer outcomes: Analyze the differences in job offer rates between white and black applicants, and explore potential factors that may contribute to these disparities, such as discrimination, qualifications, or other socioeconomic factors.\nExamine the impact of applicant characteristics on job offer decisions: Explore whether factors like education, work experience, or other applicant attributes influence the likelihood of receiving a job offer, and whether these effects differ between white and black applicants.\nAssess the role of employer characteristics in hiring decisions: Investigate whether the characteristics of the hiring organization, such as industry, size, or diversity policies, are associated with the observed differences in job offer rates between white and black applicants.\nExplore the relationship between job offer outcomes and subsequent employment or career trajectories: Analyze whether the differences in job offer rates between white and black applicants have long-term implications for their employment and career development.\nEvaluate the effectiveness of interventions or policies aimed at reducing racial disparities in hiring: Assess the impact of initiatives, such as diversity training, blind hiring practices, or affirmative action policies, on the job offer outcomes for white and black applicants."
  },
  {
    "objectID": "data_resources.html#barium",
    "href": "data_resources.html#barium",
    "title": "Data-Set Handbook",
    "section": "BARIUM",
    "text": "BARIUM\nSource: C.M. Krupp and P.S. Pollard (1999), “Market Responses to Antidumpting Laws: Some Evidence from the U.S. Chemical Industry,” Canadian Journal of Economics 29, 199-227.\nDr. Krupp kindly provided the data. They are monthly data covering February 1978 through December 1988. Used in Text: pages 349-350, 359, 363, 407, 410-411, 422, 424, 631-632, 639\nNote: Rather than just having intercept shifts for the different regimes, one could conduct a full Chow test across the different regimes.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: BARIUM.DES\nDataset title (if known): Not provided\nSuggested title: Barium Import and Economic Indicators\n\nThe dataset appears to contain information related to Chinese imports, total imports, and various economic indicators such as chemical production index, gasoline production, exchange rate index, and seasonal variables. The data spans a period of time and includes both level and log-transformed variables.\nPotential research ideas:\n\nInvestigate the relationship between Chinese imports and total imports, and how this relationship is influenced by economic indicators such as chemical production, gasoline production, and exchange rates.\nAnalyze the seasonal patterns in Chinese imports and total imports, and explore how these patterns are affected by the economic indicators.\nExamine the impact of the 6-month and 12-month periods before and after filing and decision events on Chinese imports and total imports.\nDevelop a forecasting model to predict Chinese imports and total imports based on the economic indicators and seasonal factors.\nExplore the role of the percentage of imports from China in the overall import dynamics and its relationship with the other variables in the dataset."
  },
  {
    "objectID": "data_resources.html#beauty",
    "href": "data_resources.html#beauty",
    "title": "Data-Set Handbook",
    "section": "BEAUTY",
    "text": "BEAUTY\nSource: Hamermesh, D.S. and J.E. Biddle (1994), “Beauty and the Labor Market,” American Economic Review 84, 1174-1194. Professor Hamermesh kindly provided me with the data. For manageability, I have included only a subset of the variables, which results in somewhat larger sample sizes than reported for the regressions in the Hamermesh and Biddle paper. Used in Text: pages 231, 259\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: BEAUTY.DES\nDataset title (if known): Not provided\nSuggested title: Beauty and Wages Dataset\n\nThe dataset contains information on various factors that may influence an individual’s hourly wage, including physical appearance, education, work experience, and demographic characteristics. The variables include hourly wage, log of hourly wage, indicators for above-average and below-average physical appearance, years of work experience, a measure of physical appearance on a scale of 1 to 5, union membership, self-reported health status, race, gender, marital status, region of residence, and industry of employment.\nPotential research ideas:\nInvestigate the relationship between physical appearance and hourly wages. Examine whether individuals with above-average or below-average physical appearance experience a wage premium or penalty, and explore the potential mechanisms underlying this relationship.\nAnalyze the interplay between work experience, education, and wages. Explore how the returns to experience and education may vary based on an individual’s physical appearance, and whether there are any interactions between these factors in determining hourly wages.\nAssess the role of demographic characteristics, such as race, gender, and marital status, in shaping wage outcomes. Investigate whether these factors have independent effects on wages or if they interact with physical appearance and other human capital variables.\nExplore the impact of union membership and industry of employment on hourly wages. Examine whether the effects of physical appearance, experience, and education differ across unionized and non-unionized sectors, as well as across different industries.\nInvestigate the potential mediating or moderating role of self-reported health status in the relationship between physical appearance and wages. Explore whether good health status amplifies or attenuates the impact of physical appearance on hourly wages."
  },
  {
    "objectID": "data_resources.html#benefits",
    "href": "data_resources.html#benefits",
    "title": "Data-Set Handbook",
    "section": "BENEFITS",
    "text": "BENEFITS\nSource: I collected these data from the old Michigan Department of Education web site, which used to have an annual Michigan Schools Report. I used data on most elementary schools in the state of Michigan for 1993. I dropped some schools that had suspicious-looking data. Used in Text: 218\nNotes: This is an elementary school-level version of MEAP93, which contains data for high schools.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: benefits.des\nDataset title (if known): Not provided\nSuggested title: School District and School-Level Data on Enrollment, Staffing, Expenditures, and Student Performance\n\nThe dataset contains information on various characteristics of school districts and individual schools, including enrollment, staffing, expenditures per pupil, average teacher salaries and benefits, and student performance on 4th-grade math and reading tests. The data includes both school-level and district-level variables, allowing for analysis of both individual schools and the broader district context.\nPotential research ideas:\n\nExamine the relationship between school-level characteristics (e.g., enrollment, staffing, expenditures) and student performance on standardized tests. This could help identify factors that contribute to academic success at the school level.\nInvestigate the impact of teacher salaries and benefits on school-level outcomes, such as student achievement, teacher retention, and school climate. This could provide insights into the role of teacher compensation in educational quality.\nAnalyze the variation in school-level characteristics and student performance within and across school districts. This could reveal patterns of educational equity or disparities within the broader district context.\nExplore the relationship between district-level variables (e.g., average lunch eligibility, average enrollment, average staffing) and school-level outcomes. This could shed light on the influence of district-level factors on individual schools.\nConduct a comparative analysis of school-level and district-level characteristics to identify potential areas for policy interventions or resource allocation to improve educational outcomes."
  },
  {
    "objectID": "data_resources.html#big9salary",
    "href": "data_resources.html#big9salary",
    "title": "Data-Set Handbook",
    "section": "BIG9SALARY",
    "text": "BIG9SALARY\nSource: O. Baser and E. Pema (2003), “The Return of Publications for Economics Faculty,” Economics Bulletin 1, 1-13. Professors Baser and Pema kindly provided the data. Used in Text: not used\nNotes: This is an unbalanced panel data set in the sense that as many as three years of data are available for each faculty member but where some have fewer than three years. It is not clear that something like a fixed effects or first differencing analysis makes sense: in effect, approaches that remove the heterogeneity control for too much by controlling for unobserved heterogeneity – which, in this case, includes faculty intelligence, talent, and motivation. Presumably, these factors enter into the publication index. It is hard to think we want to hold the main factors driving productivity fixed when trying to measure the effect of productivity on salary. Pooled OLS regression with “cluster robust” standard errors seems more natural. On the other hand, if we want to measure the return to having a degree from a top 20 Ph.D. program then we would want to control for factors that cause selection into a top 20 program. Unfortunately, this variable does not change over time, and so FD and FE are not applicable. One could include the top 20 dummy variable in a correlated random effects analysis.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: big9salary.des\nDataset title (if known): Not provided\nSuggested title: Faculty Salaries and Characteristics in the Big Ten Universities\n\nThe dataset contains information on the salaries and characteristics of faculty members from several major universities in the United States, including Ohio State University, the University of Iowa, Indiana University, Purdue University, Michigan State University, the University of Minnesota, the University of Michigan, the University of Wisconsin, and the University of Illinois. The variables include individual identifiers, year, salary, publication index, standardized total article pages, academic rank (assistant, associate, or full professor), administrative position (department chair), gender, and various university affiliations.\nPotential research ideas:\n\nExamine the relationship between faculty salaries and their publication productivity, controlling for factors such as academic rank, years of experience, and university affiliation. This could provide insights into the relative importance of research output in faculty compensation.\nInvestigate the gender pay gap among faculty members, exploring whether differences in salary persist even after accounting for factors like academic rank, publication record, and years of experience. This could shed light on potential gender-based disparities in faculty compensation.\nAnalyze the impact of administrative positions, such as department chair, on faculty salaries. This could help understand the potential salary premium associated with taking on additional administrative responsibilities.\nExplore the differences in faculty salaries across the various universities represented in the dataset, and investigate whether these differences can be explained by factors like institutional prestige, research productivity, or cost of living.\nAssess the relationship between the year a faculty member obtained their Ph.D. and their current salary, considering factors like academic rank and publication record. This could provide insights into the long-term career trajectories and salary progression of faculty members."
  },
  {
    "objectID": "data_resources.html#bwght",
    "href": "data_resources.html#bwght",
    "title": "Data-Set Handbook",
    "section": "BWGHT",
    "text": "BWGHT\nSource: J. Mullahy (1997), “Instrumental-Variable Estimation of Count Data Models: Applications to Models of Cigarette Smoking Behavior,” Review of Economics and Statistics 79, 596-593. Professor Mullahy kindly provided the data. He obtained them from the 1988 National Health Interview Survey. Used in Text: pages 16, 58, 109, 145, 172, 178, 181-182, 251-252, 504\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: BWGHT.DES\nDataset title (if known): Not provided\nSuggested title: Birth Weight and Maternal Smoking Dataset\n\nThe dataset contains information on birth weight, maternal smoking, family income, and other demographic variables for 1,388 observations. The variables include birth weight in ounces and pounds, log of birth weight, family income in 1988, cigarette tax and price in the home state, father’s and mother’s education levels, birth order, gender, and race. The data appears to be focused on examining the relationship between maternal smoking during pregnancy and birth weight, as well as the potential influence of other socioeconomic and demographic factors.\nPotential research ideas:\nInvestigate the impact of maternal smoking during pregnancy on birth weight, controlling for other socioeconomic and demographic factors. This could involve regression analysis to quantify the relationship and explore potential mediating or moderating effects.\nExamine the role of family income and parental education levels on birth weight, both directly and indirectly through their influence on maternal smoking behavior. This could provide insights into the socioeconomic determinants of birth outcomes.\nAnalyze the relationship between cigarette tax and price in the home state and maternal smoking behavior during pregnancy. This could inform public health policies aimed at reducing smoking among pregnant women.\nInvestigate potential differences in birth weight and maternal smoking patterns between male and female children, as well as across different birth orders. This could shed light on gender and birth order-specific factors affecting birth outcomes.\nExplore the feasibility of using log-transformed birth weight as the dependent variable in the analysis, as it may provide a more appropriate functional form for modeling the relationship between the variables."
  },
  {
    "objectID": "data_resources.html#bwght2",
    "href": "data_resources.html#bwght2",
    "title": "Data-Set Handbook",
    "section": "BWGHT2",
    "text": "BWGHT2\nSource: Dr. Zhehui Luo, a professor of epidemiology and biostatistics at MSU, kindly provided these data. She obtained them from state files linking birth and infant death certificates, and from the National Center for Health Statistics natality and mortality data. Used in Text: pages 178, 217\nNotes: There are many possibilities with this data set. In addition to number of prenatal visits, smoking and alcohol consumption (during pregnancy) are included as explanatory variables. These can be added to equations of the kind found in Exercise C6.10. In addition, the one- and five-minute APGAR scores are included. These are measures of the well being of infants just after birth. An interesting feature of the score is that it is bounded between zero and 10, making a linear model less than ideal. Still, a linear model would be informative, and you might ask students about predicted values less than zero or greater than 10.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: BWGHT2.DES\nDataset title (if known): Not provided\nSuggested title: Birth Weight and Prenatal Care Dataset\n\nThe dataset contains information on various factors related to birth weight, including the mother’s age, education, and prenatal care, as well as the father’s age and education. The dataset also includes information on the baby’s gender, the mother’s and father’s race, and measures of birth weight, including low birth weight (LBW) and very low birth weight (VLBW). The dataset has 1,832 observations.\nPotential research ideas:\n\nInvestigate the relationship between the mother’s age and birth weight. The dataset includes the mother’s age (mage) and its square (magesq), which could be used to explore potential non-linear effects.\nExamine the impact of the number of prenatal visits (npvis) and its square (npvissq) on birth weight. This could provide insights into the optimal number of prenatal visits for healthy birth outcomes.\nAnalyze the differences in birth weight between babies born to mothers of different races (mwhte, mblck, moth) and fathers of different races (fwhte, fblck, foth). This could shed light on potential disparities in birth outcomes.\nExplore the relationship between the mother’s and father’s education levels (meduc and feduc) and birth weight. This could help identify the role of parental education in promoting healthy birth outcomes.\nInvestigate the effects of maternal smoking (cigs) and alcohol consumption (drink) during pregnancy on birth weight. This could contribute to our understanding of the risks associated with these behaviors during pregnancy."
  },
  {
    "objectID": "data_resources.html#campus",
    "href": "data_resources.html#campus",
    "title": "Data-Set Handbook",
    "section": "CAMPUS",
    "text": "CAMPUS\nSource: These data were collected by Daniel Martin, a former MSU undergraduate, for a final project. They come from the FBI Uniform Crime Reports and are for the year 1992. Used in Text: pages 128-129\nNotes: Colleges and universities are now required to provide much better, more detailed crime data. A very rich data set can now be obtained, even a panel data set for colleges across different years. Statistics on male/female ratios, fraction of men/women in fraternities or sororities, policy variables – such as a “safe house” for women on campus, as was started at MSU in 1994 – could be added as explanatory variables. The crime rate in the host town would be a good control.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: CAMPUS.DES\nDataset title (if known): Not provided\nSuggested title: Campus Crime and Enrollment Data\n\nThe dataset appears to contain information about college campuses, including total enrollment, whether the college is private or not, the number of employed police officers, and the total number of campus crimes. The variables provided suggest that this dataset could be used to analyze the relationship between campus characteristics, such as enrollment and private status, and campus crime.\nPotential research ideas:\n\nInvestigate the relationship between college enrollment and campus crime. This could involve examining whether larger campuses tend to have higher levels of crime, or whether there are differences in crime rates between private and public colleges.\nAnalyze the impact of campus police presence on crime rates. The dataset includes information on the number of employed police officers, which could be used to determine if a greater police presence is associated with lower levels of campus crime.\nExplore the factors that contribute to the severity of campus crime. The dataset includes a variable for the log of total campus crimes, which could be used to identify the characteristics of colleges that experience more serious criminal incidents.\nExamine the relationship between college enrollment and the log of campus crimes. This could provide insights into how changes in enrollment size might affect the overall level of criminal activity on campus.\nInvestigate the differences in crime rates between private and public colleges. The dataset includes a variable indicating whether a college is private or not, which could be used to compare the crime patterns in these two types of institutions."
  },
  {
    "objectID": "data_resources.html#card",
    "href": "data_resources.html#card",
    "title": "Data-Set Handbook",
    "section": "CARD",
    "text": "CARD\nSource: D. Card (1995), “Using Geographic Variation in College Proximity to Estimate the Return to Schooling,” in Aspects of Labour Market Behavior: Essays in Honour of John Vanderkamp. Ed. L.N. Christophides, E.K. Grant, and R. Swidinsky, 201-222. Toronto: University of Toronto Press. Professor Card kindly provided these data. Used in Text: pages 507-508, 527\nNotes: Computer Exercise C15.3 is important for analyzing these data. There, it is shown that the instrumental variable, nearc4, is actually correlated with IQ, at least for the subset of men for which an IQ score is reported. However, the correlation between nearc4 and IQ, once the other explanatory variables are netted out, is arguably zero. (At least, it is not statistically different from zero.) In other words, nearc4 fails the exogeneity requirement in a simple linear model but it passes – at least using the crude test described above – if controls are added to the wage equation. For a more advanced course, a nice extension of Card’s analysis is to allow the return to education to differ by race. A relatively simple extension is to include blackeduc as an additional explanatory variable; its natural instrument is blacknearc4.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: CARD.DES\nDataset title (if known): Not provided\nSuggested title: National Longitudinal Survey of Young Men (NLS-YM) Dataset\n\nThe dataset appears to contain information on a sample of young men, including their educational attainment, family background, regional characteristics, employment status, and various other socioeconomic variables. The data was collected in 1976 and includes information on the respondents’ characteristics in 1966 and 1976.\nPotential research ideas:\n\nExamining the relationship between educational attainment (nearc2, nearc4, educ) and labor market outcomes (wage, lwage, exper) while controlling for family background (fatheduc, motheduc) and other demographic factors.\nInvestigating the impact of family structure (momdad14, sinmom14, step14) on educational and labor market outcomes, and exploring potential mediating factors.\nAnalyzing the role of regional characteristics (south66, south, smsa, smsa66) in shaping educational and employment opportunities, and how these effects may vary by race (black) or other individual characteristics.\nExploring the relationship between cognitive ability (IQ, KWW) and educational attainment, and how this relationship may be influenced by family background and other socioeconomic factors.\nExamining the determinants of school enrollment (enroll) in 1976, and how they may be influenced by individual, family, and regional characteristics."
  },
  {
    "objectID": "data_resources.html#catholic",
    "href": "data_resources.html#catholic",
    "title": "Data-Set Handbook",
    "section": "CATHOLIC",
    "text": "CATHOLIC\nSource: Altonji, J.G., T.E. Elder, and C.R. Taber (2005), “An Evaluation of Instrumental Variable Strategies for Estimating the Effects of Catholic Schooling,” Journal of Human Resources 40, 791-821. Professor Elder kindly provided a subset of the data, with some variables stripped away for confidentiality reasons. Used in Text: pages 260-261, 530\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: catholic.des\nDataset title (if known): Not provided\nSuggested title: Catholic High School Attendance and Academic Performance\n\nThe dataset contains information on 7,430 individuals, including their reading and mathematics standardized scores, demographic characteristics (gender, race/ethnicity), parental education levels, family income, high school graduation status, and whether they attended a Catholic high school. The data appears to focus on the relationship between Catholic high school attendance and academic performance, as well as the influence of socioeconomic and family factors on educational outcomes.\nPotential research ideas:\n\nExamine the relationship between Catholic high school attendance and academic performance, controlling for demographic and family background characteristics. This could provide insights into the potential benefits or drawbacks of Catholic education.\nInvestigate the factors that influence the decision to attend a Catholic high school, such as parental education, family income, and religious affiliation. This could help understand the socioeconomic and cultural factors that shape educational choices.\nAnalyze the differences in academic outcomes between students who attended Catholic high schools and those who did not, focusing on specific subject areas (e.g., reading, mathematics) or overall academic achievement. This could shed light on the potential educational advantages or disadvantages of Catholic schooling.\nExplore the role of parental education and family income in shaping educational attainment and academic performance, and how these factors may interact with Catholic high school attendance.\nInvestigate the long-term outcomes of Catholic high school attendance, such as college enrollment, graduation rates, or labor market outcomes, to understand the broader implications of this educational pathway.\n\n\n\n\n\nCEMENT\nSource: J. Shea (1993), “The Input-Output Approach to Instrument Selection,” Journal of Business and Economic Statistics 11, 145-156. Professor Shea kindly provided these data. Used in Text: pages 556\nNotes: Compared with Shea’s analysis, the producer price index (PPI) for fuels and power has been replaced with the PPI for petroleum. The data are monthly and have not been seasonally adjusted.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: catholic.des\nDataset title (if known): Not provided\nSuggested title: Catholic High School Attendance and Academic Performance\n\nThe dataset contains information on 7,430 individuals, including their reading and mathematics standardized scores, demographic characteristics (gender, race/ethnicity), parental education levels, family income, high school graduation status, and whether they attended a Catholic high school. The data appears to focus on the relationship between Catholic high school attendance and academic performance, as well as the influence of socioeconomic and family factors on educational outcomes.\nPotential research ideas:\n\nExamine the relationship between Catholic high school attendance and academic performance, controlling for demographic and family background characteristics. This could provide insights into the potential benefits or drawbacks of Catholic education.\nInvestigate the factors that influence the decision to attend a Catholic high school, such as parental education, family income, and religious affiliation. This could help understand the socioeconomic and cultural factors that shape educational choices.\nAnalyze the differences in academic outcomes between students who attended Catholic high schools and those who did not, focusing on specific subject areas (e.g., reading, mathematics) or overall academic achievement. This could shed light on the potential educational advantages or disadvantages of Catholic schooling.\nExplore the role of parental education and family income in shaping educational attainment and academic performance, and how these factors may interact with Catholic high school attendance.\nInvestigate the long-term outcomes of Catholic high school attendance, such as college enrollment, graduation rates, or labor market outcomes, to understand the broader implications of this educational pathway."
  },
  {
    "objectID": "data_resources.html#census2000",
    "href": "data_resources.html#census2000",
    "title": "Data-Set Handbook",
    "section": "CENSUS2000",
    "text": "CENSUS2000\nSource: Obtained from the United States Census Bureau by Professor Alberto Abadie of the Harvard Kennedy School of Government. Professor Abadie kindly provided the data. Used in Text: pages 485\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: CENSUS2000.DES\nDataset title (if known): Not provided\nSuggested title: Census 2000 Microdata Sample\n\nThe provided dataset contains information on individuals from the United States Census 2000. The variables include the state (identified by ICPSR code), the Public Use Microdata Area (PUMA), educational attainment, log of weekly income, years of workforce experience, and the square of years of experience. This dataset appears to be a sample of the Census 2000 microdata, which provides detailed information on the socioeconomic characteristics of the U.S. population.\nPotential research ideas:\nExamining the relationship between educational attainment and weekly income: Researchers could investigate how different levels of educational attainment, such as high school, college, or advanced degrees, are associated with variations in weekly income. This could provide insights into the economic returns to education.\nAnalyzing the impact of workforce experience on income: Researchers could explore the relationship between years of workforce experience and log of weekly income, as well as the potential non-linear effects captured by the “expersq” variable. This could shed light on the dynamics of income growth over an individual’s career.\nExploring regional differences in socioeconomic outcomes: Researchers could investigate how the state and PUMA variables are related to educational attainment, income, and workforce experience. This could reveal geographic patterns and disparities in socioeconomic outcomes across different regions of the United States.\nInvestigating the role of gender and other demographic factors: Researchers could analyze how variables such as gender, race, or age might be associated with differences in educational attainment, income, and workforce experience. This could provide insights into issues of equity and social mobility.\nDeveloping predictive models of income: Researchers could use the available variables to build predictive models of weekly income, exploring the relative importance of factors like education, experience, and geographic location. Such models could have applications in areas like economic forecasting or policy analysis."
  },
  {
    "objectID": "data_resources.html#ceosal1",
    "href": "data_resources.html#ceosal1",
    "title": "Data-Set Handbook",
    "section": "CEOSAL1",
    "text": "CEOSAL1\nSource: I took a random sample of data reported in the May 6, 1991 issue of Businessweek.\nUsed in Text: pages 29, 32-33, 35, 154, 211, 252-253, 257, 662\nNotes: This kind of data collection is relatively easy for students just learning data analysis, and the findings can be interesting. A good term project is to have students collect a similar data set using a more recent issue of Businessweek, and to find additional variables that might explain differences in CEO compensation. My impression is that the public is still interested in CEO compensation. An interesting question is whether the list of explanatory variables included in this data set now explain less of the variation in log(salary) than they used to.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: CEOSAL1.DES\nDataset title (if known): Not provided\nSuggested title: CEO Compensation and Firm Performance\n\nThe dataset contains information on the 1990 salaries of CEOs, along with various financial and industry-related variables for the firms they lead. The variables include the CEO’s salary in thousands of dollars, the percentage change in salary from 1989 to 1990, the firm’s sales in millions of dollars, the average return on equity from 1988 to 1990, the percentage change in return on equity during that period, the return on the firm’s stock from 1988 to 1990, and indicators for the firm’s industry (industrial, financial, consumer product, or transportation/utilities). The dataset also includes the natural logarithms of salary and sales.\nPotential research ideas:\n\n\nExamine the relationship between CEO compensation (salary and percentage change in salary) and firm performance (return on equity, return on stock, and sales). This could provide insights into the alignment between executive pay and firm outcomes.\nInvestigate whether the relationship between CEO compensation and firm performance varies across different industries, as indicated by the industry dummy variables. This could reveal industry-specific factors that influence the pay-performance link.\nAnalyze the impact of firm size, as measured by sales, on CEO compensation. This could shed light on the role of firm scale and complexity in determining executive pay.\nExplore the factors that influence the percentage change in CEO salary from one year to the next. This could help identify the drivers of short-term adjustments in executive compensation.\nAssess the usefulness of the natural logarithms of salary and sales as alternative measures of CEO compensation and firm size, respectively, in the analysis of the pay-performance relationship."
  },
  {
    "objectID": "data_resources.html#ceosal2",
    "href": "data_resources.html#ceosal2",
    "title": "Data-Set Handbook",
    "section": "CEOSAL2",
    "text": "CEOSAL2\nSource: See CEOSAL1 Used in Text: pages 62, 110, 154, 207, 324, 662\nNotes: Compared with CEOSAL1, in this CEO data set more information about the CEO, rather than about the company, is included.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: CEOSAL2.DES\nDataset title (if known): Not provided\nSuggested title: CEO Compensation and Firm Characteristics\n\nThe dataset contains information on 177 observations related to CEO compensation and various firm characteristics. The variables include the CEO’s annual salary, age, education level (college and graduate school attendance), tenure with the company and as CEO, firm sales, profits, market value, and profit margin. The data appears to be focused on understanding the factors that influence CEO compensation, such as the CEO’s experience and the firm’s financial performance.\nPotential research ideas:\n\nExamine the relationship between CEO compensation (salary) and the CEO’s educational background (college and graduate school attendance). This could provide insights into the value placed on formal education in CEO selection and compensation.\nInvestigate the impact of CEO tenure (both with the company and as CEO) on their compensation. This could reveal whether longer-serving CEOs are rewarded for their experience and institutional knowledge or if there are diminishing returns over time.\nAnalyze the association between firm financial performance (sales, profits, profit margin, and market value) and CEO compensation. This could help identify the extent to which CEO pay is tied to the company’s success.\nExplore the potential non-linear relationships between CEO tenure and compensation by including squared terms (comtensq and ceotensq). This could uncover any inflection points or changes in the compensation-tenure relationship.\nInvestigate the role of firm size (as measured by sales or market value) in determining CEO compensation. This could provide insights into whether larger firms tend to pay their CEOs more, potentially due to the increased complexity and responsibility of managing a larger organization."
  },
  {
    "objectID": "data_resources.html#charity",
    "href": "data_resources.html#charity",
    "title": "Data-Set Handbook",
    "section": "CHARITY",
    "text": "CHARITY\nSource: P.H. Franses and R. Paap (2001), Quantitative Models in Marketing Research. Cambridge: Cambridge University Press. Professor Franses kindly provided the data. Used in Text: pages 63, 111, 260, 599\nNotes: This data set can be used to illustrate probit and Tobit models, and to study the linear approximations to them.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: CHARITY.DES\nDataset title (if known): Not provided\nSuggested title: Charitable Giving Dataset\n\nThe dataset appears to contain information about charitable giving, including whether individuals responded to a mailing with a gift, the amount of the gift, the individual’s response history, and the frequency of mailings. The variables suggest that this dataset could be used to analyze patterns and factors related to charitable donations.\nPotential research ideas:\n\nInvestigating the relationship between the frequency of mailings (mailsyear) and the likelihood of responding with a gift (respond). This could provide insights into the optimal mailing strategy for maximizing donor engagement and contributions.\nAnalyzing the impact of the time since the last response (weekslast) on the amount of the current gift (gift). This could help charities understand how to time their appeals to maximize donation amounts.\nExploring the differences in giving patterns between those who have responded to the most recent mailing (resplast) and those who have not. This could inform targeted outreach strategies to different donor segments.\nExamining the factors that influence the overall response rate to mailings (propresp), such as the average gift amount (avggift) or the size of the most recent gift (giftlast). This could help charities optimize their fundraising campaigns.\nInvestigating the relationship between the amount of the most recent gift (giftlast) and the average of past gifts (avggift). This could provide insights into the consistency and patterns of individual donor behavior."
  },
  {
    "objectID": "data_resources.html#consump",
    "href": "data_resources.html#consump",
    "title": "Data-Set Handbook",
    "section": "CONSUMP",
    "text": "CONSUMP\nSource: I collected these data from the 1997 Economic Report of the President. Specifically, the data come from Tables B71, B15, B29, and B32. Used in Text: pages 363-364, 391, 422, 548-549, 555, 640\nNotes: For a student interested in time series methods, updating this data set and using it in a manner similar to that in the text could be acceptable as a final project.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: CONSUMP.DES\nDataset title (if known): Not provided\nSuggested title: Macroeconomic Consumption Data\n\nThe dataset appears to contain macroeconomic variables related to consumption, income, and interest rates in the United States from 1959 to 1995. The variables include real disposable income, real nondurable consumption, real services, population, real consumption, real interest rates, and various transformations of these variables (e.g., logs, first differences).\nPotential research ideas:\n\nInvestigate the relationship between real interest rates and consumption growth. This could involve analyzing the impact of changes in real interest rates on the dynamics of consumption and savings.\nExamine the role of income and its distribution in shaping consumption patterns. The dataset provides information on real disposable income and per capita real consumption, which could be used to study the determinants of household consumption behavior.\nAnalyze the relative importance of nondurable consumption and services in overall consumption dynamics. The dataset allows for the separate examination of these two components of consumption, which could provide insights into the drivers of consumption.\nExplore the long-run relationship between consumption and income, and how this relationship has evolved over time. The dataset includes variables that could be used to study the cointegration and error-correction properties of these macroeconomic aggregates.\nInvestigate the impact of inflation on consumption and savings decisions. The dataset includes the inflation rate, which could be used to analyze how changes in the price level affect household behavior."
  },
  {
    "objectID": "data_resources.html#corn",
    "href": "data_resources.html#corn",
    "title": "Data-Set Handbook",
    "section": "CORN",
    "text": "CORN\nSource: G.E. Battese, R.M. Harter, and W.A. Fuller (1988), “An Error-Components Model for Prediction of County Crop Areas Using Survey and Satellite Data,” Journal of the American Statistical Association 83, 28-36. This small data set is reported in the article. Used in Text: pages 745\nNotes: You could use these data to illustrate simple regression when the population intercept should be zero: no corn pixels should predict no corn planted. The same can be done with the soybean measures in the data set.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: CORN.DES\nDataset title (if known): Not provided\nSuggested title: Corn and Soybean Production Data\n\nThe dataset contains information about corn and soybean production in 37 counties. The variables include the county number, the hectares of corn and soybeans, and the satellite pixels of corn and soybeans. This data could be used to analyze the relationship between land area and satellite-based measurements of crop production.\nPotential research ideas:\n\nInvestigate the relationship between corn hectares and corn satellite pixels. This could help assess the accuracy of satellite-based crop monitoring techniques.\nAnalyze the correlation between soybean hectares and soybean satellite pixels. This could provide insights into the reliability of using satellite data to estimate soybean production.\nExplore the spatial distribution of corn and soybean production across the counties. This could identify regions with high or low productivity and help understand the factors influencing crop yields.\nDevelop a predictive model to estimate corn or soybean production based on the available variables. This could be useful for forecasting and planning purposes.\nInvestigate the potential impact of factors such as weather, soil quality, or farming practices on the relationship between land area and satellite-based measurements of corn and soybean production."
  },
  {
    "objectID": "data_resources.html#countymurders",
    "href": "data_resources.html#countymurders",
    "title": "Data-Set Handbook",
    "section": "COUNTYMURDERS",
    "text": "COUNTYMURDERS\nSource: Compiled by J. Monroe Gamble for a Summer Research Opportunities Program (SROP) at Michigan State University, Summer 2014. Monroe obtained data from the U.S. Census Bureau, the FBI Uniform Crime Reports, and the Death Penalty Information Center. Used in Text: pages 17, 64, 458, 490\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: countymurders.des\nDataset title (if known): Not provided\nSuggested title: County-Level Murder and Arrest Data\n\nThe dataset appears to contain information on murder arrests, county-level demographics, and economic indicators for counties across the United States from 1980 to 1996. The variables include the number of murder arrests, county population and population density, racial and age composition, per capita income, unemployment insurance payments, and the number of executions. This dataset could be used to explore the relationships between socioeconomic factors, law enforcement, and murder rates at the county level.\nPotential research ideas:\n\nExamining the relationship between county-level demographic factors (e.g., age distribution, racial composition, population density) and murder rates. This could provide insights into the social and economic drivers of violent crime.\nInvestigating the impact of economic indicators, such as per capita income and unemployment insurance payments, on murder rates. This could help identify potential policy interventions to address the root causes of violent crime.\nAnalyzing the relationship between the number of executions and murder rates at the county level. This could contribute to the ongoing debate about the effectiveness of capital punishment as a deterrent to crime.\nExploring the factors that influence the murder arrest rate, such as the relationship between the number of murders and the number of arrests. This could shed light on the efficiency and fairness of the criminal justice system.\nConducting a longitudinal analysis to identify trends and changes in murder rates, arrests, and other variables over the 1980-1996 period. This could provide valuable insights into the dynamics of violent crime and the effectiveness of law enforcement and policy interventions."
  },
  {
    "objectID": "data_resources.html#cps78_85",
    "href": "data_resources.html#cps78_85",
    "title": "Data-Set Handbook",
    "section": "CPS78_85",
    "text": "CPS78_85\nSource: Professor Henry Farber, now at Princeton University, compiled these data from the 1978 and 1985 Current Population Surveys. Professor Farber kindly provided these data when we were colleagues at MIT. Used in Text: pages 427, 429-430, 454\nNotes: Obtaining more recent data from the CPS allows one to track, over a long period of time, the changes in the return to education, the gender gap, black-white wage differentials, and the union wage premium.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: CPS78_85.DES\nDataset title (if known): Not provided\nSuggested title: Wage and Employment Data from the Current Population Survey (1978 and 1985)\n\nThe dataset contains information on various demographic and employment-related variables for individuals in the United States. The variables include years of schooling, region of residence (south or not), race (nonwhite or not), gender, marital status, work experience, union membership, and log hourly wage. The data spans two years, 1978 and 1985, and includes interaction terms between the year 1985 and some of the other variables.\nPotential research ideas:\n\nExamine the relationship between education and wages, and how this relationship may have changed between 1978 and 1985. Investigate whether the returns to education differ for individuals living in the South compared to other regions, and whether this varies by gender or race.\nAnalyze the impact of union membership on wages, and explore whether the effect of unions on wages has changed over time. Investigate whether the union wage premium differs for individuals with different levels of education or work experience.\nExplore the gender wage gap and how it has evolved between 1978 and 1985. Examine the role of factors such as education, work experience, and marital status in explaining the gender wage gap.\nInvestigate the relationship between work experience and wages, and whether the returns to experience have changed over time. Analyze whether the experience-wage profile differs for individuals with different levels of education or union membership.\nExamine the regional differences in wages, particularly the wage gap between the South and other regions of the United States. Explore the extent to which this regional wage gap can be explained by differences in individual characteristics, such as education, race, and gender."
  },
  {
    "objectID": "data_resources.html#cps91",
    "href": "data_resources.html#cps91",
    "title": "Data-Set Handbook",
    "section": "CPS91",
    "text": "CPS91\nSource: Professor Daniel Hamermesh, at the University of Texas, compiled these data from the May 1991 Current Population Survey. Professor Hamermesh kindly provided these data. Used in Text: page 599\nNotes: This is much bigger than the other CPS data sets even though the sample is restricted to married women. (CPS91 contains many more observations than MROZ, too.) In addition to the usual human capital variables for the women in the sample, we have information on the husband. Therefore, we can estimate a labor supply function as in Chapter 16, although the validity of potential experience as an IV for log(wage) is questionable. (MROZ contains an actual experience variable.) Perhaps more convincing is to add hours to the wage offer equation, and instrument hours with indicators for young and old children. This data set also contains a union membership indicator. The web site for the National Bureau of Economic Research makes it very easy now to download CPS data files in a variety of formats. Go to http://www.nber.org/data/cps_basic.html.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: CPS91.TXT\nDataset title (if known): Not provided\nSuggested title: Labor Force Participation and Earnings Data\n\nThe dataset contains information on various characteristics of married couples, including the husband’s and wife’s age, education, race, union status, weekly earnings, and weekly hours worked. It also includes information on the presence of children in the household, family income, and the wife’s labor force participation.\nPotential research ideas:\n\nExamine the relationship between the wife’s education level and her labor force participation. Investigate whether higher levels of education are associated with a greater likelihood of the wife being in the labor force.\nAnalyze the impact of the husband’s earnings on the wife’s decision to work. Explore whether higher-earning husbands are more likely to have wives who are not in the labor force.\nInvestigate the role of children in the household on the wife’s labor force participation. Determine whether the presence of young children (under 6 years old) is associated with a lower likelihood of the wife being employed.\nExplore the factors that influence the wife’s weekly earnings, such as her education, experience, and union status. Assess the extent to which these variables contribute to the variation in the wife’s earnings.\nAnalyze the relationship between the wife’s labor force participation and the family’s total income. Determine whether the wife’s employment status is a significant predictor of the family’s overall financial well-being."
  },
  {
    "objectID": "data_resources.html#crime1",
    "href": "data_resources.html#crime1",
    "title": "Data-Set Handbook",
    "section": "CRIME1",
    "text": "CRIME1\nSource: J. Grogger (1991), “Certainty vs. Severity of Punishment,” Economic Inquiry 29, 297-309. Professor Grogger kindly provided a subset of the data he used in his article. Used in Text: pages 78, 169, 174, 243, 268, 291, 295-296, 581-582, 597\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: CRIME1.DES\nDataset title (if known): Not provided\nSuggested title: Criminal History and Socioeconomic Characteristics\n\nThe dataset contains information on the criminal history and socioeconomic characteristics of 2,725 individuals. The variables include the number of times the individual was arrested in 1986, the number of felony arrests and property crime arrests in 1986, the proportion of prior convictions, the average sentence length, the total time in prison since age 18, the time spent in prison during 1986, the number of quarters employed in 1986, the legal income in 1986, the recent unemployment duration, and the individual’s race (black or Hispanic) and birth year (1960). The dataset also includes squared terms for the proportion of prior convictions, time in prison during 1986, and legal income in 1986.\nPotential research ideas:\n\nExamine the relationship between criminal history (e.g., number of arrests, prior convictions) and socioeconomic outcomes (e.g., employment, income) to understand the long-term consequences of criminal involvement.\nInvestigate the impact of race and ethnicity on criminal justice outcomes, such as the likelihood of arrest, conviction, and sentencing, while controlling for other relevant factors.\nAnalyze the factors that contribute to the duration of unemployment among individuals with a criminal history, and explore potential interventions to improve their labor market outcomes.\nAssess the effectiveness of incarceration on reducing recidivism by examining the relationship between time spent in prison and subsequent criminal behavior.\nExplore the role of legal income and employment in deterring criminal activity, and identify potential policy interventions to promote economic opportunities for individuals with a criminal history."
  },
  {
    "objectID": "data_resources.html#crime2",
    "href": "data_resources.html#crime2",
    "title": "Data-Set Handbook",
    "section": "CRIME2",
    "text": "CRIME2\nSource: These data were collected by David Dicicco, a former MSU undergraduate, for a final project. They came from various issues of the County and City Data Book, and are for the years 1982 and 1985. Unfortunately, I do not have the list of cities. Used in Text: pages 303-304, 439-440\nNotes: Very rich crime data sets, at the county, or even city, level, can be collected using the FBI’s Uniform Crime Reports. These data can be matched up with demographic and economic data, at least for census years. The County and City Data Book contains a variety of statistics, but the years do not always match up. These data sets can be used investigate issues such as the effects of casinos on city or county crime rates.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: CRIME2.DES\nDataset title (if known): Not provided\nSuggested title: Crime and Socioeconomic Factors in U.S. Cities\n\nThe dataset appears to contain information on various socioeconomic and crime-related variables for a sample of U.S. cities, with observations for two years (1982 and 1987). The variables include population, total number of index crimes, unemployment rate, number of police officers, per capita income, geographic region, population density, crime rate, police officer density, law enforcement expenditure, and changes in these variables over time.\nPotential research ideas:\n\nExamine the relationship between socioeconomic factors (e.g., unemployment, per capita income) and crime rates in U.S. cities. Investigate how changes in these factors over time are associated with changes in crime rates.\nAnalyze the impact of law enforcement resources (e.g., number of police officers, law enforcement expenditure) on crime rates. Explore whether changes in these resources are related to changes in crime rates.\nInvestigate the differences in crime rates and socioeconomic factors between cities in different geographic regions (West, Northeast, South). Identify any regional patterns or disparities.\nDevelop a predictive model to estimate crime rates based on the available socioeconomic and law enforcement variables. Assess the model’s accuracy and identify the most influential factors.\nExplore the relationship between population density and crime rates. Examine how changes in population density over time are associated with changes in crime rates."
  },
  {
    "objectID": "data_resources.html#crime3",
    "href": "data_resources.html#crime3",
    "title": "Data-Set Handbook",
    "section": "CRIME3",
    "text": "CRIME3\nSource: E. Eide (1994), Economics of Crime: Deterrence of the Rational Offender. Amsterdam: North Holland. The data come from Tables A3 and A6. Used in Text: pages 443-444, 455\nNotes: These data are for the years 1972 and 1978 for 53 police districts in Norway. Much larger data sets for more years can be obtained for the United States, although a measure of the “clear-up” rate is needed.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: CRIME3.DES\nDataset title (if known): Not provided\nSuggested title: Crime and Clearance Rates Dataset\n\nThe dataset appears to contain information on crime rates and clearance rates for different districts over two years, 1972 and 1978. The variables include the district number, the year, the crime rate per 1000 people, the clearance rate for the prior year and two years prior, a dummy variable indicating the year 1978, the average clearance rate, the log of the crime rate, the change in the log of the crime rate, the change in the average clearance rate, the change in the clearance rate for the prior year, and the change in the clearance rate for two years prior.\nPotential research ideas:\nInvestigate the relationship between crime rates and clearance rates. Analyze whether higher clearance rates are associated with lower crime rates, and explore the potential factors that may influence this relationship.\nExamine the changes in crime and clearance rates over time. Identify any significant trends or patterns in the data, and explore potential explanations for the observed changes.\nAnalyze the differences in crime and clearance rates across different districts. Investigate whether there are any geographic or demographic factors that may contribute to these variations.\nExplore the impact of the year 1978 on crime and clearance rates. Determine whether there were any significant changes in the data between the two years, and identify potential policy or societal factors that may have influenced these changes.\nConduct a more in-depth analysis of the changes in the various clearance rate measures (clrprc1, clrprc2, avgclr) and their relationship with the changes in crime rates (lcrime, clcrime). Investigate the potential drivers of these changes and their implications for crime prevention and law enforcement strategies."
  },
  {
    "objectID": "data_resources.html#crime4",
    "href": "data_resources.html#crime4",
    "title": "Data-Set Handbook",
    "section": "CRIME4",
    "text": "CRIME4\nSource: From C. Cornwell and W. Trumball (1994), “Estimating the Economic Model of Crime with Panel Data,” Review of Economics and Statistics 76, 360-366. Professor Cornwell kindly provided the data. Used in Text: pages 449-450, 456, 486, 556\nNotes: Computer Exercise C16.7 shows that variables that might seem to be good instrumental variable candidates are not always so good, especially after applying a transformation such as differencing across time. You could have the students do an IV analysis for just, say, 1987.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: CRIME4.DES\nDataset title (if known): Not provided\nSuggested title: North Carolina County-Level Crime and Socioeconomic Data\n\nThe dataset contains information on various crime-related and socioeconomic variables for counties in North Carolina from 1981 to 1987. The variables include crime rates, probabilities of arrest, conviction, and prison sentence, average sentence length, police per capita, population density, tax revenue per capita, regional indicators, demographic characteristics, and weekly wages for different industries. The data can be used to analyze the relationships between crime, law enforcement, and socioeconomic factors at the county level in North Carolina during this time period.\nPotential research ideas:\n\nExamining the impact of economic factors, such as wages and tax revenue, on crime rates in North Carolina counties. This could provide insights into the role of economic conditions in shaping criminal behavior.\nInvestigating the relationship between law enforcement measures (e.g., police per capita, probability of arrest) and crime rates. This could help identify the effectiveness of different policing strategies in reducing crime.\nAnalyzing the influence of demographic characteristics, such as the percentage of young males and minority populations, on crime rates. This could shed light on the social and cultural factors that contribute to criminal activity.\nExploring the regional differences in crime rates and their potential drivers, such as urban versus rural settings or the distinction between western and central North Carolina. This could inform targeted policy interventions.\nStudying the changes in crime rates and related variables over time, using the longitudinal nature of the data. This could reveal trends and patterns in the evolution of crime and its determinants in North Carolina."
  },
  {
    "objectID": "data_resources.html#discrim",
    "href": "data_resources.html#discrim",
    "title": "Data-Set Handbook",
    "section": "DISCRIM",
    "text": "DISCRIM\nSource: K. Graddy (1997), “Do Fast-Food Chains Price Discriminate on the Race and Income Characteristics of an Area?” Journal of Business and Economic Statistics 15, 391-401. Professor Graddy kindly provided the data set. Used in Text: pages 111, 161, 663\nNotes: If you want to assign a common final project, this would be a good data set. There are many possible dependent variables, namely, prices of various fast-food items. The key variable is the fraction of the population that is black, along with controls for poverty, income, housing values, and so on. These data were also used in a famous study by David Card and Alan Krueger on estimation of minimum wage effects on employment. See the book by Card and Krueger, Myth and Measurement, 1997, Princeton University Press, for a detailed analysis.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: DISCRIM.DES\nDataset title (if known): Not provided\nSuggested title: Fast Food Restaurant Data\n\nThe dataset appears to contain information about fast food restaurants, including prices, wages, staffing, and location characteristics. The data includes variables related to the first and second waves of data collection, allowing for comparisons over time. The variables cover a range of restaurant-level and community-level characteristics, such as ownership, chain affiliation, population density, crime rate, and demographic information.\nPotential research ideas:\n\nExamine the relationship between restaurant characteristics (e.g., prices, wages, staffing) and community-level factors (e.g., population density, crime rate, demographics) to understand how the local environment may influence the operations and pricing strategies of fast food restaurants.\nInvestigate the differences in restaurant characteristics and performance between company-owned and franchised establishments to identify the potential advantages and disadvantages of each ownership model.\nAnalyze the changes in restaurant characteristics (e.g., prices, wages, staffing) between the first and second waves of data collection to understand how the fast food industry has evolved over time and the factors that may have contributed to these changes.\nExplore the relationship between the chain affiliation of restaurants (e.g., Burger King, Kentucky Fried Chicken, Roy Rogers) and their performance or pricing strategies to identify any potential differences in business practices or market positioning.\nConduct a comparative analysis of fast food restaurants across different states (New Jersey and Pennsylvania) to understand how state-level factors, such as regulations or economic conditions, may influence the operations and performance of these establishments."
  },
  {
    "objectID": "data_resources.html#driving",
    "href": "data_resources.html#driving",
    "title": "Data-Set Handbook",
    "section": "DRIVING",
    "text": "DRIVING\nSource: Freeman, D.G. (2007), “Drunk Driving Legislation and Traffic Fatalities: New Evidence on BAC 08 Laws,” Contemporary Economic Policy 25, 293–308. Professor Freeman kindly provided the data. Used in Text: page 489; see also the general discussion on page 659\nNotes: Several more years of data are now available and may further shed light on the effectiveness of several traffic laws.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: driving.des\nDataset title (if known): Not provided\nSuggested title: State-Level Driving and Traffic Fatality Data\n\nThe dataset appears to contain information on various driving-related variables and traffic fatalities across the 48 continental U.S. states from 1980 to 2004. The variables include speed limits, seatbelt laws, minimum drinking age, zero tolerance laws, graduated driver’s license laws, blood alcohol limits, administrative license revocation, traffic fatalities (total, nighttime, and weekend), fatality rates per vehicle miles and population, state population, unemployment rate, and the percentage of the population aged 14-24. The dataset also includes indicator variables for each year from 1980 to 2004.\nPotential research ideas:\n\n\nExamine the impact of changes in speed limits on traffic fatalities and fatality rates over time. Investigate whether the effects vary by type of fatality (total, nighttime, or weekend).\nAnalyze the effectiveness of seatbelt laws, zero tolerance laws, and graduated driver’s license laws in reducing traffic fatalities and fatality rates. Explore whether the impact of these laws differs across states or over time.\nInvestigate the relationship between unemployment rates and traffic fatalities, and whether this relationship has changed over the study period.\nAssess the impact of changes in the minimum drinking age and blood alcohol limits on traffic fatalities and fatality rates, particularly among the 14-24 age group.\nExplore the potential interactions between various driving-related laws and policies, and their combined effect on traffic safety outcomes."
  },
  {
    "objectID": "data_resources.html#earns",
    "href": "data_resources.html#earns",
    "title": "Data-Set Handbook",
    "section": "EARNS",
    "text": "EARNS\nSource: Economic Report of the President, 1989, Table B47. The data are for the non-farm business sector. Used in Text: pages 382, 390\nNotes: These data could be usefully updated, but changes in reporting conventions in more recent ERPs may make that difficult.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: EARNS.DES\nDataset title (if known): Not provided\nSuggested title: U.S. Labor Productivity and Earnings Dataset\n\nThe dataset contains 41 observations of various labor market variables from 1947 to 1987. The variables include average real weekly earnings (wkearns), average weekly hours (wkhours), output per labor hour (outphr), hourly wage (hrwage), log-transformed hourly wage (lhrwage), log-transformed output per labor hour (loutphr), a time trend (t), and several derived variables such as the growth rates of hourly wage (ghrwage) and output per labor hour (goutphr), as well as lagged values of these growth rates.\nPotential research ideas:\n\nAnalyze the relationship between output per labor hour and hourly wage: Investigate the dynamics between productivity and earnings, and explore how changes in output per labor hour are reflected in changes in hourly wage.\nExamine the impact of working hours on earnings: Assess the relationship between average weekly hours and average real weekly earnings, and explore how changes in working hours affect overall earnings.\nInvestigate the role of technological progress: Utilize the time trend variable (t) to analyze the impact of technological advancements on labor productivity and earnings over the study period.\nExplore the persistence of wage and productivity growth: Analyze the dynamics of the growth rates (ghrwage and goutphr) and their lagged values to understand the persistence and interdependence of these variables.\nAssess the impact of macroeconomic factors: Incorporate external data (e.g., GDP, inflation, unemployment) to explore how broader economic conditions influence the labor market variables in the dataset."
  },
  {
    "objectID": "data_resources.html#econmath",
    "href": "data_resources.html#econmath",
    "title": "Data-Set Handbook",
    "section": "ECONMATH",
    "text": "ECONMATH\nSource: Compiled by Professor Charles Ballard, Michigan State University Department of Economics. Professor Ballard kindly provided the data. Used in Text: 162, 177\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: ECONMATH.DES\nDataset title (if known): Not provided\nSuggested title: Economics and Mathematics Performance Dataset\n\nThe dataset contains information on 856 observations and 17 variables related to the academic performance and background of students. The variables include age, hours worked per week, hours studying per week, whether the student took economics in high school, college GPA, high school GPA, ACT English and math scores, ACT composite score, math quiz score, gender, whether the student took a calculus course, attendance ratings, and whether the student’s parents have a college degree. The data appears to be focused on understanding the factors that influence academic performance, particularly in economics and mathematics-related courses.\nPotential research ideas:\n\n\nInvestigate the relationship between study hours, work hours, and academic performance (GPA, ACT scores, math quiz score, course score). Explore whether there are optimal levels of study and work that maximize academic success.\nAnalyze the impact of prior academic preparation (high school GPA, economics in high school, calculus course) on college-level performance in economics and mathematics-related courses. Identify the key factors that contribute to success in these fields.\nExamine the role of family background (parental education) in shaping student academic outcomes. Determine whether there are significant differences in performance based on socioeconomic status.\nExplore the gender differences in academic performance, particularly in mathematics and economics-related courses. Investigate the potential factors (e.g., study habits, attendance, prior preparation) that may contribute to any observed gender gaps.\nDevelop a predictive model to identify the key variables that best explain and predict a student’s overall course performance (score). This could help inform admissions and academic support strategies."
  },
  {
    "objectID": "data_resources.html#elem94_95",
    "href": "data_resources.html#elem94_95",
    "title": "Data-Set Handbook",
    "section": "ELEM94_95",
    "text": "ELEM94_95\nSource: Culled from a panel data set used by Professor Leslie Papke in her paper “The Effects of Spending on Test Pass Rates: Evidence from Michigan” (2005), Journal of Public Economics 89, 821-839. Used in Text: pages 161, 330-331\nNotes: Starting in 1995, the Michigan Department of Education stopped reporting average teacher benefits along with average salary. This data set includes both variables, at the school level, and can be used to study the salary-benefits tradeoff, as in Chapter 4. There are a few suspicious benefits/salary ratios, and so this data set makes a good illustration of the impact of outliers in Chapter 9.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: ELEM94_95.DES\nDataset title (if known): Not provided\nSuggested title: Elementary School District Data 1994-1995\n\nThe dataset contains information about various characteristics of elementary schools and school districts from the 1994-1995 academic year. The variables include district and school identifiers, the percentage of students eligible for free lunch, enrollment, staff per 1000 students, expenditures per pupil, average teacher salary, average teacher non-salary benefits, the percentage of students passing 4th grade math and reading tests, the ratio of average teacher non-salary benefits to average salary, and the natural logarithms of average salary, enrollment, and staff.\nPotential research ideas:\nInvestigate the relationship between school district characteristics (e.g., enrollment, staff, expenditures) and student academic performance (e.g., math and reading test scores). This could help identify factors that contribute to educational outcomes.\nAnalyze the distribution of teacher salaries and benefits across school districts and explore how these factors are related to district-level characteristics, such as enrollment, staff, and student demographics. This could provide insights into teacher compensation policies and their potential impact on teacher recruitment and retention.\nExamine the association between the percentage of students eligible for free lunch (a proxy for socioeconomic status) and other school district variables, such as expenditures per pupil and student performance. This could shed light on the role of socioeconomic factors in educational equity.\nInvestigate the relationship between the ratio of teacher non-salary benefits to average salary (variable “bs”) and other district-level characteristics. This could help understand the trade-offs between salary and benefits in teacher compensation.\nExplore the use of logarithmic transformations of variables, such as enrollment, staff, and average salary, and their potential implications for the analysis of school district data. This could provide insights into the scaling and interpretation of these variables."
  },
  {
    "objectID": "data_resources.html#expendshares",
    "href": "data_resources.html#expendshares",
    "title": "Data-Set Handbook",
    "section": "EXPENDSHARES",
    "text": "EXPENDSHARES\nSource: Blundell, R., A. Duncan, and K. Pendakur (1998), “Semiparametric Estimation and Consumer Demand,” Journal of Applied Econometrics 13, 435-461. I obtained these data from the Journal of Applied Econometrics data archive at http://qed.econ.queensu.ca/jae/. Used in Text: pages 557-558\nNotes: The dependent variables in this data set – the expenditure shares – are necessarily bounded between zero and one. The linear model is at best an approximation, but the usual IV estimator likely gives good estimates of the average partial effects.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: expendshares.des\nDataset title (if known): Not provided\nSuggested title: Household Expenditure Patterns in the UK\n\nThe dataset contains information on the expenditure patterns of households in the UK. It includes variables such as the share of expenditures on food, fuel, clothing, alcohol, transportation, and other items, as well as the total weekly expenditure, family income, age of the household head, and the number of children in the household. The dataset also includes the logarithm of total expenditure and income, as well as the square of the age of the household head.\nPotential research ideas:\n\nAnalyzing the relationship between household income and the share of expenditures on different categories: This study could investigate how the allocation of household spending across different categories (e.g., food, fuel, clothing) varies with the level of household income. It could provide insights into the consumption patterns of households at different income levels.\nExamining the impact of household demographics on expenditure patterns: This research could explore how the age of the household head and the number of children in the household influence the distribution of household spending across different categories. It could shed light on the role of demographic factors in shaping household consumption behavior.\nInvestigating the relationship between total expenditure and its logarithm: This study could explore the properties of the logarithm of total expenditure, such as its relationship with the original total expenditure variable and its potential use in modeling household consumption behavior.\nExploring the nonlinear effects of age on household expenditures: This research could investigate the potential nonlinear relationship between the age of the household head and the share of expenditures on different categories, using the age and age-squared variables provided in the dataset.\nComparing expenditure patterns across different regions or socioeconomic groups: Depending on the availability of additional information in the dataset (e.g., geographic location, socioeconomic status), this study could compare the expenditure patterns of households in different regions or socioeconomic groups, providing insights into the factors that influence household consumption behavior."
  },
  {
    "objectID": "data_resources.html#engin",
    "href": "data_resources.html#engin",
    "title": "Data-Set Handbook",
    "section": "ENGIN",
    "text": "ENGIN\nSource: Thada Chaisawangwong, a former graduate student at MSU, obtained these data for a term project in applied econometrics. They come from the Material Requirement Planning Survey carried out in Thailand during 1998. Used in Text: not used\nNotes: This is a nice change of pace from wage data sets for the United States. These data are for engineers in Thailand, and represents a more homogeneous group than data sets that consist of people across a variety of occupations. Plus, the starting salary is also provided in the data set, so factors affecting wage growth – and not just wage levels at a given point in time – can be studied. This is a good data set for a common term project that tests basic understanding of multiple regression and the interpretation of models with a logarithm for a dependent variable.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: ENGIN.DES\nDataset title (if known): Not provided\nSuggested title: Thai Engineering Workforce Dataset\n\nThe dataset appears to contain information about the characteristics and employment of a sample of Thai engineering workers. The variables include demographic information (gender), educational attainment (highest grade completed, high school graduate, college graduate, graduate school, polytechnic), work experience (years on current job, previous experience), and wage-related measures (monthly salary, starting wage, log of wage and starting wage). The dataset has 403 observations.\nPotential research ideas:\n\nExamining the relationship between educational attainment and monthly salary for Thai engineering workers. This could involve analyzing the impact of different levels of education (high school, college, graduate school, polytechnic) on wages, while controlling for other factors such as work experience.\nInvestigating the gender wage gap in the Thai engineering workforce. Researchers could explore the differences in wages between male and female engineers, and identify potential factors contributing to this gap, such as differences in educational attainment, work experience, or other characteristics.\nAnalyzing the impact of work experience on the wages of Thai engineering workers. Researchers could examine the relationship between years of experience (both current job and previous experience) and monthly salary, and explore whether there are diminishing returns to experience over time.\nStudying the transition from starting wage to current wage for Thai engineering workers. Researchers could investigate the factors that influence the change in wages over time, such as educational attainment, work experience, or other individual characteristics.\nExploring the role of educational specialization (e.g., polytechnic vs. traditional university) on the employment and wages of Thai engineering workers. This could provide insights into the labor market preferences and outcomes for different types of engineering education."
  },
  {
    "objectID": "data_resources.html#ezanders",
    "href": "data_resources.html#ezanders",
    "title": "Data-Set Handbook",
    "section": "EZANDERS",
    "text": "EZANDERS\nSource: L.E. Papke (1994), “Tax Policy and Urban Development: Evidence from the Indiana Enterprise Zone Program,” Journal of Public Economics 54, 37-49. Professor Papke kindly provided these data. Used in Text: page 363\nNotes: These are actually monthly unemployment claims for the Anderson enterprise zone. Papke used annualized data, across many zones and non-zones, in her original analysis.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: EZANDERS.DES\nDataset title (if known): Not provided\nSuggested title: Unemployment Claims and Economic Indicators\n\nThe dataset appears to contain monthly data on unemployment claims, economic indicators, and seasonal variables from 1980 to 1988. The variables include the month, unemployment claims (uclms), the change in unemployment claims (ez), year indicators, the log of unemployment claims (luclms), and monthly indicator variables.\nPotential research ideas:\n\nAnalyze the relationship between unemployment claims and economic indicators: Investigate how changes in unemployment claims (ez) are associated with the year-specific economic indicators (y81 to y88) and explore the potential drivers of these relationships.\nExamine the seasonal patterns in unemployment claims: Utilize the monthly indicator variables (jan to dec) to identify and analyze any seasonal trends or fluctuations in unemployment claims over the course of the year.\nInvestigate the impact of economic conditions on unemployment claims: Develop a regression model to understand how the year-specific economic indicators (y81 to y88) influence the level of unemployment claims (uclms or luclms) and explore the implications for policy decisions.\nAssess the predictability of unemployment claims: Explore the potential of using the available variables to develop a forecasting model for unemployment claims, which could be useful for policymakers and labor market analysts.\nAnalyze the dynamics of unemployment claims: Investigate the time-series properties of the unemployment claims data, such as the presence of autocorrelation or unit roots, and explore the implications for modeling and understanding the underlying economic processes."
  },
  {
    "objectID": "data_resources.html#ezunem",
    "href": "data_resources.html#ezunem",
    "title": "Data-Set Handbook",
    "section": "EZUNEM",
    "text": "EZUNEM\nSource: See EZANDERS Used in Text: pages 449, 486-487\nNotes: A very good project is to have students analyze enterprise, empowerment, renaissance, or opportunity zone policies in their home states. Many states now have such programs. Or, there are also national programs. A few years of panel data straddling periods of zone designation, at the city or zip code level, could make a nice study.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: EZUNEM.DES\nDataset title (if known): Not provided\nSuggested title: Unemployment Claims and Enterprise Zones\n\nThe dataset appears to contain information on unemployment claims and the presence of enterprise zones in various cities over the years 1980 to 1988. The variables include the year, unemployment claims (uclms), a binary indicator for the presence of an enterprise zone (ez), and a series of binary indicators for each year from 1981 to 1988. Additionally, there are 22 city-specific binary indicators (c1 to c22) and derived variables such as the log of unemployment claims (luclms), the change in log unemployment claims (guclms), and the change in the enterprise zone indicator (cez).\nPotential research ideas:\n\nInvestigate the impact of enterprise zones on unemployment claims: Researchers could examine whether the presence of enterprise zones (ez) has a significant effect on the level or change in unemployment claims (uclms, guclms) over time, controlling for other factors.\nAnalyze the temporal and spatial patterns of unemployment claims: Researchers could explore the trends in unemployment claims (uclms) over the study period and identify any differences in the patterns across the 22 cities.\nExamine the relationship between city-specific characteristics and unemployment claims: Researchers could investigate whether the city-specific binary indicators (c1 to c22) are associated with the level or change in unemployment claims, which may provide insights into the factors that influence local labor market conditions.\nExplore the dynamics of enterprise zone adoption and unemployment claims: Researchers could study the changes in the enterprise zone indicator (cez) and its relationship with the changes in unemployment claims (guclms) to understand the potential lagged or dynamic effects of enterprise zone policies.\nAssess the impact of macroeconomic conditions on unemployment claims: Researchers could incorporate additional data on macroeconomic variables, such as GDP growth or inflation, to examine how broader economic trends may have influenced the observed patterns in unemployment claims."
  },
  {
    "objectID": "data_resources.html#fair",
    "href": "data_resources.html#fair",
    "title": "Data-Set Handbook",
    "section": "FAIR",
    "text": "FAIR\nSource: R.C. Fair (1996), “Econometrics and Presidential Elections,” Journal of Economic Perspectives 10, 89-102. The data set is provided in the article. Used in Text: pages 350-351, 420, 422\nNotes: An updated version of this data set, through the 2004 election, is available at Professor Fair’s web site at Yale University: http://fairmodel.econ.yale.edu/rayfair/pdf/2001b.htm. Students might want to try their own hands at predicting the most recent election outcome, but they should be restricted to no more than a handful of explanatory variables because of the small sample size.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: FAIR.DES\nDataset title (if known): Not provided\nSuggested title: Political and Economic Indicators Dataset\n\nThe dataset appears to contain a collection of variables related to political and economic factors in the United States from 1916 to 1992, with observations recorded every 4 years. The variables include measures of Democratic party vote share, the party in control of the White House, incumbent status, economic growth rates, inflation rates, and various interactions between these factors.\nPotential research ideas:\n\nInvestigating the relationship between economic performance (e.g., growth rates, inflation) and voting behavior: This could involve analyzing how factors like GDP growth, inflation, and the duration of the incumbent party’s control of the White House influence the Democratic party’s vote share.\nExamining the impact of political party control on economic outcomes: Researchers could explore whether the party in power (Democratic or Republican) has a significant effect on economic indicators like growth rates and inflation.\nAnalyzing the role of incumbent status in election outcomes: The dataset includes a variable indicating whether the incumbent is running, which could be used to study the advantages or disadvantages of incumbency in presidential elections.\nExploring the dynamics of political news coverage and its influence on voting: The dataset includes variables related to the amount of “good news” and could be used to investigate how media coverage of the economy and other political factors affects voter behavior.\nInvestigating the interaction between economic and political factors: Researchers could examine how the combination of economic conditions (e.g., growth, inflation) and political factors (e.g., party control, incumbent status) influence election outcomes."
  },
  {
    "objectID": "data_resources.html#fertil1",
    "href": "data_resources.html#fertil1",
    "title": "Data-Set Handbook",
    "section": "FERTIL1",
    "text": "FERTIL1\nSource: W. Sander, “The Effect of Women’s Schooling on Fertility,” Economics Letters 40, 229-233. Professor Sander kindly provided the data, which are a subset of what he used in his article. He compiled the data from various years of the National Opinion Resource Center’s General Social Survey. Used in Text: pages 428-429, 453, 521, 597, 646\nNotes: (1) Much more recent data can be obtained from the National Opinion Research Center website, http://www.norc.org/GSS+Website/Download/. Very rich pooled cross sections can be constructed to study a variety of issues – not just changes in fertility over time. (2) It would be interesting to analyze a similar data set for a developing country, especially where efforts have been made to emphasize birth control. Some measure of access to birth control could be useful if it varied by region. Sometimes, one can find policy changes in the advertisement or availability of contraceptives.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: FERTIL1.DES\nDataset title (if known): Not provided\nSuggested title: Fertility and Education Dataset\n\nThe dataset appears to contain information on the fertility and education of a group of individuals over time. The variables include demographic characteristics such as age, race, and location, as well as educational attainment for the individual, their mother, and their father. The dataset also includes information on the number of children ever born to the individual, as well as variables that interact education with different years.\nPotential research ideas:\nInvestigate the relationship between an individual’s education and their fertility. This could involve examining how the number of children ever born is influenced by the individual’s own education, as well as the education of their parents.\nExplore the role of geographic location and community type (e.g., farm, rural, town, small city) in shaping educational and fertility outcomes. This could provide insights into the social and environmental factors that contribute to these important life events.\nAnalyze how the relationship between education and fertility has changed over time, by examining the interaction between education and the different years included in the dataset. This could shed light on the evolving dynamics between these two important life outcomes.\nInvestigate the potential differences in educational and fertility patterns between black and non-black individuals, and explore the factors that may contribute to these differences.\nDevelop a predictive model to estimate the number of children ever born based on the available demographic and educational variables. This could have practical applications in areas such as family planning and resource allocation."
  },
  {
    "objectID": "data_resources.html#fertil2",
    "href": "data_resources.html#fertil2",
    "title": "Data-Set Handbook",
    "section": "FERTIL2",
    "text": "FERTIL2\nSource: These data were obtained by James Heakins, a former MSU undergraduate, for a term project. They come from Botswana’s 1988 Demographic and Health Survey. Used in Text: page 526-527\nNotes: Currently, this data set is used only in one computer exercise. Since the dependent variable of interest – number of living children or number of children every born – is a count variable, the Poisson regression model discussed in Chapter 17 can be used. However, some care is required to combine Poisson regression with an endogenous explanatory variable (educ). I refer you to Chapter 19 of my book Econometric Analysis of Cross Section and Panel Data. Even in the context of linear models, much can be done beyond Computer Exercise C15.2. At a minimum, the binary indicators for various religions can be added as controls. One might also interact the schooling variable, educ, with some of the exogenous explanatory variables.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: FERTIL2.DES\nDataset title (if known): Not provided\nSuggested title: Fertility and Household Characteristics Dataset\n\nThe dataset appears to contain information on the fertility and household characteristics of a population. The variables include demographic information such as the month and year of birth, age, and marital status, as well as indicators of socioeconomic status, such as education, ownership of household items, and religion. The dataset also includes variables related to fertility, such as the number of children ever born, age at first birth, and use of birth control methods.\nPotential research ideas:\n\nInvestigate the relationship between socioeconomic status (as measured by variables like education, household assets, and urban/rural residence) and fertility outcomes (such as the number of children ever born and the use of birth control methods). This could provide insights into the factors that influence family planning decisions.\nAnalyze the role of religion in shaping fertility preferences and behaviors. The variables on religious affiliation (spirit, protestant, and catholic) could be used to explore how religious beliefs and practices influence family size and the use of contraception.\nExamine the impact of age at first marriage on subsequent fertility outcomes. The variable on age at first marriage could be used to investigate how this life event affects the timing and number of births.\nExplore the relationship between women’s education and their fertility preferences and behaviors. The variables on women’s education and the “ideal” number of children could be used to understand how education shapes family planning decisions.\nInvestigate the differences in fertility patterns between urban and rural areas. The urban/rural variable could be used to analyze how the place of residence influences fertility outcomes and the use of birth control methods."
  },
  {
    "objectID": "data_resources.html#fertil3",
    "href": "data_resources.html#fertil3",
    "title": "Data-Set Handbook",
    "section": "FERTIL3",
    "text": "FERTIL3\nSource: L.A. Whittington, J. Alm, and H.E. Peters (1990), “Fertility and the Personal Exemption: Implicit Pronatalist Policy in the United States,” American Economic Review 80, 545-556. The data are given in the article. Used in Text: pages 346-347, 355, 362, 363, 363, 381, 384-385, 421, 634, 639-640\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: FERTIL3.DES\nDataset title (if known): Not provided\nSuggested title: Fertility and Personal Exemption Data\n\nThe dataset appears to contain information on fertility rates (births per 1000 women aged 15-44) and personal exemption values in the United States from 1913 to 1984. The data includes variables related to fertility rates (gfr, cgfr, gfr_1, gfr_2), personal exemption values (pe, cpe, cpe_1, cpe_2, cpe_3, cpe_4), and time-related variables (year, t, tsq, tcu, ww2, pill). The dataset has 72 observations.\nPotential research ideas:\n\nExamine the relationship between personal exemption values and fertility rates: Investigate how changes in personal exemption values (pe) over time have influenced fertility rates (gfr) in the United States. This could provide insights into the economic factors that affect family planning decisions.\nAnalyze the impact of the introduction of the birth control pill: Explore how the availability of the birth control pill (pill) has affected fertility rates (gfr) in the United States. This could shed light on the societal and demographic changes associated with the widespread use of contraception.\nInvestigate the long-term trends in fertility rates: Analyze the changes in fertility rates (gfr) over the 72-year period to identify any long-term trends or patterns. This could help understand the demographic shifts and their potential implications for social and economic policies.\nExplore the role of World War II on fertility rates: Examine the impact of World War II (ww2) on fertility rates (gfr) in the United States. This could provide insights into how major historical events can influence family planning and population dynamics.\nAssess the lagged effects of personal exemption changes: Investigate the delayed effects of changes in personal exemption values (cpe_1, cpe_2, cpe_3, cpe_4) on fertility rates (gfr). This could help identify the time frame in which economic factors influence reproductive decisions."
  },
  {
    "objectID": "data_resources.html#fish",
    "href": "data_resources.html#fish",
    "title": "Data-Set Handbook",
    "section": "FISH",
    "text": "FISH\nSource: K Graddy (1995), “Testing for Imperfect Competition at the Fulton Fish Market,” RAND Journal of Economics 26, 75-92. Professor Graddy’s collaborator on a later paper, Professor Joshua Angrist at MIT, kindly provided me with these data. Used in Text: pages 422-423, 556-557\nNotes: This is a nice example of how to go about finding exogenous variables to use as instrumental variables. Often, weather conditions can be assumed to affect supply while having a negligible effect on demand. If so, the weather variables are valid instrumental variables for price in the demand equation. It is a simple matter to test whether prices vary with weather conditions by estimating the reduced form for price.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: FISH.DES\nDataset title (if known): Not provided\nSuggested title: Fish Sales and Weather Data\n\nThe dataset appears to contain information about the sales of fish to Asian and white buyers, as well as weather-related variables such as wind speed and wave height. The data includes variables related to the price and quantity of fish sold, as well as indicators for the day of the week. Additionally, there are derived variables such as average price, total quantity, and various lags and differences related to price and quantity.\nPotential research ideas:\n\nInvestigating the relationship between weather conditions (wind speed and wave height) and fish sales. This could involve analyzing how changes in weather variables impact the price and quantity of fish sold to different buyer groups.\nExamining the differences in pricing and sales patterns between Asian and white buyers. This could include analyzing factors that influence the price and quantity sold to each buyer group, as well as exploring any potential price discrimination or market segmentation.\nAnalyzing the impact of day-of-the-week on fish sales. This could involve investigating whether there are significant differences in price and quantity sold on different days of the week, and exploring potential explanations for these patterns.\nExploring the dynamics of price and quantity changes over time. This could include analyzing the trends and volatility in the average price and total quantity sold, as well as investigating the relationships between the various lagged and differenced variables.\nDeveloping a predictive model to forecast fish sales based on the available variables. This could involve using techniques such as regression analysis or time series modeling to identify the key drivers of price and quantity and generate accurate forecasts."
  },
  {
    "objectID": "data_resources.html#fringe",
    "href": "data_resources.html#fringe",
    "title": "Data-Set Handbook",
    "section": "FRINGE",
    "text": "FRINGE\nSource: F. Vella (1993), “A Simple Estimator for Simultaneous Models with Censored Endogenous Regressors,” International Economic Review 34, 441-457. Professor Vella kindly provided the data. Used in Text: page 596-597\nNotes: Currently, this data set is used in only one Computer Exercise – to illustrate the Tobit model. It can be used much earlier. First, one could just ignore the pileup at zero and use a linear model where any of the hourly benefit measures is the dependent variable. Another possibility is to use this data set for a problem set in Chapter 4, after students have read Example 4.10. That example, which uses teacher salary/benefit data at the school level, finds the expected tradeoff, although it appears to less than one-to-one. By contrast, if you do a similar analysis with FRINGE, you will not find a tradeoff. A positive coefficient on the benefit/salary ratio is not too surprising because we probably cannot control for enough factors, especially when looking across different occupations. The Michigan school-level data is more aggregated than one would like, but it does restrict attention to a more homogeneous group: high school teachers in Michigan.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: FRINGE.DES\nDataset title (if known): Not provided\nSuggested title: Employee Compensation and Characteristics\n\nThe dataset contains information on the annual earnings, hourly earnings, work experience, age, number of dependents, marital status, tenure with the current employer, education level, geographic location, gender, race, union membership, and occupation type for a sample of 616 individuals. Additionally, the dataset includes information on the annual hours worked, various employee benefits (vacation days, sick leave, insurance, and pension), and derived variables such as hourly benefits, annual hours squared, benefit-to-earnings ratio, log of annual hours, tenure squared, experience squared, log of annual earnings, pension-to-earnings ratio, and vacation and sick leave-to-earnings ratio.\nPotential research ideas:\n\nInvestigate the relationship between employee characteristics (e.g., age, education, experience) and their annual and hourly earnings, as well as the impact of these characteristics on the various employee benefits.\nAnalyze the differences in employee compensation and benefits between different geographic regions, industries, and occupations, and explore the factors that contribute to these differences.\nExamine the role of union membership in determining employee compensation and benefits, and how this relationship varies across different employee characteristics and industries.\nExplore the impact of employee benefits, such as vacation days, sick leave, insurance, and pension, on employee productivity, job satisfaction, and retention.\nInvestigate the relationship between employee compensation and the ratio of benefits to earnings, and how this ratio varies across different employee and job characteristics."
  },
  {
    "objectID": "data_resources.html#gpa1",
    "href": "data_resources.html#gpa1",
    "title": "Data-Set Handbook",
    "section": "GPA1",
    "text": "GPA1\nSource: Christopher Lemmon, a former MSU undergraduate, collected these data from a survey he took of MSU students in Fall 1994. Used in Text: pages 72, 74, 77, 113. 127. 155. 162. 225. 256. 286. 291\nNotes: This is a nice example of how students can obtain an original data set by focusing locally and carefully composing a survey.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: GPA1.DES\nDataset title (if known): Not provided\nSuggested title: Student Characteristics and Academic Performance\n\nThe dataset contains information on 141 college students, including their demographic characteristics, academic background, campus involvement, and work and lifestyle habits. The variables cover a range of topics, such as age, class standing, gender, major, grade point averages, standardized test scores, employment status, transportation methods, extracurricular activities, and parental education levels.\nPotential research ideas:\n\nExamine the relationship between student characteristics (e.g., gender, major, campus involvement) and academic performance (e.g., GPA, ACT scores). This could provide insights into factors that contribute to student success.\nInvestigate the impact of work and lifestyle habits (e.g., hours worked, transportation method, alcohol consumption) on academic outcomes. This could help identify potential areas for intervention or support to improve student well-being and academic achievement.\nAnalyze the differences in academic and extracurricular engagement between students from different socioeconomic backgrounds, as indicated by parental education levels. This could shed light on issues of educational equity and access.\nExplore the role of campus resources and involvement (e.g., living on campus, using a personal computer, participating in clubs) in shaping student experiences and outcomes. This could inform the development of targeted support programs.\nInvestigate the factors that contribute to students’ decisions to continue their studies beyond the traditional four-year timeline (i.e., fifth-year seniors). This could provide valuable insights into the challenges and motivations of non-traditional students."
  },
  {
    "objectID": "data_resources.html#gpa2",
    "href": "data_resources.html#gpa2",
    "title": "Data-Set Handbook",
    "section": "GPA2",
    "text": "GPA2\nSource: For confidentiality reasons, I cannot provide the source of these data. I can say that they come from a midsize research university that also supports men’s and women’s athletics at the Division I level. Used in Text: pages 104, 178, 202-203, 204-205, 215, 252, 256-257\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: GPA2.DES\nDataset title (if known): Not provided\nSuggested title: College Student Performance and Demographic Characteristics\n\nThe dataset contains information on 4,137 college students, including their academic performance (SAT scores, GPA, and class rank), demographic characteristics (gender, race), and high school background (graduating class size and rank). The variables provide insights into factors that may influence a student’s academic success in college.\nPotential research ideas:\n\nInvestigate the relationship between SAT scores, high school performance, and college GPA. This could help identify the key factors that contribute to academic success in college and inform admissions and academic support policies.\nExamine the differences in academic performance between athletes and non-athletes. This could provide insights into the challenges and benefits of participating in college sports and inform policies related to student-athlete support and academic integration.\nAnalyze the impact of gender and race on academic outcomes, such as GPA and SAT scores. This could shed light on potential disparities and inform diversity and inclusion initiatives in higher education.\nExplore the relationship between high school graduating class size and college performance. This could help understand the influence of high school environment on student success and guide the development of targeted support programs for students from different high school backgrounds.\nInvestigate the combined effects of multiple variables, such as SAT scores, high school rank, and demographic characteristics, on college GPA. This could lead to the development of more comprehensive predictive models for student success and inform personalized academic support strategies."
  },
  {
    "objectID": "data_resources.html#gpa3",
    "href": "data_resources.html#gpa3",
    "title": "Data-Set Handbook",
    "section": "GPA3",
    "text": "GPA3\nSource: See GPA2 Used in Text: pages 237-238, 266-267, 287-288, 444, 455\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: GPA3.DES\nDataset title (if known): Not provided\nSuggested title: Student Academic Performance Dataset\n\nThe dataset contains information about the academic performance of 732 students, including their term GPA, cumulative GPA, SAT scores, high school rank and size, and other demographic and academic variables. The data covers multiple semesters, allowing for the analysis of changes in academic performance over time.\nPotential research ideas:\n\nExamining the relationship between SAT scores, high school rank, and college GPA: This analysis could investigate how these factors influence academic performance and identify any potential predictors of student success.\nAnalyzing the impact of being a student-athlete on academic outcomes: The dataset includes information on whether a student is a football player, which could be used to explore the challenges and benefits of balancing athletic and academic commitments.\nInvestigating the role of demographic factors, such as gender and race, in academic performance: This research could shed light on potential disparities in educational outcomes and inform efforts to promote equity and inclusion.\nExploring the effects of changes in academic load (total hours) and course performance (course GPA) on term GPA: This analysis could provide insights into the factors that contribute to fluctuations in student performance over time.\nAssessing the impact of a student’s first semester on their subsequent academic trajectory: The dataset includes a variable indicating whether a student is in their first semester, which could be used to investigate the importance of the transition to college on long-term success."
  },
  {
    "objectID": "data_resources.html#happiness",
    "href": "data_resources.html#happiness",
    "title": "Data-Set Handbook",
    "section": "HAPPINESS",
    "text": "HAPPINESS\nSource: Subset of data collected by Kevin Williams for a McNair Scholars project in Summer 2008 at Michigan State University. The data come from several waves of the General Social Survey, and is therefore a pooled cross sectional data set. Professor Williams, now at Yale University, kindly provided the data. Used in Text: not used\nNotes: This data set can be used to estimate models of self-reported “happiness,” including studying whether the effects of certain variables – such as education, gender, race, and having children –changed in importance from the mid-1990s to the mid-2000s. For a similar example, see how FERTIL1 is used in Example 13.1 in the text.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: happiness.des\nDataset title (if known): Not provided\nSuggested title: General Social Survey (GSS) Happiness and Demographic Data\n\nThe dataset appears to contain information on various demographic and socioeconomic characteristics of respondents, as well as their self-reported happiness and other related variables. The variables include year, work status, occupational prestige, marital status, education, household composition, income, region, religious attendance, gun ownership, television viewing, and voting behavior. The dataset seems to span multiple years, with some variables indicating specific years (e.g., y94, y96, y98, y00, y02, y04, y06).\nPotential research ideas:\n\nExploring the relationship between socioeconomic status (e.g., education, income) and self-reported happiness: This could involve examining how factors like education, income, and occupational prestige are associated with an individual’s overall happiness and life satisfaction.\nInvestigating the role of household composition and family structure on happiness: The variables related to the presence of young children, preteens, and teenagers in the household could be used to analyze how the family dynamic and life stage influence an individual’s well-being.\nAnalyzing the impact of religious attendance and beliefs on happiness: The dataset includes information on the frequency of religious service attendance, which could be used to explore the relationship between religiosity and happiness.\nExamining the association between gun ownership and happiness: The dataset includes a variable indicating whether the respondent owns a gun, which could be used to investigate the potential link between gun ownership and overall life satisfaction.\nStudying the relationship between political affiliation and happiness: The variables related to voting behavior for George W. Bush in the 2000 and 2004 elections could be used to explore how political beliefs and preferences are associated with an individual’s self-reported happiness."
  },
  {
    "objectID": "data_resources.html#hprice1",
    "href": "data_resources.html#hprice1",
    "title": "Data-Set Handbook",
    "section": "HPRICE1",
    "text": "HPRICE1\nSource: Collected from the real estate pages of the Boston Globe during 1990. These homes sold in the Boston, MA area. Used in Text: pages 109-110, 148-149, 155-156, 160, 205, 216, 226-227, 270-271, 273, 290, 297-298\nNotes: Typically, it is very easy to obtain data on selling prices and characteristics of homes, using publicly available databases. It is interesting to match the information on houses with other information – such as local crime rates, quality of the local schools, pollution levels, and so on – and estimate the effects of such variables on housing prices.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: HPRICE1.DES\nDataset title (if known): Not provided\nSuggested title: House Price and Characteristics Dataset\n\nThe dataset contains information on 88 houses, including their price, assessed value, number of bedrooms, lot size, house size, and whether the home is of colonial style. The variables include both the original values and their logarithmic transformations, which can be useful for statistical analysis.\nPotential research ideas:\n\nInvestigating the relationship between house price and its characteristics: This study could explore how factors such as the number of bedrooms, lot size, and house size influence the price of a house. Regression analysis could be used to quantify the impact of these variables on house prices.\nAnalyzing the differences between colonial and non-colonial style homes: Researchers could investigate whether there are significant differences in the prices, assessed values, or other characteristics between colonial and non-colonial style homes in the dataset.\nExamining the accuracy of assessed values: The dataset provides both the house price and the assessed value, which could be used to assess the accuracy of the assessed values. This could provide insights into the assessment process and potential biases.\nExploring the relationship between house size and lot size: The dataset includes both the square footage of the house and the size of the lot. Researchers could investigate the correlation between these two variables and how they jointly influence house prices.\nComparing the predictive power of different variables: Researchers could use the dataset to compare the relative importance of different variables, such as the number of bedrooms, lot size, and house size, in predicting house prices. This could help identify the most influential factors in the housing market."
  },
  {
    "objectID": "data_resources.html#hprice2",
    "href": "data_resources.html#hprice2",
    "title": "Data-Set Handbook",
    "section": "HPRICE2",
    "text": "HPRICE2\nSource: D. Harrison and D.L. Rubinfeld (1978), “Hedonic Housing Prices and the Demand for Clean Air,” by Harrison, D. and D.L.Rubinfeld, Journal of Environmental Economics and Management 5, 81-102. Diego Garcia, a former Ph.D. student in economics at MIT, kindly provided these data, which he obtained from the book Regression Diagnostics: Identifying Influential Data and Sources: of Collinearity, by D.A. Belsey, E. Kuh, and R. Welsch, 1990. New York: Wiley. Used in Text: pages 106, 130, 185, 186-187, 190-191\nNotes: The census contains rich information on variables such as median housing prices, median income levels, average family size, and so on, for fairly small geographical areas. If such data can be merged with pollution data, one can update the Harrison and Rubinfeld study. Presumably, this has been done in academic journals.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: HPRICE2.DES\nDataset title (if known): Not provided\nSuggested title: Boston Housing Dataset\n\nThe dataset appears to contain information about housing prices and various characteristics of neighborhoods in the Boston area. The variables include the median housing price, crime rate, nitrous oxide levels, number of rooms, distance to employment centers, accessibility to radial highways, property tax, student-teacher ratio, and the percentage of people with lower socioeconomic status. Additionally, the dataset includes the logarithmic transformations of some of the variables, such as housing price, nitrous oxide, and property tax.\nPotential research ideas:\n\nInvestigating the relationship between housing prices and neighborhood characteristics: Researchers could explore how factors like crime rate, air quality, accessibility, and socioeconomic status influence the median housing prices in the Boston area. This could provide insights into the drivers of housing market dynamics.\nAnalyzing the impact of transportation infrastructure on housing prices: The dataset includes information on the distance to employment centers and accessibility to radial highways. Researchers could investigate how these transportation-related variables affect housing prices, which could inform urban planning and development decisions.\nExamining the role of environmental factors in housing prices: The dataset includes information on nitrous oxide levels, which can be used to study the impact of air quality on housing prices. This could have implications for environmental regulations and their effects on the housing market.\nExploring the relationship between educational resources and housing prices: The student-teacher ratio variable could be used to investigate how the quality of educational resources in a neighborhood influences housing prices. This could provide insights into the importance of educational infrastructure in residential location decisions.\nAnalyzing socioeconomic disparities in housing prices: The dataset includes the percentage of people with lower socioeconomic status, which could be used to examine how income and wealth inequalities are reflected in the housing market. This research could inform policies aimed at addressing housing affordability and accessibility."
  },
  {
    "objectID": "data_resources.html#hseinv",
    "href": "data_resources.html#hseinv",
    "title": "Data-Set Handbook",
    "section": "HSEINV",
    "text": "HSEINV\nSource: D. McFadden (1994), “Demographics, the Housing Market, and the Welfare of the Elderly,” in D.A. Wise (ed.), Studies in the Economics of Aging. Chicago: University of Chicago Press, 225-285. The data are contained in the article. Used in Text: pages 354-355, 358, 390, 609-610, 638, 783\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: HSEINV.DES\nDataset title (if known): Not provided\nSuggested title: Housing Investment and Prices\n\nThe dataset contains annual observations from 1947 to 1988 (42 observations) on various variables related to housing investment and prices in the United States. The variables include real housing investment (inv), population (pop), housing price index (price), and their logarithmic transformations (linv, lpop, lprice). The dataset also includes per capita investment (invpc), its logarithmic transformation (linvpc), and the first lags of log price (lprice_1) and log per capita investment (linvpc_1). Additionally, the dataset includes the growth rates of housing prices (gprice) and per capita investment (ginvpc).\nPotential research ideas:\n\n\nAnalyzing the relationship between housing investment and housing prices: One could investigate the dynamics between real housing investment and housing prices, exploring the potential causal relationships and the impact of factors such as population on this relationship.\nExamining the determinants of per capita housing investment: Researchers could study the factors that influence per capita housing investment, such as housing prices, population, and economic trends, to better understand the drivers of housing investment at the individual level.\nInvestigating the role of housing price expectations: Using the lagged log price variable (lprice_1), researchers could explore how past housing price movements influence current housing investment decisions and the implications for housing market dynamics.\nAnalyzing the impact of housing price growth on housing investment: The dataset provides the growth rate of housing prices (gprice), which could be used to examine how changes in housing prices affect the growth of housing investment over time.\nExploring the relationship between per capita housing investment and its growth: The dataset includes the growth rate of per capita housing investment (ginvpc), which could be used to study the factors that drive changes in per capita housing investment and the implications for the housing market."
  },
  {
    "objectID": "data_resources.html#htv",
    "href": "data_resources.html#htv",
    "title": "Data-Set Handbook",
    "section": "HTV",
    "text": "HTV\nSource: J.J. Heckman, J.L. Tobias, and E. Vytlacil (2003), “Simple Estimators for Treatment Parameters in a Latent-Variable Framework,” Review of Economics and Statistics 85, 748-755. Professor Tobias kindly provided the data, which were obtained from the 1991 National Longitudinal Survey of Youth. All people in the sample are males age 26 to 34. For confidentiality reasons, I have included only a subset of the variables used by the authors. Used in Text: pages 529, 599-600\nNotes: Because an ability measure is included in this data set, it can be used as another illustration of including proxy variables in regression models. See Chapter 9. Also, one can try the IV procedure with the ability measure included as an exogenous explanatory variable.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: HTV.DES\nDataset title (if known): Not provided\nSuggested title: Wage and Education Dataset\n\nThe dataset appears to contain information about individuals’ wages, educational attainment, family background, and geographic location. The variables include hourly wage, ability measure, highest grade completed, region of residence, potential experience, parental education, family structure, number of siblings, and college tuition. This dataset could be used to study the factors that influence individual wages and educational outcomes.\nPotential research ideas:\n\nExamining the relationship between family background (parental education, family structure, number of siblings) and educational attainment. This could provide insights into the intergenerational transmission of educational outcomes.\nInvestigating the impact of geographic location (region, urban/rural) on wages and educational attainment. This could help identify regional disparities and inform policy decisions.\nAnalyzing the role of ability and educational attainment in determining wages. This could shed light on the relative importance of cognitive skills and formal education in the labor market.\nExploring the relationship between college tuition and educational outcomes, such as college enrollment and completion rates. This could inform discussions on the affordability and accessibility of higher education.\nStudying the long-term effects of potential experience on wages, including the potential for diminishing returns. This could provide insights into the dynamics of career development and earnings over the life course."
  },
  {
    "objectID": "data_resources.html#infmrt",
    "href": "data_resources.html#infmrt",
    "title": "Data-Set Handbook",
    "section": "INFMRT",
    "text": "INFMRT\nSource: Statistical Abstract of the United States, 1990 and 1994. (For example, the infant mortality rates come from Table 113 in 1990 and Table 123 in 1994.) Used in Text: pages 320-321, 328\nNotes: An interesting exercise is to add the percentage of the population on AFDC (afdcper) to the infant mortality equation. Pooled OLS and first differencing can give very different estimates. Adding the years 1998 and 2002 and applying fixed effects seems natural. Intervening years can be added, too, although variation in the key variables from year to year might be minimal.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: INFMRT.DES\nDataset title (if known): Not provided\nSuggested title: Infant Mortality and Socioeconomic Factors\n\nThe dataset appears to contain information on infant mortality rates, AFDC (Aid to Families with Dependent Children) participation, population, per capita income, the number of physicians per 100,000 civilian population, and the percentage of the population on AFDC. The data is observed for 102 units, likely states or counties, over two years (1987 and 1990). The dataset also includes logarithmic transformations of some variables and a dummy variable indicating whether the observation is for Washington, DC.\nPotential research ideas:\n\nExamine the relationship between infant mortality rates and socioeconomic factors, such as AFDC participation, per capita income, and the availability of healthcare resources (as measured by the number of physicians per 100,000 population). This could provide insights into the drivers of infant mortality and inform policy interventions.\nInvestigate the differences in infant mortality rates and socioeconomic factors between Washington, DC, and the other units in the dataset. This could shed light on the unique challenges and disparities faced by the nation’s capital.\nAnalyze the changes in infant mortality rates, AFDC participation, and other variables between 1987 and 1990. This longitudinal analysis could reveal trends and the impact of any policy or economic changes during this period.\nExplore the role of AFDC participation and its relationship with other socioeconomic factors, such as per capita income and the availability of healthcare resources. This could provide insights into the effectiveness of the AFDC program and its impact on the well-being of families.\nDevelop a predictive model to estimate infant mortality rates based on the available socioeconomic variables. This could help identify the most influential factors and guide policymakers in targeting specific areas for intervention."
  },
  {
    "objectID": "data_resources.html#injury",
    "href": "data_resources.html#injury",
    "title": "Data-Set Handbook",
    "section": "INJURY",
    "text": "INJURY\nSource: B.D. Meyer, W.K. Viscusi, and D.L. Durbin (1995), “Workers’ Compensation and Injury Duration: Evidence from a Natural Experiment,” American Economic Review 85, 322-340. Professor Meyer kindly provided the data. Used in Text: pages 435-436, 453\nNotes: This data set also can be used to illustrate the Chow test in Chapter 7. In particular, students can test whether the regression functions differ between Kentucky and Michigan. Or, allowing for different intercepts for the two states, do the slopes differ? A good lesson from this example is that a small R-squared is compatible with the ability to estimate the effects of a policy. Of course, for the Michigan data, which has a smaller sample size, the estimated effect is much less precise (but of virtually identical magnitude).\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: INJURY.DES\nDataset title (if known): Not provided\nSuggested title: Workers’ Compensation Injury Data\n\nThe dataset appears to contain information about workers’ compensation claims, including the duration of benefits, changes in benefits, characteristics of the injured workers, and the type of injury sustained. The data includes variables related to the worker’s gender, marital status, location (Kentucky or Michigan), and industry (manufacturing or construction). The injury types covered include head, neck, upper extremities, trunk, lower back, and lower extremities, as well as occupational diseases.\nPotential research ideas:\n\nExamine the relationship between worker characteristics (gender, marital status, location) and the duration of workers’ compensation benefits. This could provide insights into potential disparities in the workers’ compensation system.\nInvestigate the impact of changes in workers’ compensation benefits on the duration of claims and the types of injuries reported. This could help policymakers understand the effects of benefit policy changes.\nAnalyze the differences in injury patterns and claim durations between the manufacturing and construction industries. This could inform industry-specific safety and rehabilitation programs.\nExplore the factors associated with occupational diseases, such as the worker’s gender, industry, or injury type. This could help identify high-risk occupations and guide preventive measures.\nAssess the impact of injury type on the duration of workers’ compensation claims. This could inform rehabilitation and return-to-work strategies for different types of injuries."
  },
  {
    "objectID": "data_resources.html#intdef",
    "href": "data_resources.html#intdef",
    "title": "Data-Set Handbook",
    "section": "INTDEF",
    "text": "INTDEF\nSource: Economic Report of the President, 2004, Tables B64, B73, and B79. Used in Text: pages 345, 363, 415, 527\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: INTDEF.DES\nDataset title (if known): Not provided\nSuggested title: Macroeconomic Variables and Fiscal Policy\n\nThe dataset contains information on various macroeconomic variables and fiscal policy indicators for the United States from 1948 to 2003. The variables include the 3-month Treasury bill rate (i3), the consumer price index (CPI) inflation rate (inf), federal receipts as a percentage of GDP (rec), federal outlays as a percentage of GDP (out), the federal budget deficit as a percentage of GDP (def), and several lagged and change variables related to these measures.\nPotential research ideas:\n\nInvestigating the relationship between interest rates (i3) and inflation (inf): This analysis could explore the dynamics between monetary policy and price stability, and how changes in interest rates may impact inflation over time.\nAnalyzing the impact of fiscal policy on economic growth: The dataset provides information on federal receipts, outlays, and the budget deficit, which could be used to examine the effects of fiscal policy measures on economic output (out) and other macroeconomic indicators.\nExploring the role of fiscal policy changes in 1977 (y77): The dataset includes a variable indicating a change in the fiscal year starting in 1977, which could be used to investigate the potential structural breaks or policy shifts and their implications for the macroeconomic variables.\nExamining the dynamics of budget deficits (def): The dataset allows for the analysis of the evolution of budget deficits over time, including the factors that may contribute to changes in the deficit, such as revenue (rec) and expenditure (out) patterns.\nInvestigating the relationship between interest rate changes (ci3) and inflation changes (cinf): This analysis could provide insights into the transmission mechanisms of monetary policy and how changes in interest rates may affect inflationary pressures in the economy."
  },
  {
    "objectID": "data_resources.html#intqrt",
    "href": "data_resources.html#intqrt",
    "title": "Data-Set Handbook",
    "section": "INTQRT",
    "text": "INTQRT\nSource: From Salomon Brothers, Analytical Record of Yields and Yield Spreads, 1990. The folks at Salomon Brothers kindly provided the Record at no charge when I was an assistant professor at MIT.\nUsed in Text: pages 388-389, 612, 617, 620, 621, 639, 640\nNotes: A nice feature of the Salomon Brothers data is that the interest rates are not averaged over a month or quarter – they are end-of-month or end-of-quarter rates. Asset pricing theories apply to such “point-sampled” data, and not to averages over a period. Most other sources report monthly or quarterly averages. This is a good data set to update and test whether current data are more or less supportive of basic asset pricing theories.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: INTQRT.DES\nDataset title (if known): Not provided\nSuggested title: Treasury Bill Yields and Spreads\n\nThe dataset appears to contain information related to Treasury bill yields and spreads. It includes variables such as bond equivalent yields for 3-month, 6-month, and 1-year Treasury bills, as well as their corresponding prices, holding yields, and spreads. The dataset also includes lagged and change variables for these measures, suggesting a time series or panel data structure. The dataset has 124 observations.\nPotential research ideas:\n\nAnalyzing the relationship between short-term Treasury bill yields (3-month and 6-month) and the longer-term 1-year yield. This could provide insights into the term structure of interest rates and the dynamics of the yield curve.\nInvestigating the determinants of holding yields for 3-month and 6-month Treasury bills. This could involve examining the relationship between holding yields and factors such as changes in bond prices, interest rates, and market conditions.\nExploring the behavior of the spread between 6-month and 3-month Treasury bill yields (spr63) and its potential implications for monetary policy and market expectations.\nAnalyzing the dynamics of changes in Treasury bill yields (cr3, cr6) and their relationship with macroeconomic variables or other financial indicators.\nExamining the persistence and predictability of holding yield changes (chy3, chy6) and their potential applications in investment strategies or risk management."
  },
  {
    "objectID": "data_resources.html#inven",
    "href": "data_resources.html#inven",
    "title": "Data-Set Handbook",
    "section": "INVEN",
    "text": "INVEN\nSource: Economic Report of the President, 1997, Tables B4, B20, B61, and B71. Used in Text: pages 391, 421-422, 423, 614, 783\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: INVEN.DES\nDataset title (if known): Not provided\nSuggested title: Macroeconomic Variables Dataset\n\nThe dataset contains 13 variables related to various macroeconomic indicators in the United States from 1959 to 1995. The variables include the 3-month Treasury bill rate (i3), the consumer price index (CPI) inflation rate (inf), the level of inventories (inven) and GDP (gdp) in billions of 1992 dollars, the real interest rate (r3), the change in inventories (cinven) and GDP (cgdp) from the previous year, the change in the 3-month Treasury bill rate (ci3), the change in the inflation rate (cinf), and the growth rates of inventories (ginven) and GDP (ggdp).\nPotential research ideas:\n\n\nInvestigate the relationship between changes in interest rates (ci3) and changes in inflation (cinf) to better understand the dynamics of monetary policy and its impact on the economy.\nAnalyze the relationship between changes in inventories (cinven) and changes in GDP (cgdp) to explore the role of inventory management in economic growth and business cycle fluctuations.\nExamine the long-term trends and patterns in the growth rates of inventories (ginven) and GDP (ggdp) to identify potential structural changes or shifts in the underlying economic dynamics.\nExplore the impact of real interest rates (r3) on the level of inventories (inven) and GDP (gdp) to understand the transmission mechanisms of monetary policy and its effects on the real economy.\nInvestigate the relationship between the 3-month Treasury bill rate (i3) and the level of GDP (gdp) to assess the predictive power of this interest rate as an indicator of economic activity."
  },
  {
    "objectID": "data_resources.html#jtrain",
    "href": "data_resources.html#jtrain",
    "title": "Data-Set Handbook",
    "section": "JTRAIN",
    "text": "JTRAIN\nSource: H. Holzer, R. Block, M. Cheatham, and J. Knott (1993), “Are Training Subsidies Effective? The Michigan Experience,” Industrial and Labor Relations Review 46, 625-636. The authors kindly provided the data. Used in Text: pages 133-134, 156, 226, 244-245, 328, 444-446, 456, 464-465, 469, 486, 521-522, 730-731, 740-741, 742\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: JTRAIN.DES\nDataset title (if known): Not provided\nSuggested title: Plant-level Data on Employment, Sales, and Training\n\nThe dataset appears to contain plant-level information on various aspects of manufacturing operations, including employment, sales, average salaries, scrap and rework rates, and training hours. The data spans three years (1987, 1988, and 1989) and includes both level and change variables, as well as some lagged variables. The dataset seems to be focused on understanding the relationship between training, productivity, and other plant-level characteristics.\nPotential research ideas:\n\nExamine the impact of training on plant-level productivity, as measured by sales, scrap, and rework rates. Investigate whether the effects of training vary based on plant characteristics, such as unionization or grant receipt.\nAnalyze the relationship between changes in employment and changes in sales, and explore how this relationship is influenced by factors like training, average salaries, and plant-level characteristics.\nInvestigate the determinants of changes in average salaries at the plant level, and assess whether factors like training, employment, and sales growth are associated with changes in compensation.\nExplore the dynamics of scrap and rework rates over time, and examine how these measures of quality are related to other plant-level variables, such as training, employment, and sales.\nAssess the impact of receiving a grant on various plant-level outcomes, such as employment, sales, and training, and investigate whether the effects of grants differ based on other plant characteristics."
  },
  {
    "objectID": "data_resources.html#jtrain2",
    "href": "data_resources.html#jtrain2",
    "title": "Data-Set Handbook",
    "section": "JTRAIN2",
    "text": "JTRAIN2\nSource: R.J. Lalonde (1986), “Evaluating the Econometric Evaluations of Training Programs with Experimental Data,” American Economic Review 76, 604-620. Professor Jeff Biddle, at MSU, kindly passed the data set along to me. He obtained it from Professor Lalonde. Used in Text: pages 16, 329-330\nNotes: Professor Lalonde obtained the data from the National Supported Work Demonstration job-training program conducted by the Manpower Demonstration Research Corporation in the mid 1970s. Training status was randomly assigned, so this is essentially experimental data. Computer Exercise C17.8 looks only at the effects of training on subsequent unemployment probabilities. For illustrating the more advanced methods in Chapter 17, a good exercise would be to have the students estimate a Tobit of re78 on train, and obtain estimates of the expected values for those with and without training. These can be compared with the sample averages.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: JTRAIN2.DES\nDataset title (if known): Not provided\nSuggested title: Job Training Program Participant Data\n\nThe dataset appears to contain information about individuals who participated in a job training program. The variables include demographic characteristics such as age, education, race, and marital status, as well as employment-related information such as real earnings in different years, unemployment status, and participation in the job training program. The dataset has 445 observations.\nPotential research ideas:\n\nEvaluating the effectiveness of the job training program: Researchers could investigate the impact of the job training program on participants’ employment and earnings outcomes, controlling for demographic and other relevant factors.\nExamining the relationship between individual characteristics and employment outcomes: Researchers could explore how factors such as age, education, race, and marital status are associated with participants’ real earnings and unemployment status before, during, and after the job training program.\nAnalyzing the role of prior employment and earnings: Researchers could investigate how participants’ employment and earnings history (e.g., real earnings in 1974 and 1975) are related to their subsequent outcomes, both in the job training program and in the labor market.\nExploring the impact of program duration: Researchers could examine whether the length of time participants spent in the job training program (as measured by the “mostrn” variable) is associated with their employment and earnings outcomes.\nInvestigating the differences between subgroups: Researchers could compare the outcomes of different subgroups, such as those with and without a high school degree, or between Black, Hispanic, and non-minority participants, to better understand the factors that may contribute to differential program impacts."
  },
  {
    "objectID": "data_resources.html#jtrain3",
    "href": "data_resources.html#jtrain3",
    "title": "Data-Set Handbook",
    "section": "JTRAIN3",
    "text": "JTRAIN3\nSource: R.H. Dehejia and S. Wahba (1999), “Causal Effects in Nonexperimental Studies: Reevaluating the Evaluation of Training Programs,” Journal of the American Statistical Association 94, 1053-1062. Professor Sergio Firpo, at Insper Institute of Education and Research in São Paulo, has used this data set in his work. He kindly provided it to me. This data set is a subset of that originally used by Lalonde in the study cited for JTRAIN2. Used in Text: pages 329-330, 457\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: JTRAIN3.DES\nDataset title (if known): Not provided\nSuggested title: Job Training and Earnings Dataset\n\nThe dataset appears to contain information about individuals who participated in a job training program. The variables include demographic characteristics (age, education, race, ethnicity, marital status), employment status (unemployment in 1974, 1975, and 1978), and earnings (in 1974, 1975, and 1978). The dataset also includes some derived variables, such as the square of age, interactions between training and earnings or unemployment, and the average of 1974 and 1975 earnings.\nPotential research ideas:\n\nExamine the impact of job training on employment and earnings outcomes. Researchers could investigate whether participation in the job training program led to higher earnings or reduced unemployment, and whether the effects varied based on individual characteristics such as age, education, or race/ethnicity.\nAnalyze the relationship between demographic factors and employment/earnings. Researchers could explore how variables like age, education, race, and marital status are associated with employment status and earnings, both before and after the job training program.\nInvestigate the persistence of the job training program’s effects. Researchers could analyze whether the impact on earnings and employment lasted beyond the initial years, or if the effects diminished over time.\nExplore the role of prior unemployment in moderating the effects of job training. Researchers could examine whether the benefits of the program were more pronounced for individuals who had been unemployed in the past, compared to those who had been continuously employed.\nConduct a cost-benefit analysis of the job training program. Researchers could estimate the program’s long-term impact on participants’ earnings and employment, and compare these benefits to the costs of implementing the program."
  },
  {
    "objectID": "data_resources.html#jtrain98",
    "href": "data_resources.html#jtrain98",
    "title": "Data-Set Handbook",
    "section": "JTRAIN98",
    "text": "JTRAIN98\nSource: This is a data set I created many years ago intended as an update to the files JTRAIN2 and JTRAIN3. While the data were partly generated by me, the data attributes are similar to data sets used to evaluate job training programs. Used in Text: 101-102, 248, 601\nNotes: The response variables, earn98 and unem98, both have discreteness: the former is a corner solutions (takes on the value zero and then a range of strictly positive values) and the latter is binary. One could use these in an exercise using methods in Chapter 17. unem98 can be used in a probit or logit model, earn98 in a Tobit model, or in Poisson regression (without assuming, of course, that the Poisson distribution is correct).\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: JTRAIN98.DES\nDataset title (if known): Not provided\nSuggested title: Job Training and Employment Outcomes\n\nThe dataset contains information on 1,130 individuals, including their participation in job training programs, demographic characteristics (age, race, ethnicity, marital status), and employment outcomes (earnings and unemployment) in 1996 and 1998. The variables provide insights into the relationship between job training, individual characteristics, and labor market outcomes over time.\nPotential research ideas:\n\nExamine the impact of job training on employment and earnings outcomes. Researchers could investigate whether participation in job training programs leads to higher earnings and lower unemployment rates, and whether the effects vary by individual characteristics such as race, ethnicity, or marital status.\nAnalyze the relationship between demographic factors and labor market outcomes. Researchers could explore how factors like age, education, race, and ethnicity influence earnings and unemployment, both before and after job training participation.\nInvestigate the long-term effects of job training on employment and earnings. Researchers could compare the outcomes of individuals who participated in job training programs in 1996 to their outcomes in 1998, to assess the persistence of the program’s effects.\nExplore the role of marital status in shaping labor market outcomes. Researchers could examine whether being married is associated with higher earnings or lower unemployment, and whether this relationship is affected by job training participation.\nConduct a comparative analysis of the employment and earnings trajectories of individuals who were unemployed in 1995 versus those who were unemployed in 1998. Researchers could investigate how job training and other factors may have influenced the subsequent labor market outcomes of these two groups."
  },
  {
    "objectID": "data_resources.html#kielmc",
    "href": "data_resources.html#kielmc",
    "title": "Data-Set Handbook",
    "section": "KIELMC",
    "text": "KIELMC\nSource: K.A. Kiel and K.T. McClain (1995), “House Prices During Siting Decision Stages: The Case of an Incinerator from Rumor through Operation,” Journal of Environmental Economics and Management 28, 241-255. Professors Kiel and McClain kindly provided the data, of which I used only a subset. Used in Text: pages 214, 431-434, 452, 454\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: KIELMC.DES\nDataset title (if known): Not provided\nSuggested title: Housing Price Dataset\n\nThe dataset appears to contain information about residential properties, including their selling prices, physical characteristics, and proximity to various features such as the central business district, interstate, and an incinerator. The data includes variables related to the age, size, and location of the houses, as well as some derived variables such as the logarithm of price and distance measures.\nPotential research ideas:\n\nInvestigate the impact of house age on selling price: Analyze the relationship between the age of the house (age and agesq) and its selling price (price, lprice, rprice, lrprice) to understand how the age of a house affects its market value.\nExamine the influence of neighborhood characteristics on housing prices: Explore the relationship between the neighborhood number (nbh) and the selling price of the houses to identify any patterns or differences in housing prices across different neighborhoods.\nAnalyze the effect of proximity to the central business district and interstate on housing prices: Investigate how the distance to the central business district (cbd) and the interstate (intst, lintst) influence the selling price of the houses.\nExplore the impact of house size and lot size on housing prices: Analyze the relationship between the square footage of the house (area, larea) and the lot size (land, lland) with the selling price of the houses.\nInvestigate the effect of environmental factors on housing prices: Examine the impact of proximity to the incinerator (dist, ldist, wind) on the selling price of the houses, and explore any potential differences in the effect between 1978 and 1981."
  },
  {
    "objectID": "data_resources.html#labsup",
    "href": "data_resources.html#labsup",
    "title": "Data-Set Handbook",
    "section": "LABSUP",
    "text": "LABSUP\nSource: The subset of data for black or Hispanic women used in J.A. Angrist and W.E. Evans (1998), “ Used in Text: pages 530-531\nNotes: This example can promote an interesting discussion of instrument validity, and in particular, how a variable that is beyond our control – for example, whether the first two children have the same gender – can, nevertheless, affect subsequent economic choices. Students are asked to think about such issues in Computer Exercise C13 in Chapter 15. A more egregious version of this mistake would be to treat a variable such as age as a suitable instrument because it is beyond our control: clearly age has a direct effect on many economic outcomes that would play the role of the dependent variable.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: LABSUP.DES\nDataset title (if known): Not provided\nSuggested title: Characteristics of Mothers and Families\n\nThe dataset contains information on 31,857 observations and 20 variables related to mothers and their families. The variables include the number of children, the gender composition of the first two children, the age of the mother at first birth, the mother’s race, employment status, income, and education level. The dataset provides insights into the demographic and socioeconomic characteristics of the sample.\nPotential research ideas:\nInvestigate the relationship between the gender composition of the first two children and the decision to have more children. Explore whether the presence of same-sex or mixed-sex children influences the likelihood of having additional children.\nAnalyze the impact of the mother’s age at first birth on her subsequent labor market outcomes, such as employment status, income, and hours worked. Examine whether there are differences in these outcomes based on the mother’s age at first birth.\nExplore the role of family income and the mother’s non-labor income on the educational attainment of the children. Investigate whether these financial factors have a significant influence on the educational outcomes of the children.\nAssess the differences in labor market participation and income between mothers of different racial and ethnic backgrounds. Identify any potential disparities and explore the factors that may contribute to these differences.\nExamine the relationship between the mother’s education level and the number of children she has. Investigate whether there is a correlation between the mother’s educational attainment and her family size."
  },
  {
    "objectID": "data_resources.html#lawsch85",
    "href": "data_resources.html#lawsch85",
    "title": "Data-Set Handbook",
    "section": "LAWSCH85",
    "text": "LAWSCH85\nSource: Collected by Kelly Barnett, an MSU economics student, for use in a term project. The data come from two sources: The Official Guide to U.S. Law Schools, 1986, Law School Admission Services, and The Gourman Report: A Ranking of Graduate and Professional Programs in American and International Universities, 1995, Washington, D.C. Used in Text: pages 105, 108, 159-160, 231-232\nNotes: More recent versions of both cited documents are available. One could try a similar analysis for, say, MBA programs or Ph.D. programs in economics. Quality of placements may be a good dependent variable, and measures of business school or graduate program quality could be included among the explanatory variables. Of course, one would want to control for factors describing the incoming class so as to isolate the effect of the program itself.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: LAWSCH85.DES\nDataset title (if known): Not provided\nSuggested title: Law School Characteristics and Outcomes\n\nThe dataset contains information on 156 law schools in the United States. The variables include the law school’s ranking, median starting salary of graduates, cost of attendance, median LSAT score and college GPA of students, library volume count, number of faculty, age of the law school, size of the entering class, and geographic location (north, south, east, or west). Additionally, there are derived variables such as the logarithm of salary and cost, student-faculty ratio, and indicators for top 10, 11-25, 26-40, and 41-60 ranked schools.\nPotential research ideas:\nExamine the relationship between law school characteristics (e.g., ranking, LSAT, GPA, library size, faculty size) and the median starting salary of graduates. This could provide insights into the factors that contribute to higher-paying job opportunities for law school graduates.\nInvestigate the differences in law school characteristics and outcomes between schools in different geographic regions (north, south, east, west). This analysis could reveal regional variations in the legal education landscape and identify potential factors driving these differences.\nAnalyze the impact of law school ranking on various outcomes, such as starting salary, student-faculty ratio, and library resources. This could help prospective law students understand the potential benefits and trade-offs of attending higher-ranked institutions.\nExplore the relationship between law school cost and other variables, such as ranking, LSAT, GPA, and starting salary. This could provide insights into the value proposition of legal education and the factors that influence the cost of attendance.\nInvestigate the factors that predict a law school’s ranking, such as LSAT, GPA, faculty size, library resources, and geographic location. This analysis could help identify the key drivers of law school prestige and inform strategic decision-making for law school administrators."
  },
  {
    "objectID": "data_resources.html#loanapp",
    "href": "data_resources.html#loanapp",
    "title": "Data-Set Handbook",
    "section": "LOANAPP",
    "text": "LOANAPP\nSource: W.C. Hunter and M.B. Walker (1996), “The Cultural Affinity Hypothesis and Mortgage Lending Decisions,” Journal of Real Estate Finance and Economics 13, 57-70. Professor Walker kindly provided the data. Used in Text: pages 257-258, 291, 329, 596\nNotes: These data were originally used in a famous study by researchers at the Boston Federal Reserve Bank. See A. Munnell, G.M.B. Tootell, L.E. Browne, and J. McEneaney (1996), “Mortgage Lending in Boston: Interpreting HMDA Data,” American Economic Review 86, 25-53.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: LOANAPP.DES\nDataset title (if known): Not provided\nSuggested title: Mortgage Loan Application Dataset\n\nThe dataset appears to contain information about mortgage loan applications, including details about the applicant, the property, the loan, and the outcome of the application. The variables cover a wide range of factors, such as the applicant’s income, employment, credit history, and demographic characteristics, as well as details about the property, the loan amount, and the type of action taken on the application.\nPotential research ideas:\n\nInvestigating the factors that influence the approval or rejection of mortgage loan applications: This could involve analyzing the relationship between applicant characteristics, property details, and loan terms with the likelihood of loan approval.\nExamining the impact of applicant demographics on loan outcomes: Researchers could explore whether factors like race, ethnicity, and gender are associated with differences in loan approval rates or loan terms.\nAnalyzing the role of credit history and financial factors in mortgage loan decisions: Researchers could investigate how variables like credit reports, debt-to-income ratios, and liquid assets influence the loan approval process.\nExploring the relationship between neighborhood characteristics and loan outcomes: Researchers could examine whether factors like unemployment rates, minority population, and property values in the surrounding area are associated with loan approval or denial.\nEvaluating the effectiveness of mortgage lending regulations and policies: Researchers could use this dataset to assess the impact of various regulatory and policy interventions on the fairness and accessibility of the mortgage lending process."
  },
  {
    "objectID": "data_resources.html#lowbrth",
    "href": "data_resources.html#lowbrth",
    "title": "Data-Set Handbook",
    "section": "LOWBRTH",
    "text": "LOWBRTH\nSource: Source: Statistical Abstract of the United States, 1990, 1993, and 1994. Used in Text: not used\nNotes: This data set can be used very much like INFMRT. It contains two years of state-level panel data. In fact, it is a superset of INFMRT. The key is that it contains information on low birth weights, as well as infant mortality. It also contains state identifies, so that several years of more recent data could be added for a term project. Putting in the variable afcdprc and its square leads to some interesting findings for pooled OLS and fixed effects (first differencing). After differencing, you can even try using the change in the AFDC payments variable as an instrumental variable for the change in afdcprc.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: LOWBRTH.DES\nDataset title (if known): Not provided\nSuggested title: State-Level Data on Low Birth Weights, Infant Mortality, and AFDC Participation\n\nThe dataset appears to contain state-level information on various health and socioeconomic indicators, including the percentage of low-birth-weight babies, infant mortality rates, the number of participants in the Aid to Families with Dependent Children (AFDC) program, population, per capita income, the number of physicians, hospital beds, and poverty rates. The data spans two years, 1987 and 1990, and includes both the original variables and their changes over time.\nPotential research ideas:\n\nExamine the relationship between AFDC participation and low birth weights or infant mortality rates. This could provide insights into the effectiveness of social welfare programs in improving health outcomes.\nInvestigate the impact of changes in per capita income, physician availability, and hospital bed capacity on the prevalence of low birth weights and infant mortality. This could help identify key factors that influence these important health indicators.\nAnalyze the spatial distribution of low birth weights, infant mortality, and AFDC participation across states. This could reveal regional patterns and help target interventions to areas with the greatest needs.\nExplore the role of poverty rates in shaping low birth weights and infant mortality, and how changes in poverty over time may have affected these outcomes.\nAssess the potential trade-offs or synergies between different state-level policies, such as AFDC funding and investments in healthcare infrastructure, in their impact on maternal and child health."
  },
  {
    "objectID": "data_resources.html#mathpnl",
    "href": "data_resources.html#mathpnl",
    "title": "Data-Set Handbook",
    "section": "MATHPNL",
    "text": "MATHPNL\nSource: Dr. Leslie Papke, an economics professor at MSU, collected these data from Michigan Department of Education web site, www.michigan.gov/mde. These are district-level data, which Professor Papke kindly provided. She has used building-level data in “The Effects of Spending on Test Pass Rates: Evidence from Michigan” (2005), Journal of Public Economics 89, 821-839. Used in Text: pages 456, 487-488\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: MATHPNL.DES\nDataset title (if known): Not provided\nSuggested title: School District Data\n\nThe dataset appears to contain information about various characteristics of school districts, including enrollment, spending, teacher salaries, student performance, and demographic factors. The variables cover a range of topics, such as district and school-level data, financial information, student outcomes, and time-varying indicators.\nPotential research ideas:\n\nExamine the relationship between school district spending (e.g., expenditure per pupil) and student academic performance (e.g., math test scores) over time, controlling for other factors such as enrollment, teacher characteristics, and socioeconomic status.\nInvestigate the impact of school choice programs (e.g., number of choice students) on student outcomes and district-level measures, such as dropout and graduation rates.\nAnalyze the effects of changes in district-level funding (e.g., foundation grants) on resource allocation and student performance, considering potential heterogeneous effects across different types of school districts.\nExplore the relationship between teacher salaries, fringe benefits, and the ability to attract and retain high-quality teachers, and how these factors may influence student achievement.\nAssess the impact of demographic changes (e.g., changes in the percentage of students eligible for free lunch) on school district performance and the challenges faced by districts in adapting to these changes."
  },
  {
    "objectID": "data_resources.html#meap00",
    "href": "data_resources.html#meap00",
    "title": "Data-Set Handbook",
    "section": "MEAP00",
    "text": "MEAP00\nSource: Michigan Department of Education, www.michigan.gov/mde Used in Text: pages 218, 292\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: MEAP00_01.LOG\nDataset title (if known): Not provided\nSuggested title: School District and Building Performance Data\n\nThe dataset contains information about various characteristics of school districts and buildings, including the percentage of students meeting satisfactory performance levels in 4th grade math and reading, the percentage of students eligible for free or reduced-price lunch, school enrollment, and expenditures per pupil. The data also includes the log-transformed values of enrollment and expenditures per pupil.\nPotential research ideas:\n\nInvestigating the relationship between school district and building characteristics, such as the percentage of students eligible for free or reduced-price lunch, and their academic performance in 4th grade math and reading. This could provide insights into the factors that influence student achievement.\nAnalyzing the association between school expenditures per pupil and student performance, both in terms of the raw expenditures per pupil and the log-transformed values. This could help identify the optimal level of funding for improving educational outcomes.\nExploring the differences in performance and characteristics between school districts and buildings of varying enrollment sizes. This could shed light on the challenges and advantages of smaller versus larger educational institutions.\nExamining the spatial distribution of the school districts and buildings, and investigating whether there are any regional or geographic patterns in the data that could influence educational outcomes.\nConducting a longitudinal analysis to understand how the variables in the dataset have changed over time and how these changes may have impacted student performance and educational resource allocation."
  },
  {
    "objectID": "data_resources.html#meap01",
    "href": "data_resources.html#meap01",
    "title": "Data-Set Handbook",
    "section": "MEAP01",
    "text": "MEAP01\nSource: Michigan Department of Education, www.michigan.gov/mde Used in Text: page 16\nNotes: This is another good data set to compare simple and multiple regression estimates. The expenditure variable (in logs, say) and the poverty measure (lunch) are negatively correlated in this data set. A simple regression of math4 on lexppp gives a negative coefficient. Controlling for lunch makes the spending coefficient positive and significant.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: MEAP01.DES\nDataset title (if known): Not provided\nSuggested title: School District and Building-Level Data\n\nThe dataset contains information about school districts and individual school buildings, including the percentage of students who performed satisfactorily on 4th-grade math and reading assessments, the percentage of students eligible for free or reduced-price lunch, school enrollment, total spending, and expenditures per pupil. The data also includes the natural logarithms of enrollment, total spending, and expenditures per pupil.\nPotential research ideas:\n\n\nExamine the relationship between school spending and student academic performance, as measured by the 4th-grade math and reading assessment scores. This could help identify whether increased funding is associated with improved educational outcomes.\nInvestigate the relationship between the percentage of students eligible for free or reduced-price lunch (a proxy for socioeconomic status) and school-level academic performance. This could provide insights into the impact of socioeconomic factors on student achievement.\nAnalyze the differences in school-level characteristics, such as enrollment and expenditures per pupil, between districts or buildings with high and low academic performance. This could help identify factors that contribute to educational disparities.\nExplore the relationship between school enrollment and per-pupil expenditures, as well as the impact of these variables on academic performance. This could inform discussions about optimal school size and resource allocation.\nConduct a longitudinal analysis to examine how changes in school-level variables, such as spending or enrollment, are associated with changes in academic performance over time. This could provide insights into the dynamics of educational improvement."
  },
  {
    "objectID": "data_resources.html#meap93",
    "href": "data_resources.html#meap93",
    "title": "Data-Set Handbook",
    "section": "MEAP93",
    "text": "MEAP93\nSource: I collected these data from the old Michigan Department of Education web site. See MATHPNL for the current web site. I used data on most high schools in the state of Michigan for 1993. I dropped some high schools that had suspicious-looking data. Used in Text: pages 44-45, 63, 110-111, 125-126, 149-150, 158, 212, 325, 329, 329, 660\nNotes: Many states have data, at either the district or building level, on student performance and spending. A good exercise in data collection and cleaning is to have students find such data for a particular state, and to put it into a form that can be used for econometric analysis.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: MEAP93.DES\nDataset title (if known): Not provided\nSuggested title: Michigan Educational Assessment Program (MEAP) 1993 Dataset\n\nThe provided dataset contains information about various characteristics of schools in Michigan, including the percentage of students in the school lunch program, school enrollment, staff per 1000 students, expenditure per student, average teacher salary and benefits, dropout rate, graduation rate, and the percentage of students passing the MEAP (Michigan Educational Assessment Program) math and science tests. The dataset also includes derived variables such as the logarithm of total compensation, expenditure, enrollment, and staff, as well as the ratio of benefits to salary.\nPotential research ideas:\n\nInvestigate the relationship between school expenditure per student and student academic performance, as measured by the MEAP math and science test scores. This could provide insights into the effectiveness of educational spending and help inform resource allocation decisions.\nAnalyze the factors that influence school dropout rates, such as enrollment, staff-to-student ratio, and the percentage of students in the school lunch program. This could help identify potential interventions to improve student retention and graduation rates.\nExamine the relationship between teacher compensation (salary and benefits) and student outcomes, including test scores and graduation rates. This could inform policies related to teacher recruitment, retention, and compensation.\nExplore the impact of school size, as measured by enrollment, on various school-level outcomes, such as expenditure per student, staff-to-student ratio, and academic performance. This could provide insights into the optimal school size for efficient resource utilization and student achievement.\nInvestigate the role of the school lunch program participation rate as a proxy for socioeconomic status and its influence on student academic performance and other school-level outcomes. This could help identify potential equity issues and inform policies aimed at addressing educational disparities."
  },
  {
    "objectID": "data_resources.html#meapsingle",
    "href": "data_resources.html#meapsingle",
    "title": "Data-Set Handbook",
    "section": "MEAPSINGLE",
    "text": "MEAPSINGLE\nSource: Collected by Professor Leslie Papke, an economics professor at MSU, from the Michigan Department of Education web site, www.michigan.gov/mde, and the U.S. Census Bureau. Professor Papke kindly provided the data. Used in Text: 110-111, 158-159, 213\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: meapsingle.des\nDataset title (if known): Not provided\nSuggested title: School District Performance and Socioeconomic Factors\n\nThe dataset contains information about school districts, including performance measures (4th grade math and reading proficiency), enrollment, expenditures per pupil, and the percentage of students eligible for free or reduced-price lunch. It also includes socioeconomic data at the zipcode level, such as median family income, the number of children, and the percentage of children not in married-couple families.\nPotential research ideas:\n\nInvestigate the relationship between school district performance (4th grade math and reading proficiency) and socioeconomic factors, such as median family income and the percentage of children not in married-couple families. This could provide insights into the impact of socioeconomic conditions on educational outcomes.\nAnalyze the relationship between school district expenditures per pupil and student performance. This could help identify whether increased funding is associated with improved academic achievement.\nExplore the differences in performance between schools with high and low percentages of students eligible for free or reduced-price lunch. This could shed light on the challenges faced by schools serving economically disadvantaged populations.\nInvestigate the role of school enrollment size on student performance. This could help determine whether smaller or larger school districts are more effective in promoting academic success.\nAnalyze the geographic distribution of school district performance and socioeconomic factors. This could reveal spatial patterns and identify areas that may require targeted interventions or additional resources."
  },
  {
    "objectID": "data_resources.html#minwage",
    "href": "data_resources.html#minwage",
    "title": "Data-Set Handbook",
    "section": "MINWAGE",
    "text": "MINWAGE\nSource: P. Wolfson and D. Belman (2004), “The Minimum Wage: Consequences for Prices and Quantities in Low-Wage Labor Markets,” Journal of Business & Economic Statistics 22, 296-311. Professor Belman kindly provided the data. Used in Text: pages 365, 393, 424, 641\nNotes: The sectors corresponding to the different numbers in the data file are provided in the Wolfson and Bellman and article.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: MINWAGE.DES\nDataset title (if known): Not provided\nSuggested title: Minimum Wage and Labor Market Data\n\nThe dataset appears to contain information on employment, wages, and other labor market variables, as well as the federal minimum wage and consumer price index. The variables include employment and hourly wages for various sectors, the civilian unemployment rate, the consumer price index, and the federal minimum wage. The dataset also includes several derived variables, such as logarithms and growth rates of the key variables.\nPotential research ideas:\n\nExamine the relationship between changes in the federal minimum wage and employment levels in different sectors. This could provide insights into the potential impacts of minimum wage policies on labor markets.\nInvestigate the relationship between changes in the consumer price index and changes in the federal minimum wage. This could help understand the potential inflationary effects of minimum wage increases.\nAnalyze the dynamics of employment and wage growth in different sectors, and how these dynamics may be influenced by changes in the minimum wage or other macroeconomic factors.\nExplore the long-term trends in the labor market variables, such as employment, wages, and unemployment, and how these trends may be related to changes in the minimum wage or other economic conditions.\nConduct a time series analysis to understand the short-term and long-term effects of minimum wage changes on various labor market outcomes, such as employment, wages, and unemployment."
  },
  {
    "objectID": "data_resources.html#mlb1",
    "href": "data_resources.html#mlb1",
    "title": "Data-Set Handbook",
    "section": "MLB1",
    "text": "MLB1\nSource: Collected by G. Mark Holmes, a former MSU undergraduate, for a term project. The salary data were obtained from the New York Times, April 11, 1993. The baseball statistics are from The Baseball Encyclopedia, 9th edition, and the city population figures are from the Statistical Abstract of the United States. Used in Text: pages 140-143, 160, 229, 235-236, 256-257\nNotes: The baseball statistics are career statistics through the 1992 season. Players whose race or ethnicity could not be easily determined were not included. It should not be too difficult to obtain the city population and racial composition numbers for Montreal and Toronto for 1993. Of course, the data can be pretty easily obtained for more recent players.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: MLB1.DES\nDataset title (if known): Not provided\nSuggested title: Major League Baseball Player Salaries and Characteristics\n\nThe dataset contains information on 353 Major League Baseball (MLB) players, including their 1993 season salary, team payroll, league affiliation, career statistics, and demographic characteristics of the cities they play in. The variables cover a wide range of player attributes, such as years in the league, games played, batting statistics, fielding positions, all-star selections, and racial/ethnic background. The dataset also includes information on the population, income, and racial/ethnic composition of the cities where the players’ teams are located.\nPotential research ideas:\n\n\nExamine the relationship between player performance statistics (e.g., batting average, home runs, RBIs) and their salaries. This could provide insights into how teams value different player attributes when determining compensation.\nInvestigate the impact of a player’s racial/ethnic background on their salary, controlling for performance and other relevant factors. This could shed light on potential biases or discrimination in the MLB player compensation system.\nAnalyze the relationship between the demographic characteristics of a player’s city (e.g., racial/ethnic composition, per capita income) and their salary or performance. This could reveal how local market factors influence player compensation and team-building strategies.\nExplore the differences in performance and salary between players in the National League and American League, as indicated by the “nl” variable. This could help understand the impact of league-specific rules and strategies on player development and compensation.\nInvestigate the factors that predict a player’s likelihood of being selected as an All-Star, such as their performance statistics, years in the league, and demographic characteristics. This could provide insights into the selection process and the criteria used by coaches and fans."
  },
  {
    "objectID": "data_resources.html#mroz",
    "href": "data_resources.html#mroz",
    "title": "Data-Set Handbook",
    "section": "MROZ",
    "text": "MROZ\nSource: T.A. Mroz (1987), “The Sensitivity of an Empirical Model of Married Women’s Hours of Work to Economic and Statistical Assumptions,” Econometrica 55, 765-799. Professor Ernst R. Berndt, of MIT, kindly provided the data, which he obtained from Professor Mroz. Used in Text: pages 240, 253, 285, 501, 511, 516, 518, 543-544, 555, 568-569, 575-576, 591-592, 597\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: MROZ.DES\nDataset title (if known): Not provided\nSuggested title: Women’s Labor Force Participation and Wages\n\nThe dataset contains information on 753 women, including their labor force participation, hours worked, number of children, age, education, wages, and other household and demographic characteristics. The variables cover various aspects of the women’s labor market experience, such as their employment status, wages, and work experience, as well as information about their families, including their husband’s characteristics and the family’s income.\nPotential research ideas:\n\nExamine the relationship between a woman’s labor force participation and the number of children in the household, particularly the impact of having young children (under 6 years old) versus older children (6-18 years old).\nInvestigate the factors that influence a woman’s reported wage, such as her education, work experience, and the presence of young children, and compare these to the estimated wage based on her earnings and hours worked.\nAnalyze the impact of the husband’s characteristics, such as his age, education, and wage, on the woman’s labor force participation and wages.\nExplore the role of the family’s income and the woman’s marginal tax rate in her decision to participate in the labor force and the number of hours she works.\nInvestigate the differences in labor market outcomes, such as wages and work experience, between women living in urban (SMSA) and non-urban areas."
  },
  {
    "objectID": "data_resources.html#murder",
    "href": "data_resources.html#murder",
    "title": "Data-Set Handbook",
    "section": "MURDER",
    "text": "MURDER\nSource: From the Statistical Abstract of the United States, 1995 (Tables 310 and 357), 1992 (Table 289). The execution data originally come from the U.S. Bureau of Justice Statistics, Capital Punishment Annual. Used in Text: pages 457, 487, 527-528\nNotes: The data set COUNTYMURDERS includes information on executions and murder rates at the county level, and provides more variation.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: MURDER.DES\nDataset title (if known): Not provided\nSuggested title: State-Level Murder and Execution Data\n\nThe dataset contains information on murder rates, executions, and unemployment rates for U.S. states over three years (1987, 1990, and 1993). The variables include the state identifier, the murder rate per 100,000 population, the total number of executions in the past three years, the annual unemployment rate, and several derived variables such as the change in murder rate, change in executions, and change in unemployment from the previous year.\nPotential research ideas:\n\nExamine the relationship between the murder rate and the number of executions in a state. This could investigate whether capital punishment has a deterrent effect on murder rates.\nAnalyze the impact of changes in unemployment rates on changes in murder rates. This could provide insights into the socioeconomic factors that influence crime rates.\nInvestigate whether the relationship between murder rates and executions or unemployment varies across different years or regions. This could help identify any temporal or geographic patterns in the data.\nExplore the factors that contribute to changes in murder rates over time, such as changes in economic conditions, criminal justice policies, or demographic characteristics.\nDevelop a predictive model to forecast future murder rates based on the available variables, which could inform policy decisions and resource allocation for crime prevention and law enforcement."
  },
  {
    "objectID": "data_resources.html#nbasal",
    "href": "data_resources.html#nbasal",
    "title": "Data-Set Handbook",
    "section": "NBASAL",
    "text": "NBASAL\nSource: Collected by Christopher Torrente, a former MSU undergraduate, for a term project. He obtained the salary data and the career statistics from The Complete Handbook of Pro Basketball, 1995, edited by Zander Hollander. New York: Signet. The demographic information (marital status, number of children, and so on) was obtained from the teams’ 1994-1995 media guides. Used in Text: pages 216-217, 258\nNotes: A panel version of this data set could be useful for further isolating productivity effects of marital status. One would need to obtain information on enough different players in at least two years, where some players who were not married in the initial year are married in later years. Fixed effects (or first differencing, for two years) is the natural estimation method.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: NBASAL.DES\nDataset title (if known): Not provided\nSuggested title: NBA Player Salaries and Performance\n\nThe dataset contains information on 269 professional basketball players, including their marital status, annual salary, years of experience, age, college education, games played, minutes played, position (guard, forward, or center), points scored, rebounds, assists, draft number, all-star selection, and other derived variables such as log of salary, race, and whether the player has children. The data appears to be focused on understanding the factors that influence the salaries and performance of NBA players.\nPotential research ideas:\n\nExamine the relationship between player characteristics (e.g., age, experience, position) and their annual salary. This could help identify the key factors that determine player compensation in the NBA.\nInvestigate the impact of player performance metrics (e.g., points, rebounds, assists) on their salaries. This could provide insights into how teams value different aspects of player contributions.\nAnalyze the differences in salaries and performance between players of different races (black vs. non-black) and marital status (married vs. unmarried). This could shed light on potential biases or disparities in the NBA labor market.\nExplore the relationship between a player’s draft position and their subsequent performance and salary. This could help assess the accuracy of the NBA draft process in identifying and developing talented players.\nInvestigate the impact of player experience (measured by years in the league and minutes played) on their performance and salary. This could provide insights into the career trajectories of NBA players and the value of experience."
  },
  {
    "objectID": "data_resources.html#ncaa_rpi",
    "href": "data_resources.html#ncaa_rpi",
    "title": "Data-Set Handbook",
    "section": "NCAA_RPI",
    "text": "NCAA_RPI\nSource: Data on NCAA men’s basketball teams, collected by Weizhao Sun for a senior seminar project in sports economics at Michigan State University, Spring 2017. He used various sources, including www.espn.com and www.teamrankings.com/ncaa-basketball/rpi-ranking/rpi-rating-by-team. Used in Text: not used\nNotes: This is a nice example of how multiple regression analysis can be used to determine whether rankings compiled by experts – the so-called pre-season RPI in this case – provide additional information beyond what we can obtain from widely available data bases. A simple and interesting question is whether, once the previous year’s post-season RPI is controlled for, does the pre-season RPI – which is supposed to add information on recruiting and player development – help to predict performance (such as win percentage or making it to the NCAA men’s basketball tournament). For the binary outcome that indicates making it to the NCAA tournament, a probit or logit model can be used for courses that introduce more advanced methods. There are some other interesting variables, such as coaching experience, that can be included, too.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: NCAA_RPI.DES\nDataset title (if known): Not provided\nSuggested title: NCAA Basketball Team Performance and Characteristics\n\nThe dataset contains information about NCAA basketball teams, including their team name, year, conference, post-season RPI (Ratings Percentage Index) ranking, pre-season RPI ranking, RPI rankings from the previous two years, recruiting rank, number of wins and losses, winning percentage, whether the team made the NCAA tournament, the coach’s experience, and whether the team is from a Power 5 conference.\nPotential research ideas:\n\nInvestigate the relationship between a team’s pre-season RPI ranking and their post-season RPI ranking. This could provide insights into the accuracy of pre-season predictions and the factors that influence a team’s performance throughout the season.\nAnalyze the impact of a team’s recruiting rank on their regular-season performance and tournament success. This could help identify the importance of recruiting high-quality players and the factors that contribute to a team’s overall competitiveness.\nExamine the differences in performance and characteristics between teams from Power 5 conferences and those from other conferences. This could shed light on the competitive landscape of NCAA basketball and the advantages or disadvantages of playing in a Power 5 conference.\nExplore the relationship between a coach’s experience and their team’s performance, both in the regular season and in the NCAA tournament. This could provide insights into the value of experienced coaching and the factors that contribute to a successful coaching career.\nInvestigate the factors that contribute to a team’s winning percentage, such as RPI rankings, recruiting, and conference affiliation. This could help identify the key drivers of team success and inform strategies for improving a team’s overall performance."
  },
  {
    "objectID": "data_resources.html#nyse",
    "href": "data_resources.html#nyse",
    "title": "Data-Set Handbook",
    "section": "NYSE",
    "text": "NYSE\nSource: These are Wednesday closing prices of value-weighted NYSE average, available in many publications. I do not recall the particular source I used when I collected these data at MIT. Probably the easiest way to get similar data is to go to the NYSE web site, www.nyse.com. Used in Text: pages 352-353, 368, 393, 398, 399, 595\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: NYSE.DES\nDataset title (if known): Not provided\nSuggested title: NYSE Stock Price Index and Related Variables\n\nThe provided dataset contains information about the NYSE stock price index and related variables. The dataset includes the following variables: price (NYSE stock price index), return (100*(p - p(-1])/p(-1))), return_1 (lagged return), t (time trend from 1 to 691), price_1 (price(-1)), price_2 (price(-2)), cprice (price - price_1), and cprice_1 (cprice(-1)). The dataset has 691 observations.\nPotential research ideas:\n\nAnalyzing the relationship between the NYSE stock price index and its lagged values: Researchers could investigate the dynamics of the NYSE stock price index by examining the relationship between the current price and its lagged values. This could provide insights into the predictability and persistence of stock price movements.\nExploring the determinants of stock returns: Researchers could analyze the factors that influence the returns of the NYSE stock price index, such as the relationship between the current and lagged returns, as well as the impact of other variables like the time trend.\nInvestigating the volatility of stock prices: Researchers could study the volatility of the NYSE stock price index by analyzing the changes in the current price compared to the previous price (cprice and cprice_1). This could help understand the dynamics of stock price fluctuations.\nExamining the relationship between stock prices and macroeconomic factors: Researchers could explore the potential links between the NYSE stock price index and broader macroeconomic variables, such as GDP, inflation, or interest rates, to understand the broader economic factors that may influence stock market performance.\nDeveloping predictive models for stock prices: Researchers could use the available variables to develop predictive models for the NYSE stock price index, aiming to improve the understanding and forecasting of stock market movements."
  },
  {
    "objectID": "data_resources.html#okun",
    "href": "data_resources.html#okun",
    "title": "Data-Set Handbook",
    "section": "OKUN",
    "text": "OKUN\nSource: Economic Report of the President, 2007, Tables B4 and B42. Used in Text: 392, 423-424\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: OKUN.DES\nDataset title (if known): Not provided\nSuggested title: Okun’s Law Dataset\n\nThe provided dataset contains information on the percentage change in real GDP (pcrgdp), the civilian unemployment rate (unem), and the change in the civilian unemployment rate (cunem) from 1959 to 2005, covering a total of 47 observations.\nPotential research ideas:\n\nInvestigating the relationship between economic growth (as measured by the percentage change in real GDP) and changes in the unemployment rate, as described by Okun’s law. This could involve estimating the Okun coefficient and analyzing its stability over time.\nExamining the predictive power of the change in the unemployment rate (cunem) for future economic growth (pcrgdp). This could provide insights into the usefulness of the unemployment rate as an indicator of economic performance.\nAnalyzing the impact of macroeconomic shocks or policy changes on the relationship between economic growth and unemployment. This could help understand the dynamics of the labor market and its response to various economic conditions.\nInvestigating the potential asymmetries in the Okun’s law relationship, i.e., whether the relationship between economic growth and unemployment differs during periods of expansion and contraction.\nComparing the performance of different empirical specifications of Okun’s law, such as the difference version, the gap version, or the dynamic version, to determine the most appropriate model for the given dataset."
  },
  {
    "objectID": "data_resources.html#openness",
    "href": "data_resources.html#openness",
    "title": "Data-Set Handbook",
    "section": "OPENNESS",
    "text": "OPENNESS\nSource: D. Romer (1993), “Openness and Inflation: Theory and Evidence,” Quarterly Journal of Economics 108, 869-903. The data are included in the article. Used in Text: pages 544-545, 555\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: OPENNESS.DES\nDataset title (if known): Not provided\nSuggested title: Openness and Economic Indicators\n\nThe dataset appears to contain information on various economic indicators for 114 observations. The variables include measures of trade openness (open, lopen, opendec), inflation (inf, linf, linfdec), per capita income (pcinc, lpcinc), land area (land, lland), and a binary indicator for major oil producers (oil). The dataset also includes a binary indicator for data quality (good).\nPotential research ideas:\n\nRelationship between trade openness and economic growth: Investigate the impact of trade openness (measured by open, lopen, or opendec) on per capita income (pcinc or lpcinc). This could provide insights into the potential benefits or drawbacks of trade liberalization policies.\nInflation dynamics and economic performance: Analyze the relationship between inflation (inf, linf, or linfdec) and other economic indicators, such as per capita income (pcinc or lpcinc) or trade openness (open, lopen, or opendec). This could help understand the role of monetary policy and its impact on economic outcomes.\nThe role of natural resources in economic development: Explore the differences in economic performance between major oil producers (oil = 1) and non-oil producers. This could shed light on the potential resource curse or blessing associated with natural resource abundance.\nDeterminants of trade openness: Investigate the factors that influence a country’s level of trade openness, such as the relationship between land area (land or lland) and trade openness (open, lopen, or opendec). This could provide insights into the geographical and structural characteristics that shape a country’s integration into the global economy.\n\ne: Data quality and economic analysis: Examine the differences in the relationships between the variables when considering the data quality indicator (good). This could help assess the reliability and robustness of the findings, as well as the importance of data quality in economic research."
  },
  {
    "objectID": "data_resources.html#pension",
    "href": "data_resources.html#pension",
    "title": "Data-Set Handbook",
    "section": "PENSION",
    "text": "PENSION\nSource: L.E. Papke (2004), “Individual Financial Decisions in Retirement Saving: The Role of Participant-Direction,” Journal of Public Economics 88, 39-61. Professor Papke kindly provided the data. She collected them from the National Longitudinal Survey of Mature Women, 1991. Used in Text: page 488\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: PENSION.DES\nDataset title (if known): Not provided\nSuggested title: Pension Plan Participation and Investment Choices\n\nThe dataset appears to contain information about individuals’ participation in pension plans, their investment choices, and various demographic and financial characteristics. The variables include family identifier, years in pension plan, profit-sharing plan participation, ability to choose investment method, gender, marital status, age, education level, family income categories, net worth, race, stock ownership, and IRA ownership.\nPotential research ideas:\n\nExamine the relationship between demographic characteristics (e.g., gender, age, education) and pension plan participation or investment choices. This could provide insights into factors that influence individuals’ retirement planning decisions.\nInvestigate the impact of financial factors, such as family income and net worth, on pension plan participation and investment preferences. This could help understand how economic status affects retirement savings and investment strategies.\nAnalyze the differences in pension plan participation and investment choices between individuals with and without access to profit-sharing plans or the ability to choose their investment method. This could shed light on the role of plan design in retirement savings behavior.\nExplore the association between stock ownership, IRA ownership, and pension plan investment choices. This could reveal how individuals integrate various retirement savings vehicles into their overall financial planning.\nInvestigate the relationship between race and pension plan participation or investment choices, considering potential socioeconomic and cultural factors that may influence retirement planning decisions."
  },
  {
    "objectID": "data_resources.html#phillips",
    "href": "data_resources.html#phillips",
    "title": "Data-Set Handbook",
    "section": "PHILLIPS",
    "text": "PHILLIPS\nSource: Economic Report of the President, 2004, Tables B42 and B64. Used in Text: pages 344-345, 364-365, 375, 390-391, 392, 392, 403, 404, 412, 423, 528, 613, 625, 628, 639, 770\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: PHILLIPS.DES\nDataset title (if known): Not provided\nSuggested title: Phillips Curve Dataset\n\nThe provided dataset contains information on the civilian unemployment rate (unem), the percentage change in the Consumer Price Index (inf), and the lagged values of these variables (inf_1 and unem_1) for the years 1948 through 2003. The dataset also includes two derived variables, cinf (the change in inflation) and cunem (the change in unemployment), which may be used to explore the relationship between inflation and unemployment, commonly known as the Phillips curve.\nPotential research ideas:\nInvestigating the relationship between inflation and unemployment: Using the provided variables, researchers could explore the validity of the Phillips curve hypothesis, which suggests an inverse relationship between these two economic indicators. This analysis could involve regression models, time series analysis, or other appropriate statistical techniques.\nAnalyzing the dynamics of inflation and unemployment: The dataset allows for the examination of how changes in inflation (cinf) and unemployment (cunem) are related over time. Researchers could investigate the lead-lag relationships between these variables and explore the potential for policy implications.\nExploring the impact of lagged variables: The inclusion of lagged values for inflation (inf_1) and unemployment (unem_1) provides an opportunity to study the role of past economic conditions in shaping current outcomes. Researchers could investigate the predictive power of these lagged variables and their influence on the Phillips curve relationship.\nComparing the Phillips curve across different time periods: Given the long time span covered by the dataset (1948-2003), researchers could analyze how the relationship between inflation and unemployment has evolved over time. This could involve identifying structural breaks, regime shifts, or other changes in the underlying dynamics.\nExtending the analysis to other economic variables: Depending on the research question, the dataset could be supplemented with additional economic indicators, such as GDP growth, interest rates, or other relevant factors. This could lead to a more comprehensive understanding of the complex interactions between inflation, unemployment, and other macroeconomic variables."
  },
  {
    "objectID": "data_resources.html#pntsprd",
    "href": "data_resources.html#pntsprd",
    "title": "Data-Set Handbook",
    "section": "PNTSPRD",
    "text": "PNTSPRD\nSource: Collected by Scott Resnick, a former MSU undergraduate, from various newspaper sources. Used in Text: pages 271, 560, 623\nNotes: The data are for the 1994-1995 men’s college basketball seasons. The spread is for the day before the game was played. One might collect more recent data and determine whether the spread has become a less accurate predictor of the actual outcome in more recent years. In other words, in the simple regression of the actual score differential on the spread, is the variance larger in more recent years. (We should fully expect the slope coefficient not to be statistically different from one.)\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: PNTSPRD.DES\nDataset title (if known): Not provided\nSuggested title: College Football Point Spread Data\n\nThe dataset appears to contain information about college football games, including the scores of the favored and underdog teams, the Las Vegas point spread, and various indicators related to the teams’ rankings and home/neutral site status. The dataset has 553 observations, and the variables include the scores of the favored and underdog teams, the point spread, whether the favored team was playing at home or on a neutral site, whether the teams were ranked in the top 25, the regions of the country the teams are from, the difference in scores, whether the spread was covered, and whether the favored team won.\nPotential research ideas:\n\nExamine the relationship between team rankings (top 25) and the ability to cover the point spread. This could provide insights into the accuracy of oddsmakers’ predictions and the potential advantages of highly ranked teams.\nInvestigate the impact of home-field advantage on the ability to cover the point spread. This could help understand the importance of playing at home in college football and how it affects the outcome of games.\nAnalyze the differences in point spread coverage between teams from different regions of the country. This could reveal regional biases or variations in the strength of college football programs across different parts of the United States.\nExplore the factors that contribute to the difference in scores between the favored and underdog teams. This could help identify the key determinants of game outcomes and the ability of teams to outperform or underperform expectations.\nDevelop a predictive model to forecast the likelihood of the favored team winning and covering the point spread. This could have practical applications for sports betting and provide insights into the factors that influence the success of college football teams."
  },
  {
    "objectID": "data_resources.html#prison",
    "href": "data_resources.html#prison",
    "title": "Data-Set Handbook",
    "section": "PRISON",
    "text": "PRISON\nSource: S.D. Levitt (1996), “The Effect of Prison Population Size on Crime Rates: Evidence from Prison Overcrowding Legislation,” Quarterly Journal of Economics 111, 319-351. Professor Levitt kindly provided me with the data, of which I used a subset. Used in Text: pages 551\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: PRISON.DES\nDataset title (if known): Not provided\nSuggested title: State-Level Prison and Crime Data\n\nThe dataset appears to contain information on various state-level characteristics, including demographics, crime rates, and prison populations, over the years 1980 to 1993. The variables include measures of the proportion of the population that is black, the proportion living in metropolitan areas, the unemployment rate, violent and property crime rates, changes in these rates, the proportion of the population in different age groups, per capita income, police per capita, and prison population per capita. The dataset also includes indicators for gubernatorial election years and the timing of final court decisions on litigation related to prisons.\nPotential research ideas:\n\n\nExamine the relationship between changes in demographic factors (e.g., age distribution, racial composition, urbanization) and changes in crime rates over time. This could provide insights into the drivers of crime and how they vary across states.\nInvestigate the impact of gubernatorial elections on criminal justice policies and outcomes, such as changes in police funding, sentencing practices, or prison populations.\nAnalyze the effects of court decisions related to prisons on subsequent changes in prison populations and crime rates. This could shed light on the role of legal interventions in shaping the criminal justice system.\nExplore the relationship between economic conditions, as measured by per capita income and unemployment, and crime rates or prison populations. This could inform policies aimed at addressing the socioeconomic factors underlying criminal behavior.\nAssess the extent to which changes in police resources, as measured by police per capita, are associated with changes in crime rates or prison populations. This could inform discussions on the role of law enforcement in crime prevention and control."
  },
  {
    "objectID": "data_resources.html#prminwge",
    "href": "data_resources.html#prminwge",
    "title": "Data-Set Handbook",
    "section": "PRMINWGE",
    "text": "PRMINWGE\nSource: A.J. Castillo-Freeman and R.B. Freeman (1992), “When the Minimum Wage Really Bites: The Effect of the U.S.-Level Minimum Wage on Puerto Rico,” in Immigration and the Work Force, edited by G.J. Borjas and R.B. Freeman, 177-211. Chicago: University of Chicago Press. The data are reported in the article. Used in Text: pages 345-346, 356-357, 400, 405\nNotes: Given the ongoing debate on the employment effects of the minimum wage, this would be a great data set to try to update. The coverage rates are the most difficult variables to construct."
  },
  {
    "objectID": "data_resources.html#recid",
    "href": "data_resources.html#recid",
    "title": "Data-Set Handbook",
    "section": "RECID",
    "text": "RECID\nSource: C.-F. Chung, P. Schmidt, and A.D. Witte (1991), “Survival Analysis: A Survey,” Journal of Quantitative Criminology 7, 59-98. Professor Chung kindly provided the data. Used in Text: pages 584-585, 597\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: RECID.DES\nDataset title (if known): Not provided\nSuggested title: Recidivism Data\n\nThe dataset appears to contain information about individuals who have been incarcerated, including their demographic characteristics, criminal history, and post-release outcomes. The variables cover various aspects such as race, substance abuse, supervision during release, marital status, criminal history, education, and the duration of their incarceration and follow-up period.\nPotential research ideas:\n\nExamining the relationship between individual characteristics (e.g., race, substance abuse, education) and the likelihood of recidivism. This could help identify risk factors and inform targeted interventions.\nInvestigating the impact of prison work programs on post-release outcomes, such as employment, recidivism, and successful reintegration into the community.\nAnalyzing the influence of the length of incarceration and the duration of the follow-up period on recidivism rates. This could provide insights into the optimal length of incarceration and the importance of post-release support.\nExploring the role of supervision during release (e.g., parole, probation) in reducing the risk of recidivism. This could inform policies and practices related to community-based supervision.\nStudying the relationship between the number of prior convictions, rule violations during incarceration, and the likelihood of recidivism. This could help identify patterns and inform interventions targeting repeat offenders."
  },
  {
    "objectID": "data_resources.html#rdchem",
    "href": "data_resources.html#rdchem",
    "title": "Data-Set Handbook",
    "section": "RDCHEM",
    "text": "RDCHEM\nSource: From Businessweek R&D Scoreboard, October 25, 1991. Used in Text: pages 63, 135-136, 154-155, 198, 211, 317-318, 328-329\nNotes: It would be interesting to collect more recent data and see whether the R&D/firm size relationship has changed over time.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: RDCHEM.DES\nDataset title (if known): Not provided\nSuggested title: R&D and Financial Performance in the Chemical Industry\n\nThe dataset appears to contain information on the research and development (R&D) spending, sales, profits, and related financial metrics for a sample of firms in the chemical industry. The variables include R&D spending, sales, profits, R&D intensity (R&D as a percentage of sales), profit margin (profits as a percentage of sales), the square of sales, and the natural logarithms of sales and R&D spending.\nPotential research ideas:\n\nExamining the relationship between R&D spending and firm performance: This study could investigate the impact of R&D investment on sales, profits, and other financial metrics, providing insights into the potential returns to R&D in the chemical industry.\nAnalyzing the determinants of R&D intensity: This research could explore the factors that influence the level of R&D spending relative to sales, such as firm size, market competition, or industry characteristics.\nInvestigating the role of profit margins in R&D investment: This study could examine how profit margins affect a firm’s ability to fund R&D activities and the potential feedback effects between profitability and R&D spending.\nExploring the nonlinear relationship between R&D and firm performance: This research could assess whether there are diminishing or increasing returns to R&D investment, and identify the optimal level of R&D spending for maximizing firm performance.\nComparing the financial performance of high-R&D and low-R&D firms: This study could compare the sales, profits, and other financial metrics between firms with different levels of R&D intensity, providing insights into the potential benefits and risks associated with R&D investment in the chemical industry."
  },
  {
    "objectID": "data_resources.html#rdtelec",
    "href": "data_resources.html#rdtelec",
    "title": "Data-Set Handbook",
    "section": "RDTELEC",
    "text": "RDTELEC\nSource: See RDCHEM Used in Text: not used\nNotes: According to these data, the R&D/firm size relationship is different in the telecommunications industry than in the chemical industry: there is pretty strong evidence that R&D intensity decreases with firm size in telecommunications. Of course, that was in 1991. The data could easily be updated, and a panel data set could be constructed for more advanced courses.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: RDTELEC.DES\nDataset title (if known): Not provided\nSuggested title: R&D and Sales Data for Firms in the Electronics Industry\n\nThe dataset contains information on 29 firms in the electronics industry, including their R&D spending (in millions of dollars), sales (in millions of dollars), R&D intensity (R&D as a percentage of sales), and the natural logarithms of R&D and sales. The dataset also includes the square of sales.\nPotential research ideas:\n\nInvestigate the relationship between R&D spending and firm sales. This could involve analyzing the correlation between the two variables, as well as exploring the potential for a non-linear relationship using the sales squared variable.\nExamine the determinants of R&D intensity in the electronics industry. This could include analyzing the impact of firm size (as measured by sales) and other firm-level characteristics on the R&D-to-sales ratio.\nExplore the role of R&D in driving productivity and growth in the electronics industry. This could involve using the log-transformed variables to estimate a production function and assess the contribution of R&D to firm output.\nAnalyze the differences in R&D and sales patterns between larger and smaller firms in the electronics industry. This could involve segmenting the sample based on firm size and comparing the relationships between the variables across the different groups.\nInvestigate the potential for scale economies in the electronics industry by examining the relationship between firm sales and the square of sales. This could provide insights into the optimal size of firms in the industry."
  },
  {
    "objectID": "data_resources.html#rental",
    "href": "data_resources.html#rental",
    "title": "Data-Set Handbook",
    "section": "RENTAL",
    "text": "RENTAL\nSource: David Harvey, a former MSU undergraduate, collected the data for 64 “college towns” from the 1980 and 1990 United States censuses.\nUsed in Text: pages 155, 444, 454-455, 486\nNotes: These data can be used in a somewhat crude simultaneous equations analysis, either focusing on one year or pooling the two years. (In the latter case, in an advanced class, you might have students compute the standard errors robust to serial correlation across the two time periods.) The demand equation would have ltothsg as a function of lrent, lavginc, and lpop. The supply equation would have ltothsg as a function of lrent, pctst, and lpop. Thus, in estimating the demand function, pctstu is used as an IV for lrent. Clearly one can quibble with excluding pctstu from the demand equation, but the estimated demand function gives a negative price effect. Getting information for 2000 and 2010, and adding many more college towns, would make for a much better analysis. Information on number of spaces in on-campus dormitories would be a big improvement, too.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: RENTAL.DES\nDataset title (if known): Not provided\nSuggested title: City-Level Rental and Demographic Data\n\nThe dataset contains information on various characteristics of cities, including population, college student enrollment, average rent, housing units, and per capita income. The data spans two time periods, 1980 and 1990, and includes both the original values and the changes in these variables over the 10-year period. The dataset appears to be focused on understanding the rental housing market and its relationship with demographic and economic factors in different cities.\nPotential research ideas:\n\nExamining the relationship between college student enrollment and rental housing market characteristics: This could involve analyzing how changes in student enrollment over time are associated with changes in average rent, renter-occupied units, and other housing market indicators.\nInvestigating the impact of population growth on rental housing affordability: Researchers could explore how changes in city population are related to changes in average rent and the availability of affordable rental housing.\nAnalyzing the role of per capita income in shaping rental housing demand and supply: This could involve studying how changes in per capita income are associated with changes in rental prices, the number of renter-occupied units, and the overall housing market.\nExploring the spatial distribution of rental housing and its relationship with demographic and economic factors: Researchers could investigate how the characteristics of rental housing vary across different cities and how these variations are linked to factors such as population, student enrollment, and income.\nAssessing the impact of economic and demographic changes on the rental housing market: Researchers could use the data to develop models that predict how changes in factors like population, income, and student enrollment might affect the rental housing market in the future."
  },
  {
    "objectID": "data_resources.html#return",
    "href": "data_resources.html#return",
    "title": "Data-Set Handbook",
    "section": "RETURN",
    "text": "RETURN\nSource: Collected by Stephanie Balys, a former MSU undergraduate, from the New York Stock Exchange and Compustat.\nUsed in Text: page 157\nNotes: More can be done with this data set. Recently, I discovered that lsp90 does appear to predict return (and the log of the 1990 stock price works better than sp90). I am a little suspicious, but you could use the negative coefficient on lsp90 to illustrate “reversion to the mean.”\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: RETURN.DES\nDataset title (if known): Not provided\nSuggested title: CEO Compensation and Firm Performance\n\nThe dataset contains information on various financial and performance metrics for a sample of 142 companies. The variables include measures of profitability (return on equity, return on capital), leverage (debt/capital), earnings (earnings per share, net income), stock prices (at the end of 1990 and 1994), CEO salary, and the percentage change in stock price from 1990 to 1994. The data appears to be focused on the relationship between CEO compensation and firm performance.\nPotential research ideas:\nExamine the relationship between CEO compensation (salary) and firm profitability (return on equity, return on capital). This could provide insights into the alignment between executive pay and company performance.\nInvestigate the impact of leverage (debt/capital) on firm performance and CEO compensation. This could shed light on how capital structure decisions influence both financial outcomes and executive incentives.\nAnalyze the association between earnings measures (earnings per share, net income) and stock price performance (1990-1994 returns). This could help understand the market’s valuation of different financial metrics.\nExplore the factors that influence the change in stock price over the 1990-1994 period, such as the relationship between CEO salary, firm profitability, and stock market returns.\nConduct a comparative analysis of the determinants of CEO compensation across different industries or firm characteristics within the sample. This could identify any systematic differences in the pay-performance relationship."
  },
  {
    "objectID": "data_resources.html#saving",
    "href": "data_resources.html#saving",
    "title": "Data-Set Handbook",
    "section": "SAVING",
    "text": "SAVING\nSource: Unknown Used in Text: not used\nNotes: I remember entering this data set in the late 1980s, and I am pretty sure it came directly from an introductory econometrics text. But so far my search has been fruitless. If anyone runs across this data set, I would appreciate knowing about it.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: SAVING.DES\nDataset title (if known): Not provided\nSuggested title: Household Savings and Consumption Data\n\nThe dataset contains information on 100 households, including their annual savings, income, family size, education level of the household head, age of the household head, and whether the household head is black. The data also includes the annual consumption of the households.\nPotential research ideas:\nInvestigate the relationship between household savings and income, controlling for factors such as family size, education, and age of the household head. This could provide insights into the determinants of household savings behavior.\nAnalyze the differences in savings and consumption patterns between households with black and non-black household heads. This could shed light on potential socioeconomic disparities and the factors that contribute to them.\nExplore the impact of household size and the education level of the household head on savings and consumption decisions. This could help understand the role of family structure and human capital in shaping household financial behavior.\nExamine the relationship between the age of the household head and savings and consumption patterns. This could provide insights into the life-cycle hypothesis and how it applies to this particular population.\nDevelop a predictive model to estimate household savings or consumption based on the available variables. This could be useful for financial planning and policy-making purposes."
  },
  {
    "objectID": "data_resources.html#school93_98",
    "href": "data_resources.html#school93_98",
    "title": "Data-Set Handbook",
    "section": "SCHOOL93_98",
    "text": "SCHOOL93_98\nSource: L.E. Papke (2005), “The Effects of Spending on Test Pass Rates: Evidence from Michigan,” Journal of Public Economics 89, 821-839. Used in Text: page 491\nNotes: This is closer to the data actually used in the Papke paper as it is at the school (building) level. It is unbalanced because data on scores and some of the spending and other variables is missing for some schools. While the usual RE and FE methods can be applied directly, obtaining the correlated random effects version of the Hausman test is more advance. Computer Exercise 17 in Chapter 14 walks the reader through it.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: SCHOOL93_98.DES\nDataset title (if known): Not provided\nSuggested title: School District Data from 1993 to 1998\n\nThe dataset contains information about various characteristics of school districts over the years 1993 to 1998. The variables include the district ID, school ID, percentage of students eligible for free lunch, enrollment, expenditure per pupil, math test scores, and several variables indicating the school year. The dataset also includes derived variables such as the real expenditure per pupil, the log of enrollment, the log of real expenditure per pupil, and the log of the average real expenditure per pupil.\nPotential research ideas:\n\nExamine the relationship between school district characteristics (e.g., enrollment, expenditure per pupil, percentage of students eligible for free lunch) and student academic performance (e.g., math test scores) over the study period.\nInvestigate the trends in real expenditure per pupil across school districts and explore the factors that may have contributed to changes in spending over time.\nAnalyze the impact of school district size (as measured by enrollment) on educational outcomes and resource allocation decisions.\nExplore the relationship between the percentage of students eligible for free lunch and other school district characteristics, such as expenditure per pupil and academic performance.\nConduct a longitudinal analysis to understand how changes in school district characteristics (e.g., enrollment, expenditure per pupil) are associated with changes in student academic performance over the study period."
  },
  {
    "objectID": "data_resources.html#sleep75",
    "href": "data_resources.html#sleep75",
    "title": "Data-Set Handbook",
    "section": "SLEEP75",
    "text": "SLEEP75\nSource: J.E. Biddle and D.S. Hamermesh (1990), “Sleep and the Allocation of Time,” Journal of Political Economy 98, 922-943. Professor Biddle kindly provided the data. Used in Text: pages 62, 105, 156-157, 251, 257, 290\nNotes: In their article, Biddle and Hamermesh include an hourly wage measure in the sleep equation. An econometric problem that arises is that the hourly wage is missing for those who do not work. Plus, the wage offer may be endogenous (even if it were always observed). Biddle and Hamermesh employ extensions of the sample selection methods in Section 17.5. See their article for details.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: SLEEP75.DES\nDataset title (if known): Not provided\nSuggested title: Sleep and Work Patterns Dataset\n\nThe dataset appears to contain information on various aspects of individuals’ sleep, work, and demographic characteristics. The variables include age, race, employment status, education, earnings, health status, leisure activities, marital status, and other socioeconomic factors. The dataset seems to focus on understanding the relationships between sleep, work, and other personal and household characteristics.\nPotential research ideas:\n\nExamine the relationship between sleep patterns (e.g., sleep duration, napping) and labor force participation or employment status. This could provide insights into the role of sleep in workforce participation and productivity.\nInvestigate the impact of demographic factors, such as age, gender, and marital status, on sleep and work patterns. This could help identify potential disparities and inform policies aimed at promoting work-life balance.\nAnalyze the association between health status (as measured by the “gdhlth” variable) and sleep or work-related variables. This could shed light on the interplay between health, sleep, and employment outcomes.\nExplore the differences in sleep and work patterns between individuals living in urban (SMSA) and rural areas. This could reveal geographical variations and inform regional policy decisions.\nInvestigate the role of household characteristics, such as the presence of young children (“yngkid”) or spousal income (“spsepay”), on the work and sleep behaviors of individuals. This could provide insights into the dynamics of work-family balance."
  },
  {
    "objectID": "data_resources.html#slp75_81",
    "href": "data_resources.html#slp75_81",
    "title": "Data-Set Handbook",
    "section": "SLP75_81",
    "text": "SLP75_81\nSource: See SLEEP75 Used in Text: pages 442-443\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: SLP75_81.DES\nDataset title (if known): Not provided\nSuggested title: Longitudinal Study of Demographic and Health Characteristics\n\nThe dataset contains information on various demographic and health-related variables for a sample of 239 individuals observed in 1975 and 1981. The variables include age, education level, general health status, gender, marital status, sleep and nap duration, work hours, and the presence of young children in the household. The data allows for the analysis of changes in these characteristics over the 6-year period.\nPotential research ideas:\n\nExamine the relationship between changes in education level and changes in general health status over time. This could provide insights into the potential long-term benefits of educational attainment on individual health outcomes.\nInvestigate the impact of changes in marital status on changes in sleep and nap duration. This could shed light on the role of social and emotional factors in sleep patterns.\nAnalyze the relationship between the presence of young children in the household and changes in work hours. This could help understand the challenges and trade-offs faced by individuals with young families in balancing work and family responsibilities.\nExplore the gender differences in the patterns of change across the various demographic and health-related variables. This could reveal insights into the unique experiences and challenges faced by men and women over the study period.\nInvestigate the interplay between changes in multiple variables, such as the combined effects of changes in education, marital status, and the presence of young children on overall well-being and life satisfaction."
  },
  {
    "objectID": "data_resources.html#smoke",
    "href": "data_resources.html#smoke",
    "title": "Data-Set Handbook",
    "section": "SMOKE",
    "text": "SMOKE\nSource: J. Mullahy (1997), “Instrumental-Variable Estimation of Count Data Models: Applications to Models of Cigarette Smoking Behavior,” Review of Economics and Statistics 79, 596-593. Professor Mullahy kindly provided the data. Used in Text: pages 177, 280-281, 288, 291-292, 555, 598-599\nNotes: If you want to do a “fancy” IV version of Computer Exercise C16.1, you could estimate a reduced form count model for cigs using the Poisson regression methods in Section 17.3, and then use the fitted values as an IV for cigs. Presumably, this would be for a fairly advanced class.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: SMOKE.DES\nDataset title (if known): Not provided\nSuggested title: Smoking Behavior and Socioeconomic Factors\n\nThe dataset appears to contain information on smoking behavior and various socioeconomic factors. The variables include years of schooling, state cigarette price, race, age, annual income, number of cigarettes smoked per day, state restaurant smoking restrictions, log of income, age squared, and log of cigarette price. This dataset could be used to explore the relationships between smoking habits and demographic, economic, and policy-related variables.\nPotential research ideas:\n\nExamining the impact of cigarette prices on smoking behavior: This study could investigate how changes in state cigarette prices affect the number of cigarettes smoked per day, taking into account other socioeconomic factors such as income and education.\nAnalyzing the role of restaurant smoking restrictions on smoking habits: Researchers could explore whether the presence of state-level restaurant smoking restrictions is associated with changes in individual smoking patterns, including the number of cigarettes consumed per day.\nInvestigating the relationship between age, income, and smoking: This study could explore how age and income levels influence smoking behavior, and whether there are any interactions between these variables in predicting cigarette consumption.\nExamining the socioeconomic disparities in smoking: Researchers could analyze the differences in smoking patterns across various socioeconomic groups, defined by factors such as education, race, and income, to identify potential areas for targeted interventions.\nExploring the long-term effects of smoking on health outcomes: Using the age and smoking data, researchers could investigate the potential long-term health consequences of smoking, such as the relationship between smoking, age, and health-related outcomes."
  },
  {
    "objectID": "data_resources.html#traffic1",
    "href": "data_resources.html#traffic1",
    "title": "Data-Set Handbook",
    "section": "TRAFFIC1",
    "text": "TRAFFIC1\nSource: I collected these data from two sources, the 1992 Statistical Abstract of the United States (Tables 1009, 1012) and A Digest of State Alcohol-Highway Safety Related Legislation, 1985 and 1990, published by the U.S. National Highway Traffic Safety Administration. Used in Text: pages 444, 446\nNotes: In addition to adding recent years, this data set could really use state-level tax rates on alcohol. Other important law changes include defining driving under the influence as having a blood alcohol level of .08 or more, which many states have adopted since the 1980s. The trend really picked up in the 1990s and continued through the 2000s. The data set DRIVING is more complete and more recent, but it is also more complicated.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: TRAFFIC1.DES\nDataset title (if known): Not provided\nSuggested title: State-Level Traffic Safety Data\n\nThe dataset contains information on various traffic safety measures for 51 states in the United States. The variables include state postal codes, indicators for administrative revocation laws and open container laws in 1990 and 1985, death rates per 100 million miles in 1990 and 1985, speed limit information for 1990 and 1985, and calculated changes in these measures between the two years.\nPotential research ideas:\n\nExamine the relationship between administrative revocation laws and traffic fatality rates. Investigate whether the presence of these laws in 1990 and 1985 had a significant impact on reducing deaths per 100 million miles.\nAnalyze the effect of open container laws on traffic safety. Explore whether states with open container laws in 1990 and 1985 experienced lower death rates compared to states without such laws.\nInvestigate the impact of changes in speed limits on traffic safety. Assess whether the transition from a 65 mph speed limit in 1985 to a 65 mph speed limit in 1990 had a measurable effect on death rates.\nExplore the relationship between changes in administrative revocation laws, open container laws, and speed limits, and the corresponding changes in traffic fatality rates. Determine if these policy changes had a cumulative effect on improving traffic safety.\nConduct a comparative analysis of traffic safety trends between 1985 and 1990. Identify the states that experienced the most significant improvements or declines in death rates and investigate the potential factors contributing to these changes."
  },
  {
    "objectID": "data_resources.html#traffic2",
    "href": "data_resources.html#traffic2",
    "title": "Data-Set Handbook",
    "section": "TRAFFIC2",
    "text": "TRAFFIC2\nSource: P.S. McCarthy (1994), “Relaxed Speed Limits and Highway Safety: New Evidence from California,” Economics Letters 46, 173-179. Professor McCarthy kindly provided the data. Used in Text: pages 364, 392, 422, 641, 659\nNotes: Many states have changed maximum speed limits and imposed seat belt laws over the past 25 years. Data similar to those in TRAFFIC2 should be fairly easy to obtain for a particular state. One should combine this information with changes in a state’s blood alcohol limit and the passage of per se and open container laws.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: TRAFFIC2.DES\nDataset title (if known): Not provided\nSuggested title: Traffic Accident Data\n\nThe dataset appears to contain information on various types of traffic accidents in a state over the years 1981 to 1989. The variables include total accidents, fatal accidents, injury accidents, property damage only accidents, and accidents on different types of roads (interstate, non-interstate, rural 65 mph roads). The dataset also includes information on unemployment rates, speed limit laws, seatbelt laws, and monthly indicators.\nPotential research ideas:\n\nExamine the impact of speed limit laws and seatbelt laws on the different types of traffic accidents (total, fatal, injury, property damage only) over time. This could provide insights into the effectiveness of these policies in improving road safety.\nInvestigate the relationship between unemployment rates and traffic accidents. This could help understand how economic conditions affect driving behavior and accident rates.\nAnalyze the seasonal patterns in traffic accidents, particularly the differences between weekends and weekdays, as well as the variations across different months. This could inform targeted safety interventions and public awareness campaigns.\nExplore the differences in accident rates and trends between interstate, non-interstate, and rural 65 mph roads. This could help identify specific areas or road types that require more attention or infrastructure improvements.\nDevelop predictive models to forecast traffic accident trends based on the available variables, such as time trends, road types, and policy changes. This could assist in resource allocation and proactive safety planning."
  },
  {
    "objectID": "data_resources.html#twoyear",
    "href": "data_resources.html#twoyear",
    "title": "Data-Set Handbook",
    "section": "TWOYEAR",
    "text": "TWOYEAR\nSource: T.J. Kane and C.E. Rouse (1995), “Labor-Market Returns to Two- and Four-Year Colleges,” American Economic Review 85, 600-614. With Professor Rouse’s kind assistance, I obtained the data from her web site at Princeton University. Used in Text: pages 137-139, 160, 254, 329\nNotes: As possible extensions, students can explore whether the returns to two-year or four-year colleges depend on race or gender. This is partly done in Problem 7.9 but where college is aggregated into one number. Also, should experience appear as a quadratic in the wage specification?\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: TWOYEAR.DES\nDataset title (if known): Not provided\nSuggested title: Two-Year College Education and Wages\n\nThe dataset appears to contain information on individuals’ educational background, work experience, and wages. The variables include gender, high school rank, degree attainment (Bachelor’s and Associate’s), race (African-American and Hispanic), work experience, credits earned at two-year and four-year institutions, log hourly wage, standardized test scores, and geographic location (city size and region). The dataset seems to focus on the relationship between two-year college education and labor market outcomes.\nPotential research ideas:\nExamine the impact of two-year college education (as measured by total 2-year credits) on log hourly wages, controlling for other factors such as work experience, standardized test scores, and geographic location.\nInvestigate the differences in educational attainment and labor market outcomes between African-American and Hispanic individuals compared to their non-minority counterparts, and explore the potential factors contributing to these disparities.\nAnalyze the relationship between high school rank and the likelihood of obtaining a Bachelor’s or Associate’s degree, and how this relationship varies across different geographic regions or city sizes.\nExplore the role of work experience (as measured by the “exper” variable) in mediating the relationship between two-year college education and wages, and whether this effect differs for individuals with different levels of educational attainment.\nInvestigate the potential interaction effects between two-year college education, four-year college education (as measured by the “univ” variable), and labor market outcomes, and how these relationships may be influenced by other demographic and geographic factors."
  },
  {
    "objectID": "data_resources.html#volat",
    "href": "data_resources.html#volat",
    "title": "Data-Set Handbook",
    "section": "VOLAT",
    "text": "VOLAT\nSource: J.D. Hamilton and L. Gang (1996), “Stock Market Volatility and the Business Cycle,” Journal of Applied Econometrics 11, 573-593. I obtained these data from the Journal of Applied Econometrics data archive at http://qed.econ.queensu.ca/jae/ Used in Text: pages 364, 637-640\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: VOLAT.DES\nDataset title (if known): Not provided\nSuggested title: Financial and Economic Time Series Data\n\nThe dataset contains monthly observations from January 1947 to June 1993 (558 observations) on various financial and economic variables. The variables include the S&P 500 index, dividend yield, 3-month Treasury bill rate, index of industrial production, and various percentage changes and differences related to these variables.\nPotential research ideas:\n\nInvestigating the relationship between stock market returns (S&P 500) and economic activity (industrial production) over time. This could involve analyzing the lead-lag relationships, the impact of changes in interest rates, and the role of dividend yields in explaining stock market performance.\nExamining the predictability of stock market returns using macroeconomic variables, such as the 3-month Treasury bill rate and its changes. This could provide insights into the efficiency of the stock market and the potential for using economic indicators to forecast future market movements.\nAnalyzing the volatility dynamics in the stock market and industrial production, and exploring the potential spillover effects between these two domains. This could include the use of time series models, such as GARCH, to capture the time-varying nature of volatility.\nStudying the impact of changes in interest rates (3-month Treasury bill) on the stock market and industrial production. This could involve investigating the transmission mechanisms and the relative importance of different channels, such as the cost of capital and the wealth effect.\nExploring the potential for using the lagged values of the percentage changes in the S&P 500 and industrial production to predict their current values. This could provide insights into the predictability of these variables and the potential for developing forecasting models."
  },
  {
    "objectID": "data_resources.html#vote1",
    "href": "data_resources.html#vote1",
    "title": "Data-Set Handbook",
    "section": "VOTE1",
    "text": "VOTE1\nSource: M. Barone and G. Ujifusa, The Almanac of American Politics, 1992. Washington, DC: National Journal. Used in Text: pages 31, 36, 159-160, 215-216, 290, 663\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: VOTE1.DES\nDataset title (if known): Not provided\nSuggested title: Congressional Election Data\n\nThe dataset contains information about congressional elections, including the state, district, candidate information (party affiliation, vote percentage, and campaign expenditures), and the percentage of the presidential vote for the candidate’s party. The data covers 173 observations, likely representing different congressional districts.\nPotential research ideas:\n\nExamine the relationship between campaign expenditures and vote share for candidates. This could provide insights into the role of money in political campaigns and the effectiveness of campaign spending.\nInvestigate the impact of candidate party affiliation on vote share, controlling for other factors such as campaign expenditures and presidential vote share. This could shed light on the influence of party identification on voter behavior.\nAnalyze the differences in campaign expenditures between Democratic and Republican candidates and how these differences may affect the electoral outcomes. This could contribute to the ongoing debate about the role of money in politics.\nExplore the relationship between the presidential vote share and the congressional candidate’s vote share within the same district. This could help understand the extent to which national political trends influence local elections.\nInvestigate the factors that contribute to the variation in the share of the vote received by the congressional candidates. This could include examining the influence of demographic, economic, or political factors on electoral outcomes."
  },
  {
    "objectID": "data_resources.html#vote2",
    "href": "data_resources.html#vote2",
    "title": "Data-Set Handbook",
    "section": "VOTE2",
    "text": "VOTE2\nSource: See VOTE1 Used in Text: pages 324-325, 444, 455-456, 663\nNotes: These are panel data, at the Congressional district level, collected for the 1988 and 1990 U.S. House of Representative elections. Of course, much more recent data are available, possibly even in electronic form.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: VOTE2.DES\nDataset title (if known): Not provided\nSuggested title: Congressional Election Data\n\nThe dataset contains information on U.S. Congressional elections, including variables related to incumbent and challenger campaign expenditures, vote shares, and other characteristics of the candidates and districts. The data covers the 1988 and 1990 election cycles and includes 186 observations.\nPotential research ideas:\nExamining the relationship between campaign expenditures and electoral outcomes: Researchers could investigate how incumbent and challenger campaign spending levels influence the share of the two-party vote received by the incumbent. This could provide insights into the role of money in congressional elections.\nAnalyzing the impact of candidate characteristics on electoral success: The dataset includes variables such as whether the incumbent is a Democrat, whether the challenger is a repeat challenger, and whether the incumbent has a law degree. Researchers could explore how these factors affect the incumbent’s probability of winning the election.\nInvestigating the role of party strength in congressional elections: The variable “prtystr” represents the percentage of the presidential vote received by the same party as the incumbent in the 1988 election. Researchers could examine how this measure of party strength influences the incumbent’s vote share.\nStudying the dynamics of incumbent-challenger competition: The dataset includes variables that capture changes in campaign expenditures and vote shares between the 1988 and 1990 elections. Researchers could analyze how these changes are related to the outcome of the 1990 election.\nExploring the impact of congressional tenure on electoral performance: The “tenure” variable indicates the number of years the incumbent has served in the House of Representatives. Researchers could investigate whether longer tenure is associated with higher vote shares or a greater probability of winning the election."
  },
  {
    "objectID": "data_resources.html#voucher",
    "href": "data_resources.html#voucher",
    "title": "Data-Set Handbook",
    "section": "VOUCHER",
    "text": "VOUCHER\nSource: Rouse, C.E. (1998), “Private School Vouchers and Student Achievement: An Evaluation of the Milwaukee Parental Choice Program,” Quarterly Journal of Economics 113, 553-602. Professor Rouse kindly provided the original data set from her paper. Used in Text: pages 529-530\nNotes: This is a condensed version of the data set used by Professor Rouse. The original data set had missing information on many variables, including pre-program and post-program test scores. I did not impute any missing data and have dropped observations that were unusable without filling in missing data. There are 990 students in the current data set but pre-program test scores are available for only 328 of them. This is a good example of where eligibility for a program is randomized but participation need not be. In addition, even if we look at just the effect of eligibility (captured in the variable selectyrs) on the math test score (mnce), we need to confront the fact that attrition (students leaving the district) can bias the results. Controlling for the pre-policy test score, mnce90, can help – but at the cost of losing two-thirds of the observations. A simple regression of mnce on selectyrs followed by a multiple regression that adds mnce90 as a control is informative. The selectyrs dummy variables can be used as instrumental variables for the choiceyrs variable to try to estimate the effect of actually participating in the program (rather than estimating the so-called intention-to-treat effect). Computer Exercise C15.11 steps through the details.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: voucher.des\nDataset title (if known): Not provided\nSuggested title: Student Voucher Program Data\n\nThe dataset appears to contain information about a student voucher program. It includes variables related to student demographics (race, gender), application and selection for the voucher program, and academic performance as measured by math test scores. The data covers the years 1990 to 1994 and includes information on whether students were selected for the program, the number of years they attended the choice school, and their math test scores.\nPotential research ideas:\nExamine the relationship between student demographics (race, gender) and selection for the voucher program. Investigate whether there are any disparities in the selection process and explore potential factors contributing to these differences.\nAnalyze the impact of the voucher program on student academic performance, as measured by math test scores. Investigate whether attending the choice school for different durations (1-4 years) has a significant effect on student outcomes.\nExplore the factors that influence a student’s decision to attend the choice school, such as the number of years they were selected for the program. Identify any patterns or trends in the choice of attending the voucher school.\nInvestigate the long-term effects of the voucher program on student outcomes, such as high school graduation rates, college enrollment, or future earnings. This could involve linking the current dataset to additional data sources.\nConduct a comparative analysis between students who were selected for the voucher program and those who were not. Examine differences in academic performance, educational attainment, or other relevant outcomes to assess the overall effectiveness of the voucher program."
  },
  {
    "objectID": "data_resources.html#wage1",
    "href": "data_resources.html#wage1",
    "title": "Data-Set Handbook",
    "section": "WAGE1",
    "text": "WAGE1\nSource: These are data from the 1976 Current Population Survey, collected by Henry Farber when he and I were colleagues at MIT in 1988. Used in Text: pages 6, 30-31, 33, 73, 86, 123, 178, 189, 214, 222, 224, 227, 228-229, 232-233, 235, 257, 265-266, 316, 644\nNotes: Barry Murphy, of the University of Portsmouth in the UK, has pointed out that for several observations the values for exper and tenure are in logical conflict. In particular, for some workers the number of years with current employer (tenure) is greater than overall work experience (exper). At least some of these conflicts are due to the definition of exper as “potential” work experience, but probably not all. Nevertheless, I am using the data set as it was supplied to me.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: WAGE1.DES\nDataset title (if known): Not provided\nSuggested title: Wage and Employment Characteristics Dataset\n\nThe dataset contains information on 526 individuals, including their average hourly earnings (wage), years of education (educ), years of potential experience (exper), years with their current employer (tenure), and various demographic and employment-related characteristics. The data includes indicators for race (nonwhite), gender (female), marital status (married), number of dependents (numdep), and whether the individual lives in a Standard Metropolitan Statistical Area (smsa). The dataset also includes information on the individual’s industry (construc, ndurman, trcommpu, trade, services, profserv) and occupation (profocc, clerocc, servocc). Additionally, the dataset includes the natural logarithm of the wage (lwage) and the squared terms of experience (expersq) and tenure (tenursq).\nPotential research ideas:\n\n\nExamine the relationship between education, experience, and wages: Investigate how an individual’s years of education and potential experience influence their average hourly earnings, and whether there are any nonlinear effects or interactions between these variables.\nAnalyze the impact of demographic factors on wages: Explore the differences in wages between individuals of different races, genders, and marital statuses, and investigate the role of the number of dependents in determining wages.\nInvestigate the influence of industry and occupation on wages: Analyze how an individual’s industry and occupation affect their average hourly earnings, and identify any potential wage premiums or discounts associated with different industries and occupations.\nExplore the role of job tenure in wage determination: Examine the relationship between an individual’s tenure with their current employer and their wages, and determine whether there are any diminishing returns or nonlinear effects associated with longer tenure.\nDevelop a predictive model for wages: Utilize the available variables to construct a regression model that can accurately predict an individual’s average hourly earnings, and identify the most important factors in determining wages."
  },
  {
    "objectID": "data_resources.html#wage2",
    "href": "data_resources.html#wage2",
    "title": "Data-Set Handbook",
    "section": "WAGE2",
    "text": "WAGE2\nSource: M. Blackburn and D. Neumark (1992), “Unobserved Ability, Efficiency Wages, and Interindustry Wage Differentials,” Quarterly Journal of Economics 107, 1421-1436. Professor Neumark kindly provided the data, of which I used just the data for 1980. Used in Text: pages 63, 104-105, 110, 160, 212, 215, 256, 301-302, 328, 502, 515, 526, 528-529, 644\nNotes: As with WAGE1, there are some clear inconsistencies among the variables tenure, exper, and age. I have not been able to track down the source of the inconsistency, and so any changes would be effectively arbitrary. Instead, I am using the data as provided by the authors of the above QJE article.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: WAGE2.DES\nDataset title (if known): Not provided\nSuggested title: Wage and Demographic Characteristics Dataset\n\nThe dataset contains information on 935 individuals, including their monthly earnings, average weekly hours, IQ scores, knowledge of the world of work, education, work experience, tenure with their current employer, age, marital status, race, region of residence, family background (number of siblings, birth order, and parents’ education), and the natural log of their wage. This dataset provides a comprehensive view of the socioeconomic and demographic factors that may influence an individual’s earnings.\nPotential research ideas:\n\nExamining the relationship between IQ, knowledge of the world of work, and wages: This study could investigate the relative importance of cognitive abilities and job-specific knowledge in determining an individual’s earnings, and how these factors interact with other demographic and educational variables.\nAnalyzing the impact of family background on educational and labor market outcomes: Researchers could explore how factors such as parental education, number of siblings, and birth order influence an individual’s educational attainment, work experience, and ultimately, their wages.\nInvestigating the role of work experience and job tenure in wage determination: This study could focus on understanding the dynamics of human capital accumulation and how it translates into higher earnings, as well as the potential differences in the returns to experience and tenure for different demographic groups.\nExploring the urban-rural wage gap and the factors contributing to it: Researchers could examine the differences in wages between individuals living in urban and rural areas, and identify the socioeconomic and geographic factors that may explain these disparities.\nAnalyzing the gender and racial wage gaps: This study could investigate the extent of wage differences between men and women, as well as between Black and non-Black individuals, and the potential drivers of these gaps, such as differences in education, work experience, and labor market discrimination."
  },
  {
    "objectID": "data_resources.html#wagepan",
    "href": "data_resources.html#wagepan",
    "title": "Data-Set Handbook",
    "section": "WAGEPAN",
    "text": "WAGEPAN\nSource: F. Vella and M. Verbeek (1998), “Whose Wages Do Unions Raise? A Dynamic Model of Unionism and Wage Rate Determination for Young Men,” Journal of Applied Econometrics 13, 163-183. I obtained the data from the Journal of Applied Econometrics data archive at http://qed.econ.queensu.ca/jae/. The JAE data archive is generally a nice resource for undergraduates looking to replicate or extend a published study. Used in Text: pages 457, 465, 472\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: WAGEPAN.DES\nDataset title (if known): Not provided\nSuggested title: Wage and Employment Panel Data\n\nThe dataset appears to contain information on the wages and employment characteristics of individuals over the years 1980 to 1987. The variables include personal identifiers, demographic characteristics (race, ethnicity, marital status, education), employment-related information (occupation, hours worked, union membership), and wage data (log of wage). The dataset has 4,360 observations.\nPotential research ideas:\nExamine the relationship between race, ethnicity, and wages: Investigate whether there are significant differences in wages between black, Hispanic, and non-minority individuals, and explore the factors that may contribute to these wage gaps.\nAnalyze the impact of union membership on wages: Assess the effect of union membership on individual wages, and explore how this relationship may vary across different occupations or industries.\nInvestigate the role of work experience and education in determining wages: Analyze the relationship between labor market experience, years of schooling, and individual wages, and explore how these factors interact to influence earnings.\nExplore the impact of marital status on employment and wages: Examine whether there are differences in employment patterns and wages between married and unmarried individuals, and investigate the potential mechanisms underlying these differences.\nAnalyze the trends in wages over time: Assess the changes in individual wages across the years 1980 to 1987, and explore the potential factors, such as economic conditions or policy changes, that may have contributed to these wage trends."
  },
  {
    "objectID": "data_resources.html#wageprc",
    "href": "data_resources.html#wageprc",
    "title": "Data-Set Handbook",
    "section": "WAGEPRC",
    "text": "WAGEPRC\nSource: Economic Report of the President, various years. Used in Text: pages 388, 421, 638\nNotes: These monthly data run from January 1964 through October 1987. The consumer price index averages to 100 in 1967. An updated set of data can be obtained electronically from http://www.gpo.gov/fdsys/browse/collection.action?collectionCode=ERP.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: WAGEPRC.DES\nDataset title (if known): Not provided\nSuggested title: Wage and Price Index Data\n\nThe dataset appears to contain information on consumer price index (price), nominal hourly wage (wage), and various transformations of these variables, such as log-transformed values (lprice and lwage), first differences (gprice and gwage), and lagged values of the first differences (gwage_1 to gwage_12 and gprice_1). The dataset has 286 observations and covers a time period indicated by the variable t, which ranges from 1 to 286.\nPotential research ideas:\n\nInvestigating the relationship between wages and prices: The dataset provides information on both the consumer price index and nominal hourly wages, as well as their log-transformed and first-difference versions. Researchers could explore the dynamic relationship between wages and prices, such as the impact of changes in wages on inflation or the role of wage-price spirals in the economy.\nAnalyzing the persistence of wage and price changes: The dataset includes several lags of the first-difference variables (gwage_1 to gwage_12 and gprice_1), which could be used to study the persistence of wage and price changes over time. Researchers could investigate the dynamics of wage and price adjustments and the factors that influence the speed of adjustment.\nExamining the impact of macroeconomic factors on wage and price dynamics: The dataset could be used to explore the influence of macroeconomic variables, such as economic growth, unemployment, or monetary policy, on the behavior of wages and prices. Researchers could develop models to understand the drivers of wage and price changes and their implications for economic policy.\nComparing the behavior of wages and prices across different sectors or industries: If additional information about the industry or sector composition of the data is available, researchers could investigate whether the dynamics of wages and prices vary across different parts of the economy. This could provide insights into the heterogeneity of wage and price adjustments.\n\ne: Evaluating the effectiveness of wage and price policies: The dataset could be used to assess the impact of policies aimed at influencing wage and price dynamics, such as minimum wage laws, wage indexation, or price controls. Researchers could analyze the effectiveness of these policies in achieving desired economic outcomes."
  },
  {
    "objectID": "data_resources.html#wine",
    "href": "data_resources.html#wine",
    "title": "Data-Set Handbook",
    "section": "WINE",
    "text": "WINE\nSource: These data were reported in a New York Times article, December 28, 1994.\nUsed in Text: not used\nNotes: The dependent variables deaths, heart, and liver each can be regressed on alcohol as nice simple regression examples. The conventional wisdom is that wine is good for the heart but not for the liver, something that is apparent in the regressions. Because the number of observations is small, this can be a good data set to illustrate calculation of the OLS estimates and statistics.\n\n\n\n\n\n\nAI Description\n\n\n\n\nDataset information:\n\nDatafile name: WINE.DES\nDataset title (if known): Not provided\nSuggested title: Wine Consumption and Health Outcomes\n\nThe dataset contains information on wine consumption and various health outcomes across 21 countries. The variables include the country, the average liters of alcohol consumed per capita from wine, the number of deaths per 100,000 people, the number of deaths from heart disease per 100,000 people, and the number of deaths from liver disease per 100,000 people.\nPotential research ideas:\nInvestigate the relationship between wine consumption and mortality rates. This could involve analyzing the correlation between alcohol consumption and overall deaths, as well as the specific relationships between wine consumption and deaths from heart disease and liver disease.\nExplore the differences in health outcomes across countries with varying levels of wine consumption. This could provide insights into the potential benefits or risks associated with different levels of wine consumption.\nExamine the role of cultural and socioeconomic factors in shaping wine consumption patterns and their impact on health outcomes. This could involve analyzing the data in the context of other country-level variables, such as GDP, education levels, or healthcare systems.\nAssess the potential impact of public health policies or interventions aimed at promoting responsible wine consumption on reducing mortality rates. This could involve comparing countries with different approaches to alcohol regulation and their corresponding health outcomes.\nConduct a comparative analysis of the effects of wine consumption on health outcomes versus other alcoholic beverages. This could help identify any unique characteristics or potential benefits associated with wine consumption."
  },
  {
    "objectID": "imewld/chapter2.html",
    "href": "imewld/chapter2.html",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "",
    "text": "Model:\n\\[salary = \\beta_0 + \\beta_1 roe + u\\]\n\n\n\n\n\n\n\nfrause ceosal1, clear\nregress salary roe    \n\ndisplay \"Prediction: \" _b[_cons] \" +\" _b[roe] \"*30\" \"=\" _b[_cons] + _b[roe] *30\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =       209\n-------------+----------------------------------   F(1, 207)       =      2.77\n       Model |  5166419.04         1  5166419.04   Prob &gt; F        =    0.0978\n    Residual |   386566563       207  1867471.32   R-squared       =    0.0132\n-------------+----------------------------------   Adj R-squared   =    0.0084\n       Total |   391732982       208  1883331.64   Root MSE        =    1366.6\n\n------------------------------------------------------------------------------\n      salary | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         roe |   18.50119   11.12325     1.66   0.098    -3.428196    40.43057\n       _cons |   963.1913   213.2403     4.52   0.000     542.7902    1383.592\n------------------------------------------------------------------------------\nPrediction: 963.19134 +18.501186*30=1518.2269"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.3-ceo-salary-and-return-on-equity",
    "href": "imewld/chapter2.html#example-2.3-ceo-salary-and-return-on-equity",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "",
    "text": "Model:\n\\[salary = \\beta_0 + \\beta_1 roe + u\\]\n\n\n\n\n\n\n\nfrause ceosal1, clear\nregress salary roe    \n\ndisplay \"Prediction: \" _b[_cons] \" +\" _b[roe] \"*30\" \"=\" _b[_cons] + _b[roe] *30\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =       209\n-------------+----------------------------------   F(1, 207)       =      2.77\n       Model |  5166419.04         1  5166419.04   Prob &gt; F        =    0.0978\n    Residual |   386566563       207  1867471.32   R-squared       =    0.0132\n-------------+----------------------------------   Adj R-squared   =    0.0084\n       Total |   391732982       208  1883331.64   Root MSE        =    1366.6\n\n------------------------------------------------------------------------------\n      salary | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         roe |   18.50119   11.12325     1.66   0.098    -3.428196    40.43057\n       _cons |   963.1913   213.2403     4.52   0.000     542.7902    1383.592\n------------------------------------------------------------------------------\nPrediction: 963.19134 +18.501186*30=1518.2269"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.4-wage-and-education",
    "href": "imewld/chapter2.html#example-2.4-wage-and-education",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.4: Wage and Education",
    "text": "Example 2.4: Wage and Education\n\n\n\n\n\n\n\nfrause wage1, clear\nregress wage educ\n\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =    103.36\n       Model |  1179.73204         1  1179.73204   Prob &gt; F        =    0.0000\n    Residual |  5980.68225       524  11.4135158   R-squared       =    0.1648\n-------------+----------------------------------   Adj R-squared   =    0.1632\n       Total |  7160.41429       525  13.6388844   Root MSE        =    3.3784\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .5413593    .053248    10.17   0.000     .4367534    .6459651\n       _cons |  -.9048516   .6849678    -1.32   0.187    -2.250472    .4407687\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.5-voting-outcomes-and-campaign-expenditures",
    "href": "imewld/chapter2.html#example-2.5-voting-outcomes-and-campaign-expenditures",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.5: Voting Outcomes and Campaign Expenditures",
    "text": "Example 2.5: Voting Outcomes and Campaign Expenditures\n\n\n\n\n\n\n\nfrause vote1, clear\nregress votea sharea\n\n\n      Source |       SS           df       MS      Number of obs   =       173\n-------------+----------------------------------   F(1, 171)       =   1017.66\n       Model |  41486.2307         1  41486.2307   Prob &gt; F        =    0.0000\n    Residual |  6971.01783       171  40.7661862   R-squared       =    0.8561\n-------------+----------------------------------   Adj R-squared   =    0.8553\n       Total |  48457.2486       172  281.728189   Root MSE        =    6.3848\n\n------------------------------------------------------------------------------\n       votea | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      sharea |   .4638269   .0145397    31.90   0.000     .4351266    .4925272\n       _cons |   26.81221   .8872146    30.22   0.000     25.06091    28.56352\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.6-ceo-salary-and-return-on-equity",
    "href": "imewld/chapter2.html#example-2.6-ceo-salary-and-return-on-equity",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.6: CEO Salary and Return on Equity",
    "text": "Example 2.6: CEO Salary and Return on Equity\n\n\n\n\n\n\n\nfrause ceosal1, clear\nqui:regress salary roe  \npredict salaryhat, xb\npredict uhat , resid\n\nlist roe salary salaryhat uhat  in 1/10\n\n\n     +--------------------------------------+\n     |  roe   salary   salary~t        uhat |\n     |--------------------------------------|\n  1. | 14.1     1095   1224.058   -129.0581 |\n  2. | 10.9     1001   1164.854   -163.8543 |\n  3. | 23.5     1122   1397.969   -275.9692 |\n  4. |  5.9      578   1072.348   -494.3483 |\n  5. | 13.8     1368   1218.508    149.4923 |\n     |--------------------------------------|\n  6. |   20     1145   1333.215   -188.2151 |\n  7. | 16.4     1078   1266.611   -188.6108 |\n  8. | 16.3     1094   1264.761   -170.7607 |\n  9. | 10.5     1237   1157.454     79.5462 |\n 10. | 26.3      833   1449.773   -616.7725 |\n     +--------------------------------------+"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.7-wage-and-education",
    "href": "imewld/chapter2.html#example-2.7-wage-and-education",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.7: Wage and Education",
    "text": "Example 2.7: Wage and Education\n\n\n\n\n\n\n\nfrause wage1, clear\nqui:regress wage educ\nsum wage educ\ndisplay \"b0+b1*E(educ)= \" _b[_cons] + _b[educ]*r(mean)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        wage |        526    5.896103    3.693086        .53      24.98\n        educ |        526    12.56274    2.769022          0         18\nb0+b1*E(educ)= 5.8961027"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.8-ceo-salary-and-return-on-equity",
    "href": "imewld/chapter2.html#example-2.8-ceo-salary-and-return-on-equity",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.8: CEO Salary and Return on Equity",
    "text": "Example 2.8: CEO Salary and Return on Equity\n\nfrause ceosal1, clear\ngen one=1\nmata:y = st_data(., \"salary\")\nmata:x = st_data(., \"one roe\")\nmata:b = invsym(x'*x)*x'*y\nmata:yhat = x*b\nmata:uhat = y - yhat\nmata:sst = sum((y :- mean(y)):^2)\nmata:sse = sum((yhat :- mean(y)):^2)\nmata:ssr = sum((y :- yhat):^2)\nmata:rsq = 1 - ssr/sst;rsq\nmata:rsq = sse/sst;rsq\n\n  .0131886241\n  .0131886241"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.10-a-log-wage-model",
    "href": "imewld/chapter2.html#example-2.10-a-log-wage-model",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.10: a log wage model",
    "text": "Example 2.10: a log wage model\nModel\n\\[log(wage) = \\beta_0 + \\beta_1 educ + u\\]\n\n\n\n\n\n\n\nfrause wage1, clear\ngen logwage = log(wage)\nregress logwage educ\n\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =    119.58\n       Model |  27.5606288         1  27.5606288   Prob &gt; F        =    0.0000\n    Residual |  120.769123       524  .230475425   R-squared       =    0.1858\n-------------+----------------------------------   Adj R-squared   =    0.1843\n       Total |  148.329751       525   .28253286   Root MSE        =    .48008\n\n------------------------------------------------------------------------------\n     logwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0827444   .0075667    10.94   0.000     .0678796    .0976091\n       _cons |   .5837727   .0973358     6.00   0.000     .3925563    .7749891\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.11-ceo-salary-and-firms-sales",
    "href": "imewld/chapter2.html#example-2.11-ceo-salary-and-firms-sales",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.11: CEO Salary and Firms Sales",
    "text": "Example 2.11: CEO Salary and Firms Sales\nModel:\n\\[log(salary) = \\beta_0 + \\beta_1 log(sales) + u\\]\n\n\n\n\n\n\n\nfrause ceosal1, clear\ngen logsalary = log(salary)\ngen logsales = log(sales)\nreg logsalary logsales\n\n\n      Source |       SS           df       MS      Number of obs   =       209\n-------------+----------------------------------   F(1, 207)       =     55.30\n       Model |  14.0661688         1  14.0661688   Prob &gt; F        =    0.0000\n    Residual |  52.6559944       207  .254376785   R-squared       =    0.2108\n-------------+----------------------------------   Adj R-squared   =    0.2070\n       Total |  66.7221632       208  .320779631   Root MSE        =    .50436\n\n------------------------------------------------------------------------------\n   logsalary | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    logsales |   .2566717   .0345167     7.44   0.000     .1886224    .3247209\n       _cons |   4.821997   .2883396    16.72   0.000     4.253538    5.390455\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.14-evaluating-a-job-training-program",
    "href": "imewld/chapter2.html#example-2.14-evaluating-a-job-training-program",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.14: Evaluating a Job Training Program",
    "text": "Example 2.14: Evaluating a Job Training Program\n\n\n\n\n\n\n\nfrause jtrain2, clear\nreg re78 train\n\n\n      Source |       SS           df       MS      Number of obs   =       445\n-------------+----------------------------------   F(1, 443)       =      8.04\n       Model |  348.013451         1  348.013451   Prob &gt; F        =    0.0048\n    Residual |  19177.6432       443  43.2903909   R-squared       =    0.0178\n-------------+----------------------------------   Adj R-squared   =    0.0156\n       Total |  19525.6566       444  43.9767041   Root MSE        =    6.5795\n\n------------------------------------------------------------------------------\n        re78 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       train |   1.794343   .6328536     2.84   0.005     .5505748    3.038111\n       _cons |   4.554802    .408046    11.16   0.000     3.752856    5.356749\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter4.html",
    "href": "imewld/chapter4.html",
    "title": "Chapter 4: Multiple Regression Analysis: Inference",
    "section": "",
    "text": "Model:\n\\[log(wage)=\\beta_0 +\\beta_1 jc + \\beta_2 univ + \\beta_3 exper + u\\]\nHypothesis\n\\[H_0: \\beta_1 = \\beta_2 \\rightarrow \\beta_1 - \\beta_2 = 0\\] \\[H_1: \\beta_1 &lt; \\beta_2 \\rightarrow \\beta_1 - \\beta_2 &lt; 0\\]\n\nfrause twoyear, clear\nreg lwage jc univ exper\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =     6,763\n-------------+----------------------------------   F(3, 6759)      =    644.53\n       Model |  357.752575         3  119.250858   Prob &gt; F        =    0.0000\n    Residual |  1250.54352     6,759  .185019014   R-squared       =    0.2224\n-------------+----------------------------------   Adj R-squared   =    0.2221\n       Total |  1608.29609     6,762  .237843255   Root MSE        =    .43014\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          jc |   .0666967   .0068288     9.77   0.000     .0533101    .0800833\n        univ |   .0768762   .0023087    33.30   0.000     .0723504    .0814021\n       exper |   .0049442   .0001575    31.40   0.000     .0046355    .0052529\n       _cons |   1.472326   .0210602    69.91   0.000     1.431041     1.51361\n------------------------------------------------------------------------------\n\n\n\ntest jc = univ\n\n\n ( 1)  jc - univ = 0\n\n       F(  1,  6759) =    2.15\n            Prob &gt; F =    0.1422\n\n\nManual transformation:\n\\[\\theta = \\beta_1 - \\beta_2 \\rightarrow \\beta_1 = \\theta + \\beta_2 \\]\n\\[log(wage)=\\beta_0 +(\\theta + \\beta_2) jc + \\beta_2 univ + \\beta_3 exper + u\\] \\[log(wage)=\\beta_0 +\\theta jc + \\beta_2 (univ+jc) + \\beta_3 exper + u\\]\n\ngen univjc = univ + jc\nreg lwage jc univjc exper\n\n\n      Source |       SS           df       MS      Number of obs   =     6,763\n-------------+----------------------------------   F(3, 6759)      =    644.53\n       Model |  357.752575         3  119.250858   Prob &gt; F        =    0.0000\n    Residual |  1250.54352     6,759  .185019014   R-squared       =    0.2224\n-------------+----------------------------------   Adj R-squared   =    0.2221\n       Total |  1608.29609     6,762  .237843255   Root MSE        =    .43014\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          jc |  -.0101795   .0069359    -1.47   0.142    -.0237761     .003417\n      univjc |   .0768762   .0023087    33.30   0.000     .0723504    .0814021\n       exper |   .0049442   .0001575    31.40   0.000     .0046355    .0052529\n       _cons |   1.472326   .0210602    69.91   0.000     1.431041     1.51361\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter4.html#testing-hypotheses-about-single-linear-combinations-of-the-parameters",
    "href": "imewld/chapter4.html#testing-hypotheses-about-single-linear-combinations-of-the-parameters",
    "title": "Chapter 4: Multiple Regression Analysis: Inference",
    "section": "",
    "text": "Model:\n\\[log(wage)=\\beta_0 +\\beta_1 jc + \\beta_2 univ + \\beta_3 exper + u\\]\nHypothesis\n\\[H_0: \\beta_1 = \\beta_2 \\rightarrow \\beta_1 - \\beta_2 = 0\\] \\[H_1: \\beta_1 &lt; \\beta_2 \\rightarrow \\beta_1 - \\beta_2 &lt; 0\\]\n\nfrause twoyear, clear\nreg lwage jc univ exper\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =     6,763\n-------------+----------------------------------   F(3, 6759)      =    644.53\n       Model |  357.752575         3  119.250858   Prob &gt; F        =    0.0000\n    Residual |  1250.54352     6,759  .185019014   R-squared       =    0.2224\n-------------+----------------------------------   Adj R-squared   =    0.2221\n       Total |  1608.29609     6,762  .237843255   Root MSE        =    .43014\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          jc |   .0666967   .0068288     9.77   0.000     .0533101    .0800833\n        univ |   .0768762   .0023087    33.30   0.000     .0723504    .0814021\n       exper |   .0049442   .0001575    31.40   0.000     .0046355    .0052529\n       _cons |   1.472326   .0210602    69.91   0.000     1.431041     1.51361\n------------------------------------------------------------------------------\n\n\n\ntest jc = univ\n\n\n ( 1)  jc - univ = 0\n\n       F(  1,  6759) =    2.15\n            Prob &gt; F =    0.1422\n\n\nManual transformation:\n\\[\\theta = \\beta_1 - \\beta_2 \\rightarrow \\beta_1 = \\theta + \\beta_2 \\]\n\\[log(wage)=\\beta_0 +(\\theta + \\beta_2) jc + \\beta_2 univ + \\beta_3 exper + u\\] \\[log(wage)=\\beta_0 +\\theta jc + \\beta_2 (univ+jc) + \\beta_3 exper + u\\]\n\ngen univjc = univ + jc\nreg lwage jc univjc exper\n\n\n      Source |       SS           df       MS      Number of obs   =     6,763\n-------------+----------------------------------   F(3, 6759)      =    644.53\n       Model |  357.752575         3  119.250858   Prob &gt; F        =    0.0000\n    Residual |  1250.54352     6,759  .185019014   R-squared       =    0.2224\n-------------+----------------------------------   Adj R-squared   =    0.2221\n       Total |  1608.29609     6,762  .237843255   Root MSE        =    .43014\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          jc |  -.0101795   .0069359    -1.47   0.142    -.0237761     .003417\n      univjc |   .0768762   .0023087    33.30   0.000     .0723504    .0814021\n       exper |   .0049442   .0001575    31.40   0.000     .0046355    .0052529\n       _cons |   1.472326   .0210602    69.91   0.000     1.431041     1.51361\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "mathref/math_1.html#introduction",
    "href": "mathref/math_1.html#introduction",
    "title": "Math Refresher",
    "section": "Introduction",
    "text": "Introduction\n\nThis is a refresher on basic calculus. It is not meant to be a substitute for a full course on calculus but rather a quick review of the fundamental concepts and techniques that will be used this semester.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_1.html#limits",
    "href": "mathref/math_1.html#limits",
    "title": "Math Refresher",
    "section": "Limits",
    "text": "Limits\nThe limit of a function \\(f(x)\\) as \\(x\\) approaches \\(a\\) is the value that \\(f(x)\\) approaches as \\(x\\) gets closer and closer to \\(a\\). We write this as:\n\\[\n\\lim_{{x \\to a}} f(x) = L\n\\]\nHere, \\(L\\) is the limit of the function \\(f(x)\\) as \\(x\\) approaches \\(a\\).\nFor example, consider the function \\(f(x) = x^2\\). The limit of \\(f(x)\\) as \\(x\\) approaches 2 is 4:\n\\[\n\\lim_{{x \\to 2}} x^2 = 4\n\\]",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_1.html#limits-to-derivatives",
    "href": "mathref/math_1.html#limits-to-derivatives",
    "title": "Math Refresher",
    "section": "Limits to Derivatives",
    "text": "Limits to Derivatives\nLimits can also be used to define derivatives. The derivative of a function \\(f(x)\\) is the slope of the function at a given point. The derivative of \\(f(x)\\) at \\(x = a\\) is written as \\(f'(a)\\). The derivative is defined as:\n\\[\nf'(a) = \\lim_{{h \\to 0}} \\frac{f(a+h) - f(a)}{h}\n\\]\nIn other words, the derivative is the slope of the function at a particular point \\(a\\). This can be approximated numerically by choosing a very small value for \\(h\\).",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_1.html#section",
    "href": "mathref/math_1.html#section",
    "title": "Math Refresher",
    "section": "",
    "text": "For example, consider the function \\(f(x) = x^2\\). The derivative of \\(f(x)\\) at \\(x = a\\) is:\n\\[\n\\begin{aligned}\nf'(a) &= \\lim_{{h \\to 0}} \\frac{(a+h)^2 - a^2}{h} \\\\\n&= \\lim_{{h \\to 0}} \\frac{a^2 + 2ah + h^2 - a^2}{h} \\\\\n&= \\lim_{{h \\to 0}} (2a + h) = 2a.\n\\end{aligned}\n\\]\nIf other methods fail, one can always rely on numerical differentiation.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_1.html#stata-and-numerical-differentiation",
    "href": "mathref/math_1.html#stata-and-numerical-differentiation",
    "title": "Math Refresher",
    "section": "Stata and Numerical Differentiation",
    "text": "Stata and Numerical Differentiation\nStata can be used to calculate numerical derivatives. mata (matrix algebra language) has powerful rutines for numerical differentiation. Stata also has some capabilities, and you can always do it manually.\n\n\nCode\nclear\nrange x -_pi _pi 100\ngen y = sin(x)\ngen dydx = (sin(x+0.01) - sin(x)) / 0.01\ndydx y x, gen(dydx2)\ngen dydx3 = cos(x)\ngen diff1 = (dydx - dydx3)\ngen diff2 = (dydx2 - dydx3)\nline diff1 diff2 x\n\n\n\n\n\nNumber of observations (_N) was 0, now 100.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_1.html#derivatives-of-common-functions",
    "href": "mathref/math_1.html#derivatives-of-common-functions",
    "title": "Math Refresher",
    "section": "Derivatives of Common Functions",
    "text": "Derivatives of Common Functions\nFor most common functions, the derivative can be calculated using the following rules:\n\nThe derivative of a constant is zero.\n\nThe derivative of \\(x^n\\) is \\(nx^{n-1}\\).\n\nThe derivative of \\(\\ln(x)\\) is \\(\\frac{1}{x}\\).\n\nThe derivative of \\(e^x\\) is \\(e^x\\).\n\nThe derivative of \\(a^x\\) is \\(a^x \\ln a\\).\n\nThere are other rules for derivatives, but these are the ones that will be used most often.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_1.html#derivatives-of-composite-functions",
    "href": "mathref/math_1.html#derivatives-of-composite-functions",
    "title": "Math Refresher",
    "section": "Derivatives of Composite Functions",
    "text": "Derivatives of Composite Functions\nThe derivative of a composite function \\(f(g(x))\\) is given by the chain rule:\n\\[\n\\frac{d}{dx} f(g(x)) = f'(g(x)) \\cdot g'(x).\n\\]\nFor example, consider the function \\(f(x) = \\ln(x^2)\\). The derivative of \\(f(x)\\) is:\n\\[\n\\begin{aligned}\n\\frac{d}{dx} \\ln(x^2) &= \\frac{1}{x^2} \\cdot \\frac{d}{dx} (x^2) \\\\\n&= \\frac{1}{x^2} \\cdot 2x \\\\\n&= \\frac{2}{x}.\n\\end{aligned}\n\\]",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_1.html#derivatives-of-sums-and-products",
    "href": "mathref/math_1.html#derivatives-of-sums-and-products",
    "title": "Math Refresher",
    "section": "Derivatives of Sums and Products",
    "text": "Derivatives of Sums and Products\nThe derivative of a sum of functions is the sum of the derivatives of the functions:\n\\[\n\\frac{d}{dx} (f(x) + g(x)) = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x).\n\\]\nThe derivative of a product of functions is given by the product rule:\n\\[\n\\frac{d}{dx} (f(x) \\cdot g(x)) = f'(x) \\cdot g(x) + f(x) \\cdot g'(x).\n\\]\nThe derivative of a quotient of functions is given by the quotient rule:\n\\[\n\\frac{d}{dx} \\left( \\frac{f(x)}{g(x)} \\right) = \\frac{f'(x) \\cdot g(x) - f(x) \\cdot g'(x)}{g(x)^2}.\n\\]\nThis is a special case of the product rule.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_1.html#maximization-and-minimization",
    "href": "mathref/math_1.html#maximization-and-minimization",
    "title": "Math Refresher",
    "section": "Maximization and Minimization",
    "text": "Maximization and Minimization\n\nDerivatives can be used to identify the maximum and minimum values of a function. Consider a function \\(f(x)\\).\nTo find the maximum (or minimum) value of \\(f(x)\\), we take the derivative of \\(f(x)\\) and set it equal to zero.\n\nThis is called the first-order condition.\nThe idea is that at the maximum (or minimum), the value of \\(f(x)\\) shouldn’t change anymore (it should be flat). Thus, the derivative of \\(f(x)\\) should be zero.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_1.html#section-1",
    "href": "mathref/math_1.html#section-1",
    "title": "Math Refresher",
    "section": "",
    "text": "For example, consider the function \\(f(x) = 5x^2 - 4x + 2\\). The derivative of \\(f(x)\\) is:\n\\[\n\\begin{aligned}\nf'(x) &= 10x - 4 = 0 \\\\\nx &= \\frac{4}{10} = 0.4.\n\\end{aligned}\n\\]\nSo when \\(x\\) is equal to 0.4, the function \\(f(x)\\) does not change anymore.\n\nThis, however, is insufficient to determine whether the function is at a maximum or a minimum.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_1.html#section-2",
    "href": "mathref/math_1.html#section-2",
    "title": "Math Refresher",
    "section": "",
    "text": "To determine this, we take the second derivative of \\(f(x)\\), known as the second-order condition:\n\\[\nf''(x) = 10 &gt; 0.\n\\]\n\nBecause the second derivative is positive, we know that \\(f(x)\\) is at a minimum when \\(x = 0.4\\).\n\nIf the second derivative were negative, we would know that \\(f(x)\\) is at a maximum when \\(x = 0.4\\).",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_1.html#why-is-this-the-case",
    "href": "mathref/math_1.html#why-is-this-the-case",
    "title": "Math Refresher",
    "section": "Why is this the case?",
    "text": "Why is this the case?\n\n\\(f'(x)\\) measures the changes in \\(f(x)\\) along \\(x\\). When \\(f'(x) = 0\\), \\(f(x)\\) is not changing anymore.\n\\(f''(x)\\) measures the changes in \\(f'(x)\\) (the changes in those changes).\n\nBecause it is positive, we know that \\(f'(x)\\) is increasing. This means that at \\(x = 0.4\\), the changes in \\(f(x)\\) are going from negative to positive, indicating a minimum.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_1.html#optimization-with-multiple-variables",
    "href": "mathref/math_1.html#optimization-with-multiple-variables",
    "title": "Math Refresher",
    "section": "Optimization with Multiple Variables",
    "text": "Optimization with Multiple Variables\nWhen considering multiple variables, we also need to rely on the first- and second-order conditions to find minimum and maximum values. Consider a function \\(f(x, y)\\). The first-order conditions are:\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial x} f(x, y) &= 0, \\\\\n\\frac{\\partial}{\\partial y} f(x, y) &= 0.\n\\end{aligned}\n\\]\nThese conditions indicate that, in the direction of \\(x\\) and \\(y\\), the function \\(f(x, y)\\) is not changing anymore. Thus, we have a potential maximum or minimum. To identify a minimum, we need second-order conditions:\n\\[H = \\begin{bmatrix}\nf_{xx} & f_{xy} \\\\\nf_{yx} & f_{yy}\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_1.html#section-3",
    "href": "mathref/math_1.html#section-3",
    "title": "Math Refresher",
    "section": "",
    "text": "\\[H = \\begin{bmatrix}\nf_{xx} & f_{xy} \\\\\nf_{yx} & f_{yy}\n\\end{bmatrix}\n\\]\nwhere \\(H\\) is the Hessian matrix.\n\nIf \\(\\text{Det}(H) &gt; 0\\) and \\(f_{xx} &gt; 0\\), then we have a minimum.\nIf \\(\\text{Det}(H) &gt; 0\\) and \\(f_{xx} &lt; 0\\), then we have a maximum.\nIf \\(\\text{Det}(H) &lt; 0\\), then we have a saddle point.\nIf \\(\\text{Det}(H) = 0\\), the result is inconclusive.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_1.html#optimization-with-constraints",
    "href": "mathref/math_1.html#optimization-with-constraints",
    "title": "Math Refresher",
    "section": "Optimization with Constraints",
    "text": "Optimization with Constraints\nWhen optimizing a function with constraints, we can use the method of Lagrange multipliers. Consider a function \\(f(x, y)\\) subject to the constraint \\(g(x, y) = z\\). The Lagrangian is:\n\\[\nL(x, y, \\lambda) = f(x, y) + \\lambda (z - g(x, y)).\n\\]\n\nThe Lagrangian is the function \\(f(x, y)\\) plus the constraint \\(g(x, y)\\) multiplied by a constant \\(\\lambda\\).\nThe constant \\(\\lambda\\) is called the Lagrange multiplier.\nThe constraint is written as the difference between the constant \\(z\\) and the function \\(g(x, y)\\).\nThe Lagrangian is then optimized with respect to \\(x\\), \\(y\\), and \\(\\lambda\\).",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_1.html#section-4",
    "href": "mathref/math_1.html#section-4",
    "title": "Math Refresher",
    "section": "",
    "text": "These are the equivalent first-order conditions:\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial x} L(x, y, \\lambda) &= 0, \\\\\n\\frac{\\partial}{\\partial y} L(x, y, \\lambda) &= 0, \\\\\n\\frac{\\partial}{\\partial\\lambda} L(x, y, \\lambda) &= z - g(x, y) = 0.\n\\end{aligned}\n\\]\nThe last condition is the constraint, and it implies that the constraint must be satisfied. The second-order conditions are the same as before.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_3.html#random-variables",
    "href": "mathref/math_3.html#random-variables",
    "title": "Math Refresher",
    "section": "Random Variables",
    "text": "Random Variables\n\nA random variable is a variable whose value is determined by the outcome of a random experiment.\n\nFor example, if we toss a coin, the outcome is random, but the possible values of \\(X\\) are 0 and 1.\nIf we roll a die, the outcome is random with possible values 1, 2, 3, 4, 5, and 6.\nExact temperature in a room\n\n\nThere are two kinds of random variables:\n\nDiscrete random variables can only take on a finite number of values. For example, the number of heads in 10 coin tosses is a discrete random variable.\n\nThe probability of observing a particular value is not always zero\n\nContinuous random variables can take on any value in a range. For example, the height of a randomly selected person is a continuous random variable.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_3.html#section",
    "href": "mathref/math_3.html#section",
    "title": "Math Refresher",
    "section": "",
    "text": "If \\(X\\) is discrete random variable, then \\(P(X=c)\\) is the probability that \\(X\\) takes on the value \\(c\\). It can be any value between 0 and 1. ()\nBy definition, the sum of all probabilities for all feasible values of \\(X\\) is 1. That is, \\(\\sum_{c} P(X=c)=1\\).\nIf \\(X\\) is continuous random variable, then \\(P(X=c)=0\\) for any value \\(c\\).\n\nThe probability to observe a particular number is zero.\nInstead, when using continuous data, we focus on the probability of observing a value in a range. For example, \\(P(1.7 \\leq X \\leq 1.8)\\) is the probability that \\(X\\) is between 1.7 and 1.8, which can be any value between 0 and 1.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_3.html#stata-and-random-variables",
    "href": "mathref/math_3.html#stata-and-random-variables",
    "title": "Math Refresher",
    "section": "Stata and Random Variables",
    "text": "Stata and Random Variables\n\nComputers CANNOT generate random numbers. They can only generate pseudo-random numbers.\n\nRandom numbers cannot be reproduced.\nPseudo-random numbers can be reproduced, if we know initial conditions. (seed)\n\nFor most purposes, pseudo-random numbers are good enough.\n\n\nStata has many built-in function to generate random numbers.\n\nhelp random for more information.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_3.html#probability-distributions",
    "href": "mathref/math_3.html#probability-distributions",
    "title": "Math Refresher",
    "section": "Probability Distributions",
    "text": "Probability Distributions\n\nA probability distribution is a function that assigns probabilities to the values of a random variable.\n\nFor discrete random variables, we can use a table to describe the probability distribution. For example, the probability distribution of the number of heads in 5 coin tosses is:\n\n\n\n\n\nNumber of heads\nProbability\n\n\n\n\n0\n0.03125\n\n\n1\n0.15625\n\n\n2\n0.3125\n\n\n3\n0.3125\n\n\n4\n0.15625\n\n\n5\n0.03125\n\n\n\nIn this case, the sum of all probabilities is 1.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_3.html#probability-density-functions",
    "href": "mathref/math_3.html#probability-density-functions",
    "title": "Math Refresher",
    "section": "Probability Density Functions",
    "text": "Probability Density Functions\n\nFor continuous random variables, we can use a function to describe the probability distribution.\n\nFor example, we can say that the probability distribution of the height of a randomly selected person is:\n\n\n\\[f(x)\\]\nThis function has important properties:\n\n\\(f(x) \\geq 0\\) for all \\(x\\).\n\\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\).\n\\(P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) dx\\).\n\\(P(X \\leq a) + P(X &gt; a) = 1\\).\n\\(P(a \\leq X \\leq b) = P(X &lt; b) - P(X &lt; a)\\).",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_3.html#stata-and-empirical-distributions",
    "href": "mathref/math_3.html#stata-and-empirical-distributions",
    "title": "Math Refresher",
    "section": "Stata and Empirical Distributions",
    "text": "Stata and Empirical Distributions\n\nTheoryDiscreteContinuous\n\n\n\nGiven a dataset, you can use different tools to estimate the probability distribution or the probability density function of a random variable.\n\nFor example, you can use histograms, or frequency tables, to estimate the probability distribution of a discrete random variable.\nYou can use kernel density plots to estimate the probability density function of a continuous random variable.\n\n\n\n\n\n\nCode\nsysuse nlsw88.dta, clear\nreplace grade  = 11 if grade &lt;11\nfre grade\n\n\n\n\n\n(NLSW, 1988 extract)\n(211 real changes made)\n\ngrade -- Current grade completed\n-----------------------------------------------------------\n              |      Freq.    Percent      Valid       Cum.\n--------------+--------------------------------------------\nValid   11    |        334      14.87      14.88      14.88\n        12    |        943      41.99      42.02      56.91\n        13    |        176       7.84       7.84      64.75\n        14    |        187       8.33       8.33      73.08\n        15    |         92       4.10       4.10      77.18\n        16    |        252      11.22      11.23      88.41\n        17    |        106       4.72       4.72      93.14\n        18    |        154       6.86       6.86     100.00\n        Total |       2244      99.91     100.00           \nMissing .     |          2       0.09                      \nTotal         |       2246     100.00                      \n-----------------------------------------------------------\n\n\n\n\n\n\nCode\nkdensity wage, scale(1.25) title(\"Wage Density f(X)\")",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_3.html#joint-probability-distributions",
    "href": "mathref/math_3.html#joint-probability-distributions",
    "title": "Math Refresher",
    "section": "Joint Probability Distributions",
    "text": "Joint Probability Distributions\n\nThe joint probability distribution of \\(X\\) and \\(Y\\) is a function that assigns probabilities to the values of \\(X\\) and \\(Y\\).\nFor discrete random variables, we can use a table to describe the joint probability distribution.\n\n\n\nCode\ntab race married, cell nofreq\n\n\n\n           |        Married\n      Race |    Single    Married |     Total\n-----------+----------------------+----------\n     White |     21.68      51.20 |     72.89 \n     Black |     13.76      12.20 |     25.96 \n     Other |      0.36       0.80 |      1.16 \n-----------+----------------------+----------\n     Total |     35.80      64.20 |    100.00 \n\n\n\nIt must be the case that the sum of all probabilities is 1.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_3.html#section-1",
    "href": "mathref/math_3.html#section-1",
    "title": "Math Refresher",
    "section": "",
    "text": "For continuous variables, estimation and graphical representation is tricky\nit must be the case that:\n\n\\[\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x,y) dx dy = 1\\]\n\nYou may be able to use scatter plots, or contour plots, to represent the joint probability distribution of two continuous random variables.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_3.html#section-2",
    "href": "mathref/math_3.html#section-2",
    "title": "Math Refresher",
    "section": "",
    "text": "Scatter PlotHeatplotBiDensity\n\n\n\n\nCode\nscatter wage ttl_exp , msize(2) mcolor(%10)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nqui:ssc install heatplot\nheatplot wage ttl_exp , \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nqui:ssc install bidensity\nbidensity wage ttl_exp, levels(10)",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_3.html#conditional-probability",
    "href": "mathref/math_3.html#conditional-probability",
    "title": "Math Refresher",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nThe conditional probability of \\(X\\) given \\(Y\\) is:\n\\[P(x|y) = \\frac{P(x,y)}{P(y)}\\]\nor, the conditional probabilty density function:\n\\[f(x|y) = \\frac{f(x,y)}{f(y)}\\]\nAnd if \\(X\\) and \\(Y\\) are independent, then:\n\\(P(x|y) = P(x)\\) or \\(f(x|y) = f(x)\\).",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_3.html#marginal-probability-distributions",
    "href": "mathref/math_3.html#marginal-probability-distributions",
    "title": "Math Refresher",
    "section": "Marginal Probability Distributions",
    "text": "Marginal Probability Distributions\nThe marginal probability distribution of \\(X\\) is the probability distribution of \\(X\\) ignoring/regardless the values of \\(Y\\). This can be expressed as:\n\\[P(x) = \\sum_{z=-\\infty}^{\\infty} P(X=x,y=z) \\text{ or }\nf_x(x) = \\int_{z=-\\infty}^{\\infty} f(x,z)dz\\]\nThis is also refer to “integrating out” the variable \\(Y\\) or averaging over \\(Y\\).\n\\[P(x) = \\sum_{z=-\\infty}^{\\infty} P(X=x|y=z)P_y(z) \\text{ or }\nf_x(x) = \\int_{z=-\\infty}^{\\infty} f(x|z)f_y(z)dz\\]",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_3.html#independence",
    "href": "mathref/math_3.html#independence",
    "title": "Math Refresher",
    "section": "Independence",
    "text": "Independence\nTwo random variables \\(X\\) and \\(Y\\) are independent if and only if:\n\\[P(x,y) = P(x)P(y) \\text{ or } f(x,y)=f(x)*f(y)\\]\nThat means the conditional probability of \\(X\\) given \\(Y\\) is the same as the marginal probability of \\(X\\).\n\\(P(x|y) = P(x)\\) or \\(f(x|y) = f(x)\\).",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_3.html#summary-statistics",
    "href": "mathref/math_3.html#summary-statistics",
    "title": "Math Refresher",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nGiven a random variable \\(X\\), there are several summary statistics that can be used to describe the distribution of \\(X\\), without describing the entire distribution",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_3.html#central-tendency",
    "href": "mathref/math_3.html#central-tendency",
    "title": "Math Refresher",
    "section": "Central Tendency",
    "text": "Central Tendency\n\nMean: average value of \\(X\\).\n\n\\[\\bar x = E(X) = \\sum_{x} xP(X=x) \\text{ or } E(X) = \\int_{-\\infty}^{\\infty} xf(x)dx\\]\n\nMedian: middle value of \\(X\\).\nPercentile: values that identify the boundaries of the distribuion. Median is the 50th percentile.\n\n\\[Q_y(p) = E(Y \\leq Q_y) = p\\]\n\nMode: most frequent value of \\(X\\).\n\nsum var,d in Stata will give you the mean, median, and selected quantiles.\nmode can be estimated using egen, or based on empirical distribution.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_3.html#dispersion",
    "href": "mathref/math_3.html#dispersion",
    "title": "Math Refresher",
    "section": "Dispersion",
    "text": "Dispersion\n\nVariance: Average squared deviation from the mean.\n\n\\[Var(X) = E(X-\\mu)^2 = \\sum_{x} (x-\\mu)^2P(X=x) \\text{ or } Var(X) = \\int_{-\\infty}^{\\infty} (x-\\mu)^2f(x)dx\\]\n\nStandard deviation: square root of the variance. Easier to interpret.\nRange: difference between the maximum and minimum values of \\(X\\).\nInterquartile range: difference between the 75th and 25th percentiles of \\(X\\).\n\nsum var,d and tabstat can provide you with most of this information.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_3.html#some-useful-distributions",
    "href": "mathref/math_3.html#some-useful-distributions",
    "title": "Math Refresher",
    "section": "Some useful distributions",
    "text": "Some useful distributions\nDiscrete distributions\n\nBernoulli distribution: \\(X \\sim Bernoulli(p)\\), where \\(p \\in [0,1]\\).\n\n\\(E(X)=p\\) and variance \\(Var(X)=p(1-p)\\).\nFlip a coin with probability \\(p\\) of getting heads.\nrbinomial(1, p)\n\nBinomial distribution: \\(X \\sim Binomial(n,p)\\), where \\(p \\in [0,1]\\) and \\(n&gt;0\\)\n\n\\(E(x)=np\\) and \\(Var(X)=np(1-p)\\).\nDistribution of the number of successes in \\(n\\) independent Bernoulli trials.\nrbinomial(n, p)\n\nPoisson distribution: \\(X \\sim Poisson(\\lambda)\\), where \\(\\lambda&gt;0\\)\n\n\\(E(X)=Var(x)=\\lambda\\), Typically used for counts.\nFor example, the number of customers arriving at a store in a given hour.\nrpoisson(lambda)",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "mathref/math_3.html#section-3",
    "href": "mathref/math_3.html#section-3",
    "title": "Math Refresher",
    "section": "",
    "text": "Continuous distributions\n\nUniform distribution: \\(X \\sim Uniform(a,b)\\)\n\n\\(f(x)=\\frac{1}{b-a}\\) for \\(a \\leq x \\leq b\\), and \\(f(x)=0\\) otherwise.\n\\(E(X)=\\frac{a+b}{2}\\) and \\(Var(X)=\\frac{(b-a)^2}{12}\\).\nruniform(a, b)\n\nNormal distribution: \\(X \\sim Normal(\\mu,\\sigma^2)\\)\n\n\\(f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\).\n\\(E(X)=\\mu\\) and \\(Var(X)=\\sigma^2\\).\nrnormal(mu, sigma)\n\n\nOther useful distributions include:\n\nt-distribution, Chi-squared distribution, F-distribution\n\nhelp density_functions help random_number_functions",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher"
    ]
  },
  {
    "objectID": "proposal_assessment.html",
    "href": "proposal_assessment.html",
    "title": "Critical Assessment: ESG and Financial Performance Research Proposal",
    "section": "",
    "text": "The research proposal aims to investigate the relationship between ESG (Environmental, Social, and Governance) compliance and financial performance in the oil and energy sectors. While the objective is timely and relevant, several aspects require refinement for enhanced academic rigor and practical viability."
  },
  {
    "objectID": "proposal_assessment.html#research-relevance",
    "href": "proposal_assessment.html#research-relevance",
    "title": "Critical Assessment: ESG and Financial Performance Research Proposal",
    "section": "2.1 Research Relevance",
    "text": "2.1 Research Relevance\n\nAddresses a critical gap in understanding ESG impact on financial performance\nFocus on oil and energy sectors is particularly relevant given current global sustainability challenges\nComparative approach between ESG and non-ESG companies provides clear contrasts"
  },
  {
    "objectID": "proposal_assessment.html#data-sources",
    "href": "proposal_assessment.html#data-sources",
    "title": "Critical Assessment: ESG and Financial Performance Research Proposal",
    "section": "2.2 Data Sources",
    "text": "2.2 Data Sources\n\nComprehensive range of data sources identified:\n\nSEC.gov (EDGAR Access)\nMarketLine reports\nFinancial databases (Yahoo Finance, Google Finance)\nESG performance sources (Calvert)\n\nMix of qualitative and quantitative data points"
  },
  {
    "objectID": "proposal_assessment.html#methodological-concerns",
    "href": "proposal_assessment.html#methodological-concerns",
    "title": "Critical Assessment: ESG and Financial Performance Research Proposal",
    "section": "3.1 Methodological Concerns",
    "text": "3.1 Methodological Concerns\n\n3.1.1 Sample Size Limitations\n\nCurrent case study approach (2 pairs of companies) is too narrow\n\nLimited statistical significance\nMay not be representative of industry-wide trends\nSusceptible to company-specific factors\n\n\n\n\n3.1.2 Variable Selection\n\nFinancial Metrics\n\nCurrent metrics are comprehensive but need clearer temporal boundaries\nNeed to account for industry-specific cyclical patterns\nShould consider lag effects between ESG implementation and financial outcomes\n\nESG Metrics\n\nNeed clearer definition of what constitutes “ESG compliance”\nShould address potential variations in ESG rating methodologies across different providers\nNeed to account for changes in ESG criteria over time"
  },
  {
    "objectID": "proposal_assessment.html#statistical-framework",
    "href": "proposal_assessment.html#statistical-framework",
    "title": "Critical Assessment: ESG and Financial Performance Research Proposal",
    "section": "3.2 Statistical Framework",
    "text": "3.2 Statistical Framework\n\n3.2.1 Methodology Option #1 (Mutual Fund Analysis)\nLimitations: - Selection bias in mutual fund composition - Difficulty in isolating ESG effects from other investment criteria - Complex fee structures may affect performance metrics\nImprovements Needed: 1. Define specific mutual fund selection criteria 2. Develop controls for fund size and investment strategy 3. Account for market timing effects\n\n\n3.2.2 Methodology Option #2 (Sector-Wide Comparison)\nStrengths: - Broader sample size - More robust statistical analysis possible - Better industry representation\nRecommended Enhancements: 1. Include panel data analysis 2. Add control variables for: - Company size - Geographic location - Regulatory environment - Market conditions"
  },
  {
    "objectID": "proposal_assessment.html#phase-1-data-collection-and-preparation-3-4-months",
    "href": "proposal_assessment.html#phase-1-data-collection-and-preparation-3-4-months",
    "title": "Critical Assessment: ESG and Financial Performance Research Proposal",
    "section": "4.1 Phase 1: Data Collection and Preparation (3-4 months)",
    "text": "4.1 Phase 1: Data Collection and Preparation (3-4 months)\n\nSample Selection\n\n# Pseudocode for sample selection\ncompanies &lt;- select_companies(\n  sector = c(\"oil\", \"energy\"),\n  market_cap_threshold = 1B,\n  years = 2015:2023\n)\n\nData Collection Framework\n\nFinancial metrics (quarterly data)\nESG scores (annual data)\nMarket indicators\nControl variables"
  },
  {
    "objectID": "proposal_assessment.html#phase-2-statistical-analysis-2-3-months",
    "href": "proposal_assessment.html#phase-2-statistical-analysis-2-3-months",
    "title": "Critical Assessment: ESG and Financial Performance Research Proposal",
    "section": "4.2 Phase 2: Statistical Analysis (2-3 months)",
    "text": "4.2 Phase 2: Statistical Analysis (2-3 months)\n\nPanel Data Analysis\n\n# Fixed Effects Model\nmodel_fe &lt;- plm(\n  financial_performance ~ esg_score + controls,\n  data = panel_data,\n  index = c(\"company\", \"year\"),\n  model = \"within\"\n)\n\nRobustness Checks\n\nHausman test for model specification\nSensitivity analysis for ESG metrics\nAlternative performance measures"
  },
  {
    "objectID": "proposal_assessment.html#phase-3-causality-analysis-2-3-months",
    "href": "proposal_assessment.html#phase-3-causality-analysis-2-3-months",
    "title": "Critical Assessment: ESG and Financial Performance Research Proposal",
    "section": "4.3 Phase 3: Causality Analysis (2-3 months)",
    "text": "4.3 Phase 3: Causality Analysis (2-3 months)\n\nDifference-in-Differences\n\nExploit ESG policy changes\nNatural experiments in regulatory environment\n\nInstrumental Variables\n\nIndustry-specific instruments\nRegulatory changes as instruments"
  },
  {
    "objectID": "proposal_assessment.html#technical-viability",
    "href": "proposal_assessment.html#technical-viability",
    "title": "Critical Assessment: ESG and Financial Performance Research Proposal",
    "section": "5.1 Technical Viability",
    "text": "5.1 Technical Viability\n\nData Availability: HIGH\n\nMost required data is publicly available\nSome ESG data may require subscription access\n\nMethodological Feasibility: MEDIUM\n\nStatistical approaches are well-established\nChallenge in establishing causality\nNeed for robust controls"
  },
  {
    "objectID": "proposal_assessment.html#resource-requirements",
    "href": "proposal_assessment.html#resource-requirements",
    "title": "Critical Assessment: ESG and Financial Performance Research Proposal",
    "section": "5.2 Resource Requirements",
    "text": "5.2 Resource Requirements\n\nData Resources\n\nFinancial database subscriptions\nESG rating database access\nComputing resources for large dataset analysis\n\nTechnical Skills\n\nStatistical software proficiency (R/Python)\nPanel data analysis expertise\nFinancial modeling experience"
  },
  {
    "objectID": "proposal_assessment.html#timeline-and-milestones",
    "href": "proposal_assessment.html#timeline-and-milestones",
    "title": "Critical Assessment: ESG and Financial Performance Research Proposal",
    "section": "5.3 Timeline and Milestones",
    "text": "5.3 Timeline and Milestones\n\n\n\nPhase\nDuration\nKey Deliverables\n\n\n\n\nData Collection\n3-4 months\nClean, merged dataset\n\n\nInitial Analysis\n2-3 months\nPreliminary results\n\n\nCausality Analysis\n2-3 months\nFinal statistical models\n\n\nDocumentation\n1-2 months\nResearch paper draft"
  },
  {
    "objectID": "proposal_assessment.html#primary-risks",
    "href": "proposal_assessment.html#primary-risks",
    "title": "Critical Assessment: ESG and Financial Performance Research Proposal",
    "section": "6.1 Primary Risks",
    "text": "6.1 Primary Risks\n\nData Quality\n\nRisk: Inconsistent ESG ratings across providers\nMitigation: Use multiple ESG data sources and create composite scores\n\nMethodological\n\nRisk: Endogeneity in ESG adoption\nMitigation: Robust instrumental variables and natural experiments\n\nExternal Validity\n\nRisk: Limited generalizability\nMitigation: Expand sample size and include international companies"
  },
  {
    "objectID": "proposal_assessment.html#contingency-plans",
    "href": "proposal_assessment.html#contingency-plans",
    "title": "Critical Assessment: ESG and Financial Performance Research Proposal",
    "section": "6.2 Contingency Plans",
    "text": "6.2 Contingency Plans\n\nData Availability Issues\n\nAlternative data sources identified\nSimplified models possible with reduced variable set\n\nStatistical Challenges\n\nMultiple methodological approaches prepared\nConsultation with statistical experts planned"
  },
  {
    "objectID": "proposal_assessment.html#immediate-actions",
    "href": "proposal_assessment.html#immediate-actions",
    "title": "Critical Assessment: ESG and Financial Performance Research Proposal",
    "section": "7.1 Immediate Actions",
    "text": "7.1 Immediate Actions\n\nMethodology Refinement\n\nAdopt Methodology Option #2 with enhancements\nDevelop detailed statistical analysis plan\nEstablish clear variable definitions\n\nData Strategy\n\nBegin with pilot data collection\nValidate data quality and availability\nEstablish data processing pipelines"
  },
  {
    "objectID": "proposal_assessment.html#long-term-considerations",
    "href": "proposal_assessment.html#long-term-considerations",
    "title": "Critical Assessment: ESG and Financial Performance Research Proposal",
    "section": "7.2 Long-term Considerations",
    "text": "7.2 Long-term Considerations\n\nScope Extension\n\nConsider international markets\nInclude additional energy subsectors\nExamine regulatory impact\n\nPublication Strategy\n\nTarget journals in:\n\nEnergy economics\nSustainable finance\nCorporate governance"
  },
  {
    "objectID": "quarto/stata_basics.html",
    "href": "quarto/stata_basics.html",
    "title": "Stata-output",
    "section": "",
    "text": "Analyzing Oaxaca dataset\nsmaller\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n&gt; (2)\n\n\n\n\n\nyears of education\n0.0885***\n0 &gt; .0794***\n\n\n\n(0.00519)\n(0.0 &gt; 0522)\n\n\n \n\n\nyears of work\n0.0153***\n0. &gt; 00399*\n\n\nexperience\n(0.00126)\n(0.0 &gt; 0188)\n\n\n \n\n\nyears of job tenure\n\n0. &gt; 00407*\n\n\n\n\n(0.0 &gt; 0196)\n\n\n \n\n\nage of respondent\n\n0 &gt; .0114***\n\n\n\n\n(0.0 &gt; 0174)\n\n\n \n\n\nConstant\n2.136***\n&gt; 1.915***\n\n\n\n(0.0654)\n(0. &gt; 0727)\n\n\n\n\n\nObservations\n1434\n&gt; 1434"
  },
  {
    "objectID": "quizes/quiz_1.html",
    "href": "quizes/quiz_1.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Name: ________________________________________________________\n\nHousehold Budget surveys are an example of:\n\n\npanel data\nexperimental data\ntime series data\ncross-sectional data\n\n\nA dependent variable is also known as a(n) _____.\n\n\nexplanatory variable\ncontrol variable\npredictor variable\nresponse variable\n\n\nThe Zero conditional mean assumption means\n\n\n\\(E(u│x)=0\\)\n\\(E(\\hat u | x)=0\\)\n\\(E(u^2│x)=0\\)\n\\(E(\\hat u ^2│x)=0\\)\n\n\nThe explained sum of squares (SSE) for the regression function, \\(y=\\beta_0 + \\beta_1 x + u\\), is defined as _____.\n\n\n\\(\\sum(\\hat y_i - \\bar{y})^2\\)\n\\(\\sum(y_i - \\hat y_i )^2\\)\n\\(\\sum \\hat u\\)\n\\(\\sum \\hat u^2\\)\n\n\nThe error term in a regression equation is said to exhibit homoskedasticty if _____.\n\n\nit has zero conditional mean\nit has the same variance for all values of the explanatory variable\nit has the same value for all values of the explanatory variable\nif the error term has a value of one given any value of the explanatory variable\n\n\nBonus: Why do we care about the Zero Conditional mean assumption?"
  },
  {
    "objectID": "quizes/quiz_1.html#quiz-1",
    "href": "quizes/quiz_1.html#quiz-1",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Name: ________________________________________________________\n\nHousehold Budget surveys are an example of:\n\n\npanel data\nexperimental data\ntime series data\ncross-sectional data\n\n\nA dependent variable is also known as a(n) _____.\n\n\nexplanatory variable\ncontrol variable\npredictor variable\nresponse variable\n\n\nThe Zero conditional mean assumption means\n\n\n\\(E(u│x)=0\\)\n\\(E(\\hat u | x)=0\\)\n\\(E(u^2│x)=0\\)\n\\(E(\\hat u ^2│x)=0\\)\n\n\nThe explained sum of squares (SSE) for the regression function, \\(y=\\beta_0 + \\beta_1 x + u\\), is defined as _____.\n\n\n\\(\\sum(\\hat y_i - \\bar{y})^2\\)\n\\(\\sum(y_i - \\hat y_i )^2\\)\n\\(\\sum \\hat u\\)\n\\(\\sum \\hat u^2\\)\n\n\nThe error term in a regression equation is said to exhibit homoskedasticty if _____.\n\n\nit has zero conditional mean\nit has the same variance for all values of the explanatory variable\nit has the same value for all values of the explanatory variable\nif the error term has a value of one given any value of the explanatory variable\n\n\nBonus: Why do we care about the Zero Conditional mean assumption?"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "The goal of the class is for you to become familiar and proficient with some essential tools that are used in most empirical analysis.\nWhile learning to implement all the methods we cover by hand is a great excercise to learn what they do, and how they work, it may not be a feasible practice in most real-world work, unless you decide to follow that path (Econometrics/applied Econometrics).\nFor this purpose, the main software we will use in this class (as evidence from all the code shared in the slides) its Stata. A self contained program that is yet flexible enough to add custom add on programs/commands.\nNevertheless, if you are new to Stata, there are quite few resources you may want to look into using this software"
  },
  {
    "objectID": "resources.html#stata",
    "href": "resources.html#stata",
    "title": "Resources",
    "section": "Stata",
    "text": "Stata\n\nStata Free-Webinars: https://www.stata.com/training/webinar/\nStata Past Recorded Webinars: https://www.stata.com/training/webinar_series/past-webinar-recordings/\nStata Video-Tutorials: https://www.stata.com/links/video-tutorials/\nGeneral Learning resources: https://www.stata.com/links/resources-for-learning-stata/\nExcellent Stata tutorial for beginners: https://grodri.github.io/stata/index\nOur own Tutorial! Stata Basics\nAlso, you may want to check the following for a quick reference on\n\nData Management\nPublication Ready Tables\nGraphs\n\n\nBut of course, Stata is not free. There are other resources you may want to explore, if you are interested in doing econometric analysis, but no longer have access to Stata. These are R, Python and Julia."
  },
  {
    "objectID": "resources.html#r-julia-python",
    "href": "resources.html#r-julia-python",
    "title": "Resources",
    "section": "R, Julia, Python",
    "text": "R, Julia, Python\nThese software are free, but usually require add-ons from different sources to estimate specialized models. They also have a steep, or rather steep-er (than Stata) learning curve. However, it is smart to learn other languages, at least to implement basic analysis. One resource you may find very convinient is the following:\n\nR, Python, Julia: http://www.upfie.net/\n\nThis site and its author(s) have put together a set of companion books to go along with the Textbook “Introductory Econometrics: A Modern Approach”. These books are rather inexpensive, providing some of the authors own insights, with full code in all three languages, that replicate the examples in the textbook.\n\nExample Codes: http://www.upfie.net/code.html\n\nThe authors also suggest other resources that could be of interest\n\nFurther Resources: http://www.upfie.net/links.html"
  },
  {
    "objectID": "resources.html#quarto",
    "href": "resources.html#quarto",
    "title": "Resources",
    "section": "Quarto",
    "text": "Quarto\nQuarto is not a programming language. Rather an interpreter that converts plain text to nicely formating documents, presentations, websites, etc. This site, for instance, was built using Quarto.\nBecause of this, I’m encouraging the use of Quarto, combined with nbstata/python, to produce answers to ALL homeworks or group works. So it will be easy to check and cross check your work with the code.\nTo use this, you need to have R-Studio here, or Visual Studio Code here with Quarto plug-in (if you use VSC) in your computers. You will also need python and nbstata.\nA good place to start learning how to use Quarto for dynamic documents its here (for R-studio) or here (for VCS).\nI also have a small example using Quarto with Stata here.\nTry it on, and let me know if you have any problems."
  },
  {
    "objectID": "resources.html#other-resources-data",
    "href": "resources.html#other-resources-data",
    "title": "Resources",
    "section": "Other Resources: Data",
    "text": "Other Resources: Data\nIf you are looking for small and easy to work datasets for your projects, you may want to check the following:\n\nfrause: Its a repository I created to load data from the web into Stata. Contains all datasets used in Intro to Ecometrics book by Wooldridge.\ndatasciencedojo: This is a nice website that provides access to raw data that could be used for your projects. See their webpage."
  },
  {
    "objectID": "rm-data/homework.html",
    "href": "rm-data/homework.html",
    "title": "HomeWorks",
    "section": "",
    "text": "This document provides the homework for the each week of the course."
  },
  {
    "objectID": "rm-data/homework.html#hw01",
    "href": "rm-data/homework.html#hw01",
    "title": "HomeWorks",
    "section": "Homework 1",
    "text": "Homework 1\nChoose one of the following topics, and create a PDF version of it using Quarto. For this, create a new repository in your GitHub account, this repository should be named homework_1, and contain the QMD file that you will use to create the PDF.\nFor this homework use the following resources\n\nTemplate: template html\n\nJust copy the Heading of this file in your QMD file\n\nBibliography: reference.bib\n\nAdd this in the same folder as your QMD file\n\nAll figures, if any, can be saved as PNG or JPG files from the linked pages.\nTables, if any, may have to be replicated using markdown tables.\n\nSubmit the link to your repository, the PDF and QMD files via email.\n\nTopics\n\nThe Impact of Resource Management in StarCraft: A Strategic Analysis My replication\n\nhtml\npdf\nqmd\n\nThe Strategic Depth of StarCraft and Its Esports Legacy html pdf\nThe Mathematics of Dungeons and Dragons: A Statistical Adventure html pdf\nThe Rise of LitRPG: Blending Literature and Gaming html pdf\nThe Impact of ‘The Good Guys’ on Modern Fantasy Literature html pdf\nEconomic Dynamics in Eric Ugland’s ‘The Good Guys’ Series html pdf\nThe Impact of House Allegiances on Power Dynamics in Westeros html pdf"
  },
  {
    "objectID": "rm-data/homework.html#hw02",
    "href": "rm-data/homework.html#hw02",
    "title": "HomeWorks",
    "section": "Homework 2",
    "text": "Homework 2\n\nDownload country–year panel data on three variables (“indicators”) of your choice from the World Bank website https://data.worldbank.org/, or using their API program (wbdata).\nOnce you have the data, clean it up and save a tidy version of it in a Stata file.\nIndicate which countries are ranked highest and lowest in each of the three indicators in the year 2000.\nWrite a short report on what you did to obtain the data, how many countries and years you ended up with in the data (after cleannig), and what difficulties you encountered, if any.\n\nIt is not necessary to download the data directly into Stata. You can download the data in CSV format, excel, and then Copy-paste or import it into Stata."
  },
  {
    "objectID": "rm-data/homework.html#hw03",
    "href": "rm-data/homework.html#hw03",
    "title": "HomeWorks",
    "section": "Homework 3",
    "text": "Homework 3\n\nChoose the same 2016/7 season from the football dataset as in data exercise (Book) and produce a different table showing the extent of home team advantage. Compare the results and discuss what you find.\n\nor\n\nUsing the wms-management-survey dataset, pick a country different from Mexico, reproduce all figures and tables of the Book case study, and compare your results to what was found for Mexico."
  },
  {
    "objectID": "rm-data/homework.html#hw04",
    "href": "rm-data/homework.html#hw04",
    "title": "HomeWorks",
    "section": "Homework 4",
    "text": "Homework 4\nNothing here"
  },
  {
    "objectID": "rm-data/homework.html#hw05",
    "href": "rm-data/homework.html#hw05",
    "title": "HomeWorks",
    "section": "Homework 5",
    "text": "Homework 5\nNothing here"
  },
  {
    "objectID": "rm-data/homework.html#hw06",
    "href": "rm-data/homework.html#hw06",
    "title": "HomeWorks",
    "section": "Homework 6",
    "text": "Homework 6\nq1: Use the wms-management-survey dataset and pick a country. (Not two students should pick the same country). Estimate a linear regression with the management quality score (X) and employment (Y). Interpret the slope coefficient, create its 95% CI, and interpret that, too. Explore potential nonlinearities in the patterns of association by lpoly. Estimate a regression that can capture those nonlinearities, and carry out a test to see if you can reject that the linear approximation was good enough for the population of firms represented by the data."
  },
  {
    "objectID": "rm-data/homework.html#hw07",
    "href": "rm-data/homework.html#hw07",
    "title": "HomeWorks",
    "section": "Homework 7",
    "text": "Homework 7"
  },
  {
    "objectID": "rm-data/homework.html#hw08",
    "href": "rm-data/homework.html#hw08",
    "title": "HomeWorks",
    "section": "Homework 8",
    "text": "Homework 8"
  },
  {
    "objectID": "rm-data/homework.html#hw09",
    "href": "rm-data/homework.html#hw09",
    "title": "HomeWorks",
    "section": "Homework 9",
    "text": "Homework 9\nUse the stocks-prices dataset that can be found here. This dataset contains the closing prices for the SP500, Apple, Disney, GameStop, Meta and Nvidia at the end of the month. Using this data, choose one of the companies and reproduce the analysis of the Returns on a company stock and market returns case study (for example SP500 and Apple).\nThe full case study can be found here. You need to produce the same tables and figures as in the case study, with brief explanations of what you find. No need to use the Daily data, just the Monthly data."
  },
  {
    "objectID": "rm-data/homework.html#hw10",
    "href": "rm-data/homework.html#hw10",
    "title": "HomeWorks",
    "section": "Homework 10",
    "text": "Homework 10\nUse the data on house prices here. This dataset contains the prices of houses in a city, along with many other house charcteristics. You are obtaining 80% of the data to estimate the model. Build a model that you think its best to predict the price of a house. You can use any of the variables in the dataset, and you can create new variables if you think they are useful.\nDescribe at least 3 different specifications you tried, and which one you choose and why. The one with the best fit on the test data (I have access to) can skip next homework.\nSee the winner here"
  },
  {
    "objectID": "rm-data/homework.html#hw11",
    "href": "rm-data/homework.html#hw11",
    "title": "HomeWorks",
    "section": "Homework 11",
    "text": "Homework 11"
  },
  {
    "objectID": "rm-data/homework.html#hw12",
    "href": "rm-data/homework.html#hw12",
    "title": "HomeWorks",
    "section": "Homework 12",
    "text": "Homework 12"
  },
  {
    "objectID": "rm-data/homework.html#hw13",
    "href": "rm-data/homework.html#hw13",
    "title": "HomeWorks",
    "section": "Homework 13",
    "text": "Homework 13"
  },
  {
    "objectID": "rm-data/hw-material/report1-example.html",
    "href": "rm-data/hw-material/report1-example.html",
    "title": "Example Replication Report",
    "section": "",
    "text": "asdas This template replicates the structure and content of the report “The Impact of Resource Management in StarCraft: A Strategic Analysis” by Kathryn Janeway.\nIt considers all elements of the original report, but does not include the image for replication.\n\n---\ntitle: \"The Impact of Resource Management in StarCraft: A Strategic Analysis\"\nformat: \n    pdf:\n        documentclass: article \n        number-sections: true\n        margin-top: 1in\n        margin-bottom: 1in\n        margin-left: 1in\n        margin-right: 1in\n        linestretch:  1.5\n        fontsize: 12pt\n    html: default\nexecute: \n  echo: false\n  warning: false     \nauthor: \"Kathryn Janeway\"        \nbibliography: references.bib\n---\n\n# Introduction\n\nThis report examines the crucial role of resource management in the popular real-time strategy game StarCraft. We will explore how effective resource allocation influences gameplay dynamics and strategic decision-making. The analysis will include a mathematical model of resource gathering, a visualization of unit production rates, and a comparison of resource types across different races.\n\n# Resource Dynamics in StarCraft\n\n## Mathematical Model of Resource Gathering\n\nIn StarCraft, the rate of resource accumulation can be modeled using a simple differential equation. If we denote the amount of resources as $R$ and time as $t$, we can express the rate of change of resources as:\n\n$$\n\\frac{dR}{dt} = \\alpha N - \\beta P\n$${#eq-resource}\n\nWhere $\\alpha$ is the gathering rate per worker, $N$ is the number of workers, $\\beta$ is the consumption rate, and $P$ is the production rate of units or structures. This model, as shown in @eq-resource, forms the basis of the game's economic system [@choi2015].\n\n## Unit Production Rates\n\nTo illustrate the impact of resource management on unit production, we've created a visualization of unit production rates for different races in StarCraft.\n\n:::{#fig-production fig-pos=\"H\" }\n\n![](figure_production.png)\n\nUnit Production Rates by Race in StarCraft\n\n:::\n\nAs shown in @fig-production, the Zerg race has the highest unit production rate, reflecting their swarm-based strategy. This aligns with the game's design philosophy, where each race has unique strengths and weaknesses[^1].\n\n## Resource Types Comparison\n\nStarCraft features two primary resource types: minerals and vespene gas. Their availability and usage vary across races:\n\n:::{#tbl-resources tbl-pos=\"H\"}\n\n| Race    | Mineral Usage | Gas Usage | Resource Dependency |\n|---------|---------------|-----------|---------------------|\n| Terran  | High          | Medium    | Balanced            |\n| Protoss | Medium        | High      | Gas-heavy           |\n| Zerg    | High          | Low       | Mineral-heavy       |\n\nResource Usage by Race \n\n:::\n\n@tbl-resources illustrates how different races prioritize resources, influencing their strategic options and tech progression paths.\n\n# Strategic Implications\n\nThe resource management system in StarCraft creates a complex strategic landscape. Players must balance immediate needs with long-term goals, deciding whether to invest in economy, technology, or military strength. According to @kim2010, successful players often demonstrate superior resource management skills, which translate into strategic advantages on the battlefield.\n\n# Conclusion\n\nResource management is a cornerstone of StarCraft gameplay, deeply influencing strategic decisions and overall game outcomes. The mathematical model, production rate analysis, and resource usage comparison presented in this report highlight the intricate balance between gathering, allocation, and consumption of resources. Understanding these dynamics is crucial for players aiming to master the game and for game designers seeking to create balanced and engaging strategy games.\n\n[^1]: This design approach contributes to StarCraft's enduring popularity in esports and casual gaming circles.\n\n# References\n\n::: {#refs}\n:::"
  },
  {
    "objectID": "rm-data/hw-material/report2.html",
    "href": "rm-data/hw-material/report2.html",
    "title": "The Strategic Depth of StarCraft and Its Esports Legacy",
    "section": "",
    "text": "StarCraft, a real-time strategy game developed by Blizzard Entertainment, has left an indelible mark on the esports landscape. This report explores the strategic complexity of StarCraft and its enduring impact on competitive gaming. We will examine the game’s mechanics, its influence on player decision-making, and its role in shaping the modern esports industry."
  },
  {
    "objectID": "rm-data/hw-material/report2.html#player-decision-making-and-apm",
    "href": "rm-data/hw-material/report2.html#player-decision-making-and-apm",
    "title": "The Strategic Depth of StarCraft and Its Esports Legacy",
    "section": "Player Decision-Making and APM",
    "text": "Player Decision-Making and APM\nOne of the key metrics in competitive StarCraft is Actions Per Minute (APM). This measure reflects a player’s ability to execute complex strategies rapidly. Kim et al. (2016) found a strong correlation between APM and player skill level. The following figure illustrates this relationship:\n\n\n\n\n\n\n\n\nFigure 1: Correlation between APM and Player Skill Rating\n\n\n\n\n\nAs shown in Figure 1, there is a clear positive correlation between a player’s skill rating and their APM. This relationship highlights the importance of both strategic thinking and mechanical skill in competitive StarCraft."
  },
  {
    "objectID": "rm-data/hw-material/report2.html#starcraft-tournaments-and-prize-pools",
    "href": "rm-data/hw-material/report2.html#starcraft-tournaments-and-prize-pools",
    "title": "The Strategic Depth of StarCraft and Its Esports Legacy",
    "section": "StarCraft Tournaments and Prize Pools",
    "text": "StarCraft Tournaments and Prize Pools\nThe popularity of StarCraft as an esport is evident in the number and scale of tournaments organized worldwide. The following table shows the top StarCraft tournaments by prize pool:\n\n\n\nTable 1: Top StarCraft Tournaments by Prize Pool\n\n\n\n\n\nTournament Name\nYear\nPrize Pool (USD)\nWinner\n\n\n\n\nWCG 2005\n2005\n$75,000\nLi “Sky” Xiaofeng\n\n\nBlizzCon 2007\n2007\n$100,000\nYoan “ToD” Merlo\n\n\nWCG 2009\n2009\n$200,000\nJae Ho “Moon” Jang\n\n\n\n\n\n\nAs seen in Table 1, the prize pools for StarCraft tournaments have grown significantly over time, reflecting the game’s increasing popularity and the growth of the esports industry as a whole."
  },
  {
    "objectID": "rm-data/hw-material/report2.html#footnotes",
    "href": "rm-data/hw-material/report2.html#footnotes",
    "title": "The Strategic Depth of StarCraft and Its Esports Legacy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGame-tree complexity is a measure used in game theory to quantify the number of possible game states in a given game. It provides a way to compare the complexity of different games mathematically.↩︎"
  },
  {
    "objectID": "rm-data/hw-material/report4.html",
    "href": "rm-data/hw-material/report4.html",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "",
    "text": "This report explores the emerging genre of LitRPG (Literary Role-Playing Game), a unique fusion of literature and gaming elements. We will examine the key characteristics of LitRPG, its growing popularity, and its impact on both the literary and gaming worlds. The report includes an analysis of typical LitRPG progression systems, reader engagement statistics, and future trends in the genre."
  },
  {
    "objectID": "rm-data/hw-material/report4.html#defining-litrpg",
    "href": "rm-data/hw-material/report4.html#defining-litrpg",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "Defining LitRPG",
    "text": "Defining LitRPG\nLitRPG is a literary genre that incorporates elements of role-playing games into the narrative structure. According to Johnson (2021), the genre typically features characters progressing through a game-like world, complete with statistics, levels, and skill trees. This unique blend of storytelling and gaming mechanics has led to a surge in popularity among readers who enjoy both literature and video games."
  },
  {
    "objectID": "rm-data/hw-material/report4.html#character-progression-in-litrpg",
    "href": "rm-data/hw-material/report4.html#character-progression-in-litrpg",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "Character Progression in LitRPG",
    "text": "Character Progression in LitRPG\nOne of the key features of LitRPG is the quantifiable progression of characters. This is often represented by a character’s stats or attributes, which can be expressed mathematically. A common formula for calculating a character’s overall power level in many LitRPG novels is:\n\\[\nPower Level = \\sqrt{(Strength + Agility) \\times Intelligence} \\times Level\n\\tag{1}\\]\nThis equation (Equation 1) demonstrates how various attributes contribute to a character’s overall capabilities, providing readers with a tangible sense of growth and achievement."
  },
  {
    "objectID": "rm-data/hw-material/report4.html#reader-engagement-in-litrpg",
    "href": "rm-data/hw-material/report4.html#reader-engagement-in-litrpg",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "Reader Engagement in LitRPG",
    "text": "Reader Engagement in LitRPG\nThe unique structure of LitRPG novels has led to high levels of reader engagement. To illustrate this, we conducted a survey of 1000 LitRPG readers, asking them to rate their engagement levels compared to traditional fantasy novels.\n\n\n\n\n\n\n\n\nFigure 1: Reader Engagement: LitRPG vs Traditional Fantasy\n\n\n\n\n\nAs shown in Figure 1, LitRPG novels tend to generate higher levels of reader engagement compared to traditional fantasy novels. This increased engagement can be attributed to the interactive elements and clear progression systems inherent in LitRPG stories."
  },
  {
    "objectID": "rm-data/hw-material/report4.html#popular-litrpg-subgenres",
    "href": "rm-data/hw-material/report4.html#popular-litrpg-subgenres",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "Popular LitRPG Subgenres",
    "text": "Popular LitRPG Subgenres\nThe LitRPG genre has spawned several popular subgenres, each with its own unique characteristics and fan base. The table below outlines some of the most prominent subgenres:\n\n\n\nTable 1: Popular LitRPG Subgenres\n\n\n\n\n\nSubgenre\nDescription\nPopularity Rating\n\n\n\n\nDungeon Core\nProtagonist is a dungeon\n8/10\n\n\nVR LitRPG\nSet in virtual reality\n9/10\n\n\nApocalypse LitRPG\nReal world becomes game-like\n7/10\n\n\nCultivation LitRPG\nBased on Eastern cultivation novels\n8/10\n\n\n\n\n\n\nAs seen in Table 1, VR LitRPG is currently the most popular subgenre, likely due to its relatability and connection to current technological trends1."
  },
  {
    "objectID": "rm-data/hw-material/report4.html#footnotes",
    "href": "rm-data/hw-material/report4.html#footnotes",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe popularity of VR LitRPG may also be influenced by the growing interest in virtual reality technology in the real world.↩︎"
  },
  {
    "objectID": "rm-data/hw-material/report6.html",
    "href": "rm-data/hw-material/report6.html",
    "title": "Economic Dynamics in Eric Ugland’s ‘The Good Guys’ Series",
    "section": "",
    "text": "This report examines the economic systems and dynamics present in Eric Ugland’s popular LitRPG series, “The Good Guys.” We will explore how the author integrates economic principles into the game-like world, analyzing their impact on character development and plot progression. The report will include a mathematical model of the in-game economy, a visual representation of economic trends, and a comparative analysis of different economic zones within the series."
  },
  {
    "objectID": "rm-data/hw-material/report6.html#the-mathematical-foundation-of-the-in-game-economy",
    "href": "rm-data/hw-material/report6.html#the-mathematical-foundation-of-the-in-game-economy",
    "title": "Economic Dynamics in Eric Ugland’s ‘The Good Guys’ Series",
    "section": "The Mathematical Foundation of the In-Game Economy",
    "text": "The Mathematical Foundation of the In-Game Economy\nIn “The Good Guys,” the in-game economy is governed by a complex system of resource generation, currency valuation, and market dynamics. We can model the basic economic growth within the game using a modified version of the Solow-Swan model, as shown in Equation 1:\n\\[\nY(t) = K(t)^\\alpha (A(t)L(t))^{1-\\alpha} e^{\\beta Q(t)}\n\\tag{1}\\]\nWhere:\n\nY(t) is the total production in the game world at time t\nK(t) is the capital stock\nA(t) is the level of technology\nL(t) is the labor force (players and NPCs)\nQ(t) represents the impact of quests and missions\n\\(\\alpha\\) and \\(\\beta\\) are constants representing the elasticity of output with respect to capital and quests, respectively\n\nThis equation demonstrates how the unique elements of the game world, such as quests (Q), interact with traditional economic factors to drive growth and development."
  },
  {
    "objectID": "rm-data/hw-material/report6.html#economic-trends-across-game-zones",
    "href": "rm-data/hw-material/report6.html#economic-trends-across-game-zones",
    "title": "Economic Dynamics in Eric Ugland’s ‘The Good Guys’ Series",
    "section": "Economic Trends Across Game Zones",
    "text": "Economic Trends Across Game Zones\nTo visualize the economic disparities between different zones in the game world, we’ve compiled data on average player wealth across five major regions. Figure 1 illustrates these differences:\n\n\n\n\n\n\n\n\nFigure 1: Average Player Wealth Across Game Zones\n\n\n\n\n\nAs evident from Figure 1, there is a significant increase in average player wealth as they progress through the game zones. This economic progression serves as a motivator for players to advance in the game, mirroring real-world economic incentives1."
  },
  {
    "objectID": "rm-data/hw-material/report6.html#comparative-analysis-of-economic-systems",
    "href": "rm-data/hw-material/report6.html#comparative-analysis-of-economic-systems",
    "title": "Economic Dynamics in Eric Ugland’s ‘The Good Guys’ Series",
    "section": "Comparative Analysis of Economic Systems",
    "text": "Comparative Analysis of Economic Systems\nThe diverse economic systems present in “The Good Guys” series offer a rich ground for analysis. Table 1 provides a comparison of these systems:\n\n\n\nTable 1: Comparison of Economic Systems in “The Good Guys”\n\n\n\n\n\n\n\n\n\n\n\nEconomic System\nPrimary Currency\nMain Economic Activities\nPlayer Impact\n\n\n\n\nStarting Town\nCopper Coins\nBasic trade, simple quests\nLow\n\n\nMerchant City\nGold Coins\nComplex trade, investments\nHigh\n\n\nDragon’s Lair\nDragon Scales\nRare item trade, high-risk ventures\nVery High\n\n\n\n\n\n\nAs shown in Table 1, the economic systems become more complex and impactful as players progress, offering increasing opportunities for wealth accumulation and economic strategy."
  },
  {
    "objectID": "rm-data/hw-material/report6.html#footnotes",
    "href": "rm-data/hw-material/report6.html#footnotes",
    "title": "Economic Dynamics in Eric Ugland’s ‘The Good Guys’ Series",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis economic progression system is a common feature in many RPGs and LitRPG novels, serving as a form of “gamified capitalism” that keeps players engaged and motivated.↩︎"
  },
  {
    "objectID": "rm-data/hw10/Models_vs_ppl.html",
    "href": "rm-data/hw10/Models_vs_ppl.html",
    "title": "Untitled",
    "section": "",
    "text": "Lazaroes\nNO HW10\nChening\nHW reg target council* suburb_* seller_* i.type_h distance distance_2 rooms rooms_2 bathroom bathroom_2 car l_landsize latitud latitud_2 longitud longitud_2 prop_count date c.rooms#c.bathroom c.distance#c.(l_landsize l_buildingarea rooms car i.type_h) l_buildingarea yearbuilt\nEmi\nNo best model. Ask to clarify\nJOE\nNO HW10\nBrendon\nreg log_price rooms manyb_room type_h distance goodagent goodsuburb, robust\nKailin\nregress price rooms rooms_squared distance bathroom car landsize landsize_squared buildingarea yearbuilt\nShane\nreg ln_price i.suburb i.region i.type i.bedroom2 i.bathroom i.car i.region#i.bed_spline* ln_area i.yearbuilt ln_land c.ln_area#c.ln_land sq_dist_center dist_cbd_car date c.date#c.ln_area c.date#i.bedroom2 c.date#c.ln_land c.date#c.cbd_car2"
  },
  {
    "objectID": "rm-data/Peer_review/Cheming/peer_review.html",
    "href": "rm-data/Peer_review/Cheming/peer_review.html",
    "title": "Feedback on Predicting the Next-Quarter Corporate Profit with Vector Autoregression",
    "section": "",
    "text": "Positive Feedback\nThe theoretical background and framework is really strong.\nYour research question is very clearly laid out.\nYour interpretation of the VAR results is the most fleshed out. This is a strong part of your analysis.\nIt’s great that you explained the limitations of the regression model in your conclusion.\n\n\nQuestions and Suggestions\nI suggest avoiding the use of first-person narrative in a report like this.\nI couldn’t find an explanation for the RHS variables, do you explain what those are in the report?\nThe use of math is impressive, but I think you could clarify your explanation of why the VAR formula is relevant to your specific research.\nI think something needs to be fixed where you’re referencing your regression table. See the following sentence: The result of the regression is in “../results/reg_table_1.txt”, and we only present the significant part related to corporate profit here.\nI think all your figures could use a little bit more explanantion and interpretations.\nDid you use splines to create your last graph?\nTypo: heterskedasticity - You misspelled heteroskedasticity."
  },
  {
    "objectID": "rm-data/research-proposals/proposal1.html",
    "href": "rm-data/research-proposals/proposal1.html",
    "title": "The Impact of Remote Work on Urban Housing Prices",
    "section": "",
    "text": "Introduction\nThe COVID-19 pandemic has dramatically altered work arrangements worldwide, with a significant shift towards remote work. This change has potential far-reaching implications for urban housing markets. This research proposal aims to investigate the impact of increased remote work adoption on housing prices in major urban centers.\n\n\nBackground and Research Question\nThe traditional model of urban development has been centered around the concept of central business districts, where job concentration drives housing demand in surrounding areas. This model has been a cornerstone of urban economics for decades (Alonso 1964). However, the rapid adoption of remote work, accelerated by the COVID-19 pandemic, challenges this traditional understanding (Dingel and Neiman 2020).\nRemote work allows employees to live further from their workplace, potentially reducing the premium placed on centrally located housing. Early evidence suggests that this shift is already impacting housing markets, with some urban centers experiencing slower price growth or even price declines, while suburban and rural areas see increased demand (Ramani and Bloom 2021). However, the long-term implications of this trend remain unclear.\nMain Research Question: How has the increase in remote work adoption influenced housing prices in major urban centers?\nSecondary Research Questions:\n\nIs there a correlation between the percentage of remote workers in a city and changes in housing prices?\nHow does the impact of remote work on housing prices vary across different urban areas?\n\n\n\nPotential Data Sources\n\nRemote Work Adoption: U.S. Census Bureau’s American Community Survey (ACS)\nHousing Prices: Zillow Home Value Index (ZHVI)\nUrban Characteristics: U.S. Census Bureau’s City and Town Population Totals\nEmployment Data: Bureau of Labor Statistics\n\n\n\nPotential Approach\nThis study will employ a difference-in-differences (DiD) approach to analyze the impact of remote work adoption on housing prices. We will use the COVID-19 pandemic as an exogenous shock that dramatically increased remote work adoption. The treatment group will consist of cities with high levels of remote work adoption, while the control group will include cities with lower levels of remote work adoption.\nWe will control for various urban characteristics, such as population size, pre-pandemic economic conditions, and industry composition. To address potential endogeneity concerns, we will use an instrumental variable approach, using pre-pandemic internet connectivity as an instrument for remote work adoption.\n\n\nExpected Findings\nWe anticipate finding a negative relationship between remote work adoption and housing price growth in urban centers. We expect this effect to be more pronounced in cities with a higher concentration of jobs suitable for remote work. However, we also anticipate heterogeneity in the results, with some cities potentially showing resilience in housing prices despite increased remote work.\n\n\nConclusion\nThis research will contribute to our understanding of how changing work patterns influence urban housing markets. The findings will have implications for urban planning, housing policy, and corporate real estate strategies in the post-pandemic era.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/remote-work-housing-prices\nThis repository will contain all data processing scripts, analysis code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nAlonso, William. 1964. Location and Land Use: Toward a General Theory of Land Rent. Harvard University Press.\n\n\nDingel, Jonathan I, and Brent Neiman. 2020. “How Many Jobs Can Be Done at Home?” Journal of Public Economics 189: 104235.\n\n\nRamani, Arjun, and Nicholas Bloom. 2021. “Work from Home and the Office Real Estate Apocalypse.” National Bureau of Economic Research."
  },
  {
    "objectID": "rm-data/research-proposals/proposal2.html",
    "href": "rm-data/research-proposals/proposal2.html",
    "title": "The Effect of Social Media Sentiment on Stock Market Volatility",
    "section": "",
    "text": "Introduction\nIn the age of digital communication, social media platforms have become significant sources of information and sentiment expression. This research proposal aims to investigate the relationship between social media sentiment and stock market volatility, focusing on how sentiment expressed on platforms like Twitter can influence or predict market movements.\n\n\nBackground and Research Question\nThe Efficient Market Hypothesis (EMH) suggests that stock prices reflect all available information (Fama 1970). However, behavioral finance theories argue that investor sentiment can lead to deviations from fundamental values (Baker and Wurgler 2007). With the rise of social media, these platforms have become a new source of real-time information and sentiment that could potentially influence market behavior.\nPrevious studies have shown that social media sentiment can predict stock returns and trading volume (Bollen, Mao, and Zeng 2011). However, the relationship between social media sentiment and market volatility is less explored. Volatility is a crucial measure of market risk and plays a significant role in asset pricing and risk management (Poon and Granger 2003).\nMain Research Question: To what extent does social media sentiment predict or influence stock market volatility?\nSecondary Research Questions: 1. How does the predictive power of social media sentiment vary across different industries or company sizes? 2. Is there a lag between changes in social media sentiment and changes in market volatility?\n\n\nPotential Data Sources\n\nSocial Media Data: Twitter API for tweets related to specific stocks or market indices\nStock Market Data: Yahoo Finance API for historical stock prices and volatility indices\nCompany Information: Compustat database for company characteristics\nNews Sentiment: RavenPack database for comparison with traditional media sentiment\n\n\n\nPotential Approach\nWe will use natural language processing (NLP) techniques to analyze the sentiment of tweets related to specific stocks or market indices. We will then construct a daily sentiment index for each stock or index in our sample.\nTo analyze the relationship between social media sentiment and market volatility, we will use a Vector Autoregression (VAR) model. This will allow us to capture the dynamic relationships between sentiment and volatility while controlling for other factors that might influence volatility, such as trading volume and macroeconomic news.\nWe will also conduct Granger causality tests to determine if social media sentiment has predictive power for future volatility. To address potential endogeneity concerns, we will use an instrumental variable approach, using exogenous events that affect sentiment but are unlikely to directly affect volatility.\n\n\nExpected Findings\nWe anticipate finding a significant relationship between social media sentiment and stock market volatility. We expect that negative sentiment will be associated with increased volatility, while positive sentiment may have a stabilizing effect. We also anticipate that the strength of this relationship may vary across different industries, with tech stocks potentially showing a stronger link to social media sentiment.\n\n\nConclusion\nThis research will contribute to our understanding of how information dissemination through social media influences financial markets. The findings will have implications for risk management strategies, regulatory policies regarding social media and market manipulation, and the development of sentiment-based trading strategies.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/social-media-market-volatility\nThis repository will contain all data collection scripts, sentiment analysis code, econometric models, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nBaker, Malcolm, and Jeffrey Wurgler. 2007. “Investor Sentiment in the Stock Market.” Journal of Economic Perspectives 21 (2): 129–52.\n\n\nBollen, Johan, Huina Mao, and Xiaojun Zeng. 2011. “Twitter Mood Predicts the Stock Market.” Journal of Computational Science 2 (1): 1–8.\n\n\nFama, Eugene F. 1970. “Efficient Capital Markets: A Review of Theory and Empirical Work.” The Journal of Finance 25 (2): 383–417.\n\n\nPoon, Ser-Huang, and Clive WJ Granger. 2003. “Forecasting Volatility in Financial Markets: A Review.” Journal of Economic Literature 41 (2): 478–539."
  },
  {
    "objectID": "rm-data/research-proposals/proposal4.html",
    "href": "rm-data/research-proposals/proposal4.html",
    "title": "The Economic Impact of Artificial Intelligence Adoption in Small and Medium Enterprises",
    "section": "",
    "text": "Introduction\nArtificial Intelligence (AI) is rapidly transforming various sectors of the economy. While much attention has been given to AI adoption in large corporations, its impact on Small and Medium Enterprises (SMEs) remains understudied. This research proposal aims to investigate the economic effects of AI adoption on SMEs, focusing on productivity, profitability, and employment dynamics.\n\n\nBackground and Research Question\nAI technologies have the potential to significantly enhance business processes, decision-making, and customer interactions (Brynjolfsson, Rock, and Syverson 2017). However, the adoption and impact of AI may differ between large corporations and SMEs due to differences in resources, expertise, and scale (Ghobakhloo 2020).\nSMEs play a crucial role in most economies, often accounting for the majority of businesses and employment (Ayyagari, Demirguc-Kunt, and Maksimovic 2011). Understanding how AI adoption affects these firms is essential for policymakers and business leaders. Previous studies have examined the impact of digital technologies on SMEs (Neirotti, Raguseo, and Paolucci 2018), but the specific effects of AI adoption remain unclear.\nMain Research Question: What is the economic impact of AI adoption on Small and Medium Enterprises?\nSecondary Research Questions:\n\nHow does AI adoption affect productivity and profitability in SMEs?\nWhat are the employment effects of AI adoption in SMEs?\nHow does the impact of AI adoption vary across different industries and firm sizes within the SME category?\n\n\n\nPotential Data Sources\n\nAI Adoption Data: Surveys conducted by national statistical offices or industry associations\nFirm-level Financial Data: Orbis database by Bureau van Dijk\nEmployment Data: National labor force surveys or social security records\nIndustry Classification: Standard Industrial Classification (SIC) codes\nFirm Characteristics: National business registries\n\n\n\nPotential Approach\nWe will use a difference-in-differences (DiD) approach to estimate the causal effect of AI adoption on various economic outcomes. We will compare the performance of SMEs that adopt AI (treatment group) with similar firms that do not adopt AI (control group) before and after the adoption.\nTo address potential selection bias, we will use propensity score matching to ensure that the treatment and control groups are comparable in terms of observable characteristics. We will also employ an instrumental variable approach, using the geographical variation in broadband internet availability as an instrument for AI adoption.\nTo examine heterogeneous effects, we will interact the AI adoption variable with industry and firm size indicators. We will also conduct a series of robustness checks, including placebo tests and alternative definitions of AI adoption.\n\n\nExpected Findings\nWe anticipate finding positive effects of AI adoption on productivity and profitability in SMEs, although these effects may vary across industries and firm sizes. We expect to see larger positive effects in knowledge-intensive industries and for firms at the upper end of the SME size spectrum.\nRegarding employment, we anticipate a more nuanced picture. While AI adoption may lead to some job displacement in certain roles, it may also create new job opportunities and increase demand for skilled workers. Overall, we expect to find a skill-biased change in employment structure rather than a significant net change in total employment.\n\n\nConclusion\nThis research will provide valuable insights into the economic impact of AI adoption on SMEs, a crucial but often overlooked segment of the economy. The findings will have important implications for policymakers designing support programs for SMEs, for business leaders making technology investment decisions, and for workers and educators preparing for the future of work in an AI-driven economy.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/ai-impact-sme\nThis repository will contain all data processing scripts, econometric models, visualization code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nAyyagari, Meghana, Asli Demirguc-Kunt, and Vojislav Maksimovic. 2011. “Small Vs. Young Firms Across the World: Contribution to Employment, Job Creation, and Growth.” World Bank Policy Research Working Paper, no. 5631.\n\n\nBrynjolfsson, Erik, Daniel Rock, and Chad Syverson. 2017. “Artificial Intelligence and the Modern Productivity Paradox: A Clash of Expectations and Statistics.” National Bureau of Economic Research.\n\n\nGhobakhloo, Morteza. 2020. “Industry 4.0, Digitization, and Opportunities for Sustainability.” Journal of Cleaner Production 252: 119869.\n\n\nNeirotti, Paolo, Elisabetta Raguseo, and Emilio Paolucci. 2018. “Digital Capabilities and the Performance of Small and Medium-Sized Enterprises: The Mediating Role of Productivity.” Information & Management 55 (7): 871–85."
  },
  {
    "objectID": "rm-data/research-proposals/proposal6.html",
    "href": "rm-data/research-proposals/proposal6.html",
    "title": "The Economics of Springfield: A Case Study of Resource Allocation in The Simpsons",
    "section": "",
    "text": "Introduction\nThe long-running animated series “The Simpsons” provides a unique lens through which to examine economic principles in a fictional setting. This research proposal aims to analyze the economic structure and resource allocation mechanisms in the town of Springfield, using it as a case study to explore real-world economic theories and phenomena.\n\n\nBackground and Research Question\n“The Simpsons” presents a microcosm of American society, complete with its own economic system (Alberti 2004). The town of Springfield features various industries, public services, and economic actors that interact in ways that often mirror real-world economic dynamics. From the nuclear power plant’s monopoly to Moe’s Tavern’s role in the informal economy, Springfield offers numerous examples of economic principles in action (Scanlan and Feinberg 2013).\nThe show’s longevity and consistency in depicting Springfield’s economy provide a unique opportunity to study how a fictional economy responds to various shocks and policy interventions over time. This can offer insights into the applicability and limitations of economic theories in a controlled, albeit fictional, environment.\nMain Research Question: How does the fictional economy of Springfield in “The Simpsons” reflect real-world economic principles and theories?\nSecondary Research Questions:\n\nHow does the monopolistic structure of the Springfield Nuclear Power Plant affect resource allocation in the town?\nWhat role does corruption (as exemplified by Mayor Quimby) play in Springfield’s economic development?\nHow do external shocks (e.g., the various disasters that befall Springfield) impact its economic resilience?\n\n\n\nPotential Data Sources\n\nEpisode Scripts: Transcripts of all “The Simpsons” episodes\nFan Wikis: Detailed information about Springfield’s businesses and economic events\nAcademic Literature: Existing analyses of economics in “The Simpsons”\nReal-world Economic Data: For comparison with Springfield’s fictional economy\n\n\n\nPotential Approach\nWe will employ a mixed-methods approach, combining qualitative content analysis with quantitative modeling. First, we will conduct a systematic review of all episodes, coding for economic events, policies, and outcomes. This will allow us to create a comprehensive database of Springfield’s economic history.\nUsing this database, we will construct economic models of Springfield, estimating key parameters such as the town’s GDP, unemployment rate, and income distribution. We will then use these models to simulate the effects of various economic policies and shocks depicted in the show.\nTo analyze the role of institutions, we will use a comparative institutional analysis, contrasting Springfield’s institutions with real-world examples. We will also employ network analysis to map the economic relationships between characters and businesses in Springfield.\n\n\nExpected Findings\nWe anticipate finding that Springfield’s economy exhibits many real-world economic phenomena, including market failures, externalities, and public choice problems. We expect to see significant impacts of the nuclear plant’s monopoly on resource allocation and evidence of the “resource curse” related to Springfield’s over-reliance on this industry.\nWe also anticipate finding that corruption plays a significant role in Springfield’s economic outcomes, potentially leading to inefficient resource allocation. However, we expect to observe a high degree of economic resilience, with Springfield quickly recovering from numerous disasters, possibly highlighting the role of social capital in economic recovery.\n\n\nConclusion\nThis research will provide a novel perspective on economic principles by examining them through the lens of a popular cultural artifact. While based on a fictional setting, the findings may offer insights into real-world economic dynamics, particularly in small, isolated economies. Moreover, this study could demonstrate the potential of using popular media as a tool for economic education and analysis.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/springfield-economics\nThis repository will contain all data collection scripts, economic models, visualization code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nAlberti, John. 2004. Leaving Springfield: The Simpsons and the Possibility of Oppositional Culture. Wayne State University Press.\n\n\nScanlan, Stephen J, and Seth L Feinberg. 2013. “The Simpsons and the Economy.” The Simpsons in the Classroom: Embiggening the Learning Experience with the Wisdom of Springfield, 116–30."
  },
  {
    "objectID": "rm-data/research-proposals/proposal8.html",
    "href": "rm-data/research-proposals/proposal8.html",
    "title": "The Economics of the Matrix: Scarcity, Choice, and Human Capital in a Simulated Reality",
    "section": "",
    "text": "Introduction\nThe Matrix trilogy presents a unique scenario where most of humanity exists within a simulated reality. This research proposal aims to analyze the economic implications of such a system, focusing on concepts of scarcity, choice, and human capital in both the simulated world and the real world of the films.\n\n\nBackground and Research Question\nThe Matrix depicts a world where machines use human bodies as an energy source, while human minds exist in a simulated reality (Irwin 2002). This scenario raises intriguing questions about the nature of economic activity in a world where physical scarcity is largely artificial, but cognitive scarcity remains real (Chalmers 2005).\nThe films also present two contrasting economic systems: the seemingly abundant but illusory economy of the Matrix, and the scarce, survival-focused economy of Zion. This dichotomy offers a unique opportunity to explore how different resource constraints shape economic behavior and institutions (Yeffeth 2003).\nMoreover, the concept of human capital takes on new dimensions in the Matrix, where individuals can instantly learn new skills by downloading them directly to their minds. This aspect of the films allows us to explore questions about the nature of skill acquisition, the value of education, and the role of human capital in economic growth.\nMain Research Question: How do the economic systems depicted in the Matrix trilogy reflect and challenge traditional economic concepts of scarcity, choice, and human capital?\nSecondary Research Questions:\n\nHow does the artificial scarcity in the Matrix compare to real-world economic scarcity?\nWhat are the economic implications of instant skill acquisition as depicted in the films?\nHow does the economy of Zion reflect real-world economies operating under extreme resource constraints?\n\n\n\nPotential Data Sources\n\nFilm Scripts: Detailed scripts of the Matrix trilogy\nSupplementary Material: The Animatrix and related comics\nFan Wikis: Detailed information about the Matrix universe\nEconomic Literature: Theories on scarcity, choice, and human capital\nTechnological Forecasting: Predictions about future AI and VR technologies\n\n\n\nPotential Approach\nWe will employ a mixed-methods approach, combining qualitative analysis of the films with economic modeling. First, we will conduct a systematic review of the trilogy, coding for economic concepts, resource allocation mechanisms, and depictions of skill acquisition.\nUsing this data, we will construct economic models of both the Matrix and Zion, estimating key parameters such as production possibilities, resource constraints, and returns to human capital. We will then compare these models with real-world economic data and theories.\nTo analyze the implications of instant skill acquisition, we will develop a theoretical model of human capital accumulation based on the Matrix’s depiction, comparing it with standard models of education and on-the-job training.\nWe will also use comparative analysis to contrast the economic systems of the Matrix and Zion with various real-world economic systems, from abundance-oriented tech economies to resource-constrained developing economies.\n\n\nExpected Findings\nWe anticipate finding that the Matrix’s economy challenges traditional concepts of scarcity, potentially offering insights into post-scarcity economic theories. We expect to see that while physical scarcity is artificially imposed in the Matrix, cognitive scarcity (e.g., limitations on attention and decision-making) remains a key economic factor.\nRegarding human capital, we expect to find that the Matrix’s depiction of instant skill acquisition has profound implications for theories of human capital and economic growth, potentially highlighting the importance of knowledge distribution over knowledge creation in driving economic progress.\nFor Zion’s economy, we anticipate finding parallels with real-world economies operating under extreme resource constraints, potentially offering insights into economic resilience and adaptation.\n\n\nConclusion\nThis research will provide a novel perspective on fundamental economic concepts by examining them through the lens of a popular science fiction narrative. While based on a fictional setting, the findings may offer insights into real-world economic phenomena, particularly as we move towards increasingly digitalized and AI-driven economies. Moreover, this study could demonstrate the potential of using science fiction scenarios as thought experiments for economic theory and policy.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/matrix-economics\nThis repository will contain all data collection scripts, economic models, visualization code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nChalmers, David J. 2005. “The Matrix as Metaphysics.” Science Fiction and Philosophy: From Time Travel to Superintelligence, 36–53.\n\n\nIrwin, William. 2002. The Matrix and Philosophy: Welcome to the Desert of the Real. Open Court.\n\n\nYeffeth, Glenn. 2003. Taking the Red Pill: Science, Philosophy and the Religion in the Matrix. BenBella Books."
  },
  {
    "objectID": "rm-data/slides/CaseStudy_SLR.html",
    "href": "rm-data/slides/CaseStudy_SLR.html",
    "title": "Gender Gap in Earnings: A Case Study",
    "section": "",
    "text": "The gender gap in earnings is a critical issue in labor economics, reflecting potential inequalities in the labor market. This case study investigates whether there are systematic wage differences between male and female workers, focusing on data from the U.S. Current Population Survey (CPS) for 2014. The study aims to understand the factors contributing to the gender gap and examines specific occupations to provide detailed insights into these disparities.\n\n\n\n\n\nThe CPS is a monthly survey conducted by the U.S. Census Bureau and the Bureau of Labor Statistics. It provides comprehensive data on labor force characteristics, including employment, earnings, and demographic information. The survey uses a rotating panel design, where households are interviewed for four consecutive months, not interviewed for the next eight months, and then interviewed again for four months. This design helps in capturing both short-term and long-term labor market trends.\n\n\n\nTo focus on the working-age population, the sample includes individuals aged 16-65 who are employed and have reported earnings. Self-employed individuals are excluded to maintain consistency in earnings data, as self-employment income can vary significantly and may not be directly comparable to wage and salary earnings. After applying these restrictions, the dataset for 2014 consists of 149,316 observations.\n\n\n\nWeekly earnings data are collected before tax deductions. High earnings are top-coded at $2,884.6 to account for inflation and to prevent the influence of outliers on the analysis. This top-coding represents the top 2.5% of earnings in 2014. To control for differences in hours worked, weekly earnings are divided by ‘usual’ weekly hours, as reported in the survey. This adjustment is crucial because women may work fewer hours on average than men, affecting their total earnings.\n\n\n\n\nThe descriptive statistics provide an overview of the earnings distribution by gender. The table below summarizes the key percentiles of earnings for male and female workers:\n\n\n\nGender\nMean\np25\np50\np75\np90\np95\n\n\n\n\nMale\n24\n13\n19\n30\n45\n55\n\n\nFemale\n20\n11\n16\n24\n36\n45\n\n\n\nThese statistics reveal a 17% average difference in per-hour earnings between men and women, highlighting a significant gender gap.\n\n\n\n\n\nThe analysis focuses on computer science occupations, which are often associated with high earnings and significant gender disparities. The sample size for this occupation is 4,740. The regression model used is:\n\\[\n\\ln(w)_E = \\alpha + \\beta \\times G_{female}\n\\]\nWhere \\(G_{female}\\) is a binary variable indicating if the individual is female. The regression estimate \\(\\hat{\\beta} = -0.1475\\) suggests that female employees in computer science earn 14.7% less on average than their male counterparts. The 95% confidence interval for this estimate is [-18.2%, -11.2%], which does not include zero, allowing us to rule out equal average earnings with 95% confidence. This finding is statistically significant, with a standard error of 0.0177.\n\n\n\nFor market research analysts and marketing specialists, the sample size is 281, with females comprising 61% of the sample. The regression estimate \\(\\hat{\\beta} = -0.113\\) indicates that female analysts earn 11.3% less on average. However, the 95% confidence interval [-23%, +1%] includes zero, indicating that we cannot rule out equal average earnings with 95% confidence. The p-value of 0.068 suggests that the result is not statistically significant at the 5% level, although it is at the 10% level.\n\n\n\n\n\n\nThe difference in confidence intervals between the two occupations can be attributed to:\n\nTrue Difference: The gender gap is higher in computer science occupations, possibly due to industry-specific factors such as negotiation practices, discrimination, or differences in experience and education levels.\nStatistical Error: The smaller sample size for market analysts may lead to more variability in estimates. Smaller samples tend to have larger standard errors, resulting in wider confidence intervals. This variability can obscure true differences in earnings.\n\n\n\n\nTo formally test whether average earnings are the same by gender, we examine if the coefficient on the gender variable is zero. The t-statistic for market analysts is 1.8, which falls within the critical values for a 5% significance level (±2), indicating that we cannot reject the null hypothesis of equal average earnings. The p-value of 0.07 further supports this conclusion, as it is greater than the 0.05 threshold for statistical significance.\n\n\n\n\nThis case study highlights the complexities of analyzing the gender gap in earnings. While significant disparities exist in computer science occupations, the evidence is less clear for market research analysts due to sample size limitations. The findings underscore the need for further data collection and analysis to draw more definitive conclusions about the gender gap across different sectors. Understanding these disparities is crucial for developing policies aimed at achieving gender equality in the workplace."
  },
  {
    "objectID": "rm-data/slides/CaseStudy_SLR.html#introduction",
    "href": "rm-data/slides/CaseStudy_SLR.html#introduction",
    "title": "Gender Gap in Earnings: A Case Study",
    "section": "",
    "text": "The gender gap in earnings is a critical issue in labor economics, reflecting potential inequalities in the labor market. This case study investigates whether there are systematic wage differences between male and female workers, focusing on data from the U.S. Current Population Survey (CPS) for 2014. The study aims to understand the factors contributing to the gender gap and examines specific occupations to provide detailed insights into these disparities."
  },
  {
    "objectID": "rm-data/slides/CaseStudy_SLR.html#data-collection-and-methodology",
    "href": "rm-data/slides/CaseStudy_SLR.html#data-collection-and-methodology",
    "title": "Gender Gap in Earnings: A Case Study",
    "section": "",
    "text": "The CPS is a monthly survey conducted by the U.S. Census Bureau and the Bureau of Labor Statistics. It provides comprehensive data on labor force characteristics, including employment, earnings, and demographic information. The survey uses a rotating panel design, where households are interviewed for four consecutive months, not interviewed for the next eight months, and then interviewed again for four months. This design helps in capturing both short-term and long-term labor market trends.\n\n\n\nTo focus on the working-age population, the sample includes individuals aged 16-65 who are employed and have reported earnings. Self-employed individuals are excluded to maintain consistency in earnings data, as self-employment income can vary significantly and may not be directly comparable to wage and salary earnings. After applying these restrictions, the dataset for 2014 consists of 149,316 observations.\n\n\n\nWeekly earnings data are collected before tax deductions. High earnings are top-coded at $2,884.6 to account for inflation and to prevent the influence of outliers on the analysis. This top-coding represents the top 2.5% of earnings in 2014. To control for differences in hours worked, weekly earnings are divided by ‘usual’ weekly hours, as reported in the survey. This adjustment is crucial because women may work fewer hours on average than men, affecting their total earnings."
  },
  {
    "objectID": "rm-data/slides/CaseStudy_SLR.html#descriptive-statistics",
    "href": "rm-data/slides/CaseStudy_SLR.html#descriptive-statistics",
    "title": "Gender Gap in Earnings: A Case Study",
    "section": "",
    "text": "The descriptive statistics provide an overview of the earnings distribution by gender. The table below summarizes the key percentiles of earnings for male and female workers:\n\n\n\nGender\nMean\np25\np50\np75\np90\np95\n\n\n\n\nMale\n24\n13\n19\n30\n45\n55\n\n\nFemale\n20\n11\n16\n24\n36\n45\n\n\n\nThese statistics reveal a 17% average difference in per-hour earnings between men and women, highlighting a significant gender gap."
  },
  {
    "objectID": "rm-data/slides/CaseStudy_SLR.html#analysis-of-gender-gap-in-specific-occupations",
    "href": "rm-data/slides/CaseStudy_SLR.html#analysis-of-gender-gap-in-specific-occupations",
    "title": "Gender Gap in Earnings: A Case Study",
    "section": "",
    "text": "The analysis focuses on computer science occupations, which are often associated with high earnings and significant gender disparities. The sample size for this occupation is 4,740. The regression model used is:\n\\[\n\\ln(w)_E = \\alpha + \\beta \\times G_{female}\n\\]\nWhere \\(G_{female}\\) is a binary variable indicating if the individual is female. The regression estimate \\(\\hat{\\beta} = -0.1475\\) suggests that female employees in computer science earn 14.7% less on average than their male counterparts. The 95% confidence interval for this estimate is [-18.2%, -11.2%], which does not include zero, allowing us to rule out equal average earnings with 95% confidence. This finding is statistically significant, with a standard error of 0.0177.\n\n\n\nFor market research analysts and marketing specialists, the sample size is 281, with females comprising 61% of the sample. The regression estimate \\(\\hat{\\beta} = -0.113\\) indicates that female analysts earn 11.3% less on average. However, the 95% confidence interval [-23%, +1%] includes zero, indicating that we cannot rule out equal average earnings with 95% confidence. The p-value of 0.068 suggests that the result is not statistically significant at the 5% level, although it is at the 10% level."
  },
  {
    "objectID": "rm-data/slides/CaseStudy_SLR.html#discussion",
    "href": "rm-data/slides/CaseStudy_SLR.html#discussion",
    "title": "Gender Gap in Earnings: A Case Study",
    "section": "",
    "text": "The difference in confidence intervals between the two occupations can be attributed to:\n\nTrue Difference: The gender gap is higher in computer science occupations, possibly due to industry-specific factors such as negotiation practices, discrimination, or differences in experience and education levels.\nStatistical Error: The smaller sample size for market analysts may lead to more variability in estimates. Smaller samples tend to have larger standard errors, resulting in wider confidence intervals. This variability can obscure true differences in earnings.\n\n\n\n\nTo formally test whether average earnings are the same by gender, we examine if the coefficient on the gender variable is zero. The t-statistic for market analysts is 1.8, which falls within the critical values for a 5% significance level (±2), indicating that we cannot reject the null hypothesis of equal average earnings. The p-value of 0.07 further supports this conclusion, as it is greater than the 0.05 threshold for statistical significance."
  },
  {
    "objectID": "rm-data/slides/CaseStudy_SLR.html#conclusion",
    "href": "rm-data/slides/CaseStudy_SLR.html#conclusion",
    "title": "Gender Gap in Earnings: A Case Study",
    "section": "",
    "text": "This case study highlights the complexities of analyzing the gender gap in earnings. While significant disparities exist in computer science occupations, the evidence is less clear for market research analysts due to sample size limitations. The findings underscore the need for further data collection and analysis to draw more definitive conclusions about the gender gap across different sectors. Understanding these disparities is crucial for developing policies aimed at achieving gender equality in the workplace."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#course-overview",
    "href": "rm-data/slides/week01/week01.html#course-overview",
    "title": "Introduction to Reproducible Research",
    "section": "Course Overview",
    "text": "Course Overview\nDuration: 3 hours (180 minutes)\n\nIntroduction to Reproducible Research\n\nZotero for Bibliography Management\nVersion Control with Git, GitHub, and GitHub-Desktop\n\nVisual Studio Code (VSC) for Coding\n\nQuarto and VSC for Documents\n\nOther Essential Tools and Practices\n\nPractical Exercise\nQ&A and Resources for Further Learning\n\n\nWelcome to our course on Reproducible Research in Economics. Over the next 3 hours, we’ll cover a range of tools and practices that will help you make your research more transparent, collaborative, and reproducible."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#why-reproducibility-matters-in-economics",
    "href": "rm-data/slides/week01/week01.html#why-reproducibility-matters-in-economics",
    "title": "Introduction to Reproducible Research",
    "section": "Why Reproducibility Matters in Economics",
    "text": "Why Reproducibility Matters in Economics\n\nEnhances credibility and transparency of research\n\nEnables easier verification, replocation and extension of studies\n\nFacilitates collaboration and knowledge sharing\nAligns with growing expectations from journals and funding bodies\n\nMany journals now require reproducibility as part of the submission process\n\n\n\nReproducibility is crucial in economics as it ensures that other researchers can replicate and build upon your work. This transparency enhances the credibility of your findings and contributes to the cumulative nature of scientific knowledge."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#tools-well-cover",
    "href": "rm-data/slides/week01/week01.html#tools-well-cover",
    "title": "Introduction to Reproducible Research",
    "section": "Tools We’ll Cover",
    "text": "Tools We’ll Cover\n\nGit & GitHub for version control\nZotero for bibliography management\nVisual Studio Code as our integrated development environment\nQuarto for dynamic document creation\nSupporting tools for project organization and data management\n\n\nThese tools form a powerful ecosystem that will streamline your research workflow and make reproducibility a natural part of your process."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#setting-up-your-environment",
    "href": "rm-data/slides/week01/week01.html#setting-up-your-environment",
    "title": "Introduction to Reproducible Research",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nCreate a GitHub account https://github.com/\nDownload and install GitHub Desktop https://desktop.github.com/download\nInstall Zotero and the Zotero Connector for your browser https://www.zotero.org/download\nInstall Visual Studio Code https://code.visualstudio.com/Download\nInstall Quarto https://quarto.org/\n(Optional) Install R, Python, or Stata as needed\n\n\nWe’ll guide you through the installation process for each of these tools. Don’t worry if you encounter any issues - we’re here to help!"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#why-do-we-care-about-version-control",
    "href": "rm-data/slides/week01/week01.html#why-do-we-care-about-version-control",
    "title": "Introduction to Reproducible Research",
    "section": "Why do we care about Version Control?",
    "text": "Why do we care about Version Control?\n\nLife without version control\n\nDo you keep every variant of every program you ever wrote on a project?\nIf so, how do you keep track of them? What if there is a bug?\nIf only keep latests, how do you know what you tried?\n\nDo you have the ThesisV1, ThesisV123, ThesisFinal, ThesisFinalFinal, ThesisFinalFinalFinal problem?"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#what-is-git",
    "href": "rm-data/slides/week01/week01.html#what-is-git",
    "title": "Introduction to Reproducible Research",
    "section": "What is Git?",
    "text": "What is Git?\n\nGit is a program that does version control (tracking changes in files)\nIt is the most popular version control program in software development and data science\nIt is “easy” to set up and get started\nThere are many programs that add intuitive interfaces on top of Git\nGit integrates seamlessly with online collaboration tools like GitHub and GitLab"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#why-use-git",
    "href": "rm-data/slides/week01/week01.html#why-use-git",
    "title": "Introduction to Reproducible Research",
    "section": "Why use Git?",
    "text": "Why use Git?\n\nGit is the current standard for version control.\nIt has been used in the software industry for long, and it is now being adopted in other fields.\nIt can help you keep track of changes in your code and documents.\n\nIt does not save every version of every file, but it saves the changes you made.\nWhich means, you can go back to a previous version of your code or document.\nIt also helps you keep track of who made what changes and when (Comments)\n\nCaveat: hard to keep track of changes in binary files (e.g., .docx, .pdf, .xlsx)"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#how-to-use-git",
    "href": "rm-data/slides/week01/week01.html#how-to-use-git",
    "title": "Introduction to Reproducible Research",
    "section": "How to use Git?",
    "text": "How to use Git?\n\nYou can use Git from the command line! (Most powerful, but steep learning curve)\nOr you can use a GUI (Graphical User Interface) like GitHub Desktop, or Rstudio (which has Git integrated for Projects)\nIf you need a remote repository, you can use GitHub, GitLab, or Bitbucket (among others)\n\nGitHub is the most popular, and we will be using it."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#basic-git-concepts",
    "href": "rm-data/slides/week01/week01.html#basic-git-concepts",
    "title": "Introduction to Reproducible Research",
    "section": "Basic Git Concepts",
    "text": "Basic Git Concepts\n\nRepository: A project’s folder containing all files and version history. It can be local (your PC) or remote (GitHub).\nCommit: A snapshot of your project at a specific point in time (like a save point in a game)\n\nCommit message: A brief description of the changes made. All changes are local.\n\nPush: Sending your commits to a remote repository (like GitHub)\nPull: Getting changes from a remote repository (from GitHub to your PC)\n\n\n\n\nIf you want to use GitHub as a Dropbox-like service, you need to commit and push your changes."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#additional-git-concepts",
    "href": "rm-data/slides/week01/week01.html#additional-git-concepts",
    "title": "Introduction to Reproducible Research",
    "section": "Additional Git Concepts",
    "text": "Additional Git Concepts\n\nBranch: An independent line of development\nMerge: Combining changes from different branches\n\nSo far in our work we have been working on the main branch. That should be the case for most of your work.\n\n\n\nThese concepts form the foundation of version control with Git. Understanding them will help you manage your project’s history effectively."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#github-desktop-getting-started",
    "href": "rm-data/slides/week01/week01.html#github-desktop-getting-started",
    "title": "Introduction to Reproducible Research",
    "section": "GitHub Desktop: Getting Started",
    "text": "GitHub Desktop: Getting Started\n\nGitHub Desktop provides a user-friendly interface for Git operations, that works well with GitHub.\n\nIt is a good way to get started with Git, create repositories, and manage your projects.\n\nDownload and install GitHub Desktop\nLog in with your GitHub account\n\nCreate a new repository or clone an existing one\n\nEvery one can have access to public repositories (transparent) but only you and collaborators can access private repositories.\n\nMake changes, commit, and push to GitHub\n\n\nGitHub Desktop provides a user-friendly interface for Git operations, making it easier for beginners to get started with version control."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#what-should-a-the-git-repository-contain",
    "href": "rm-data/slides/week01/week01.html#what-should-a-the-git-repository-contain",
    "title": "Introduction to Reproducible Research",
    "section": "What should a the Git repository contain?",
    "text": "What should a the Git repository contain?\n\nThe Core of the project (The code, the data, the documents):\n\nCode (.do, .R, .m, .jl)\n.txt files, .md. files, .qmd files.\nLaTeX .tex, .bib, etc\n\nYou may also want to include (small) Raw Data (.csv)\n\nor a link to the data (other ways to access it)\n\nNormally, you wouldnt want binary files, but the following are exceptions:\n\nPDF files\nWord, Excel, PowerPoint files\nPerhaps images, if they are part of the project"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#section",
    "href": "rm-data/slides/week01/week01.html#section",
    "title": "Introduction to Reproducible Research",
    "section": "",
    "text": "You could include full datasets, but consider:\n\nGitHub doesn’t allow files larger than 100 MB, or projects with total size larger than 1 GB.\nLook into Git LFS (Large File Storage) for large files."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#what-could-you-ignore",
    "href": "rm-data/slides/week01/week01.html#what-could-you-ignore",
    "title": "Introduction to Reproducible Research",
    "section": "What could you ignore?",
    "text": "What could you ignore?\nConsider adding a .gitignore file to your repository to exclude:\n\nTemporary files (.bak, .swp, .log, .aux)\nJunk files (.RData, .pyc)\nor simply any format or file you do not want to include. (keep the repository clean)"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#project-workflow",
    "href": "rm-data/slides/week01/week01.html#project-workflow",
    "title": "Introduction to Reproducible Research",
    "section": "Project Workflow",
    "text": "Project Workflow\n\n\nStart a new project: Create a new repository on GitHub or Github Desktop\n\nClone it to your local machine (if you started on GitHub)\n\n\nCreate your project structure, add files, and start working.\nWhen you have made changes, and are ready to “save” them, commit them (Local Snapshot).\n\nAdd a message that describes the changes you made (make them meaningful).\n\nWhen you are ready to share your changes with the world, push them to GitHub (Remote repository).\nIf working with others, you may want to pull changes from GitHub to your local machine."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#section-1",
    "href": "rm-data/slides/week01/week01.html#section-1",
    "title": "Introduction to Reproducible Research",
    "section": "",
    "text": "Commit often, push when you are ready to share.\n\nSmall mistakes are easier to fix.\n\nA bit of a challenge if working with multiple computers, but it is doable.\nAnd always check, everything works before pushing."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#introduction-to-zotero",
    "href": "rm-data/slides/week01/week01.html#introduction-to-zotero",
    "title": "Introduction to Reproducible Research",
    "section": "Introduction to Zotero",
    "text": "Introduction to Zotero\n\nSomething that is not often taught in school is how to manage your bibliography.\n\nYou work on your paper, forget where you got a quote, and then you have to go back and find it!\nWorse, you have to format your bibliography, and you have to do it manually.\nthen you change text, and drop a citation from text and refereces don’t match.\n\nThis is where Zotero comes in.\n\nIt is a free, open-source reference management software.\nIt helps you collect, organize, cite, and share your research sources.\n\nMore importantly, it will create bib files for all your references.\n\nIt integrates with word processors and browsers.\n\n\n\nZotero is a powerful tool that simplifies the process of managing your research sources and creating bibliographies."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#key-zotero-features",
    "href": "rm-data/slides/week01/week01.html#key-zotero-features",
    "title": "Introduction to Reproducible Research",
    "section": "Key Zotero Features",
    "text": "Key Zotero Features\n\nBrowser connector for easy capture of online sources\nPDF annotation and management\nCitation style editor for customized formatting\n\nMany of Standard styles are already included. So you dont have to worry about formatting.\n\nSynchronization across devices (cloud storage)\n\n\nThese features make Zotero an essential tool for managing your research literature and ensuring accurate, consistent citations in your work."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#integrating-zotero-with-your-workflow",
    "href": "rm-data/slides/week01/week01.html#integrating-zotero-with-your-workflow",
    "title": "Introduction to Reproducible Research",
    "section": "Integrating Zotero with Your Workflow",
    "text": "Integrating Zotero with Your Workflow\n\nUse Zotero Connector to save sources while browsing\nOrganize sources into collections for different projects\nUse tags and notes for efficient source management\nGenerate in-text citations and bibliographies in your documents\n\n\nIntegrating Zotero into your research workflow can significantly streamline your literature management and citation process."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#why-not-word-or-google-docs",
    "href": "rm-data/slides/week01/week01.html#why-not-word-or-google-docs",
    "title": "Introduction to Reproducible Research",
    "section": "Why not Word or Google Docs?",
    "text": "Why not Word or Google Docs?\n\nGit can keep track of changes in your code, and documents, but you need a good editor to write them.\nMSWord, Google Docs, etc are good for writing, binary files…Git cannot read them.\nThus, you need to write your code in plain text files (more later).\n\nFor example, most software code is written in plain text files.\nThis is also true for LaTeX, html and Markdown (more later).\n\nA good alternative for this is Visual Studio Code (VSC)."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#what-is-visual-studio-code",
    "href": "rm-data/slides/week01/week01.html#what-is-visual-studio-code",
    "title": "Introduction to Reproducible Research",
    "section": "What is Visual Studio Code?",
    "text": "What is Visual Studio Code?\n\nFree, open-source code editor by Microsoft that is constantly updated.\nSupports multiple programming languages (R, Python, julia, Stata (with plugins))\nRich ecosystem of extensions (for Git, Quarto, Stata, Latex, etc)\nIntegrated terminal (will become important later)\nUsually keeps your last session open, so you can pick up where you left off.\n\n\nVisual Studio Code has become one of the most popular code editors due to its flexibility, performance, and extensive feature set."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#setting-up-vsc",
    "href": "rm-data/slides/week01/week01.html#setting-up-vsc",
    "title": "Introduction to Reproducible Research",
    "section": "Setting Up VSC",
    "text": "Setting Up VSC\n\nDownload and install Visual Studio Code\nInstall language extensions for R, Python, or Stata This will give you syntax highlighting, code completion, and other language-specific features Even allow for running code from the editor\nInstall helpful extensions like GitLens and Copilot (Free AI for coding)\n\n\nWe’ll walk through the setup process and highlight some of the most useful extensions for economic research."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#what-can-vsc-do-for-you",
    "href": "rm-data/slides/week01/week01.html#what-can-vsc-do-for-you",
    "title": "Introduction to Reproducible Research",
    "section": "What can VSC do for you?",
    "text": "What can VSC do for you?\n\nAs said before. Great editor for plain text files Code (R, Julia, Stata, Python, etc). And documents (Markdown, LaTeX, etc).\nIt has a robust markdown support.\nIt has a built-in terminal, so you can run your code from the editor."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#introduction-to-quarto",
    "href": "rm-data/slides/week01/week01.html#introduction-to-quarto",
    "title": "Introduction to Reproducible Research",
    "section": "Introduction to Quarto",
    "text": "Introduction to Quarto\n\nNext-generation tool for literate programming\nSupports multiple languages (R, Python, Julia, Observable JS, Stata with plugins)\nIt can be used to create documents (PDF’s, Word, others), websites (like Mine), books (see examples), and presentations (like this one!)\nExtends and improves upon R Markdown"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#what-is-quarto",
    "href": "rm-data/slides/week01/week01.html#what-is-quarto",
    "title": "Introduction to Reproducible Research",
    "section": "What is Quarto?",
    "text": "What is Quarto?\n\nIs an open-source scientific and technical publishing system\nYou can use plain text markdown in your favorite editor.\nCreate dynamic content with Python, R, Julia, and Observable.\nPublish reproducible, production quality articles, presentations, dashboards, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more.\nWrite using Pandoc markdown would allow you to including equations (LaTeX), citations (Zotero), crossrefs, figures panels, callouts, advanced layout, and more."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#how-quarto-works",
    "href": "rm-data/slides/week01/week01.html#how-quarto-works",
    "title": "Introduction to Reproducible Research",
    "section": "How Quarto Works?",
    "text": "How Quarto Works?\nWhen you render a .qmd file with Quarto, the executable code blocks are processed by Jupyter, and the resulting combination of code, markdown, and output is converted to plain markdown. Then, this markdown is processed by Pandoc, which creates the finished format."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#creating-a-quarto-document",
    "href": "rm-data/slides/week01/week01.html#creating-a-quarto-document",
    "title": "Introduction to Reproducible Research",
    "section": "Creating a Quarto Document",
    "text": "Creating a Quarto Document\n\nIn VSC, create a new file: Quarto document.\n\nSave it in the folder where you have your data and code.\n\nAdd or modify the YAML header for document metadata. This tells Quarto how to render the document, who wrote it, other relevant information.\n\nDocument Body: Markdown for text formatting (plain text), with minimal formatting.\nYou can include images, links, tables, citations, etc. But also code chunks for running and displaying analysis. In some cases (R), you can also add In-line code for dynamic text"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#another-flavor-of-quarto-visual-editor",
    "href": "rm-data/slides/week01/week01.html#another-flavor-of-quarto-visual-editor",
    "title": "Introduction to Reproducible Research",
    "section": "Another Flavor of Quarto: Visual Editor",
    "text": "Another Flavor of Quarto: Visual Editor\n\nQuarto also has a visual editor option. This is more user-friendly for first timers, and has a more WYSIWYG feel.\nIt does allow for more advanced features including Citations!\nSince Visual Editor is simpler to use, lets cover the basics of using plain text Quarto in VSC."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#first-the-yaml-header",
    "href": "rm-data/slides/week01/week01.html#first-the-yaml-header",
    "title": "Introduction to Reproducible Research",
    "section": "First, the YAML Header",
    "text": "First, the YAML Header\n\nYAML HeaderYAML Header HTMLYAML Header PDF\n\n\nYAML is not necessary for Quarto, but it is useful for metadata and formatting. It typically would include:\n\nTitle\nAuthor\nFormat (HTML, PDF, Word, etc)\nOther metadata (date, keywords, etc)\n\n\n\n---\ntitle: \"My Economic Analysis\"\nauthor: \"Your Name\"\nformat: html\ndate: last-modified\n---\n\n\n---\ntitle: \"My Economic Analysis\"\nauthor: \n  - name: Author One\n  - name: Author Two\nformat: \n  pdf: \n    number-sections: true\n    documentclass: article\ndate: today\n---"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#yaml-header-options",
    "href": "rm-data/slides/week01/week01.html#yaml-header-options",
    "title": "Introduction to Reproducible Research",
    "section": "YAML Header Options",
    "text": "YAML Header Options\nFor a template, you can use the following template.qmd\nFor more options see YAML PDF Options\n\nThis example shows the basic structure of a Quarto document. The YAML header sets document properties."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#then-the-body-text-formatting",
    "href": "rm-data/slides/week01/week01.html#then-the-body-text-formatting",
    "title": "Introduction to Reproducible Research",
    "section": "Then the Body: Text Formatting",
    "text": "Then the Body: Text Formatting\nQuarto uses Pandoc markdown for text formatting. This is a simple, plain text format that is easy to write and read in any text editor."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#then-the-body-text-formatting-1",
    "href": "rm-data/slides/week01/week01.html#then-the-body-text-formatting-1",
    "title": "Introduction to Reproducible Research",
    "section": "Then the Body: Text Formatting",
    "text": "Then the Body: Text Formatting\n\nHeadingsH RenderedFormatF RenderedListL rendered\n\n\n# Heading 1\n\n## Heading 2\n\n### Heading 3\n\n\n\nHeading 1\n\n\n\nHeading 2\nHeading 3\n\n\n\n\n*italics*, **bold**, ***bold italics***\n\n~~strikeout~~, `code`\n\nsuperscript^2^, subscript~2~\n\n\n\nitalics, bold, bold italics\nstrikeout, code\nsuperscript2, subscript2\n\n\n\nLists:\n\n- Item 1\n- Item 2\n  - Subitem 1\n  - Subitem 2\n\nLists:\n\n1. item 4\n    1. item 5 \n    1. item 5  \n2. ds\n\n\n\n\nLists:\n\nItem 1\nItem 2\n\nSubitem 1\nSubitem 2\n\n\n\nLists:\n\nitem 4\n\nitem 5\nitem 5\n\n\nds"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#heading-2",
    "href": "rm-data/slides/week01/week01.html#heading-2",
    "title": "Introduction to Reproducible Research",
    "section": "Heading 2",
    "text": "Heading 2\nHeading 3"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#authoring-in-quarto",
    "href": "rm-data/slides/week01/week01.html#authoring-in-quarto",
    "title": "Introduction to Reproducible Research",
    "section": "Authoring in Quarto",
    "text": "Authoring in Quarto\n\nQuarto supports various types of content for the creation of academic writting.\nThis include: equations, tables, images, footnotes, references and cross-references.\nWe will see how to include these in your document."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#equations",
    "href": "rm-data/slides/week01/week01.html#equations",
    "title": "Introduction to Reproducible Research",
    "section": "Equations",
    "text": "Equations\nQuarto supports LaTeX equations. You can include them in your document using the standard LaTeX syntax, for inline equations: $...$, and for display equations: $$...$$.\nFor example, the theory of relativity can be expressed as (\\(E=mc^2\\))$E=mc^2$. But also as:\n$$E = \\frac{{m \\cdot c^2}}{{\\sqrt{1 - \\frac{{v^2}}{{c^2}}}}}\n$$\n\\[E = \\frac{{m \\cdot c^2}}{{\\sqrt{1 - \\frac{{v^2}}{{c^2}}}}}\n\\]"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#equations-cross-references",
    "href": "rm-data/slides/week01/week01.html#equations-cross-references",
    "title": "Introduction to Reproducible Research",
    "section": "Equations cross-references",
    "text": "Equations cross-references\nDisplay equations can be cross-referenced. For that you need to add a label to the equation, and then reference it in the text.\nFor example:\n$$E = mc^2\n$${#eq-emc2}\n\\[E = mc^2\n\\qquad(1)\\]\nYou can reference this equation as @eq-emc2 which renders Equation 1.\nMore complex equations can be created using LaTeX syntax."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#tables-and-figures",
    "href": "rm-data/slides/week01/week01.html#tables-and-figures",
    "title": "Introduction to Reproducible Research",
    "section": "Tables and Figures",
    "text": "Tables and Figures\nFor Quarto, you can keep track of Tables and Figures. However, you can use “anything” as a figure or table.\nThe simplest approach is the following:\n\n\n:::{#tbl-mytable}\n  content...\n:::\n\n:::{#fig-myfig}\n  content...\n:::\n\nQuarto will not Care what is the content. A “Table” could be containing a figure, and a “Figure” could be a table.\nAnd you can reference them as @tbl-mytable and @fig-myfig. Quarto will take care of the numbering."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#tables-how",
    "href": "rm-data/slides/week01/week01.html#tables-how",
    "title": "Introduction to Reproducible Research",
    "section": "Tables: how?",
    "text": "Tables: how?\nYou can add tables 4 different ways:\n\nA file with markdown code: Markdown tables will render in any format.\nA file with LaTeX code: LaTeX tables will only render in PDF.\nA file with HTML code: HTML tables will render in any format.\n\nif you are using R, kable and gt will render tables in HTML and PDF."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#tables-markdown",
    "href": "rm-data/slides/week01/week01.html#tables-markdown",
    "title": "Introduction to Reproducible Research",
    "section": "Tables: Markdown",
    "text": "Tables: Markdown\n\n\n:::{#tbl-mytable}\n\n{{&lt; include table_example.txt &gt;}}\n\nThis are notes for the table\n\nAnd this the Title\n:::\n\nCrossreference:\n\n@tbl-mytable shows nothing.\n\n\n\n\nTable 1: And this the Title\n\n\n\n\n\nColumn 1\nColumn 2\nColumn 3\n\n\n\n\nData\nData\nData\n\n\nData\nData\nData\n\n\n\nThis are notes for the table\n\n\n\nCrossreference:\nTable 1 shows nothing.\n\nThe table is included in table_markdown.txt and is rendered in the document."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#tables-latex",
    "href": "rm-data/slides/week01/week01.html#tables-latex",
    "title": "Introduction to Reproducible Research",
    "section": "Tables: LaTeX",
    "text": "Tables: LaTeX\nThis wont render in HTML, but will render in PDF.\n\n\n:::{#tbl-mytable}\n\n{{&lt; include table_latex.txt &gt;}}\n\nThis are notes for the table\n\nAnd this the Title\n:::\n\n:::{#tbl-mytable}\n\n\\begin{tabular}{|c|c|c|}\n    \\hline\n    Column 1 & Column 2 & Column 3 \\\\\n    \\hline\n    Row 1 & Cell 1 & Cell 2 \\\\\n    \\hline\n    Row 2 & Cell 3 & Cell 4 \\\\\n    \\hline\n\\end{tabular}\n\nThis are notes for the table\n\nAnd this the Title\n:::"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#tables-html",
    "href": "rm-data/slides/week01/week01.html#tables-html",
    "title": "Introduction to Reproducible Research",
    "section": "Tables: HTML",
    "text": "Tables: HTML\nThere are two ways to include HTML tables in Quarto:\n\n\n:::{#tbl-mytable}\n\n{{&lt; include table_html.txt &gt;}}\n\nThis are notes for the table\n\nAnd this the Title\n:::\n\n:::{#tbl-mytable}\n\n```{=html}\n{{&lt; include table_html.txt &gt;}}\n```\n\nThis are notes for the table\n\nAnd this the Title\n:::"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#tables-figure",
    "href": "rm-data/slides/week01/week01.html#tables-figure",
    "title": "Introduction to Reproducible Research",
    "section": "Tables: Figure",
    "text": "Tables: Figure\nIf you are in a pinch, you can use a figure (from a well formated Excel file) as a table.\n\n\n:::{#tbl-mytable2}\n\n![](transition-git.jpg){width=50% fig-align=\"center\"}\n\nThis are notes for the table\n\nAnd this the Title\n:::\n\n\n\n\nTable 2: And this the Title\n\n\n\n\n\n\n\nThis are notes for the table"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#tables-comment",
    "href": "rm-data/slides/week01/week01.html#tables-comment",
    "title": "Introduction to Reproducible Research",
    "section": "Tables: comment",
    "text": "Tables: comment\nWhen producing tables for PDF, you may notice that tables do not always render where expected.\nLaTeX is trying to optimize the layout of the document.\nTo force a table to render where you want, you can use the tbl-pos=\"H\" attribute.\n:::{#tbl-mytable tbl-pos=\"H\"}\n\n{{&lt; include table_latex.txt &gt;}}\n\nThis are notes for the table\n\nAnd this the Title\n:::"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#figures",
    "href": "rm-data/slides/week01/week01.html#figures",
    "title": "Introduction to Reproducible Research",
    "section": "Figures",
    "text": "Figures\nAs with tables, you can include figures in your document using the following syntax:\n\n:::{#fig-myfig1}\n\n![](transition-git.jpg){width=50% fig-align=\"center\"}\n\nThis are notes for the Figure\n\nAnd this the title\n:::\nYou can use .png, .jpg, .svg, .pdf, and other image formats, with few restrictions.\nWorks across all formats.\n\n\n\n\n\n\n\n\n\nThis are notes for the Figure\n\n\nFigure 1: And this the title\n\n\n\n::::"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#footnotes",
    "href": "rm-data/slides/week01/week01.html#footnotes",
    "title": "Introduction to Reproducible Research",
    "section": "Footnotes",
    "text": "Footnotes\nFootnotes should not be used excessively, but they can be useful for additional information or references.\nTo add a footnote, use the following syntax:\nThis is a footnote[^1]. But this too ^[This is another footnote].\n\n[^1]: This is the text of the footnote.\nThis is a footnote1. But this too 2.\nThis is the text of the footnote.This is another footnote"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#citations-source-code",
    "href": "rm-data/slides/week01/week01.html#citations-source-code",
    "title": "Introduction to Reproducible Research",
    "section": "Citations: Source code",
    "text": "Citations: Source code\nQuarto supports citations using the @key syntax. It you cite a reference, it will be included in the bibliography.\nFirst, you need to include a .bib file with your references in the YAML header.\nbibliography: references.bib\nAccording to @smith2021, this citation works. However not everyone likes it [@doe2021]. However [@smith2021; @doe2021] works too.\nAccording to Smith (2021), this citation works. However not everyone likes it (Doe 2021). However (Smith 2021; Doe 2021) works too.\n\n\nDoe, Jane. 2021. “Advancements in Machine Learning Algorithms.” International Journal of Artificial Intelligence 5 (3): 50–65. https://doi.org/10.5678/ijai.2021.67890.\n\n\nSmith, John. 2021. “A Comprehensive Study on Data Analysis Techniques.” Journal of Data Science 10 (2): 100–120. https://doi.org/10.1234/jds.2021.12345."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#citations-visual-editor",
    "href": "rm-data/slides/week01/week01.html#citations-visual-editor",
    "title": "Introduction to Reproducible Research",
    "section": "Citations: Visual Editor",
    "text": "Citations: Visual Editor\nIf you are using Visual Editor (cntrl+shift+F4), you can add citations by using cntrl+shift+F8.\nThis will open a search box where you can search for the reference you want to cite. And then add it to the text.\nQuarto will add the reference.bib file to the YAML header, and will add the information to the bibliography."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#last-but-not-least-rendering",
    "href": "rm-data/slides/week01/week01.html#last-but-not-least-rendering",
    "title": "Introduction to Reproducible Research",
    "section": "Last but not least: Rendering",
    "text": "Last but not least: Rendering\nTo render your document you have two options:\n\nPress the Botton on the upper right corner of the VSC window. This creates a preview of the document, and you can see how it looks. This also renders one format at a time.\nUse the terminal. You can use the terminal to render the document in multiple formats at once.\n\nJust type: quarto render path_name/file.qmd and it will render the document in the formats specified in the YAML header.\n\nThere are other options, you can explore at your leisure."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#project-organization",
    "href": "rm-data/slides/week01/week01.html#project-organization",
    "title": "Introduction to Reproducible Research",
    "section": "Project Organization",
    "text": "Project Organization\nThis is one more thing we never really learn, but it is important! Declutter your projects!\nGood Organization will make it easy to find things, and to share your work with others.\n\nOne Project, One Folder\nUse consistent folder structures\nSeparate data, code, and output\nUse relative paths for reproducibility\nCreate a README file to guide others\n\n\nGood project organization is crucial for reproducibility and collaboration. It helps others (and your future self) understand and navigate your project easily."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#suggested-folder-structure",
    "href": "rm-data/slides/week01/week01.html#suggested-folder-structure",
    "title": "Introduction to Reproducible Research",
    "section": "Suggested Folder Structure",
    "text": "Suggested Folder Structure\n\nData and CodeResults and Documents\n\n\nroot-project/\n├── data/\n│   ├── raw/       &lt;- Depending on size: full raw data or instructions to get it.\n│   ├── processed/ &lt;- Stores your cleaned data\n│   ├── final/     &lt;- Opt: Stores the final Data for analysis\n│   └── doc/       &lt;- Opt: Manuals, dictionaries, codebooks, etc.\n├── code/          &lt;- Will contain all code files: Try to keep \"Small\" files (specific tasks).\n│   ├── 01-setup.do    &lt;- Setup file (installing packages, setting up directories, etc.)\n│   ├── 02-cleaning.do &lt;- Cleaning file (data cleaning and preprocessing)\n|   ├── 03-analysis.do &lt;- Analysis file (main analysis script)\n|   ├── 04-visuals.do  &lt;- Opt: Visualizations file and (creating plots and figures)\n\n\n├── results/\n│   ├── figures/ &lt;- All the figures you have created.\n│   ├── tables/  &lt;- All the tables you have created.\n│   └── other/   &lt;- .ster, .log, etc\n├── reports/\n│   ├── proposal/      &lt;- OPT: Include the proposal and any feedback you have received.\n│   ├── papers/        &lt;- May include the Raw tex (Qmd, Tex, bib) and the PDF\n│   └── presentations/ &lt;- If you have any presentations, include them here.\n└── README.md &lt;- A readme file with instructions on how to understand the project."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#package-management",
    "href": "rm-data/slides/week01/week01.html#package-management",
    "title": "Introduction to Reproducible Research",
    "section": "Package Management",
    "text": "Package Management\n\nDepending on the Software, it may be easier, or harder to manage packages.\n\nStata: ssc, net\nR: renv, packrat, checkpoint\nPython: pip, conda\n\nIdeally document versions, or “save” all the packages you are using.\n\nIn Stata, most required tools are available out of the box.\n\nYou may want to use ssc or net to install additional packages.\nBut some may require going to the authors website.\n\n\n\nPackage management ensures that your code runs with the same package versions across different environments, enhancing reproducibility."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#data-storage-and-sharing",
    "href": "rm-data/slides/week01/week01.html#data-storage-and-sharing",
    "title": "Introduction to Reproducible Research",
    "section": "Data Storage and Sharing",
    "text": "Data Storage and Sharing\n\nFor Small Projects, you can use GitHub to store your data.\nFor larger ones, use repositories like Zenodo or OSF for data sharing\nProvide metadata and documentation for your datasets. Specially when you create it.\nConsider data anonymization when necessary, or synthetic data if not possible to share.\nUse proper licensing for your data and code\n\n\nProper data management and sharing practices are essential for reproducibility and can increase the impact of your research."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#ai-and-generative-assistants",
    "href": "rm-data/slides/week01/week01.html#ai-and-generative-assistants",
    "title": "Introduction to Reproducible Research",
    "section": "AI and Generative Assistants",
    "text": "AI and Generative Assistants\n\nAI and GPT can assist in coding, writing, and data analysis. But be aware of their limitations.\nGitHub Copilot can help with code completion and suggestions, as well as text completition.\n\nIt uses your “work-environment” to suggest code, or comments.\n\nBut, like other tools, it is not perfect. It may suffer from allucinations, or code that does not work.\n\nIt is a tool, not a replacement for your work.\n\nUse it to speed up your work, but always check the results.\n\nExample: Recently, we use AI to go over many files of data summaries, and it was able to find the data we were looking for. Required some cleaning, but it was faster than doing it manually.\n\n\n\nAI coding assistants can significantly boost productivity, but it’s important to understand their capabilities and limitations, especially in a research context."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#example",
    "href": "rm-data/slides/week01/week01.html#example",
    "title": "Introduction to Reproducible Research",
    "section": "Example",
    "text": "Example\n\nWe will go over the repository for a mock-up project, created exclusively for this course.\nThe repository contains all the elements we have discussed: data, code, documents, and more.\nThis is a simple example, but it illustrates how you can structure your own projects for reproducibility and transparency.\nAssume the project to be a -mock- academic paper. Its not meant to be a real paper.\n\nRepository: https://github.com/friosavila/rm-example"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#thank-you",
    "href": "rm-data/slides/week01/week01.html#thank-you",
    "title": "Introduction to Reproducible Research",
    "section": "Thank You!",
    "text": "Thank You!\nQuestions? Feedback?\n\nThank you for participating in this course on reproducible research in economics. We hope you found it valuable and are excited to apply these tools and practices in your own work."
  },
  {
    "objectID": "rm-data/slides/week03.html#motivation",
    "href": "rm-data/slides/week03.html#motivation",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Motivation",
    "text": "Motivation\n\nYou want to understand the market conditions for hotels in Vienna, using prices.\n\nHow should you start the analysis itself?\nHow to describe the data and present the key features?\nHow to explore the data and check whether it is clean enough for (further) analysis?"
  },
  {
    "objectID": "rm-data/slides/week03.html#exploratory-data-analysis-eda---describing-variables",
    "href": "rm-data/slides/week03.html#exploratory-data-analysis-eda---describing-variables",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Exploratory data analysis (EDA) - describing variables",
    "text": "Exploratory data analysis (EDA) - describing variables\n5 reasons to do EDA:\n\nTo check data cleaning (part of iterative process)\nTo guide subsequent analysis (for further analysis)\nTo give context of the results of subsequent analysis (for interpretation)\nTo ask additional questions (for specifying the (research) question)\nOffer simple, but possibly important answers to questions.\n\nAll and all, EDA should help you identify some of the key features of the data and how they relate to each other."
  },
  {
    "objectID": "rm-data/slides/week03.html#key-tasks-describe-variables",
    "href": "rm-data/slides/week03.html#key-tasks-describe-variables",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Key tasks: describe variables",
    "text": "Key tasks: describe variables\n\n\nLook at key variables:\n\nwhat values they can take and\nhow often they take each of those values.\nare there extreme values\n\n\nDescribe what you see:\n\nDescriptive statistics: key features summarized\nto understand variables you work with\nto make comparisons"
  },
  {
    "objectID": "rm-data/slides/week03.html#frequency-values",
    "href": "rm-data/slides/week03.html#frequency-values",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Frequency values",
    "text": "Frequency values\n\nThe frequency or more precisely, absolute frequency or count, of a value of a variable is simply the number of observations with that particular value.\nThe relative frequency is the frequency expressed in relative, or percentage, terms: the proportion of observations with that particular value among all observations.\n\nIf missing values exist, Relative frequency could be relative to total observations or to non-missing observations.\n\nCan also use Probabilities: the relative likelihood of a value of a variable.\nHow to? tabulate [variable], or better yet fre [variable] (SSC install)"
  },
  {
    "objectID": "rm-data/slides/week03.html#the-distribution",
    "href": "rm-data/slides/week03.html#the-distribution",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The distribution",
    "text": "The distribution\nA key part of EDA is to look at (empirical) distribution of most important variables.\n\nAll variables have a distribution.\nThe distribution determines the frequency of each value in the data.\nMay be expressed in terms of absolute frequencies (number of observations) or relative frequencies (percent of observations).\nThe distribution of a variable completely describes the variable as it occurs in the data."
  },
  {
    "objectID": "rm-data/slides/week03.html#histograms",
    "href": "rm-data/slides/week03.html#histograms",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Histograms",
    "text": "Histograms\nHistogram reveals important properties of a distribution:\n\nAs with tabulation, Histograms show the empirical distribution of a variable.\nThey may allow you to see:\n\nNumber and location of modes: these are the peaks in the distribution (compared to neighbors).\n\nShape of the distribution:\n\nCenter, tails, if its symmetric or not, Long [left or right tails], and extreme values.\n\nExtreme values: values that are very different from the rest. Extreme values are at the far end of the tails of histograms. (may even be signal of errors or missing)"
  },
  {
    "objectID": "rm-data/slides/week03.html#extreme-values",
    "href": "rm-data/slides/week03.html#extreme-values",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Extreme values",
    "text": "Extreme values\n\nExtreme values are substantially larger or smaller values for one or a handful of observations. Big departures from distribution.\nNeed conscious decision.\n\nIs this an error? (drop or replace)\nIs this not an error, code for missing? (replace)\nIs this not an error but not part of what we want to talk about? (drop?)\nIs this an integral feature of the data? (keep)"
  },
  {
    "objectID": "rm-data/slides/week03.html#how-to",
    "href": "rm-data/slides/week03.html#how-to",
    "title": "Exploratory Analysis and Comparisons",
    "section": "How to?",
    "text": "How to?\nHistograms in Stata are created with the histogram command.\nhistogram [variable] [if in] [fweight], [bin(#) width(#) discrete] ///\n          [density] [frequency] [fraction] \n\nYou can only create the histogram of one variable at a time. (unless combined)\nand you can determine how “fine” or “coarse” the histogram is. (A bit of art)"
  },
  {
    "objectID": "rm-data/slides/week03.html#hotel-stars-histograms",
    "href": "rm-data/slides/week03.html#hotel-stars-histograms",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Hotel Stars histograms",
    "text": "Hotel Stars histograms\n\n\n\n\nCode\nqui:histogram stars, d frequency ///\n    scale(1.5) addlabels xlabel(1(.5)5)\n\n\n\n\n\nAbsolute frequency\n\n\n\n\n\n\n\nCode\nqui:histogram stars, d percent ///\n    scale(1.5) addlabels xlabel(1(.5)5)\n\n\n\n\n\nRelative frequency"
  },
  {
    "objectID": "rm-data/slides/week03.html#hotel-price-histograms",
    "href": "rm-data/slides/week03.html#hotel-price-histograms",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Hotel price histograms",
    "text": "Hotel price histograms\n\n\n\n\nCode\nqui:histogram price, d  ///\n    scale(1.5) width(1) \n\n\n\n\n\nPrice Distribution 1$ bin\n\n\n\n\n\n\n\nCode\nqui:histogram price,  ///\n    scale(1.5) width(30)\n\n\n\n\n\nPrice Distribution 30$ bin"
  },
  {
    "objectID": "rm-data/slides/week03.html#alternative-kdensity",
    "href": "rm-data/slides/week03.html#alternative-kdensity",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Alternative Kdensity",
    "text": "Alternative Kdensity\n\nPerhaps one weakness of Histograms are the implicit binning. The density “jumps” from bin to bin.\nAn alternative would be use smaller bins, requesting jumps to be smoother.\nThis is done with Kernel Density Estimation (KDE) plots. kdensity in Stata.\nTwo limitations:\n\nNot useful with discrete or limited variables\nAlso requires the use of bandwiths"
  },
  {
    "objectID": "rm-data/slides/week03.html#kdensity-for-price",
    "href": "rm-data/slides/week03.html#kdensity-for-price",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Kdensity for price",
    "text": "Kdensity for price\n\nExmp1Exmp2Exmp3\n\n\n\n\nCode\n *| kdensity price,   ///\n    scale(1.5)  note(\"\") bw(10)\n\n\n\n\n\n\nCode\n kdensity price,   ///\n    scale(1.5)  note(\"\") bw(1)\n\n\n\n\n\nBandwidth of 1\n\n\n\n\n\n\n\n\nCode\n kdensity price,   ///\n    scale(1.5)  note(\"\") bw(30)\n\n\n\n\n\nBandwidth of 30"
  },
  {
    "objectID": "rm-data/slides/week03.html#eda-and-cleaning---vienna-hotels",
    "href": "rm-data/slides/week03.html#eda-and-cleaning---vienna-hotels",
    "title": "Exploratory Analysis and Comparisons",
    "section": "EDA and cleaning - Vienna hotels",
    "text": "EDA and cleaning - Vienna hotels\n\nStart with full data N=428\nTabulate key qualitative variables\nAccommodation type - could be apartment, etc. Focus on hotels. N=264\nStars - focus on 3, 3.5, 4 stars. &lt;3 not well covered, &gt;4 vary a lot. N=218\nLook at quantitative variables, focus on extreme values.\nStart with price. p=1012 likely error drop. keep others N= 217\nDistance: some hotels are far away. define cutoff. drop beyond 8km N=214\nCheck why hotels could be far away. Find variable city_actual. Tabulate. Realise few hotels are not in Vienna. Drop them. N=207\nthe final cut: Hotels, 3 to 4 stars, below 1000 euros, less than 8km from center, in Vienna actual N=207."
  },
  {
    "objectID": "rm-data/slides/week03.html#what-are-summary-statistics",
    "href": "rm-data/slides/week03.html#what-are-summary-statistics",
    "title": "Exploratory Analysis and Comparisons",
    "section": "What are summary statistics?",
    "text": "What are summary statistics?\n\nSummary statistics are numbers that summarize the distribution of a variable.\n\nThey provide numbers for the central tendency, spread, and shape.\n\nSummary statistics are used to describe the data and to make comparisons between different datasets."
  },
  {
    "objectID": "rm-data/slides/week03.html#summary-statistics-central-tendency",
    "href": "rm-data/slides/week03.html#summary-statistics-central-tendency",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Summary statistics: Central Tendency",
    "text": "Summary statistics: Central Tendency\n\nThe most used statistic is the mean:\n\n\\[\\bar{x} = \\frac{\\sum x_i}{n}\\]\nwhere \\(x_i\\) is the value of variable \\(x\\) for observation \\(i\\) in the dataset that has \\(n\\) observations in total. Two key features:\n\nAdd a constant, the mean changes by the same constant.\nMultiply by a constant, the mean changes by the same constant."
  },
  {
    "objectID": "rm-data/slides/week03.html#the-expected-value",
    "href": "rm-data/slides/week03.html#the-expected-value",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The Expected value",
    "text": "The Expected value\n\nThe expected value is the value that one can expect for a randomly chosen observation. It relates to the distribution of the population, not the sample\nThe notation for the expected value is \\(E[x]\\).\nFor a quantitative variable, the expected value is the mean\nFor a qualitative variable, means are not defined, but you can consider proportions."
  },
  {
    "objectID": "rm-data/slides/week03.html#the-median-and-other-quantiles",
    "href": "rm-data/slides/week03.html#the-median-and-other-quantiles",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The median and other quantiles",
    "text": "The median and other quantiles\n\nThe median is another statistic of central tendency. It indicates the middle value of the distribution. Its a special case of quantiles.\n\nIts main advantage with the mean is that it is less sensitive to extreme values.\n\nquantiles: a quantile is the value that divides the observations in the dataset to two parts in specific proportions.\n\\[Q_\\tau(Y) \\rightarrow \\frac{1}{N}\\sum I(y&lt;Q_\\tau) = \\tau \\]\nThe median and 25th and 75th percentiles are the most common quantiles used in EDA."
  },
  {
    "objectID": "rm-data/slides/week03.html#the-mode",
    "href": "rm-data/slides/week03.html#the-mode",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The mode",
    "text": "The mode\n\nYet another measure of central tendency.\nThe mode is the value with the highest frequency in the data (the most common).\nIf distributions have multimodal, you may be able to obtain multiple modes.\nMultiple modes are apart from each other, each standing out in its “neighborhood”, but they may have different frequencies."
  },
  {
    "objectID": "rm-data/slides/week03.html#summary",
    "href": "rm-data/slides/week03.html#summary",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Summary",
    "text": "Summary\n\nThe mean, median and mode are different statistics for the central value of the distribution\nThey try to provide you the most representative value of the distribution.\n\nThe mode is the most frequent value\nThe median is the middle value\nThe mean is the value that one can expect for a randomly chosen observation.\n\n\ntabstat vars, stats(mean p50 )\nsummarize vars, detail"
  },
  {
    "objectID": "rm-data/slides/week03.html#spread-of-distributions",
    "href": "rm-data/slides/week03.html#spread-of-distributions",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Spread of distributions",
    "text": "Spread of distributions\n\nSpread of distributions is often used in analysis.\n\nIt tells you how concentrated or dispersed the values of a variable are.\n\nThe statistics that measure the spread of distributions are the range, inter-quantile ranges, the standard deviation and the variance."
  },
  {
    "objectID": "rm-data/slides/week03.html#ranges",
    "href": "rm-data/slides/week03.html#ranges",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Ranges",
    "text": "Ranges\nThere are three common measures of ranges:\n\nThe range is the difference between the highest value (the maximum) and the lowest value (the minimum) of a variable.\nThe inter-quantile ranges is the difference between two quantiles- the third quartile (the 75th percentile) and the first quartile (the 25th percentile). Can be used as an alternative to Standard deviation.\nThe 90-10 percentile range gives the difference between the 90th percentile and the 10th percentile."
  },
  {
    "objectID": "rm-data/slides/week03.html#standard-deviation",
    "href": "rm-data/slides/week03.html#standard-deviation",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Standard deviation",
    "text": "Standard deviation\nThe most widely used measure of spread is the standard deviation, and Its square is the variance.\n\\[\n\\begin{aligned}\nVar[x] &= \\frac{\\sum (x_i - \\bar{x})^2}{n}=S^2_x \\\\\nStd[x] &= \\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n}}=S_x\n\\end{aligned}\n\\]\n\nThe variance is less intuitive measure (Squared), but easier to work with (mean)\nThe SD captures typical (not Mean) differences from the mean.\nFor the same mean, higher SD means more volatility."
  },
  {
    "objectID": "rm-data/slides/week03.html#coefficient-of-variation",
    "href": "rm-data/slides/week03.html#coefficient-of-variation",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Coefficient of variation",
    "text": "Coefficient of variation\nUnit Free alternative, Coefficient of variation:\n\\[CV = \\frac{Std[x]}{\\bar{x}}\n\\]\n\nThe coefficient of variation is the standard deviation divided by the mean. It reads, how much variation is there in the data relative to the mean."
  },
  {
    "objectID": "rm-data/slides/week03.html#other-uses-for-sd-standardized-values",
    "href": "rm-data/slides/week03.html#other-uses-for-sd-standardized-values",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Other uses for SD: Standardized values",
    "text": "Other uses for SD: Standardized values\nThe SD is often used to re-calculate differences between values in order to express them as typical distance.\n\\[x_{standardized} = \\frac{(x - \\bar{x})}{Std[x]}\n\\]\n\nThe standardized value has a mean of zero and a standard deviation of one.\nRepresents the difference from the mean in units of standard deviation.\n\nFor example: a standardized value of one shows a value is one standard deviation larger than the mean; a standardized value of negative one shows a value is one standard deviation smaller than the mean"
  },
  {
    "objectID": "rm-data/slides/week03.html#distribution-shape-skewness",
    "href": "rm-data/slides/week03.html#distribution-shape-skewness",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Distribution Shape: Skewness",
    "text": "Distribution Shape: Skewness\n\nA distribution is skewed if it isn’t symmetric.\n\nIt may be skewed in two ways, having a long left tail or having a long right tail.\nExample: hotel price distributions having a long right tail\n\nSkewness and the presence of extreme values are related.\nWhen extreme values are important for the analysis, skewness of distributions is important, too."
  },
  {
    "objectID": "rm-data/slides/week03.html#skewness-measures",
    "href": "rm-data/slides/week03.html#skewness-measures",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Skewness measures",
    "text": "Skewness measures\nThere are two common measures of skewness:\n\\[\nSk^1 = \\frac{(\\bar{x} - median(x))}{Std[x]} \\text{ and } Sk^2 = \\frac{\\sum(x_i-\\bar x)^3}{Std[x]^3}\n\\]\n\nWhen the distribution is symmetric its mean = median.\nSkewed to the right \\(\\bar x &gt; Q_{50}(x)\\).\nWhen a distribution is skewed with a long left tail the mean is smaller than the median\nTo make this measure comparable, better to standardize the measure\n\\(SK^2\\) is another Skewness measure. if Possitive, Skewed to the right, if negative, to the left."
  },
  {
    "objectID": "rm-data/slides/week03.html#stata-corner-how-to",
    "href": "rm-data/slides/week03.html#stata-corner-how-to",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Stata Corner: How to?",
    "text": "Stata Corner: How to?\nTwo basic options to get summary statistics in Stata:\n\nsummarize command: provides basic statistics for all variables in the dataset. Include detail option for more statistics.\ntabstat command: provides more flexibility. You can choose which statistics to show and for which variables.\nuse estpost to store the results and create well formatted tables.\nSee Stata Summary Statistics for examples on how to use these commands."
  },
  {
    "objectID": "rm-data/slides/week03.html#visualizing-summary-statistics",
    "href": "rm-data/slides/week03.html#visualizing-summary-statistics",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Visualizing summary statistics",
    "text": "Visualizing summary statistics\n\nAs mentioned before Histograms are a good way to visualize the distribution of a variable.\nHowever, if you would like to also visualize the summary statistics, you can use box plots\nThe box plot is a visual representation of many quantiles and extreme values."
  },
  {
    "objectID": "rm-data/slides/week03.html#box-plot",
    "href": "rm-data/slides/week03.html#box-plot",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Box Plot",
    "text": "Box Plot\n\nFull SampleBy Stars\n\n\n\n\nCode\nuse data_slides/hotels-vienna.dta, clear\nqui:drop if price&gt;800\ngraph box price, scale(1.4)  ///\n  ytitle(\"Price in dollars (log Scale)\") \n\n\n\n\n\nBox Plot: Viena prices\n\n\n\n\n\n\n\n\nCode\nuse data_slides/hotels-vienna.dta, clear\nqui:drop if price&gt;800\n graph box price if stars&gt;1, scale(1.4) ///\n  over(stars)  xsize(10) ysize(4) ///\n  ytitle(\"Price in dollars (log Scale)\")  \n\n\n\n\n\nBox Plot: Viena prices"
  },
  {
    "objectID": "rm-data/slides/week03.html#theoretical-distributions",
    "href": "rm-data/slides/week03.html#theoretical-distributions",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Theoretical distributions",
    "text": "Theoretical distributions\n\nTheoretical distributions are distributions of variables with idealized properties.\nTheoretical distributions are fully captured by few parameters: these are statistics determine the whole distributions\nFor example, the normal distribution is fully captured by two parameters: the mean and the standard deviation.\nThey may not accomodate empirical data"
  },
  {
    "objectID": "rm-data/slides/week03.html#theoretical-distributions-1",
    "href": "rm-data/slides/week03.html#theoretical-distributions-1",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Theoretical distributions",
    "text": "Theoretical distributions\nTheoretical distributions can be helpful:\n\nHave well-known properties!\nIn real life, many variables surprisingly close to theoretical distributions.\nWill be useful when generalizing from data"
  },
  {
    "objectID": "rm-data/slides/week03.html#the-normal-distribution",
    "href": "rm-data/slides/week03.html#the-normal-distribution",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The Normal distribution",
    "text": "The Normal distribution\n\n\n\nHistogram is bell-shaped\nOutcome (event), can take any value\nDistribution is captured by \\(\\mu\\) the mean and \\(\\sigma\\) the SD\nSymmetric = median, mean (and mode) are the same."
  },
  {
    "objectID": "rm-data/slides/week03.html#the-log-normal-distribution",
    "href": "rm-data/slides/week03.html#the-log-normal-distribution",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The log-normal distribution",
    "text": "The log-normal distribution\n\n\n\nAsymmetrically distributed with long right tails.\n\nDerived from a normally distributed variable (x), transform it: (\\(x^* = e^x\\)). The result is a distributed log-normal.\n\nAlways non-negative\nExample distributions of income, or firm size.\n\n\n\n\nNumber of observations (_N) was 0, now 10,000.\n(bin=40, start=.79124224, width=1.5880267)"
  },
  {
    "objectID": "rm-data/slides/week03.html#the-normality-of-reality",
    "href": "rm-data/slides/week03.html#the-normality-of-reality",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The Normality of Reality",
    "text": "The Normality of Reality\n\nQuite suprisingly, many variables tend to follow normal distributions.\n\nEspecially when adding them up.\n\nMay not be a good approximation when\n\nThere are reasons for non-symmetry (e.g. income)\nIf extreme values are “common”\n\nVariables are well approximated by the log-normal if they are the result of many things multiplied."
  },
  {
    "objectID": "rm-data/slides/week03.html#data-visualization-essentials",
    "href": "rm-data/slides/week03.html#data-visualization-essentials",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Data Visualization Essentials",
    "text": "Data Visualization Essentials\n\n\n\nPurposeful Decision-Making\n\nAvoid default settings\nDefine purpose, focus, and audience\nChoose appropriate graph type\n\nKey Considerations\n\nData type (qualitative, quantitative, time series)\nFormatting (colors, fonts, sizes)\nEssential elements: title, axis labels, legend\n\n\n\n\nOne Graph, One Message\n\nTailor complexity to audience (general vs. specialist)\nBe explicit about purpose and target audience"
  },
  {
    "objectID": "rm-data/slides/week03.html#data-visualization-process",
    "href": "rm-data/slides/week03.html#data-visualization-process",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Data Visualization Process",
    "text": "Data Visualization Process\n\n\n\nPlanning\n\nDetermine content and audience\nSelect graph type and elements\nSeek help when needed (AI, online resources)\n\nExecution\n\nInclude supporting elements for understanding\nEnsure readability (use scale() function)\n\n\n\n\nEssential Components\n\nTitle (if not in the document)\nAxis titles and labels (what’s being measured)\nLegend (group explanations)\n\nFinal Check\n\nVerify all elements support the main message\nConfirm graph is clear and accessible to the audience\n\n\n\nSee DataViz for a guide of how to create graphs in Stata."
  },
  {
    "objectID": "rm-data/slides/week03.html#ai-and-data-explorationviz",
    "href": "rm-data/slides/week03.html#ai-and-data-explorationviz",
    "title": "Exploratory Analysis and Comparisons",
    "section": "AI and data exploration/Viz",
    "text": "AI and data exploration/Viz\n\nAI is very good at describing the data, if you give it the tools (data)\nPretty good with python, but less proficient with Stata for complex graphs.\nStill good to have someone to ask without judgement."
  },
  {
    "objectID": "rm-data/slides/week03.html#summary-steps-of-eda",
    "href": "rm-data/slides/week03.html#summary-steps-of-eda",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Summary steps of EDA",
    "text": "Summary steps of EDA\n\nFirst focus on the most important variables. Go back to look at others if subsequent analysis suggests to.\nFor qualitative variables, list relative frequencies.\nFor quantitative variables, look at histograms. May decide for transformation, learn about key aspects of data.\nCheck for extreme values. Decide what to do with them.\nLook at summary statistics. It may prompt actions, such as focusing on some part of the dataset.\nDo further exploration if necessary (time series data, comparisons across groups of observations, correlations, etc.)"
  },
  {
    "objectID": "rm-data/slides/week03.html#motivation-1",
    "href": "rm-data/slides/week03.html#motivation-1",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Motivation",
    "text": "Motivation\n\nAre larger companies better managed?\n\nAnswering this question may help in benchmarking management practices in a specific company, assessing the value of a company, or estimating the potential benefits of a merger between two companies.\nTo answer this question you downloaded data from the World Management Survey.\n\nHow should you use the data to measure firm size and the quality of management?\nHow should you assess whether larger companies are better managed?"
  },
  {
    "objectID": "rm-data/slides/week03.html#the-y-and-the-x",
    "href": "rm-data/slides/week03.html#the-y-and-the-x",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The \\(y\\) and the \\(x\\)",
    "text": "The \\(y\\) and the \\(x\\)\n\nMuch of data analysis is built on comparing values of a \\(y\\) variable against one, or more, \\(x\\) variables.\nOur job is to uncover the patterns of association:\n\nHow observations with particular values of one variable (\\(x\\)) tend have particular values of the other variable (\\(y\\)).\n\nThe role of \\(y\\) is different from the role of \\(x\\).\n\nWe are interested in \\(y\\)\n\\(X's\\) are factors that you will use to analyze \\(y\\)."
  },
  {
    "objectID": "rm-data/slides/week03.html#the-y-and-the-x-1",
    "href": "rm-data/slides/week03.html#the-y-and-the-x-1",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The \\(y\\) and the \\(x\\)",
    "text": "The \\(y\\) and the \\(x\\)\n\nThis asymmetry comes from the goal of our analysis.\nGoal 1: predicting the value of a \\(y\\) variable with the help of other variables - many \\(x\\) variables, such as \\(x_1\\), \\(x_2\\),…\n\nThis is more useful when we do not know \\(y\\) but know \\(x\\).\n\nGoal 2: learn about the effect of a causal variable \\(x\\) on an outcome variable \\(y\\).\nAssuming everything else remains constant, What the value of \\(y\\) would be if we could change \\(x\\)"
  },
  {
    "objectID": "rm-data/slides/week03.html#comparison-and-conditioning",
    "href": "rm-data/slides/week03.html#comparison-and-conditioning",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Comparison and conditioning",
    "text": "Comparison and conditioning\n\nSimilar ideas: Comparison \\(\\rightarrow\\) conditioning\nWe compare \\(y\\), by values of \\(x\\) \\(\\rightarrow\\) we condition y on x.\n\n\\(x\\) (by the values of which we make comparisons) \\(\\rightarrow\\) conditioning variable.\n\\(y\\) \\(\\rightarrow\\) outcome variable.\n\nCompare salaries of workers (\\(y\\)) with low and high level of education (\\(x\\))\n\nsalary is the outcome\neducation is the conditioning variable."
  },
  {
    "objectID": "rm-data/slides/week03.html#comparisons-and-conditional-distributions",
    "href": "rm-data/slides/week03.html#comparisons-and-conditional-distributions",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Comparisons and conditional distributions",
    "text": "Comparisons and conditional distributions\n\n\n\nThe conditional distribution of a variable is the distribution of the outcome variable given the conditioning variable: \\(f(y|X=x)\\)\nStraightforward if \\(x\\) is qualitative (simple if binary)\nWith quantitative variables, this definition is less intuitive.\n\n\n\n\nCode\nqui: use \"data_slides/hotels-vienna-london\", clear\ndrop if price &gt; 1000\nset scheme white2\ncolor_style tableau\ntwo (kdensity price) ///\n(kdensity price if city==\"Vienna\") ///\n(kdensity price if city==\"London\"), ///\nlegend(order(1 \"All\" 2 \"Vienna\" 3 \"London\")) ///\nxtitle(\"Hotel Prices\") xsize(9) ysize(6)\n\n\n(395 observations deleted)"
  },
  {
    "objectID": "rm-data/slides/week03.html#conditional-statistic",
    "href": "rm-data/slides/week03.html#conditional-statistic",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Conditional statistic",
    "text": "Conditional statistic\nIf there is a Conditional distribution, there is a conditional statistic.\n\nConditional Stat is the Stat of a variable for each value of the conditioning variable.\n\nThe conditional expectation of variable y for different values of variable \\(x\\) is \\(E[y|x]\\)\n\nThis is a function: for a value of \\(x\\), the conditional expectation is the expected value of \\(y\\) for observations that have that \\(x\\) value\nIt gives different values for different values of \\(x\\)."
  },
  {
    "objectID": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size",
    "href": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Case Study - Management quality and firm size",
    "text": "Case Study - Management quality and firm size\n\nQuestion: Are larger Firms Better managed?\nData: World Management Survey\nAnswering this questions may help inform policy decisions.\nHow to measure firm size and quality of management?"
  },
  {
    "objectID": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size-1",
    "href": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size-1",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Case Study - Management quality and firm size",
    "text": "Case Study - Management quality and firm size\n\nInterviews by CEO/senior managers, based on that a score is given. Average across different domains.\n\ntracking and reviewing performance or\ntime horizon and breadth of targets, etc\n\nNormalized - standardized score\nFirm size: Consider three bins: small (100–199), medium (200–999), large (1000+)"
  },
  {
    "objectID": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size-2",
    "href": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size-2",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Case Study - Management quality and firm size",
    "text": "Case Study - Management quality and firm size\n\n\n\n\n\n\n\nmean\np50\nsd\n\n\n\n\nSmall\n2.68\n2.78\n0.51\n\n\nMedium\n2.94\n3.00\n0.62\n\n\nLarge\n3.19\n3.08\n0.55\n\n\nTotal\n2.94\n2.94\n0.60\n\n\nObservations\n300\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmall\nMedium\nLarge\nTotal\n\n\n\n\n1\n19.44\n8.33\n6.94\n10.67\n\n\n2\n37.50\n28.85\n26.39\n30.33\n\n\n3\n31.94\n35.90\n30.56\n33.67\n\n\n4\n11.11\n21.79\n27.78\n20.67\n\n\n5\n0.00\n5.13\n8.33\n4.67\n\n\nTotal\n100.00\n100.00\n100.00\n100.00\n\n\nN\n300"
  },
  {
    "objectID": "rm-data/slides/week03.html#other-options",
    "href": "rm-data/slides/week03.html#other-options",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Other options",
    "text": "Other options\n\nSince \\(x\\) is qualitative, and there are “enough” observations in each category, its also posible to plot the conditional distribution of \\(y\\) for each value of \\(x\\).\n\ntwo (histogram management ), by(firm_size) \ntwo (kdensity management ), by(firm_size) \ngraph box management, over(firm_size) intensity(30)"
  },
  {
    "objectID": "rm-data/slides/week03.html#conditional-and-joint-distributions",
    "href": "rm-data/slides/week03.html#conditional-and-joint-distributions",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Conditional and joint distributions",
    "text": "Conditional and joint distributions\n\nThe previous Design assume \\(x\\) to be discrete (Made Discrete). But what if not? Too many values!.\n\nNeed to think about joint distributions\n\nThe joint distribution of two variables shows the probabilities (frequencies) of each value combination of the two variables.\nA scatter plot is a two-dimensional graph with the values of each of the two variables measured on its two axes.\nWorks better when dataset is relatively small.\nFor larger samples, we can bin values, and use “bin scatter”\nBin scatter shows conditional means for bins we created"
  },
  {
    "objectID": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size-3",
    "href": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size-3",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Case Study - Management quality and firm size",
    "text": "Case Study - Management quality and firm size\n\n\n\n\nCode\nscatter management emp_firm, xtitle(\"Firm size\") ytitle(\"Management score\") ///\n  legend(off) scale(1.5) \n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsort emp_firm\nqui:drop2 emp_firm_bin emp_mean_bin\nxtile emp_firm_bin = _n, n(20)\nbysort emp_firm_bin: egen emp_mean_bin=mean(emp_firm)\nbysort emp_firm_bin:egen mean_mng=mean(management)\n\nscatter mean_mng emp_mean_bin, xtitle(\"Firm size\") ytitle(\"Management score\") ///\n  scale(1.5) legend(off) ylabel(1/5) ///\n  note(\"Using 20 bins\")\n\n\nvariable emp_firm_bin not found\nvariable emp_mean_bin not found\n\n\n\n\n\n\n\n\n\n\n\nSome association shown. Scatter not easy to read, bin-scatter shows positive (weak) association. Notice Scale of y-axis. Flat line for large firms"
  },
  {
    "objectID": "rm-data/slides/week03.html#other-options-1",
    "href": "rm-data/slides/week03.html#other-options-1",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Other Options",
    "text": "Other Options\n\nModel Based ScatterplotScale Change\n\n\n\n\nCode\ntwo (scatter management emp_firm) ///\n    (lfitci management emp_firm, fcolor(%30)), ///\n    xtitle(\"Firm size\") ytitle(\"Management score\") ///\n    legend(off) scale(1.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nscatter management emp_firm, ///\n  xtitle(\"Firm size\") ytitle(\"Management score\") ///\n  xscale(log) scale(1.5) xlabel(100 250 500 1000 2000 3000 4000 5000)"
  },
  {
    "objectID": "rm-data/slides/week03.html#dependence-and-independence",
    "href": "rm-data/slides/week03.html#dependence-and-independence",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Dependence and independence",
    "text": "Dependence and independence\n\nDependence of two variables \\(y\\) and \\(x\\) means that the conditional distributions of \\(y|x\\) changes with \\(x\\)\n\nThis is what we showed earlier\n\nIndependence means the opposite: the distribution of \\(y|x\\) is the same, regardless of the value of \\(x\\).\nDependence, may take many forms.\n\n\\(y\\) may be more or less spread out for different \\(x\\) values.\nthe mean of \\(y\\) is different for different \\(x\\) values.\n\n\n\\[E[y|X=x_1] \\neq E[y|X=x_2]\\]"
  },
  {
    "objectID": "rm-data/slides/week03.html#mean-dependence",
    "href": "rm-data/slides/week03.html#mean-dependence",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Mean dependence",
    "text": "Mean dependence\n\nMean-dependence: conditional expectation \\(E[y|x]\\) varies with the value of \\(x\\).\nTwo variables are positively mean-dependent if the average of two variables increase together.\nCovariance and Correlation Coefficient are measures of mean linear dependence.\n\nThey measure the same thing, but the correlation coefficient is a standardized version of the covariance."
  },
  {
    "objectID": "rm-data/slides/week03.html#covariance",
    "href": "rm-data/slides/week03.html#covariance",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Covariance",
    "text": "Covariance\nThe formula for the covariance between two variables \\(x\\) and \\(y\\) with n observations is:\n\\[Cov[x, y] = \\frac{1}{n}\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})\n\\]\n\nThe Covariance is the average of the product of the deviations of the two variables from their respective means.\nPositive covariance: positive deviations of \\(x\\) go with positive deviations of \\(y\\).\nNegative covariance: positive deviations of \\(x\\) go with negative deviations of \\(y\\)."
  },
  {
    "objectID": "rm-data/slides/week03.html#the-correlation-coefficient",
    "href": "rm-data/slides/week03.html#the-correlation-coefficient",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The correlation coefficient",
    "text": "The correlation coefficient\n\\[Corr[x, y] = \\rho_{xy}= \\frac{Cov[x, y]}{Std[x]Std[y]}\\]\n\\[-1 \\leq Corr[x, y] \\leq 1\\]\n\nThe correlation coefficient is the standardized version of the covariance.\nIt is bound to be between negative one and positive one."
  },
  {
    "objectID": "rm-data/slides/week03.html#dependence-mean-dependence-correlation",
    "href": "rm-data/slides/week03.html#dependence-mean-dependence-correlation",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Dependence, mean-dependence, correlation",
    "text": "Dependence, mean-dependence, correlation\n\n\n\n\n\n\nNote\n\n\nIf two variables are independent, they are also mean-independent, Thus \\(E[y|x] = E[y]\\) of any value of x.\n\n\n\n\n\nIs this true the other way around?\n\nNo, it is not.\n\nSpecial cases:\n\n\\(\\rho = 0\\) but mean dependence (Sqrt of x)\n\\(\\rho = 0\\) and mean independence but different spread of \\(y\\) (Heteroskedasticity)"
  },
  {
    "objectID": "rm-data/slides/week03.html#case-study---management-quality-firm-size-industry",
    "href": "rm-data/slides/week03.html#case-study---management-quality-firm-size-industry",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Case Study - Management quality, firm size, Industry",
    "text": "Case Study - Management quality, firm size, Industry\n\nMeasures of management quality and their correlation with size by industry\n\n\nIndustry\nCorrelation\nObservations\n\n\n\n\nAuto\n0.50\n26\n\n\nChemicals\n0.05\n69\n\n\nElectronics\n0.33\n24\n\n\nFood, drinks, tobacco\n0.05\n34\n\n\nMaterials, metals\n0.32\n50\n\n\nTextile, apparel\n0.29\n43\n\n\nWood, furniture, paper\n0.28\n29\n\n\nOther\n0.44\n25\n\n\nAll\n0.30\n300"
  },
  {
    "objectID": "rm-data/slides/week03.html#measuring-a-latent-concept",
    "href": "rm-data/slides/week03.html#measuring-a-latent-concept",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Measuring a latent concept",
    "text": "Measuring a latent concept\n\nOften a concept is hard, even impossible, to measure…directly\nWe often call them Latent variables: A variable that is not observed nor can be measured.\nExamples:\n\nQuality of management at a firm - it is a concept that may be measured with a collection of variables, not a single one of them\nIQ - measured by a series of quiz-like questions.\nEmployment satisfaction - measured by a series of questions about the job\n\nHow do you combine multiple observed variables"
  },
  {
    "objectID": "rm-data/slides/week03.html#condensing-information",
    "href": "rm-data/slides/week03.html#condensing-information",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Condensing information",
    "text": "Condensing information\n\nAlternatives:\n\nUse one observed variable only: perhaps the one that is the best measure\nUse all variables individually\nSummarize them into a single variable\n\nUse a weighted average of all variables\nPrincipal component analysis (PCA)\nLatent variable analysis, ETC"
  },
  {
    "objectID": "rm-data/slides/week03.html#using-a-single-variable-or-a-few",
    "href": "rm-data/slides/week03.html#using-a-single-variable-or-a-few",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Using a single variable (or a few)",
    "text": "Using a single variable (or a few)\n\nUsing one measured variable and exclude the rest has the advantage of easy interpretation.\n\nThe others could be used for robustness checks\n\nIt has the disadvantage of discarding potentially useful information.\nBut, can be often a sensible start"
  },
  {
    "objectID": "rm-data/slides/week03.html#using-an-weighted-average",
    "href": "rm-data/slides/week03.html#using-an-weighted-average",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Using an [Weighted] Average",
    "text": "Using an [Weighted] Average\n\nTaking the average of all measured variables makes use of all information.\n\n\\[\\bar{z_i} = \\frac{1}{k}\\sum_{j=1}^k z_i^j \\text{ or }\n\\bar{z_i} = \\frac{\\sum_{j=1}^k w_j \\times z_i^j}{\\sum_{j=1}^k w_j}\n\\]\n\nAll should be measured in the same Scale. Simple and a natural interpretation\nYou can also use weights to give more importance to some variables than others.\nOr can use sub-groups indices to create a composite index."
  },
  {
    "objectID": "rm-data/slides/week03.html#using-an-weighted-average-1",
    "href": "rm-data/slides/week03.html#using-an-weighted-average-1",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Using an [Weighted] Average",
    "text": "Using an [Weighted] Average\n\nIMPORTANT: All variables should be measured in the same scale.\n\nOtherwise, the average would be meaningless.\n\nThus, need bring it to common scale.\n\nstandardization: Z-score \\[\\tilde z_i^j = \\frac{z_i^j - \\bar{z}}{s_{z}}\\]\n0-1 scale: Min-Max scaling \\[\\tilde z_i^j = = \\frac{z_i^j - \\min(z^j)}{\\max(z^j) - \\min(z^j)}\\]\n\n\n\nYou may also want to consider using same units (dollars) or use transformations (logs)"
  },
  {
    "objectID": "rm-data/slides/week03.html#let-the-decide",
    "href": "rm-data/slides/week03.html#let-the-decide",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Let the 🖥️ decide",
    "text": "Let the 🖥️ decide\n\nSome times you may need to use other methods to combine variables. Machine learning methods!\n\n\nPrincipal component analysis (PCA) is a method used for Data Reduction. Get weights to combine variables.\nThe weights are constructed based on how correlated variables are. (high correlation, high weight)\nA Bit of black box method. But commonly used in practice: Wealth index, etc.\n\npca var1 var2 var3 .. , components(#)\npredict pca\n\nCan give odd results"
  },
  {
    "objectID": "rm-data/slides/week03.html#what-to-use",
    "href": "rm-data/slides/week03.html#what-to-use",
    "title": "Exploratory Analysis and Comparisons",
    "section": "What to use?",
    "text": "What to use?\n\nZ-scores, and averages, are simple, easy to understand, - Transparent\nTypically marginally different to PCA (Try both)\nBut, Need to pay attention\n\nLook at correlation signs, you may check it first (PCA is better here) (EDA. Do signs make sense?)\nSensitive to extreme values (But can be fixed)"
  },
  {
    "objectID": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size-4",
    "href": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size-4",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Case Study - Management quality and firm size",
    "text": "Case Study - Management quality and firm size\n\nThe latent concept here is the overall quality of management, but we have 18 variables that measure different aspects of management.\nEach were measured on a scale of 1 (worst practice) to 5 (best practice).\nLets use Simple Average and PCA to create a composite index."
  },
  {
    "objectID": "rm-data/slides/week03.html#stata-corner",
    "href": "rm-data/slides/week03.html#stata-corner",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Stata Corner",
    "text": "Stata Corner\n\nuse \"data_slides\\wb-mx-management.dta\", clear\n** Simple mean\negen mng_mean = rowmean(perf* talent* lean*)\n** PCA\npca perf* talent* lean*, components(1)\npredict mng_pca\nlabel var mng_mean \"Management Score (Mean)\"\nlabel var mng_pca \"Management Score (PCA)\""
  },
  {
    "objectID": "rm-data/slides/week03.html#scatter-pca-vs-mean",
    "href": "rm-data/slides/week03.html#scatter-pca-vs-mean",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Scatter PCA vs Mean",
    "text": "Scatter PCA vs Mean\n\nA Correlation analysis could also be useful to compare the two measures."
  },
  {
    "objectID": "rm-data/slides/week03.html#comparison-and-variation-in-x",
    "href": "rm-data/slides/week03.html#comparison-and-variation-in-x",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Comparison and variation in \\(x\\)",
    "text": "Comparison and variation in \\(x\\)\n\nVariation in the conditioning variable is necessary to make comparisons.\nExample: to uncover the effect of price changes on sales you need many observations with different price values.\nGeneralization: The more variation is there in the conditioning variable the better are the chances for comparison.\n\nBecause the more likely you can capture “reality”"
  },
  {
    "objectID": "rm-data/slides/week03.html#source-of-variation",
    "href": "rm-data/slides/week03.html#source-of-variation",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Source of variation",
    "text": "Source of variation\n\nNot all variation is the same. you must ask:\n\nWhy is there variation in the conditioning variable?\n\nDepending on the source of variation, the interpretation of the comparison may be different.\n\nGood variation: You can make causal statements\nBad variation: At best you can make correlation statements"
  },
  {
    "objectID": "rm-data/slides/week03.html#the-good-experimental-data",
    "href": "rm-data/slides/week03.html#the-good-experimental-data",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The Good: Experimental data",
    "text": "The Good: Experimental data\n\nSay you have an intervention or treatment.\n\nSome people get the treatment, others do not.\n\nIn experimental data, there is controlled variation: a rule deciding treatment\nThis is the best source of variation for causal analysis.\n\nDifferences in the outcome variable will be due to the treatment variable only.\n\nExample: drug trial\n\nMedical experiment: some patients receive the drug while others receive a placebo (treatment/control)\nOutcome is recovery from the illness or not"
  },
  {
    "objectID": "rm-data/slides/week03.html#the-bad-observational-data",
    "href": "rm-data/slides/week03.html#the-bad-observational-data",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The Bad? Observational data",
    "text": "The Bad? Observational data\n\nMost data used in business, economics and policy analysis are observational.\nIn observational data, no variable is fully controlled.\nThey are the results of decisions, choices, interactions, expectations, etc. (sources of variation)\n\nSome of this could be random (good) but not all\n\nYou can still make comparisons, but you must be careful.\n\nAny difference in the outcome variable could be for other reasons.\nDoes smoking cause cancer? Or are people who smoke more likely to have other habits that cause cancer?"
  },
  {
    "objectID": "rm-data/slides/week03.html#source-of-variation-and-causal-analysis",
    "href": "rm-data/slides/week03.html#source-of-variation-and-causal-analysis",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Source of variation and causal analysis",
    "text": "Source of variation and causal analysis\n\nExperimental data: - Easy - if conditioning variable is experimentally controlled -\n\nMade sure that differences in the outcome variable are due to that variable only\n\nObservational data: - Hard - many other things may be different when the value of the conditioning variable differs\n\nYou must be careful in making causal statements\n\n\nHowever, There are -advanced- methods that can help identify causal relationships in observational data. (Advanced Econometrics)"
  },
  {
    "objectID": "rm-data/slides/week03.html#ai-and-patterns",
    "href": "rm-data/slides/week03.html#ai-and-patterns",
    "title": "Exploratory Analysis and Comparisons",
    "section": "AI and patterns",
    "text": "AI and patterns\n\nGenAI is great to give you a first review of patterns – similar to a few lines of code, or panda profiler in Python\njudgment of correlation (weak, strong) is often wrong.\nyou need to know what pattern to pursue\nCan ask to explain different metrics of dependence"
  },
  {
    "objectID": "rm-data/slides/week03.html#summary-1",
    "href": "rm-data/slides/week03.html#summary-1",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Summary",
    "text": "Summary\n\nBe explicit about what \\(y\\) and \\(x\\) are in your data and how they are related to the question of your analysis.\nFor qualitative variables, correlation can be shown by summarizing conditional probabilities (frequencies).\nFor quantitative variables, scatterplots offer a visual insight to the pattern of the relationship.\nThe correlation coefficient captures a simple measure of mean dependence."
  },
  {
    "objectID": "rm-data/slides/week03.html#functional-form-ln-transformation",
    "href": "rm-data/slides/week03.html#functional-form-ln-transformation",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Functional form: ln transformation",
    "text": "Functional form: ln transformation\n\nOften, quasi-nonlinear patterns can be approximated with \\(y\\) or \\(x\\) transformed by taking logs.\nWhen transformed by taking the natural logarithm, differences in variable values we approximate relative/percentage differences.\n\n\\[ln(x + \\Delta x) - ln(x) \\approx \\frac{\\Delta x}{x}\\]"
  },
  {
    "objectID": "rm-data/slides/week03.html#logarithmic-transformation---interpretation",
    "href": "rm-data/slides/week03.html#logarithmic-transformation---interpretation",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Logarithmic transformation - interpretation",
    "text": "Logarithmic transformation - interpretation\n\n\\(ln(x)\\): the natural logarithm of x\n\nOften refered as \\(log(x)\\) or \\(ln(x)\\) but they are often the same\n\nThis transformation “compresses” the distribution of x\nbut:\n\n\\(x\\) needs to be a positive number\n\\(ln(0)\\) or \\(ln(-|x|)\\) are not defined in \\(\\mathbb{R}\\).\n\nAdvantage: Log transformation allows for comparison in relative terms – percentages\n\n\\[\\begin{aligned}\nln(a) - ln(b) &\\approx \\frac{a-b}{0.5(a+b)} \\\\\nln(1.01)-ln(1) &= 0.0099 \\approx 0.01 \\\\\nln(1.1)-ln(1) &= 0.095 \\approx 0.1\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rm-data/slides/week03.html#logarithmic-functions-of-y-andor-x",
    "href": "rm-data/slides/week03.html#logarithmic-functions-of-y-andor-x",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Logarithmic Functions of y and/or x",
    "text": "Logarithmic Functions of y and/or x\n\nThis transformation works well if \\(\\Delta x\\) is small (\\(&lt;0.3\\))\nFor larger differences, relative differences need to be calculated by hand\nA difference of 0.1 log units corresponds to a 10% difference\nFor larger differences,\n\nif log difference is +1.0, it corresponds to a +170% difference\nif log difference is -1.0, it corresponds to a -63% difference\n\nThis transformation will be used often in economics."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression",
    "href": "rm-data/slides/week05-06.html#regression",
    "title": "Regression Analysis I",
    "section": "Regression",
    "text": "Regression\n\nRegression is the most widely used method of comparison in data analysis.\nDoing so uncovers the pattern of association between \\(y\\) and \\(x\\). Here, it is important what you use as \\(y\\) or \\(x\\).\nSimple regression uncovers mean-dependence between two variables.\n\nIt amounts to comparing average values of one variable, called the dependent variable (\\(y\\)), for observations that are different in the other variable, the explanatory variable (\\(x\\)).\n\nMultiple regression analysis involves more variables -&gt; for later."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression---uses",
    "href": "rm-data/slides/week05-06.html#regression---uses",
    "title": "Regression Analysis I",
    "section": "Regression - uses",
    "text": "Regression - uses\n\nDiscovering patterns of association between variables is often a good starting point even if our question is more ambitious.\nCausal analysis: uncovering the effect of one variable on another variable. Concerned with one parameter.\nPredictive analysis: what to expect of a \\(y\\) variable (long-run polls, hotel prices) for various values of another \\(x\\) variable (immediate polls, distance to the city center)."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression---names-and-notation",
    "href": "rm-data/slides/week05-06.html#regression---names-and-notation",
    "title": "Regression Analysis I",
    "section": "Regression - names and notation",
    "text": "Regression - names and notation\n\nRegression analysis is a method that uncovers the average value of a variable \\(y\\) for different values of another variable \\(x\\).\n\n\\[\\mathbb{E}[y|x]=y^E = f(x)\\]\n\ndependent variable or left-hand-side variable, or simply the \\(y\\) variable,\nexplanatory variable, right-hand-side variable, or simply the \\(x\\) variable\n“regress y on x,” or “run a regression of y on x” = do simple regression analysis with \\(y\\) as the dependent variable and \\(x\\) as the explanatory variable."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression---type-of-patterns",
    "href": "rm-data/slides/week05-06.html#regression---type-of-patterns",
    "title": "Regression Analysis I",
    "section": "Regression - type of patterns",
    "text": "Regression - type of patterns\nRegression may find:\n\nLinear patterns: positive (negative) association - average \\(y\\) tends to be higher (lower) at higher values of \\(x\\).\nNon-linear patterns: association may be even non-monotonic - \\(y\\) tends to be higher for higher values of \\(x\\) in a certain range of the \\(x\\) variable and lower for higher values of \\(x\\) in another range of the \\(x\\) variable\nNo association or relationship (A flat line)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#non-parametric-and-parametric-regression",
    "href": "rm-data/slides/week05-06.html#non-parametric-and-parametric-regression",
    "title": "Regression Analysis I",
    "section": "Non-parametric and parametric regression",
    "text": "Non-parametric and parametric regression\n\nNon-parametric regressions describe the \\(\\mathbb{E}[y] = f(x)\\) pattern without imposing a specific functional form on \\(f\\).\nData driven and flexible, no theory\nCan capture any pattern\nParametric regressions impose a functional form on \\(f\\). Parametric examples include:\n\nlinear functions: \\(f(x) = a + bx\\);\nexponential functions: \\(f(x) = a x^b\\);\nquadratic functions: \\(f(x) = a + bx + cx^2\\),\nor any functions which have parameters of a, b, c, etc.\n\nRestrictive, but they produce readily interpretable numbers."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#non-parametric-regression-average-by-each-value",
    "href": "rm-data/slides/week05-06.html#non-parametric-regression-average-by-each-value",
    "title": "Regression Analysis I",
    "section": "Non-parametric regression: average by each value",
    "text": "Non-parametric regression: average by each value\n\nNon-parametric regressions come (also) in various forms.\nMost intuitive non-parametric regression for \\(\\mathbb{E}[y|x] = f(x)\\) shows average \\(y\\) for each and every value of \\(x\\).\nWorks well when \\(x\\) has few values and there are many observations in the data,\nThere is no functional form imposed on \\(f\\) here."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#non-parametric-regression-categorical-variable",
    "href": "rm-data/slides/week05-06.html#non-parametric-regression-categorical-variable",
    "title": "Regression Analysis I",
    "section": "Non-parametric regression: Categorical variable",
    "text": "Non-parametric regression: Categorical variable\n\nSometimes, there are no straightforward functional form on \\(f\\) (linear not meaningful).\n\nCategorical variables\nOrdered variables.\n\nFor example, Hotels: average price of hotels with the same numbers of stars and compare these averages = non-parametric regression analysis."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#non-parametric-regression-bins",
    "href": "rm-data/slides/week05-06.html#non-parametric-regression-bins",
    "title": "Regression Analysis I",
    "section": "Non-parametric regression: bins",
    "text": "Non-parametric regression: bins\n\nWith many \\(x\\) values therea are two ways to do non-parametric regression analysis: bins and smoothing.\nBins - based on grouped values of \\(x\\) (Discretization of \\(x\\))\n\nBins are disjoint categories (no overlap) that span the entire range of \\(x\\) (no gaps).\n\nMany ways to create bins - equal size, equal number of observations per bin, or bins defined by analyst.\n\nsee binscatter or make your own"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#non-parametric-regression-lpoly",
    "href": "rm-data/slides/week05-06.html#non-parametric-regression-lpoly",
    "title": "Regression Analysis I",
    "section": "Non-parametric regression: lpoly",
    "text": "Non-parametric regression: lpoly\n\nProduce “smooth” graph - both continuous and has no kink at any point.\nalso called smoothed conditional means plots = non-parametric regression shows conditional means, smoothed to get a better image.\nLowess = most widely used non-parametric regression methods that produce a smooth graph.\nlocally weighted scatterplot smoothing (sometimes abbreviated as “loess”).\nA smooth curve fit around a bin scatter.\nwider bandwidth results in a smoother graph but may miss important details of the pattern.\nnarrower bandwidth produces a more rugged-looking graph\nIn Stata one of the commands for this its lpoly but also lowess."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#non-parametric-regression-lowess-loess",
    "href": "rm-data/slides/week05-06.html#non-parametric-regression-lowess-loess",
    "title": "Regression Analysis I",
    "section": "Non-parametric regression: lowess (loess)",
    "text": "Non-parametric regression: lowess (loess)\n\nSmooth non-parametric regression methods, including lowess, do not produce numbers that would summarize the \\(\\mathbb{E}[y]|x = f(x)\\) pattern.\nProvide a value \\(\\mathbb{E}[y]\\) for each of the particular \\(x\\) values that occur in the data, as well as for all \\(x\\) values in-between.\nGraph – we interpret these graphs in qualitative, not quantitative ways.\nThey can show interesting shapes in the pattern, such as non-monotonic parts, steeper and flatter parts, etc.\nGreat way to find relationship patterns"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels",
    "href": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels",
    "title": "Regression Analysis I",
    "section": "Case Study: Finding a good deal among hotels",
    "text": "Case Study: Finding a good deal among hotels\n\n\nCode\nset scheme white2\ncolor_style tableau\nuse data_slides/hotels-vienna.dta, clear\nqui:drop if distance&gt;6\ntwo (lpolyci price distance, bw(.3) fcolor(%20)) ///\n(lpolyci price distance, bw(.6) fcolor(%20)) ///\n(lpolyci price distance, bw(.15) fcolor(%20)), ///\nlegend(order(2 \"bw(.3)\" 4 \"bw(.6)\" 6 \"bw(.15)\")) ///\nytitle(\"Price\") xtitle(\"Distance from CityCenter\")"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#linear-regression-1",
    "href": "rm-data/slides/week05-06.html#linear-regression-1",
    "title": "Regression Analysis I",
    "section": "Linear regression",
    "text": "Linear regression\nLinear regression is the most widely used method in data analysis.\n\nImposes linearity assumption of the function \\(f\\) in \\(\\mathbb{E}[y|x] = f(x)\\). (Linearity of Coefficients)\nLinear functions have two parameters, also called coefficients: the intercept and the slope. \\(\\mathbb{E}[y|x] = \\alpha + \\beta x\\)\nCan be any function, including any nonlinear function, of the original variables themselves\nThis line is the best-fitting line one can draw through the scatterplot.\nIt is the best fit in the sense that it is the line that is closest to all points of the scatterplot."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#linear-regression---assumption-vs-approximation",
    "href": "rm-data/slides/week05-06.html#linear-regression---assumption-vs-approximation",
    "title": "Regression Analysis I",
    "section": "Linear regression - assumption vs approximation",
    "text": "Linear regression - assumption vs approximation\n\nAssumption: The regression function is linear in its coefficients.\nApproximation: Whatever the form of the \\(\\mathbb{E}[y|x] = f(x)\\) the \\(\\mathbb{E}[y|x] = \\alpha + \\beta x\\) regression fits a line through it.\n\nThis may or may not be a good approximation."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#linear-regression-coefficients",
    "href": "rm-data/slides/week05-06.html#linear-regression-coefficients",
    "title": "Regression Analysis I",
    "section": "Linear regression coefficients",
    "text": "Linear regression coefficients\n\\[\\mathbb{E}[y|x] = \\alpha + \\beta x\\]\nTwo coefficients:\n\nintercept: \\(\\alpha =\\) average value of \\(y\\) when \\(x\\) is zero:\n\n\\(\\mathbb{E}[y|x=0] = \\alpha + \\beta \\times 0 = \\alpha\\).\n\nslope: \\(\\beta =\\) expected difference in \\(y\\) corresponding to a one unit difference in x.\n\n\\(\\mathbb{E}[y|x=x_0+1] - \\mathbb{E}[y|x_0] = (\\alpha + \\beta \\times (x_0 + 1)) - (\\alpha + \\beta \\times x_0) = \\beta\\)."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression---slope-coefficient-interpretation",
    "href": "rm-data/slides/week05-06.html#regression---slope-coefficient-interpretation",
    "title": "Regression Analysis I",
    "section": "Regression - slope coefficient interpretation",
    "text": "Regression - slope coefficient interpretation\nSeveral good ways to interpret the slope coefficient\n\n\\(y\\) is \\(\\beta\\) higher, on average, for observations with a one-unit higher value of \\(x\\).\nComparing two observations that differ in \\(x\\) by one unit, we expect \\(y\\) to be \\(\\beta\\) higher for the observation with one unit higher \\(x\\).\n\nAvoid using\n\n“decrease/increase” – not right, unless time series or causal relationship only\n“effect” – not right, unless causal relationship"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression-binary-x",
    "href": "rm-data/slides/week05-06.html#regression-binary-x",
    "title": "Regression Analysis I",
    "section": "Regression: binary \\(x\\)",
    "text": "Regression: binary \\(x\\)\nSimplest case:\n\n\\(x\\) is a binary variable, zero or one.\n\\(\\alpha\\) is the average value of \\(y\\) when \\(x\\) is zero (\\(\\mathbb{E}[y|x=0] = \\alpha\\)).\n\\(\\beta\\) is the difference in average \\(y\\) between observations with \\(x=1\\) and observations with \\(x=0\\)\n\n\\(\\mathbb{E}[y|x=1] - \\mathbb{E}[y|x=0]= \\beta\\).\n\nGraphically, the regression line of linear regression goes through two points: average \\(y\\) when \\(x\\) is zero (\\(\\alpha\\)) and average \\(y\\) when \\(x\\) is one (\\(\\alpha + \\beta\\))."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression-coefficient-formula",
    "href": "rm-data/slides/week05-06.html#regression-coefficient-formula",
    "title": "Regression Analysis I",
    "section": "Regression coefficient formula",
    "text": "Regression coefficient formula\nNotation\n\nPopulation coefficients are \\(\\alpha\\) and \\(\\beta\\).\nSample estimates - \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)\nThe slope coefficient formula is \\[\\hat{\\beta} = \\frac{\\text{Cov}[x, y]}{\\text{Var}[x]} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n\\]\nSlope coefficient formula is normalized version of the covariance between \\(x\\) and \\(y\\)."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression-coefficient-formula-1",
    "href": "rm-data/slides/week05-06.html#regression-coefficient-formula-1",
    "title": "Regression Analysis I",
    "section": "Regression coefficient formula",
    "text": "Regression coefficient formula\n\nThe intercept – average \\(y\\) minus average \\(x\\) multiplied by the estimated slope \\(\\hat{\\beta}\\). \\[\\hat{\\alpha} = \\bar{y} - \\hat{\\beta} \\bar{x}\n\\]\nThe formula of the intercept reveals that the regression line always goes through the point of average \\(x\\) and average y."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#ordinary-least-squares-ols",
    "href": "rm-data/slides/week05-06.html#ordinary-least-squares-ols",
    "title": "Regression Analysis I",
    "section": "Ordinary Least Squares (OLS)",
    "text": "Ordinary Least Squares (OLS)\n\n\n\nThe formulas Provided for the slope and intercept can be found using OLS (an estimation method).\nOLS gives the best-fitting linear regression line.\nIt gives the best line by minimizing the squared of the model errors.\n\n\n\n\nCode\nclear \nqui:set obs 20\nqui:gen x = rnormal()+1\nqui:gen y = 1+x+rnormal()\nqui:gen yh=1+x\ntwo (scatter y x, msize(3) mcolor(gs3%50)) ///\n   (line yh x, color(navy)) (pcarrow yh x y x, color(gs9)), ///\nlegend(off)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression-coefficient-formula-2",
    "href": "rm-data/slides/week05-06.html#regression-coefficient-formula-2",
    "title": "Regression Analysis I",
    "section": "Regression coefficient formula",
    "text": "Regression coefficient formula\n\nOrdinary Least Squares – OLS is method used to find the best fit minimizing the Squared “residuals”.\n\\[\\min_{\\alpha,\\beta} \\sum_{i=1}^n (y_i - \\alpha - \\beta x_i)^2\\]\n\nFor this minimization problem, we can use calculus to give \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\), the values for \\(\\alpha\\) and \\(\\beta\\) that give the minimum. Please check out U7.1."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#recap",
    "href": "rm-data/slides/week05-06.html#recap",
    "title": "Regression Analysis I",
    "section": "Recap",
    "text": "Recap\n\nSimple regression analysis amounts to comparing average values of a dependent variable (\\(y\\)) for observations that are different in the explanatory variable (\\(x\\)).\nSimple regression in any way or form: comparing conditional means."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels-1",
    "href": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels-1",
    "title": "Regression Analysis I",
    "section": "Case Study: Finding a good deal among hotels",
    "text": "Case Study: Finding a good deal among hotels\n\n\nCode\nuse data_slides/hotels-vienna.dta, clear\nqui:drop if distance&gt;6\nqui: keep if inrange(stars,3,4)\nqui: drop if price&gt;300\ntwo (lpolyci price distance, bw(.3) fcolor(%20)) ///\n(lfitci price distance, fcolor(%20)) ///\n(scatter price distance, color(%20)), ///\nlegend(order(2 \"bw(.3)\" 4 \"Linear\" )) ///\nytitle(\"Price\") xtitle(\"Distance from CityCenter\") ///\nscale(1.4) note(Alpha = 131.9 Beta = -12)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#predicted-values",
    "href": "rm-data/slides/week05-06.html#predicted-values",
    "title": "Regression Analysis I",
    "section": "Predicted values",
    "text": "Predicted values\n\nThe predicted value of the dependent variable is the best guess for its average value, given \\(x\\), using our model.\nIn a linear regression they are given by: \\(\\hat{y} = \\hat{\\alpha} + \\hat{\\beta}x\\)\nWhat about non-parametric regressions\n\nIt depends on how the Model was estimated."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#residuals",
    "href": "rm-data/slides/week05-06.html#residuals",
    "title": "Regression Analysis I",
    "section": "Residuals",
    "text": "Residuals\n\nThe residual is the difference between the actual value and predicted value of an observation: \\(e_i = y_i - \\hat{y}_i\\), where \\(\\hat{y}_i = \\hat{\\alpha} + \\hat{\\beta}x_i\\)\nThe residual is meaningful only for actual observation, cannot be “predicted” out of sample.\nBut, The residual may be important on its own right.\nMay help in identifying Outliers: Cases where \\(y\\) is much higher or much lower than “it should be” (based on the regression). (Good deals or places to avoid)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels-2",
    "href": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels-2",
    "title": "Regression Analysis I",
    "section": "Case Study: Finding a good deal among hotels",
    "text": "Case Study: Finding a good deal among hotels\n\n\nCode\nqui:drop2 pr_hat res\nqui:reg price distance\nqui:predict pr_hat\nqui:predict res, res\nqui:sort res\nlist hotel_id price distance pr_hat res star in 1/5\n\n\nvariable pr_hat not found\nvariable res not found\n\n     +------------------------------------------------------------+\n     | hotel_id   price   distance     pr_hat         res   stars |\n     |------------------------------------------------------------|\n  1. |    22080      54        1.1   118.6571   -64.65714       3 |\n  2. |    22122      59         .8   122.2709    -63.2709       3 |\n  3. |    21912      60        1.1   118.6571   -58.65714       4 |\n  4. |    22073      59        1.2   117.4525   -58.45255       3 |\n  5. |    22127      58        1.4   115.0434   -57.04337     3.5 |\n     +------------------------------------------------------------+\n\n\nNot the best model (functional form , other characteristics), but a good start!"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression-modelling-r2",
    "href": "rm-data/slides/week05-06.html#regression-modelling-r2",
    "title": "Regression Analysis I",
    "section": "Regression modelling: \\(R^2\\)",
    "text": "Regression modelling: \\(R^2\\)\n\nFitness of a regression captures how predicted values compare to the actual values.\nR-squared (R^2) represents how much of the variation in \\(y\\) is captured by the regression, and how much is left for residual variation \\[R^2 = \\frac{\\text{Var}[\\hat{y}]}{\\text{Var}[y]} = 1 - \\frac{\\text{Var}[e]}{\\text{Var}[y]}\n\\]\nThis follows: \\[\\text{Var}[y] = \\text{Var}[\\hat{y}] + \\text{Var}[e]\\]"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#model-fit---r2",
    "href": "rm-data/slides/week05-06.html#model-fit---r2",
    "title": "Regression Analysis I",
    "section": "Model fit - R^2",
    "text": "Model fit - R^2\n\nR-squared (or R^2) can also be identified for non-parametric regressions. \\[R^2_1 = \\frac{\\text{Var}[\\hat{y}]}{\\text{Var}[y]} \\text{ or } R^2_2= 1 - \\frac{\\text{Var}[e]}{\\text{Var}[y]}\n\\]\n\nThey may not be the same!\n\nYou could also estimate it using the “Squared correlation” between \\(y\\) and \\(\\hat y\\).\nThe value of R-squared is always between zero and one.\nR-squared is zero, if the predicted values are just the average of the observed outcome \\(\\hat{y}_i = \\bar{y}_i\\), \\(\\forall i\\)."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#model-fit---how-to-use-r2",
    "href": "rm-data/slides/week05-06.html#model-fit---how-to-use-r2",
    "title": "Regression Analysis I",
    "section": "Model fit - how to use \\(R^2\\)",
    "text": "Model fit - how to use \\(R^2\\)\n\nR-squared may help in choosing between different versions of regression for the same data.\n\nChoose between regressions with different functional forms\nPredictions are likely to be better with high \\(R^2\\)\nMore on this later\n\nR-squared matters less when the goal is to characterize the association between \\(y\\) and \\(x\\)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression-and-causation",
    "href": "rm-data/slides/week05-06.html#regression-and-causation",
    "title": "Regression Analysis I",
    "section": "Regression and causation",
    "text": "Regression and causation\n\nUp to this point, try to be very careful to use neutral language, not talk about causation\nThink back to sources of variation in \\(x\\)\nWhen we have observational data, and we pick \\(x\\) and \\(y\\) and decide how to run the regression\nRegression is a method of comparison: it compares observations that are different in variable \\(x\\) and shows corresponding average differences in variable \\(y\\). Not necessarily Causal Relations."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression-and-causation---possible-relations",
    "href": "rm-data/slides/week05-06.html#regression-and-causation---possible-relations",
    "title": "Regression Analysis I",
    "section": "Regression and causation - possible relations",
    "text": "Regression and causation - possible relations\n\nSlope of the \\(\\mathbb{E}[y|x] = \\alpha + \\beta x\\) regression is not zero in our data\nSeveral reasons, not mutually exclusive:\n\n\\(x\\) causes \\(y\\): Yay!\n\\(y\\) causes \\(x\\). Noo!\nA third variable causes both \\(x\\) and \\(y\\) (or many such variables do) Double NoO!\n\nIn reality if we have observational data, there is a mix of these relations."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression-and-causation-1",
    "href": "rm-data/slides/week05-06.html#regression-and-causation-1",
    "title": "Regression Analysis I",
    "section": "Regression and causation",
    "text": "Regression and causation\n\nYes: “correlation (regression) does not imply causation”\n\nBetter: we should not infer cause and effect from comparisons in observational data.\n\nSuggested approach is two steps\n\nFirst interpret precisely the object (correlation of slope coefficient)\nConclude and discuss causal claims if any"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels-3",
    "href": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels-3",
    "title": "Regression Analysis I",
    "section": "Case Study: Finding a good deal among hotels",
    "text": "Case Study: Finding a good deal among hotels\n\nFit and causation\nThe R-squared of the regression is 0.10 = 10%.\nThere is a lot left unexplained.\n\nStill, good for cross-sectional regression with a single explanatory variable.\nIn any case it is the fit of the best-fitting line."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels-4",
    "href": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels-4",
    "title": "Regression Analysis I",
    "section": "Case Study: Finding a good deal among hotels",
    "text": "Case Study: Finding a good deal among hotels\n\nSlope is -12\nDoes that mean that a longer distance causes hotels to be cheaper?"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#summary-take-away",
    "href": "rm-data/slides/week05-06.html#summary-take-away",
    "title": "Regression Analysis I",
    "section": "Summary take-away",
    "text": "Summary take-away\n\nRegression – method to compare average \\(y\\) across observations with different values of \\(x\\).\nNon-parametric regressions (bin scatter, lowess, lpoly): use them to visualize complicated patterns of association between \\(y\\) and \\(x\\), No Number to interpret.\nLinear regression – linear approximation of the average pattern of association \\(y\\) and \\(x\\)\nWhen \\(\\beta\\) is not zero, one of three things (+ any combination) may be true:\n\n\\(x\\) causes y\n\\(y\\) causes x\nA third variable causes both \\(x\\) and y"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#motivation",
    "href": "rm-data/slides/week05-06.html#motivation",
    "title": "Regression Analysis I",
    "section": "Motivation",
    "text": "Motivation\n\nInterested in the pattern of association between life expectancy in a country and how rich that country is.\n\nUncovering that pattern is interesting for many reasons: discovery and learning from data.\n\nIdentify countries where people live longer than what we would expect based on their income, or countries where people live shorter lives.\nAnalyzing regression residuals.\nGetting a good approximation of the \\(y_E = f(x)\\) function is important."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#functional-form-1",
    "href": "rm-data/slides/week05-06.html#functional-form-1",
    "title": "Regression Analysis I",
    "section": "Functional form",
    "text": "Functional form\n\nSo far, we have only considered linear regression. (aside from non-parametric regressions)\nRelationships between \\(y\\) and \\(x\\) are often complicated!\nWhen and why care about the shape of a regression?\n\nWhen we need to talk about the non-average person.\n\nHow can we capture function form better?\n\nWe can transform variables in a simple linear regression."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#functional-form---linear-approximation",
    "href": "rm-data/slides/week05-06.html#functional-form---linear-approximation",
    "title": "Regression Analysis I",
    "section": "Functional form - linear approximation",
    "text": "Functional form - linear approximation\n\nLinear regression is a linear approximation to a regression of unknown shape:\nBut, we may want to modify the regression to better characterize nonlinear patterns\n\nprediction or analyze residuals - better fit\nwe want to go beyond the average pattern of association (different \\(x\\)s)\nall we care about is the average pattern of association, but the linear approximation is bad\n\nNot care\n\nif all we care about is the average pattern of association,\nif linear regression is good approximation to the average pattern"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#functional-form---types",
    "href": "rm-data/slides/week05-06.html#functional-form---types",
    "title": "Regression Analysis I",
    "section": "Functional form - types",
    "text": "Functional form - types\nNon linearities can be captured in many ways: - Natural log transformation: \\(\\ln(x)\\) when interested in relative differences - Piecewise linear splines: For flexibility in the pattern of association - Polynomials - quadratic form: Flexible yet simple"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#log-transformation",
    "href": "rm-data/slides/week05-06.html#log-transformation",
    "title": "Regression Analysis I",
    "section": "log transformation",
    "text": "log transformation\n\nSome times, some patterns are better approximated when \\(y\\) or \\(x\\) are measured as relative differences\n\nParticularly relevant if there is no natural base for comparison.\n\nTaking the natural logarithm of a variable is often a good solution in such cases, because they approximate relative differences."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#logarithmic-transformation---interpretation",
    "href": "rm-data/slides/week05-06.html#logarithmic-transformation---interpretation",
    "title": "Regression Analysis I",
    "section": "Logarithmic transformation - interpretation",
    "text": "Logarithmic transformation - interpretation\n\n\\(\\ln(x)\\) or \\(\\log(x)\\) is the natural logarithm of \\(x\\)\n\nYou can only use it if \\(x\\) is always a positive number\n\\(\\ln(0)\\) or \\(\\ln(\\text{negative number })\\) are not \\(Real\\)\n\nUsing this transformation, you can compare relative differences: \\[\\ln(x + \\Delta x) - \\ln(x) \\approx \\frac{\\Delta x}{x}\\]\nas long as \\(\\Delta x\\) is small.\n\n\\(\\ln(1.01)-\\ln(1) = 0.0099 \\approx 0.01\\)\n\\(\\ln(1.1)-\\ln(1) = 0.095 \\approx 0.1\\)\nbut…\\(\\ln(1.4)-\\ln(1) = 0.336\\) much less than 0.4"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#when-to-take-logs",
    "href": "rm-data/slides/week05-06.html#when-to-take-logs",
    "title": "Regression Analysis I",
    "section": "When to take logs?",
    "text": "When to take logs?\n\nWhen comparison makes mores sense in relative terms\n\nPercentage differences, relative differences, growth rates\n\nMost important examples\n\nPrices\nSales, turnover, GDP\nPopulation, employment\nCapital stock, inventories"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#interpreting-parameters-of-regressions-with-log-variables",
    "href": "rm-data/slides/week05-06.html#interpreting-parameters-of-regressions-with-log-variables",
    "title": "Regression Analysis I",
    "section": "Interpreting parameters of regressions with log variables",
    "text": "Interpreting parameters of regressions with log variables\n\nlog-lin modellin-log modellog-log modelKeep in mind\n\n\n\\(\\ln(y^E) = \\alpha + \\beta x_i\\)\n\nlog \\(y\\), level \\(x\\)\n\\(\\alpha\\) is average \\(\\ln(y)\\) when \\(x\\) is zero. (Often meaningless.)\n\\(\\beta\\): \\(y\\) is \\(\\beta * 100\\) percent higher, on average for observations with one unit higher \\(x\\).\n\n\n\n\\(y^E = \\alpha + \\beta\\ln(x_i)\\)\n\nlevel \\(y\\), log \\(x\\)\n\\(\\alpha\\) is : average \\(y\\) when \\(\\ln(x)\\) is zero (and thus \\(x\\) is one), not very meaningful.\n\\(\\beta\\): \\(y\\) is \\(\\beta/100\\) units higher, on average, for observations with one percent higher \\(x\\).\n\n\n\n\\(\\ln(y^E) = \\alpha + \\beta\\ln(x_i)\\)\n\nlog \\(y\\), log \\(x\\)\n\\(\\alpha\\): is average \\(\\ln(y)\\) when \\(\\ln(x)\\) is zero. (Often meaningless.)\n\\(\\beta\\): \\(y\\) is \\(\\beta\\) percent higher on average for observations with one percent higher \\(x\\).\n\nElasticity!\n\n\n\n\n\nPrecise interpretation is key\nThe interpretation of the slope (and the intercept) coefficient(s) differs in each case!\nOften verbal comparison is made about a 10% difference in \\(x\\) if using level-log or log-log regression."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#to-take-log-or-not-to-take-log",
    "href": "rm-data/slides/week05-06.html#to-take-log-or-not-to-take-log",
    "title": "Regression Analysis I",
    "section": "To Take log or Not to Take log",
    "text": "To Take log or Not to Take log\nDecide for substantive reason:\n\nTake logs if variable is likely affected in multiplicative ways\nDon’t take logs if variable is likely affected in additive ways\n\nDecide for statistical reason:\n\nLinear regression is better at approximating average differences if distribution of dependent variable is closer to normal.\nTake logs if skewed distribution with long right tail\nMost often the substantive and statistical arguments are aligned"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#to-take-log-or-not-to-take-log-1",
    "href": "rm-data/slides/week05-06.html#to-take-log-or-not-to-take-log-1",
    "title": "Regression Analysis I",
    "section": "To Take log or Not to Take log",
    "text": "To Take log or Not to Take log\n\nLog needs variable to be positive: Never negative, never zero\nSometimes you may be able to combine Logs with Dummies (if zero or negative values are present)\nSometimes adding a constant seems to do the trick\n\n\\(\\ln(x+1)\\) if \\(x\\) is positive or zero\nBut Not a good solution. May need to consider other transformations"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#hotel-price-distance-regression-and-functional-form",
    "href": "rm-data/slides/week05-06.html#hotel-price-distance-regression-and-functional-form",
    "title": "Regression Analysis I",
    "section": "Hotel price-distance regression and functional form",
    "text": "Hotel price-distance regression and functional form\nComparing different models\n\nCode\nqui {\n  set linesize 255\n  capture gen log_price = log(price)\n  capture gen log_distance = log(distance)\n  regress price distance\n  est sto m1\n  regress log_price distance\n  est sto m2\n  regress price log_distance\n  est sto m3\n  regress log_price log_distance\n  est sto m4\n}\nesttab m1 m2 m3 m4, se md nostar nonumber note(\"\")\n\n\n\n\n\n\n\n\n\n\n\n\nprice\nlog_price\nprice\nlog_price\n\n\n\n\ndistance\n-12.05\n-0.104\n\n\n\n\n\n(2.001)\n(0.0161)\n\n\n\n\nlog_distance\n\n\n-21.28\n-0.176\n\n\n\n\n\n(2.251)\n(0.0183)\n\n\n_cons\n131.9\n4.829\n114.8\n4.682\n\n\n\n(3.740)\n(0.0301)\n(2.087)\n(0.0169)\n\n\nN\n321\n321\n320\n320\n\n\n\nAs Excercise, plot the different models."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#which-model-shall-we-choose---substantive-reasons",
    "href": "rm-data/slides/week05-06.html#which-model-shall-we-choose---substantive-reasons",
    "title": "Regression Analysis I",
    "section": "Which model shall we choose? - Substantive reasons",
    "text": "Which model shall we choose? - Substantive reasons\n\nIt depends on the goal of the analysis!\nPrices\n\nWe are after a good deal on a single night – absolute price differences are meaningful.\nPercentage differences in price may remain valid if inflation and seasonal fluctuations affect prices proportionately.\nOr we are after relative differences - we do not mind about the magnitude that we are paying, we only need the best deal.\n\nDistance\n\nDistance could make more sense in miles than in relative terms – given our purpose is to find a relatively cheap hotel."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#which-model-shall-we-choose---statistical-reasoning",
    "href": "rm-data/slides/week05-06.html#which-model-shall-we-choose---statistical-reasoning",
    "title": "Regression Analysis I",
    "section": "Which model shall we choose? - Statistical reasoning",
    "text": "Which model shall we choose? - Statistical reasoning\n\nVisual inspection\n\nWhich model captures patterns better?\n\nCompare fit measure (\\(R^2\\))\n\nBut be careful. If \\(y\\) is in logs, \\(R^2\\) is not directly comparable to \\(R^2\\) when \\(y\\) is in levels.\nits like comparing apples and oranges\n\nFinal verdict:\n\nYour call…."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#other-transformations-splines",
    "href": "rm-data/slides/week05-06.html#other-transformations-splines",
    "title": "Regression Analysis I",
    "section": "Other transformations: splines",
    "text": "Other transformations: splines\n\nWarning Splines are another way to estimate non-parametric models. Just a bit more parametric.\nA regression with a piecewise linear spline (of \\(x\\)) results in connected line segments for the mean dependent variable,\n\neach line segment corresponding to a specific interval of the explanatory variable.\n\nThe points of connection are called knots,\nThe places of the knots (the boundaries of the intervals of the explanatory variable) need to be specified by the analyst.\nPlot-twist: The segments need not to be linear!"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#other-transformations-splines-1",
    "href": "rm-data/slides/week05-06.html#other-transformations-splines-1",
    "title": "Regression Analysis I",
    "section": "Other transformations: splines",
    "text": "Other transformations: splines\n\nAdvantage: We can interpret parameters!\nThe formula:\n\n\\[y^E = \\alpha_1 + \\beta_1 x[\\text{if } $x$ &lt; k_1] + (\\alpha_2 + \\beta_2 x)[\\text{if } k_1 \\leq $x$ \\leq k_2] + \\dots + (\\alpha_m + \\beta_m x)[\\text{if } $x$ \\geq k_{m-1}]\\]\nBut we we usually assume that \\(\\alpha_2, \\alpha_3, \\dots, \\alpha_m = 0\\) and only allow \\(\\beta's\\) to change"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#other-transformations-splines-2",
    "href": "rm-data/slides/week05-06.html#other-transformations-splines-2",
    "title": "Regression Analysis I",
    "section": "Other transformations: splines",
    "text": "Other transformations: splines\nInterpretation of the most important parameters\n\n\\(\\alpha\\): average \\(y\\) when \\(x\\) is zero.\n\\(\\beta_1\\): How much higher \\(y\\) is, on average, for observations with one unit higher \\(x\\) value, if \\(x \\in (-\\infty , k_1)\\).\n\\(\\beta_2\\): How much higher \\(y\\) is, on average, for observations with one unit higher \\(x\\) value, if \\(x \\in (k_2, k_2)\\).\nEtc. This is the “slope” of the line segment.\nYou can also use marginal slopes."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#section",
    "href": "rm-data/slides/week05-06.html#section",
    "title": "Regression Analysis I",
    "section": "",
    "text": "Splines - Example\n\nHow to use itResults\n\n\n\nYou need to create all necessary variables, given your knots.\nSay we choose knots 1 and 2 for the price analysis\n\n\n*              v Knot  v knot2\nmkspline dist1 1 dist2 2 dist3= distance  \n\n\n\n\n\nCode\nregress price dist1 dist2 dist3\n\n\n\n      Source |       SS           df       MS      Number of obs   =       321\n-------------+----------------------------------   F(3, 317)       =     41.08\n       Model |  162850.558         3  54283.5192   Prob &gt; F        =    0.0000\n    Residual |  418846.994       317  1321.28389   R-squared       =    0.2800\n-------------+----------------------------------   Adj R-squared   =    0.2731\n       Total |  581697.551       320  1817.80485   Root MSE        =    36.349\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       dist1 |  -75.91485   8.747817    -8.68   0.000    -93.12596   -58.70373\n       dist2 |  -.1596926   7.344118    -0.02   0.983    -14.60907    14.28968\n       dist3 |    2.54916   3.843377     0.66   0.508     -5.01259    10.11091\n       _cons |   174.5939   6.098417    28.63   0.000     162.5954    186.5924\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#other-transformations-splines-3",
    "href": "rm-data/slides/week05-06.html#other-transformations-splines-3",
    "title": "Regression Analysis I",
    "section": "Other transformations: splines",
    "text": "Other transformations: splines\n\nSplines can handles any kind of nonlinearity\nOffers a lot of flexibility,\nBut requires decisions from the analyst\n\nHow many knots?\nWhere to locate them\nDecision based on scatterplot, theory / business knowledge\nMachine learning\n\nThey can also be more complicated: quadratic, cubic or B-splines. Smooth and flexible."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#polynomials",
    "href": "rm-data/slides/week05-06.html#polynomials",
    "title": "Regression Analysis I",
    "section": "Polynomials",
    "text": "Polynomials\nThis is a simpler way to capture non-linearities\n\nQuadratic function of the explanatory variable, allowing for a smooth change in the slope\n\nTechnically: quadratic function is not a linear function (a parabola, not a line), but the model is still linear in its coefficients.\n\nHandles nonlinearities similar to a parabola.\nLess flexible, but easier interpretation!\n\nJust need basic calculus, or “logic”"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#the-quadratic-form",
    "href": "rm-data/slides/week05-06.html#the-quadratic-form",
    "title": "Regression Analysis I",
    "section": "The quadratic form",
    "text": "The quadratic form\n\\[y^E = \\alpha + \\beta_1 x + \\beta_2 x^2\\]\n\n\\(\\beta_1\\) has no interpretation (unless x=0),\n\\(\\beta_2\\neq 0\\) if the functional form is U-shaped (\\(\\beta_2 &gt; 0\\)) or inverted U-shaped (\\(\\beta_2 &lt; 0\\)).\n\nBut you may not see it in the data\n\nThe slope: \\(\\beta_1 + 2\\beta_2 x\\) is different at different values of \\(x\\).\nYou can use slope for comparing the effect of \\(x\\) on \\(y\\) for small changes in \\(x\\).\nFor large changes, need to calculate manually."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#life-expectancy-and-income",
    "href": "rm-data/slides/week05-06.html#life-expectancy-and-income",
    "title": "Regression Analysis I",
    "section": "Life expectancy and income",
    "text": "Life expectancy and income\n\nIs there a relationship between How long people live in a country and how rich that country is?\nTo identify countries where people live longer than what we would expect based on their income, or countries where people live shorter lives.\nAnalyzing regression residuals – getting a good approximation of the \\(y_E = f(x)\\) function is important."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#life-expectancy-and-income-1",
    "href": "rm-data/slides/week05-06.html#life-expectancy-and-income-1",
    "title": "Regression Analysis I",
    "section": "Life expectancy and income",
    "text": "Life expectancy and income\n\nuse data_slides/wb-lifeexpectancy.dta, clear\nkeep if year == 2017\nsum gdppc lifeexp\n\n(4,847 observations deleted)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       gdppc |        182    19.22786    20.38674   .6707771   113.2622\n     lifeexp |        182    72.30765    7.648017     52.214   84.68049"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#life-expectancy-and-gdp",
    "href": "rm-data/slides/week05-06.html#life-expectancy-and-gdp",
    "title": "Regression Analysis I",
    "section": "Life expectancy and GDP",
    "text": "Life expectancy and GDP\n\nLife Exp vs GDPpcLife Exp vs Log GDPpcRegressionInteresting Findings\n\n\n\n\nCode\nscatter lifeexp gdppc, scale(1.4) ///\nytitle(Life Expectancy) xtitle(GDP per capita) ///\nxlabel(0(25)100)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nscatter lifeexp gdppc, scale(1.4) ///\nytitle(Life Expectancy) xtitle(GDP per capita) ///\nxscale(log) xlabel(1 2 5 10 25 50 100)\n\n\n\n\n\n\n\n\n\n\n\n\ngen log_gdp = log(gdppc)\nregress lifeexp log_gdp\npredict resid, res\npredict life_hat\nsort resid\n\n\n      Source |       SS           df       MS      Number of obs   =       182\n-------------+----------------------------------   F(1, 180)       =    382.77\n       Model |  7200.86382         1  7200.86382   Prob &gt; F        =    0.0000\n    Residual |  3386.21735       180  18.8123186   R-squared       =    0.6802\n-------------+----------------------------------   Adj R-squared   =    0.6784\n       Total |  10587.0812       181  58.4921612   Root MSE        =    4.3373\n\n------------------------------------------------------------------------------\n     lifeexp | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     log_gdp |   5.333648   .2726172    19.56   0.000     4.795712    5.871585\n       _cons |   59.65933   .7220205    82.63   0.000     58.23462    61.08404\n------------------------------------------------------------------------------\n(option xb assumed; fitted values)\n\n\n\n\n\nlist countryname lifeexp gdppc life_hat if inrange(_n,1,5) | inrange(_n,178,182) \n\n\n     +---------------------------------------------------+\n     |       countryname   lifeexp      gdppc   life_hat |\n     |---------------------------------------------------|\n  1. | Equatorial Guinea    57.939   22.29894   76.21785 |\n  2. |           Nigeria    53.875   5.351441   68.60581 |\n  3. |          Eswatini    58.268   9.567586   71.70473 |\n  4. |     Cote d'Ivoire    54.102   3.564596   66.43867 |\n  5. |           Lesotho    54.568   2.845889   65.23766 |\n     |---------------------------------------------------|\n178. |           Lebanon    79.758   11.64702    72.7537 |\n179. |           Vietnam    76.454   6.233485   69.41956 |\n180. |           Vanuatu    72.334    2.82708   65.20229 |\n181. |         Nicaragua    75.653   5.169298   68.42111 |\n182. |   Solomon Islands    71.006   2.126353   63.68308 |\n     +---------------------------------------------------+"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#life-expectancy-and-income-can-we-do-better",
    "href": "rm-data/slides/week05-06.html#life-expectancy-and-income-can-we-do-better",
    "title": "Regression Analysis I",
    "section": "Life expectancy and income: Can we do better?",
    "text": "Life expectancy and income: Can we do better?\n\nProbably…Linear regression seems a good fit, but you can try other functional forms.\n\nQuadradic, Splines, etc\n\nKeep \\(y\\) is the same for easy comparison\n\nExplore the residuals, see if you can do better."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#which-functional-form-to-choose---guidelines",
    "href": "rm-data/slides/week05-06.html#which-functional-form-to-choose---guidelines",
    "title": "Regression Analysis I",
    "section": "Which functional form to choose? - guidelines",
    "text": "Which functional form to choose? - guidelines\nStart with deciding whether you care about nonlinear patterns or not.\n\nLinear approximation OK if focus is on an average association.\nTransform variables for a better interpretation of the results (e.g. log), and it often makes linear regression better approximate the average association.\nAccommodate a nonlinear pattern if our focus is\n\non prediction,\nanalysis of residuals,\nabout how an association varies beyond its average.\n\nKeep in mind - simpler the better!"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#data-is-messy",
    "href": "rm-data/slides/week05-06.html#data-is-messy",
    "title": "Regression Analysis I",
    "section": "Data Is Messy",
    "text": "Data Is Messy\n\nClean and neat data exist only in dreams, and textbooks\nData may be messy in many ways\n\nStructure, storage type differs from what we want\nNeeds cleaning (see Chapters 1,2)\nSome observations are influential\nVariables measured with error\nSome observations may represent more individuals (Weights?)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#extreme-values",
    "href": "rm-data/slides/week05-06.html#extreme-values",
    "title": "Regression Analysis I",
    "section": "Extreme values",
    "text": "Extreme values\n\nSome observations may contain Extreme values, compared to the rest of the data\nExtreme values examples\n\nBanking sector employment share in countries. Luxembourg: 10%\nHotel price of 1 US dollars, 10,000 US dollars\nProduction of a small firm: 1,000,000,000 units"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#influential-observations",
    "href": "rm-data/slides/week05-06.html#influential-observations",
    "title": "Regression Analysis I",
    "section": "Influential observations",
    "text": "Influential observations\n\nInfluential observations\n\nTheir inclusion or exclusion influences the regression line (sensitivity)\nInfluential observations are often extreme values (on \\(x\\) or \\(y\\))\nBut not all extreme values are influential observations\n\nInfluential observations example\n\nVery large tech companies in a regression of size and average wage\nA single mixed-race worker in a regression"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#what-to-do-with-them",
    "href": "rm-data/slides/week05-06.html#what-to-do-with-them",
    "title": "Regression Analysis I",
    "section": "What to do with them?",
    "text": "What to do with them?\n\nDepends on why they are extreme\nIf by mistake: may want to drop them (EUR1000+) (or Impute them)\nIf by nature: don’t want to drop them (Part of the distribution)\nGrey zone: patterns work differently for them for substantive reasons\nGeneral rule: avoid dropping observations based on value of \\(y\\) variable\n\nDropping extreme observations by \\(x\\) variable may be OK\nBut those are valuable as they represent informative and large variation"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#measurement-errors-in-variables",
    "href": "rm-data/slides/week05-06.html#measurement-errors-in-variables",
    "title": "Regression Analysis I",
    "section": "Measurement Errors In Variables",
    "text": "Measurement Errors In Variables\n\nGoal: measuring the association between variables\n\nFurthermore, we are interested in the estimated value (not just the sign)\n\nBut, observed variables have measurement error\n\nMistake, hard-to-measure data, created variables\n\nOften cannot do anything about it!\nSo, what the consequence is of such errors??\nDoes the answer depend on the type of measurement error?"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#classical-measurement-error",
    "href": "rm-data/slides/week05-06.html#classical-measurement-error",
    "title": "Regression Analysis I",
    "section": "Classical Measurement Error",
    "text": "Classical Measurement Error\n\\[w = w^* + \\varepsilon\\]\n\nis zero on average (so it does not affect the average of the measured variable) and\nis independent of all other relevant variables, including the error-free variable.\n\n\nRecording errors: E.g., due to mistakes in entering data\nReporting errors: in surveys or administrative data If they are random around the true quantities"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#cme-in-the-dependent-variable-y",
    "href": "rm-data/slides/week05-06.html#cme-in-the-dependent-variable-y",
    "title": "Regression Analysis I",
    "section": "CME in the dependent variable (\\(y\\))",
    "text": "CME in the dependent variable (\\(y\\))\nConsider \\(y = y^* + e\\), where \\(y\\) is the measured variable, \\(y^*\\) is the error-free variable, and \\(e\\) is the measurement error (noise).\nThe slope coefficient of \\(y\\) and \\(y^*\\) on \\(x\\) are:\n\\[\\beta^* = \\frac{\\text{Cov}[y^*, x]}{\\text{Var}[x]} \\text{ and } \\beta = \\frac{\\text{Cov}[y^* + e, x]}{\\text{Var}[x]}\n\\]\n\\[\\beta = \\frac{\\text{Cov}[y^*, x]}{\\text{Var}[x]} + \\left(\\frac{\\text{Cov}[e, x]}{\\text{Var}[x]}\\approx  0\\right) \\approx \\beta^*  \n\\]\n\nConsequence: classical measurement error in \\(y\\) is not expected to affect the regression coefficients."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#cme-in-the-explanatory-variable-x",
    "href": "rm-data/slides/week05-06.html#cme-in-the-explanatory-variable-x",
    "title": "Regression Analysis I",
    "section": "CME in the explanatory variable (\\(x\\))",
    "text": "CME in the explanatory variable (\\(x\\))\nConsider \\(x = x^* + e\\), where \\(x\\) is the measured variable, \\(x^*\\) is the error-free variable, and \\(e\\) is the measurement error (noise).\nThe slope coefficient are: \\[\\beta^* = \\frac{\\text{Cov}[y, x^*]}{\\text{Var}[x^*]} \\text{ and } \\beta = \\frac{\\text{Cov}[y, x^*+e]}{\\text{Var}[x^*+e]}\n\\]\n\\[\\beta = \\frac{\\text{Cov}[y, x^*]+(\\text{Cov}[y, e]\\approx 0)}{\\text{Var}[x^*]+\\text{Var}[e]} = \\beta^* \\frac{\\text{Var}[x^*]}{\\text{Var}[x^*]+\\text{Var}[e]}\n\\]\n\nConsequence: CME in \\(x\\) will affect the regression coefficients. (Towards zero) Attenuation Bias\n\\(\\alpha\\) is also affected"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#classical-measurement-error-in-the-explanatory-variable-x",
    "href": "rm-data/slides/week05-06.html#classical-measurement-error-in-the-explanatory-variable-x",
    "title": "Regression Analysis I",
    "section": "Classical measurement error in the explanatory variable (\\(x\\))",
    "text": "Classical measurement error in the explanatory variable (\\(x\\))\n\nNoise to signal ratio is: \\(\\frac{\\text{Var}[e]}{\\text{Var}[x^*]}\\)\n\nHow much noise is there compared to the true variation \\(x*\\)\n\nWhen the noise-to-signal ratio is low, we may safely ignore the problem.\n\nthis happens often when\n\nwhen we are confident that recording errors are at not important\nwhen our data has an aggregate variable estimated from very large samples.\n\n\nWhen the noise-to-signal ratio is substantial\n\nwe may be better of assessing its consequences."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#extra-non-classical-measurement-error",
    "href": "rm-data/slides/week05-06.html#extra-non-classical-measurement-error",
    "title": "Regression Analysis I",
    "section": "Extra: non-classical measurement error",
    "text": "Extra: non-classical measurement error\n\nIn real-life data measurement error in variables may or may not be classical\n\nVery often, it isn’t!\n\nVariables measured with error may be less dispersed (non-zero mean)\n\nExample: Self reported Income\n\nMeasurement error may be related to variables of interest\n\nExample: Self-reported weight and height\n\nThis often means that modelling needs to be redesigned"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#classical-measurement-error-summary",
    "href": "rm-data/slides/week05-06.html#classical-measurement-error-summary",
    "title": "Regression Analysis I",
    "section": "Classical measurement error summary",
    "text": "Classical measurement error summary\n\nCME in the dependent (\\(y\\)) variable is not expected to affect the regression coefficients.\nCME in the explanatory (\\(x\\)) variable will affect the regression coefficients.\n\nThe estimated beta will be closer to zero than it would be without measurement error.\n\nAlmost all variables are measured with error. Need to think about consequences."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#hotel-ratings-and-measurement-error",
    "href": "rm-data/slides/week05-06.html#hotel-ratings-and-measurement-error",
    "title": "Regression Analysis I",
    "section": "Hotel ratings and measurement error",
    "text": "Hotel ratings and measurement error\n\nReview the case of Ratings and Prices of hotels\n\nAverage customer ratings are noisy and bad proxies for the true quality of a hotel.\nThe true quality of a hotel is unobserved.\n\nRegressions for hotels with few ratings are likely to produce attenuated slope coefficients.\nAnd that is what you can find!\n\nI would argue the same with Amazon reviews\n\nBut what to do?\n\nperhaps Robustness checks. Restrict regressions to cases with different levels of Measurement error."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#using-weights-in-regressions",
    "href": "rm-data/slides/week05-06.html#using-weights-in-regressions",
    "title": "Regression Analysis I",
    "section": "Using weights in regressions",
    "text": "Using weights in regressions\n\nDifferent observations may have different weights (importance or size)\n\nto denote different size of larger units in the data\npopulation of countries\n\nUse weights of size IF want to uncover the patterns of association for the individuals\n\nwho make up the larger units (e.g., people in countries),\n\nAlso, use weights when you want your data to be representative of the population\n\nwhen you want to generalize the results to the population"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#life-expectancy-and-gdp-per-capita---weights",
    "href": "rm-data/slides/week05-06.html#life-expectancy-and-gdp-per-capita---weights",
    "title": "Regression Analysis I",
    "section": "Life expectancy and GDP per capita - weights",
    "text": "Life expectancy and GDP per capita - weights\n\n\nCode\nqui: reg lifeexp log_gdp [w=population ]\npredict life_hatw\nqui: reg lifeexp log_gdp \npredict life_hatnw\n*p*redict life_hat\ntwo (scatter lifeexp gdppc [w=population ],  color(%50)) ///\n(scatter lifeexp gdppc [w=population ] if inlist(countryname,\"China\",\"India\",\"United States\"),  color(%50) mlabel(countryname)) ///\n(line life_hatw life_hatnw gdppc,sort  ), scale(1.4) legend(off) ///\nytitle(Life Expectancy) xtitle(GDP per capita) ///\nxscale(log) xlabel(1 2 5 10 25 50 100)\n\n\n(option xb assumed; fitted values)\n(option xb assumed; fitted values)\n(analytic weights assumed)\n(analytic weights assumed)\n(analytic weights assumed)\n(analytic weights assumed)\n(analytic weights assumed)\n(analytic weights assumed)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#summary-take-away-1",
    "href": "rm-data/slides/week05-06.html#summary-take-away-1",
    "title": "Regression Analysis I",
    "section": "Summary take-away",
    "text": "Summary take-away\n\nNonlinear functional forms may or may not be important for regression analysis.\nThey are usually important for prediction.\nless important for causal analysis.\nWhen important, we have multiple options: Logs, splines, polynomials\nInfluential observations and other extreme values are usually best analyzed with the rest of the data\nDiscard them only if you have a good reason."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#generalizing-reminder",
    "href": "rm-data/slides/week05-06.html#generalizing-reminder",
    "title": "Regression Analysis I",
    "section": "Generalizing: reminder",
    "text": "Generalizing: reminder\n\nWe have uncovered some pattern in our data. We are interested in generalize the results.\nQuestion: Is the pattern we see in our data\n\nTrue in general?\nor is it just a special case (unique to the sample)?\n\nInference - the act of generalizing results\n\nFrom a particular dataset to other situations.\n\nFrom a sample to population = statistical inference\nBeyond (other dates, countries, people, firms) = external validity"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#generalizing-linear-regression",
    "href": "rm-data/slides/week05-06.html#generalizing-linear-regression",
    "title": "Regression Analysis I",
    "section": "Generalizing Linear Regression",
    "text": "Generalizing Linear Regression\n\nWe estimated the linear model\n\n\\(\\hat{\\beta}\\) is the average difference in \\(y\\) in the dataset between observations that are different in terms of \\(x\\) by one unit.\n\\(\\hat{y}_i\\) best guess for the expected value (average) of the dependent variable for observation \\(i\\) with value \\(x_i\\)\n\nSometimes all we care about patterns in the data we have.\nBut often we are interested in patterns beyond the Dataset.\nTo what extent they can be generalized"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#statistical-inference-confidence-interval",
    "href": "rm-data/slides/week05-06.html#statistical-inference-confidence-interval",
    "title": "Regression Analysis I",
    "section": "Statistical Inference: Confidence Interval",
    "text": "Statistical Inference: Confidence Interval\n\nThe 95% CI of the slope coefficient is similar to estimating a 95% CI of any other statistic. \\[CI(\\hat{\\beta})_{95\\%} = [\\hat{\\beta} - 1.96SE(\\hat{\\beta}), \\hat{\\beta} + 1.96SE(\\hat{\\beta})]\n\\]\nThe standard error (SE) of the slope coefficient\n\nis conceptually the same as the SE of any statistic.\nmeasures the spread of the values of the statistic across hypothetical repeated samples drawn from the same population our data represents"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#standard-error-of-the-slope",
    "href": "rm-data/slides/week05-06.html#standard-error-of-the-slope",
    "title": "Regression Analysis I",
    "section": "Standard Error of the Slope",
    "text": "Standard Error of the Slope\n\n\n\\[SE(\\hat{\\beta}) = \\frac{Std[e]}{\\sqrt n Std[x]}\\]\n\nWhere:\n\nResidual: \\(e = y - \\hat{\\alpha} - \\hat{\\beta}x\\)\n\\(Std[e]\\), the standard deviation (SD) of the regression residual,\n\\(Std[x]\\), the SD of the explanatory variable,\n\\(\\sqrt{n}\\) , Often we use \\(\\sqrt{n - 2}\\).\n\n\n\n\nA smaller standard error translates into narrower CI and more precise estimates.\nWe get more precision if\n\nsmaller the standard deviation of the residual (better fit)\nlarger the standard deviation of the explanatory variable – more variation in \\(x\\) is good.\nmore data.\n\nThis formula is correct assuming homoskedasticity"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#heteroskedasticity-robust-se",
    "href": "rm-data/slides/week05-06.html#heteroskedasticity-robust-se",
    "title": "Regression Analysis I",
    "section": "Heteroskedasticity Robust SE",
    "text": "Heteroskedasticity Robust SE\n\nSimple SE formula is not correct in general.\nHomoskedasticity assumption, the goodness of fit of the regression line is the same across the entire range of the \\(x\\) variable.\n\nResiduals are spread evenly around the regression line.\n\nIn general this is not true\nHeteroskedasticity: the fit may differ at different values of \\(x\\) so that the spread of actual \\(y\\) around the regression is different for different values of \\(x\\)\nSo…what to do?\n\nNeed to adjust the SE formula!"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#heteroskedasticity-you-have-options",
    "href": "rm-data/slides/week05-06.html#heteroskedasticity-you-have-options",
    "title": "Regression Analysis I",
    "section": "Heteroskedasticity: You have options",
    "text": "Heteroskedasticity: You have options\n\nThere are many ways to correct for heteroskedasticity\n\nGeneralized least squares (GLS)\nWeighted least squares (WLS)\nFeasible generalized least squares (FGLS)\nHuber-White robust standard errors\n\nTraditionally, you also want to test if you have a Heteroskedasticity problem\n\nWhite test, Breusch-Pagan test\n\n\nBut for now lets assume you have a heteroskedasticity problem"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#heteroskedasticity-robust-se-1",
    "href": "rm-data/slides/week05-06.html#heteroskedasticity-robust-se-1",
    "title": "Regression Analysis I",
    "section": "Heteroskedasticity Robust SE",
    "text": "Heteroskedasticity Robust SE\n\nWhite-Huber Robust SE is correct with and without heteroskedasticity.\nSame properties as before: smaller when \\(Std[e]\\) is small, \\(Std[x]\\) is large and \\(n\\) is large\nMathematically, Huber-White SE “corrects” the simple SE using the residuals from the regression. \\[Var_r(\\hat{\\beta}) = \\frac{\\sum (x_i-\\bar x)^2 \\hat e_i^2}{\\left(\\sum (x_i-\\bar x)^2\\right)^2}\\]\n\nNote: there are many heteroskedastic-robust formula: ‘HC0’, ‘HC1’, ‘HC2’, ‘HC3’. Stata uses HC1 when you ask for robust SE: regress y x, robust"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#anythings-else",
    "href": "rm-data/slides/week05-06.html#anythings-else",
    "title": "Regression Analysis I",
    "section": "Anythings else?",
    "text": "Anythings else?\n\nNope\n\nCoefficient and \\(R^2\\) remain the same\n\nJust make sure you are using robust SE.\nSE may be similar, but most likely larger than the simple SE"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#testing-if-true-beta-is-zero",
    "href": "rm-data/slides/week05-06.html#testing-if-true-beta-is-zero",
    "title": "Regression Analysis I",
    "section": "Testing if (true) beta is zero",
    "text": "Testing if (true) beta is zero\n\nTesting hypotheses: decide if a statement about a general pattern is true.\nThe question: are the Dependent variable and the explanatory variable related at all?\nThe null and the alternative: \\[H_0: \\beta_{true} = 0, H_A: \\beta_{true} \\neq 0\\]\nThe t-statistic is: \\[t = \\frac{\\hat{\\beta} - 0}{SE(\\hat{\\beta})}\\]\nOften \\(t = 2\\) (1.96) is the critical value, which corresponds to 95% CI. or 5% significance level (\\(\\alpha\\)) \\((t = 2.6 \\rightarrow 99\\%)\\)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#testing-if-true-beta-is-zero-1",
    "href": "rm-data/slides/week05-06.html#testing-if-true-beta-is-zero-1",
    "title": "Regression Analysis I",
    "section": "Testing if (true) beta is zero",
    "text": "Testing if (true) beta is zero\nPractical guidance, Same as before!:\n\nChoose a critical value.\n\np-value, the probability of a false positive in our dataset\nBalancing act: false positive (FP) and negative (FN)\n\nHigher critical value\n\nFP: less likely (less likely rejection of the null).\nFN: more likely (high risk of not rejecting a null even though it’s false)\n\nTypical critical values: 5% (1.96)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#ohh-that-p5-cutoff",
    "href": "rm-data/slides/week05-06.html#ohh-that-p5-cutoff",
    "title": "Regression Analysis I",
    "section": "Ohh, that \\(p=5\\%\\) cutoff",
    "text": "Ohh, that \\(p=5\\%\\) cutoff\n\nWhen testing, you start with a critical value first\nOften the standard to publish a result is to have a p value below 5%.\n\nArbitrary, but…there is lots of discussion about it.\n\nIf you find a result that cannot be told apart from 0 at 1% (max 5%), you should say that explicitly.\n\nSometimes that is what you want to say. A non-significant result is also a result."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#dealing-with-5-10",
    "href": "rm-data/slides/week05-06.html#dealing-with-5-10",
    "title": "Regression Analysis I",
    "section": "Dealing with 5-10%",
    "text": "Dealing with 5-10%\n\nSometimes regression result may be significant at 10%.\nWhat not to do? Avoid language like…\n\n“a barely detectable statistically significant difference” (\\(p=0.073\\))\n“a margin at the edge of significance” (\\(p=0.0608\\))\n\nSometimes you work on a proposal: Proof of concept.\n\nTo be lenient is okay. You may need more power!\nSay the point estimate and note the 95% confidence interval.\n\nSometimes you are looking for a proof. Beyond reasonable doubt.\n\nHere you wanna be below 1% (or less)\nBe honest…present the p-value, and the CI."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#p-hacking",
    "href": "rm-data/slides/week05-06.html#p-hacking",
    "title": "Regression Analysis I",
    "section": "p-Hacking",
    "text": "p-Hacking\n\nJust as before. Be honest. Do not fixate on the 5% level.\n\nSuggestion:\n\nPresent your most conservative result first\n\nExample: if uncertain, keep extreme values in.\n\nShow robustness checks: many additional regressions with different decisions"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#chance-events-and-size-of-data",
    "href": "rm-data/slides/week05-06.html#chance-events-and-size-of-data",
    "title": "Regression Analysis I",
    "section": "Chance Events And Size of Data",
    "text": "Chance Events And Size of Data\n\nSome times you just need more power.\nFinding patterns by chance may go away with more observations\nSpecificities to a single dataset may be less important if more sources\nMore observations help only if\n\nErrors and idiosyncrasies affect some observations but not all\nAdditional observations are from appropriate source\nIf worried about specificities of Vienna more observations from Vienna would not help"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#prediction-uncertainty",
    "href": "rm-data/slides/week05-06.html#prediction-uncertainty",
    "title": "Regression Analysis I",
    "section": "Prediction uncertainty",
    "text": "Prediction uncertainty\n\nGoal: predicting the value of \\(y\\) for observations outside the dataset, when only the value of \\(x\\) is known.\nWe predict \\(y\\) based on coefficient estimates, which are relevant in the general pattern/population. With linear regression you have a simple model: \\[y_i = \\hat{\\alpha} + \\hat{\\beta}x_i + \\epsilon_i\n\\]\nThe estimated statistic here is a predicted value for a particular observation \\(\\hat{y}_j\\). For an observation \\(j\\) with known value \\(x_j\\) this is \\[\\hat{y}_j = \\hat{\\alpha} + \\hat{\\beta}x_j\\]"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#prediction-uncertainty-1",
    "href": "rm-data/slides/week05-06.html#prediction-uncertainty-1",
    "title": "Regression Analysis I",
    "section": "Prediction uncertainty",
    "text": "Prediction uncertainty\n\nYou can produce two kinds of intervals:\n\nConfidence interval for the predicted value/regression line\n\nUncertainty comes from \\(\\hat{\\alpha}, \\hat{\\beta}\\)\n\nPrediction interval, uncertainty comes from \\(\\hat{\\alpha}, \\hat{\\beta}\\) and \\(\\epsilon_i\\)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#confidence-interval-of-the-regression-line-i.",
    "href": "rm-data/slides/week05-06.html#confidence-interval-of-the-regression-line-i.",
    "title": "Regression Analysis I",
    "section": "Confidence interval of the regression line I.",
    "text": "Confidence interval of the regression line I.\n\nThe predicted value \\(\\hat{y}_j\\) is based on \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) only.\n\nThus, the CI of the predicted value combines the CI for \\(\\hat{\\alpha}\\) and the CI for \\(\\hat{\\beta}\\).\n\nWhat to expect if we know the value of \\(x_j\\) and \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)?.\n\\[95\\%CI(\\hat{y}_j) = \\hat{y_j} \\pm 1.96 SE(\\hat{y}_j)\\]\nThe standard error of the predicted value is \\[SE(\\hat{y}_j) = \\frac{Std[e]}{\\sqrt{\\frac{1}{n}+ \\frac{(x_j - \\bar{x})^2}{nVar[x]}}}\n\\]\nUse robust SE formula in practice, but a simple formula is instructive"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#prediction-interval",
    "href": "rm-data/slides/week05-06.html#prediction-interval",
    "title": "Regression Analysis I",
    "section": "Prediction interval",
    "text": "Prediction interval\n\nPrediction interval answers:\n\nWhere to expect the particular \\(y_j\\) value if we know the corresponding \\(x_j\\) value and the estimates of the regression coefficients?\n\nThe CI of the predicted value is about \\(\\hat{y}_j\\): Where would the average value of the dependent variable be if we know \\(x_j\\).\nThe PI (prediction interval) is about \\(y_j\\) itself not its average value, what range of values we expect for \\(y_j\\) if we know \\(x_j\\).\nSo PI starts with CI. But adds additional uncertainty (\\(Std[\\epsilon_i]\\)) that actual \\(y_j\\) will be around its conditional."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#prediction-interval-1",
    "href": "rm-data/slides/week05-06.html#prediction-interval-1",
    "title": "Regression Analysis I",
    "section": "Prediction interval",
    "text": "Prediction interval\n\nThe formula for the 95% prediction interval is\n\n\\(95\\%PI(\\hat{y}_j) = \\hat{y} \\pm 1.96SPE(\\hat{y}_j)\\)\n\\(SPE(\\hat{y}_j) = Std[e]\\sqrt{\\textbf{1} + \\frac{1}{n} + \\frac{(x_j - \\bar{x})^2}{nVar[x]}}\\)\n\nSPE – Standard Prediction Error (SE of prediction)\nSummarizes the additional uncertainty: the actual \\(y_j\\) value is expected to be spread around its average value.\n\nThis is best estimated by the standard deviation of the residual \\(e\\).\n\nIn the formula, all elements get very small if \\(n\\) gets large, except for the new element."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#prediction-interval-2",
    "href": "rm-data/slides/week05-06.html#prediction-interval-2",
    "title": "Regression Analysis I",
    "section": "Prediction interval",
    "text": "Prediction interval\n\n\nCode\nqui:ssc install frause,\nqui:frause oaxaca, clear\nqui: drop if runiform()&lt;.8 \nqui: sort age\nqui: reg lnwage age\nqui: predict lnwage_hat\nqui: predict se_ci, stdp\nqui: predict se_pi, stdf\nqui: gen ci_low = lnwage_hat - 1.96*se_ci\nqui: gen ci_up = lnwage_hat + 1.96*se_ci\nqui: gen pi_low = lnwage_hat - 1.96*se_pi\nqui: gen pi_up = lnwage_hat + 1.96*se_pi\ntwoway  (rarea pi_low pi_up age, color(gs5%25) ) ///\n(rarea ci_low ci_up age, color(gs5%25) ) ///\n       (scatter lnwage age, color(navy)) (line lnwage_hat age, color(navy)) ///\n       , legend(off) ytitle(Log Wages) xtitle(Age)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#external-validity-1",
    "href": "rm-data/slides/week05-06.html#external-validity-1",
    "title": "Regression Analysis I",
    "section": "External validity",
    "text": "External validity\n\nStatistical inference helps us generalize to the population or general pattern\nIs this true beyond the data (other dates, countries, people, firms)?\n\nwe can’t assess it using our data.\n\nWe’ll never really know. Only think, investigate, make assumption, and hope…"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#however",
    "href": "rm-data/slides/week05-06.html#however",
    "title": "Regression Analysis I",
    "section": "However…",
    "text": "However…\n\nAnalyzing other data can help!\nFocus on \\(\\beta\\), the slope coefficient on \\(x\\).\nThe three common dimensions of generalization are: time, space, and other groups.\nTo learn about external validity, we always need additional data, on say, other countries or time periods.\nWe can then repeat regression and see if slope is similar!\nMeta-analysis: combining results from different studies to learn about external validity."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#stability-of-hotel-prices---idea",
    "href": "rm-data/slides/week05-06.html#stability-of-hotel-prices---idea",
    "title": "Regression Analysis I",
    "section": "Stability of hotel prices - idea",
    "text": "Stability of hotel prices - idea\n\nHere we ask different questions: whether we can infer something about the price–distance pattern for situations outside the data:\n\nIs the slope coefficient close to what we have in Vienna, November, weekday:\n\nOther dates (we will do this)\nOther cities\nOther type of accommodation: apartments\n\nCompare them to our benchmark model result"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#benchmark-model",
    "href": "rm-data/slides/week05-06.html#benchmark-model",
    "title": "Regression Analysis I",
    "section": "Benchmark model",
    "text": "Benchmark model\n\nThe benchmark model is a spline with a knot at 2 miles.\nDependent variable: log price\nData is restricted to 2017, November weekday in Vienna, 3-4 star hotels, within 8 miles."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#comparing-dates",
    "href": "rm-data/slides/week05-06.html#comparing-dates",
    "title": "Regression Analysis I",
    "section": "Comparing dates",
    "text": "Comparing dates\n\nResultsDiscussion\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-NOV-weekday\n2017-NOV-weekend\n2017-DEC-holiday\n2018-JUNE-weekend\n\n\n\n\ndist_0_2\n-0.31\n-0.44\n-0.36\n-0.31\n\n\n\n(0.038)\n(0.052)\n(0.041)\n(0.037)\n\n\ndist_2_7\n0.02\n0.00\n0.07\n0.04\n\n\n\n(0.033)\n(0.036)\n(0.050)\n(0.039)\n\n\nConstant\n5.02\n5.51\n5.13\n5.16\n\n\n\n(0.042)\n(0.067)\n(0.048)\n(0.050)\n\n\nObservations\n207\n125\n189\n181\n\n\nR.squared\n0.314\n0.430\n0.382\n0.306\n\n\n\nNote: Robust standard errors in parentheses *** p&lt;0.01, ** p&lt;0.05, * p&lt;0.1\n\n\n\nNovember weekday and the June weekend: \\(\\hat{\\beta}_1 = -0.31\\)\nEstimate is similar for December (-0.36 log units)\nDifferent for the November weekend: they are 0.44 log units or 55% (exp(0.44) - 1) cheaper during the November weekend.\n\nThe corresponding 95% confidence intervals overlap somewhat: they are [-0.39,-0.23] and [-0.54,-0.34].\nThus we cannot say for sure that the price–distance patterns are different during the weekday and weekend in November."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#stability-of-hotel-prices---takeaway",
    "href": "rm-data/slides/week05-06.html#stability-of-hotel-prices---takeaway",
    "title": "Regression Analysis I",
    "section": "Stability of hotel prices - takeaway",
    "text": "Stability of hotel prices - takeaway\n\nFairly stable overtime but uncertainty is larger\nEvidence of some external validity in Vienna\nExternal validity – if model applied beyond data, there is additional uncertainty!"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#take-away",
    "href": "rm-data/slides/week05-06.html#take-away",
    "title": "Regression Analysis I",
    "section": "Take-away",
    "text": "Take-away\n\nRegression Fundamentals: Regression analysis is essential for identifying relationships between variables. It’s used for both causal and predictive analysis.\nCoefficient Interpretation: In linear regression, the intercept and slope have specific meanings, indicating the expected values and changes in the dependent variable relative to the explanatory variable.\nNon-linear Relationships: Non-linear patterns can be captured using non-parametric methods and transformations like logs and splines. Depends on the goal of the analysis.\nData Challenges: Real-world data issues like measurement errors and extreme values can affect regression results. Analysts must carefully manage these challenges.\nGeneralization and Inference: Statistical inference helps generalize findings beyond the sample, but external validity requires additional data and context."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#reading-homework",
    "href": "rm-data/slides/week05-06.html#reading-homework",
    "title": "Regression Analysis I",
    "section": "Reading-Homework",
    "text": "Reading-Homework\nRead Case Study as an example of a simple linear regression analysis."
  },
  {
    "objectID": "rm-data/slides/week08.html#what-we-will-see-today",
    "href": "rm-data/slides/week08.html#what-we-will-see-today",
    "title": "Modeling Probabilities",
    "section": "What we will see today",
    "text": "What we will see today\n\nLinear Probability Model - LPM\nLogit & probit\nGoodness of fit\nDiagnostics\nSummary"
  },
  {
    "objectID": "rm-data/slides/week08.html#motivation",
    "href": "rm-data/slides/week08.html#motivation",
    "title": "Modeling Probabilities",
    "section": "Motivation",
    "text": "Motivation\n\nWhat are the health benefits of not smoking? Considering the 50+ population, we can investigate if differences in smoking habits are correlated with differences in health status.\n\ngood health vs bad health"
  },
  {
    "objectID": "rm-data/slides/week08.html#binary-events",
    "href": "rm-data/slides/week08.html#binary-events",
    "title": "Modeling Probabilities",
    "section": "Binary events",
    "text": "Binary events\n\nSome outcomes are things that either happen or don’t happen, which can be captured by binary variables\n\ne.g. a person is healthy or not, a person is employed or not, a person is a smoker or not. We dont see a person that is half healthy, half employed, or half a smoker.\n\nHow can we model these events?\n\nWe have seen this before. Instead of modeling the value itself, we model the probability of the event happening.\n\n\n\\[E[y] = P[y = 1]\\]\n\nIn fact, the average of a 0–1 event is the probability of that event happening. Which can also be estimated as conditional probabilities:\n\n\\[E[y|x_1, x_2, ...] = P[y = 1|x_1, x_2, ...]\\]\n\nGood news, we can use the same tools we have been using to model these probabilities."
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-linear-probability-model",
    "href": "rm-data/slides/week08.html#lpm-linear-probability-model",
    "title": "Modeling Probabilities",
    "section": "LPM: Linear probability model",
    "text": "LPM: Linear probability model\n\nLinear Probability Model (LPM) is a linear regression with a binary dependent variable\n\nIt has the goal of modeling the probability of an event happening\n\nA linear regressions with binary dependent variables shows:\n\ndifferences in expected \\(y\\) by \\(x\\), represent diferences in probability of \\(y = 1\\) by \\(x\\).\n\nIntroduce notation for probability: \\(y^P = P[y = 1|x_1, x_2, . . .]\\)\nLinear probability model (LPM) regression is \\(y^P = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\)"
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-interpretation",
    "href": "rm-data/slides/week08.html#lpm-interpretation",
    "title": "Modeling Probabilities",
    "section": "LPM: Interpretation",
    "text": "LPM: Interpretation\n\\[y^P = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\]\n\nSo far nothing changes in terms of modelling or estimation. However, the interpretation of the coefficients changes.\n\n\\(y^P\\) denotes the probability that the dependent variable is one, conditional on the right-hand-side variables of the model.\n\\(\\beta_0\\) shows the predicted probability of \\(y\\) if all \\(x\\) are zero.\n\\(\\beta_1\\) shows the difference in the probability that \\(y = 1\\) for observations that are different in \\(x_1\\) but are the same in terms of \\(x_2\\). (ceteris paribus)"
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-modelling",
    "href": "rm-data/slides/week08.html#lpm-modelling",
    "title": "Modeling Probabilities",
    "section": "LPM: Modelling",
    "text": "LPM: Modelling\n\nLinear probability model (LPM) can be estimated using OLS. (just like linear regression)\nWe can use all transformations in \\(x\\), that we used before:\n\nLog, Polinomials, Splines, dummies, interactions, etc. They all work.\n\nAll formulae and interpretations for standard errors, confidence intervals, hypotheses and p-values of tests are the same.\nIMPORTANT Heteroskedasticity robust error are essential in this case!\n\nBy construction LPMs are heteroskedastic!\nAnd ignoring this fact will lead to biased standard errors and confidence intervals."
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-prediction",
    "href": "rm-data/slides/week08.html#lpm-prediction",
    "title": "Modeling Probabilities",
    "section": "LPM: Prediction",
    "text": "LPM: Prediction\n\nPredicted values - \\(\\hat{y}_P\\) - may be problematic. Although they are calculated the same way, they need to be interpreted as probabilities.\n\n\\[\\hat{y}^P = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2\\]\n\nPredicted values need to be between 0 and 1 because they are probabilities\nBut in LPM, predictions may be below 0 and above 1. No formal bounds in the model.\n\nmore likely than certain, less likely than impossible"
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-prediction-1",
    "href": "rm-data/slides/week08.html#lpm-prediction-1",
    "title": "Modeling Probabilities",
    "section": "LPM: Prediction",
    "text": "LPM: Prediction\n\nWhen to get worried?:\n\nWith continuous variables that can take any value (GDP, Population, sales, etc), this could be a serious issue (extrapolation)\n\nWe need to check if predictions are within the 0-1 range at least “in-sample”. But, this is not a guarantee that it will be the case “out-of-sample”.\n\nWith binary variables, no problem (‘saturated models’) (interpolation)\n\nNot problem because “simple” means will always be between 0 and 1.\n\n\nSo, a problem if goal is prediction!\nNot a big issue for inference → uncover patterns of association.\n\nBut it may give biased estimates…(in theory)"
  },
  {
    "objectID": "rm-data/slides/week08.html#cs-does-smoking-pose-a-health-risk",
    "href": "rm-data/slides/week08.html#cs-does-smoking-pose-a-health-risk",
    "title": "Modeling Probabilities",
    "section": "CS: Does smoking pose a health risk?",
    "text": "CS: Does smoking pose a health risk?\n\nThis is on of the few datasets from the book that is not directly available from their website. If interested, you need to go over the repository, and follow the instructions to access the data.\n\nThus, we will use a different dataset to illustrate the concepts."
  },
  {
    "objectID": "rm-data/slides/week08.html#cs-does-smoking-during-pregnancy-affect-birth-weight",
    "href": "rm-data/slides/week08.html#cs-does-smoking-during-pregnancy-affect-birth-weight",
    "title": "Modeling Probabilities",
    "section": "CS: Does smoking during pregnancy affect birth weight?",
    "text": "CS: Does smoking during pregnancy affect birth weight?\n\nThe question is whether, and by how much, smoking during pregnancy affects the likelihood that a baby is born with low birth weight.\nWe will use “lbw” dataset from Stata’s example datasets.\nThe dataset contains information on 189 observations of mothers and their newborns.\n\nlow is a binary variable indicating whether the baby was born with low birth weight (&lt;2500gr &lt;5.5lbs).\nsmoke is a binary variable indicating whether the mother smoked during pregnancy."
  },
  {
    "objectID": "rm-data/slides/week08.html#data",
    "href": "rm-data/slides/week08.html#data",
    "title": "Modeling Probabilities",
    "section": "Data",
    "text": "Data\n\n\\(low = 1\\) if baby was born with low birth weight\n\\(low = 0\\) if baby was born with normal weight -Some demographic information on all individual\nWe exclude women &lt;15 years old and &gt;40 years old\n\nAlso exclude women with Weight &gt; 200lbs (before pregnancy)"
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-in-stata",
    "href": "rm-data/slides/week08.html#lpm-in-stata",
    "title": "Modeling Probabilities",
    "section": "LPM: in Stata",
    "text": "LPM: in Stata\n\nStart with a simple univariate model: \\(P[low|smoke] = \\alpha + \\beta[smoke]\\)\n\n\nwebuse lbw, clear\ndrop if age &lt; 15 | age &gt; 40 | lwt &gt; 200\nreg low smoke, robust\n\n\n\n\n(Hosmer & Lemeshow data)\n(10 observations deleted)\n\nLinear regression                               Number of obs     =        179\n                                                F(1, 177)         =       4.70\n                                                Prob &gt; F          =     0.0316\n                                                R-squared         =     0.0272\n                                                Root MSE          =     .46208\n\n------------------------------------------------------------------------------\n             |               Robust\n         low | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       smoke |    .157405   .0726412     2.17   0.032     .0140507    .3007593\n       _cons |   .2568807   .0420845     6.10   0.000     .1738289    .3399326\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-interpretation-1",
    "href": "rm-data/slides/week08.html#lpm-interpretation-1",
    "title": "Modeling Probabilities",
    "section": "LPM: Interpretation",
    "text": "LPM: Interpretation\nInterpretation:\n\nThe coefficient on smoker shows the difference in the probability of a baby.\nBabies are 15.7 percentage points more likely to be born with low birth weight if the mother smoked during pregnancy.\n\nAre you comparing Apples to apples?\nLets add additional controls to capture other factors"
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-with-many-regressors-i",
    "href": "rm-data/slides/week08.html#lpm-with-many-regressors-i",
    "title": "Modeling Probabilities",
    "section": "LPM: with many regressors I",
    "text": "LPM: with many regressors I\n\nMultiple regression – closer to causality\nCompare women who are very similar in many respects but are different in smoking habits\nSmokers / non-smokers – different in many other behaviors and conditions:\n\npersonal traits (age, race)\nbehavior pre-pregnancy (Pre-pregnancy weight)\nMedical history (History of Hypertension)\nbackground for pregnancy (Number of prenatal visits, Previous premature labor)"
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-with-many-regressors-ii",
    "href": "rm-data/slides/week08.html#lpm-with-many-regressors-ii",
    "title": "Modeling Probabilities",
    "section": "LPM with many regressors II",
    "text": "LPM with many regressors II\n\nMay also consider functional form selection or interactions\nTrial and error, or theory-based\nUseful to check bivariate relationships (scatter plots, Lpoly, correlations)\n\nFor now, assume linear relationships"
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-with-many-regressors-iii",
    "href": "rm-data/slides/week08.html#lpm-with-many-regressors-iii",
    "title": "Modeling Probabilities",
    "section": "LPM with many regressors III",
    "text": "LPM with many regressors III\n\nCode\nqui {\ngen any_premature = ptl &gt;0\nren ftv no_of_visits_1tr\nren ht hist_hyper\nren lwt wgt_bef_preg\nreg low smoke age i.race any_premature hist_hyper  no_of_visits_1tr wgt_bef_preg, robust nohead\n}\nest store lpm_results\nesttab lpm_results,   se  wide nonumber ///\ncollabel(b se) md drop(1.race) nomtitle b(3) nonotes\n\n\n\n\n\nb\nse\n\n\n\n\nsmoke\n0.151*\n(0.075)\n\n\nage\n-0.005\n(0.007)\n\n\n2.race\n0.210\n(0.112)\n\n\n3.race\n0.126\n(0.079)\n\n\nany_premature\n0.275**\n(0.097)\n\n\nhist_hyper\n0.396**\n(0.143)\n\n\nno_of_visits_1tr\n-0.005\n(0.034)\n\n\nwgt_bef_preg\n-0.002\n(0.002)\n\n\n_cons\n0.464\n(0.252)\n\n\nN\n179\n\n\n\n\nRobust standard errors in parentheses * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "rm-data/slides/week08.html#detour-regression-tables",
    "href": "rm-data/slides/week08.html#detour-regression-tables",
    "title": "Modeling Probabilities",
    "section": "Detour: Regression Tables",
    "text": "Detour: Regression Tables\n\nIf need to show many explanatory variables\nDo not show table 12*2 rows, people will not see it.\n\nAvoid copy pasting from your document! Those tables are unwieldy.\n\nEither only show selected variables (smoke + 2-3 others)\nOr may need to create two columns. (a bit more work)\n\nIn my case, Wide format did the trick.\n\nMake site you have title, N of observations, footnote on SE, stars.\nSE, stars: many different notations. Check carefully.\n\nesttab default is \\(p^{***}= p&lt;0.001\\), \\(0.01\\) and \\(0.05\\)\nIn papers there is \\(p^{***}=p&lt;0.01\\), \\(0.05\\) and \\(0.1\\)."
  },
  {
    "objectID": "rm-data/slides/week08.html#does-smoking-pose-a-health-risk-for-the-baby",
    "href": "rm-data/slides/week08.html#does-smoking-pose-a-health-risk-for-the-baby",
    "title": "Modeling Probabilities",
    "section": "Does smoking pose a health risk for the baby?",
    "text": "Does smoking pose a health risk for the baby?\n\nCoefficient on smoking during pregnancy is -.151.\n\nWomen who smoked during pregnancy are 15.1 percentage points more likely to have a baby with low birth weight.\n\nThe 95% confidence interval is relatively wide \\([0.002, 0.300]\\), but it does not contain zero\nAge, Race?, Nr of Visits and Pre-pregnancy weight do not seem to be factors\nHypertension and previous premature labor are significant factors, increasing the probability of low birth weight by 40pp and 27.5pp, respectively."
  },
  {
    "objectID": "rm-data/slides/week08.html#lpms-predicted-probabilities",
    "href": "rm-data/slides/week08.html#lpms-predicted-probabilities",
    "title": "Modeling Probabilities",
    "section": "LPM’s predicted probabilities",
    "text": "LPM’s predicted probabilities\n\n\nCode\nqui: reg low smoke age i.race any_premature hist_hyper  no_of_visits_1tr wgt_bef_preg, robust nohead\nqui: predict low_hat\nqui:histogram low_hat\nsum low_hat\n\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     low_hat |        179    .3184358    .1878437  -.0346098   .9483117"
  },
  {
    "objectID": "rm-data/slides/week08.html#analysis-of-lpms-predicted-probabilities",
    "href": "rm-data/slides/week08.html#analysis-of-lpms-predicted-probabilities",
    "title": "Modeling Probabilities",
    "section": "Analysis of LPM’s predicted probabilities",
    "text": "Analysis of LPM’s predicted probabilities\n\nWhat to doWhat we find\n\n\n\nDrill down in distribution:\n\nLooking at the composition of people: top vs bottom part of probability distribution\nLook at average values of covariates for top and bottom X% of predicted probabilities!\n\n\n\n\n\n\nCode\nsort low_hat\nset linesize 255\n \nqui: gen flag = 1 if _n&lt;=5\nqui: replace  flag = 2 if _n&gt;=_N-4\nlist low_hat smoke age race any_premature hist_hyper  no_of_visits_1tr wgt_bef_preg   if flag==1\nlist low_hat smoke age race any_premature hist_hyper  no_of_visits_1tr wgt_bef_preg   if flag==2\n\n\n\n     +---------------------------------------------------------------------------------+\n     |   low_hat       smoke   age    race   any_pr~e   hist_h~r   no_of_~r   wgt_be~g |\n     |---------------------------------------------------------------------------------|\n  1. | -.0346098   Nonsmoker    32   White          0          0          2        186 |\n  2. | -.0280454   Nonsmoker    36   White          0          0          0        175 |\n  3. |  .0019138   Nonsmoker    32   White          0          0          0        170 |\n  4. |  .0158307   Nonsmoker    23   White          0          0          0        190 |\n  5. |  .0284496   Nonsmoker    28   White          0          0          0        167 |\n     +---------------------------------------------------------------------------------+\n\n     +--------------------------------------------------------------------------------+\n     |  low_hat       smoke   age    race   any_pr~e   hist_h~r   no_of_~r   wgt_be~g |\n     |--------------------------------------------------------------------------------|\n175. | .7197446      Smoker    34   Black          0          1          0        187 |\n176. | .7369648   Nonsmoker    17   Black          0          1          0        142 |\n177. | .8160616      Smoker    18   Black          1          0          0        110 |\n178. | .8545191   Nonsmoker    26   Other          1          1          1        154 |\n179. | .9483117   Nonsmoker    25   Other          1          1          0        105 |\n     +--------------------------------------------------------------------------------+"
  },
  {
    "objectID": "rm-data/slides/week08.html#probability-models-logit-and-probit",
    "href": "rm-data/slides/week08.html#probability-models-logit-and-probit",
    "title": "Modeling Probabilities",
    "section": "Probability models: logit and probit",
    "text": "Probability models: logit and probit\n\nPrediction: predicted probability need to be between 0 and 1\n\nThus, for prediction, we must use non-linear models\nActually, its a quasi-linear model.\n\nThe model, itself, is linear in the parameters\nbut need to relate this to the probability of the \\(y = 1\\) event, using a nonlinear function that maps the linear index into a 0-1 range: ‘Link function’\n\n\\[\\begin{aligned}\nXB &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... \\\\\ny^P &= F(XB) \\rightarrow y^P \\in (0,1)\n\\end{aligned}\n\\]\n\nTwo options: Logit and probit – different link function"
  },
  {
    "objectID": "rm-data/slides/week08.html#link-functions-i.",
    "href": "rm-data/slides/week08.html#link-functions-i.",
    "title": "Modeling Probabilities",
    "section": "Link functions I.",
    "text": "Link functions I.\nCall \\(XB = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...\\)\n\nThe Logit:\n\\(y^P = \\Lambda(XB) = \\frac{\\exp(XB)}{1 + \\exp(XB)}\\)\nThe probit:\n\\(y^P = \\Phi(XB) \\rightarrow \\Phi(z) = \\int_{-\\infty}^z \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz\\)\n\nwhere \\(\\Lambda()\\) is called logistic function, and \\(\\Phi()\\) is the cumulative distribution function (CDF) of the standard normal distribution."
  },
  {
    "objectID": "rm-data/slides/week08.html#link-functions-ii.",
    "href": "rm-data/slides/week08.html#link-functions-ii.",
    "title": "Modeling Probabilities",
    "section": "Link functions II.",
    "text": "Link functions II.\n\n\n\nBoth link functions are S-shaped curves bounded between 0 and 1.\nThere is but a small difference between the two.\nbut estimated coefficients will be different.\n\n\n\n\nCode\nqui {\nclear\nrange p 0 1 202\ndrop if p==0 | p==1 \ngen x = invnormal(p)\ngen y = (x+rnormal())&gt;0\nreg  y x\npredict y_1\nlogit y x\npredict y_2\nprobit y x\npredict y_3\ndrop if abs(x)&gt;2\ntwo (line y_1 x ) (line y_2 x) (line y_3 x), ///\nlegend(order(1 \"LPM\" 2 \"Logit\" 3 \"Probit\") pos(3) ring(0) col(1)) \n}"
  },
  {
    "objectID": "rm-data/slides/week08.html#logit-and-probit-interpretation",
    "href": "rm-data/slides/week08.html#logit-and-probit-interpretation",
    "title": "Modeling Probabilities",
    "section": "Logit and probit interpretation",
    "text": "Logit and probit interpretation\n\nBoth the probit and the logit transform the \\(\\beta_0 + \\beta_1 x_1 + ...\\) linear combination using a link function that shows an S-shaped curve.\nThe slope of this curve keeps changing as we change whatever is inside, but it’s steepest when \\(y^P = 0.5\\) (inflection point)\nThe difference in \\(y^P\\) corresponds to changes in probabilities, between any two values of \\(x\\).\nTo find how much is related to a particular \\(x\\), You need to take the partial derivatives.\nImportant consequence: no direct interpretation of the raw coefficient values!\n\nThus, always know if you are interpreting the raw coefficients or the marginal differences."
  },
  {
    "objectID": "rm-data/slides/week08.html#marginal-differences-marginal-effects",
    "href": "rm-data/slides/week08.html#marginal-differences-marginal-effects",
    "title": "Modeling Probabilities",
    "section": "Marginal differences (marginal effects)",
    "text": "Marginal differences (marginal effects)\n\nNOTE As before, the word “effect” should be used with caution. In the book, they use “marginal differences” instead. as a more neutral term.\n\n\nLink functions makes associates \\(\\Delta x\\) into \\(\\Delta y_P\\). we do not interpret raw coefficients! (except for direction)\nInstead, transform them into ‘marginal differences’ for interpretation purposes\nThey are also called ‘marginal effects’ or ‘average marginal effects (AME)’ or ‘average partial effects’.\nAverage marginal difference has the same interpretation as the coefficient of linear probability models, but with caveats."
  },
  {
    "objectID": "rm-data/slides/week08.html#marginal-differences-discrete-x",
    "href": "rm-data/slides/week08.html#marginal-differences-discrete-x",
    "title": "Modeling Probabilities",
    "section": "Marginal differences: Discrete \\(x\\)",
    "text": "Marginal differences: Discrete \\(x\\)\n\nif \\(x\\) is a categorical (0-1), the marginal difference is the difference in the predicted probability of \\(y = 1\\), that corresponds to a change from \\(x = 0\\) to \\(x = 1\\).\n\n\\[\\Delta y^P = y^P(x = 1) - y^P(x = 0)\\]\n\nThen we simply “average” this difference across all observations."
  },
  {
    "objectID": "rm-data/slides/week08.html#marginal-differences-continous-x",
    "href": "rm-data/slides/week08.html#marginal-differences-continous-x",
    "title": "Modeling Probabilities",
    "section": "Marginal differences: continous \\(x\\)",
    "text": "Marginal differences: continous \\(x\\)\n\nIf \\(x\\) is continuous, the marginal difference is calculated as the derivative (for a small change in \\(x\\)).\n\n\\[\\frac{\\partial y^P}{\\partial x_1} = \\beta_1 \\cdot f(XB)\\]\n\nWhich is then averaged across all observations to report a single number.\nIn practice, we interpret this as the change in the probability of \\(y = 1\\) for a one-unit change in \\(x_1\\)."
  },
  {
    "objectID": "rm-data/slides/week08.html#how-to-estimate-this-models",
    "href": "rm-data/slides/week08.html#how-to-estimate-this-models",
    "title": "Modeling Probabilities",
    "section": "How to estimate this models?",
    "text": "How to estimate this models?\nMaximum likelihood estimation!\n\nWhen estimating a logit or probit model, we use ‘maximum likelihood’ estimation.\n\nSee 11.U2 for details.\n\nIdea for maximum likelihood is another way to get coefficient estimates.\n\n1st You specify a (conditional) distribution, that you will use during the estimation.\n\nThis is logistic for logit and normal for probit model.\n\n2nd You maximize this function w.r.t. your \\(\\beta\\) parameters → gives the maximum likelihood for this model.\n\nDifferent from OLS: No closed form solution → need to use search algorithms.\n\nThus… more computationally intensive."
  },
  {
    "objectID": "rm-data/slides/week08.html#predictions-for-lmp-logit-and-probit-i.",
    "href": "rm-data/slides/week08.html#predictions-for-lmp-logit-and-probit-i.",
    "title": "Modeling Probabilities",
    "section": "Predictions for LMP, Logit and Probit I.",
    "text": "Predictions for LMP, Logit and Probit I.\n\n\nCode\nqui {\n  webuse lbw, clear\n  drop if age &lt; 15 | age &gt; 40 | lwt &gt; 200\n  gen any_premature = ptl &gt;0\n  ren ftv no_of_visits_1tr\n  ren ht hist_hyper\n  ren lwt wgt_bef_preg\n  gen black = 2.race\n  gen other = 3.race\n  reg low smoke age i.black i.other i.any_premature hist_hyper , robust nohead\n  predict low_hat_ols\n  est sto lpm_results\n  logit low smoke age black other any_premature hist_hyper , robust nohead\n  predict low_hat_logit\n   probit low smoke age black other any_premature hist_hyper , robust nohead\n  predict low_hat_probit\n   two (scatter  low_hat_logit low_hat_probit low_hat_ols) ///\n      (line low_hat_ols low_hat_ols, sort), ///\n      legend(order(1 \"Logit\" 2 \"Probit\" 3 \"LPM\") pos(3) ring(0) col(1))\n}"
  },
  {
    "objectID": "rm-data/slides/week08.html#coefficient-results-for-logit-and-probit",
    "href": "rm-data/slides/week08.html#coefficient-results-for-logit-and-probit",
    "title": "Modeling Probabilities",
    "section": "Coefficient results for logit and probit",
    "text": "Coefficient results for logit and probit\n\nCode\nqui {\n  logit low smoke age i.black i.other i.any_premature hist_hyper , robust nohead\n  est sto logit_results\n  margins, dydx(*) post\n  est sto logit_mfx\n  probit low smoke age i.black i.other i.any_premature hist_hyper , robust nohead  \n  est sto probit_results\n  margins, dydx(*) post\n  est sto probit_mfx\n}\nesttab lpm_results logit_results  logit_mfx probit_results probit_mfx, se  nonumber drop(0.*) ///\nmtitle(LPM Logit Logit_MFX Probit Probit_MFX) collabel(none) md ///\nstar(* 0.10 ** 0.05 *** 0.01) nonotes \n\n\n\n\n\n\n\n\n\n\n\n\n\nLPM\nLogit\nLogit_MFX\nProbit\nProbit_MFX\n\n\n\n\nmain\n\n\n\n\n\n\n\nsmoke\n0.166**\n0.925**\n0.169**\n0.549**\n0.169**\n\n\n\n(0.0739)\n(0.397)\n(0.0695)\n(0.235)\n(0.0697)\n\n\nage\n-0.00703\n-0.0447\n-0.00818\n-0.0271\n-0.00835\n\n\n\n(0.00669)\n(0.0377)\n(0.00688)\n(0.0221)\n(0.00679)\n\n\n1.black\n0.192*\n1.012*\n0.202*\n0.607*\n0.202*\n\n\n\n(0.111)\n(0.526)\n(0.109)\n(0.314)\n(0.108)\n\n\n1.other\n0.150*\n0.884**\n0.164**\n0.524**\n0.163**\n\n\n\n(0.0770)\n(0.435)\n(0.0787)\n(0.254)\n(0.0779)\n\n\n1.any_premature\n0.284***\n1.330***\n0.279***\n0.810***\n0.280***\n\n\n\n(0.0972)\n(0.441)\n(0.0944)\n(0.270)\n(0.0956)\n\n\nhist_hyper\n0.370***\n1.720**\n0.315***\n1.063**\n0.327***\n\n\n\n(0.137)\n(0.674)\n(0.117)\n(0.415)\n(0.122)\n\n\n_cons\n0.270\n-0.971\n\n-0.576\n\n\n\n\n(0.183)\n(0.983)\n\n(0.577)\n\n\n\nN\n179\n179\n179\n179\n179\n\n\n\nStandard errors in parentheses * p &lt; 0.1, ** p &lt; 0.04, *** p &lt; 0.01"
  },
  {
    "objectID": "rm-data/slides/week08.html#does-smoking-pose-a-health-risk-logit-and-probit",
    "href": "rm-data/slides/week08.html#does-smoking-pose-a-health-risk-logit-and-probit",
    "title": "Modeling Probabilities",
    "section": "Does smoking pose a health risk?– logit and probit",
    "text": "Does smoking pose a health risk?– logit and probit\n\nLPM – interpret the coefficients as usual.\nLogit, probit - Interpret the marginal differences. Basically the same.\n\nMarginal differences are essentially the same across the logit and the probit.\nEssentially the same as the corresponding LPM coefficients.\n\nHappens often:\n\nOften LPM is good enough for interpretation.\nCheck if logit/probit very different.\n\nif so, Investigate functional forms if yes."
  },
  {
    "objectID": "rm-data/slides/week08.html#goodness-of-fit",
    "href": "rm-data/slides/week08.html#goodness-of-fit",
    "title": "Modeling Probabilities",
    "section": "Goodness of fit",
    "text": "Goodness of fit\n\nThere is no generally accepted goodness of fit measure\n\nThis is because we do not observe probabilities only 1 and 0, so we cannot FIT those probabilities.\n\nThere are, however, other options to evaluate the quality of the model.\n\nR-squared\nBrier score\nPseudo R-squared\nlog-loss"
  },
  {
    "objectID": "rm-data/slides/week08.html#goodness-of-fit-r-squared",
    "href": "rm-data/slides/week08.html#goodness-of-fit-r-squared",
    "title": "Modeling Probabilities",
    "section": "Goodness of fit: R-squared",
    "text": "Goodness of fit: R-squared\n\\[R^2 = 1 - \\frac{\\sum_{i=1}^n (\\hat{y}_P^i - y_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\\]\n\nR-squared is not useful for binary outcomes\n\nIt can be calculated, but it lacks the interpretation we had for linear models, because we are fitting the probabilities, not the outcomes."
  },
  {
    "objectID": "rm-data/slides/week08.html#brier-score",
    "href": "rm-data/slides/week08.html#brier-score",
    "title": "Modeling Probabilities",
    "section": "Brier score",
    "text": "Brier score\n\\[\\text{Brier} = \\frac{1}{n}\\sum_{i=1}^n (\\hat{y}_P^i - y_i)^2\\]\n\nThe Brier score is the average distance (mean squared difference) between predicted probabilities and the actual value of \\(y\\).\nSmaller the Brier score, the better.\nWhen comparing two predictions, the one with the smaller Brier score is the better prediction because it produces less (squared) error on average.\nRelated to a main concept in prediction: mean squared error (MSE)"
  },
  {
    "objectID": "rm-data/slides/week08.html#pseudo-r2",
    "href": "rm-data/slides/week08.html#pseudo-r2",
    "title": "Modeling Probabilities",
    "section": "Pseudo R2",
    "text": "Pseudo R2\n\\[pR^2 = 1 - \\frac{\\text{Log-likelihood}_{\\text{model}}}{\\text{Log-likelihood}_{\\text{intercept only}}}\\]\n\nIt is similar to the R-squared, as it measures the goodness of fit, tailored to nonlinear models and binary outcomes.\n\nMost widely used: McFadden’s R-squared (Stata uses this)\n\nComputes the ratio of log-likelihood of the model vs intercept only.\nCan be computed for the logit and the probit but not for the linear probability model. (unless you re-define the Log-likelihood)"
  },
  {
    "objectID": "rm-data/slides/week08.html#log-loss",
    "href": "rm-data/slides/week08.html#log-loss",
    "title": "Modeling Probabilities",
    "section": "Log-loss",
    "text": "Log-loss\n\\[\\text{Log-loss} = \\frac{1}{n}\\sum_{i=1}^n \\left[y_i \\log(\\hat{y}_P^i) + (1-y_i) \\log(1-\\hat{y}_P^i) \\right]\\]\n\nThe log-loss is a measured derived from the log-likelihood function. It measures how much observed data dissagrees with the predicted probabilities.\nThe smaller (close to zero) the log-loss, the better the model."
  },
  {
    "objectID": "rm-data/slides/week08.html#practical-use",
    "href": "rm-data/slides/week08.html#practical-use",
    "title": "Modeling Probabilities",
    "section": "Practical use",
    "text": "Practical use\n\nThere are several measured of model fit, but they often give the same ranking of models.\nDo not use R-squared. Even for LPM, it has no interpretation.\nIf using probit vs logit: pseudo R-squared may be used to rank logit and probit models.\nUse, especially for prediction: Brier score is a metric that can be computed for all models and is used in prediction."
  },
  {
    "objectID": "rm-data/slides/week08.html#bias-of-the-predictions",
    "href": "rm-data/slides/week08.html#bias-of-the-predictions",
    "title": "Modeling Probabilities",
    "section": "Bias of the predictions",
    "text": "Bias of the predictions\n\nPost-prediction: we may be interested to study some features of our model\nOne specific goal: evaluating the bias of the prediction.\n\nProbability predictions are unbiased if they are right on average = the average of predicted probabilities is equal to the actual probability of the outcome.\nIf the prediction is unbiased, the bias is zero.\n\nUnless the model is really bad, unconditional bias is not a big issue.\n\nOnly Probit will be biased."
  },
  {
    "objectID": "rm-data/slides/week08.html#calibration",
    "href": "rm-data/slides/week08.html#calibration",
    "title": "Modeling Probabilities",
    "section": "Calibration",
    "text": "Calibration\n\nUnbiasedness refers to the whole distribution of probability predictions.\nA finer and stricter concept is calibration\nA prediction is well calibrated if the actual probability of the outcome is equal to the predicted probability for each and every value of the predicted probability. \\[E(y|y^P) = y^P\\]\n‘Calibration curve’ is used to show this.\nA model may be unbiased (right on average) but not well calibrated\n\nunderestimate high probability events and overestimate low probability ones"
  },
  {
    "objectID": "rm-data/slides/week08.html#calibration-curve",
    "href": "rm-data/slides/week08.html#calibration-curve",
    "title": "Modeling Probabilities",
    "section": "Calibration curve",
    "text": "Calibration curve\n\nHorizontal axis shows the values of all predicted probabilities (\\(\\hat{y}_P\\)).\nVertical axis shows the fraction of \\(y = 1\\) observations for all observations with the corresponding predicted probability.\nThe closer the curve is to the 45-degree line, the better the calibration.\n\nIn practice:\n\nCreate bins for predicted probabilities and make comparisons of the actual event’s probability.\nOr use non-parametric methods to estimate the calibration curve."
  },
  {
    "objectID": "rm-data/slides/week08.html#calibration-curve-1",
    "href": "rm-data/slides/week08.html#calibration-curve-1",
    "title": "Modeling Probabilities",
    "section": "Calibration curve",
    "text": "Calibration curve\n\n\nCode\ntwo (lpoly low low_hat_ols) (lpoly low low_hat_logit) ///\n(lpoly low low_hat_probit) (function y = x, range(0 1) lcolor(black%50)), ///\nlegend(order(1 \"LPM\" 2 \"Logit\" 3 \"Probit\") ) scale(1.5) ///\nxtitle(\"Predicted probability\") ytitle(\"Avg outcome\")"
  },
  {
    "objectID": "rm-data/slides/week08.html#probability-models-summary",
    "href": "rm-data/slides/week08.html#probability-models-summary",
    "title": "Modeling Probabilities",
    "section": "Probability models summary",
    "text": "Probability models summary\n\nFind patterns when \\(y\\) is binary can be done using model probability with regressions\nLinear probability model is mostly good enough, easy inference.\n\nto-go for data exploration, quick analysis, and diagnostics\nbut, predicted values could be below 0, above 1\n\nLogit (and probit) - better when aim is prediction, predicted values strictly between 0-1\nMost often, LPM, logit, probit - similar inference\nUse marginal (average) differences (with logit/probit)\nNo trivial goodness of fit. Brier score or pseudo-R-Squared.\nCalibration is important for prediction."
  },
  {
    "objectID": "rm-data/slides/week10.html#motivation",
    "href": "rm-data/slides/week10.html#motivation",
    "title": "Prediction Setup",
    "section": "Motivation",
    "text": "Motivation\n\nImagine you want to sell your car soon and need to predict its price. You have data on similar used cars, and several regression models could help estimate its value now and in a year. How do you choose the best model?\nOr, take an ice cream shop—using past sales and temperature data, you want to predict sales for the coming days. What factors should you consider to ensure your prediction is as accurate as possible?"
  },
  {
    "objectID": "rm-data/slides/week10.html#prediction-basics-the-process",
    "href": "rm-data/slides/week10.html#prediction-basics-the-process",
    "title": "Prediction Setup",
    "section": "Prediction Basics: The Process",
    "text": "Prediction Basics: The Process\n\nThe idea of behind prediction is that we want to assign a value of \\(y\\)(\\(\\hat y\\)) (target or outcome) for observations we do not have the value.\nWhat we have, in our original data or working sample, is a set of observations with \\(y\\) and \\(x\\) values. We use them to “learn” the patterns of association between \\(y\\) and \\(x\\).\nBut for the target observations, we only have \\(x\\) values."
  },
  {
    "objectID": "rm-data/slides/week10.html#prediction-basics-the-process-1",
    "href": "rm-data/slides/week10.html#prediction-basics-the-process-1",
    "title": "Prediction Setup",
    "section": "Prediction Basics: The Process",
    "text": "Prediction Basics: The Process\n\nThe process of prediction involves:\n\nDetermine what is the target variable \\(y\\) and the features \\(x\\).\nUsing the original data to estimate a model that describes the patterns of association between \\(y\\) and \\(x\\).\nUsing the estimated model to predict the value of \\(y\\) for the target observations.\n\nEvaluation of the prediction (how actual data compares to predicted data)\nRinse and repeat: Choose a different model, evaluate, choose the best model.\n\nBest model for the Live data not the original data."
  },
  {
    "objectID": "rm-data/slides/week10.html#cs-price-cars",
    "href": "rm-data/slides/week10.html#cs-price-cars",
    "title": "Prediction Setup",
    "section": "CS: Price cars",
    "text": "CS: Price cars\n\nYou want to sell your car through online advertising\nTarget is continuous (in dollars)\nFeatures are continuous or categorical\nThe business question:\n\nWhat price should you put into the ad?"
  },
  {
    "objectID": "rm-data/slides/week10.html#cs-price-apartments",
    "href": "rm-data/slides/week10.html#cs-price-apartments",
    "title": "Prediction Setup",
    "section": "CS: Price apartments",
    "text": "CS: Price apartments\n\nYou are planning to run an AirBnB business\nTarget is continuous (in dollars)\nFeatures are varied from text to binary\nThe business question:\n\nHow should you price apartments/houses?"
  },
  {
    "objectID": "rm-data/slides/week10.html#cs-predict-companys-exit-from-business",
    "href": "rm-data/slides/week10.html#cs-predict-companys-exit-from-business",
    "title": "Prediction Setup",
    "section": "CS: Predict company’s exit from business",
    "text": "CS: Predict company’s exit from business\n\nYou have a consulting company\nPredict which firms will go out of business (exit) from a pool of partners\nTarget is binary: exit / stay\nFeatures of financial and management info\nBusiness decision:\n\nWhich firms to give loan to?"
  },
  {
    "objectID": "rm-data/slides/week10.html#predictive-analysis-what-is-new",
    "href": "rm-data/slides/week10.html#predictive-analysis-what-is-new",
    "title": "Prediction Setup",
    "section": "Predictive Analysis: what is new?",
    "text": "Predictive Analysis: what is new?\n\nMost of econometrics focused on finding relationships between \\(X\\) and \\(Y\\)\n\nWhat is the relationship like (+/-, linear, etc.)\nIs it a robust relationship – true in the population /general pattern? (causal?)\n\nNow, we use \\(x_1, x_2, \\dots\\) to predict \\(y\\): \\(\\hat{y}_j = \\hat{f}(x_j)\\), How is this different?\n\n\n\nWe care less about\n\nIndividual coefficient values, multicollinearity\n\nWe still care about the stability of our results.\nBut, should we care about causality?\n\nNot so much, we care more about making the best prediction."
  },
  {
    "objectID": "rm-data/slides/week10.html#what-are-we-predicting",
    "href": "rm-data/slides/week10.html#what-are-we-predicting",
    "title": "Prediction Setup",
    "section": "What are we predicting?",
    "text": "What are we predicting?\n\n\\(Y\\) is quantitative (e.g price)\n\nQuantitative prediction. Expected price of a car given its characteristics.\nits a “Regression” problem\nWe could obtain a point prediction or an interval prediction (As before)\n\n\\(Y\\) is binary (e.g. Default or nor)\n\n\\(Y\\) takes values in a finite set of (unordered) classes (survived/died, sold/not sold, car model)\nWe may want to types of predictions: Probability prediction (\\(\\hat P(y|x=1)\\)) or classification (\\(\\hat y = 1 \\text{ or }0| x\\))\n\nTime series prediction (Forecasting).\n\nMake predictions about the future based on historical and current data.\n\\(\\hat y_t = f(x_{t-1}, x_{t-2}, \\dots)\\)"
  },
  {
    "objectID": "rm-data/slides/week10.html#what-is-different",
    "href": "rm-data/slides/week10.html#what-is-different",
    "title": "Prediction Setup",
    "section": "What is Different?",
    "text": "What is Different?\n\nIn principle, the process is the same as before, but we have a diffent goal.\n\nWe need to pay more attention to other aspects of the process.\n\nLabel Engeneering: What transformation is suitable for the target variable?\nFeature engineering (variable selection): What variables and functional forms should be included\nModel estimation and prediction, based on variable selection\n\nDecisions regarding model complexity and Specification (Know-How or Machine Learning)\n\nModel evaluation considering complexity and loss\nKey idea: Focus on systematically combine estimation and model selection"
  },
  {
    "objectID": "rm-data/slides/week10.html#the-tool-supervised-machine-learning",
    "href": "rm-data/slides/week10.html#the-tool-supervised-machine-learning",
    "title": "Prediction Setup",
    "section": "The Tool: Supervised Machine Learning",
    "text": "The Tool: Supervised Machine Learning\n\nSupervised Machine Learning is a set of tools that help us predict the value of a target variable \\(Y\\) based on a set of features \\(X\\).\nWe will use one of the oldest and most commonly used method:\n\n\n\nLinear Regression"
  },
  {
    "objectID": "rm-data/slides/week10.html#the-prediction-error",
    "href": "rm-data/slides/week10.html#the-prediction-error",
    "title": "Prediction Setup",
    "section": "The Prediction Error",
    "text": "The Prediction Error\n\nWe have seen this.\nThe Regression model can produce a Predicted value \\(\\hat{y}_j\\) for target observation \\(j\\)\nbut, actual value \\(y_j\\) is not known (that is why we are predicting)\nThus, there will be a prediction error\n\n\\[e_j = \\hat{y}_j - y_j\n\\]\n\nError = actual value - predicted value"
  },
  {
    "objectID": "rm-data/slides/week10.html#the-prediction-error-1",
    "href": "rm-data/slides/week10.html#the-prediction-error-1",
    "title": "Prediction Setup",
    "section": "The Prediction Error",
    "text": "The Prediction Error\n\nThe ideal prediction error, is zero: our predicted value is right on target.\n\nRare…\n\nThe prediction error carries information: direction and size.\nDirection of miss:\n\nPositive if we overpredict\nNegative if we underpredict\nDegree of wrongness depends on the decision problem. (price is right?)\n\nSize:\n\nmagnitude of the error may depend on the nature of the problem and the loss function."
  },
  {
    "objectID": "rm-data/slides/week10.html#decomposing-the-prediction-error",
    "href": "rm-data/slides/week10.html#decomposing-the-prediction-error",
    "title": "Prediction Setup",
    "section": "Decomposing the prediction error",
    "text": "Decomposing the prediction error\nAssume the best model for \\(Y\\) is \\(Y= f(X,Z)+\\epsilon\\), but we estimate \\(Y^E = g(X,Z)\\), and obtain \\(\\hat g(X,Z)\\)\n\nThe prediction error can be decomposed into three parts:\n\nestimation error: Difference between \\(g(X)\\) and \\(\\hat g(X)\\)\nmodel error: Difference between \\(f(X)\\) and \\(g(X)\\).\ngenuine error: error that cant be eliminated even if have the best possible model. \\(\\epsilon\\)"
  },
  {
    "objectID": "rm-data/slides/week10.html#interval-prediction-for-quantitative-target-variables",
    "href": "rm-data/slides/week10.html#interval-prediction-for-quantitative-target-variables",
    "title": "Prediction Setup",
    "section": "Interval prediction for quantitative target variables",
    "text": "Interval prediction for quantitative target variables\n\nOne advantage of regressions - it’s easy quantify uncertainty of prediction\n\nThis can be used to obtain Interval predictions\n\nInterval predictions quantify 2-out-of-3 sources of prediction uncertainty: estimation error and genuine (or irreducible) error.\nThey do not include the third source, model uncertainty! (Bayesian methods can help with this)\nThe 95% prediction interval (PI) tells where to expect the actual value for the target observation.\nThe PI for linear regression requires homoskedasticity. (but could be relaxed)"
  },
  {
    "objectID": "rm-data/slides/week10.html#loss-functions",
    "href": "rm-data/slides/week10.html#loss-functions",
    "title": "Prediction Setup",
    "section": "Loss Functions",
    "text": "Loss Functions\n\nWe use a Loss function to quantify the cost of prediction error\n\nIt attaches a value to the prediction error, specifying how bad it is\nThus, Loss function determines best predictor\n\nIdeally, it is derived from decision problem,\n\nHow much more costly is to overpredict than underpredict?\n\nIn practice, highly crafted loss functions are rare (Machine learning, Neural Networks, Etc), so we use common ones\nLoss functions could be used to both estimate, but also to evaluate/compare models\nPlot Twist: Loss function for OLS is the L2 Square loss function"
  },
  {
    "objectID": "rm-data/slides/week10.html#loss-functions-1",
    "href": "rm-data/slides/week10.html#loss-functions-1",
    "title": "Prediction Setup",
    "section": "Loss Functions",
    "text": "Loss Functions\n\nThe most important Loss functions have the following characteristics:\n\nSymmetry: losses due to errors in opposing direction are similar\n\nAsymmetric loss: overprediction is more costly than underprediction\n\nConvexity: Twice as large errors generate more than twice as large losses. (We penalize large errors more than small ones)\n\nLinear loss: Errors are penalized proportionally to their size"
  },
  {
    "objectID": "rm-data/slides/week10.html#loss-functions-of-various-shapes",
    "href": "rm-data/slides/week10.html#loss-functions-of-various-shapes",
    "title": "Prediction Setup",
    "section": "Loss Functions of Various Shapes",
    "text": "Loss Functions of Various Shapes\n\n\nCode\nqui {\nclear\nset scheme white2\ncolor_style tableau\nset obs 301\nrange r -5 5 \ngen ll = r^2\ngen ll2 = 2*abs(r)\ngen ll3 = 1.5*r^2*(r&gt;0)+0.5*r^2*(r&lt;0)\ndrop if ll&gt;30 | ll2&gt;30 | ll3&gt;30\nline ll ll2 ll3 r, lw(1 1 1) ///\n    legend(order(1 \"Symetric-Convex\" 2 \"Symetric-Linear\" 3 \"Asymetric-Convex\") )\n}"
  },
  {
    "objectID": "rm-data/slides/week10.html#examples-1-used-cars",
    "href": "rm-data/slides/week10.html#examples-1-used-cars",
    "title": "Prediction Setup",
    "section": "Examples 1 – used cars",
    "text": "Examples 1 – used cars\n\nThe loss function for predicting the value of our used car depends on how we value money and how we value how much time it takes to sell our car (value of your car).\nA too low prediction may lead to selling our car cheap but fast;\nA too high prediction may make us wait a long time and, possibly, revising the sales price downwards before selling our car.\nWhat kind of loss function would make sense?"
  },
  {
    "objectID": "rm-data/slides/week10.html#examples-2---creditors",
    "href": "rm-data/slides/week10.html#examples-2---creditors",
    "title": "Prediction Setup",
    "section": "Examples 2 - creditors",
    "text": "Examples 2 - creditors\n\nCreditors decide whether to issue a loan only to potential debtors that are predicted to pay it back with high likelihood.\nTwo kinds of errors are possible:\n\ndebtors that would pay back their loan don’t get a loan\ndebtors that would not pay back their loan get one nevertheless.\n\nThe costs of the first error are due to missed business opportunity; the costs of the second error are due to direct loss of money.\nThese losses may be quantified in relatively straightforward ways.\nWhat kind of loss function would make sense?"
  },
  {
    "objectID": "rm-data/slides/week10.html#common-loss-functions",
    "href": "rm-data/slides/week10.html#common-loss-functions",
    "title": "Prediction Setup",
    "section": "Common loss functions",
    "text": "Common loss functions\n\n\n\\[SQR: L(e_j) = e^2_j = (\\hat{y}_j - y_j)^2\n\\]\n\nThe most widely used loss function\nSymmetric: Losses due to errors in opposing direction are same\nConvex: Twice as large errors generate more than twice (4x) as large losses\n\n\n\\[ABS: L(e_j) = |e_j| = Abs(\\hat{y}_j - y_j)\n\\]\n\nUsed for Median regression (Quantile regression)\nSymmetric: Losses due to errors in opposing direction are same\nLinear: Twice as large errors generate twice as large losses\nQuantile Regressions use Asymetric loss functions"
  },
  {
    "objectID": "rm-data/slides/week10.html#mean-squared-error-mse",
    "href": "rm-data/slides/week10.html#mean-squared-error-mse",
    "title": "Prediction Setup",
    "section": "Mean Squared Error: MSE",
    "text": "Mean Squared Error: MSE\n\nThe most common way to quantify and aggregate the loss function is using the Mean Squared Error (MSE)\nSquared loss \\(\\rightarrow\\) Mean Squared Error (MSE)\n\n\\[\\begin{align*}\n\\text{MSE} &= \\frac{1}{K} \\sum_{k=1}^K (\\hat{y}_k - y_k)^2 \\\\\n\\text{RMSE} &= \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{K} \\sum_{k=1}^K (\\hat{y}_k - y_k)^2}\n\\end{align*}\n\\]\n\nUsing this function typically implies we are interested in the Mean as the best predictor"
  },
  {
    "objectID": "rm-data/slides/week10.html#mse-decomposition-bias-and-variance",
    "href": "rm-data/slides/week10.html#mse-decomposition-bias-and-variance",
    "title": "Prediction Setup",
    "section": "MSE decomposition : Bias and Variance",
    "text": "MSE decomposition : Bias and Variance\n\nThe MSE can be decomposed into two parts: Bias and Variance \\[\\begin{align*}\nMSE &= \\frac{1}{J}\\sum_{j=1}^J (\\hat{y}_j - y_j)^2 \\\\\n&= \\left(\\frac{1}{J}\\sum_{j=1}^J (\\hat{y}_j - y_j)\\right)^2 + \\frac{1}{J}\\sum_{j=1}^J (\\hat y_j - \\bar{\\hat y})^2 \\\\\n&= \\text{Bias}^2 + \\text{PredictionVariance}\n\\end{align*}\n\\]\nThe bias of a prediction is the average of its prediction error.\n\nHow far off is the average prediction from the actual value?\n\nThe variance of a prediction describes shows how it varies around its average."
  },
  {
    "objectID": "rm-data/slides/week10.html#mse-decomposition-bias-and-variance-1",
    "href": "rm-data/slides/week10.html#mse-decomposition-bias-and-variance-1",
    "title": "Prediction Setup",
    "section": "MSE decomposition : Bias and Variance",
    "text": "MSE decomposition : Bias and Variance\n\\[\\begin{align*}\n\\text{MSE} &= \\frac{1}{K} \\sum_{k=1}^K (\\hat{y}_k - y_k)^2 \\\\\n         &= \\left(\\frac{1}{K} \\sum_{k=1}^K (\\hat{y}_k - \\bar{y})\\right)^2 + \\frac{1}{K} \\sum_{k=1}^K (y_k - \\bar{y})^2 \\\\\n         &= \\text{Bias}^2 + \\text{PredictionVariance}\n\\end{align*}\\]\n\nOLS is unbiased. Some other methods will allow for some bias in return for lower variance."
  },
  {
    "objectID": "rm-data/slides/week10.html#model-selection",
    "href": "rm-data/slides/week10.html#model-selection",
    "title": "Prediction Setup",
    "section": "Model selection",
    "text": "Model selection\n\nModel selection is finding the best fit while avoiding overfitting, and aiming for high external validity.\nTo do this, we aim to choose a model that is flexible enough to capture the patterns in the data but not too flexible to capture noise.\n\nBias-Variance tradeoff\nBalancing the complexity of the model\n\nConsider two models. They could be different for two reasons:\n\ndifferent functional forms (spline vs. quadratic)\ndifferent number of variables (simple vs. complex)"
  },
  {
    "objectID": "rm-data/slides/week10.html#how-to-choose-a-model",
    "href": "rm-data/slides/week10.html#how-to-choose-a-model",
    "title": "Prediction Setup",
    "section": "How to choose a model?",
    "text": "How to choose a model?\n\nTypically, we would say that the best model is the one that has the highest \\(R^2\\) or the lowest MSE.\n\nThis may be true for the original data, but not for the target observations.\nAlso, \\(R^2\\) and MSE always increase when we add more variables.\n\nSo, what we need is a way to check how well the models predict the target observations. (unobserved cases)\nWe want to avoid overfitting at all costs."
  },
  {
    "objectID": "rm-data/slides/week10.html#example",
    "href": "rm-data/slides/week10.html#example",
    "title": "Prediction Setup",
    "section": "Example",
    "text": "Example\n\nAssume the true model is \\(y = 1 + x -.5 x^2 + \\epsilon\\), with \\(\\epsilon ~ N(0,1.5)\\)\nwe create 100 observations from the above process but use only 30 for modeling. We try running regressions with ever more complex models\nMake predictions, and see how well we did.\n\n\n\n\n\n\\(R^2\\)\nMSE\nOoS MSE\n\n\n\n\n^1\n.3224669\n1.934762\n2.949084\n\n\n^2\n.6255072\n1.069401\n2.93478\n\n\n^3\n.6344791\n1.043781\n2.978827\n\n\n^4\n.713217\n.8189369\n4.138006\n\n\n^5\n.7259535\n.7825666\n3.557651\n\n\n\nSee how \\(R^2\\) increases monotonously, improving “in-sample fit”. But, the OoS MSE worsens after a certain point."
  },
  {
    "objectID": "rm-data/slides/week10.html#example-2",
    "href": "rm-data/slides/week10.html#example-2",
    "title": "Prediction Setup",
    "section": "Example 2",
    "text": "Example 2"
  },
  {
    "objectID": "rm-data/slides/week10.html#underfit-vs-overfit",
    "href": "rm-data/slides/week10.html#underfit-vs-overfit",
    "title": "Prediction Setup",
    "section": "Underfit vs overfit",
    "text": "Underfit vs overfit\n\nA model that fits the data worse in the “working/original” data compared to the “live/target” data is said to “underfit” the model.\n\nSimple: we should build a better model.\n\nIf the model fits the data working better than target data, then it over-fits it. Needs to be corrected.\n\nOverfitting\n\nOverfitting is a key aspect of external validity\n\nFinding a model that fits the data better than alternative models, but makes worse actual prediction.\n\nOverfitting is a common problem in prediction analysis"
  },
  {
    "objectID": "rm-data/slides/week10.html#reason-for-overfitting",
    "href": "rm-data/slides/week10.html#reason-for-overfitting",
    "title": "Prediction Setup",
    "section": "Reason for overfitting",
    "text": "Reason for overfitting\nThe typical reason for overfitting is fitting a model that is too complex on the dataset. - Complexity: number of estimated coefficients - Often: fitting a model with too many predictor variables. - Including too many variables from the dataset that do not really add to the predictive power of the regression. - Problems of multicollinearity, too many interactions, etc. - Too detailed nonlinear patterns - as piecewise linear splines with many knots - polynomials of high degree."
  },
  {
    "objectID": "rm-data/slides/week10.html#finding-the-best-model-by-best-fit-and-penalty",
    "href": "rm-data/slides/week10.html#finding-the-best-model-by-best-fit-and-penalty",
    "title": "Prediction Setup",
    "section": "Finding the best model by best fit and penalty",
    "text": "Finding the best model by best fit and penalty\n\nAs shown earlier, traditional measures of fit, such as \\(R^2\\) and MSE, are not good for finding the best model. They always increase with the number of variables.\nWe were only able to conclude model fitness because we had the true model (and the Out-of-Sample data)\n\nThis is not the case in practice.\n\nSo, how do we find the best model?\n\nIndirectly: Penalize the number of variables\nDirectly: Use a training-test sample"
  },
  {
    "objectID": "rm-data/slides/week10.html#indirect-evaluation-criteria",
    "href": "rm-data/slides/week10.html#indirect-evaluation-criteria",
    "title": "Prediction Setup",
    "section": "Indirect evaluation criteria",
    "text": "Indirect evaluation criteria\n\nMain methods: AIC, BIC and adjusted \\(R^2\\)\n\nAdvantage: easy to compute\nDisadvantage: assumptions. They may not penalize enough.\n\nAdjusted \\(R^2\\) – just add a penalty for having many RHS vars\n\ncorrects with \\((n - 1)/(n - k - 1)\\)\n\nAIC = \\(-2 \\times \\ln(\\text{likelihood}) + 2 \\times k\\)\nBIC = \\(-2 \\times \\ln(\\text{likelihood}) + \\ln(N) \\times k\\)\n\nBoth are based on information loss theory\nBIC puts heavier penalty on models with many RHS variables, than AIC."
  },
  {
    "objectID": "rm-data/slides/week10.html#example-redone",
    "href": "rm-data/slides/week10.html#example-redone",
    "title": "Prediction Setup",
    "section": "Example, redone",
    "text": "Example, redone\n\n\nCode\n* This is the full code for previous and current example\nclear\nset seed 10\nset obs 100\ngen x = runiform(-2 ,2)\ngen y = 1 + x - 0.5 * x^2 + 1.5* rnormal()\n \ncapture program drop fit_stat1\nprogram fit_stat1, rclass\n    tempvar yhat smp\n    gen `smp' = e(sample)\n    predict `yhat'\n    replace `yhat'=(y-`yhat')^2\n    \n    matrix result=e(r2)\n    sum `yhat' if `smp', meanonly\n    matrix result=result, r(mean)\n    sum `yhat' if !`smp', meanonly\n    matrix result=result, r(mean)\n    return matrix result = result\nend\n\nreg y c.x if _n&lt;=30\nfit_stat1\nmatrix rr2=r(result)\nreg y c.x##c.x if _n&lt;=30\nfit_stat1\nmatrix rr2=rr2\\r(result)\nreg y c.x##c.x##c.x if _n&lt;=30\nfit_stat1\nmatrix rr2=rr2\\r(result)\nreg y c.x##c.x##c.x##c.x if _n&lt;=30\nfit_stat1\nmatrix rr2=rr2\\r(result)\nreg y c.x##c.x##c.x##c.x##c.x if _n&lt;=30\nfit_stat1\nmatrix rr2=rr2\\r(result)\n\ncapture program drop fit_stat2\nprogram fit_stat2, rclass\n    tempvar yhat smp\n    gen `smp' = e(sample)\n    predict `yhat'\n    replace `yhat'=(y-`yhat')^2    \n    matrix result=e(r2), e(r2_a)\n    sum `yhat' if `smp', meanonly\n    matrix result=result, r(mean)\n    sum `yhat' if !`smp', meanonly\n    matrix result=result, r(mean)\n    qui: estat ic\n    matrix result=result, r(S)[1,5..6]\n    return matrix result = result\nend\n\nreg y c.x if _n&lt;=30\nfit_stat2\nmatrix rr2=r(result)\nreg y c.x##c.x if _n&lt;=30\nfit_stat2\nmatrix rr2=rr2\\r(result)\nreg y c.x##c.x##c.x if _n&lt;=30\nfit_stat2\nmatrix rr2=rr2\\r(result)\nreg y c.x##c.x##c.x##c.x if _n&lt;=30\nfit_stat2\nmatrix rr2=rr2\\r(result)\nreg y c.x##c.x##c.x##c.x##c.x if _n&lt;=30\nfit_stat2\nmatrix rr2=rr2\\r(result)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\)\n\\(aR^2\\)\nMSE\nOoS MSE\nAIC\nBIC\n\n\n\n\n^1\n0.3225\n0.2983\n1.9348\n2.9491\n108.9358\n111.7382\n\n\n^2\n0.6255\n0.5978\n1.0694\n2.9348\n93.1493\n97.3529\n\n\n^3\n0.6345\n0.5923\n1.0438\n2.9788\n94.4218\n100.0266\n\n\n^4\n0.7132\n0.6673\n0.8189\n4.1380\n89.1439\n96.1499\n\n\n^5\n0.7260\n0.6689\n0.7826\n3.5577\n89.7810\n98.1882"
  },
  {
    "objectID": "rm-data/slides/week10.html#finding-the-best-model-by-training-and-test-samples",
    "href": "rm-data/slides/week10.html#finding-the-best-model-by-training-and-test-samples",
    "title": "Prediction Setup",
    "section": "Finding the best model by training and test samples",
    "text": "Finding the best model by training and test samples\n\nSimilar to bootstrapping (brute force approach for SE), its also possible to use a “bute-force” approach to find the best model.\nThis would require “imitating” the process of out-of-sample prediction.\n\nCut the dataset into training and test sample (80-20 ?)\nChoose Some evaluation criterion (loss function)\nEstimate the model on the training sample\nPredict and evaluate the model on the test sample\n\nProblem: 80% (or less), may be too small training. And 20% (one shoot) could be different from the rest of the data."
  },
  {
    "objectID": "rm-data/slides/week10.html#lets-make-things-better-k-fold-cross-validation",
    "href": "rm-data/slides/week10.html#lets-make-things-better-k-fold-cross-validation",
    "title": "Prediction Setup",
    "section": "Lets make things Better: K-fold cross-validation",
    "text": "Lets make things Better: K-fold cross-validation\n\nIf one is not enough, why not use more?\nSplit sample into \\(k=5\\) groups (equal size)\nNow, assume that each “fold” is the test sample, and the rest is the training sample.\n\nDo the excercise k times, and every observation will be in the test sample once.\n\nAdd up the MSEs, or get the average MSE.\nStill has a random component, but less so than a single split."
  },
  {
    "objectID": "rm-data/slides/week10.html#k-fold-cross-validation",
    "href": "rm-data/slides/week10.html#k-fold-cross-validation",
    "title": "Prediction Setup",
    "section": "K-fold cross-validation",
    "text": "K-fold cross-validation\n\nCode\n* This is the full code for previous and current example\nclear\nset seed 10\nqui: set obs 30\ngen x = runiform(-2 ,2)\ngen y = 1 + x - 0.5 * x^2 + 1.5* rnormal()\n\n** Create 5 folds\ngen fold = mod(_n,5)+1\n\ncapture program drop fit_stat3\nprogram fit_stat3, rclass\n    tempvar yhat aux\n    qui:gen `yhat' = .\n    qui:forvalues i = 1/5 {\n        reg `0' if fold!=`i'\n        predict `aux'\n        replace `yhat' = `aux' if fold==`i'\n        drop `aux'\n    }\n    qui: replace `yhat'=(y-`yhat')^2    \n    sum `yhat', meanonly\n    return scalar mse = r(mean)\nend\n\nfit_stat3 y c.x \ndisplay \"^1, mse: \" %5.3f `r(mse)' _n\nfit_stat3 y c.x##c.x \ndisplay \"^2, mse: \" %5.3f `r(mse)' _n\nfit_stat3 y c.x##c.x##c.x \ndisplay \"^3, mse: \" %5.3f `r(mse)' _n\nfit_stat3 y c.x##c.x##c.x##c.x \ndisplay \"^4, mse: \" %5.3f `r(mse)' _n\nfit_stat3 y c.x##c.x##c.x##c.x##c.x \ndisplay \"^5, mse: \" %5.3f `r(mse)' _n\n\n^1, mse: 4.851\n^2, mse: 3.196\n^3, mse: 2.805\n^4, mse: 3.085\n^5, mse: 3.160"
  },
  {
    "objectID": "rm-data/slides/week10.html#when-k-rightarrow-n.-loocv",
    "href": "rm-data/slides/week10.html#when-k-rightarrow-n.-loocv",
    "title": "Prediction Setup",
    "section": "When K \\(\\rightarrow\\) N. LooCV",
    "text": "When K \\(\\rightarrow\\) N. LooCV\n\nK-fold cross-validation has two weaknesses:\n\nRandomness: different splits may lead to different results\nSmall sample size for training may be too small\n\nAn alternative to address both is increasing K … until K=N\n\nThis is the Leave-one-out cross-validation (LOOCV)\nTrain on N-1 observations, predict on the left-out (N=1) observation\n\nThis can be computationally expensive, unless you are estimating Linear regression models.\n\nLOOCV its really fast for OLS"
  },
  {
    "objectID": "rm-data/slides/week10.html#bic-vs-test-rmse",
    "href": "rm-data/slides/week10.html#bic-vs-test-rmse",
    "title": "Prediction Setup",
    "section": "BIC vs test RMSE",
    "text": "BIC vs test RMSE\n\nIn practice, BIC is the best indirect criterion – closest to test sample.\nThe advantage of BIC is that it needs no sample splitting which may be a problem in small samples.\nThe advantage of test MSE is that it makes no assumption.\nBIC is a good first run, quick, is often not very wrong.\nTest MSE is the best, but may be computationally expensive."
  },
  {
    "objectID": "rm-data/slides/week10.html#external-validity-and-stable-patterns",
    "href": "rm-data/slides/week10.html#external-validity-and-stable-patterns",
    "title": "Prediction Setup",
    "section": "External validity and stable patterns",
    "text": "External validity and stable patterns\n\nBIC, Training-test, k-fold cross-validation… All very nice\n\nIn the end, they all use the information in the data.\n\nHow would things look for the target observation(s)? unknown!!\nThe issue of stationarity – how our data is related to other datasets we may use our model\nIn the end we can’t know but need to think about it.\n\nPlus, if there is no external validity, your model fit in an outside data source is likely to be worse…"
  },
  {
    "objectID": "rm-data/slides/week10.html#external-validity-and-stable-patterns-1",
    "href": "rm-data/slides/week10.html#external-validity-and-stable-patterns-1",
    "title": "Prediction Setup",
    "section": "External validity and stable patterns",
    "text": "External validity and stable patterns\n\nMost predictions will be on future data\nHigh external validity requires that the environment is stationary.\nStationarity means that the way variables are distributed remains the same over time.\n\nEnsures that the relationship between predictors and the target variable is the same in the data and the forecasted future.\n\nIf the relationship breaks down whatever we establish in our data won’t be true in the future, leading to wrong forecasts."
  },
  {
    "objectID": "rm-data/slides/week10.html#external-validity-and-stable-patterns-2",
    "href": "rm-data/slides/week10.html#external-validity-and-stable-patterns-2",
    "title": "Prediction Setup",
    "section": "External validity and stable patterns",
    "text": "External validity and stable patterns\n\nExternal validity and stable patterns - Very broad concept\nIt’s about representativeness of actual data \\(\\rightarrow\\) to live data\n\nRemember hotels? (other dates, other cities).\n\nDomain knowledge can help. Inner knowledge of the process can help.\nStudy if patterns were stable in the past / other locations were stable can help."
  },
  {
    "objectID": "rm-data/slides/week10.html#main-takeaways",
    "href": "rm-data/slides/week10.html#main-takeaways",
    "title": "Prediction Setup",
    "section": "Main takeaways",
    "text": "Main takeaways\n\nPrediction uses the original data with \\(y\\) and \\(x\\) to predict the value of \\(y\\) for observations in the live data, in which \\(x\\) is observed but \\(y\\) is not\nPrediction uses a model that describes the patterns of association between \\(y\\) and \\(x\\) in the original data\nCross-validation can help find the best model in the population, or general pattern, represented by the original data\nStability of the patterns of association is needed for a prediction with high external validity"
  },
  {
    "objectID": "rm-data/slides/week10.html#cs-predicting-the-price-of-used-cars",
    "href": "rm-data/slides/week10.html#cs-predicting-the-price-of-used-cars",
    "title": "Prediction Setup",
    "section": "CS: Predicting the price of used cars",
    "text": "CS: Predicting the price of used cars\n\nSUse data from here for replication.\nDrop car with price&gt;20k or those price = 1$.\nWhile distribution is skewed, Log(price) is not normal either. We will keep it as is.\nTwo areas, we keep Chicago only.\ndrop if odometer&gt;100K miles\nVariables: price, age, odometer, type, condition, cylinders, dealer"
  },
  {
    "objectID": "rm-data/slides/week10.html#cs-competing-models",
    "href": "rm-data/slides/week10.html#cs-competing-models",
    "title": "Prediction Setup",
    "section": "CS: Competing models:",
    "text": "CS: Competing models:\n\nModel 1 age, age squared\nModel 2 age, age squared, odometer, odometer squared\nModel 3 age, age squared, odometer, odometer squared, LE, condition , dealer\nModel 4 age, age squared, odometer, odometer squared, LE, condition , dealer, SE, XLE, cylinder\nModel 5 same as Model 4 but with all variables interacted with age\nModel 6 same as Model 4 but with all variables interacted with condition\n\n\n\nCode\n* setup\nuse data_slides/used-cars.dta, clear\nglobal model1 age c.age#c.age\nglobal model2 $model1 lnodometer c.lnodometer#c.lnodometer\nglobal model3 $model2 LE i.condition_recode i.dealer\nglobal model4 $model3 SE XLE cylinder\nglobal model5 c.($model4)##c.age\nglobal model6 c.($model4)##i.condition_recode"
  },
  {
    "objectID": "rm-data/slides/week10.html#cs-test-data",
    "href": "rm-data/slides/week10.html#cs-test-data",
    "title": "Prediction Setup",
    "section": "CS: Test Data",
    "text": "CS: Test Data\n\nAssume that 20% of the data is “target” data.\nLets run all models, using different evaluation criteria. And see which one is the best.\nSee below for the program..Rather long\n\n\n\nCode\ngen rnd = runiform()\nsort rnd\ngen test = _n/_N&lt;=.2\n\ncapture program drop fit_stat4\nprogram fit_stat4, rclass\n    reg `0' if test==0\n    tempvar yxhat\n    predict `yxhat' if test==1\n    replace `yxhat' = (price-`yxhat')^2  if test==1\n    matrix result = e(r2), e(r2_a)\n    estat ic\n    matrix result = result, r(S)[1,5..6]\n    ** K-fold cross-validation\n    capture drop fold\n    gen fold = runiform() if test==0\n    sort fold \n    drop fold\n    gen fold = mod(_n,5)+1 if test==0\n    \n    *** Kfold1\n    tempvar yhat aux\n    qui:gen `yhat' = .\n    qui:forvalues i = 1/5 {\n        reg `0' if fold!=`i' & test==0\n        predict `aux'\n        replace `yhat' = `aux' if fold==`i' & test==0\n        drop `aux'\n    }\n    qui: replace `yhat'=(price-`yhat')^2     \n    sum `yhat' if test==0, meanonly\n    matrix result = result, sqrt(r(mean))\n    *** Kfold2\n    capture drop fold\n    gen fold = runiform() if test==0\n    sort fold \n    drop fold\n    gen fold = mod(_n,5)+1 if test==0\n    \n    drop `yhat'  \n    qui:gen `yhat' = .\n    qui:forvalues i = 1/5 {\n        reg `0' if fold!=`i' & test==0\n        predict `aux'\n        replace `yhat' = `aux' if fold==`i' & test==0\n        drop `aux'\n    }\n    qui: replace `yhat'=(price-`yhat')^2    \n    sum `yhat' if test==0, meanonly\n    matrix result = result, sqrt(r(mean))\n    ** test on live data\n    sum `yxhat' if test==1, meanonly\n    matrix result = result, sqrt(r(mean))\n    matrix colname result = r2 r2_a AIC BIC Kfold1 Kfold2 test\n    ** \n    return matrix result = result\nend"
  },
  {
    "objectID": "rm-data/slides/week10.html#cs-results",
    "href": "rm-data/slides/week10.html#cs-results",
    "title": "Prediction Setup",
    "section": "CS: Results",
    "text": "CS: Results\n\nCode\nqui: fit_stat4 price $model1\nmatrix fresults = r(result)\nqui: fit_stat4 price $model2\nmatrix fresults = fresults\\r(result)\nqui: fit_stat4 price $model3\nmatrix fresults = fresults\\r(result)\nqui: fit_stat4 price $model4\nmatrix fresults = fresults\\r(result)\nqui: fit_stat4 price $model5\nmatrix fresults = fresults\\r(result)\nqui: fit_stat4 price $model6\nmatrix fresults = fresults\\r(result)\nmatrix rowname fresults = Model1 Model2 Model3 Model4 Model5 Model6\nset linesize 250\nesttab matrix(fresults, fmt(%8.3f)) , md  nomtitle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr2\nr2_a\nAIC\nBIC\nKfold1\nKfold2\ntest\n\n\n\n\nModel1\n0.711\n0.708\n4823.338\n4834.032\n2472.245\n2470.654\n1697.617\n\n\nModel2\n0.776\n0.772\n4760.551\n4778.374\n2185.247\n2204.960\n1345.265\n\n\nModel3\n0.779\n0.771\n4766.944\n4802.590\n2270.672\n2307.392\n1346.395\n\n\nModel4\n0.785\n0.775\n4765.275\n4811.613\n2388.426\n2259.348\n1381.180\n\n\nModel5\n0.797\n0.778\n4772.246\n4857.794\n2605.005\n2425.626\n1481.571\n\n\nModel6\n0.851\n0.824\n4724.611\n4867.191\n2555.466\n2670.216\n1523.569"
  },
  {
    "objectID": "rm-data/slides/week12.html#motivation",
    "href": "rm-data/slides/week12.html#motivation",
    "title": "Probability and Classification",
    "section": "Motivation",
    "text": "Motivation\n\nSay you work for a consultancy agency helping a bank. You are asked to predict which firms will default on their loans.\n\nWhat would be better: a model that predicts probabilities or a model that classifies firms into default or not default?\nHow do you use to decide which firms should get a loan.\n\nCompanies need to assess the likelihood of their suppliers or clients staying in business, as it impacts their own operations.\n\nHow to use historical data on company exits, along with key features, to predict the probability of a company’s exit."
  },
  {
    "objectID": "rm-data/slides/week12.html#previously-on-da",
    "href": "rm-data/slides/week12.html#previously-on-da",
    "title": "Probability and Classification",
    "section": "Previously on DA",
    "text": "Previously on DA\n\nIn the previous weeks, we have covered the basics of prediction when the target is quantitative.\n\nAlmost stright forward: predict the value of the target. Consider many specifications and pick the best one.\n\nWe also cover the basic of probability modeling\n\nLPM, Logit and probit models: When your dependent variable is Binary.\n\nHowever, we have not fully cover how to use these models for prediction."
  },
  {
    "objectID": "rm-data/slides/week12.html#prediction-with-qualitative-target",
    "href": "rm-data/slides/week12.html#prediction-with-qualitative-target",
    "title": "Probability and Classification",
    "section": "Prediction with qualitative target",
    "text": "Prediction with qualitative target\n\nConsider cases where \\(Y\\) is qualitative\n\nWhether a debtor defaults (will default) on their loan\nEmail is spam or not\nGame result is win / lose (no draw).\n\nFor all this cases, the target (dep variable) is binary.\nThe question is: Given this, What is the best way to predict the target?\n\nPredict the probability of “success” (default, spam, win)\nor make a classification (default, spam, win) based on a probability."
  },
  {
    "objectID": "rm-data/slides/week12.html#the-process",
    "href": "rm-data/slides/week12.html#the-process",
    "title": "Probability and Classification",
    "section": "The process",
    "text": "The process\n\nPredict probability: We have done this.\n\nPredicted probability between 0 and 1 (logit, probit or LPM in extreme cases)\nFor each observation we predicted a probability. Often that is it.\n\n\nif logit: \\[\\Pr[y_i = 1|x_i] = \\Lambda \\times (\\beta_0 + \\beta_1x_i) = \\frac{\\exp (\\beta_0 + \\beta_1x_i)}{1 + \\exp (\\beta_0 + \\beta_1x_i)}\\]\n\nThats it! You can probably go couple of steps further and use various specifications, as well as LASSO (for logit) to pick the best model.\nBest model can still be picked based on RMSE, brier score or Calibration."
  },
  {
    "objectID": "rm-data/slides/week12.html#refresher-probability-models",
    "href": "rm-data/slides/week12.html#refresher-probability-models",
    "title": "Probability and Classification",
    "section": "Refresher: Probability Models",
    "text": "Refresher: Probability Models\n\nLPM - not this time: Predicted value MUST be between 0 and 1\nLogit or probit (or other non-linear probability models)\nNonlinear probability models \\[\\Pr[y_i = 1|x_i] = \\Lambda(\\beta_0 + \\beta_1x_i) = \\frac{\\exp (\\beta_0 + \\beta_1x_i)}{1 + \\exp (\\beta_0 + \\beta_1x_i)}\\]\n\nPredicted probability between 0 and 1\nStarts with a linear combination of the explanatory variables\nMultiplies them with coefficients, just like linear regression\nAnd then transforms that into something that is always between 0 and 1, the predicted probability."
  },
  {
    "objectID": "rm-data/slides/week12.html#whats-new-with-binary-target",
    "href": "rm-data/slides/week12.html#whats-new-with-binary-target",
    "title": "Probability and Classification",
    "section": "What’s New with Binary target?",
    "text": "What’s New with Binary target?\n\nThe predicted Probability is not a value.\nDesire to classify\n\nassign 0 or 1\nbased on a probability that comes from a model\nBut how?\n\nWe also need new measures of fit\n\nSome based on probabilities\nOthers based on classification"
  },
  {
    "objectID": "rm-data/slides/week12.html#whats-not-new-with-binary-target",
    "href": "rm-data/slides/week12.html#whats-not-new-with-binary-target",
    "title": "Probability and Classification",
    "section": "What’s NOT new with Binary target?",
    "text": "What’s NOT new with Binary target?\n\nNeed best fit\n\nWith highest external validity\n\nUsual worries: overfit\n\nCross-validation helps avoid worst overfit\n\nModels similar to those used earlier\n\nRegression-like models (probability models)\nTree-based models (CART, Random Forest) &lt;- We will not cover this"
  },
  {
    "objectID": "rm-data/slides/week12.html#probability-prediction-and-process",
    "href": "rm-data/slides/week12.html#probability-prediction-and-process",
    "title": "Probability and Classification",
    "section": "Probability prediction and process",
    "text": "Probability prediction and process\n\nWe build models to predict probability when:\n\naim is to predict probabilities – (Duh!)\naim is to classify (predict 0 or 1) – (we need probabilities first)\n\nBuild models\n\nseveral Logit models by domain knowledge\nLASSO - Logit LASSO\n\nPick the best model via cross-validation using RMSE / Brier score\n\nOr other LOSS function, if you have one"
  },
  {
    "objectID": "rm-data/slides/week12.html#classification-process",
    "href": "rm-data/slides/week12.html#classification-process",
    "title": "Probability and Classification",
    "section": "Classification process",
    "text": "Classification process\n\nAfter you predict your conditional probability, you can make classifications based on some threshold or Rule.\n\nFor example if \\(\\Pr[y = 1] &gt; 0.5\\) then predict 1, otherwise predict 0\nBut we can choose any threshold. but how? (say top 10%?)\n\nWe need to consider that we can make errors\n\nFalse negative\nFalse positive\n\nThus we need to consider a threshold that minimizes the expected errors"
  },
  {
    "objectID": "rm-data/slides/week12.html#classification-table-confusion-matrix",
    "href": "rm-data/slides/week12.html#classification-table-confusion-matrix",
    "title": "Probability and Classification",
    "section": "Classification Table: Confusion Matrix",
    "text": "Classification Table: Confusion Matrix\n\n\n\n\n\n\n\n\n\n\n\\(y_j = 0\\)\n\\(y_j = 1\\)\nTotal\n\n\n\n\n\\(\\hat{y}_j = 0\\)\nTN\nFN\nTN + FN\n\n\nPredicted negative\n(true negative)\n(false negative)\n(all classified negative)\n\n\n\\(\\hat{y}_j = 1\\)\nFP\nTP\nFP + TP\n\n\nPredicted positive\n(false positive)\n(true positive)\n(all classified positive)\n\n\nTotal\nTN + FP\nFN + TP\nTN + FN + FP + TP\n\n\n(all actual negative)\n(all actual positive)\n(N, all observations)"
  },
  {
    "objectID": "rm-data/slides/week12.html#classification-table-making-errors",
    "href": "rm-data/slides/week12.html#classification-table-making-errors",
    "title": "Probability and Classification",
    "section": "Classification Table: making errors",
    "text": "Classification Table: making errors\n\n\n\n\n\n\n\n\n\n\n\\(y_j = 0\\)\n\\(y_j = 1\\)\nTotal\n\n\n\n\n\\(\\hat{y}_j = 0\\)\nPredict firm stay\nPredict firm stay\nTN + FN\n\n\nPredicted negative\n(Firm did stay )\n(Firm exited )\n(all classified stay )\n\n\n\\(\\hat{y}_j = 1\\)\nPredict firm exit\nPredict firm exit\nFP + TP\n\n\nPredicted positive\n(Firm stayed )\n(Firm did exit)\n(all classified exit)\n\n\nTotal\nTN + FP\nFN + TP\nTN + FN + FP + TP\n\n\n(all actual stay )\n(all actual exit)\n(N, all observations)"
  },
  {
    "objectID": "rm-data/slides/week12.html#measures-of-classification",
    "href": "rm-data/slides/week12.html#measures-of-classification",
    "title": "Probability and Classification",
    "section": "Measures of classification",
    "text": "Measures of classification\nThere are several measures of classification, each with a different focus.\n\nAccuracy \\(=(TP+TN)/N\\)\n\nThe proportion of rightly guessed observations\nHit rate\n\nSensitivity \\(=TP / (TP+FN)\\)\n\nThe proportion of true positives among all actual positives\nProbability of predicted \\(y\\) is 1 conditional on \\(y = 1\\)\n\nSpecificity \\(= TN/(TN+FP)\\)\n\nThe proportion of true negatives among all actual negatives\nProbability predicted \\(y\\) is 0 conditional on \\(y = 0\\)"
  },
  {
    "objectID": "rm-data/slides/week12.html#measures-of-classification-1",
    "href": "rm-data/slides/week12.html#measures-of-classification-1",
    "title": "Probability and Classification",
    "section": "Measures of classification",
    "text": "Measures of classification\n\nThe key point is that there is a trade-off between making false positive and false negative errors.\nThis is the essential insight in classification\nThis can be expressed with specificity and sensitivity."
  },
  {
    "objectID": "rm-data/slides/week12.html#roc-curve",
    "href": "rm-data/slides/week12.html#roc-curve",
    "title": "Probability and Classification",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nThe ROC curve is a popular graphic for simultaneously displaying specificity and sensitivity for all possible thresholds.\nROC: Receiver operating characteristic curve\n\nName from engineering\n\nFor each threshold, we can compute confusion table \\(\\rightarrow\\) calculate sensitivity and specificity\nThen, we can plot sensitivity vs 1-specificity for all thresholds\n\nHorizontal axis: False positive rate (one minus specificity) = the proportion of FP among actual negatives\nVertical axis: is true positive rate (sensitivity) = proportion of TP among actual positives"
  },
  {
    "objectID": "rm-data/slides/week12.html#roc-curve-intuition",
    "href": "rm-data/slides/week12.html#roc-curve-intuition",
    "title": "Probability and Classification",
    "section": "ROC Curve Intuition",
    "text": "ROC Curve Intuition\n\nConsider this:\n\nIf the threshold is 0, we predict all observations as 1. The sensitivity is 1, but the specificity is 0.\nIf the threshold is 1, we predict all observations as 0. The sensitivity is 0, but the specificity is 1.\nThe “ideal” threshold is somewhere in between.\n\nROC curve shows how true positives and false positives increases relative to each other."
  },
  {
    "objectID": "rm-data/slides/week12.html#roc-curve-intuition-1",
    "href": "rm-data/slides/week12.html#roc-curve-intuition-1",
    "title": "Probability and Classification",
    "section": "ROC Curve Intuition",
    "text": "ROC Curve Intuition\n\nRandom CovariateNof ChildrenFull Model"
  },
  {
    "objectID": "rm-data/slides/week12.html#area-under-roc-curve",
    "href": "rm-data/slides/week12.html#area-under-roc-curve",
    "title": "Probability and Classification",
    "section": "Area Under ROC Curve",
    "text": "Area Under ROC Curve\n\nROC curve: the closer it is to the top left column, the better the (insample) prediction.\nArea under ROC (AUC) curve summarizes quality of probabilistic prediction\n\nFor all possible threshold choices\nArea \\(=\\) 0.5 if random classification\nArea \\(&gt;\\) 0.5 if curve mostly over 45 degree line\n\nAUC is a good statistic to compare models\n\nDefined from a non-threshold dependent model (ROC)\nThe larger the better\nRanges between 0 and 1."
  },
  {
    "objectID": "rm-data/slides/week12.html#stata-corner",
    "href": "rm-data/slides/week12.html#stata-corner",
    "title": "Probability and Classification",
    "section": "Stata Corner",
    "text": "Stata Corner\n\nLogit estimation: logit y x1 x2 x3\nPredict probabilities: predict yhat, pr\nClassification:\n\ngen yhat_class = (yhat &gt; 0.5)\nestat classification\n\nROC curve: lroc"
  },
  {
    "objectID": "rm-data/slides/week12.html#model-selection-nr.1-probability-models",
    "href": "rm-data/slides/week12.html#model-selection-nr.1-probability-models",
    "title": "Probability and Classification",
    "section": "Model selection Nr.1: Probability models",
    "text": "Model selection Nr.1: Probability models\n\nModel selection when we have no loss function, based on probability models only\n\nPredict probabilities (No actual classification)\nUse predicted probability to calculate RMSE\nPick by smallest RMSE\n\nOr\n\nDraw up ROC curve and get AUC, Pick the model with the largest AUC\nMore frequently used in practice\nLess sensitive to class imbalance\n\nIn practice, AUC is more frequently used"
  },
  {
    "objectID": "rm-data/slides/week12.html#how-we-make-classification-from-predicted-probability",
    "href": "rm-data/slides/week12.html#how-we-make-classification-from-predicted-probability",
    "title": "Probability and Classification",
    "section": "How we make classification from predicted probability?",
    "text": "How we make classification from predicted probability?\n\nWe set a threshold!\nThe process of classification\n\nIf probability of event is higher than this threshold\\(\\rightarrow\\) assign (predict) class 1; and 0 otherwise.\n\nWho sets the threshold?\n\nUsually approximated by 0.5\nor by the frequency of the event in the data"
  },
  {
    "objectID": "rm-data/slides/week12.html#classification-select-the-threshold-with-loss-function",
    "href": "rm-data/slides/week12.html#classification-select-the-threshold-with-loss-function",
    "title": "Probability and Classification",
    "section": "Classification: select the threshold with loss function",
    "text": "Classification: select the threshold with loss function\n\nFind optimal threshold with loss function.\n\nA loss function is a dollar value assigned to false positive and false negative.\nMost often the costs of FP and FN are very different.\n\n\nConsider loss function\n\\[E[loss] = \\Pr[FN] \\times loss(FN) + \\Pr[FP] \\times loss(FP)\\]\n\nIn ideal case, the minimization of this suggests that optimal threshold is:\n\n\\[\\text{Threshold} = \\frac{loss(FP)}{loss(FN) + loss(FP)}\\]\n\nOr we can try finding the threshold that minimizes the expected loss using Cross-validation. (Software issue)"
  },
  {
    "objectID": "rm-data/slides/week12.html#when-to-use-formula",
    "href": "rm-data/slides/week12.html#when-to-use-formula",
    "title": "Probability and Classification",
    "section": "When to use Formula",
    "text": "When to use Formula\n\nFormula\n\nWhen dataset is “large”\nWhen our model has a “good” fit \\(\\text{Threshold}_{\\min E (loss)} = \\frac{loss(FP)}{loss(FN) + loss(FP)}\\)\n\nIn practice\n\nPro: easy to use, often close enough\nCon: not the best cutoff, especially for smaller data, and poorer model"
  },
  {
    "objectID": "rm-data/slides/week12.html#class-imbalance",
    "href": "rm-data/slides/week12.html#class-imbalance",
    "title": "Probability and Classification",
    "section": "Class imbalance",
    "text": "Class imbalance\n\nA potential issue for some dataset - relative frequency of the classes.\nClass imbalance = the event we care about is very rare or very frequent (\\(\\Pr(y = 1)\\) or \\(\\Pr(y = 0)\\) is very small)\n\nFraud, Sport injury\n\nWhat is rare?\n\nSomething like 1%, 0.1%. (10% should be okay.)\nDepends on size: in larger dataset we can identify rare patterns better.\n\nConsequence: Hard to find those rare events.\n\nYou may be able to identify some patterns by chance."
  },
  {
    "objectID": "rm-data/slides/week12.html#class-imbalance-the-consequences",
    "href": "rm-data/slides/week12.html#class-imbalance-the-consequences",
    "title": "Probability and Classification",
    "section": "Class imbalance: the consequences",
    "text": "Class imbalance: the consequences\n\nMethods we use are not good at handling it.\n\nBoth for the models to predict probabilities, and for the measures of fit used for model selection.\n\nThe functional form assumptions behind the logit model tend to matter more, the closer the probabilities are to zero or one.\nCross-validation can be less effective at avoiding overfitting with very rare or very frequent events if the dataset is not very big. (Many samples will not even have the event.)\nUsual measures of fit can be less good at differentiating models.\nConsequence: Model fitting and selection setup not ideal"
  },
  {
    "objectID": "rm-data/slides/week12.html#class-imbalance-what-to-do",
    "href": "rm-data/slides/week12.html#class-imbalance-what-to-do",
    "title": "Probability and Classification",
    "section": "Class imbalance: what to do",
    "text": "Class imbalance: what to do\n\nWhat to do? Two key insights.\n\nKnow when it’s happening, and be ready for poor performance.\nMay need an action: rebalance sample to help build better models\n\nDownsampling – randomly drop observations from frequent class to balance out more\n\nBefore: 100,000 observations 1% event rate (99,000 \\(y = 1\\), 1,000 \\(y = 0\\))\nAfter 10,000 observations 10% event rate (9,000 \\(y = 1\\), 1,000 \\(y = 0\\))\n\nOver-sampling of rare events\ntry Smart algorithms: Synthetic Minority Over-Sampling Technique (SMOTE)\n\nCreate synthetic observations that are similar to the rare events\nsynthetic rare = Combination of rare and infrequent events"
  },
  {
    "objectID": "rm-data/slides/week12.html#case-study",
    "href": "rm-data/slides/week12.html#case-study",
    "title": "Probability and Classification",
    "section": "Case study",
    "text": "Case study"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-case-study-background",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-case-study-background",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Case study: background",
    "text": "Firm exit case study: Case study: background\n\nBanks and business partners are often interested in the stability of their customers.\nPredicting which firms will be around to do business with is an important part of many prediction projects.\nWorking with financial and non-financial information, your task may be to predict which firms are more likely to default than others.\nGoal: Predict corporate default - exit from the market.\n\nWe have to figure out and decide on target, features, etc."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-bisnode-firms-dataset",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-bisnode-firms-dataset",
    "title": "Probability and Classification",
    "section": "Firm exit case study: bisnode-firms dataset",
    "text": "Firm exit case study: bisnode-firms dataset\n\nFirm data\nMany different type of variables\n\nFinancial, Management, Ownership, Status (HQ)\n\nDataset is a panel data\nRows are identified by company id (comp-id) and year.\nWe’ll focus on a cross-section of 2012."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-label-target-engineering",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-label-target-engineering",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Label (target) engineering",
    "text": "Firm exit case study: Label (target) engineering\n\nDefining our target. There is no “exit” - we have to define it!\nOption: If a firm is operational in year \\(t\\), but is not in business in \\(t + 2\\) -&gt; Exit.\nThis definition is broad\n\nDefaults / forced exit\nOrderly closure\nAcquisitions"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-sample-design",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-sample-design",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Sample design",
    "text": "Firm exit case study: Sample design\n\nLook at a cross section in 2012\n\nIf alive in Year=2014, status_alive=1\n\nKeep if established in 2012\nWe do not care about all firms. Not very small and very large\n\nBelow 10 million euros\nAbove 1000 euros\n\nHardest call: keep when important variables are not missing\n\nBalance sheet like liquid assets\nOwnership like foreign\nIndustry classification\n\nEnd with 19K observation, 20% default rate"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-features---overview",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-features---overview",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Features - overview",
    "text": "Firm exit case study: Features - overview\n\nKey predictors\n\nsize: sales, sales growth\nmanagement: foreign, female, young, number of managers\nregion, industry, firm age\nother financial variables from the balance sheet and P&L.\n\nFor financial variables, we use ratios (to sales or size of balance sheet).\nHere it will turn out be important to look at functional form carefully, especially regarding financial variables.\nMix domain knowledge and statistics.\nPlenty of analyst calls."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-feature-engineering",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-feature-engineering",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Feature engineering",
    "text": "Firm exit case study: Feature engineering\n\nGrowth rates\n\n1 year growth rate of sales. Log difference.\nCould use longer time period, but Lose observations\n\n\nOwnership, management info\n\nKeep if well covered, impute some, but drop if key vars missing\nSometimes simplify (unless big data): ceo_young = ceo_age_mod &lt;40  & ceo_age_mod &gt;15\n\nIndustry categories - need simplify\nForeign ownership - above a threshold\nNumerical variables from balance sheet: Check functional form - logs, polynomials"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-feature-engineering-tools",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-feature-engineering-tools",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Feature engineering tools",
    "text": "Firm exit case study: Feature engineering tools\n\nCheck coverage (missing values)\nDecide on imputation vs drop\nCategorical (factor) variables\nNumerical variables\n\nCheck functional form - logs, polynomials\nLook at relationships in scatterplot, loess and decide"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-feature-engineering-1",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-feature-engineering-1",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Feature engineering",
    "text": "Firm exit case study: Feature engineering\n\nMay need to make cleaning steps.\nCreate binary variables (flags) when implementing changes to values.\nWhen financial values are negative: replace with zero and add a flag to capture imputation.\n\nZeros will not work with logs.\n\nAnnual growth in sales (difference in log sales) vs default\n\nTry editing variables by Winsorizing and adding flags for extreme values.\nSome ODD shapes due to extreme values."
  },
  {
    "objectID": "rm-data/slides/week12.html#the-weird-shape",
    "href": "rm-data/slides/week12.html#the-weird-shape",
    "title": "Probability and Classification",
    "section": "The Weird Shape",
    "text": "The Weird Shape"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-winsorizing",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-winsorizing",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Winsorizing",
    "text": "Firm exit case study: Winsorizing\n\nWhen edge of a distribution is weird…\nWinsorizing is a process to keep observations with extreme values in sample\n\nfor each variable, we\n\nidentify a threshold value, and replace values outside that threshold with the threshold value itself\nand add a flag variable.\n\n\nTwo ways to do it:\n\nan automatic approach, where the lowest and highest 1 percent or 5 percent is replaced and flagged.\nPick thresholds by domain knowledge as well as by looking at lowess. Preferred."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-firm-sales-growth",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-firm-sales-growth",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Firm sales growth",
    "text": "Firm exit case study: Firm sales growth\n\n\nThe winsorized value simply equals original value in a range and flat below/after."
  },
  {
    "objectID": "rm-data/slides/week12.html#case-study-firm-exit-model-features-1",
    "href": "rm-data/slides/week12.html#case-study-firm-exit-model-features-1",
    "title": "Probability and Classification",
    "section": "Case study: firm exit: Model features 1",
    "text": "Case study: firm exit: Model features 1\n\nFirm: Age of firm, squared age, a dummy if newly established, industry categories, location regions for its headquarters, and dummy if located in a big city.\nFinancial 1: Winsorized financial variables: fixed, liquid (incl current), intangible assets, current liabilities, inventories, equity shares, subscribed capital, sales revenues, income before tax, extra income, material, personal and extra expenditure.\nFinancial 2: Flags (extreme, low, high, zero - when applicable) and polynomials: Quadratic terms are created for profit and loss, extra profit and loss, income before tax, and share equity.\nGrowth: Sales growth is captured by a winsorized growth variable, its quadratic term and flags for extreme low and high values."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-model-features-2",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-model-features-2",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Model features 2",
    "text": "Firm exit case study: Model features 2\n\nHR: For the CEO: female dummy, winsorized age and flags, flag for missing information, foreign management dummy; and labor cost, and flag for missing labor cost information.\nData Quality: Variables related to the data quality of the financial information flag for a problem, and the length of the year that the balance sheet covers.\nInteractions: Interactions with sales growth, firm size, and industry."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-models",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-models",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Models",
    "text": "Firm exit case study: Models\nModels (number of predictors)\n\nLogit M1: handpicked few variables (\\(p = 11\\))\nLogit M2: handpicked few variables + Firm (\\(p = 18\\))\nLogit M3: Firm, Financial 1, Growth (\\(p = 35\\))\nLogit M4: M3 + Financial 2 + HR + Data Quality (\\(p = 79\\))\nLogit M5: M4 + interactions (\\(p = 153\\))\nLogit LASSO: M5 + LASSO (\\(p = 142\\))\nNumber of coefficients = N of predictors +1 (constant)"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-data",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-data",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Data",
    "text": "Firm exit case study: Data\n\n\\(N = 19,036\\)\n\\(N = 15,229\\) in work set (80%)\nCross validation 5x training + test sets\n\nUsed for cross-validation\n\n\\(N = 3,807\\) in holdout set (20%)\n\nUsed only for diagnostics of selected model."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-comparing-model-fit",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-comparing-model-fit",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Comparing model fit",
    "text": "Firm exit case study: Comparing model fit\n\n\n\n\nVariables\nCoefficients\nCV RMSE\n\n\n\n\nLogit M1\n4\n12\n0.374\n\n\nLogit M2\n9\n19\n0.366\n\n\nLogit M3\n22\n36\n0.364\n\n\nLogit M4\n30\n80\n0.362\n\n\nLogit M5\n30\n154\n0.363\n\n\nLogit LASSO\n30\n143\n0.362\n\n\n\n\n5-fold cross-validated on work set, average RMSE\nWill use Logit M4 model as benchmark"
  },
  {
    "objectID": "rm-data/slides/week12.html#classification",
    "href": "rm-data/slides/week12.html#classification",
    "title": "Probability and Classification",
    "section": "Classification",
    "text": "Classification\n\nPicked a model on RMSE/Brier score\nFor classification, we will need a threshold"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-roc-curve",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-roc-curve",
    "title": "Probability and Classification",
    "section": "Firm exit case study: ROC curve",
    "text": "Firm exit case study: ROC curve\n\n\nROC curve shows trade-off for various values of the threshold\nGo through values of the ROC curve for selected threshold values, between 0.05 and 0.75, by steps of 0.05"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-auc",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-auc",
    "title": "Probability and Classification",
    "section": "Firm exit case study: AUC",
    "text": "Firm exit case study: AUC\n\n\n\nModel\nRMSE\nAUC\n\n\n\n\nLogit M1\n0.374\n0.738\n\n\nLogit M2\n0.366\n0.771\n\n\nLogit M3\n0.364\n0.777\n\n\nLogit M4\n0.362\n0.782\n\n\nLogit M5\n0.363\n0.777\n\n\nLogit LASSO\n0.362\n0.768\n\n\n\n\nCan calculate the AUC for all our models\nModel selection by RMSE or AUC\nHere: same (could be different if close)"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-comparing-two-thresholds",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-comparing-two-thresholds",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Comparing two thresholds",
    "text": "Firm exit case study: Comparing two thresholds\n\nTake the Logit M4 model, predict probabilities and use that to classify on the holdout set\nTwo thresholds: 50% and 20%\nPredict exit if probability &gt; threshold"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-comparing-two-thresholds-1",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-comparing-two-thresholds-1",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Comparing two thresholds",
    "text": "Firm exit case study: Comparing two thresholds\n\n\n\n\n\n\n\n\n\n\n\n\n\nThreshold: 0.5\n\n\nThreshold: 0.2\n\n\n\n\n\n\n\nActual stay\nActual exit\nTotal\nActual stay\nActual exit\nTotal\n\n\nPredicted stay\n75%\n15%\n90%\n57%\n7%\n64%\n\n\nPredicted exit\n4%\n6%\n10%\n22%\n14%\n36%\n\n\nTotal\n79%\n21%\n100%\n79%\n21%\n100%"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-threshold-choice-consequences",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-threshold-choice-consequences",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Threshold choice consequences",
    "text": "Firm exit case study: Threshold choice consequences\n\nHaving a higher threshold leads to\n\nfewer predicted exits:\n\n10% when the threshold is 50% (36% for threshold 20%).\n\nfewer false positives (4% versus 22%)\nmore false negatives (15% versus 7%).\n\nThe 50% threshold leads to a higher accuracy rate than the 20% threshold\n\n50% threshold: 75% + 6% = 81%\n20% threshold: 57% + 14% = 71%\neven though the 20% threshold is very close to the actual proportion of exiting firms."
  },
  {
    "objectID": "rm-data/slides/week12.html#summary",
    "href": "rm-data/slides/week12.html#summary",
    "title": "Probability and Classification",
    "section": "Summary",
    "text": "Summary\nFirst option: no loss fn\n\nOn the work set, do 5 fold CV and loop over models\nDo Probability predictions\nCalculate average RMSE on test for each fold\nDraw ROC Curve and calculate AUC for each fold\nPick best model based on avg RMSE\nTake best model and estimate RMSE on holdout\\(\\rightarrow\\)best guess for live data performance\nOutput: probability ranking - most likely to least likely.\nShow ROC curve and confusion table with logit on holdout 4 at \\(t = 0.5\\) and \\(t = 0.2\\) - to illustrate trade-off."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-the-loss-function",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-the-loss-function",
    "title": "Probability and Classification",
    "section": "Firm exit case study: The loss function",
    "text": "Firm exit case study: The loss function\n\nLoss function = FN, FP\nWhat matters is FN/FP\nFN=10\n\nIf the model predicts staying in business and the firm exits the market (a false negative), the bank loses all 10 thousand euros.\n\nFP=1\n\nIf predict exit and the bank denies the loan but the firm stays in business in fact (a false positive), the bank loses the profit opportunity of 1 thousand euros.\n\nWith correct decisions, there is no loss."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-finding-the-threshold",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-finding-the-threshold",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Finding the threshold",
    "text": "Firm exit case study: Finding the threshold\n\nFind threshold by formula or algo\nFormula: the optimal classification threshold is \\(1/11 = 0.091\\)\nAlgo: search thru possible cutoffs"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-finding-the-threshold-1",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-finding-the-threshold-1",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Finding the threshold",
    "text": "Firm exit case study: Finding the threshold\n\nConsider all thresholds \\(T = 0.01, 0.02—1\\)\nCalculate the expected loss for all thresholds\nPick when loss function has the minimum\nDone in CV, this is fold Nr.5."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study",
    "href": "rm-data/slides/week12.html#firm-exit-case-study",
    "title": "Probability and Classification",
    "section": "Firm exit case study",
    "text": "Firm exit case study\n\nModel selection process\n\nPredict probabilities\nUse predicted probabilities and loss function to pick optimal threshold\nUse that threshold to calculate expected loss\nPick model with smallest expected loss (in 5-fold CV)\n\nWe run the threshold selection algorithm on the work set, with 5-fold cross-validation.\nBest is model Logit M4\nthe optimal classification threshold by algo is 0.082. Close to formula (0.091)\nThe average expected loss of 0.64."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-studysummary-of-process-with-loss-function",
    "href": "rm-data/slides/week12.html#firm-exit-case-studysummary-of-process-with-loss-function",
    "title": "Probability and Classification",
    "section": "Firm exit case study:Summary of process with loss function",
    "text": "Firm exit case study:Summary of process with loss function\n\nOn the work set, do 5 fold CV and loop over models\nDo Probability predictions\nCalculate average RMSE on each test folds\nDraw ROC Curve and find optimal threshold with loss function (1,10)\n\nshow: threshold search - loss plots and ROC curve for fold 5\n\nSummarize: for each model: average of optimal thresholds, threshold for fold 5, average expected loss, expected loss for fold Nr.5.\nPick best model based on average expected loss\nTake best model, re-estimate it on work set + find optimal threshold and estimate expected loss on holdout set\n\nHere is the continued content in Quarto format:"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-studysummary-of-process-with-loss-function-1",
    "href": "rm-data/slides/week12.html#firm-exit-case-studysummary-of-process-with-loss-function-1",
    "title": "Probability and Classification",
    "section": "Firm exit case study:Summary of process with loss function",
    "text": "Firm exit case study:Summary of process with loss function\n\nConfusion table on holdout with optimal threshold\\(\\rightarrow\\)what to expect in live data."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-cart",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-cart",
    "title": "Probability and Classification",
    "section": "Firm exit case study: CART",
    "text": "Firm exit case study: CART\n\nCART\n\na small tree we built for illustration purposes\nwith only three variables:\n\nfirm size (sales),\nbinary variable for having a foreign management\nBinary if the firm is new.\n\nTerminal nodes with share of exit predictions"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-random-forest",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-random-forest",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Random Forest",
    "text": "Firm exit case study: Random Forest\n\nThe model outperforms the logit models, with a cross validated RMSE of 0.358 and AUC of 0.808.\nWe used predicted probabilities to find the optimal thresholds, and used this to make the classification.\nThe expected loss: 0.587\n\nsmaller than for the best logit (0.642)\n\nFor the random forest we re-estimate the model on work set, and do prediction on holdout set.\nHoldout RMSE RF is 0.358 (vs 0.366 best logit)\nHoldout AUC is 0.808 vs 0.784 for best logit."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-random-forest-1",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-random-forest-1",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Random Forest",
    "text": "Firm exit case study: Random Forest\n\nNote that finding the optimal threshold is rather important.\nIf used a 0.5 threshold, the expected loss jumped to \\(-1.540\\) vs \\(-0.587\\) for the best threshold model.\nThis is 2.6 times the loss from the optimal threshold.\nThe default option in random forest (and many ML models) for classification is majority voting\nMajority voting is threshold=50%"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-random-forest-2",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-random-forest-2",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Random Forest",
    "text": "Firm exit case study: Random Forest\n\nNote that finding the optimal threshold is rather important.\nUsed a 0.5 threshold, the expected loss jumped to \\(-1.540\\) vs \\(-0.587\\) for the best threshold model.\nThis is 2.6 times the loss from the optimal threshold.\nThe default option in random forest (and many ML models) for classification is majority voting\nMajority voting is threshold=50% - NO!!!!!\nDon’t use it!!!!!!\nUnless loss function: FN=FP"
  },
  {
    "objectID": "rm-data/slides/week12.html#repetition-for-sake-of-argument",
    "href": "rm-data/slides/week12.html#repetition-for-sake-of-argument",
    "title": "Probability and Classification",
    "section": "Repetition for sake of argument",
    "text": "Repetition for sake of argument\nIf you don’t have a loss function, you can’t classify."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-random-forest-3",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-random-forest-3",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Random Forest",
    "text": "Firm exit case study: Random Forest\n\nNo loss function\n\nPredict probabilities\n\nLoss function\n\nPredict probabilities\nTake these probabilities and classify by threshold selected\n\nAlternative: use threshold and change the classification rule\n\nCan be done in caret/ranger"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-comparing-two-thresholds-2",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-comparing-two-thresholds-2",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Comparing two thresholds",
    "text": "Firm exit case study: Comparing two thresholds\n\nPredict exit if probability&gt;10.9%\nExpected loss: \\((1.33 \\times 10 + 45.4 \\times 1)/100 = 0.587\\)\n\n\n\n\n\nactual stay\nactual exit\n\n\n\n\npredicted stay\n33.6%\n1.3%\n\n\npredicted exit\n45.4%\n19.7%"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-summary-of-process-with-rf",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-summary-of-process-with-rf",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Summary of process with RF",
    "text": "Firm exit case study: Summary of process with RF\n\nRun probability forest on work set with 5-CV\nGet average (ie over the folds) RMSE and AUC\nNow use loss function (1,10) and search for best thresholds and expected loss over folds\nShow ROC, loss on fold 5\nOptimal Threshold, average expected loss is calculated\nTake model to holdout and estimate RMSE, AUC and expected loss\\(\\rightarrow\\)what you expect in live data\n+1 Show expected loss with classification RF and default majority voting to compare"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-summary-of-model-for-model-selection",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-summary-of-model-for-model-selection",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Summary of model for model selection",
    "text": "Firm exit case study: Summary of model for model selection\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nPreds\nCoeffs\nRMSE\nAUC\nthreshold\nexp. loss\n\n\n\n\nLogit M1\n11\n12\n0.374\n0.736\n0.089\n0.722\n\n\nLogit M4\n36\n79\n0.362\n0.784\n0.082\n0.619\n\n\nLogit LASSO\n36\n143\n0.362\n0.768\n0.106\n0.642\n\n\nRF probability\n36\nn.a.\n0.354\n0.808\n0.098\n0.587\n\n\n\n\nRMSE, AZC, Threshold, Loss: all 5-fold CV results (averages)."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-business-application",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-business-application",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Business application",
    "text": "Firm exit case study: Business application\n\nConsider this setup\n\nFor each firm we review, we get 1000 euros in revenues,\nLoss function: loans to bad companies = \\(-10,000\\) euros,\nmissed loans to good ones = \\(-1,000\\) euros."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-business-application-1",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-business-application-1",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Business application",
    "text": "Firm exit case study: Business application\n\nSimplest model 1 classifies with expected loss 0.722 euro per firm, the Random Forest model has 0.587 euro.\nBuilding a better model yields 135 euros higher profit per firm \\((0.722 - 0.587) \\times 1000 = 135\\)\nIf we do 1000 deals, it is 135,000 euros in profit.\nIf a regulator asks for an interpretable model, we shall compare with the logit M4 model and have 103,000 euros in expected profit.\nWhy does it matter?"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-business-application-2",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-business-application-2",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Business application",
    "text": "Firm exit case study: Business application\n\nRandom Forest gets us 135K profit, best logit is 103K compared to some simple model.\nWe can take this and compare to development costs\nProfit for good analysis."
  },
  {
    "objectID": "rm-data/slides/week12.html#summary-1",
    "href": "rm-data/slides/week12.html#summary-1",
    "title": "Probability and Classification",
    "section": "Summary",
    "text": "Summary\n\nDecide whether the goal is predicting probabilities or classification.\nThe outcome of prediction with a binary target variable is always the predicted probabilities as a function of predictors.\nWhen our goal is probability prediction, we should find the best model that predicts probabilities by cross-validation + RMSE/AUC.\nWhen our goal is classification, we should find the best model that has the smallest expected loss.\n\nWith formula for threshold or search algorithm\nFinding the optimal classification threshold needs a loss function."
  },
  {
    "objectID": "rm-data/slides/week12.html#summary-2",
    "href": "rm-data/slides/week12.html#summary-2",
    "title": "Probability and Classification",
    "section": "Summary",
    "text": "Summary\n\nWithout a loss function, no classification.\nIf you don’t have one, make it up.\nDon’t rely on default 0.5."
  },
  {
    "objectID": "rmethods/10_pooldata.html#pooling-data-together-cross-section-and-panel-data",
    "href": "rmethods/10_pooldata.html#pooling-data-together-cross-section-and-panel-data",
    "title": "Pool Cross-section and Panel Data",
    "section": "Pooling Data together: Cross-section and Panel Data",
    "text": "Pooling Data together: Cross-section and Panel Data\n\nUp to this point, we have cover the analysis of cross-section data.\n\nMany individuals at a single point in time.\n\nTowards the end of the semester, We will also cover the analysis of time series data.\n\nA single individual across time.\n\nToday, we will cover the analysis of panel data and repeated crossection: Many individuals across time.\nThis type of data, also known as longitudinal data, has advantages over crossection, as it provides more information that helps dealing with the unknown of \\(e\\).\nAnd its often the only way to answer certain questions."
  },
  {
    "objectID": "rmethods/10_pooldata.html#pooling-independent-crossections",
    "href": "rmethods/10_pooldata.html#pooling-independent-crossections",
    "title": "Pool Cross-section and Panel Data",
    "section": "Pooling independent crossections",
    "text": "Pooling independent crossections\n\nWe first consider the case of independent crossections.\n\nWe have access to surveys that may be collected regularly. (Household budget surveys)\nWe assume that individuals across this surveys are independent from each other (no panel structure).\n\nThis scenario is typically used for increasing sample-sizes and thus power of analysis (larger N smaller SE)\nOnly minor considerations are needed when analyzing this type of data.\n\nWe need to account for the fact Data comes from different years. This can be done by including year dummies.\nMay need to Standardize variables to make them comparable across years. (inflation adjustments, etc.)"
  },
  {
    "objectID": "rmethods/10_pooldata.html#example",
    "href": "rmethods/10_pooldata.html#example",
    "title": "Pool Cross-section and Panel Data",
    "section": "Example",
    "text": "Example\nLets use the data fertil1 to estimate the changes in fertility rates across time. This data comes from the General Social Survey.\n\n\nCode\nfrause fertil1, clear\nregress kids educ age agesq black east northcen west farm othrural town smcity i.year, robust  \n\n\n\nLinear regression                               Number of obs     =      1,129\n                                                F(17, 1111)       =      10.19\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.1295\n                                                Root MSE          =     1.5548\n\n------------------------------------------------------------------------------\n             |               Robust\n        kids | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |  -.1284268    .021146    -6.07   0.000    -.1699175   -.0869362\n         age |   .5321346   .1389371     3.83   0.000     .2595258    .8047433\n       agesq |   -.005804   .0015791    -3.68   0.000    -.0089024   -.0027056\n       black |   1.075658   .2013188     5.34   0.000     .6806496    1.470666\n        east |    .217324    .127466     1.70   0.088    -.0327773    .4674252\n    northcen |    .363114   .1167013     3.11   0.002     .1341342    .5920939\n        west |   .1976032   .1626813     1.21   0.225     -.121594    .5168003\n        farm |  -.0525575   .1460837    -0.36   0.719    -.3391886    .2340736\n    othrural |  -.1628537   .1808546    -0.90   0.368    -.5177087    .1920014\n        town |   .0843532   .1284759     0.66   0.512    -.1677295    .3364359\n      smcity |   .2118791   .1539645     1.38   0.169    -.0902149    .5139731\n             |\n        year |\n         74  |   .2681825   .1875121     1.43   0.153    -.0997353    .6361003\n         76  |  -.0973795   .1999339    -0.49   0.626    -.4896701    .2949112\n         78  |  -.0686665   .1977154    -0.35   0.728    -.4566042    .3192713\n         80  |  -.0713053   .1936553    -0.37   0.713    -.4512767    .3086661\n         82  |  -.5224842   .1879305    -2.78   0.006    -.8912228   -.1537456\n         84  |  -.5451661   .1859289    -2.93   0.003    -.9099776   -.1803547\n             |\n       _cons |  -7.742457   3.070656    -2.52   0.012     -13.7674   -1.717518\n------------------------------------------------------------------------------\n\n\n\nThis allow us to see how fertility rates have changed across time.\nOne could even interact the year dummies with other variables to see how the effect of other variables have changed across time.\n\n\n\nCode\nfrause cps78_85, clear\nregress lwage i.year##c.(educ i.female) exper expersq union, robust cformat(%5.4f)\n\n\n\nLinear regression                               Number of obs     =      1,084\n                                                F(8, 1075)        =     110.48\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.4262\n                                                Root MSE          =      .4127\n\n------------------------------------------------------------------------------\n             |               Robust\n       lwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     85.year |     0.1178     0.1239     0.95   0.342      -0.1253      0.3609\n        educ |     0.0747     0.0060    12.40   0.000       0.0629      0.0865\n    1.female |    -0.3167     0.0347    -9.12   0.000      -0.3848     -0.2486\n             |\n year#c.educ |\n         85  |     0.0185     0.0095     1.94   0.053      -0.0002      0.0371\n             |\n year#female |\n       85 1  |     0.0851     0.0518     1.64   0.101      -0.0165      0.1866\n             |\n       exper |     0.0296     0.0037     8.10   0.000       0.0224      0.0368\n     expersq |    -0.0004     0.0001    -5.11   0.000      -0.0006     -0.0002\n       union |     0.2021     0.0293     6.89   0.000       0.1446      0.2597\n       _cons |     0.4589     0.0855     5.37   0.000       0.2911      0.6267\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/10_pooldata.html#good-old-friend-chow-test",
    "href": "rmethods/10_pooldata.html#good-old-friend-chow-test",
    "title": "Pool Cross-section and Panel Data",
    "section": "Good old Friend: Chow test",
    "text": "Good old Friend: Chow test\n\nThe Chow test can be used to test whether the coefficients of a regression model are the same across two groups.\n\nwe have seen this test back when we were discussing dummy variables.\n\nWe can also use this test to check if coefficients of a regression model are the same across two time periods. (Has the wage structure changed across time?)\n\nThis is the case of interest here.\n\nNot much changes with before. Although it can be a bit more tedious to code."
  },
  {
    "objectID": "rmethods/10_pooldata.html#example-1",
    "href": "rmethods/10_pooldata.html#example-1",
    "title": "Pool Cross-section and Panel Data",
    "section": "Example",
    "text": "Example\n\n\nCode\nfrause cps78_85, clear\nregress lwage i.year##c.(educ i.female exper expersq i.union), robust\n\n\n\nLinear regression                               Number of obs     =      1,084\n                                                F(11, 1072)       =      82.83\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.4276\n                                                Root MSE          =     .41278\n\n------------------------------------------------------------------------------\n             |               Robust\n       lwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     85.year |   .1219978   .1521927     0.80   0.423    -.1766315    .4206271\n        educ |   .0768148   .0063312    12.13   0.000     .0643918    .0892378\n    1.female |  -.3155108   .0348402    -9.06   0.000    -.3838737    -.247148\n       exper |   .0249177   .0042985     5.80   0.000     .0164833    .0333522\n     expersq |  -.0002844   .0000918    -3.10   0.002    -.0004645   -.0001043\n     1.union |   .2039824   .0381315     5.35   0.000     .1291616    .2788033\n             |\n year#c.educ |\n         85  |    .013927   .0103252     1.35   0.178    -.0063329    .0341869\n             |\n year#female |\n       85 1  |   .0846136   .0524618     1.61   0.107    -.0183258     .187553\n             |\nyear#c.exper |\n         85  |   .0095289   .0073767     1.29   0.197    -.0049454    .0240033\n             |\n        year#|\n   c.expersq |\n         85  |  -.0002399   .0001592    -1.51   0.132    -.0005522    .0000724\n             |\n  year#union |\n       85 1  |  -.0018095   .0594387    -0.03   0.976    -.1184389      .11482\n             |\n       _cons |    .458257     .09386     4.88   0.000     .2740868    .6424271\n------------------------------------------------------------------------------\n\n\n\ntest 85.year#c.educ 85.year#1.female 85.year#c.exper   85.year#c.expersq 85.year#1.union\n\n\n ( 1)  85.year#c.educ = 0\n ( 2)  85.year#1.female = 0\n ( 3)  85.year#c.exper = 0\n ( 4)  85.year#c.expersq = 0\n ( 5)  85.year#1.union = 0\n\n       F(  5,  1072) =    1.65\n            Prob &gt; F =    0.1443"
  },
  {
    "objectID": "rmethods/10_pooldata.html#using-pool-crossection-for-causal-inference",
    "href": "rmethods/10_pooldata.html#using-pool-crossection-for-causal-inference",
    "title": "Pool Cross-section and Panel Data",
    "section": "Using Pool Crossection for Causal Inference",
    "text": "Using Pool Crossection for Causal Inference\n\nOne advantage of pooling crossection data is that it could to be used to estimate causal effects using a method known as Differences in Differences (DnD)\nConsider the following case:\n\nThere was a project regarding the construction of an incinerator in a city. You are asked to evaluate what the impact of this was on the prices of houses around the area.\nYou have access to data for two years: 1978 and 1981.\nIn 1978, there was no information about the project. In 1981, the project was announced, but it only began operations in 1985."
  },
  {
    "objectID": "rmethods/10_pooldata.html#section",
    "href": "rmethods/10_pooldata.html#section",
    "title": "Pool Cross-section and Panel Data",
    "section": "",
    "text": "we could start estimating the project using the simple model: \\[rprice = \\beta_0 + \\beta_1 nearinc + e\\]\n\nusing only 1981 data. But this would not be a good idea. Why?\n\n\nCode\nfrause kielmc, clear\nregress rprice nearinc if year == 1981, robust\n\n\n\nLinear regression                               Number of obs     =        142\n                                                F(1, 140)         =      24.35\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.1653\n                                                Root MSE          =      31238\n\n------------------------------------------------------------------------------\n             |               Robust\n      rprice | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     nearinc |  -30688.27   6219.265    -4.93   0.000     -42984.1   -18392.45\n       _cons |   101307.5   2951.195    34.33   0.000     95472.84    107142.2\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/10_pooldata.html#section-1",
    "href": "rmethods/10_pooldata.html#section-1",
    "title": "Pool Cross-section and Panel Data",
    "section": "",
    "text": "We could also estimate the model using only 1971 data. What would this be showing us?\n\n\n\nCode\nregress rprice nearinc if year == 1978, robust\n\n\n\nLinear regression                               Number of obs     =        179\n                                                F(1, 177)         =       9.87\n                                                Prob &gt; F          =     0.0020\n                                                R-squared         =     0.0817\n                                                Root MSE          =      29432\n\n------------------------------------------------------------------------------\n             |               Robust\n      rprice | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     nearinc |  -18824.37   5992.564    -3.14   0.002    -30650.44   -6998.302\n       _cons |   82517.23   1881.165    43.86   0.000     78804.83    86229.63\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/10_pooldata.html#section-2",
    "href": "rmethods/10_pooldata.html#section-2",
    "title": "Pool Cross-section and Panel Data",
    "section": "",
    "text": "So, using 1981 data we capture the Total price difference between houses near and far from the incinerator.\n\nThis captures both the announcement effect of the project, but also other factors (where would an incinerator be built?).\n\nUsing 1978 data we capture the price difference between houses near and far from the incinerator in the absence of the project.\n\nThis captures the effect of other factors that may be correlated with the incinerator project.\n\nUse both to see the impact!\n\n\\[Effect = -30688.27-(-18824.37)= -11863.9\\]\n\nThis is in essence a DnD model"
  },
  {
    "objectID": "rmethods/10_pooldata.html#difference-in-differences",
    "href": "rmethods/10_pooldata.html#difference-in-differences",
    "title": "Pool Cross-section and Panel Data",
    "section": "Difference in Differences",
    "text": "Difference in Differences\n\n\n\n\n\n\n\n\n\n\nControl\nTreatment\nTreat-Control\n\n\n\n\nPre-\n\\(\\bar y_{00}\\)\n\\(\\bar y_{10}\\)\n\\(\\bar y_{10}\\)-\\(\\bar y_{00}\\)\n\n\nPost-\n\\(\\bar y_{01}\\)\n\\(\\bar y_{11}\\)\n\\(\\bar y_{10}\\)-\\(\\bar y_{00}\\)\n\n\nPost-pre\n\\(\\bar y_{01}\\)-\\(\\bar y_{00}\\)\n\\(\\bar y_{11}\\)-\\(\\bar y_{10}\\)\nDD\n\n\n\n\nPost-Pre:\n\nTrend changes for the control\nTrend changes for the treated: A mix of the impact of the treatment and the trend change.\n\nTreat-Control:\n\nBaseline difference when looking at Pre-period\nTotal Price differentials when looking at Post-period: Mix of the impact of the treatment and the baseline difference.\n\nTake the Double Difference and you get the treatment effect."
  },
  {
    "objectID": "rmethods/10_pooldata.html#difference-in-differences-regression",
    "href": "rmethods/10_pooldata.html#difference-in-differences-regression",
    "title": "Pool Cross-section and Panel Data",
    "section": "Difference in Differences: Regression",
    "text": "Difference in Differences: Regression\n\nThis could also be achieved using a regression model:\n\n\\[ y = \\beta_0 + \\beta_1 post + \\beta_2 treat + \\beta_3 post*treat + e\\]\nWhere \\(\\beta_3\\) is the treatment effect. (only for 2x2 DD)\n\n\nCode\nregress rprice nearinc##y81, robust\n\n\n\nLinear regression                               Number of obs     =        321\n                                                F(3, 317)         =      17.75\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.1739\n                                                Root MSE          =      30243\n\n------------------------------------------------------------------------------\n             |               Robust\n      rprice | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   1.nearinc |  -18824.37    5996.47    -3.14   0.002    -30622.28   -7026.461\n       1.y81 |   18790.29   3498.376     5.37   0.000     11907.32    25673.26\n             |\n nearinc#y81 |\n        1 1  |   -11863.9   8635.585    -1.37   0.170    -28854.21    5126.401\n             |\n       _cons |   82517.23   1882.391    43.84   0.000     78813.67    86220.79\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/10_pooldata.html#difference-in-differences-regression-controls",
    "href": "rmethods/10_pooldata.html#difference-in-differences-regression-controls",
    "title": "Pool Cross-section and Panel Data",
    "section": "Difference in Differences: Regression + controls",
    "text": "Difference in Differences: Regression + controls\n\nOne advantage of DD is that it can control for those unobserved factors that may be correlated with outcome.\n\nWithout controls, however, estimates may not have enough precision.\n\nBut, we could add controls!\n\n\\[ y = \\beta_0 + X \\gamma + \\beta_1 post + \\beta_2 treat + \\beta_3 post*treat + e\\]\n\nBut its not as easy as it may seem! (just adding regressions is not a good approach)\nThis method requires other assumptions! (\\(\\gamma\\) is fixed), which may be very strong.\n\n\nNote: For DD to work, you need to assume the two groups follow the same path in the absence of the treatment. (Parallel trends assumption)\nOtherwise, you are just using trend differences!"
  },
  {
    "objectID": "rmethods/10_pooldata.html#diff-in-diff-in-diff",
    "href": "rmethods/10_pooldata.html#diff-in-diff-in-diff",
    "title": "Pool Cross-section and Panel Data",
    "section": "Diff in Diff in Diff",
    "text": "Diff in Diff in Diff\nAn Alternative approach is to use a triple difference model.\nSetup:\n\nYou still have two groups: Control and Treatment (which are easily identifiable)\nYou have two time periods: Pre and Post (which are also easily identifiable)\nYou have a different sample, where you can identify controls and treatment, as well as the pre- and post- periods. This sample was not treated!\n\nEstimation:\n\nEstimate the DD for the Original Sample, and the new untreated sample.\nObtaining the difference between these two estimates will give you the triple difference.\n\nExample: Smoking ban analysis based on age. (DD) But using both treated and untreated States (DDD)"
  },
  {
    "objectID": "rmethods/10_pooldata.html#general-framework-and-pseudo-panels",
    "href": "rmethods/10_pooldata.html#general-framework-and-pseudo-panels",
    "title": "Pool Cross-section and Panel Data",
    "section": "General Framework and Pseudo Panels",
    "text": "General Framework and Pseudo Panels\n\nOne general Structure for Policy analysis is the use of Pseudo Panels structure.\n\nPseudo panels are a way to use repeated crossection data, but controlling for some unobserved heterogeneity across specific groups. (the pseudo panels)\n\nFor Pseudo-panels, we need to identify a group that could be followed across time.\n\nThis cannot be a group of individuals (repeated crosection).\nBut we could use groups of states, cohorts (year of birth), etc.\n\nIn this case, the data would look like this: \\[y_{igt} = \\lambda_t + \\alpha_g + \\beta x_{gt} + z_{igt}\\gamma +  e_{igt}\\]\nWhere \\(g\\) is the group, \\(t\\) is the time, and \\(i\\) is the individual.\nThis model can be estimated by using dummies. (one dummy for each group and time-period)\nAnd \\(\\beta\\) is the coefficient of interest. (impact of the Policy \\(x_{gt}\\)).\n\nThis may ony work if we assume \\(\\beta\\) is constant across time and groups."
  },
  {
    "objectID": "rmethods/10_pooldata.html#alternative",
    "href": "rmethods/10_pooldata.html#alternative",
    "title": "Pool Cross-section and Panel Data",
    "section": "Alternative",
    "text": "Alternative\n\nWe could also use a more general model: \\[y_{igt} = \\lambda_{gt}+ \\beta x_{gt} + z_{igt}\\gamma +  e_{igt}\\]\nwhere \\(\\lambda_{gt}\\) is a group-time fixed effect. (Dummy for each group-time combination)\n\nNevertheless, while more flexible, this also imposes other types of assumptions, and might even be unfeasible if we have a large number of groups and time periods.\n\nStill, we require \\(\\beta\\) to be homogenous. If that is not the case, you may still suffer from contamination bias."
  },
  {
    "objectID": "rmethods/10_pooldata.html#period-panel-data",
    "href": "rmethods/10_pooldata.html#period-panel-data",
    "title": "Pool Cross-section and Panel Data",
    "section": "2-period Panel data",
    "text": "2-period Panel data\n\nPanel Data, or longitudinal data, is a type of data that has information about the same individual across time.\nThe simplest Structure is one where individuals are followed over only 2 periods.\nThe main advantage of panel data (even two periods version) is that it allows us to control for unobserved heterogeneity across individuals.\n\nBut only if you want to assume fixed effects are constant across time."
  },
  {
    "objectID": "rmethods/10_pooldata.html#section-3",
    "href": "rmethods/10_pooldata.html#section-3",
    "title": "Pool Cross-section and Panel Data",
    "section": "",
    "text": "So how does this reflects in the model specification?\n\n\\[y_{it} = \\beta_0 + \\beta_1 x_{it} + \\beta_2 z_{t} + \\beta_3 w_{i} + e_i + e_t + e_{it}\\]\n\nWhere \\(i\\) refers to individuals or panel units, and \\(t\\) refers to time periods.\nAlso, \\(X's\\), \\(X's\\) \\(W's\\) are variables that vary across individual and time, across time or across individuals.\nThere are also three types of errors. Those that contains unobserved that vary across individuals \\(e_i\\), across time \\(e_t\\), and across individuals and time \\(e_{it}\\) (Idiosyncratic error).\n\\(e_i\\) is usually referred to as the individual fixed effect, and \\(e_t\\) as the time fixed effect.\nIn a 2 period panel, controlling for time-effects is may not be necessary (its just one dummy)\nWhat is more concerning is the unobserved individual fixed effect.\n\nThis is pretty similar to the generalized Pooling model we saw before."
  },
  {
    "objectID": "rmethods/10_pooldata.html#how-estimation-changes",
    "href": "rmethods/10_pooldata.html#how-estimation-changes",
    "title": "Pool Cross-section and Panel Data",
    "section": "How estimation changes",
    "text": "How estimation changes\nFor time use, we assume we control with a single dummy.\n\nYou can choose to “ignore” individual effects.\n\n\\[y_{it} = \\beta_0 + \\beta_1 x_{it} + \\beta_2 w_{i} + \\delta t + (v_{it} = e_i + e_{it})\\]\n\nRequires \\(e_i\\) to be uncorrelated with \\(x_{it}\\) (otherwise is biased), and Standard Errors will need to be clustered at the individual level.\n\n\nYou can aim to estimate all individual fixed effects using dummies (FE estimator). \\[y_{it} = \\beta_0 + \\beta_1 x_{it} + \\delta t + \\sum \\alpha_i D_i + e_{it})\\]\n\nTime fixed variables cannot be estimated anymore"
  },
  {
    "objectID": "rmethods/10_pooldata.html#section-4",
    "href": "rmethods/10_pooldata.html#section-4",
    "title": "Pool Cross-section and Panel Data",
    "section": "",
    "text": "You can estimate the model in differences (FD estimator)\n\n\\[\\begin{aligned}\ny_{i1} &= \\beta_0 + \\beta_1 x_{i1} + \\delta + e_i + e_{i1} \\\\\ny_{i0} &= \\beta_0 + \\beta_1 x_{i0} + e_i + e_{i0} \\\\\n\\Delta y_{i} &= \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta_1 \\Delta x_{i1} + \\delta + \\Delta e_{i}\n\\end{aligned}\n\\]\n\nNow you have only 1 observation per panel, instead of 2. And the result would be identical to FE estimator."
  },
  {
    "objectID": "rmethods/10_pooldata.html#example-2",
    "href": "rmethods/10_pooldata.html#example-2",
    "title": "Pool Cross-section and Panel Data",
    "section": "Example",
    "text": "Example\n\n\nCode\n** This data is in wide format\nfrause slp75_81, clear\n** Lets reshape it so its in standard long format\ngen id = _n\nreshape long educ gdhlth marr slpnap totwrk yngkid, i(id) j(year)\ngen time = year==81\nxtset id time\n** Regression as Pool Crossection\nqui: reg slpnap time totwrk educ marr yngkid gdhlth male, cluster(id)\nest sto m1\n** using FE\nqui: areg slpnap time totwrk educ marr yngkid gdhlth male, absorb(id) cluster(id)\nest sto m2\n** using FD\nqui: reg d.slpnap d.time d.totwrk d.educ d.marr d.yngkid d.gdhlth d.male, robust\nest sto m3\n\n\n(j = 75 81)\n\nData                               Wide   -&gt;   Long\n-----------------------------------------------------------------------------\nNumber of observations              239   -&gt;   478         \nNumber of variables                  21   -&gt;   16          \nj variable (2 values)                     -&gt;   year\nxij variables:\n                          educ75 educ81   -&gt;   educ\n                      gdhlth75 gdhlth81   -&gt;   gdhlth\n                          marr75 marr81   -&gt;   marr\n                      slpnap75 slpnap81   -&gt;   slpnap\n                      totwrk75 totwrk81   -&gt;   totwrk\n                      yngkid75 yngkid81   -&gt;   yngkid\n-----------------------------------------------------------------------------\n\nPanel variable: id (strongly balanced)\n Time variable: time, 0 to 1\n         Delta: 1 unit"
  },
  {
    "objectID": "rmethods/12_timeseries.html#the-nature-of-time-series-data",
    "href": "rmethods/12_timeseries.html#the-nature-of-time-series-data",
    "title": "Times Series Part-I",
    "section": "The nature of time series data",
    "text": "The nature of time series data\n\nTime series “works” different from Repeated crossection.\n\nYou do not have access to a random sample. (Window of time if fixed)\nYou have access to a single “random” time line\n\nAnd in time series, one needs to be quite aware that Data has Baggage…What you see today is the product of everything that happens in the far past.\nThis is what we call Past Dependent, or simple serial correlation.\n\nAnd is why we need to be careful when we use time series data."
  },
  {
    "objectID": "rmethods/12_timeseries.html#the-nature-of-time-series-data-1",
    "href": "rmethods/12_timeseries.html#the-nature-of-time-series-data-1",
    "title": "Times Series Part-I",
    "section": "The nature of time series data",
    "text": "The nature of time series data\n\nData cannot not be arbitrarily reordered. (Past affect future)\n\nTypical features: serial correlation/nonindependence of observations\n\nRandomness of the data comes from the uncertainty of shocks that affects a variable over time, not from sampling.\nYour “Sample” is one realized path that you observe in a narrow window of time.\nBecause observations are no longer independent, we will need to worry about correlation across time.\nIn fact, because data may be strongly correlated across time (say your age), it may generate some problems when applying OLS.\n\nHighly correlated data (high innertia) will have common “trends” that do not necessarity reflect the causal relationship between variables.\n\nSo, we must learn “new” tools to deal with this problem."
  },
  {
    "objectID": "rmethods/12_timeseries.html#static-model",
    "href": "rmethods/12_timeseries.html#static-model",
    "title": "Times Series Part-I",
    "section": "1: Static model",
    "text": "1: Static model\n\nThe static model is the simplest model for analyzing time series data. (like SLRM)\nA Static model aims to find correlations between contemporaneous variables.\n\nImplicity, this assumes there are no dynamic interactions among variables\n\n\n\\[GDP_t = a_0 + a_1 educ_t + a_2 Invest_t + a_3 Unemp_t + u_t\\]\nEducation, investment and Unemployment rate are assumed to affect GDP contemporaneously. But Lags of Leads of the data has no effect on GDP.\n\nThese models are not useful for Forecasting, and Only produces reasonable estimates under very strong assumptions (we will see this later)."
  },
  {
    "objectID": "rmethods/12_timeseries.html#finite-distributed-lag-model-fdl",
    "href": "rmethods/12_timeseries.html#finite-distributed-lag-model-fdl",
    "title": "Times Series Part-I",
    "section": "2: Finite Distributed Lag model (FDL)",
    "text": "2: Finite Distributed Lag model (FDL)\n\nThe FDL model is a simple extension of the static model that allows for dynamic interactions of independent variables.\n\nFinite Because we choose How far back (lags) to add to the model\nDistributed Because each lag will have a different effect on the dependent variable.\n\nSimple Example: \\[fr_t = a_0 + a_1 te_t +e_t\\] \\[fr_t = a_0 + a_1 te_t + a_2 te_{t-1}+ a_3 te_{t-2}+e_t\\]\n\n\\(fr_t\\): Fertility Rate; \\(te_t\\): Tax exemption\nThis is an FDL model with 2 lags."
  },
  {
    "objectID": "rmethods/12_timeseries.html#section",
    "href": "rmethods/12_timeseries.html#section",
    "title": "Times Series Part-I",
    "section": "",
    "text": "More Generality, FDL of order q is defined as:\n\n\\[y_t = a_0 + \\sum_{k=0}^q \\delta_k z_{t-k} + e_t\\]\n\nYou can choose Lags using F-statistic, but also considering the “loss” of Degrees of freedom.\n\nMore lags, less data to estimate the coefficients, more coefficients to estimate\nCoefficients may suffer from High Collinearity\nAllow us to draw inference on Duration of effects.\n\nTwo Types of Effects:\n\nTransitory effects \\(\\frac{\\partial y_t}{\\partial z_{t-q}}=\\delta_q\\)\nPermanent effect \\(\\frac{\\partial y_t}{\\partial z}=\\sum \\frac{\\partial y_t}{\\partial z_{t-q}}=\\sum \\delta_k\\)"
  },
  {
    "objectID": "rmethods/12_timeseries.html#section-1",
    "href": "rmethods/12_timeseries.html#section-1",
    "title": "Times Series Part-I",
    "section": "",
    "text": "What do you expect to see?\n\n\n\n\n\n\n\n\n\nTransitory\n\n\n\n\n\n\n\nPermanent\n\n\n\n\n\n\nTransitory effects measure the short-term effect on outcome (Only of the additional unit)\nPermanent effects measure the long-term effect on outcome (adding up Transitory effects)"
  },
  {
    "objectID": "rmethods/12_timeseries.html#infinite-distributed-lag-model-idl",
    "href": "rmethods/12_timeseries.html#infinite-distributed-lag-model-idl",
    "title": "Times Series Part-I",
    "section": "3: Infinite Distributed Lag model (IDL)",
    "text": "3: Infinite Distributed Lag model (IDL)\n\nThis is a more advanced model that allows for the effects of independent variables to last forever, but how?\n\nA model with infinite number of lags cannot be estimated…unless some restrictions are imposed.\n\n\n\\[ \\text{Wrong: } y_t = a_0 + \\sum_{k=0}^{\\infty} \\delta_k z_{t-k} + e_t\\] \\[ \\text{Better: } y_t = a_0 + \\sum_{k=0}^{\\infty} \\gamma \\delta^k z_{t-k} + e_t\\]\n\nSo we went from pretending to estimate an infinite number of coefficients \\(\\delta_k\\) to estimating only two parameters \\(\\gamma\\) and \\(\\delta\\).\n\nThis is called the Geometric Distributed Lag model."
  },
  {
    "objectID": "rmethods/12_timeseries.html#section-2",
    "href": "rmethods/12_timeseries.html#section-2",
    "title": "Times Series Part-I",
    "section": "",
    "text": "GDL requires an additional “Trick”:\n\n\\[\\begin{aligned}\ny_t &= a_0 + \\gamma z_t + \\gamma \\rho z_{t-1} + \\dots + e_t \\\\\ny_{t-1} &= a_0 + \\gamma z_{t-1} + \\gamma \\rho z_{t-2} + \\dots + e_{t-1}\n\\end{aligned}\n\\]\n\nSubtracting the second equation (times \\(\\rho\\) ) from the first one, we get:\n\n\\[y_t =  \\rho y_{t-1} + a_0 (1-\\rho) + \\gamma z_t  + v_{t}\\]\nWhich requires really strong assumptions!\n\nThe short and Long effects are:\n\n\\[\\text{Short}\\frac{\\partial y_t}{\\partial z_{t-k}}=\\gamma \\rho^k \\text{ and }\n\\text{Long}\\frac{\\partial y_t}{\\partial z}=\\frac{\\gamma}{1-\\rho}\\]"
  },
  {
    "objectID": "rmethods/12_timeseries.html#rational-distributed-lag-model-rdl",
    "href": "rmethods/12_timeseries.html#rational-distributed-lag-model-rdl",
    "title": "Times Series Part-I",
    "section": "4: Rational Distributed Lag model (RDL)",
    "text": "4: Rational Distributed Lag model (RDL)\n\nBecause IDL imposes strong assumptions on coefficients, we can relax them by allowing for lags. This is called the RDL model. \\[y_t = a_0 + \\gamma_0 z_t + \\gamma_1 z_t  +\\delta y_{t-1} + e_t- \\rho e_{t-1}\\]\nWhich has the following short and long effects:\n\n\\[\\text{ Short:}\\frac{\\partial y_t}{\\partial z_t} = \\gamma_0  \\] \\[\\text{ Short:}\\frac{\\partial y_t}{\\partial z_{t-k}} = \\rho^{k-1}(\\rho \\gamma_0 + \\gamma_1) \\] \\[\\text{ Long:}\\frac{\\partial y_t}{\\partial z} = \\frac{\\gamma_0 + \\gamma_1}{1-\\rho}\n\\]"
  },
  {
    "objectID": "rmethods/12_timeseries.html#assumptions-m1-and-m2",
    "href": "rmethods/12_timeseries.html#assumptions-m1-and-m2",
    "title": "Times Series Part-I",
    "section": "Assumptions: M1 and M2",
    "text": "Assumptions: M1 and M2\nA1. Linear in Parameters: Same old, same old, \\(y_t = \\beta_0 + \\beta_1 x_{1t} + \\dots + \\beta_k x_{kt} + u_t\\)\nA2. No Perfect Collinearity: Also Same old, same old"
  },
  {
    "objectID": "rmethods/12_timeseries.html#section-3",
    "href": "rmethods/12_timeseries.html#section-3",
    "title": "Times Series Part-I",
    "section": "",
    "text": "The Stronger ones\n\\[X=\\begin{pmatrix}\nx_{11} & x_{12} & \\dots & x_{1k} \\\\\nx_{21} & x_{22} & \\dots & x_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{T1} & x_{T2} & \\dots & x_{Tk}\n\\end{pmatrix}\n\\]\nA3. Zero Conditional Mean\n\\[E(u_t|X)=0\n\\]\nSo that \\(X\\) is strictly Exogenous (across all possible times).\nNot only \\(x_t\\) should not be affected by \\(u_t\\), but neither should \\(x_{t-1}\\) nor \\(x_{t+1}\\)\nA1-A3 will guarantee that OLS is unbiased."
  },
  {
    "objectID": "rmethods/12_timeseries.html#what-about-std-errors",
    "href": "rmethods/12_timeseries.html#what-about-std-errors",
    "title": "Times Series Part-I",
    "section": "What about Std Errors?",
    "text": "What about Std Errors?\nA4: Strong Homoskedasticity\n\\[Var(u_t|X)=\\sigma^2\n\\]\nA5: No Serial Correlation (Correlation across time of the errors)\n\\[Corr(u_t,u_s|X)=0 \\text{ for all } t\\neq s\n\\]\nAlso difficult to fulfill, because unobserved may have inertia, and depend on past values."
  },
  {
    "objectID": "rmethods/12_timeseries.html#section-4",
    "href": "rmethods/12_timeseries.html#section-4",
    "title": "Times Series Part-I",
    "section": "",
    "text": "Nevertheless, A1-A5: Standard errors can be estimated using the usual formula:\n\\[\\begin{aligned}\n\\hat{Var}(\\hat{\\beta}) &= \\hat{\\sigma}^2(X'X)^{-1} \\\\\n\\hat{Var}(\\hat{\\beta_k}) &= \\frac{\\hat{\\sigma}^2}{SST_k(1-R^2_k)} \\\\\n\\hat \\sigma^2 &= \\frac{1}{T-k-1}\\sum_{t=1}^T \\hat{u}_t^2\n\\end{aligned}\n\\]\nWhich are BLUE! (Best Linear Unbiased Estimators)\nA6: Normality, The \\(\\beta\\)’s are normally distributed, and F-tests and t-tests are valid."
  },
  {
    "objectID": "rmethods/12_timeseries.html#example-the-effet-of-inflation-and-deficit-on-interest-rates",
    "href": "rmethods/12_timeseries.html#example-the-effet-of-inflation-and-deficit-on-interest-rates",
    "title": "Times Series Part-I",
    "section": "Example: The effet of inflation and Deficit on Interest rates",
    "text": "Example: The effet of inflation and Deficit on Interest rates\nModel: \\(i_t = \\beta_0 + \\beta_1 inf_t + \\beta_2 def_t + u_t\\)\nA1: \\(\\checkmark\\) (but questionable)\nA2: \\(\\checkmark\\) (almost never a problem)\nA3: NO! Deficits and inflation today may affect adjustments in the future (\\(u_{t+1}\\)), Similarly, \\(u_t\\) may have to be adjusted in the future using Deficits and inflation.\nA4: Perhaps? Usually there is a direct relationship between deficit and uncertainty, which will generate heteroskedasticity.\nA5: NO! There could be many things in \\(u_t\\) that are correlated across time. (taxes?)\nA6: NO…the errors are almost never normal\n\nfrause intdef, clear\nreg i3 inf def\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =        56\n-------------+----------------------------------   F(2, 53)        =     40.09\n       Model |  272.420338         2  136.210169   Prob &gt; F        =    0.0000\n    Residual |  180.054275        53  3.39725047   R-squared       =    0.6021\n-------------+----------------------------------   Adj R-squared   =    0.5871\n       Total |  452.474612        55  8.22681113   Root MSE        =    1.8432\n\n------------------------------------------------------------------------------\n          i3 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         inf |   .6058659   .0821348     7.38   0.000     .4411243    .7706074\n         def |   .5130579   .1183841     4.33   0.000     .2756095    .7505062\n       _cons |   1.733266    .431967     4.01   0.000     .8668497    2.599682\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/12_timeseries.html#event-studies",
    "href": "rmethods/12_timeseries.html#event-studies",
    "title": "Times Series Part-I",
    "section": "1: Event Studies",
    "text": "1: Event Studies\n\nWe can use Dummies to represent Transitory shocks (events) on the outcome\n\nDummies for the impact of Covid (if we assume effect was transitory), 0 for all periods except for months we were at home.\n\nOr use Dummies to capture permanent changes in the outcome\n\nDummies for ChatGPT. 0 before the introduction, 1 after\n\nPossible to use Lags of Dummies to see the dynamics of the impact.\n\nWith Time series may not be as useful, because its easy to mix event effects with trends, although one could also directly control for trends.\n\n\n\\[FRate_t = 98.7 + 0.08 PE_t - 24.24 WW2 - 31.6 Pill_t+ e_t\\]\n\n\\(Pill_t\\), \\(WW2_t\\) are dummies for the introduction of the pill (permanent) and WW2 (transitory) effects on Fertility rate."
  },
  {
    "objectID": "rmethods/12_timeseries.html#logs-and-growth-models",
    "href": "rmethods/12_timeseries.html#logs-and-growth-models",
    "title": "Times Series Part-I",
    "section": "2: Logs and Growth models",
    "text": "2: Logs and Growth models\n\nVery Similar to what was done in Cross Sectional Models.\nUsing Logs of the Dep variable changes the interpretation of the coefficients.\n\n\\[\\Delta log(x)\\simeq \\%\\Delta x\\]\n\nBecause of that, you can use “log-models” and a trend to estimate the growth rates.\n\nreg log_gdp year\nThe coefficient of year should give you the average growth rate of GDP.\n\nBut the model can also be used in levels to identify trends."
  },
  {
    "objectID": "rmethods/12_timeseries.html#trends-and-seasonality",
    "href": "rmethods/12_timeseries.html#trends-and-seasonality",
    "title": "Times Series Part-I",
    "section": "3: Trends and Seasonality",
    "text": "3: Trends and Seasonality\n\nTrends are very common in time series data.\n\nBecause of the “inertia” of the data, its very common to see variables sharing common trends even if they are completely unrelated. (GDP and age)\n\nIgnoring this may cause problems, as one may identify spurious relationships. (things that look to have significant effects, even tho they are not related)\nConsider the following model (investment on housing, and housing prices):\n\n\n\nCode\nqui:frause hseinv,clear\nreg  linvpc lprice\n\n\n\n      Source |       SS           df       MS      Number of obs   =        42\n-------------+----------------------------------   F(1, 40)        =     10.53\n       Model |  .254364468         1  .254364468   Prob &gt; F        =    0.0024\n    Residual |  .966255566        40  .024156389   R-squared       =    0.2084\n-------------+----------------------------------   Adj R-squared   =    0.1886\n       Total |  1.22062003        41   .02977122   Root MSE        =    .15542\n\n------------------------------------------------------------------------------\n      linvpc | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      lprice |   1.240943   .3824192     3.24   0.002     .4680452    2.013841\n       _cons |  -.5502345   .0430266   -12.79   0.000    -.6371945   -.4632746\n------------------------------------------------------------------------------\n\n\n\nIf we estimate this model, we find a very strong relationship, perhaps because of common trends. Adding a trend, however, may change the results.\n\n\\[E(log(invpc_t)|x) = -20.04 -0.38 log(price) + 0.009 year \\]\n\n\nCode\nreg linvpc lprice  year\n\n\n\n      Source |       SS           df       MS      Number of obs   =        42\n-------------+----------------------------------   F(2, 39)        =     10.08\n       Model |  .415945108         2  .207972554   Prob &gt; F        =    0.0003\n    Residual |  .804674927        39   .02063269   R-squared       =    0.3408\n-------------+----------------------------------   Adj R-squared   =    0.3070\n       Total |  1.22062003        41   .02977122   Root MSE        =    .14364\n\n------------------------------------------------------------------------------\n      linvpc | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      lprice |  -.3809612   .6788352    -0.56   0.578    -1.754035    .9921125\n        year |   .0098287   .0035122     2.80   0.008     .0027246    .0169328\n       _cons |  -20.03976   6.964526    -2.88   0.006    -34.12684   -5.952675\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/12_timeseries.html#trends-and-seasonality-1",
    "href": "rmethods/12_timeseries.html#trends-and-seasonality-1",
    "title": "Times Series Part-I",
    "section": "3: Trends and Seasonality",
    "text": "3: Trends and Seasonality\n\nJust as time series are characterized by trends, they are also characterized by seasonality.\n\nSeasonality is the presence of regular patterns in the data that repeat over fixed periods of time.\nSeasonality is a form of “deterministic” variation, because it is predictable.\n\nFor example, If you look at Public expenditure, you will see that its higher the last year that a president is in office. (election year)\nSimilarly, you will see higher expenditure in December, because of Christmas.\nAs with trends, this may cause spurious relations, thus, its recommended to control for seasonality adding dummies.\n\nquarter, month, day of the week, year after election, etc.\n\nAs simple as adding dummies for each month, or quarter, etc."
  },
  {
    "objectID": "rmethods/12_timeseries.html#r2-and-spurious-regressions",
    "href": "rmethods/12_timeseries.html#r2-and-spurious-regressions",
    "title": "Times Series Part-I",
    "section": "4: \\(R^2\\) and Spurious Regressions",
    "text": "4: \\(R^2\\) and Spurious Regressions\n\nOne of the consequences of spurious regressions is that the \\(R^2\\) will be inflated. (caputred by the common trend or seasonality\nEven if we add trends or seaonalities, the default \\(R^2\\) will be too large. (Because it still describes ALL variation)\nA better approach to understand the true explanatory power of the model is to use an \\(R^2\\) that adjusts for trends and seasonality.\n\n\\[y_t = \\beta_0 + \\beta_1 x_{1t} + \\beta_2 x_{2t} + \\theta \\times t + \\sum \\gamma_k \\times D_k + u_t\\]\nWhere \\(D_k\\) are dummies for seasonality, and \\(\\theta \\times t\\) is a trend."
  },
  {
    "objectID": "rmethods/12_timeseries.html#r2-and-spurious-regressions-1",
    "href": "rmethods/12_timeseries.html#r2-and-spurious-regressions-1",
    "title": "Times Series Part-I",
    "section": "4: \\(R^2\\) and Spurious Regressions",
    "text": "4: \\(R^2\\) and Spurious Regressions\n\nTo estimate the adjusted \\(R^2\\) it may be better to use de-trended and de-seasonalized data.\n\n\\[\\tilde w_t = w_t - E(w_t| t , D_1, D_2, \\dots, D_k) \\forall w \\in {y, x_1, x_2}\\]\n\nEstimate model\n\n\\[\\tilde y_t = \\beta_1 \\tilde x_{1t} + \\beta_2 \\tilde x_{2t} + u_t\\]\n\nCalculate the \\(R^2\\) using the “correct” \\(SST\\) and \\(SSE\\) using the demeaned data.\n\n\\[aR^2 = 1-\\frac{\\sum \\hat u^2_t}{\\sum \\tilde y^2_t}\\]"
  },
  {
    "objectID": "rmethods/1_introduction.html#what-is-econometrics",
    "href": "rmethods/1_introduction.html#what-is-econometrics",
    "title": "Introduction",
    "section": "What is econometrics?",
    "text": "What is econometrics?\nEconometrics is an amalgamation of Statistics and Economics, that typically analysis nonexperimental data.\n\nStatistics: Because we make use of numerous properties and mathematical properties to obtain derive Statitics related to our data\nEconomics: Because we aknowledge that we use data that comes from Agents interactions, and as such as subject to erros.\n\nWe use both tools to analyze data from the world around us.\n\nYour Economic intuition to make sense and explain relationships that you find, and mathematics/statistics to obtain estimates that are statistically sound."
  },
  {
    "objectID": "rmethods/1_introduction.html#when-is-it-useful",
    "href": "rmethods/1_introduction.html#when-is-it-useful",
    "title": "Introduction",
    "section": "When is it useful?",
    "text": "When is it useful?\nEconometrics is useful whenever we aim to:\n\nTest theories, Explore theoretical relationships, Verify Predictions\n\nBut also\n\nWe use Econometrics when we want to evaluate policies, or provide evidence for policy makers. Find that Causal effect.\n\nCaveat: We may not have the best data for this, but we can come up with cleaver designs to still do our job!"
  },
  {
    "objectID": "rmethods/1_introduction.html#where-does-econometric-fits-in-empirical-research",
    "href": "rmethods/1_introduction.html#where-does-econometric-fits-in-empirical-research",
    "title": "Introduction",
    "section": "Where does Econometric Fits in Empirical Research",
    "text": "Where does Econometric Fits in Empirical Research\nObvious answer\n\nYou will need Econometrics methods to analyze your data, however, (just as a reminder) you should be aware of “HOW” an Empirical Research should be made:\n\nS1. Research Question:\n\nA question that is Answerable (within bounderies of Time/Money/and availabilty)\nThat should help us understand a Topic Better\nThat is Specific Enough to be feasible, but General Enough to be of interest.\n\nKeep it simple"
  },
  {
    "objectID": "rmethods/1_introduction.html#where-does-econometric-fits-in-empirical-research-1",
    "href": "rmethods/1_introduction.html#where-does-econometric-fits-in-empirical-research-1",
    "title": "Introduction",
    "section": "Where does Econometric Fits in Empirical Research",
    "text": "Where does Econometric Fits in Empirical Research\nS2. Construct an Economic model\n\nTo understand what the problem before you start analyzing the research question. May not need a formal modeling (Heavy Math), but enough to have some understanding of the problem.\n\nS3. Decide on the Econometric model\n\nYou need to decide what data is needed and is available your model.\nHow you will estimate the model (assumptions on methods)\n\nS4. Estimate model, and Analyze Data\n\nEstimate the model, using Economtric tools and Methods fitted to the data and the research question.\nExplain results in light of your Economic Model, and theoretical predictions. The Economist in you!"
  },
  {
    "objectID": "rmethods/1_introduction.html#so-you-need-data",
    "href": "rmethods/1_introduction.html#so-you-need-data",
    "title": "Introduction",
    "section": "So you need Data",
    "text": "So you need Data"
  },
  {
    "objectID": "rmethods/1_introduction.html#need-for-data",
    "href": "rmethods/1_introduction.html#need-for-data",
    "title": "Introduction",
    "section": "Need for Data",
    "text": "Need for Data\nDifferent types of data may allow for using different econometric methodologies, and answer different types of questions.\n\nKeep in mind you will only have access to SAMPLES, never the Population\nThere will be instances that you come close to Population data, ie Census.\nBut Even Census data is not the Population (or Super Population we use in Econometrics)."
  },
  {
    "objectID": "rmethods/1_introduction.html#types-of-data",
    "href": "rmethods/1_introduction.html#types-of-data",
    "title": "Introduction",
    "section": "Types of Data:",
    "text": "Types of Data:\nCross-Section: Sample of the population collects data on Many individuals in a single point in time.\nTime Series Data: Data collected on a single individual across time.\nPanel Data: Data collected for Many individuals who are followed across time.\nRepeated Cross-Section: Pooled Cross-Section Data for different individuals collected at different points in time. Individuals are not followed across time."
  },
  {
    "objectID": "rmethods/1_introduction.html#visually",
    "href": "rmethods/1_introduction.html#visually",
    "title": "Introduction",
    "section": "Visually",
    "text": "Visually\n\nCrossectionTime SeriesPanel DataRepeated Crossection"
  },
  {
    "objectID": "rmethods/1_introduction.html#before-the-break",
    "href": "rmethods/1_introduction.html#before-the-break",
    "title": "Introduction",
    "section": "Before the Break:",
    "text": "Before the Break:\nCausality, Ceteris Paribus, and Counterfactuals\nThee important concepts for the Class\n\nCausality: This is what most applied research aims to identify. A causal effect is a change the variable interest experiences, only because a second variable changed, while all other factors remained FIXED.\n\nThis is different from associations or correlations.\n\nCeteris Paribus: In Econometric analysis, ceteris paribus implies that all factors, except the one analyzed, are assumed constant (There is no change), thus leading to causality\nCounterfactual: It is the consideration of what would have been if only a single factor changed in the analysis (for a given observation).\n\nWhat if didn’t apply to the MSC at Levy? If you got miss your plane to the US? etc."
  },
  {
    "objectID": "rmethods/1_introduction.html#thinking-about-counterfactuals-is-key",
    "href": "rmethods/1_introduction.html#thinking-about-counterfactuals-is-key",
    "title": "Introduction",
    "section": "Thinking about Counterfactuals is Key",
    "text": "Thinking about Counterfactuals is Key\nFor empirical work that aims to identify Causal Effects, it is important to understand the concept of counterfactual.\n\nIt will help you understand what is what you need to analyze,\nHow could those effects be identified in ideal scenarios (Experiments)\nWhat the limitation of those scenarios are\nAnd what alternatives are there to void those limitations"
  },
  {
    "objectID": "rmethods/1_introduction.html#example-causal-effect-of-fertilizer-on-crops",
    "href": "rmethods/1_introduction.html#example-causal-effect-of-fertilizer-on-crops",
    "title": "Introduction",
    "section": "Example: Causal effect of Fertilizer on Crops",
    "text": "Example: Causal effect of Fertilizer on Crops\nRQ: By how much will the production of soybeans increase if one increases the amount of fertilizer applied to the ground?\nCF: Same Piece of Land with and without Fertilizer (Impossible)\nEXP: Randomly Use Fertilizers Across different Plots of Land (Expensive but feasible)\nEA: Use Regressions to keep other all factors that can affect Land productivity fixed when Analyzing Expost Data (Inexpensive)"
  },
  {
    "objectID": "rmethods/1_introduction.html#example-causal-effect-of-smoking-on-babies-health",
    "href": "rmethods/1_introduction.html#example-causal-effect-of-smoking-on-babies-health",
    "title": "Introduction",
    "section": "Example: Causal Effect of Smoking on Babie’s health",
    "text": "Example: Causal Effect of Smoking on Babie’s health\nRQ: Does Smoking during Pregnancy decreases birthweight?\nCF: We consider the same woman. In one case she smokes through pregnancy, in the other she doesnt. Compare Babies Weight.\nEXP: Select a random sample of Pregnant Women and randomly select those who will be “forced” to smoke during pregnancy.\nEXP1: Select a Randome sample of PW with history of smoking. Randomly offer them a voucher and Counceling to quit smoking.\nEA: Consider women with similar characteristics, except for smoking, and compare their babies outcomes."
  },
  {
    "objectID": "rmethods/3_MLRM.html#why-stay-with-1-when-you-can-use-many-why-not",
    "href": "rmethods/3_MLRM.html#why-stay-with-1-when-you-can-use-many-why-not",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Why stay with 1 when you can use Many? … Why not?",
    "text": "Why stay with 1 when you can use Many? … Why not?\n\nThe SLRM we cover last week is a powerful tool to understand the mechanics behind regression analysis, however is too limited.\n\nUse one control? to fix everything ?!\n\nThe Natural alternative is to relax the assumption and Make things more flexible.\n\nIn other words…Allow for adding More controls\n\n\nThus, instead of:\n\\[y_i = \\beta_0 + \\beta_1 x_i + e_i\n\\]\nwe have to consider:\n\\[y_i = \\beta_0 + \\beta_1 x_{1i} +\\beta_2 x_{2i} + \\dots + \\beta_k x_{ki} + u_i\n\\]\nHow many can we add? and why does it help?"
  },
  {
    "objectID": "rmethods/3_MLRM.html#the-power-of-mlr-why-do-more-controls-help",
    "href": "rmethods/3_MLRM.html#the-power-of-mlr-why-do-more-controls-help",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "The power of MLR: Why do more controls help?",
    "text": "The power of MLR: Why do more controls help?\n\nOne more explicitly accounts for variables that before were hidden in \\(e_i\\).\nWe add \\(x_{2i},x_{3i},\\dots,x_{ki}\\) to the model model, and is no longer in \\(e_i\\)\nAllows for richer model specifications and nonlinearities:\nBefore: \\(y_i = \\beta_0 + \\beta_1 x_{1i} + e_i\\)\nNow : \\(y_i = \\beta_0 + \\beta_1 x_{1i} +\\beta_2 x^2_{1i} + \\beta_3 x^{1/2}_{1i} + \\beta_4 x^{-1}_{1i} + \\beta_5 x_{2i}+\\dots+e_i\\)\n\nThus, we can get closer to the unknown Population function, and explicitly handle some endogeneity problems (we control for it).\n\n\n\n\n\n\nWith great power…\n\n\nBeing able to add more controls is good, but:\n\nMay make things worse (bad controls)\nOr might not be feasible (small Sample)\nOr may be difficult to interpret (unless you know how to)"
  },
  {
    "objectID": "rmethods/3_MLRM.html#mostly-the-same",
    "href": "rmethods/3_MLRM.html#mostly-the-same",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Mostly the same",
    "text": "Mostly the same\n\nLinear in Parameters: \\(y = X\\beta + e\\) (And this is the pop function)\nRandom Sampling from the population of interest. (So errors \\(e_i\\) is independent from \\(e_j\\))\nNo Perfect Collinearity:\nThis is the alternative to \\(Var(x)&gt;0\\) (SLRM), and deserves more attention.\n\n\nWe want each variable in \\(X\\) to have some independent variation, from all other variables in the model.\n\nIn the SLRM, the independent variation idea was with respect to the constant.\n\nIf a variable was a linear combination of others, then \\(\\beta's\\) cannot be identified. You need to choose what to keep:\n\n\\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1+X_2) + e \\\\\n&=  \\beta_0 + (\\beta_1+\\beta_3) X_1 + (\\beta_2+\\beta_3) X_2 + e  \n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#section-1",
    "href": "rmethods/3_MLRM.html#section-1",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "Zero Conditional mean (Exogeneity): \\(E(e_i|X)=0\\)\nRequires that the errors and the explanatory variables are uncorrelated. This is “easier” to achieve, because we can now move variables form the error to the model.\nHowever, there could be things you can’t controls for (and remain lurking in your errors)\n\n\nI call this the most important assumption, because is the hardest to deal with\n\nIf A1-A4 Hold, then your estimates will be unbiased!\n\nHomoskedasticity Same as before. Errors dispersion does not change with respect to all \\(X's\\). \\[Var(e|X)=c\n\\]\n\nJust as with SLRM, this assumption will help with the estimation of Standard Errors."
  },
  {
    "objectID": "rmethods/3_MLRM.html#mlrm-estimation",
    "href": "rmethods/3_MLRM.html#mlrm-estimation",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "MLRM estimation",
    "text": "MLRM estimation\nAs before, not much has changed. We are still interested in finding \\(\\beta's\\) that Minimizes the (squared) error of the model when compared to the observed data:\n\\[\\hat \\beta = \\min_\\beta \\sum (y_i-X_i'\\beta)^2 = \\min_\\beta \\sum (y_i-\\beta_0-\\beta_1 x_{1i}-\\dots-\\beta_k x_{ki})^2\n\\]\nThe corresponding FOC generate \\(K+1\\) equations to identify \\(K+1\\) parameters:\n\\[\\begin{aligned}\n\\sum (y_i-X_i'\\beta) &= 0  \\\\\n\\sum x_{1i}(y_i-X_i'\\beta) &= 0 \\\\\n\\sum x_{2i}(y_i-X_i'\\beta) &= 0 \\\\ \\dots \\\\\\\n\\sum x_{ki}(y_i-X_i'\\beta) &= 0\n\\end{aligned} \\rightarrow X'(y-X\\beta) =0 \\rightarrow \\hat \\beta = (X'X)^{-1}X'y\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#mata-interlute-for-those-curious",
    "href": "rmethods/3_MLRM.html#mata-interlute-for-those-curious",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "mata Interlute (for those curious)",
    "text": "mata Interlute (for those curious)\n\n\nCode\nfrause gpa1, clear\ngen one =1 \nmata: y=st_data(.,\"colgpa\"); mata: x=st_data(.,\"hsgpa act one\")\nmata: xx=x'x ; ixx=invsym(xx) ; xy = x'y \nmata: b = ixx * xy ; b\n\n\n                 1\n    +---------------+\n  1 |  .4534558853  |\n  2 |  .0094260123  |\n  3 |  1.286327767  |\n    +---------------+"
  },
  {
    "objectID": "rmethods/3_MLRM.html#you-got-the-betas-how-do-you-interpret-them",
    "href": "rmethods/3_MLRM.html#you-got-the-betas-how-do-you-interpret-them",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "You got the \\(\\beta's\\), how do you interpret them?",
    "text": "You got the \\(\\beta's\\), how do you interpret them?\nInterpretation of MLRM is similar to the SLRM. For most cases, you simply look into the coefficients, and interpret effects in terms of Changes:\n\\[\\begin{aligned}\ny_i = \\hat\\beta_0 + \\hat\\beta_1 x_{1i}  + \\hat\\beta_2 x_{2i} + e_i \\\\\n\\Delta y_i =  \\hat\\beta_1 \\Delta  x_{1i}  + \\hat\\beta_2 \\Delta  x_{2i} + \\Delta e_i\n\\end{aligned}\n\\]\nUnder A1-A5 I can make use the above to make interpretations\n\n\\(\\hat \\beta_0\\) has no effect on “changes” of \\(y\\). Only its levels.\n\\(\\hat \\beta_1\\) indicates how much \\(\\Delta y_i\\) will be if \\(\\Delta x_{1i}\\) increases in 1 unit, if both \\(\\Delta x_{2i}\\) and \\(\\Delta e_i\\) remain constant (Ceteris Paribus)\n\n\\(\\Delta e_i=0\\) by assumption, and \\(\\Delta x_{2i}=0\\) because we are explicitly controlling for it (We impute this based on extrapolations)\nYou could also analyze the effect of \\(\\Delta x_{1i}\\) and \\(\\Delta x_{2i}\\) Simultaneously!"
  },
  {
    "objectID": "rmethods/3_MLRM.html#example",
    "href": "rmethods/3_MLRM.html#example",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Example",
    "text": "Example\n\nCode\nqui: frause wage1, clear\nqui: reg lwage educ exper tenure\nlocal b0:display %5.3f _b[_cons]\nlocal b1:display %5.3f _b[educ]\nlocal b2:display %5.3f _b[exper]\nlocal b3:display %5.3f _b[tenure]\ndisplay \"\\$log(wage) = `b0' + `b1' educ + `b2' exper + `b3' tenure$\"\n\n\\(log(wage) = 0.284 + 0.092 educ + 0.004 exper + 0.022 tenure\\)\n\n\\(\\beta_0\\) has no effect on changes, but level.\n\nIf someone has no education, experience or tenure, log(wages) will be 0.284. Why not wages? and Does it make sense to assume 0 education, experience and tenure?\n\n\\(\\beta_1\\): An additional year of education increases wages in 0.092log points or about 9.2%, if Experience and tenure do not change (ceteris paribus).\n\nNotes:\n\nThink of Interpretations as counterfactual: \\(y_{post} - y_{pre}\\)\nAssumption: Other factors (unobserved \\(e\\)) remain fixed (is it always credible??)\nEffects can be combined. What if a person gains 1 year of education but losses 3 of tenure?"
  },
  {
    "objectID": "rmethods/3_MLRM.html#more-on-interpretation",
    "href": "rmethods/3_MLRM.html#more-on-interpretation",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "More on Interpretation",
    "text": "More on Interpretation\nUnder A1-A5, you can still interpret results as “counterfactual” at the individual level. However, its more common to do it based on Conditional means:\n\\[\\frac {\\Delta E(y|X)}{\\Delta X_k} \\simeq E(y|X_{-k},X_k+1)-E(y|X)\n\\]\nWhich mostly changes Language.\n\nThe expected effect of an increase in \\(X\\) in one unit."
  },
  {
    "objectID": "rmethods/3_MLRM.html#alternative-interpretation-partialling-out",
    "href": "rmethods/3_MLRM.html#alternative-interpretation-partialling-out",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Alternative Interpretation: Partialling out",
    "text": "Alternative Interpretation: Partialling out\n\nAn alternative way of interpreting (and understanding) MLRM is to think about partialling out interpretation.\nThis interpretation is based on the Frisch-Waugh-Lowell Theorem, which states that the following models should give you the SAME \\(\\beta's\\):\n\n\\[\\begin{aligned}\ny &= \\color{blue}{\\beta_1 } X_1 + \\beta_2 X_2 + e \\\\\n(I-P_{X^c_2}) y &= \\color{green}{\\beta_1} (I-P_{X^c_2}) X_1 + e \\\\\nP_{X^c_2} &= X^c_2 (X'^{c}_2  X^{c}_2) X'^{c}_2 : \\text{Projection Matrix}\n\\end{aligned}\n\\]\n\n\n\n\n\n\nPartialling out\n\n\n\\(\\beta_1\\) can be interpreted as the effect of \\(X_1\\) on \\(y\\), after all variation related to \\(X_2\\) has been “eliminated”.\nThus \\(\\beta_1\\) is the effect uniquely driven by \\(X_1\\)."
  },
  {
    "objectID": "rmethods/3_MLRM.html#example-1",
    "href": "rmethods/3_MLRM.html#example-1",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Example",
    "text": "Example\n\nqui {\n  frause oaxaca, clear\n  drop if lnwage==.\n  reg lnwage educ exper tenure\n  est sto m1\n  reg educ        exper tenure\n  predict r_educ , res\n  reg lnwage      exper tenure\n  predict r_lnwage , res\n  reg r_lnwage r_educ\n  est sto m2\n  reg lnwage educ\n  est sto m3\n}\nesttab m1 m2 m3, se  \n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                   lnwage        r_lnwage          lnwage   \n------------------------------------------------------------\neduc               0.0870***                       0.0800***\n                (0.00516)                       (0.00539)   \n\nexper              0.0113***                                \n                (0.00154)                                   \n\ntenure            0.00837***                                \n                (0.00188)                                   \n\nr_educ                             0.0870***                \n                                (0.00516)                   \n\n_cons               2.140***     8.93e-10           2.434***\n                 (0.0650)        (0.0124)        (0.0636)   \n------------------------------------------------------------\nN                    1434            1434            1434   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/3_MLRM.html#estimator-properties-unbiased",
    "href": "rmethods/3_MLRM.html#estimator-properties-unbiased",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Estimator Properties: Unbiased",
    "text": "Estimator Properties: Unbiased\nRecall, the estimator of \\(\\beta's\\) when you have multiple dependent variables:\n\\[\\begin{aligned}\n0  &: \\hat \\beta = (X'X)^{-1} X'y \\\\\nA1 \\text{ & }  A2 &: \\hat \\beta = (X'X)^{-1} X'(X\\beta + e) \\\\\n1  &: \\hat \\beta = (X'X)^{-1} X'X\\beta + (X'X)^{-1} X'e \\\\\nA3 &: det(X'X)\\neq 0 \\rightarrow (X'X)^{-1} \\text{ exists} \\\\\n2  &: \\hat \\beta = \\beta + (X'X)^{-1} X'e \\\\\nA4 &: E(e|X)=0 \\rightarrow E[(X'X)^{-1} X'e]=0 \\\\\n3  &: E(\\hat\\beta)= \\beta \\text{ unbiased}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#estimator-properties-variance-under-homoskedasticity",
    "href": "rmethods/3_MLRM.html#estimator-properties-variance-under-homoskedasticity",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Estimator Properties: Variance under Homoskedasticity",
    "text": "Estimator Properties: Variance under Homoskedasticity\nLets start with (2). \\(\\beta's\\) are random functions of the errors. Thus its variance will depend on \\(e\\).\n\\[\\begin{aligned}\n1 &: \\hat \\beta = \\beta + (X'X)^{-1} X'e \\\\\n2 &:\\hat \\beta - \\beta = (X'X)^{-1} X'e \\\\\n3 &: Var(\\hat \\beta - \\beta) = Var((X'X)^{-1} X'e) \\\\\n4 &: Var(\\hat \\beta - \\beta) = (X'X)^{-1} X' Var(e) X (X'X)^{-1}  \\\\\n\\end{aligned}\n\\]\n\\(Var(e)\\) considers variance and covariance of each \\(e_i\\) and its combinations."
  },
  {
    "objectID": "rmethods/3_MLRM.html#section-2",
    "href": "rmethods/3_MLRM.html#section-2",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "By assumption A2, \\(cov(e_i,e_j)=0\\). And by assumption A5 \\(Var(e_i)=Var(e_j)\\).\n\\[\\begin{aligned}\nVar(\\hat \\beta - \\beta) &= (X'X)^{-1} X' \\sigma_e^2 I X (X'X)^{-1} \\\\\nVar(\\hat \\beta - \\beta) &= \\sigma_e^2 (X'X)^{-1} \\\\\nVar(\\hat \\beta_j - \\beta_j) &= \\frac{\\sigma_e^2}{SST_j (1-R^2_j)}\n\\end{aligned}\n\\]\nBut we do not know \\(\\sigma^2_e\\). Thus, we also “estimate it”\n\\[\\hat \\sigma^2_e = \\frac{\\sum \\hat e^2}{N-K-1}\n\\]\nWhich is unbiased estimator for \\(\\sigma^2_e\\) if A1-A5 hold."
  },
  {
    "objectID": "rmethods/3_MLRM.html#section-3",
    "href": "rmethods/3_MLRM.html#section-3",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "\\[\\begin{aligned}\nVar(\\hat \\beta - \\beta) &= \\sigma_e^2 (X'X)^{-1} \\\\\nVar(\\hat \\beta_j - \\beta_j) &= \\frac{\\sigma_e^2}{SST_j (1-R^2_j)}  \\\\\\\n& = \\frac{\\sigma_e^2}{(N-1)Var(X_j) (1-R^2_j)} = \\frac{\\sigma_e^2}{(N-1)Var(X_j)}VIF_j\n\\end{aligned}\n\\]\nTo consider:\n\n\\(Var(\\beta)\\) increases with \\(\\sigma_e^2\\). More variation in the error, more variation of the coefficients.\n\\(Var(\\beta)\\) decreases with Sample size \\(N\\)\n\\(Var(\\beta)\\) also decreases with Variation in \\(X\\)\nHowever, it increases if there is less unique variation (Multicolinearity problem and VIF)"
  },
  {
    "objectID": "rmethods/3_MLRM.html#quick-note",
    "href": "rmethods/3_MLRM.html#quick-note",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Quick Note",
    "text": "Quick Note\n\n\\(R^2\\) are the same as SLRM: How much of variation is explained by the model.\n\nAlso \\(R^2 = corr(y,\\hat y)^2\\)\n\nThe fitted line goes over the “mean” of all variables\nMLRM Fits hyper-planes to the data\nRegression through the origin still a bad idea\nAlso, under A1-A5 OLS is the Best Linear Unbiased Estimator (BLUE)"
  },
  {
    "objectID": "rmethods/3_MLRM.html#ignoring-variables",
    "href": "rmethods/3_MLRM.html#ignoring-variables",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Ignoring Variables",
    "text": "Ignoring Variables\nIn the MLRM framework, its easier to see what happens when important variables are ignored.\n\\[\\text{True: } y = b_0 + b_1 x_1 + b_2 x_2 + e\n\\]\nBut instead you estimate the following :\n\\[\\text{Estimated: }y = g_0 + g_1 x_1 + v\n\\]\nUnless stronger assumptions are imposed, \\(g_1\\) will be a biased estimate of \\(b_1\\).\n\\[\\begin{aligned}\n\\hat g_1 &= \\frac{\\sum \\tilde x_1 \\tilde y}{\\sum \\tilde x_1^2}\n         = \\frac{\\sum \\tilde x_1 (b_1 \\tilde x_1 +\\tilde b_2 \\tilde x_2 + e) }{\\sum \\tilde x_1^2} \\\\\n         &= \\frac{b_1 \\sum \\tilde x_1^2}{\\sum \\tilde x_1^2}\n          + b_2 \\frac{\\sum \\tilde x_1\\tilde x_2}{\\sum \\tilde x_1^2}\n          +\\frac{\\sum \\tilde x_1 e}{\\sum \\tilde x_1^2} \\\\\n         &= b_1+b_2 \\delta_1 +\\frac{\\sum \\tilde x_1 e}{\\sum \\tilde x_1^2} \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#section-6",
    "href": "rmethods/3_MLRM.html#section-6",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "This implies that \\(g_1\\) is biased:\n\\[E(\\hat g_1) = b_1+b_2 \\delta_1\n\\]\nWhere \\(\\delta_1\\) is the coefficient in \\(x_2=\\delta_0+\\delta_1 x_1 + v\\).\nImplications:\n\nUnless\n\n\\(\\delta_1\\) is zero (\\(x_1\\) and \\(x_2\\) are linearly independent) or,\n\\(b_2\\) is zero (\\(x_2\\) was irrelevant)\n\nignoring \\(x_2\\) will generate biased (and inconsistent) estimates for \\(b_1\\).\n\nIn models with more controls, the direction of the biases will be harder to define, but similar rule’s of thumb can be used."
  },
  {
    "objectID": "rmethods/3_MLRM.html#adding-irrelevant-controls",
    "href": "rmethods/3_MLRM.html#adding-irrelevant-controls",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Adding irrelevant controls",
    "text": "Adding irrelevant controls\nAdding irrelevant controls will have no effect on bias and consistency.\nif your model is:\n\\[y=b_0+b_1 x_1 +e\n\\]\nbut you estimate:\n\\[y=g_0+g_1 x_1+g_2 x_2 +v\n\\]\nyour model is still unbiased:\n\\[\\begin{aligned}\ng &= (X'X)^{-1}X'(X \\beta^+ + e) \\\\\n    \\beta^+ &= [\\beta \\ ; 0] \\\\\ng &=  \\beta^+ + (X'X)^{-1}X'e \\rightarrow E(g) = \\beta^+\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#adding-bad-controls",
    "href": "rmethods/3_MLRM.html#adding-bad-controls",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Adding “bad” Controls",
    "text": "Adding “bad” Controls\nThe worst case, yet hard to see, is when you add “bad” Controls, also known as Colliers.\nFor example:\n\nSay you want to analyze the effect of education on wages, and you control for occupation. Will it create an unbiased estimate for education?\n\nNo. Your education affects your occupation choice. So some of the effect of education will be “absorbed” by occupation.\n\nSay you want to see the impact of health expenditure on health, and you control for “#visits to the doctor”\n\nThis may also affect your estimates, as expenditure may change how many times you Visits are highly related.\n\n\nIn general, you want to avoid using “channels” as Controls."
  },
  {
    "objectID": "rmethods/3_MLRM.html#what-about-standard-errors",
    "href": "rmethods/3_MLRM.html#what-about-standard-errors",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "What about Standard Errors",
    "text": "What about Standard Errors\n\nCase 1Case 2Case 3\n\n\nOmitting relevant variables that are correlated to \\(X's\\)\nWe wont talk about this. It violates A4, and creates endogeneity\n\n\nOmitting relevant variables that are uncorrelated to \\(X's\\)\n\nOmitted variables will be in the error \\(e\\). Thus variance of coefficients will be larger\n\n\\[\\begin{aligned}\nTrue: & y = b_0 + b_1 x_1 + b_2 x_2 + e  \\\\\nEstimated: & y = g_0 + g_1 x_1 + v   \\\\\n& Var(e)&lt;Var(v) \\rightarrow Var(b_1)&lt;Var(g_1)\n\\end{aligned}\n\\]\nThus Adding controls in Randomized experiements is still a good idea!\n\n\nAdding Irrelevant controls (related to X’s)\nCoefficients are unbiased, and \\(\\sigma^2_e\\) will also be unbiased.\nHowever, you may increase Multicolinearity in the model increasing \\(R_j^2\\) and \\(VIF_j\\).\nVariance of relevant coefficients will be larger.\n\\[\\begin{aligned}\nTrue: & y = b_0 + b_1 x_1  + e  \\\\\nEstimated: & y = g_0 + g_1 x_1 + g_2 x_2 + v   \\\\\n& Var(b_1)&lt;Var(g_1)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#prediction",
    "href": "rmethods/3_MLRM.html#prediction",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Prediction",
    "text": "Prediction\n\nYou can use MLRM to obtain predictions of outcomes.\nThey will be subject to the model specification.\nFor prediction you do not need to worry about “endogeneity” as much. Just on Predictive power (how ??)\n\n\nqui:frause oaxaca, clear\ngen wage = exp(lnwage)\nqui:reg wage educ female age agesq single married\npredict wage_hat\nlist wage wage_hat educ female age agesq single married in 1/5\n\n(213 missing values generated)\n(option xb assumed; fitted values)\n\n     +----------------------------------------------------------------------+\n     |     wage   wage_hat   educ   female   age   agesq   single   married |\n     |----------------------------------------------------------------------|\n  1. | 41.80602   25.61872      9        1    37    1369        1         0 |\n  2. | 36.63003   31.70813      9        0    62    3844        0         1 |\n  3. | 23.54788   30.71257   10.5        1    40    1600        0         1 |\n  4. | 29.76191    42.7976     12        0    55    3025        0         0 |\n  5. | 44.95504   35.76914     12        0    36    1296        0         1 |\n     +----------------------------------------------------------------------+"
  },
  {
    "objectID": "rmethods/3_MLRM.html#efficient-market",
    "href": "rmethods/3_MLRM.html#efficient-market",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Efficient Market",
    "text": "Efficient Market\n\nWe could use MLRM to test theories, like the Efficient Market Theory.\nFor housing, the Assessed price of a house should be all information needed to assess the price of the house. (other ammenities should not matter)\n\nfrause hprice1, clear\nqui:reg price assess bdrms llotsize lsqrft colonial\nmodel_display\nprice_hat = 206.645 + 1.007 assess + 11.404 bdrms + 1.363 llotsize - 38.335 lsqrft + 9.297 colonial\nN= 88 R2=0.831\nqui:reg lprice lassess bdrms llotsize lsqrft colonial\nmodel_display\nlprice_hat = 0.210 + 1.036 lassess + 0.025 bdrms + 0.008 llotsize - 0.092 lsqrft + 0.045 colonial\nN= 88 R2=0.777"
  },
  {
    "objectID": "rmethods/3_MLRM.html#testing-for-discrimination-cp",
    "href": "rmethods/3_MLRM.html#testing-for-discrimination-cp",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Testing for Discrimination (CP)",
    "text": "Testing for Discrimination (CP)\n\nWe could test for discrimination: Unexplained differences in outcomes once other factors are kept fixed.\nIt does require that groups are similar in terms of unobservables.\n\nqui: frause oaxaca, clear\nqui:reg lnwage female \nmodel_display\nlnwage_hat = 3.440 - 0.173 female\nN= 1434 R2=0.027\nqui:reg lnwage female educ age agesq single married exper tenure\nmodel_display\nlnwage_hat = 0.383 - 0.160 female + 0.064 educ + 0.113 age - 0.001 agesq - 0.072 single - 0.094 married - 0.000 exper + 0.007 tenure\nN= 1434 R2=0.345"
  },
  {
    "objectID": "rmethods/3_MLRM.html#treatment-evaluation",
    "href": "rmethods/3_MLRM.html#treatment-evaluation",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Treatment Evaluation",
    "text": "Treatment Evaluation\n\nUnder Random Assingment SRM was enough to estimate ATTs.\nBut if assigment was conditionally random, a better approach would be using MLRM\n\nfrause jtrain98, clear\nqui:reg earn98 train \nmodel_display\nearn98_hat = 10.610 - 2.050 train\nN= 1130 R2=0.016\nqui:reg earn98 train earn96 educ age married\nmodel_display\nearn98_hat = 4.667 + 2.411 train + 0.373 earn96 + 0.363 educ - 0.181 age + 2.482 married\nN= 1130 R2=0.405"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#introduction",
    "href": "rmethods/5_FXMRA.html#introduction",
    "title": "Multiple Regression Analysis",
    "section": "Introduction",
    "text": "Introduction\n\nMultiple Linear Regression models (MLRM), estimated via OLS, have very good properties, if all Assumptions (A1-A5,A6’) Hold.\nUp until now, we have discussed how to estimate them, and analyze them under “optimal” assumptions, in simplified cases.\nToday we will be adding other “minor” Features to MLR, and aim to better understand its features"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#scaling-and-shifting",
    "href": "rmethods/5_FXMRA.html#scaling-and-shifting",
    "title": "Multiple Regression Analysis",
    "section": "Scaling and shifting",
    "text": "Scaling and shifting\n\nSomething that we do not emphasize enough. Before analyzing your data, its important to analyze the nature of the data (summary stats, ranges, scales)\nWhen I talk about Scaling and shifting, I refer exclusibly to affine transormations of the following type:\n\n\\[x^* = a*x+c \\text{ or } x^* = a*(x+c1)+c2\n\\]\nThey either Shift, or change the scale of the data. Not the shape! (logs change shape)\n\nIf one applies affine transformations to the data, it will have NO effect on your model what-so-ever. (Same t’s same F’s, same \\(R^2\\))\nBut, your \\(\\beta's\\) will change. This could help understading and explaining the results."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#example",
    "href": "rmethods/5_FXMRA.html#example",
    "title": "Multiple Regression Analysis",
    "section": "Example:",
    "text": "Example:\n\n\nCode\nset linesize 255\nfrause bwght, clear\ngen bwkg = bwghtlbs*0.454\ngen bwgr = bwkg*1000\nregress bwght male white cigs lfaminc \nest sto m1\nregress bwghtlbs male white cigs lfaminc\nest sto m2\nregress bwkg male white cigs lfaminc\nest sto m3\nregress bwgr male white cigs lfaminc\nest sto m4\n\n\n\nBirthweight and Cig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOz\n\nlbs\n\nKgs\n\nGr\n\n\n\n\n\nmale\n3.123***\n\n0.195***\n\n0.089***\n\n88.605***\n\n\n\n\n(1.071)\n[2.916]\n(0.067)\n[2.916]\n(0.030)\n[2.916]\n(30.389)\n[2.916]\n\n\nwhite\n5.404***\n\n0.338***\n\n0.153***\n\n153.346***\n\n\n\n\n(1.392)\n[3.882]\n(0.087)\n[3.882]\n(0.039)\n[3.882]\n(39.497)\n[3.882]\n\n\ncigs\n-0.480***\n\n-0.030***\n\n-0.014***\n\n-13.628***\n\n\n\n\n(0.091)\n[-5.288]\n(0.006)\n[-5.288]\n(0.003)\n[-5.288]\n(2.577)\n[-5.288]\n\n\nlfaminc\n1.053*\n\n0.066*\n\n0.030*\n\n29.867*\n\n\n\n\n(0.632)\n[1.664]\n(0.040)\n[1.664]\n(0.018)\n[1.664]\n(17.946)\n[1.664]\n\n\n_cons\n110.603***\n\n6.913***\n\n3.138***\n\n3138.351***\n\n\n\n\n(2.071)\n[53.410]\n(0.129)\n[53.410]\n(0.059)\n[53.410]\n(58.760)\n[53.410]\n\n\nN\n1388\n\n1388\n\n1388\n\n1388\n\n\n\nR2\n0.046\n\n0.046\n\n0.046\n\n0.046"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#scaling-xs-and-ys",
    "href": "rmethods/5_FXMRA.html#scaling-xs-and-ys",
    "title": "Multiple Regression Analysis",
    "section": "Scaling X’s and Y’s",
    "text": "Scaling X’s and Y’s\n\nRe-scaling \\(y\\) will affect the all coefficients.\n\nReducing Scale, reduces scale of coefficients\n\nRe-scaling \\(x's\\) will only affect its coefficient and possible the constant.\n\nReducing (increasing) Scale will increase (reduce) Scale of coefficient\n\nIn both cases, Shifting the variable only affects the constant.\n\n\n\n\n\n\n\nImportant\n\n\nRe-Scaling is an important tool/trick that can be used for interpreting more complex models."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#beta-or-standardized-coefficients",
    "href": "rmethods/5_FXMRA.html#beta-or-standardized-coefficients",
    "title": "Multiple Regression Analysis",
    "section": "Beta or Standardized Coefficients",
    "text": "Beta or Standardized Coefficients\n\nIn some fields (health), making inferences based on default scales can be difficult (the impact of 1microgram ?).\nTo avoid this type of problem researchers may opt to use Standardized or Beta coefficients.\n\nHow a \\(sd\\) change in \\(X's\\) affect the outcome (in \\(sd\\))\n\nGetting these coefficient is similar to applying the following transformation to all variables:\n\n\\[\\tilde w = \\frac{w-\\bar w}{\\sigma_w} \\rightarrow E(\\tilde w)=0 \\text{ and } Var(\\tilde w) = 1\n\\]\nreg y x1 x2 x3, beta\nest sto m1\nesttab m1, beta \n\nIt also helps you make comparison of the relative importance of each covariate explanatory power."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-single-dummies",
    "href": "rmethods/5_FXMRA.html#functional-forms-single-dummies",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Single Dummies",
    "text": "Functional Forms: Single Dummies\n\nDummies are variables that take only two values (preferably 0 and 1).\nThey are used to capture qualitative (binary) characteristics (ie Democrat, Union worker, etc)\nWhen used in regression analysis, they represent “shifts” in the Intercept: \\[y = b_0 + b_1 male + b_2 x_1 + b_3 x_2 + e\n\\]\n\nHere, \\(b_0\\) would be the “intercept” for “women” (base) while \\(b_0+b_1\\) would be the intercept for men.\n\nUnder A4, \\(b_1\\) is the expected outcome difference men have over women, everything else constant.\n\n\nUnless further restrictions are used, you can’t add Dummies for both categories in the model.\n\n* Stata Code\nreg y x1 x2 d    &lt;-- Possible if d = 0 or 1\nreg y x1 x2 i.d  &lt;-- Better"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-multiple-dummies",
    "href": "rmethods/5_FXMRA.html#functional-forms-multiple-dummies",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Multiple Dummies",
    "text": "Functional Forms: Multiple Dummies\n\nWe can use dummies to represent multiple (nonoverlapping) characteristics like Race, ranking or age group).\nOne needs a “base” or comparison group to analyze coefficients (or more).\nOrdered variables can be used as continuous, but using them as dummies requires creating dummies for each category.\n\n\\[\\begin{aligned}\ny &= b_0 + b_1 black + b_2 hispanic + b_3 other + b_4 x + e & || Base = White \\\\\ny &= b_0 + b_1 young + b_2 old + b_3 x + e & || Base = Adult\n\\end{aligned}\n\\]\n\nWhen using with ordered data, multiple dummies may create somewhat counterintuitive results\n\ntab race, gen(race_)  &lt;- creates dummies\nreg y i.race x1 x2 x3 &lt;- generally uses first group as base\nreg y ib2.race x1 x2 x3 &lt;- indicates a particular \"base\""
  },
  {
    "objectID": "rmethods/5_FXMRA.html#example-1",
    "href": "rmethods/5_FXMRA.html#example-1",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\n\nfrause beauty, clear\n** Union also a dummy. \n** looks as Continous\nqui:reg lwage exper union educ female looks\nest sto m1\ngen looks_good = looks&gt;=4 if !missing(looks)\nqui:reg lwage exper union educ female looks_good\nest sto m2\nqui:reg lwage exper union educ female i.looks\nest sto m3\nqui:reg lwage exper union educ female ib3.looks\nest sto m4\nesttab m1 m2 m3 m4, se star( * 0.1 ** 0.05 *** 0.01  ) nogaps nomtitle\ndisplay _n \"Exact Change Union : \" %5.3f (exp(_b[union])-1)*100 \"%\"\n\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n----------------------------------------------------------------------------\nexper              0.0137***       0.0134***       0.0135***       0.0135***\n                (0.00119)       (0.00120)       (0.00120)       (0.00120)   \nunion               0.201***        0.201***        0.196***        0.196***\n                 (0.0305)        (0.0307)        (0.0306)        (0.0306)   \neduc               0.0737***       0.0750***       0.0735***       0.0735***\n                (0.00528)       (0.00528)       (0.00528)       (0.00528)   \nfemale             -0.448***       -0.450***       -0.446***       -0.446***\n                 (0.0293)        (0.0294)        (0.0293)        (0.0293)   \nlooks              0.0555***                                                \n                 (0.0201)                                                   \nlooks_good                         0.0276                                   \n                                 (0.0299)                                   \n1.looks                                                 0          -0.266** \n                                                      (.)         (0.134)   \n2.looks                                             0.146          -0.121***\n                                                  (0.139)        (0.0439)   \n3.looks                                             0.266**             0   \n                                                  (0.134)             (.)   \n4.looks                                             0.264*       -0.00255   \n                                                  (0.136)        (0.0312)   \n5.looks                                             0.422**         0.156   \n                                                  (0.173)         (0.111)   \n_cons               0.408***        0.565***        0.338**         0.604***\n                 (0.0968)        (0.0774)         (0.149)        (0.0781)   \n----------------------------------------------------------------------------\nN                    1260            1260            1260            1260   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01\n\nExact Change Union : 21.598%"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-logarithms",
    "href": "rmethods/5_FXMRA.html#functional-forms-logarithms",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Logarithms",
    "text": "Functional Forms: Logarithms\n\nUsing Logarithms can help modeling some nonlinearities in the data.\nBecause it changes the “shape” of variables, it also changes the interpretation (Changes vs %Changes)\nBy reducing dispersion of dep. variable, CLM assumptions may hold.\n\nBut:\n\nCannot or should not be applied to all data types (ie Dummies, negatives, shares)\n(log-lin model): It is often better to use the exact percentage change rather than approximation: \\[\\begin{aligned}\nlog(y) &= b_0 + b_1 x_1 + b_2 x_2 + b_3 D + e \\\\\n\\frac{\\% \\Delta y}{\\Delta D} &= 100 (exp(b_3)-1)\\%\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-polynomials-x2-x3-etc",
    "href": "rmethods/5_FXMRA.html#functional-forms-polynomials-x2-x3-etc",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Polynomials (\\(x^2, x^3, etc\\))",
    "text": "Functional Forms: Polynomials (\\(x^2, x^3, etc\\))\n\nUp to this point, we have only considered linear models (\\(X's\\) enter asis or in logs). This almost always works! (Taylor expansion justification)\n\nSpecially if interested in Average Effects\n\nSome times, you may be interest in capturing some heterogeneity for \\(dy/dx\\). That can be done just adding “ANY” transformation of \\(X\\) in the model (\\(sin(x), 1/x, \\sqrt x\\), etc)\nFor practical, and theoretical purposes, however, we usually concentrate on quadratic terms.\n\nFor example: Increasing returns with decreasing marginal returns\nWe may be interested in “turning” points\n\nHowever, we now need to be careful about marginal effects!"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section",
    "href": "rmethods/5_FXMRA.html#section",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "\\[\\begin{aligned}\ny &=b_0+b_1 x_1 + b_2 x_1^2 + b_3 x_2 + e \\\\\n\\frac{dy}{dx_1} &= b_1+2b_2 x_1 =0 \\\\\nx_1^* &= - \\frac{b_1}{2b_2} x_1\n\\end{aligned}\n\\]\n\n\n\n\n\n\nTo consider\n\n\n\nMarginal effects are no longer constant. You need an \\(x_1\\) value to obtain them (mean? average?)\nWith Quadratic models, there is ALWAYS a turning point (but may not be relevant)\nMFX can be positive or negative for some value of \\(x_1\\) (but may not be relevant)\nUnless something else is done, coefficients may not make sense on their own.\n\n\n\n\n\nWhy not add further polynomials?\n\nEstimating them is easy (except for numerical precision), but adds complexity for interpretation. Nothing else."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#example-2",
    "href": "rmethods/5_FXMRA.html#example-2",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\n\nfrause hprice2, clear\ngen rooms2=rooms*rooms\nqui:reg lprice lnox dist rooms \nest sto m0\nqui:reg lprice lnox dist rooms rooms2\nest sto m1\nqui:reg lprice lnox dist c.rooms c.rooms#c.rooms\nest sto m2\nesttab m0 m1 m2, se varwidth(20) star(* 0.1 ** 0.05 *** 0.01) nogaps\n\n\n--------------------------------------------------------------------\n                              (1)             (2)             (3)   \n                           lprice          lprice          lprice   \n--------------------------------------------------------------------\nlnox                       -0.968***       -0.975***       -0.975***\n                          (0.110)         (0.106)         (0.106)   \ndist                      -0.0291***      -0.0223**       -0.0223** \n                         (0.0102)       (0.00995)       (0.00995)   \nrooms                       0.302***       -0.724***       -0.724***\n                         (0.0189)         (0.171)         (0.171)   \nrooms2                                     0.0794***                \n                                         (0.0131)                   \nc.rooms#c.rooms                                            0.0794***\n                                                         (0.0131)   \n_cons                       9.793***        13.05***        13.05***\n                          (0.271)         (0.599)         (0.599)   \n--------------------------------------------------------------------\nN                             506             506             506   \n--------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01\n\n\n\nNegative coefficient for \\(rooms\\), so is there a problem?\n\nFind “turnpoint” and summary Stats\n\n\nTurn point: 4.55\n\n\n\n    Variable |       Min        p1        p5       p10       p25       p50       p75       p90       p99       Max\n-------------+----------------------------------------------------------------------------------------------------\n       rooms |      3.56      4.52       5.3      5.59      5.88      6.21      6.62      7.15      8.34      8.78\n------------------------------------------------------------------------------------------------------------------\n\n\n\nDoes it make a difference how we estimate the model?\n\n\nqui:reg lprice lnox dist rooms rooms2\nmargins, dydx(rooms)\nqui:reg lprice lnox dist c.rooms c.rooms#c.rooms\nmargins, dydx(rooms) \n\n\nAverage marginal effects                                   Number of obs = 506\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  rooms\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       rooms |  -.7236433   .1706763    -4.24   0.000    -1.058973   -.3883139\n------------------------------------------------------------------------------\n\nAverage marginal effects                                   Number of obs = 506\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  rooms\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       rooms |   .2747106   .0188463    14.58   0.000     .2376831    .3117382\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-interactions-i-d1d2",
    "href": "rmethods/5_FXMRA.html#functional-forms-interactions-i-d1d2",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Interactions I (\\(d1*d2\\))",
    "text": "Functional Forms: Interactions I (\\(d1*d2\\))\n\nIt is possible to use multiple (unrelated) dummy variables.\nDummy interactions are feasible to allow for differential means across groups combined groups.\nYou still need a reference group that should be identified: \\[\\begin{aligned}\ny &= a_0 + a_1 female + a_2 union + a_3 female \\times union + e \\\\\ny &= b_0 + b_1 female \\times nonunion + b_2 male \\times union +b_3 female \\times union + e\n\\end{aligned}\n\\] Both models are equivalent. Also \\[\\begin{aligned}\n& E(y|male,nonunion)    && =a_0  &&= b_0   \\\\\n& E(y|female,nonunion) && = a_0 + a_1 && = b_0 + b_1 \\\\\n& E(y|male,union) && =a_0+a_2  &&= b_0+b_2 \\\\\n& E(y|female,union)  && = a_0 + a_1 + a_2 + a_3  &&= b_0 + b_3\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-1",
    "href": "rmethods/5_FXMRA.html#section-1",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "For this, you may need to use manual dummy creation, or use explicit interactions:\nreg y i.d1 i.d2 i.d1#i.d2\nreg y i.d1##i.d2\nreg y i.d1#i.d2\n\nYou set the interactions\nSimilar to one, but Stata does it for you\nCreates full set of interactions, as in 2nd model before\n\nOptions 1 and 3 will allow you using margins. For overall groups (all women, all unions) you need to decide how to get representative samples."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-interactions-ii-x1x2",
    "href": "rmethods/5_FXMRA.html#functional-forms-interactions-ii-x1x2",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Interactions II (\\(x1*x2\\))",
    "text": "Functional Forms: Interactions II (\\(x1*x2\\))\n\nYou may be interested in allowing for some interaction across continuous variables.\n\nie Interacted effect of household size and number of bedrooms\n\nAs with Polynomials, this allows for heterogeneity, thus effects are not constant.\n\n\\[\\begin{aligned}\ny &= a_0 + a_1 x_1 + a_2 x_2 + a_3 x_1 x_2 + e \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_1} &= a_1  + a_3 x_2 \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_2} &= a_2  + a_3 x_1\n\\end{aligned}\n\\]\n\nThus, coefficients, on their own, are difficult to interpret, unless \\(x_1\\) or \\(x_2\\) are zero"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-2",
    "href": "rmethods/5_FXMRA.html#section-2",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Shortcut: Affine transformation\n\nThere is a trick that could help easy and direct interpretation. re-scaling variables:\n\n\\[\\begin{aligned}\ny &= b_0 + b_1 x_1 + b_2 x_2 + b_3 (x_1-\\bar x_1)(x_2-\\bar x_2) + e \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_1} &= b_1  + b_3 (x_2-\\bar x_2) \\simeq b_1 \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_2} &= b_2  + b_3 (x_1-\\bar x_1) \\simeq b_2\n\\end{aligned}\n\\]\n\nAlso works with quadratic terms!\n\n\\[\\begin{aligned}\ny &= b_0 + b_1 x_1 + b_2 (x_1-\\bar x_1)^2 + b_3 x_2 + e \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_1} &= b_1  + 2 b_2 (x_1-\\bar x_1) \\simeq b_1 \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-interactions-iii-d1x1",
    "href": "rmethods/5_FXMRA.html#functional-forms-interactions-iii-d1x1",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Interactions III (\\(d1*x1\\))",
    "text": "Functional Forms: Interactions III (\\(d1*x1\\))\n\nDummy variables allows for shifts to the constant (intercept).\nInteracting with continuous variables allows for shifts in slopes!.\n\nThis can be useful to testing hypothesis: differences in returns to education by gender.\n\n\n\\[wage=b_0 + b_1 female + b_2 educ + b_3 educ \\times female + e\n\\]\n\n\\(b_1\\): Baseline wage differential between men and women.\n\\(b_2+b_3\\): Returns to education for women.\n\\(b_1 + b_3 \\overline{educ}\\): Average wage difference between men and women.\n\nStata:\nreg y x1 i.d c.x1#i.d"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-full-interactions-with-dummies",
    "href": "rmethods/5_FXMRA.html#functional-forms-full-interactions-with-dummies",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Full Interactions (with dummies)",
    "text": "Functional Forms: Full Interactions (with dummies)\n\nIt is possible to estimate models where all variables are interacted with a single dummy. This allows you to test the hypothesis if two groups have the same underlying parameters.\n\nDo men and women have the same wage structure?\n\nFull interactions is equivalent to estimating separate models:\n\n\\[\\begin{aligned}\nFT: & y = b_0 + b_1 x_1 + b_2 x_2 + g_0 d +g_1 x_1 d +g_2 x_2 d +e \\\\\nD0: & y = b_0 + b_1 x_1 + b_2 x_2  +e  && \\text{ if d=0 } \\\\\nD1: & y = (b_0+g_0) + (b_1+g_1) x_1 + (b_2+g_2) x_2 +e && \\text{ if d=1 } \\\\\n    & y = a_0 + a_1 x_1 + a_1 x_2 +e && \\text{ if d=1 } \\\\\nCS1: & H_0: g_0=g_1=g_2=0\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-3",
    "href": "rmethods/5_FXMRA.html#section-3",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Chow test\n\n\\(CS1\\) can be tested using F-stat for multiple hypothesis.\nBut, under homoskedasticty, one could also use what is known as the Chow test\n\n\\[\\begin{aligned}\nM1 &: y = b_0 + b_1 x_1 + b_2 x_2 + e \\\\\nM2 &: y = b_0 + b_1 x_1 + b_2 x_2 + b_3 d + e \\\\\nif \\ D=0 &: y = b_{00} + b_{01} x_1 + b_{02} x2 + e_0 \\\\\nif \\ D=1 &: y = b_{10} + b_{11} x_1 + b_{12} x2 + e_1\n\\end{aligned}\n\\]\nF-Stat (similar to before):\n\\[\\begin{aligned}\nF_{M1} = \\frac{(SSR_{M1}-SSR_0-SSR_1)/(k+1)}{(SSR_0+SSR_1)/(n - 2(k+1))} \\\\\nF_{M2} = \\frac{(SSR_{M2}-SSR_0-SSR_1)/k}{(SSR_0+SSR_1)/(n - 2(k+1))}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#example-3",
    "href": "rmethods/5_FXMRA.html#example-3",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\n\n frause gpa3, clear\ndrop if cumgpa==0\nreplace sat = sat /100\nqui:reg cumgpa sat hsperc tothrs\nest sto m1\nqui:reg cumgpa sat hsperc tothrs female\nest sto m2\nqui:reg cumgpa sat hsperc tothrs if female==0\nest sto m3\nqui:reg cumgpa sat hsperc tothrs if female==1\nest sto m4\nqui:reg cumgpa i.female##c.(sat hsperc tothrs)\nest sto m5\nesttab m1 m2 m3 m4 m5, mtitle( Simple With_fem Men Women Full_int) ///\nse star(* .1 ** 0.05 *** 0.01) nogaps noomitted \n\n(98 observations deleted)\nvariable sat was int now float\n(634 real changes made)\n\n--------------------------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)             (5)   \n                   Simple        With_fem             Men           Women        Full_int   \n--------------------------------------------------------------------------------------------\nsat                0.0933***       0.0938***       0.0679***        0.177***       0.0679***\n                 (0.0133)        (0.0130)        (0.0151)        (0.0244)        (0.0146)   \nhsperc           -0.00865***     -0.00730***     -0.00748***     -0.00869***     -0.00748***\n                (0.00105)       (0.00106)       (0.00119)       (0.00219)       (0.00116)   \ntothrs          -0.000599       -0.000586        -0.00155**       0.00141        -0.00155** \n               (0.000662)      (0.000647)      (0.000771)       (0.00111)      (0.000748)   \nfemale                              0.277***                                                \n                                 (0.0493)                                                   \n0.female                                                                                0   \n                                                                                      (.)   \n1.female                                                                           -0.855** \n                                                                                  (0.333)   \n1.female#c~t                                                                        0.109***\n                                                                                 (0.0310)   \n1.female#c~c                                                                     -0.00121   \n                                                                                (0.00271)   \n1.female#c~s                                                                      0.00296** \n                                                                                (0.00145)   \n_cons               1.900***        1.782***        2.070***        1.215***        2.070***\n                  (0.149)         (0.147)         (0.173)         (0.257)         (0.168)   \n--------------------------------------------------------------------------------------------\nN                     634             634             483             151             634   \n--------------------------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;.1, ** p&lt;0.05, *** p&lt;0.01\n\n\n\ntest 1.female#c.sat 1.female#c.hsperc 1.female#c.tothrs\ntest 1.female 1.female#c.sat 1.female#c.hsperc 1.female#c.tothrs\nmargins female, dydx(sat hsperc tothrs)\n\n\n ( 1)  1.female#c.sat = 0\n ( 2)  1.female#c.hsperc = 0\n ( 3)  1.female#c.tothrs = 0\n\n       F(  3,   626) =    6.26\n            Prob &gt; F =    0.0003\n\n ( 1)  1.female = 0\n ( 2)  1.female#c.sat = 0\n ( 3)  1.female#c.hsperc = 0\n ( 4)  1.female#c.tothrs = 0\n\n       F(  4,   626) =   12.75\n            Prob &gt; F =    0.0000\n\nAverage marginal effects                                   Number of obs = 634\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  sat hsperc tothrs\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nsat          |\n      female |\n          0  |   .0679302    .014607     4.65   0.000     .0392454    .0966149\n          1  |   .1772582   .0273126     6.49   0.000     .1236229    .2308936\n-------------+----------------------------------------------------------------\nhsperc       |\n      female |\n          0  |    -.00748   .0011573    -6.46   0.000    -.0097526   -.0052073\n          1  |  -.0086922   .0024524    -3.54   0.000    -.0135081   -.0038763\n-------------+----------------------------------------------------------------\ntothrs       |\n      female |\n          0  |  -.0015482   .0007477    -2.07   0.039    -.0030165   -.0000798\n          1  |    .001412   .0012472     1.13   0.258    -.0010371    .0038612\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#avg-partial-effects-vs-partial-effects-at-x_c",
    "href": "rmethods/5_FXMRA.html#avg-partial-effects-vs-partial-effects-at-x_c",
    "title": "Multiple Regression Analysis",
    "section": "Avg Partial effects vs Partial effects at \\(X_c\\)",
    "text": "Avg Partial effects vs Partial effects at \\(X_c\\)\n\nWhenever you have interactions, higher order polynomials (or any nonlinear transformation of \\(X\\)), marginal effects are no longer constant, and may depend on additional information:\n\n\\[y = b_0 + b_1 x_1 + b_2 x_1^2 + e \\rightarrow \\frac{dy}{dx} = b_1 + 2b_2 x_1\n\\]\n\nWhat to do in this cases?\n\nEstimate Average marginal effects: \\(AME = E\\left(\\frac{dy}{dx}\\right) = b_1 + 2b_2 \\overline{x}_1\\)\nEstimate Marginal effects at means: \\(MEM = \\frac{dy}{dx}\\Big|_{x=\\bar x} = b_1 + 2b_2 \\overline{x}_1\\)\nEstimate Marginal effects at relevant values\nReport ALL marginal effects"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-4",
    "href": "rmethods/5_FXMRA.html#section-4",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "In Stata you can do this only for interactions. For constructed variables you need f_able, or do it by hand.\n\nreg y c.x1##c.x1##c.x1\nmargins, dydx(x1) &lt;-- Default is Average Marginal Effects\nmargins, dydx(x1) atmeans &lt;-- Request marginal effects at means\nmargins, dydx(x1) at(x1=(1/5)) &lt;-- Request marginal effects at specific values of x1\n* and plot afterwards\nmarginsplot"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#goodness-of-fit-r2-vs-r2_adj",
    "href": "rmethods/5_FXMRA.html#goodness-of-fit-r2-vs-r2_adj",
    "title": "Multiple Regression Analysis",
    "section": "Goodness of Fit: \\(R^2\\) vs \\(R^2_{adj}\\)",
    "text": "Goodness of Fit: \\(R^2\\) vs \\(R^2_{adj}\\)\nWith Great power…\nIMPORTANT: Low \\(R^2\\) does not mean a bad model, nor high \\(R^2\\) mean a good one.\n\nIf \\(N\\) is constant, adding more variables to your model will increase the Goodness of fit \\(R^2\\) (even if marginally)\n\nThis may lead to the incorrect intuition of choosing models with the highest \\(R^2\\)\nThis is wrong because \\(R^2\\) only measures in-sample fitness.\n\nAlternative, the Adjusted \\(R^2\\) (\\(R_{adj}^2\\)), which penalizes using multiple controls\n\n\\[R^2_{adj} = 1-\\frac{SSR/(n-k-1)}{SST/(n-1)}=1-(1-R^2)\\frac{n-1}{n-k-1}\n\\]\n\nMore controls \\(k\\) will not always increase \\(R^2_{adj}\\)"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-5",
    "href": "rmethods/5_FXMRA.html#section-5",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "\\(R^2_{adj}\\) and Model Selection\n\n\\(R^2_{adj}\\) can be used to choose between nested models.\n\nIf adding variables improves \\(R_{adj}^2\\), then choose that model.\n\nBut it can also be used to choose between non-nested models:\n\n\\[\\begin{aligned}\nM1: & y = b_0 + b_1 x_1 + b_2 x_2 + e \\\\\nM2: & y = b_0 + b_1 x_1 + b_3 x_3 + e \\\\\nM3: & y = b_0 + b_1 ln(x_1) + b_2 ln(x_2) + e \\\\\nM4: & y = b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3 + e\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#log-models-and-prediction",
    "href": "rmethods/5_FXMRA.html#log-models-and-prediction",
    "title": "Multiple Regression Analysis",
    "section": "log models and Prediction",
    "text": "log models and Prediction\n\nTransforming the Depvariable with logs is quite useful for interpretation, and addressing overdispersion\nHowever, obtaining predictions from such models is not straight forward:\n\n\\[\\begin{aligned}\nln(y) &= a_0 + a_1 x_1 + a_2 x_2 + \\varepsilon \\\\\ny &= exp(a_0 + a_1 x_1 + a_2 x_2 + \\varepsilon ) \\\\\nE(y|x_1,x_2) &=E(e^{a_0 + a_1 x_1 + a_2 x_2}) \\times E(e^ \\varepsilon ) \\\\\n& E(e^ \\varepsilon )\\neq 1\n\\end{aligned}\n\\]\n\nTo make Predictions in a log model we need some approximation for \\(E(e^ \\varepsilon )\\)"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-6",
    "href": "rmethods/5_FXMRA.html#section-6",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "We have Options:\nLets call \\(E(e^ \\varepsilon ) = \\alpha_0\\)\nOption 1 : \\(\\alpha_0 = n^{-1} \\sum( \\exp {\\hat\\varepsilon})\\)\nOption 2 : Under Normality of \\(\\varepsilon\\), \\(\\alpha_0 = \\exp(\\hat \\sigma^2/2)\\)\nOption 3 : Call \\(\\hat m = \\exp(a_0 + a_1 x_1 + a_2 x_2)\\).\nRegress \\(y\\) on \\(\\hat m\\) without intercept. \\(\\alpha_0 = \\frac{\\hat m'y}{\\hat m'\\hat m}\\)\n\nYour \\(\\hat y\\) prediction can now be used to estimate a comparable \\(R^2\\)\n\n\\[R^2 = Corr(y,\\hat y)^2 \\text{ or } 1-\\frac{\\sum(y_i-\\alpha_0 \\hat m_i)^2}{\\sum(y-\\bar y)^2}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-7",
    "href": "rmethods/5_FXMRA.html#section-7",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Example\n\nfrause oaxaca, clear\ndrop if lnwage==.\ngen wage = exp(lnwage)\nqui:reg lnwage educ exper tenure female married divorced\npredict lnw_hat\npredict lnw_res, res\n** Case 1:\negen alpha_01 = mean( exp(lnw_res))\n** Case 2:\nqui:sum lnw_res\ngen alpha_02 = exp(r(Var)/2)\ngen elnw_hat = exp(lnw_hat)\nqui: reg wage elnw_hat, nocons\ngen alpha_03 = _b[elnw_hat]\ngen wage_1 = elnw_hat\ngen wage_2 = elnw_hat*alpha_01\ngen wage_3 = elnw_hat*alpha_02\ngen wage_4 = elnw_hat*alpha_03\nmata:  y = st_data(.,\"wage\"); my = mean(y)\nmata:  yh = st_data(.,\"wage_1 wage_2 wage_3 wage_4\")\nmata:\"R2_1 \"; 1 - sum((y:-yh[,1]):^2)/sum( (y:-my):^2 )\nmata:\"R2_2 \"; 1 - sum((y:-yh[,2]):^2)/sum( (y:-my):^2 )\nmata:\"R2_3 \"; 1 - sum((y:-yh[,3]):^2)/sum( (y:-my):^2 )\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n(option xb assumed; fitted values)\n  R2_1 \n  .1569552664\n  R2_2 \n  .1692562931\n  R2_3 \n  .1658805115"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#limited-dependent-variables",
    "href": "rmethods/5_FXMRA.html#limited-dependent-variables",
    "title": "Multiple Regression Analysis",
    "section": "Limited Dependent variables",
    "text": "Limited Dependent variables\n\nSo far, we have impliclity assumed your dep. variable is continuous and unbounded.\nHowever, OLS imposes no distributional assumptions (A6 is more convinience)\nThis means that LRM using OLS can be used for variables with limited distribution!\n\nlike Dummies or count variables"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#linear-probability-model---lpm",
    "href": "rmethods/5_FXMRA.html#linear-probability-model---lpm",
    "title": "Multiple Regression Analysis",
    "section": "Linear Probability Model - LPM",
    "text": "Linear Probability Model - LPM\n\nLPM can be used when the dep.variable is a dummy, and the goal is to explain the Likelihood of something to happen.\n\n\\[\\begin{aligned}\nD &= b_0 + b_1 x_1 + b_2 x_2 +b_3 x_3 + e \\\\\nE(D|Xs) &= P(D=1|Xs) \\\\\n        &= b_0 + b_1 x_1 + b_2 x_2 +b_3 x_3\n\\end{aligned}\n\\]\nNote:\n\nFor marginal effects, we no longer consider effects at the individual level.\nInstead we look into conditional means, and likelihood\n\n\nCode\nfrause mroz, clear\nqui: reg inlf  age educ exper kidsge6 kidslt6 nwifeinc \nmodel_display\n\nE(inlf|X) = 0.707 - 0.018 age + 0.040 educ + 0.023 exper + 0.013 kidsge6 - 0.272 kidslt6 - 0.003 nwifeinc\nN = 753 R^2 = 0.254"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-8",
    "href": "rmethods/5_FXMRA.html#section-8",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Problems with LPM\n\nLPM are easy to estimate and interpret but it has some problems:\n\nPredictions could fall below 0 or above 1 (what does it mean?)\nUnless more flexible functional forms are allowed, mfx are fixed.\nThe model is, by construction, Heteroskedastic:\n\n\n\\[Var(y|x)=p(x)*(1-p(x))\n\\]\nThus SE will be incorrect, affecting inference"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#modeling-count-data",
    "href": "rmethods/5_FXMRA.html#modeling-count-data",
    "title": "Multiple Regression Analysis",
    "section": "Modeling Count Data",
    "text": "Modeling Count Data\n\nYou could also use LRM (via OLS) to model count data.\n\nCount data is always possitive, but with discrete values\n\n\n\\[Children = b_0 + b_1 age + b_2 education + e\\]\n\nNothing changes for estimation, but its useful to change language:\n\n\nCode\nfrause fertil2, clear\nqui reg children age educ\nmodel_display\n\nE(children|X) = -1.997 + 0.175 age - 0.090 educ\nN = 4361 R^2 = 0.560\n\n1 year of education decreases # of children in .09.\n1 year of education decreases Fertility .09 children per women.\nEvery 100 women, If they were 1 year more educated, we would expect to see 9 fewer children among them."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#prediction-policy-and-shifting",
    "href": "rmethods/5_FXMRA.html#prediction-policy-and-shifting",
    "title": "Multiple Regression Analysis",
    "section": "Prediction, Policy and Shifting",
    "text": "Prediction, Policy and Shifting\n\nAs mentioned before, intercepts, or constant in model regressions are usually meaningless.\n\nBecause \\(a_0 = E(y|X=0)\\) (does it make sense)\n\nConstant, however, can be useful if we apply some transformations to the data. \\[y = b_0 +  b_1 (x_1 - c_1) +  b_2 (x_2 - c_2) +  b_3 (x_3 - c_3) +e\n\\]\n\nIn this case \\(b_0\\) is the expected value of \\(y\\) when \\(x_1=c_1\\), \\(x_2=c_2\\) and \\(x_3=c_3\\). Thus, its now Useful!\n\nUsing this affine transformation, we can easily make predictions (and get SE) for any specific values of interest.\n\nGranted, you could also use “margins”\n\n\n\n\nCode\nfrause gpa2, clear\ngen sat0=sat-1200\ngen hsperc0=hsperc-30\ngen hsize0=hsize-5\ngen hsize20=hsize^2-25\nqui:reg colgpa sat hsperc c.hsize##c.hsize\nmargins, at(sat = 1200 hsperc = 30 hsize = 5)\nreg colgpa sat0 hsperc0 hsize0 hsize20\n\n\n\nAdjusted predictions                                     Number of obs = 4,137\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\nAt: sat    = 1200\n    hsperc =   30\n    hsize  =    5\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _cons |   2.700075   .0198778   135.83   0.000     2.661104    2.739047\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     4,137\n-------------+----------------------------------   F(4, 4132)      =    398.02\n       Model |  499.030503         4  124.757626   Prob &gt; F        =    0.0000\n    Residual |  1295.16517     4,132  .313447524   R-squared       =    0.2781\n-------------+----------------------------------   Adj R-squared   =    0.2774\n       Total |  1794.19567     4,136  .433799728   Root MSE        =    .55986\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        sat0 |   .0014925   .0000652    22.89   0.000     .0013646    .0016204\n     hsperc0 |  -.0138558    .000561   -24.70   0.000    -.0149557   -.0127559\n      hsize0 |  -.0608815   .0165012    -3.69   0.000    -.0932328   -.0285302\n     hsize20 |   .0054603   .0022698     2.41   0.016     .0010102    .0099104\n       _cons |   2.700075   .0198778   135.83   0.000     2.661104    2.739047\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-9",
    "href": "rmethods/5_FXMRA.html#section-9",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Policy Evaluation\n\nWhen modeling \\(y = b_0 + \\delta \\ trt + b_1 x_1 + b_2 x_2 + e\\) the treatment effect \\(\\delta\\) was estimated under homogeneity assumption (only intercept shift)\nThis assumption can be relaxed by estimating separate models or using interactions.\nEffects can be estimated manually (separate models), margins or using shifts!\n\nUsing Separate models: \\[\\begin{aligned}\ny &= b^0_0 +  b^0_1 x_1 + b^0_2 x_2 + e^0 \\text{ if trt=0} \\\\\ny &= b^1_0 +  b^1_1 x_1 + b^1_2 x_2 + e^1 \\text{ if trt=1} \\\\\n& ATE = E(\\hat y_1 - \\hat y_0 ) \\\\\n& ATT = E(\\hat y_1 - \\hat y_0 | trt=1) \\\\\n& ATU = E(\\hat y_1 - \\hat y_0 | trt=0)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-10",
    "href": "rmethods/5_FXMRA.html#section-10",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Or using Model Shits\n\\[\\begin{aligned}\ny  &= b_0 + \\delta_{ate} trt + b_1 x_1 + g_1 trt (x_1- E(x_1)) + e \\\\\ny  &= b_0 + \\delta_{att} trt + b_1 x_1 + g_1 trt (x_1- E(x_1|trt=1)) + e \\\\\ny  &= b_0 + \\delta_{atu} trt + b_1 x_1 + g_1 trt (x_1- E(x_1|trt=0)) + e \\\\\n\\end{aligned}\n\\]\n\n\nCode\nfrause jtrain98, clear\nforeach i in earn96 educ age married {\n  sum `i' if train==0, meanonly\n  gen atu_`i' = (`i' - r(mean))*train\n  sum `i' if train==1, meanonly\n  gen att_`i' = (`i' - r(mean))*train\n  sum `i' , meanonly\n  gen ate_`i' = (`i' - r(mean))*train\n}\nqui:reg earn98 train earn96 educ age married\nest sto m1\nqui:reg earn98 train earn96 educ age married ate*\nest sto m2\nqui:reg earn98 train earn96 educ age married atu*\nest sto m3\nqui:reg earn98 train earn96 educ age married att*\nest sto m4\n\nesttab m1 m2 m3 m4, keep(train) mtitle(Homogenous ATE ATU ATT) se\n\n\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n               Homogenous             ATE             ATU             ATT   \n----------------------------------------------------------------------------\ntrain               2.411***        3.106***        3.533***        2.250***\n                  (0.435)         (0.532)         (0.667)         (0.449)   \n----------------------------------------------------------------------------\nN                    1130            1130            1130            1130   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/7_spec.html#what-do-we-mean-with-model-miss-specification",
    "href": "rmethods/7_spec.html#what-do-we-mean-with-model-miss-specification",
    "title": "Multiple Regression Analysis",
    "section": "What do we mean with model miss-specification",
    "text": "What do we mean with model miss-specification\n\nThere are various kinds of model specification we will talk about.\n\nThere are important variables you did not include in your model: Endogeneity\nYou added all relevant variables…just not in the right way.\nYou added proxies for variables you had no access to (Question change)\nYou have all relevant data, but with errors.\nYou have some missing data"
  },
  {
    "objectID": "rmethods/7_spec.html#section",
    "href": "rmethods/7_spec.html#section",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Simple linear functions work in almost ALL cases. They can be thought as first order Taylor expansions: \\[\\begin{aligned}\ny &= f(x) + e \\\\\nf(x) &\\simeq f(x_0)\n+\\frac{\\partial f(x)}{\\partial x}|_{x=x_0}\n(x-x_0)+R+e \\\\\nf(x) &\\simeq \\color{red}{ f(x_0)}\n\\color{red}{-\\frac{\\partial f(x)}{\\partial x}|_{x=x_0} x_0}\n+\\frac{\\partial f(x)}{\\partial x}|_{x=x_0} x+R+e \\\\\ny &= \\color{red}{\\beta_0}+\\beta_1 x + R+ e\n\\end{aligned}\n\\]\n\nSo, for “reasonable” values of X, or when analyzing average marginal effects \\(R\\) should be small enough to be ignored.\n\nIn other words, for Overall effects Simple linear model works reasonably well! (most of the time)"
  },
  {
    "objectID": "rmethods/7_spec.html#section-1",
    "href": "rmethods/7_spec.html#section-1",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "If you are interested in individuals (or alike people), you may need flexiblity!\nIgnoring functional form misspecification imposes unwanted assumptions (homogeneity), that could create further problems.\n\nSpecially if data is skewed\n\nBut how flexible is flexible enough?\n\nWe will only consider quadratic terms and interactions,\nbut there is a large literature on making very flexible estimations (non-paramatric analysis)\n\n\n\n\nCode\nclear\nset seed 10\nset obs 1000\ngen p = (2*_n-1)/(2*_N) \ngen x = invchi2(5, p)/2\ngen y = 1 + x + (x-2.5)^2 + rnormal()  \nreg y x\ndisplay \"Quadratic\"\nqui:reg y c.x##c.x\nmargins, dydx(x)\ndisplay \"Cubic\"\nqui:reg y c.x##c.x##c.x\nmargins, dydx(x)\n\n\nNumber of observations (_N) was 0, now 1,000.\n\n      Source |       SS           df       MS      Number of obs   =     1,000\n-------------+----------------------------------   F(1, 998)       =   1287.16\n       Model |  22189.0552         1  22189.0552   Prob &gt; F        =    0.0000\n    Residual |  17204.2788       998  17.2387563   R-squared       =    0.5633\n-------------+----------------------------------   Adj R-squared   =    0.5628\n       Total |  39393.3339       999  39.4327667   Root MSE        =     4.152\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |   2.983973   .0831722    35.88   0.000      2.82076    3.147185\n       _cons |  -1.467351   .2458875    -5.97   0.000    -1.949866   -.9848348\n------------------------------------------------------------------------------\nQuadratic\n\nAverage marginal effects                                 Number of obs = 1,000\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  x\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |   1.033401   .0258934    39.91   0.000     .9825894    1.084213\n------------------------------------------------------------------------------\nCubic\n\nAverage marginal effects                                 Number of obs = 1,000\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  x\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |   1.041387   .0292892    35.56   0.000     .9839117    1.098863\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/7_spec.html#reset-ramsey-test",
    "href": "rmethods/7_spec.html#reset-ramsey-test",
    "title": "Multiple Regression Analysis",
    "section": "Reset Ramsey test",
    "text": "Reset Ramsey test\n\nIntuition: If the model is misspecified, perhaps we need to control for more non-linearities and interactions.\nNaive test: Add more controls (quadratics and interactions) (like White test, this will grow fast)\nReset - Ramsey test: Get predictions from original model, and add it as control\n\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\delta_1 \\hat y^2 + \\delta_2 \\hat y^3 +e\n\\]\n\\(H_0: \\delta_1 = \\delta_2 = 0\\): (everything is awesome)\n\\(H_1: H_0\\) is false: we need to fix the problem\n\nRRT does not tell you “How” to fix the problem.\n\nestat ovtest\n(bad name tho)"
  },
  {
    "objectID": "rmethods/7_spec.html#davidson-mackinnon-test",
    "href": "rmethods/7_spec.html#davidson-mackinnon-test",
    "title": "Multiple Regression Analysis",
    "section": "Davidson-MacKinnon test",
    "text": "Davidson-MacKinnon test\nTwo non-tested models:\n\\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + e \\\\\ny &= \\gamma_0 + \\gamma_1 log(x_1) + \\gamma_2 log(x_2) + e \\\\\n\\end{aligned}\n\\]\n\nWhich one is more appropriate? eq1? or eq2? This are non-nested models, so its difficult to say.\n\nYou could nest them:\n\n\n\\[y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 log(x_1) + \\theta_4 log(x_2) + e\n\\]\nand test \\(\\theta_1=\\theta_2=0\\) or \\(\\theta_3=\\theta_4=0\\)."
  },
  {
    "objectID": "rmethods/7_spec.html#section-2",
    "href": "rmethods/7_spec.html#section-2",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "or the “true” Davidson-MacKinnon test:\n\nFirst Obtain predictions from competing models: \\[\\begin{aligned}\n\\hat y &= \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 \\\\\n\\check y &= \\hat \\gamma_0 + \\hat\\gamma_1 log(x_1) + \\hat\\gamma_2 log(x_2) \\\\\n\\end{aligned}\n\\]\nThen add the predictions as added controls in the alternative model: \\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\theta_1 \\check y +e \\\\\ny &= \\gamma_0 + \\gamma_1 log(x_1) + \\gamma_2 log(x_2) + \\theta_1 \\hat y + e \\\\\n\\end{aligned}\n\\]\n\nUnfortunately, you may ended up with conflicting results."
  },
  {
    "objectID": "rmethods/7_spec.html#a-re-tell-of-omitted-variable-bias",
    "href": "rmethods/7_spec.html#a-re-tell-of-omitted-variable-bias",
    "title": "Multiple Regression Analysis",
    "section": "A re-tell of Omitted variable Bias",
    "text": "A re-tell of Omitted variable Bias\n\nWe know this. If a variable that SHOULD be in the model is not added, it will generate an OMV, unless it was uncorrelated to the model error.\n\nLesson: add important variables!\n\nWhat if those variables are not available? how do you solve the problem?\n\nIV (we will talk about that later) or\nProxy Variable (a bandaid)"
  },
  {
    "objectID": "rmethods/7_spec.html#proxies",
    "href": "rmethods/7_spec.html#proxies",
    "title": "Multiple Regression Analysis",
    "section": "Proxies",
    "text": "Proxies\nConsider: \\[log(wages) = \\beta_0 + \\color{blue}{\\beta_1} exper + \\color{blue}{\\beta_2} educ + \\beta_3 skill + e\n\\]\nWhere you are really interested in \\(\\beta_1 \\And \\beta_2\\).\n\nSince we dont have \\(skill\\), and omitting it will bias our coefficients, we can use a proxy \\(ASVAB\\).\n\n\\[log(wages) = \\beta_0 + \\color{blue}{\\beta_1} exper + \\color{blue}{\\beta_2} educ + \\gamma_3 ASVAB + e\n\\]\n\nand done?"
  },
  {
    "objectID": "rmethods/7_spec.html#section-3",
    "href": "rmethods/7_spec.html#section-3",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Using a Proxy will work only under the following condition:\n\nConditioning on the observed variable and proxy, the unobserved variable has to be uncorrelated to other variables in the model:\n\n\\[\\begin{aligned}\nE(x_3^*|x_1,x_2,x_3)&=\\alpha_0 + \\alpha_1 x_3 \\\\\nE(skill|exper,educ,ASVAB)&=\\alpha_0 + \\alpha_1 ASVAB\n\\end{aligned}\n\\]\nIf this happens, you can still estimate \\(\\beta_1 \\And \\beta_2\\), although the constant and slope of the proxy varible will be biased for the proxied variable.\n\\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x^*_3 + e \\ ; \\\n\\color{blue}{x^*_3 =  \\delta_0 + \\delta_1 x_3 + v} \\\\\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (\\delta_0 + \\delta_1 x_3 + v) + e \\\\\n&= \\color{brown}{\\beta_0 +\\beta_3\\delta_0} \\color{black}{+ \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 \\delta_1 x_3 +} \\color{green}{\\beta_3 v + e} \\\\\n&=\\color{brown}{\\alpha_0} + \\beta_1 x_1 + \\beta_2 x_2 + \\alpha_1 x_3 + \\color{green}{u} \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/7_spec.html#section-4",
    "href": "rmethods/7_spec.html#section-4",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "What about Lags (of dep variable)?\n\nIncreses Data requirements (panel? pseudo panel?)\nFurther assumptions are required (Past exogenous of present)\nBut allows controlling for underlying factors or historical factors\n\n\n\nCode\nfrause crime2, clear\nqui:reg crmrte unem llawexpc if year == 87\nest sto m1\nqui:reg crmrte unem llawexpc lcrmrt_1 if year == 87\nest sto m2\nqui:reg ccrmrte unem llawexpc if year==87  \nest sto m3\nesttab m1 m2 m3, se star(* .1 ** 0.05 *** 0.01) b(3) ///\nmtitle(crimert crimert change_crrt)\n\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                  crimert         crimert     change_crrt   \n------------------------------------------------------------\nunem               -3.659           0.346          -0.125   \n                  (3.471)         (2.127)         (2.152)   \n\nllawexpc           16.452         -20.059*        -10.377   \n                 (18.531)        (11.842)        (11.487)   \n\nlcrmrt_1                          127.111***                \n                                 (14.399)                   \n\n_cons              10.655        -337.106***       79.288   \n                (134.223)        (89.507)        (83.200)   \n------------------------------------------------------------\nN                      46              46              46   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;.1, ** p&lt;0.05, *** p&lt;0.01\n\n\nNote: Skip 9-2c and 9-3"
  },
  {
    "objectID": "rmethods/7_spec.html#why-is-x-not-the-real-x",
    "href": "rmethods/7_spec.html#why-is-x-not-the-real-x",
    "title": "Multiple Regression Analysis",
    "section": "Why is \\(X\\) not the real \\(X\\)?",
    "text": "Why is \\(X\\) not the real \\(X\\)?\n\nOften we treat data as if it they were perfect measures of the true data. But is that the case?\n\nAge: Do you report age in years, months, days, hours, minutes, etc\nWeight and Height: Even if measured, how accurate it can be? and do they make mistakes?\nIncome: Do people report income accurately? or they Lie? why?\n\nDepending on the type of error, magnitude, and if the affected variable is dep or indep, it may have diffrent consequences for OLS.\nFor now we will concentrate on a specific kind of measurement error: Classical measurement error\n\n\\[\\begin{aligned}\ny_{obs} &= y_{true} + \\varepsilon \\\\\nE(\\varepsilon) &=0; cov(\\varepsilon,y_{true})=0; cov(\\varepsilon,X's)=0\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/7_spec.html#error-in-y-dep-variable",
    "href": "rmethods/7_spec.html#error-in-y-dep-variable",
    "title": "Multiple Regression Analysis",
    "section": "Error in \\(y\\) (dep variable)",
    "text": "Error in \\(y\\) (dep variable)\n\nInstead of: \\(y^* = x\\beta + e\\)\nWe estimate \\(y^*+\\varepsilon = x\\beta + e \\rightarrow y^* = x\\beta + e-\\varepsilon\\)\nThis implies that \\(\\beta's\\) can still be unbiased when applying OLS.\nHowever variance will be larger than when using true data:\n\n\n\nCode\nqui: frause oaxaca, clear\nset seed 101\ngen lnwage2=lnwage + rnormal(2) \nqui:reg lnwage educ exper female\nest sto m1\nqui:reg lnwage2 educ exper female\nest sto m2\nesttab m1 m2, se\n\n\n(213 missing values generated)\n\n--------------------------------------------\n                      (1)             (2)   \n                   lnwage         lnwage2   \n--------------------------------------------\neduc               0.0858***       0.0902***\n                (0.00521)        (0.0120)   \n\nexper              0.0147***       0.0171***\n                (0.00126)       (0.00291)   \n\nfemale            -0.0949***      -0.0759   \n                 (0.0251)        (0.0580)   \n\n_cons               2.219***        4.132***\n                 (0.0687)         (0.159)   \n--------------------------------------------\nN                    1434            1434   \n--------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/7_spec.html#error-in-x-indep-variable",
    "href": "rmethods/7_spec.html#error-in-x-indep-variable",
    "title": "Multiple Regression Analysis",
    "section": "Error in \\(X\\) (indep variable)",
    "text": "Error in \\(X\\) (indep variable)\n\nInstead of: \\(y = \\beta_0 + \\beta_1 x^* + e\\)\nWe estimate \\(y = \\gamma_0 + \\gamma_1 (x^* + \\varepsilon) + v\\)\nBy adding an error \\(\\varepsilon\\) that has a zero relationship with \\(y\\), the “average” coefficient \\(\\gamma_1\\) will be between the true \\(\\beta_1\\) and 0. \\[\\begin{aligned}\n\\gamma_1 &=\\frac{\\sum (y-\\bar y)(x^* + \\varepsilon - \\bar x)}{\\sum (x^* + \\varepsilon - \\bar x)^2} =\\frac{\\sum (y-\\bar y)(x^* - \\bar x)+ \\sum (y-\\bar y) \\varepsilon}{\\sum (x^* - \\bar x)^2 + \\sum \\varepsilon^2} \\\\\n&= \\frac{\\sum (y-\\bar y)(x^* - \\bar x)}{\\sum (x^* - \\bar x)^2 + \\sum \\varepsilon^2} \\frac{\\sum (x^* - \\bar x)^2}{\\sum (x^* - \\bar x)^2} \\\\\n& =\\beta_1 \\frac{\\sigma^2_x}{\\sigma^2_x + \\sigma^2_\\varepsilon}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/7_spec.html#section-5",
    "href": "rmethods/7_spec.html#section-5",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Code\nfrause oaxaca, clear\nqui:sum educ\ngen educ_error = educ + rnormal()*r(sd)\nsum educ educ_error\nqui:reg lnwage educ\nest sto m1\nqui:reg lnwage educ_error\nest sto m2\nesttab m1 m2, se\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        educ |      1,647    11.40134    2.374952          5       17.5\n  educ_error |      1,647    11.36352    3.400767   .6707422   26.90462\n\n--------------------------------------------\n                      (1)             (2)   \n                   lnwage          lnwage   \n--------------------------------------------\neduc               0.0800***                \n                (0.00539)                   \n\neduc_error                         0.0399***\n                                (0.00395)   \n\n_cons               2.434***        2.898***\n                 (0.0636)        (0.0475)   \n--------------------------------------------\nN                    1434            1434   \n--------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/7_spec.html#missing-data-assume-sample-is-complete",
    "href": "rmethods/7_spec.html#missing-data-assume-sample-is-complete",
    "title": "Multiple Regression Analysis",
    "section": "Missing Data (Assume Sample is complete)",
    "text": "Missing Data (Assume Sample is complete)\n\nWhat is it? you dont have data! Your \\(N\\) falls.\n\nSome data for some observations are missing.\nWe may or may not know why they are missing\nand they maybe missing at random, or following unknown patterns.\n\nIf we are Missing data, and we do not know why, its a problem. We cant know if the sample represents the population, thus cannot be used for analysis."
  },
  {
    "objectID": "rmethods/7_spec.html#section-6",
    "href": "rmethods/7_spec.html#section-6",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "How to deal with it?\n\nif Missing completely at random (MCAR), analysis can be done as usual (no effects except smaller N)\nif Missing at random (MAR), the analysis can be done, often using standard methods:\n\nMissingness depends on observed factors (\\(X's\\)).\nIt is also known as exogenous sample selection.\nIntuitively, because all factors that determine selection are exogenous, you can identify who in the population is identified (Regression for men, women, high education, etc)\n\nIf Missing not at random (MNAR), you cant address the problem with standard analysis.\n\nSome methods such as Heckman selection or truncated regression, could be used. (advanced)\nOther wise, you can’t analyze the data (in a satisfactory manner)\nIntuitively, missingness is determined by unobserved factors, which also determines the outcome. (ie Analyze high wage population only)"
  },
  {
    "objectID": "rmethods/7_spec.html#outliers-and-influencers",
    "href": "rmethods/7_spec.html#outliers-and-influencers",
    "title": "Multiple Regression Analysis",
    "section": "Outliers and influencers",
    "text": "Outliers and influencers\n\nNot all data is made equal, and not all data has the same weight when estimating regressions.\nObservations with high Influence are those with outliers based on the conditional distribution (\\(y|x\\)).\n\nWhile outliers are not necessarily bad for analysis, it is important to understand how sensitive your results are to excluding some observations.\n\nObservations with high leverage are those with unusual characteristics.(\\(X's\\))\nCombination of both may have strong impacts on the regression analysis."
  },
  {
    "objectID": "rmethods/7_spec.html#section-7",
    "href": "rmethods/7_spec.html#section-7",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Leverage of an observation is determined by the following:\n\nDefine \\(H = X(X'X)^{-1}X'\\)\nLeverage \\(h_i = H[i,i]\\)\nHigh \\(h_i\\) denotes more influence in the model. (sensitive)\n\nInfluence is typically detected based on “studentized” residuals\n\n\\[r_i =  \\frac{\\hat e}{s_{-i}\\sqrt{1-h_i}}\n\\]"
  },
  {
    "objectID": "rmethods/7_spec.html#example",
    "href": "rmethods/7_spec.html#example",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\n\n\nCode\nqui:{\nfrause oaxaca, clear\ndrop if lnwage==.\nreg lnwage educ exper tenure female age\npredict lev, lev\nsum lev, meanonly\nreplace lev=lev/r(mean)\npredict rst, rstud\n}\nset scheme white2\ncolor_style tableau\nscatter lev rst"
  },
  {
    "objectID": "rmethods/7_spec.html#solutions",
    "href": "rmethods/7_spec.html#solutions",
    "title": "Multiple Regression Analysis",
    "section": "Solutions",
    "text": "Solutions\n\nThe problem with OLS is that it provides “too much weight” to outliers.\nThis is similar to the mean, which may not be very stable with extreme distributions.\n\nThere are at least two solutions to problems with outliers.\n\nRobust Regression (different from regression with robust Standard errors)\n\nThe idea is to penalize outliers, to reduce the impact on the estimated coefficients."
  },
  {
    "objectID": "rmethods/7_spec.html#section-8",
    "href": "rmethods/7_spec.html#section-8",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Quantile (median) Regression\n\nModifies the objective function to be minized:\n\n\n\\[\\beta's=\\min_\\beta \\sum |y-x\\beta|\n\\]\n\nInstead of using the squared of errors, it uses the absolute value.\n\nby doing this, coefficients are not sensitive to outliers! (as the median is better than the mean to capture typical values)\nDrawbacks: Its slower than OLS, and it can be difficult to interpret\n\n\nrreg &lt;- Robust Regression\nqreg &lt;- Quantile Regression"
  },
  {
    "objectID": "rmethods/9_ldvm.html#what-do-we-mean-limited",
    "href": "rmethods/9_ldvm.html#what-do-we-mean-limited",
    "title": "Limited Dependent Variable Models",
    "section": "What do we mean Limited??",
    "text": "What do we mean Limited??\n\n\nCode\nclear\nset obs 2000\ngen r1 = runiform()\ngen r2 = rchi2(5)/5 \ngen r3 = round(rchi2(3))*3\ngen r4 = rnormal()\nset scheme white2\ncolor_style tableau\nhistogram r1, name(m1, replace) \nhistogram r2, name(m2, replace)\nhistogram r3, name(m3, replace) width(1)  \nhistogram r4, name(m4, replace)\ngraph combine m1 m2 m3 m4\ngraph export images/fig9_1.png, width(1000) replace\n\n\n\nLimited Dependent variables"
  },
  {
    "objectID": "rmethods/9_ldvm.html#section",
    "href": "rmethods/9_ldvm.html#section",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "What do we mean Limited??\n\nWhen we think about “limited dependent variable” models, we refer to models when the distribution of the dep.variable is “limited”\n\nIn other words. The values it can take are restricted! (positive, or only integer), within a range, etc\n\nCan you still use LRM for them?\nWill anything change if you do?\nDo we care?"
  },
  {
    "objectID": "rmethods/9_ldvm.html#no-we-dont-but..",
    "href": "rmethods/9_ldvm.html#no-we-dont-but..",
    "title": "Limited Dependent Variable Models",
    "section": "No we dont, but..",
    "text": "No we dont, but..\n\nWe dont really care. In fact we have already use LRM on that fashion:\n\nLPM: Dep variable was a Dummy\nWages: Always positive\n# Children: Countable\n\nBut, there are couple of things one should consider.\n\nModels of this kind are usually heteroskedastic by construction. (robust? Weighted?)\nPredictions could made no sense.\nThere are better models we could use to analyze the data\n\n\nBetter under some assumptions\n\nHowever, this models cannot be estimated using OLS (there is no “close form solution”)\nWe may need to learn a new method: Maximum Likelihood"
  },
  {
    "objectID": "rmethods/9_ldvm.html#probits-and-logits",
    "href": "rmethods/9_ldvm.html#probits-and-logits",
    "title": "Limited Dependent Variable Models",
    "section": "Probits and Logits",
    "text": "Probits and Logits\n\nLPM are easy, fast, and good for most data analysis (exploration). But they have some limitations.\nMost limitations can be overcome with alternative models: Logit or Probit\nIn constrast with LPM (which aims to explain individual outcomes), Logit/probit aims to explain Conditional Probabilities:\n\n\\[p(y=1|x) = G(x\\beta)\\]\n\nwhere the function \\(G()\\) makes sure the predicted outcome is always between 0 and 1.\nCaveat: Because \\(G()\\) is nonlinear, this is a nonlinear model, and marginal effects are harder to estimate."
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-1",
    "href": "rmethods/9_ldvm.html#section-1",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "What to use for \\(G()\\)\n\nTwo leading options:\n\n\\[logit: G(x\\beta) = \\frac{\\exp{x\\beta}}{1+\\exp{x\\beta}}\\] \\[probit: G(x\\beta) = \\Phi(x\\beta)=\\int_{-\\infty}^{x\\beta}\\phi(z)dz\\]\n\nBut in practice Either will work. Then why the difference?"
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-2",
    "href": "rmethods/9_ldvm.html#section-2",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "Probits and Logits: Latent variables\n\nIt all comes down to the Latent variable!\nAssumption:\n\nEverybody has a latent score on every “binary” decision: The value to a decision \\(y^*\\) \\[y^* = x\\beta + e \\]\nIf \\(y^*\\) is above certain threshold (\\(y^*&gt;0\\)), you “do” something (\\(y=1\\)). If not you dont (\\(y=0\\)).\n\nThus the choice between logit and probit depends on the distribution of \\(e\\).\n\n\\(e\\) is normal, then probit\n\\(e\\) is logistic, then logit"
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-3",
    "href": "rmethods/9_ldvm.html#section-3",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "Some Math\nLatent Model:\n\\[ y^* = x\\beta + e \\]\nWe aim to measure the probablity of a positive latent.\n\\[\\begin{aligned}\nP(y^*&gt;0|x) & = P(x\\beta + e&gt;0|x) \\\\\n& = P( e&gt;- x\\beta|x) \\\\\n& = 1 - P( e &lt; - x\\beta|x) = 1-G( - x\\beta|x) \\\\\n& = G(x\\beta)\n\\end{aligned}\n\\]\nlast step valid only if \\(G()\\) is symetrical."
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-4",
    "href": "rmethods/9_ldvm.html#section-4",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "Marginal Effects?\n\nSame as before. The partial derivative!\n\n\\[\\begin{aligned}\np(y=1|x) &= G(\\beta_0 + \\beta_1 x_1 +\\beta_2 x_2 ) \\\\\n\\frac{\\partial p(y=1|x)}{\\partial x_1} = G'(x\\beta)\\beta_1=g(x\\beta)\\beta_1\n\\end{aligned}\n\\]\n\nBut if variables are dummies, we need to estimate true effect.\n\n\\[\\begin{aligned}\np(y=1|x) &= G(\\beta_0 + \\beta_1 x_1 +\\beta_2 D_2 ) \\\\\n\\frac{\\partial p(y=1|x)}{\\partial D_2} = G(\\beta_0 + \\beta_1 x_1 +\\beta_2 )-G(\\beta_0 + \\beta_1 x_1 )\n\\end{aligned}\n\\]\nand yes, you could also have interactions, polynomials, etc"
  },
  {
    "objectID": "rmethods/9_ldvm.html#mle-how-does-this-work",
    "href": "rmethods/9_ldvm.html#mle-how-does-this-work",
    "title": "Limited Dependent Variable Models",
    "section": "MLE: How does this work?",
    "text": "MLE: How does this work?\n\nMLE: Maximum Likelihood Estimator, is an alternative method to OLS that allows you to estimate parameters in nonlinear models.\nThe idea of the method is to “model” the conditional distribution of the data \\(F(y|x,\\theta)\\) or \\(f(y|x,\\theta)\\), assuming \\(X's\\) are given and modifying values of \\(\\theta\\) (distribution parameters).\n\\(LRM\\) could be estimated via MLE, but you will need More assumptions:\n\nThe error \\(e\\) is normal.\n\nThen “simply” find the parameters for the mean and variance that “maximizes” the probability that data Comes a given distribution.\nIn the case of Probit/logit, there is “only” one paramter we need to identify. The conditional probabilty \\(p(y=1|X)\\).\n\nExcept that we allow this to vary by \\(X\\)"
  },
  {
    "objectID": "rmethods/9_ldvm.html#likelihood-function-for-logitprobit",
    "href": "rmethods/9_ldvm.html#likelihood-function-for-logitprobit",
    "title": "Limited Dependent Variable Models",
    "section": "Likelihood function for Logit/probit",
    "text": "Likelihood function for Logit/probit\n\\[L_i = G(x\\beta)^{y=1}*(1-G(x\\beta))^{y=0}\n\\]\nUnder Independence:\n\\[L_D = L_1 \\times L_2 \\times \\dots L_N\n\\]\nThus we need to find the \\(\\beta's\\) that make \\(L_D\\) the largest.\nBut because we like sums over products:\n\\[LL_D = \\sum_{i=1}^N log(L_i)\n\\]"
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-5",
    "href": "rmethods/9_ldvm.html#section-5",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "Code\n  clear\n  set obs 25\n  gen r = runiform()&lt;.7\n  mata: \n    r = st_data(.,\"r\")\n    ll = J(99,2,0)\n    for(i=1;i&lt;=99;i++){\n      theta = i/100\n      // Log Properties\n      ll[i,]= theta,exp(sum(log(theta:^(r:==1) :* (1-theta):^(r:==0))))\n    }\n  end\n  qui getmata ll*=ll , force\n  ren ll1 theta\n  ren ll2 likelihood\n  *scatter likelihood theta \n\n\nNumber of observations (_N) was 0, now 25."
  },
  {
    "objectID": "rmethods/9_ldvm.html#testing",
    "href": "rmethods/9_ldvm.html#testing",
    "title": "Limited Dependent Variable Models",
    "section": "Testing?",
    "text": "Testing?\n\nYou can test two things:\n\nTest coefficients (\\(\\beta\\))\nTest marginal effects (\\(G'(x\\beta)\\beta\\))\n\nBoth test will most likely agree with each other, but some contradictions may arise. ### How?\nz-test and/or Wald test: Similar to t-test and Joint F-test we cover before. But, we now make the assumption of normality (not t-distribution)\nLog-Likelihood test. Similar to F-test for restricted and unrestricted model:\n\nEstimate both Restricted and unrestricted model. And obtain their Log Likelihoods (\\(\\mathcal{L}_ur\\)) and (\\(\\mathcal{L}_r\\)).\n\n\\[LR = 2 (\\mathcal{L}_ur-\\mathcal{L}_r) \\overset{a}\\sim \\chi^2_q\\]"
  },
  {
    "objectID": "rmethods/9_ldvm.html#stata---example",
    "href": "rmethods/9_ldvm.html#stata---example",
    "title": "Limited Dependent Variable Models",
    "section": "Stata - Example",
    "text": "Stata - Example\n\n\nCode\nfrause mroz, clear\n* LPM with Robust Standard errors\nqui:reg inlf nwifeinc educ exper expersq age kidslt6 kidsge6, robust\nest sto m1\nqui:logit inlf nwifeinc educ exper expersq age kidslt6 kidsge6, \nest sto m2a\nqui:margins, dydx(*) post\nest sto m2b\nprobit inlf nwifeinc educ exper expersq age kidslt6 kidsge6, \nest sto m3a\nqui:margins, dydx(*) post\nest sto m3b\n\n\n\nIteration 0:   log likelihood =  -514.8732  \nIteration 1:   log likelihood = -402.06651  \nIteration 2:   log likelihood = -401.30273  \nIteration 3:   log likelihood = -401.30219  \nIteration 4:   log likelihood = -401.30219  \n\nProbit regression                                       Number of obs =    753\n                                                        LR chi2(7)    = 227.14\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -401.30219                             Pseudo R2     = 0.2206\n\n------------------------------------------------------------------------------\n        inlf | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    nwifeinc |  -.0120237   .0048398    -2.48   0.013    -.0215096   -.0025378\n        educ |   .1309047   .0252542     5.18   0.000     .0814074     .180402\n       exper |   .1233476   .0187164     6.59   0.000     .0866641    .1600311\n     expersq |  -.0018871      .0006    -3.15   0.002     -.003063   -.0007111\n         age |  -.0528527   .0084772    -6.23   0.000    -.0694678   -.0362376\n     kidslt6 |  -.8683285   .1185223    -7.33   0.000    -1.100628    -.636029\n     kidsge6 |    .036005   .0434768     0.83   0.408     -.049208    .1212179\n       _cons |   .2700768    .508593     0.53   0.595    -.7267473    1.266901\n------------------------------------------------------------------------------\n\n\n\n\nCode\nset linesize 255\n*| classes: larger\ndisplay \"Prob Models\"\nesttab m1 m2a m2b m3a m3b, scalar(r2 ll) cell(b(fmt(%5.3f)) ///\nse(par([ ])) p( par(( )) ) )  gap  mtitle(LPM Logit Logit-mfx Probit Probit-mfx)\n\n\nProb Models\n\n-----------------------------------------------------------------------------\n                      (1)          (2)          (3)          (4)          (5)\n                      LPM        Logit    Logit-mfx       Probit   Probit-mfx\n                   b/se/p       b/se/p       b/se/p       b/se/p       b/se/p\n-----------------------------------------------------------------------------\nmain                                                                         \nnwifeinc           -0.003       -0.021       -0.004       -0.012       -0.004\n                  [0.002]      [0.008]      [0.001]      [0.005]      [0.001]\n                  (0.026)      (0.011)      (0.010)      (0.013)      (0.012)\n\neduc                0.038        0.221        0.039        0.131        0.039\n                  [0.007]      [0.043]      [0.007]      [0.025]      [0.007]\n                  (0.000)      (0.000)      (0.000)      (0.000)      (0.000)\n\nexper               0.039        0.206        0.037        0.123        0.037\n                  [0.006]      [0.032]      [0.005]      [0.019]      [0.005]\n                  (0.000)      (0.000)      (0.000)      (0.000)      (0.000)\n\nexpersq            -0.001       -0.003       -0.001       -0.002       -0.001\n                  [0.000]      [0.001]      [0.000]      [0.001]      [0.000]\n                  (0.002)      (0.002)      (0.001)      (0.002)      (0.001)\n\nage                -0.016       -0.088       -0.016       -0.053       -0.016\n                  [0.002]      [0.015]      [0.002]      [0.008]      [0.002]\n                  (0.000)      (0.000)      (0.000)      (0.000)      (0.000)\n\nkidslt6            -0.262       -1.443       -0.258       -0.868       -0.261\n                  [0.032]      [0.204]      [0.032]      [0.119]      [0.032]\n                  (0.000)      (0.000)      (0.000)      (0.000)      (0.000)\n\nkidsge6             0.013        0.060        0.011        0.036        0.011\n                  [0.014]      [0.075]      [0.013]      [0.043]      [0.013]\n                  (0.337)      (0.422)      (0.421)      (0.408)      (0.407)\n\n_cons               0.586        0.425                     0.270             \n                  [0.152]      [0.860]                   [0.509]             \n                  (0.000)      (0.621)                   (0.595)             \n-----------------------------------------------------------------------------\nN                     753          753          753          753          753\nr2                  0.264                                                    \nll                 -423.9       -401.8                    -401.3             \n-----------------------------------------------------------------------------\n\n\n\ndisplay \"LR test\"\nqui:probit inlf nwifeinc educ exper expersq age kidslt6 kidsge6 motheduc fatheduc, \nest sto unrestricted\nqui:probit inlf nwifeinc educ exper expersq age kidslt6 kidsge6 , \nest sto restricted\nlrtest unrestricted restricted\n\nLR test\n\nLikelihood-ratio test\nAssumption: restricted nested within unrestricted\n\n LR chi2(2) =   0.29\nProb &gt; chi2 = 0.8668"
  },
  {
    "objectID": "rmethods/9_ldvm.html#censored-and-truncated-data",
    "href": "rmethods/9_ldvm.html#censored-and-truncated-data",
    "title": "Limited Dependent Variable Models",
    "section": "Censored and Truncated Data",
    "text": "Censored and Truncated Data\n\nLogits and Probits, are not the only models that require MLE for estimation.\n\nAmong Discrete data models, you also have ologit/oprobit for ordered responses. mlogit/mprobit for unordered ones. Extends on logit/probit.\n\nThere are other interesting cases:\n\nWhen Data is censored.\nWhen Data is truncated."
  },
  {
    "objectID": "rmethods/9_ldvm.html#three-cases",
    "href": "rmethods/9_ldvm.html#three-cases",
    "title": "Limited Dependent Variable Models",
    "section": "Three Cases",
    "text": "Three Cases\n\nCase 1Case 2Case 3\n\n\n\n\\(y\\) is “conditionally-normal” and is Fully Observed.\nYou can estimate the model using OLS or ML\n\n\n\nCode\nqui:{\n  clear\n  set obs 999\n  gen p   = _n/(_N+1)\n  gen fob = invnormal(p)\n}\nqui:histogram fob\n\n\n\n\n\n\n\n\n\n\n\n\nData is observed for everyone, but is “censored” for some. tobit\n\nEither corner solution (how many hours you study) or Recoded: \\(y_{obs} = max(c,y^*)\\)\n\n\n\n\nCode\nqui: replace fob = -2 if fob&lt;-2\nqui:histogram fob, xlabel(-4 (2) 4)\n\n\n\n\n\n\n\n\n\n\n\n\nBelow (or above) some threshold, you do not have information on \\(y\\). truncreg \\[y_{obs} = y^* \\text{ if } y^*&gt;c\\]"
  },
  {
    "objectID": "rmethods/9_ldvm.html#estimation-censored-and-corner-solution",
    "href": "rmethods/9_ldvm.html#estimation-censored-and-corner-solution",
    "title": "Limited Dependent Variable Models",
    "section": "Estimation: Censored and Corner Solution",
    "text": "Estimation: Censored and Corner Solution\nIf data is censored or corner solution the estimation strategy is based on:\n\\[\\begin{aligned}\nL_i &= \\frac{1}{\\sigma} \\phi\\left( \\frac{y-x\\beta}{\\sigma} \\right) \\text{ if } y&gt;c \\\\\n    &= 1-\\Phi\\left(\\frac{x\\beta}{\\sigma} \\right) \\text{ if } y\\leq c \\\\\n\\end{aligned}\n\\]\nIf data is truncated, we need to “adjust” the distribution of what is observed\n\\[\\begin{aligned}\nL_i &= \\frac{1}{\\Phi\\left( x\\beta/\\sigma \\right)} \\frac{1}{\\sigma} \\phi\\left( \\frac{y-x\\beta}{\\sigma} \\right) \\text{ if } y&gt;c \\\\  \n\\end{aligned}\n\\]\nWe will put -truncated regression- on the side for now. But see here for an example."
  },
  {
    "objectID": "rmethods/9_ldvm.html#interpretation-it-depends",
    "href": "rmethods/9_ldvm.html#interpretation-it-depends",
    "title": "Limited Dependent Variable Models",
    "section": "Interpretation: It depends!",
    "text": "Interpretation: It depends!\n\nWhat are you interested in analyzing? and what type of data you have?\n\n\nLatent variable\\(P(y&gt;0|x)\\)\\(E(y|y&gt;0,x)\\)\\(E(y|x)\\)\n\n\n\nEasiest Case. Just need to consider the coefficients (as in LRM)\n\n\\[\n\\begin{aligned}\nE(y^*|x) &= x\\beta \\\\\n\\frac{\\partial E(y^*|x)}{\\partial x } &= \\beta_x\n\\end{aligned}\n\\]\n\nThe same applies if model was censored.\n\n\n\n\nIts an alternative approach to Probit models, where you are interest in analyzing why is data Not censored, or why is it above some threshold. (why people work)\nExtensive margin effect. \\[\n\\begin{aligned}\nP(y&gt;0|x) &= \\Phi\\left(\\frac{x\\beta}{\\sigma}\\right) \\\\\n\\frac{\\partial P(y&gt;0|x)}{\\partial x } &= \\frac{\\beta_x}{\\sigma} \\phi\\left(\\frac{x\\beta}{\\sigma}\\right)\n\\end{aligned}\n\\]\n\nNote: Coefficients \\(\\beta\\) need to be Standardized.\n\n\n\nIf corner solution, one may be interested in the effect of those with positive outcomes only.\nThis is the intensive margin effect. \\[\n\\begin{aligned}\nE(y|y&gt;0,x) &= x\\beta + \\sigma \\frac{\\phi(x\\beta / \\sigma )}{\\Phi(x\\beta / \\sigma )} \\\\\n\\frac{\\partial E(y|y&gt;0,x)}{\\partial x } &= \\beta_x\n\\left[ 1-\\frac{\\phi(x\\beta / \\sigma )}{\\Phi(x\\beta / \\sigma )} \\left( \\frac{x\\beta }{\\sigma }+ \\frac{\\phi(x\\beta / \\sigma )}{\\Phi(x\\beta / \\sigma )}\\right) \\right]\n\\end{aligned}\n\\]\n\n\n\n\nIn this case, one may be interested in estimating the expected effect on everyone.\nCombines both Intensive and extensive margin effects. Comparable to OLS.\n\n\\[\n\\begin{aligned}\nE(y|x) &= p(y&gt;0|x)*E(y|y&gt;0,x) + (1-p(y&gt;0|x))*0 \\\\\n\\frac{\\partial E(y|x)}{\\partial x } &= \\beta_x \\Phi(x\\beta)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/9_ldvm.html#example",
    "href": "rmethods/9_ldvm.html#example",
    "title": "Limited Dependent Variable Models",
    "section": "Example",
    "text": "Example\n\n\nCode\nfrause mroz, clear\nqui:tobit hours nwifeinc educ c.exper##c.exper   age kidslt6 kidsge6 , ll(0)\nqui:emargins, dydx(*) estore(m1)\nqui:emargins, dydx(*) predict(p(0,.)) estore(m2)\nqui:emargins, dydx(*) predict(e(0,.)) estore(m3)\nqui:emargins, dydx(*) predict(ystar(0,.)) estore(m4)\nesttab m1 m2 m3 m4, mtitle(Latent P(y&gt;0) E(y|y&gt;0) E(y) ) b(3) se\n\n\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                   Latent          P(y&gt;0)        E(y|y&gt;0)            E(y)   \n----------------------------------------------------------------------------\nnwifeinc           -8.814*         -0.002*         -3.969*         -5.189*  \n                  (4.459)         (0.001)         (2.008)         (2.621)   \n\neduc               80.645***        0.022***       36.312***       47.473***\n                 (21.583)         (0.006)         (9.703)        (12.621)   \n\nexper              91.929***        0.026***       37.593***       48.793***\n                  (7.997)         (0.002)         (2.966)         (3.587)   \n\nage               -54.405***       -0.015***      -24.497***      -32.026***\n                  (7.418)         (0.002)         (3.362)         (4.292)   \n\nkidslt6          -894.020***       -0.246***     -402.551***     -526.278***\n                (111.878)         (0.028)        (50.749)        (64.706)   \n\nkidsge6           -16.218          -0.004          -7.303          -9.547   \n                 (38.641)         (0.011)        (17.404)        (22.752)   \n----------------------------------------------------------------------------\nN                     753             753             753             753   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/9_ldvm.html#tobit-has-problems-too",
    "href": "rmethods/9_ldvm.html#tobit-has-problems-too",
    "title": "Limited Dependent Variable Models",
    "section": "Tobit has problems too",
    "text": "Tobit has problems too\n\nThat simple equation, too much aggregation\nHayek (in Fear the Boom and Bust)\n\n\nTobit, when addressing corner solutions, aims to explain two different actions (Engagement and intensity) with the same model. However, this may not be appropriate all the time.\n\nHW-Examples?\n\nWhen this happens, other models may be more appropritate like\n\ntwo part model: (literally model using two equations)\nHurdle Model (craggit or churdle)\n\nAlso…Normality…"
  },
  {
    "objectID": "rmethods/9_ldvm.html#censored-regression",
    "href": "rmethods/9_ldvm.html#censored-regression",
    "title": "Limited Dependent Variable Models",
    "section": "Censored Regression",
    "text": "Censored Regression\n\nApplies to the same cases as Tobit model. But, it usually refers to Censoring at other points of the distribution (upper censoring? mixed censoring?)\nFurthermore, applies to cases with different censoring thresholds!\n\nTypical Example, Unemployment duration\n\n\n\n\nCode\nqui:frause recid, clear\ngen lldur = ldurat             // Lower Limit\ngen uudur = ldurat if cens==0  // upper limit = . if censored.\nintreg lldur uudur workprg priors tserved felon alcohol drugs black married educ age\n\n\n(893 missing values generated)\n\nFitting constant-only model:\n\nIteration 0:   log likelihood = -2188.8689  \nIteration 1:   log likelihood = -1732.7406  \nIteration 2:   log likelihood = -1680.7927  \nIteration 3:   log likelihood =  -1680.427  \nIteration 4:   log likelihood =  -1680.427  \n\nFitting full model:\n\nIteration 0:   log likelihood = -2116.9831  \nIteration 1:   log likelihood = -1639.9495  \nIteration 2:   log likelihood =  -1597.634  \nIteration 3:   log likelihood = -1597.0592  \nIteration 4:   log likelihood =  -1597.059  \nIteration 5:   log likelihood =  -1597.059  \n\nInterval regression                                 Number of obs     =  1,445\n                                                           Uncensored =    552\n                                                        Left-censored =      0\n                                                       Right-censored =    893\n                                                       Interval-cens. =      0\n\n                                                    LR chi2(10)       = 166.74\nLog likelihood = -1597.059                          Prob &gt; chi2       = 0.0000\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     workprg |  -.0625715   .1200369    -0.52   0.602    -.2978396    .1726965\n      priors |  -.1372529   .0214587    -6.40   0.000    -.1793111   -.0951947\n     tserved |  -.0193305   .0029779    -6.49   0.000    -.0251672   -.0134939\n       felon |   .4439947   .1450865     3.06   0.002     .1596303     .728359\n     alcohol |  -.6349092   .1442166    -4.40   0.000    -.9175686   -.3522499\n       drugs |  -.2981602   .1327356    -2.25   0.025    -.5583171   -.0380033\n       black |  -.5427179   .1174428    -4.62   0.000    -.7729014   -.3125343\n     married |   .3406837   .1398431     2.44   0.015     .0665964    .6147711\n        educ |   .0229196   .0253974     0.90   0.367    -.0268584    .0726975\n         age |   .0039103   .0006062     6.45   0.000     .0027221    .0050984\n       _cons |   4.099386    .347535    11.80   0.000      3.41823    4.780542\n-------------+----------------------------------------------------------------\n    /lnsigma |   .5935864   .0344122    17.25   0.000     .5261398     .661033\n-------------+----------------------------------------------------------------\n       sigma |    1.81047   .0623022                      1.692387    1.936792\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/9_ldvm.html#truncated",
    "href": "rmethods/9_ldvm.html#truncated",
    "title": "Limited Dependent Variable Models",
    "section": "Truncated",
    "text": "Truncated\n\nIf Data is simply not there, as shown before, one needs to adjust Estimates.\nmarginal effects decisions are similar to Tobit\n\n\nfrause mroz, clear\nqui:truncreg hours nwifeinc educ c.exper##c.exper   age kidslt6 kidsge6 , ll(0)\nemargins, dydx(*) estore(m1b)\nemargins, dydx(*) predict(e(0,.)) estore(m2b)\nesttab m1 m1b m3 m2b, mtitle(Lat-Tobit Lat-Trunc E(y&gt;0)-Tobit E(y&gt;0)-Trunc ) b(3) se\n\n\nAverage marginal effects                                   Number of obs = 428\nModel VCE: OIM\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  nwifeinc educ exper age kidslt6 kidsge6\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    nwifeinc |   .1534399   5.164279     0.03   0.976    -9.968361    10.27524\n        educ |  -29.85254   22.83935    -1.31   0.191    -74.61684    14.91176\n       exper |   48.00824   8.578316     5.60   0.000     31.19504    64.82143\n         age |  -27.44381   8.293458    -3.31   0.001    -43.69869   -11.18893\n     kidslt6 |  -484.7109   153.7881    -3.15   0.002      -786.13   -183.2918\n     kidsge6 |  -102.6574   43.54347    -2.36   0.018    -188.0011   -17.31379\n------------------------------------------------------------------------------\n\nAverage marginal effects                                   Number of obs = 428\nModel VCE: OIM\n\nExpression: E(hours|hours&gt;0), predict(e(0,.))\ndy/dx wrt:  nwifeinc educ exper age kidslt6 kidsge6\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    nwifeinc |   .1094149   3.682546     0.03   0.976    -7.108243    7.327072\n        educ |  -21.28723   16.25065    -1.31   0.190    -53.13793    10.56346\n       exper |   32.66986   5.277772     6.19   0.000     22.32562    43.01411\n         age |  -19.56962   5.823226    -3.36   0.001    -30.98293   -8.156303\n     kidslt6 |  -345.6374   107.9599    -3.20   0.001    -557.2349   -134.0399\n     kidsge6 |  -73.20291   30.80594    -2.38   0.017    -133.5814   -12.82438\n------------------------------------------------------------------------------\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                Lat-Tobit       Lat-Trunc    E(y&gt;0)-Tobit    E(y&gt;0)-Trunc   \n----------------------------------------------------------------------------\nnwifeinc           -8.814*          0.153          -3.969*          0.109   \n                  (4.459)         (5.164)         (2.008)         (3.683)   \n\neduc               80.645***      -29.853          36.312***      -21.287   \n                 (21.583)        (22.839)         (9.703)        (16.251)   \n\nexper              91.929***       48.008***       37.593***       32.670***\n                  (7.997)         (8.578)         (2.966)         (5.278)   \n\nage               -54.405***      -27.444***      -24.497***      -19.570***\n                  (7.418)         (8.293)         (3.362)         (5.823)   \n\nkidslt6          -894.020***     -484.711**      -402.551***     -345.637** \n                (111.878)       (153.788)        (50.749)       (107.960)   \n\nkidsge6           -16.218        -102.657*         -7.303         -73.203*  \n                 (38.641)        (43.543)        (17.404)        (30.806)   \n----------------------------------------------------------------------------\nN                     753             428             753             428   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/9_ldvm.html#poisson",
    "href": "rmethods/9_ldvm.html#poisson",
    "title": "Limited Dependent Variable Models",
    "section": "Poisson",
    "text": "Poisson\n\nSome times, Data may be non-negative, and/or countable. OLS works well, but we could do better\nWith Count data, some data transformations (logs) are not possible, because of the zeroes.\nSo instead of assuming \\(y|x \\sim N(\\mu_x,\\sigma)\\), one could assume \\(y|x \\sim poisson(\\mu_x)\\)\n\n\\[P(y=k,\\mu_x) = \\frac{\\mu_x^k e ^{-\\mu_x}}{k!} \\text{ with } \\mu_x=\\exp(x\\beta)\\]\n\nFor a Poisson:\n\n\\(E(y|x) = \\exp{x\\beta}\\) and \\(Var(y|x) = \\exp{x\\beta}\\)\n\nAs hinted before, Count data is heteroskedastic. And Poisson assumes some structure to that."
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-6",
    "href": "rmethods/9_ldvm.html#section-6",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "Also convinient that Poisson models are very easy to interpret! (just like Log-lin models)\nAfter estimation:\n\n\\[\\frac{\\Delta \\% E(y|x)}{\\Delta x} \\simeq \\beta_x \\times 100 \\text{ or } (\\exp \\beta_x-1)\\times 100 \\]\n\nOther points.\n\nThe variance imposed in Poisson is very restrictive. This is a problem for Variance estimation!\nSolution: use Robust Standard Errors!\nLike LRM, poisson is robust to errors when modeling the conditional mean.\nPoisson is a very good alternative for continuous data too (if using Robust SE)\n\nWage models, trade models"
  },
  {
    "objectID": "rmethods/9_ldvm.html#example-1",
    "href": "rmethods/9_ldvm.html#example-1",
    "title": "Limited Dependent Variable Models",
    "section": "Example",
    "text": "Example\n\nfrause crime1, clear\nqui: reg narr86 pcnv avgsen tottime ptime86 qemp86 inc86 black hispan born60\nest sto m1\nqui:poisson narr86 pcnv avgsen tottime ptime86 qemp86 inc86 black hispan born60, robust\nest sto m2\nqui:emargins, dydx(*) estore(m3)\nesttab m1 m2 m3, se b(3) mtitle(LRM Poisson Poisson-mfx) ///\nkeep(pcnv ptime86  qemp86 inc86 black hispan) label varwidth(20) wrap\n\n\n--------------------------------------------------------------------\n                              (1)             (2)             (3)   \n                              LRM         Poisson     Poisson-mfx   \n--------------------------------------------------------------------\nmain                                                                \nproportion of prior        -0.132**        -0.402***       -0.162***\nconvictions               (0.040)         (0.101)         (0.040)   \n\nmos. in prison             -0.041***       -0.099***       -0.040***\nduring 1986               (0.009)         (0.022)         (0.009)   \n\n# quarters employed,       -0.051***       -0.038          -0.015   \n1986                      (0.014)         (0.034)         (0.014)   \n\nlegal income, 1986,        -0.001***       -0.008***       -0.003***\n$100s                     (0.000)         (0.001)         (0.001)   \n\n=1 if black                 0.327***        0.661***        0.267***\n                          (0.045)         (0.099)         (0.042)   \n\n=1 if Hispanic              0.194***        0.500***        0.202***\n                          (0.040)         (0.092)         (0.038)   \n--------------------------------------------------------------------\nObservations                 2725            2725            2725   \n--------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/9_ldvm.html#other-methods-of-interest",
    "href": "rmethods/9_ldvm.html#other-methods-of-interest",
    "title": "Limited Dependent Variable Models",
    "section": "Other Methods of interest",
    "text": "Other Methods of interest\n\nMLE opens the door to other methods that may be more approriate to analyze data\nThey may even be able to handle otherwise unsolvable data problems.\n\nologit, oprobit: Ordered qualitative variables\nmlogit, mprobit: Unordered Qualitative variables\nheckman: Endogenous Sample Selection\nfractional regression model: When the depvariable is an index\netc etc\n\nWorth knowing, but not for the exam!"
  },
  {
    "objectID": "rmethods/homework_2.html",
    "href": "rmethods/homework_2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Similar to HW1, propose a concise research project. Specify the dependent variable of interest, along with control variables.\nThe model specification must include both continuous and discrete variables in your model.\nDescribe your economic model and its corresponding econometric model, accompanied by a succinct description of your anticipated findings."
  },
  {
    "objectID": "rmethods/homework_2.html#note",
    "href": "rmethods/homework_2.html#note",
    "title": "Homework 2",
    "section": "Note",
    "text": "Note\nExplore all datasets available in frause and utilize the data from these datasets to determine the variables for analysis and the controls to incorporate into your homework.\nThe examples within the textbook can serve as valuable guidelines for your considerations here.\nYou have the freedom to explore other sources. If you do so, please include the data alongside your homework submission.\nExceptionally unique responses (distinguished by their thoroughness, detail, innovation, and presentation) may merit extra points."
  },
  {
    "objectID": "rmethods/index.html",
    "href": "rmethods/index.html",
    "title": "Research Methods: Econometrics I",
    "section": "",
    "text": "Class syllabus is here.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods: Econometrics I"
    ]
  },
  {
    "objectID": "rmethods/index.html#syllabus",
    "href": "rmethods/index.html#syllabus",
    "title": "Research Methods: Econometrics I",
    "section": "",
    "text": "Class syllabus is here.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods: Econometrics I"
    ]
  },
  {
    "objectID": "rmethods/index.html#zoom-link",
    "href": "rmethods/index.html#zoom-link",
    "title": "Research Methods: Econometrics I",
    "section": "Zoom Link",
    "text": "Zoom Link\nWhen there is need to attend a class online, and you have requested to attend online, use the following link:\nZoom class",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods: Econometrics I"
    ]
  },
  {
    "objectID": "rmethods/index.html#introduction",
    "href": "rmethods/index.html#introduction",
    "title": "Research Methods: Econometrics I",
    "section": "Introduction",
    "text": "Introduction\n\nChapter 1\nOnce Upon a time, why Econometrics?:",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods: Econometrics I"
    ]
  },
  {
    "objectID": "rmethods/index.html#part-i-basic-tools",
    "href": "rmethods/index.html#part-i-basic-tools",
    "title": "Research Methods: Econometrics I",
    "section": "Part I: Basic Tools",
    "text": "Part I: Basic Tools\n\n1. Chapter 2:\nThe Simple Regression Model\n\n\n2. Chapter 3:\nMultiple Regression Analysis: Estimation\n\n\n3. Chapter 4 - 5\nMRA: Inference and Asymptotics\n\n\nHomeWork 1 Also Github link\n\n\nDue Date September 27",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods: Econometrics I"
    ]
  },
  {
    "objectID": "rmethods/index.html#part-ii-addressing-problems-with-mra",
    "href": "rmethods/index.html#part-ii-addressing-problems-with-mra",
    "title": "Research Methods: Econometrics I",
    "section": "Part II: Addressing Problems with MRA",
    "text": "Part II: Addressing Problems with MRA\n\n4. Chapter 6 - 7\nMRA: Scaling, functional forms, Goodness of Fit, and Qualitative Information\n\n\n5. Chapter 8\nHeteroskedasticity\n\n\n6. Chapter 9\nFurther Problems: Functional form, Missing variables, measurement errors, missing data\n\n\nHomeWork 2 Also Github link\n\n\nDue Date October 25 !!\n\n\nReview Class: Wednesday October 18\n\n\nMIDTERM! October 20: 9:30 - 12:50\n\n7. Chapter 15\nInstrumental Variables and 2SLS\n\n\n8. Chapter 17\nLimited Dep Variables",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods: Econometrics I"
    ]
  },
  {
    "objectID": "rmethods/index.html#part-iii-panel-data-methods",
    "href": "rmethods/index.html#part-iii-panel-data-methods",
    "title": "Research Methods: Econometrics I",
    "section": "Part III: Panel Data Methods",
    "text": "Part III: Panel Data Methods\n\n9. Chapter 13\nPool Cross Section and Panel Data\n\n\n10. Chapter 14\nAdvanced Panel Data Methods\n\n\n[HomeWork 3] Github\n\n\nDue Date November 29 at 1pm",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods: Econometrics I"
    ]
  },
  {
    "objectID": "rmethods/index.html#part-iv-time-series",
    "href": "rmethods/index.html#part-iv-time-series",
    "title": "Research Methods: Econometrics I",
    "section": "Part IV: Time Series",
    "text": "Part IV: Time Series\n\n11. Chapter 10 - 11\nBasics of Regression analysis with TSD\n\n\n12. Chapter 12 & 18\nAdvanced TSD Problems\n\n\nFinal\n\nPossible Date December 13",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods: Econometrics I"
    ]
  },
  {
    "objectID": "rmethods/Question 1.html",
    "href": "rmethods/Question 1.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Question 1\nconsider the following matrixes\n\\(A=\\begin{pmatrix} 1 & 2 & 3 \\\\\n3 & 3 & 8\n\\end{pmatrix}\\)\n\\(B=\\begin{pmatrix} 3 & 1 & 6 \\\\\n2 & 1 & 2\n\\end{pmatrix}\\)\n\\(C=\\begin{pmatrix} 5 & 1 & 6 \\\\\n1 & 1 & 2 \\\\\n4 & 2 & 6\n\\end{pmatrix}\\)\nConsider the following Operations:\n\\(A + B\\)\n\\(A + B'\\)\n\\(A * B\\)\n\\(Det(C)\\)\n\\(C^{-1}\\)\n\\(A' * B\\)\n\\((A * B')^{-1}\\)\nIndicate if they are valid operations. if not, explain why If they are obtain the resulting matrices.\nQuestion 2\nConsider the following functions:\n\\(y=1+x^2-3x^3\\)\n\\(y=ln(3x^2+1)\\)\n\\(y=exp(x+x*z)\\)\nFor each one estimate: \\(\\frac{\\partial y}{∂ x}\\) and \\(\\frac{\\partial^2 y}{∂ x^2}\\)\nQuestion 3 Solve the following set of equations. If not, indicate why they cant be solved:\nEq1\n\\(\\begin{aligned}\n2x+y  &=  3 \\\\\n2x+2y &= -3\n\\end{aligned}\\)\nEq 2\n\\(\\begin{aligned}\n2x+ y + z &= 1 \\\\\n3x+ 2y - z &= 2 \\\\\nx+ y - 2 z &= 3 \\\\\n\\end{aligned}\\)\nEq 3\n\\(x^2 + x - 2 =0\\)\nQuestion 4: Maximization\n\nConsider the following set of functions:\n\n\\(\\begin{aligned}\ny &= A  x^α z ^\\beta \\\\\nC &= p_x x + P_z z\n\\end{aligned}\\)\nSet up the constrained maximization problem.\n\nconsider the following function:\n\n\\(y = f(z)\\)\nIf you were to maximize this function, what are the first order conditions?\nIf there is a unique solution, what are the conditions to verify it is the maximum.\nQuestion 5\nP1)\nConsider the following set of numbers\n\\(x = \\{10, 4, 5, 7, 3, 6, 9\\}\\)\nEstimate the mean, median, and variance of this data.\np2)\nSay that X is a continuous random variable that ranges from -10 to 10.\n\nWhat is the probability that X=1 (\\(P(x=1)\\))\nTrue or False \\(P(X&gt;1) + p(X&lt;1)=1\\)\nTrue or False \\(P(X&gt;1) + p(-1&lt;X&lt;2)\\) can be larger than 1.\n\nP3)\nSay that Z1 and Z2 are two random variables that follow a bernulli distribution. Assume that the following probabilities are true:\n\np(z1=0 & z2=0) = 0.1\np(z1=1 & z2=1) = 0.2\np(z1=0) = 0.5\np(z2=1) = 0.6\n\nWhat is the probability for:\n\nProbability Z1=1 conditional on z2=0\nunconditional probability of Z1=1\n\np4. Given two random variables x_1 and x_2, with means and variances \\(\\mu_1\\), \\(\\mu_2\\) and \\(\\sigma^2_1\\) and \\(\\sigma^2_2\\).\nif z = x_1 - x_2, what is the mean and variance of Z\nwhat is the covariance between z and x_1."
  },
  {
    "objectID": "rmethods/Syllabus.html",
    "href": "rmethods/Syllabus.html",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "",
    "text": "Instructor: Fernando Rios-Avila\nOffice: Room 307, Blithewood\nOffice Hours: Friday, 9:00 am to 10:00 am or by appointment\nPhone: 845-758-7719\nEmail: friosavi@levy.org\nTime and Location: Wednesday, 9:30 am to 12:50 pm"
  },
  {
    "objectID": "rmethods/Syllabus.html#course-description",
    "href": "rmethods/Syllabus.html#course-description",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Course Description",
    "text": "Course Description\nThe course aims to provide students with a foundation in applied econometrics that is required for the successful completion of the program. The emphasis of the course is on understanding the intuition behind model estimation, hypothesis testing, and economic interpretation of statistical results.\nWe begin by discussing the nature of econometrics and economic data. This is followed by a discussion of estimation and inference in univariate and multivariate regression models of cross-sectional data. We will review some of the consequences of heteroscedasticity, measurement errors, and endogeneity, among other issues of model specification and data measures. The second part of the course will cover advanced regression models such as limited dependent variables, panel data, and time series data.\nThe class is taught through a combination of lectures, discussion, homework, quizzes, and exams. Student involvement and participation in class are highly encouraged."
  },
  {
    "objectID": "rmethods/Syllabus.html#required-text",
    "href": "rmethods/Syllabus.html#required-text",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Required Text",
    "text": "Required Text\nIntroductory Econometrics: A Modern Approach\nby Jeffrey M. Wooldridge\n\nWhile any edition will do, exercises and exam material will be taken from the 7th edition.\n\nSuggested:\nUsing R for Introductory Econometrics (recommended)\nby Florian Heiss\n\nThis book introduces the free programming language and software package R with a focus on the implementation of standard tools and methods used in econometrics. It builds on the textbook “Introductory Econometrics: A Modern Approach” by Jeffrey M. Wooldridge. The book can be accessed online here.\nThe author also provides textbooks using the same structure, introducing Julia and python."
  },
  {
    "objectID": "rmethods/Syllabus.html#course-website",
    "href": "rmethods/Syllabus.html#course-website",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Course website",
    "text": "Course website\nWe will be using Github pages to provide all lectures and homework. It is your responsibility to access the site or communicate with me if any questions should arise.\nThis is where assignments, readings, and other information will be posted."
  },
  {
    "objectID": "rmethods/Syllabus.html#grading",
    "href": "rmethods/Syllabus.html#grading",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Grading",
    "text": "Grading\nGrading will be based on homework assignments, quizzes, and exams. Their grade distribution is as follows:\n\nHomework (55%): Three homework assignments will be given throughout the semester. Homework assignments are prepared for you to implement the methodologies covered in class, as well as encourage you to interpret the results. Each homework will consist of a small research project where you will be asked to answer a series of questions, as if you were writing a research paper.\nThe homework assignments will require using data and the statistical software Stata. You are free to use any of the data sets that come along with the textbook. They can be accessed using frause in Stata. It is encouraged that homework assignments are prepared and submitted in pairs.\nAll homework assignments will be posted on the course website. When submitting your homework, prepare a pdf, html, or doc file with your answers, and a do file with the code used to answer the questions. It is highly encouraged that you use markdown or quarto to prepare your homework, as it easily allows you to incorporate all necessary information to reproduce your results.\nMidterm and Final (40%): Two exams will be given. Each one is prepared to test you on concepts, interpretation, and intuition behind the econometric topics reviewed in class. The exams will be open book and open notes. You are allowed to use any printed material, including the textbook, your notes, and any other material you may find useful. You are not allowed to use any electronic devices, including computers, tablets, or phones, except for the use of a calculator.\nQuestions for the midterm and final will include three sections:\n\nMultiple-choice questions and concept questions.\nAnalytical section, equation solving, and derivations.\nEmpirical section, where you will be asked to interpret results from a regression analysis, as well as implement statistical tests.\n\nAnalytical and empirical sections will be taken from the problem sets and computational exercises in the textbook.\nQuizzes (5%): After each topic, there will be multiple-choice quizzes to test your knowledge of important concepts and ideas seen in class. There will also be open-ended questions or extra projects that will be provided during the semester. This includes 5 extra credit points.\nClass Participation: Class participation is highly encouraged. You are to participate in class discussions. You are also encouraged to ask questions and provide answers to questions asked in class. This counts for up to 5% of extra credit for your final grade."
  },
  {
    "objectID": "rmethods/Syllabus.html#attendance",
    "href": "rmethods/Syllabus.html#attendance",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Attendance:",
    "text": "Attendance:\nClass attendance, in-person or online, is highly recommended. Classes will not be recorded, but for exceptional cases, a link will be provided to attend the class online. Material for exams and homework will come from both class lectures as well as the book.\nThe only acceptable excuses for missing a test are medical reasons or family emergencies. If you have a legitimate excuse, a make-up exam will be issued soon after the date of the original exam. Any issues should be discussed with me before the actual exam takes place."
  },
  {
    "objectID": "rmethods/Syllabus.html#course-software",
    "href": "rmethods/Syllabus.html#course-software",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Course Software",
    "text": "Course Software\nThere are several statistical packages for analyzing data. In this course, we will be using the software Stata to cover all materials in class. Slides are self-replicable, thus you can copy and paste almost all code provided to replicate the results seen in class. The Institute will be providing you with licenses for Stata/BE for the length of the course.\nStata offers many free short webinars and video tutorials that may be useful if you never used Stata before, or even if you have some experience with it. Please see the resources page for more information.\nIf you decide to, you can use R, Julia, or Python to study and work on the course materials and homework. One of the recommended books has nice introductions and code that can help you get started with these software packages. The resources page has additional information on how to get started with these software packages.\nAs with many other skills, the best way to learn is to simply work with the packages, work on the book exercises, and ask any questions to me or your classmates when you find a problem you could not find a solution for."
  },
  {
    "objectID": "rmethods/Syllabus.html#additional-information",
    "href": "rmethods/Syllabus.html#additional-information",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Additional Information:",
    "text": "Additional Information:\nAll students are responsible for knowing Bard’s Policy on Academic Honesty as published in Bard College Student Handbook."
  },
  {
    "objectID": "rmethods/Syllabus.html#syllabus",
    "href": "rmethods/Syllabus.html#syllabus",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Syllabus:",
    "text": "Syllabus:\n\nIntroduction: What is Econometrics?\n\nPart I: Basic tools\n\nThe Simple Regression Model\nMultiple Regression Analysis: Estimation\nMRA: Inference and Asymptotics\n\nPart II: Addressing Problems with MRA\n\nMRA: Scaling, functional forms, Goodness of Fit, and Qualitative Information\nHeteroskedasticity\nFurther Problems: Functional form, Missing variables, measurement errors, missing data\nInstrumental Variables and 2SLS\nLimited Dep Variables\n\nPart III: Panel Data Methods\n\nPool Cross Section and Panel Data\nAdvanced Panel Data Methods\n\nPart IV: Time Series\n\nBasics of Regression analysis with time series data\nAdvanced TSD Problems"
  },
  {
    "objectID": "rmethods2/HomeWork1.html",
    "href": "rmethods2/HomeWork1.html",
    "title": "Homework I",
    "section": "",
    "text": "Consider the following IO table for a hypothetical economy:\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgriculture\nManufacture\nServices\nConstruction\nFinal Demand\nTotal Output\n\n\n\n\nAgriculture\n160\n230\n260\n290\n340\n\n\n\nManufacture\n210\n190\n450\n170\n340\n\n\n\nServices\n410\n380\n200\n160\n350\n\n\n\nConstruction\n180\n320\n240\n170\n280\n\n\n\nLabor\n320\n240\n350\n400\n\n\n\n\n\n\nCalculate total output by Industry\nProvide the Matrix of technical coefficients and labor coefficients for this Economy\nAssume that Final demand has shifted. There is a 30% increase in demand in Construction, but with a 10% decline in demand for services and Manufacturing. Estimate the changes expected in total ouput for all Sectors, as well as the changes in Labor Inputs."
  },
  {
    "objectID": "rmethods2/HomeWork1.html#part-i-io-tables",
    "href": "rmethods2/HomeWork1.html#part-i-io-tables",
    "title": "Homework I",
    "section": "",
    "text": "Consider the following IO table for a hypothetical economy:\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgriculture\nManufacture\nServices\nConstruction\nFinal Demand\nTotal Output\n\n\n\n\nAgriculture\n160\n230\n260\n290\n340\n\n\n\nManufacture\n210\n190\n450\n170\n340\n\n\n\nServices\n410\n380\n200\n160\n350\n\n\n\nConstruction\n180\n320\n240\n170\n280\n\n\n\nLabor\n320\n240\n350\n400\n\n\n\n\n\n\nCalculate total output by Industry\nProvide the Matrix of technical coefficients and labor coefficients for this Economy\nAssume that Final demand has shifted. There is a 30% increase in demand in Construction, but with a 10% decline in demand for services and Manufacturing. Estimate the changes expected in total ouput for all Sectors, as well as the changes in Labor Inputs."
  },
  {
    "objectID": "rmethods2/HomeWork1.html#sec-part2",
    "href": "rmethods2/HomeWork1.html#sec-part2",
    "title": "Homework I",
    "section": "Part II: MLE",
    "text": "Part II: MLE\nConsider data from the American Time use Survey for 2019 atus_2019.dta. This data contains aggregates on various time use activities for 9K individuals. Because this is survey data, be mindful of the sampling weights. You can use either wt06 or wtfinal as the sampling weight.\n\nAt Levy, Household production activities are typically classified as\n\nCore: Main activities taking care of the household\nProc: Procurement, shopping, and other activities related to the household production\nacare and ccare: Activities related to the care of children and other adults in the household\n\nIn the dataset, these variables contain information on hours spend on these activities per day.\nwith this in mind, what is the average time spent on total household production activities per day? when weighted and when unweighted? why are they different?\nEstimate the average time spend on Total household production between weekends and weekdays (use variable wkend_wkday) Are they statistically different?\nHours of Household production have a large share of zeros (about 11% in the data). Because of this, using a simple Linear model may not be appropriate. Instead estimate a Tobit model and Poisson model using individual and household characteristics (plus others of your choice). Discuss why you choose to control for these variables, and intepret the results.\n\nFor the tobit model answer, is this a problem of corner solution or censoring? How would this affect the estimation of marginal effects?"
  },
  {
    "objectID": "rmethods2/HomeWork1.html#part-iii-inequality-gaps",
    "href": "rmethods2/HomeWork1.html#part-iii-inequality-gaps",
    "title": "Homework I",
    "section": "Part III: Inequality Gaps",
    "text": "Part III: Inequality Gaps\nThe GINI index is commonly used to measure income or wealth inequality. However, you could also use the GINI index to measure inequality in other variables.\n\nProduce a table that decomposes the GINI of total hours of household production by source. That is Core, Procurement, child care and adult care.\nWhich one is the component with the greatest share of household production.\nWhich component shows the greatest concentration?\nWhat is the greatest contributor to overall inequality?"
  },
  {
    "objectID": "rmethods2/HomeWork1.html#part-iv-explaining-gaps",
    "href": "rmethods2/HomeWork1.html#part-iv-explaining-gaps",
    "title": "Homework I",
    "section": "Part IV: Explaining Gaps",
    "text": "Part IV: Explaining Gaps\n\nConsidering the methodology known as Oaxaca-Blinder decomposition. Using this methodology, analyze the gender gap on household production using a similar model specification as you did in Part II. Discuss the results.\n\nInclude the use of weights.\nFor better understanding of the gaps, include summary statistics and model coefficients for the relevant regressions and variables."
  },
  {
    "objectID": "rmethods2/index.html",
    "href": "rmethods2/index.html",
    "title": "Research Methods II",
    "section": "",
    "text": "If you are reading this, you are probably enrolled in the course. This page will contain information about the course, including the syllabus, assignments, and other relevant information.\nAs the course progresses, I will add links to the syllabus, assignments, and other relevant information.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods II"
    ]
  },
  {
    "objectID": "rmethods2/index.html#syllabus",
    "href": "rmethods2/index.html#syllabus",
    "title": "Research Methods II",
    "section": "",
    "text": "If you are reading this, you are probably enrolled in the course. This page will contain information about the course, including the syllabus, assignments, and other relevant information.\nAs the course progresses, I will add links to the syllabus, assignments, and other relevant information.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods II"
    ]
  },
  {
    "objectID": "rmethods2/index.html#sessions",
    "href": "rmethods2/index.html#sessions",
    "title": "Research Methods II",
    "section": "Sessions",
    "text": "Sessions\nSession 1: Surveys, IO and SAM\nSlides: html and pdf\nReadings: link\nSession 2: MLE & Limited Dependent Variables\nSlides: html and pdf\nReadings: link\nSession 3: Measuring Inequality\nSlides: html and pdf\nReadings: link\nSession 4: Analyzing Gaps\nSlides: html and pdf\nReadings: link",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods II"
    ]
  },
  {
    "objectID": "rmethods2/index.html#homework-1",
    "href": "rmethods2/index.html#homework-1",
    "title": "Research Methods II",
    "section": "Homework 1",
    "text": "Homework 1\nThe first homework is due on TBD. You can find the instructions here.\nSession 5: Significance and Missing Data\nSlides: html and pdf\nReadings: link\nSession 6: Imputation: Statistical Matching\nSlides: html and pdf\nReadings: link\nSession 7: Micro-Simulations, and Monte Carlo Methods\nSlides: html and pdf\nReadings: link",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods II"
    ]
  },
  {
    "objectID": "rmethods2/index.html#homework-2",
    "href": "rmethods2/index.html#homework-2",
    "title": "Research Methods II",
    "section": "Homework 2",
    "text": "Homework 2\nThe Second homework is due on TBD. You can find the instructions here.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods II"
    ]
  },
  {
    "objectID": "rmethods2/session_2.html#introduction-to-maximum-likelihood-estimation",
    "href": "rmethods2/session_2.html#introduction-to-maximum-likelihood-estimation",
    "title": "Research Methods II",
    "section": "Introduction to Maximum Likelihood Estimation",
    "text": "Introduction to Maximum Likelihood Estimation\n\nThis is something we have seen before.\nMLE is a method to estimate parameters of a model.\n\nIt can be used to estimate paramaters of linear and nonlinear models\n\nThe idea is to find the values of the parameters that maximize the likelihood function.\n\nBut what does it mean?\n\n\n\nThe likelihood function is the probability of observing the data given the parameters of the model.\n\nSo, MLE tries to maximize that probability, under the assumption that we know the distribution of the data.\nIn other words, we try to identify distributions! (not only conditional mean functions)"
  },
  {
    "objectID": "rmethods2/session_2.html#data",
    "href": "rmethods2/session_2.html#data",
    "title": "Research Methods II",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "rmethods2/session_2.html#mle-estimation",
    "href": "rmethods2/session_2.html#mle-estimation",
    "title": "Research Methods II",
    "section": "MLE estimation",
    "text": "MLE estimation\n\nTo identify the parameters of the model, we need to impose assumptions about the distribution of the data.\nFor simplicitly, lets make the assumption that the data is normally distributed.\nThe likelihood function for a single observation is:\n\n\\[L_i(\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2}\\left(\\frac{y_i-\\mu}{\\sigma}\\right)^2 }\\]\n\nAnd under independent observations assumptions, the Likelihood function for the sample is:\n\n\\[LL(\\mu,\\sigma) = \\prod_{i=1}^n L_i(\\mu,\\sigma)\\]"
  },
  {
    "objectID": "rmethods2/session_2.html#graphical-representation",
    "href": "rmethods2/session_2.html#graphical-representation",
    "title": "Research Methods II",
    "section": "Graphical representation",
    "text": "Graphical representation"
  },
  {
    "objectID": "rmethods2/session_2.html#how-good-we-did",
    "href": "rmethods2/session_2.html#how-good-we-did",
    "title": "Research Methods II",
    "section": "How Good we did?",
    "text": "How Good we did?"
  },
  {
    "objectID": "rmethods2/session_2.html#section",
    "href": "rmethods2/session_2.html#section",
    "title": "Research Methods II",
    "section": "",
    "text": "Under that assumption, the likelihood function for a single observation is:\n\n\\[L_i(\\beta,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2}\\left(\\frac{y_i-x_i'\\beta}{\\sigma}\\right)^2 }\\]\n\nWhich can be used to construct the MLE estimator for OLS.\nPlot-twist: The MLE estimator for OLS is the same as the OLS estimator."
  },
  {
    "objectID": "rmethods2/session_2.html#limited-dependent-variables-1",
    "href": "rmethods2/session_2.html#limited-dependent-variables-1",
    "title": "Research Methods II",
    "section": "Limited Dependent Variables",
    "text": "Limited Dependent Variables\n\nLimited dependent variables are variables that are limited in their range of values.\n\nFor example, binary variables, or variables that are bounded between 0 and 1.\nOr variables that are bounded between 0 and some positive number.\nOr variables bounded to take only positive values.\netc\n\nVery Special Case: Endogenous Sample Selection\n\nLooks unbounded, but you only observe a subset of the population."
  },
  {
    "objectID": "rmethods2/session_2.html#binary-data-lpmprobitlogit",
    "href": "rmethods2/session_2.html#binary-data-lpmprobitlogit",
    "title": "Research Methods II",
    "section": "Binary Data: LPM/Probit/logit",
    "text": "Binary Data: LPM/Probit/logit\n\nProbit and logit are two models that are used to model binary dependent variables. (Dummies)\n\nYou can also use OLS (LPM), but has drawbacks\nYou also need to make sure your Dependent Variable is binary!\n\nWhen your Dep variable is binary, your goal is determine the probability of observing a 1 (success) of something to happen given a set of covariates.\n\n\\[P(y_i=1|x_i) = G(x_i'\\beta)\\]\nThe choice of \\(G\\) is what makes the difference between LPM, a probit and logit."
  },
  {
    "objectID": "rmethods2/session_2.html#probitlogit",
    "href": "rmethods2/session_2.html#probitlogit",
    "title": "Research Methods II",
    "section": "Probit/Logit",
    "text": "Probit/Logit\n\n\nLOGIT\n\\[G(Z) = \\frac{e^{Z}}{1+e^{Z}} = \\Lambda(Z)\\]\n\nProbit\n\\[G(Z) = \\int_{-\\infty}^z \\phi(v) dv = \\Phi(Z)\\]\n\n\nBoth make sure that \\(0\\leq G(Z) \\leq 1\\), which doesnt happen with LPM (\\(G(Z)=Z\\))\nAnd with this, we can use MLE to estimate the parameters of the model.\n\n\\[LL(\\beta) = \\prod_{i=1}^n G(x_i'\\beta)^{y_i} (1-G(x_i'\\beta))^{1-y_i}\\]"
  },
  {
    "objectID": "rmethods2/session_2.html#section-1",
    "href": "rmethods2/session_2.html#section-1",
    "title": "Research Methods II",
    "section": "",
    "text": "One could also think of the probit and logit as a transformation of a latent variable \\(Y^*\\).\n\\[Y^*_i = x_i'\\beta + \\epsilon_i\\]\n\nThe latent variable is not observed. However, when \\(Y^*_i&gt;0\\), we observe \\(Y_i=1\\).\n\nHere the probabilty of observing a \\(Y_i=1\\) is:\n\\[\\begin{aligned}\nP(y_i=1|x_i) &= P(y^*_i&gt;0|x_i) = P(x_i'\\beta + \\epsilon_i&gt;0|x_i) \\\\\n  &= P( \\epsilon_i&gt;-x_i'\\beta |x_i) = 1-P( \\epsilon_i&lt;-x_i'\\beta |x_i) \\\\\n  &= 1-G(-x_i'\\beta)\n\\end{aligned}\n\\]\nAnd if \\(G'\\) is symetrical (logit/probit/lpm):\n\\[P(y_i=1|x_i) = G(x_i'\\beta)\\]"
  },
  {
    "objectID": "rmethods2/session_2.html#marginal-effects-and-testing",
    "href": "rmethods2/session_2.html#marginal-effects-and-testing",
    "title": "Research Methods II",
    "section": "Marginal Effects and testing",
    "text": "Marginal Effects and testing\n\nLPM estimates can be interpreted Directly as the change in P(y=1|X)\nFor Logit and probit, we need to compute the marginal effects.\n\n\\[P(y_i=1|x_i) = G(x_i'\\beta)\\]\n\\[\\frac{\\partial P(y_i=1|x_i)}{\\partial x_{ij}} = g(x_i'\\beta)\\beta_j\\]\n\nFor testing,\n\nYou can use the t-test (or z-test for logit/probit) for coefficients or marginal effects\nOr use LR test for joint significance of a set of coefficients.\n\n\n\\[LR = 1- 2 (LL_ur - LL_r) \\sim \\chi^2_q\\]"
  },
  {
    "objectID": "rmethods2/session_2.html#example-stata",
    "href": "rmethods2/session_2.html#example-stata",
    "title": "Research Methods II",
    "section": "Example Stata",
    "text": "Example Stata\nLoad the data\n\nwebuse nhanes2d, clear\ndes highbp height weight age female\nsum   highbp height weight age female i.race [w=finalwgt]\n\n\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nhighbp          byte    %8.0g               * High blood pressure\nheight          float   %9.0g                 Height (cm)\nweight          float   %9.0g                 Weight (kg)\nage             byte    %9.0g                 Age (years)\nfemale          byte    %8.0g      female     Female\n\n    Variable |     Obs      Weight        Mean   Std. dev.       Min        Max\n-------------+-----------------------------------------------------------------\n      highbp |  10,351   117157513    .3685423   .4824328          0          1\n      height |  10,351   117157513    168.4599   9.699111      135.5        200\n      weight |  10,351   117157513    71.90064   15.43281      30.84     175.88\n         age |  10,351   117157513    42.25264   15.50249         20         74\n      female |  10,351   117157513    .5206498   .4995975          0          1\n-------------+-----------------------------------------------------------------\n             |\n        race |\n      White  |  10,351   117157513    .8791545   .3259634          0          1\n      Black  |  10,351   117157513    .0955059   .2939267          0          1\n      Other  |  10,351   117157513    .0253396   .1571621          0          1\n\n\nEstimate mode: LPM using weights\n\nreg highbp height weight age female i.race [pw=finalwgt]\n* or svy: reg highbp height weight age female i.race \n\n(sum of wgt is 117,157,513)\n\nLinear regression                               Number of obs     =     10,351\n                                                F(6, 10344)       =     531.97\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.2110\n                                                Root MSE          =     .42864\n\n------------------------------------------------------------------------------\n             |               Robust\n      highbp | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      height |   -.006271   .0008179    -7.67   0.000    -.0078742   -.0046677\n      weight |   .0097675    .000369    26.47   0.000     .0090441    .0104909\n         age |   .0096675   .0002984    32.40   0.000     .0090826    .0102524\n      female |  -.0794025   .0142293    -5.58   0.000    -.1072948   -.0515103\n             |\n        race |\n      Black  |   .0647166   .0170488     3.80   0.000     .0312977    .0981355\n      Other  |   .0869917   .0381158     2.28   0.022     .0122775     .161706\n             |\n       _cons |    .347133   .1403729     2.47   0.013     .0719749     .622291\n------------------------------------------------------------------------------\n\n\nEstimate mode: Logit using weights\n\nlogit highbp height weight age female i.race [pw=finalwgt]\n* or svy: reg highbp height weight age female i.race \n\n\nIteration 0:  Log pseudolikelihood =  -77110184  \nIteration 1:  Log pseudolikelihood =  -63830529  \nIteration 2:  Log pseudolikelihood =  -63604963  \nIteration 3:  Log pseudolikelihood =  -63604252  \nIteration 4:  Log pseudolikelihood =  -63604252  \n\nLogistic regression                                    Number of obs =  10,351\n                                                       Wald chi2(6)  = 1473.91\n                                                       Prob &gt; chi2   =  0.0000\nLog pseudolikelihood = -63604252                       Pseudo R2     =  0.1752\n\n------------------------------------------------------------------------------\n             |               Robust\n      highbp | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      height |  -.0328739   .0045382    -7.24   0.000    -.0417687   -.0239791\n      weight |   .0514503   .0022181    23.20   0.000     .0471028    .0557977\n         age |   .0496323   .0017152    28.94   0.000     .0462706     .052994\n      female |  -.4472131   .0777753    -5.75   0.000    -.5996498   -.2947764\n             |\n        race |\n      Black  |    .351346   .0915423     3.84   0.000     .1719264    .5307656\n      Other  |   .4929785   .1961652     2.51   0.012     .1085017    .8774552\n             |\n       _cons |  -.7501284   .7683899    -0.98   0.329    -2.256145    .7558881\n------------------------------------------------------------------------------\n\n\nJoint significance test\n\ntest 2.race 3.race\n\n\n ( 1)  [highbp]2.race = 0\n ( 2)  [highbp]3.race = 0\n\n           chi2(  2) =   20.25\n         Prob &gt; chi2 =    0.0000\n\n\nMarginal Effects: You need to use margins command\n\nmargins, dydx(*)\n\n\nAverage marginal effects                                Number of obs = 10,351\nModel VCE: Robust\n\nExpression: Pr(highbp), predict()\ndy/dx wrt:  height weight age female 2.race 3.race\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      height |  -.0059971   .0008145    -7.36   0.000    -.0075936   -.0044007\n      weight |    .009386   .0003507    26.76   0.000     .0086985    .0100734\n         age |   .0090543   .0002562    35.34   0.000     .0085522    .0095564\n      female |  -.0815842   .0141075    -5.78   0.000    -.1092344   -.0539339\n             |\n        race |\n      Black  |   .0654291   .0173285     3.78   0.000     .0314659    .0993923\n      Other  |   .0925816     .03772     2.45   0.014     .0186517    .1665115\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\n\nPredicted Probabilities\n\npredict pr_hat\nhistogram pr_hat  \ngraph export s2fig2.png, replace width(1000)\n\nFrom here, we could also predict HighBP\n\ngen dpr_hat = pr_hat&gt;.5\ntab dpr_hat highbp [w=finalwgt]\n\n\n           |  High blood pressure\n   dpr_hat |         0          1 |     Total\n-----------+----------------------+----------\n         0 |  61819680   20494538 |  82314218 \n         1 |  12160331   22682964 |  34843295 \n-----------+----------------------+----------\n     Total |  73980011   43177502 | 117157513"
  },
  {
    "objectID": "rmethods2/session_2.html#tobit-1",
    "href": "rmethods2/session_2.html#tobit-1",
    "title": "Research Methods II",
    "section": "Tobit",
    "text": "Tobit\n\nTobit models are to analyze data with censored information.\nCensored data means that the data is there… but you dont know the exact value.\nFor example, if you have data on income, but you only know that some people earn less than 10K, but you dont know how much less.\nThe fact that you can see the data, even if you do not know the exact value, helps you to estimate the parameters of the model."
  },
  {
    "objectID": "rmethods2/session_2.html#visualizing-the-problem",
    "href": "rmethods2/session_2.html#visualizing-the-problem",
    "title": "Research Methods II",
    "section": "Visualizing the problem",
    "text": "Visualizing the problem"
  },
  {
    "objectID": "rmethods2/session_2.html#visualizing-the-problem-1",
    "href": "rmethods2/session_2.html#visualizing-the-problem-1",
    "title": "Research Methods II",
    "section": "Visualizing the problem",
    "text": "Visualizing the problem"
  },
  {
    "objectID": "rmethods2/session_2.html#tobit-model",
    "href": "rmethods2/session_2.html#tobit-model",
    "title": "Research Methods II",
    "section": "Tobit Model",
    "text": "Tobit Model\n\nThe idea of the Tobit model is “model” not only why \\(y\\) changes when \\(X\\) changes, but also why y is censored.\n\nAlthough you do that with the same parameters, under normality assumptions.\n\nWhen the data is censored, the likelihood function is similar to a probit model, when the data is not censored, the likelihood function is similar to a linear model:\n\n\\[\\begin{aligned}\nL_i(\\beta,\\sigma) &= \\Phi\\left(\\frac{y^c-x_i'\\beta}{\\sigma}\\right) \\text{if } y_i = y^c \\\\\nL_i(\\beta,\\sigma) &= \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2}\\left(\\frac{y_i-x_i'\\beta}{\\sigma}\\right)^2 } \\text{if } y_i &gt; y^c\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_2.html#estimation-stata",
    "href": "rmethods2/session_2.html#estimation-stata",
    "title": "Research Methods II",
    "section": "Estimation Stata",
    "text": "Estimation Stata\nIn Stata, you can estimate a Tobit model using the tobit command.\ntobit y x1 x2 x3, ll(#)\n\ny: dependent variable\nx1 x2 x3: independent variable\nll(#): is the value of the censoring point."
  },
  {
    "objectID": "rmethods2/session_2.html#visualizing-the-solution",
    "href": "rmethods2/session_2.html#visualizing-the-solution",
    "title": "Research Methods II",
    "section": "Visualizing the solution",
    "text": "Visualizing the solution"
  },
  {
    "objectID": "rmethods2/session_2.html#latent-variable",
    "href": "rmethods2/session_2.html#latent-variable",
    "title": "Research Methods II",
    "section": "Latent Variable",
    "text": "Latent Variable\n\nThe easiest to interpret is the latent variable.\nFor example, say that you are interested in the effect of education on wages, but wages are censored at 10.\nIn this case the coefficients of the Tobit model are the same as the coefficients of the linear model.\n\ntobit y x, ll(0)\n\nuse margins if you have interactions or polynomial terms."
  },
  {
    "objectID": "rmethods2/session_2.html#data-is-corner-solution",
    "href": "rmethods2/session_2.html#data-is-corner-solution",
    "title": "Research Methods II",
    "section": "Data is Corner Solution:",
    "text": "Data is Corner Solution:\n\nIf data is corner solution, then you need to decide what to interpret.\n\nFor example, say you are interested in the effect of education hours of work\nHours of work cannot fall below 0.\nBut you know education has a positive effect (on something)\n\nWould you be interested in the effect on the probability of working?\nThe effect on hours of work for those who work?\nThe overall average effect on hours of work? (some will enter the labor force, some will work more hours)"
  },
  {
    "objectID": "rmethods2/session_2.html#probability-of-working",
    "href": "rmethods2/session_2.html#probability-of-working",
    "title": "Research Methods II",
    "section": "Probability of Working",
    "text": "Probability of Working\n\\[P(y_i&gt;0|x_i) = \\Phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right)\\]\nmargins, dydx(x) predict(pr(0,.))\n\npredict(pr(0,.)) says you are interested in the probability that data was not censored…or in this case that was not a corner solution"
  },
  {
    "objectID": "rmethods2/session_2.html#eyy0x",
    "href": "rmethods2/session_2.html#eyy0x",
    "title": "Research Methods II",
    "section": "E(Y|Y>0,X)",
    "text": "E(Y|Y&gt;0,X)\n\\[\\begin{aligned}\ny_i &= x_i'\\beta + \\epsilon_i \\ || E(|y&gt;0,X) \\\\\nE(y_i|y_i&gt;0,x_i) &= x_i'\\beta + \\sigma\\lambda\\left(\\frac{x_i'\\beta}{\\sigma}\\right) \\\\\n\\lambda(z) &= \\frac{\\phi(z)}{\\Phi(z)}\n\\end{aligned}\n\\]\nThis is the expected value of the latent variable, conditional on the latent variable being positive.\nmargins, dydx(x) predict(e(0,.))\n\npredict(e(0,.)) says you are interested in the expected change only for those who currently work."
  },
  {
    "objectID": "rmethods2/session_2.html#eyx",
    "href": "rmethods2/session_2.html#eyx",
    "title": "Research Methods II",
    "section": "E(Y|X)",
    "text": "E(Y|X)\n\\[\\begin{aligned}\nE(y_i|x_i) &= E(y_i|y_i&gt;0,x_i) * P(y_i&gt;0|x_i) + 0 * (1-P(y_i&gt;0|x_i)) \\\\\nE(y_i|x_i) &= \\Phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right)\n\\left( x_i'\\beta + \\sigma\\lambda\\left(\\frac{x_i'\\beta}{\\sigma}\\right)\\right) \\\\\nE(y_i|x_i) &= \\Phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right) x_i'\\beta + \\sigma \\phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right)\n\\end{aligned}\n\\]\nmargins, dydx(x) predict(ystar(0,.))\n\npredict(ystar(0,.)) says you are interested in the average effect considering those who work and those who do not work."
  },
  {
    "objectID": "rmethods2/session_2.html#visualizing-the-solution-1",
    "href": "rmethods2/session_2.html#visualizing-the-solution-1",
    "title": "Research Methods II",
    "section": "Visualizing the solution",
    "text": "Visualizing the solution"
  },
  {
    "objectID": "rmethods2/session_2.html#exogenous-sample-selection",
    "href": "rmethods2/session_2.html#exogenous-sample-selection",
    "title": "Research Methods II",
    "section": "Exogenous Sample Selection",
    "text": "Exogenous Sample Selection\n\nFirst: Samples already represent a selection of the population.\n\nhowever, because the selection is random, all assumptions of OLS are satisfied. (if they are true for the population.)\n\nSecond: Some times selection may not be random, but based on observed (and control) characteristics\n\nNot a problem either. Since you could at least say something for those you observe. (if you have the right variables)\nThis was exogenous sample selection.\n\n\nie: You want to estimate the effect of education on wages, but you only have data for highly educated people."
  },
  {
    "objectID": "rmethods2/session_2.html#endogenous-sample-selection",
    "href": "rmethods2/session_2.html#endogenous-sample-selection",
    "title": "Research Methods II",
    "section": "Endogenous Sample Selection",
    "text": "Endogenous Sample Selection\n\nThird: Selection may be based on unobserved characteristics.\n\nThis is a problem. The reason why we do not observe data is for unknown reasons (part of the error).\nBecause we cannot control for it, it will bias our estimates. (like omitted variable bias)\nThis is endogenous sample selection.\n\n\nie: - You want to estimate the effect of education on wages, but you only have data for those who work. - Those who work do so because they may have been offer higher wages for unknown reasons. (high skill? high motivation? )"
  },
  {
    "objectID": "rmethods2/session_2.html#heckman-selection-model",
    "href": "rmethods2/session_2.html#heckman-selection-model",
    "title": "Research Methods II",
    "section": "Heckman Selection Model",
    "text": "Heckman Selection Model\n\nThe Heckman selection model is an estimation method that allows you to correct a specific kind of endogenous sample selection.\nConsider the following model:\n\n\\[y_i = x_i'\\beta + \\epsilon_i\\]\n\nIn absence of selection, we can assume standard assumtions and estimate the model using OLS.\nHowever, if we have endogenous selection, usually means that we have a second equation that determines the selection.\n\n\\[s_i^* = z_i'\\gamma + \\eta_i\\]"
  },
  {
    "objectID": "rmethods2/session_2.html#section-4",
    "href": "rmethods2/session_2.html#section-4",
    "title": "Research Methods II",
    "section": "",
    "text": "The Full model:\n\\[\\begin{aligned}\ny_i &= x_i'\\beta + \\epsilon_i \\text{ if } s_i^*&gt;0 \\\\\ns_i^* &= z_i'\\gamma + \\eta_i \\\\\n\\epsilon &\\sim N(0,\\sigma_\\epsilon) \\\\\n\\eta &\\sim N(0,1) \\\\\ncorr(\\epsilon,\\eta) &= \\rho\n\\end{aligned}\n\\]\n\n\\(x_i\\) and \\(z_i\\) are not necessarily the same. But they are exogenous to the error terms.\nThe selection equation depends on observable \\(z_i\\) and unobservable \\(\\eta_i\\) factors.\nThe unobserable factors are correlated with the error term of the main equation.\nThe problem: if \\(\\rho\\neq 0\\) then \\(E(\\epsilon|s_i^*&gt;0,x_i) \\neq 0\\)."
  },
  {
    "objectID": "rmethods2/session_2.html#solution",
    "href": "rmethods2/session_2.html#solution",
    "title": "Research Methods II",
    "section": "Solution",
    "text": "Solution\n\nThe solution is to “control” for the unobserved factors that are correlated with \\(\\epsilon\\).\n\n\\[\\begin{aligned}\ny_i &= x_i'\\beta + \\epsilon_i \\ || \\ E( * |x_i,s_i^*&gt;0) \\\\\nE(y_i|x_i,s_i^*&gt;0) &= x_i'\\beta + E(\\epsilon_i|x,z,\\eta, s_i^*&gt;0) \\\\\n&= x_i'\\beta + E(\\epsilon_i|\\eta,s_i^*&gt;0) \\\\\n  &= x_i'\\beta + \\rho \\frac{\\phi(z_i'\\gamma)}{\\Phi(z_i'\\gamma)} \\\\\n  &= x_i'\\beta + \\rho \\lambda (z_i'\\gamma) \\\\\n\\end{aligned}\n\\]\nThus the new model is:\n\\[y_i  = x_i'\\beta + \\rho \\lambda (z_i'\\gamma) + \\varepsilon_i\\]\nwhere \\(\\gamma\\) is estimated using a probit model."
  },
  {
    "objectID": "rmethods2/session_2.html#implementation",
    "href": "rmethods2/session_2.html#implementation",
    "title": "Research Methods II",
    "section": "Implementation",
    "text": "Implementation\n\nTwo options:\n\nEstimate both outcome and selection equation jointly using MLE.\n\n\nRequires careful setup of the likelihood function.\nImposes the assumption of joint normality of the error terms.\n\n\nEstimate it using a two-step procedure.(Heckit)\n\n\nEstimate the selection equation using probit. \\(z_i'\\gamma\\)\nEstimate the outcome equation using OLS, inclusing inverse mills ratio. \\(\\lambda (z_i'\\gamma)\\)\nStd Errs need to be corrected\n\nConsideration: In contrast with IV, Heckman does not require an instrument, but having one is highly recommended."
  },
  {
    "objectID": "rmethods2/session_2.html#example-stata-1",
    "href": "rmethods2/session_2.html#example-stata-1",
    "title": "Research Methods II",
    "section": "Example Stata",
    "text": "Example Stata\nLets start by loading some data\n\nwebuse womenwk, clear\ndescribe\n\n\nContains data from https://www.stata-press.com/data/r18/womenwk.dta\n Observations:         2,000                  \n    Variables:             6                  3 Mar 2022 07:43\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\ncounty          byte    %9.0g                 County of residence\nage             byte    %8.0g                 Age in years\neducation       byte    %8.0g                 Years of schooling\nmarried         byte    %8.0g                 1 if married spouse present\nchildren        byte    %8.0g                 # of children under 12 years old\nwage            float   %9.0g                 Hourly wage; missing, if not\n                                                working\n-------------------------------------------------------------------------------\nSorted by: \n\n\nIn Stata, we can use command heckman to estimate the Heckman selection model, but lets start by doing this manually\n\ngen works = (wage!=.)\n** Selection model\nprobit works married children educ age\npredict zg, xb\n\n\nIteration 0:  Log likelihood = -1266.2225  \nIteration 1:  Log likelihood = -1031.4962  \nIteration 2:  Log likelihood = -1027.0625  \nIteration 3:  Log likelihood = -1027.0616  \nIteration 4:  Log likelihood = -1027.0616  \n\nProbit regression                                       Number of obs =  2,000\n                                                        LR chi2(4)    = 478.32\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -1027.0616                             Pseudo R2     = 0.1889\n\n------------------------------------------------------------------------------\n       works | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     married |   .4308575    .074208     5.81   0.000     .2854125    .5763025\n    children |   .4473249   .0287417    15.56   0.000     .3909922    .5036576\n   education |   .0583645   .0109742     5.32   0.000     .0368555    .0798735\n         age |   .0347211   .0042293     8.21   0.000     .0264318    .0430105\n       _cons |  -2.467365   .1925635   -12.81   0.000    -2.844782   -2.089948\n------------------------------------------------------------------------------\n\n\nThis selection equation can be interpreted the usual way\nThe outcome model:\n\ngen mill = normalden(zg)/normal(zg)\nreg  wage educ age mill\nest sto hkit\n\n\n      Source |       SS           df       MS      Number of obs   =     1,343\n-------------+----------------------------------   F(3, 1339)      =    173.01\n       Model |  14904.6806         3  4968.22688   Prob &gt; F        =    0.0000\n    Residual |   38450.214     1,339  28.7156191   R-squared       =    0.2793\n-------------+----------------------------------   Adj R-squared   =    0.2777\n       Total |  53354.8946     1,342  39.7577456   Root MSE        =    5.3587\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   education |   .9825259   .0504982    19.46   0.000     .8834616     1.08159\n         age |   .2118695   .0206636    10.25   0.000      .171333     .252406\n        mill |   4.001616   .5771027     6.93   0.000     2.869492    5.133739\n       _cons |   .7340391   1.166214     0.63   0.529    -1.553766    3.021844\n------------------------------------------------------------------------------\n\n\nLets compare the outcome with Stata’s Heckman\n\nset linesize 255\nreg wag educ age\nest sto ols\nheckman wage educ age, select(works = married children educ age) twostep\nest sto hecktwo\nheckman wage educ age, select(works = married children educ age) \nest sto heckmle\n\n\nesttab ols hkit hecktwo heckmle, se(%9.3f) b(%9.3f) star(* 0.10 ** 0.05 *** 0.01) nogaps mtitle(OLS Heckit Hkm-two Hkm-mle) nonum\n\n\n----------------------------------------------------------------------------\n                      OLS          Heckit         Hkm-two         Hkm-mle   \n----------------------------------------------------------------------------\nmain                                                                        \neducation           0.897***        0.983***        0.983***        0.990***\n                  (0.050)         (0.050)         (0.054)         (0.053)   \nage                 0.147***        0.212***        0.212***        0.213***\n                  (0.019)         (0.021)         (0.022)         (0.021)   \nmill                                4.002***                                \n                                  (0.577)                                   \n_cons               6.085***        0.734           0.734           0.486   \n                  (0.890)         (1.166)         (1.248)         (1.077)   \n----------------------------------------------------------------------------\nworks                                                                       \nmarried                                             0.431***        0.445***\n                                                  (0.074)         (0.067)   \nchildren                                            0.447***        0.439***\n                                                  (0.029)         (0.028)   \neducation                                           0.058***        0.056***\n                                                  (0.011)         (0.011)   \nage                                                 0.035***        0.037***\n                                                  (0.004)         (0.004)   \n_cons                                              -2.467***       -2.491***\n                                                  (0.193)         (0.189)   \n----------------------------------------------------------------------------\n/mills                                                                      \nlambda                                              4.002***                \n                                                  (0.607)                   \n----------------------------------------------------------------------------\n/                                                                           \nathrho                                                              0.874***\n                                                                  (0.101)   \nlnsigma                                                             1.793***\n                                                                  (0.028)   \n----------------------------------------------------------------------------\nN                    1343            1343            2000            2000   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.10, ** p&lt;0.05, *** p&lt;0.01"
  },
  {
    "objectID": "rmethods2/session_2.html#interpretation",
    "href": "rmethods2/session_2.html#interpretation",
    "title": "Research Methods II",
    "section": "Interpretation",
    "text": "Interpretation\n\nIt depends…\nbut the most likely scenario is to interpret the outcomes for everyone (thus just look at coefficients of the outcome equation)\nBut you can also obtain effects for those who work, or the average effect.\nThe Mills ratio can be interpreted as the direction of the selection.\n\nIf positive, then those who work are those who earn more\nIf negative, then those who work are those who earn less"
  },
  {
    "objectID": "rmethods2/session_2.html#extra-example",
    "href": "rmethods2/session_2.html#extra-example",
    "title": "Research Methods II",
    "section": "Extra example",
    "text": "Extra example\n\nfrause oaxaca, clear\nreg lnwage educ exper tenure \nest sto ols\nheckman lnwage educ exper tenure  , select(lfp =educ age   married divorced kids6 kids714) \nest sto hk_mle\nheckman lnwage educ exper tenure  , select(lfp =educ age   married divorced kids6 kids714) two\nest sto hk_two\n\n\nesttab ols hk_mle hk_two, se(%9.3f) b(%9.3f) star(* 0.10 ** 0.05 *** 0.01) nogaps mtitle(OLS Heckit Hkm-two) nonum  \n\n\n------------------------------------------------------------\n                      OLS          Heckit         Hkm-two   \n------------------------------------------------------------\nmain                                                        \neduc                0.087***        0.091***        0.094***\n                  (0.005)         (0.005)         (0.006)   \nexper               0.011***        0.011***        0.010***\n                  (0.002)         (0.002)         (0.002)   \ntenure              0.008***        0.008***        0.007***\n                  (0.002)         (0.002)         (0.002)   \n_cons               2.140***        2.079***        2.024***\n                  (0.065)         (0.067)         (0.072)   \n------------------------------------------------------------\nlfp                                                         \neduc                                0.168***        0.183***\n                                  (0.025)         (0.025)   \nage                                -0.028***       -0.029***\n                                  (0.006)         (0.006)   \nmarried                            -0.853***       -0.832***\n                                  (0.191)         (0.185)   \ndivorced                           -0.324          -0.239   \n                                  (0.229)         (0.222)   \nkids6                              -0.590***       -0.573***\n                                  (0.069)         (0.070)   \nkids714                            -0.318***       -0.307***\n                                  (0.057)         (0.058)   \n_cons                               1.454***        1.313***\n                                  (0.336)         (0.355)   \n------------------------------------------------------------\n/                                                           \nathrho                              0.343***                \n                                  (0.082)                   \nlnsigma                            -0.750***                \n                                  (0.020)                   \n------------------------------------------------------------\n/mills                                                      \nlambda                                              0.293***\n                                                  (0.066)   \n------------------------------------------------------------\nN                    1434            1647            1647   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.10, ** p&lt;0.05, *** p&lt;0.01"
  },
  {
    "objectID": "rmethods2/session_4.html#introduction",
    "href": "rmethods2/session_4.html#introduction",
    "title": "Research Methods II",
    "section": "Introduction",
    "text": "Introduction\n\nWhat do we mean by decomposition?\n\n\nIn Economics, decomposition refers to the process breaking down an index or aggregate measure into factors that explain it.\nWe have done some of this last week with Decomposition by groups and sources.\nBut there are other types of decompositions that are useful in Economics.\n\n\\[\n\\begin{aligned}\ny &=A L^\\alpha K ^\\beta \\\\\n\\frac{\\Delta y}{y} &=\\frac{\\Delta A}{A} + \\alpha \\frac{\\Delta L}{L} + \\beta \\frac{\\Delta K}{K}\n\\end{aligned}\n\\]\n\nToday we will focus on a different kind of Decomposition: Decomposition of gaps."
  },
  {
    "objectID": "rmethods2/session_4.html#section",
    "href": "rmethods2/session_4.html#section",
    "title": "Research Methods II",
    "section": "",
    "text": "Introduction to Oaxaca-Blinder Decomposition\n\nThe most common type of decomposition of gaps is the Oaxaca-Blinder Decomposition.\nThe idea behind is as follows:\n\nThere are two groups you are interested in comparing.\nGroup A has a higher average value of a variable of interest than Group B.\n\nWhat explains the difference??\n\nThe Oaxaca-Blinder Decomposition allows you to answer this question.\n\nThe difference could be due to differences in the characteristics\nOr it could be due to differences in the returns to those characteristics."
  },
  {
    "objectID": "rmethods2/session_4.html#section-1",
    "href": "rmethods2/session_4.html#section-1",
    "title": "Research Methods II",
    "section": "",
    "text": "Introduction to Oaxaca-Blinder Decomposition\n\nIn 1973 Oaxaca and Blinder independently proposed a very similar method to decompose the differences in averages value of a variable of interest between two groups.\n\n\n“Male-Female Wage Differentials in Urban Labor Markets” Oaxaca (IER 1973)\n\n\n“Wage Discrimination: Reduced Form and Structural Estimates” Blinder (JHR 1973)\n\n\nWhich become the basis for what is known as the Oaxaca-Blinder Decomposition.\nHeavily used in Labor Economics, it can be helpful to explain what factors relate to: Union premiums, povery gaps, gender wage gaps, etc."
  },
  {
    "objectID": "rmethods2/session_4.html#section-2",
    "href": "rmethods2/session_4.html#section-2",
    "title": "Research Methods II",
    "section": "",
    "text": "General Framework\n\nSuppose we have two groups: \\(A\\) and \\(B\\), with Data generating processes (DGP) that are defined as:\n\n\\[\\begin{aligned}\nY_a = G_a(X_a,\\epsilon_a) \\\\\nY_b = G_b(X_b,\\epsilon_b) \\\\\n\\end{aligned}\n\\]\n\nDifferences between two groups could be explain by:\n\nDifferences in observed characteristics \\(X_a\\) and \\(X_b\\)\nDifferences in unobserved \\(\\epsilon_a\\) and \\(\\epsilon_b\\)\nDifferences in the Funcional forms \\(G_a\\) and \\(G_b\\)\n\nTo some extent, this suggests something akin to a counterfactual."
  },
  {
    "objectID": "rmethods2/session_4.html#section-3",
    "href": "rmethods2/session_4.html#section-3",
    "title": "Research Methods II",
    "section": "",
    "text": "Imposing Restrictions\nTo implement OB, we need to impose some restrictions on the model.\n1st Functional form: Linear in parameters \\[G_a(X_k,\\epsilon_k) = X_k \\beta_k + \\epsilon_k\n\\]\n2nd Errors are independent by group: \\[\\varepsilon_i \\perp D | X_i\n\\]\nThis makes it possible to have other problems in the model (like endogeneity) and still get aggregate consistent estimates. (but lets assume Zero Conditional Mean )\n3rd Homoskedastic (across groups)\n4th We only care about Differences in means"
  },
  {
    "objectID": "rmethods2/session_4.html#section-4",
    "href": "rmethods2/session_4.html#section-4",
    "title": "Research Methods II",
    "section": "",
    "text": "OB In Action\n\nSuppose the models are given by:\n\n\\[Y_k = X_k \\beta_k + \\epsilon_k\n\\]\n\nThen, the “average” outcome for each group is given by:\n\n\\[\\bar Y_k =\\bar X_k \\beta_k\n\\]\n\nThis is useful, because we could now use it for creating a counterfactual.\n\n\\(\\bar X_a \\beta_a\\) is the average wage of group A\n\\(\\bar X_b \\beta_a\\) is the average wage of group B if “paid like” group A.\n\nor Average Wages of Group A if they had the characteristics of Group B.\n\n\nWith this, we can now obtain the OB Decomposition."
  },
  {
    "objectID": "rmethods2/session_4.html#section-5",
    "href": "rmethods2/session_4.html#section-5",
    "title": "Research Methods II",
    "section": "",
    "text": "\\[\\begin{aligned}\n\\Delta \\bar Y &= \\bar Y_a - \\bar Y_b \\\\\n&= \\bar X_a \\beta_a - \\bar X_b \\beta_b \\\\\n\\end{aligned}\n\\]\n\nNow we need a Counterfactual: What if Group A were paid as of Group B? \\(\\bar X_a \\beta_b\\)\n\n\\[\\begin{aligned}\n\\Delta \\bar Y &= \\bar Y_a - \\bar Y_b + \\bar Y^c_a - \\bar Y^c_a\\\\\n&= \\color{blue}{\\bar X_a \\beta_a} - \\color{red}{\\bar X_b \\beta_b + \\bar X_a \\beta_b} - \\color{blue}{\\bar X_a \\beta_b} \\\\\n&= \\color{blue}{(\\bar X_a \\beta_a- \\bar X_a \\beta_b)}  + \\color{red}{ (\\bar X_a \\beta_b - \\bar X_b \\beta_b)} \\\\\n&= \\color{blue}{\\bar X_a (\\beta_a- \\beta_b)}  + \\color{red}{  (\\bar X_a - \\bar X_b) \\beta_b} \\\\\n&= \\color{blue}{\\bar X_a \\Delta \\beta}  + \\color{red}{  \\Delta \\bar X \\beta_b} \\\\\n\\end{aligned}\n\\]\nThus Differences in averages is decomposed into two parts:\n\nDiff in \\(X's\\) and (weighted by \\(\\beta_b\\))\nDiff in \\(\\beta's\\). (weighted by \\(\\bar X_a\\))"
  },
  {
    "objectID": "rmethods2/session_4.html#section-7",
    "href": "rmethods2/session_4.html#section-7",
    "title": "Research Methods II",
    "section": "",
    "text": "OB: From Aggregate to detailed\n\nThe previous decomposition is for Aggregates.\n\nThey are robust to endogeneity, (if endogeneity is the same across groups)\n\nIf the model is correctly specified, we can also decompose the differences in the detailed level.\n\n\\[\\begin{aligned}\n\\Delta X \\beta_b &= \\beta_{a0}-\\beta_{b0} + \\sum_{j=1}^k \\bar X_{aj}(\\beta_{aj} - \\beta_{bj}) \\\\\n\\bar X_a \\Delta \\beta &= \\sum_{j=1}^k (\\bar X_{aj} - \\bar X_{bj}) \\beta_{bj}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_4.html#section-8",
    "href": "rmethods2/session_4.html#section-8",
    "title": "Research Methods II",
    "section": "",
    "text": "Note:\n\nThe decomposition is not unique.\n\nDepends on the “counterfactual” we choose.\n\nThe decomposition is not causal, but may be indicative of potential causes.\nThe decomposition is not a test of discrimination.\n\nAs it does not assess why the differences in \\(\\beta\\) exist, nor what explains the differences in \\(\\bar X\\)."
  },
  {
    "objectID": "rmethods2/session_4.html#section-9",
    "href": "rmethods2/session_4.html#section-9",
    "title": "Research Methods II",
    "section": "",
    "text": "In a picture\n\nOption 1Option 2note\n\n\n\n\n\n\n\n\n\nNeither option is “correct” or “incorrect”.\nThey are just different ways of measuring the gaps.\nHowever, you may want to consider which one is more appropriate for your research question.\nOr consider other decomposition options\n\nSingle Ref group with 3-way Decomposition"
  },
  {
    "objectID": "rmethods2/session_4.html#section-10",
    "href": "rmethods2/session_4.html#section-10",
    "title": "Research Methods II",
    "section": "",
    "text": "3-way Decomposition\n\nOption 1Option 2note\n\n\n\n\n\n\n\n\n\nMathematically:\n\n\\[\\begin{aligned}\n\\Delta \\bar Y &= \\bar Y_a - \\bar Y_b \\\\\n&= \\bar X_a \\beta_a - \\bar X_b \\beta_b \\\\\n&= \\bar X_a \\Delta \\beta  +   \\Delta \\bar X \\beta_b +   \\Delta \\bar X \\beta_a - \\Delta \\bar X \\beta_a \\\\\n&= {\\bar X_a \\Delta \\beta}  + \\Delta \\bar X \\beta_a - \\Delta \\bar X \\Delta \\beta \\\\\n\\text{ or } \\\\\n&= \\bar X_a \\Delta \\beta  +   \\Delta \\bar X \\beta_b +   \\bar X_b \\Delta \\beta - \\bar X_b \\Delta \\beta  \\\\\n&= \\bar X_b \\Delta \\beta + \\Delta \\bar X \\beta_b + \\Delta X \\Delta \\beta\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_4.html#example",
    "href": "rmethods2/session_4.html#example",
    "title": "Research Methods II",
    "section": "Example:",
    "text": "Example:\n\n\nCode\nfrause oaxaca, clear\ndrop if lnwage == .\n\n\n\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n\n\nFirst things first:\n\nDefine your groups of interest: Men vs Women\nIdentify your model of interest: Simple Specification\nAnd obtain Summary Statistics\n\n\n\nCode\ntabstat lnwage educ exper age married , by(female)\n\n\n\nSummary statistics: Mean\nGroup variable: female (sex of respondent (1=female))\n\n  female |    lnwage      educ     exper       age   married\n---------+--------------------------------------------------\n       0 |  3.440222  11.81425  14.07684  38.43142  .5219707\n       1 |  3.266761  11.23206  12.13769  39.28697  .4128843\n---------+--------------------------------------------------\n   Total |  3.357604  11.53696  13.15324  38.83891  .4700139\n------------------------------------------------------------\n\n\n\n\nCode\ngen cns = 1\nqui:mean educ exper age married cns if female == 0\nmatrix x_men = e(b)\nqui:mean educ exper age married cns if female == 1\nmatrix x_women = e(b)\n\n\nSecond, estimate models of interest\n\n\nCode\nqui: reg lnwage educ exper age married if female==0\nest sto men\nmatrix b_men = e(b)\nqui: reg lnwage educ exper age married if female==1\nmatrix b_women = e(b)\nest sto women\nesttab men women, nogaps se mtitle(\"Men\" \"Women\")\n\n\n\n--------------------------------------------\n                      (1)             (2)   \n                      Men           Women   \n--------------------------------------------\neduc               0.0540***       0.0853***\n                (0.00595)       (0.00871)   \nexper            -0.00495*        0.00776*  \n                (0.00197)       (0.00346)   \nage                0.0216***      0.00808** \n                (0.00202)       (0.00271)   \nmarried             0.173***       -0.121** \n                 (0.0293)        (0.0421)   \n_cons               1.950***        1.947***\n                 (0.0757)         (0.124)   \n--------------------------------------------\nN                     751             683   \n--------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n\n\n\nCode\nmatrix DX = x_men - x_women \nmatrix DB = b_men - b_women\n\nmatrix DX_bw = DX * b_women'\nmatrix Xm_Db = x_men * DB'\n\nmatrix DX_bm = DX * b_men'\nmatrix Xw_Db = x_women * DB'\n\nmatrix dDX_bw = vecdiag(DX' * b_women)'\nmatrix dXm_Db = vecdiag(x_men' * DB)'\n\nmatrix dDX_bm = vecdiag(DX' * b_men)'\nmatrix dXw_Db = vecdiag(x_women' * DB)'\n\n** Total Gap Returns\nmatrix result = DX_bw + Xm_Db, DX_bw, Xm_Db, DX_bm, Xw_Db\nmatrix result =result\\ dDX_bw + dXm_Db, dDX_bw, dXm_Db, dDX_bm, dXw_Db\nmatrix colname result = DY  DX_bw Xm_Db DX_bm  Xw_Db\nmatrix coleq result = \"\" op1 op1 op2 op2\n\n\nmatrix list result, format(%9.4f)\n\n\n\nresult[6,5]\n                      op1:     op1:     op2:     op2:\n              DY    DX_bw    Xm_Db    DX_bm    Xw_Db\n     y1   0.1735   0.0446   0.1289   0.0222   0.1513\n   educ  -0.3192   0.0496  -0.3689   0.0315  -0.3507\n  exper  -0.1638   0.0150  -0.1789  -0.0096  -0.1542\n    age   0.5141  -0.0069   0.5210  -0.0185   0.5326\nmarried   0.1401  -0.0132   0.1533   0.0189   0.1213\n  _cons   0.0023   0.0000   0.0023   0.0000   0.0023\n\n\n\nNegative numbers “Contract” the gap,\nPositive, “Expand” the gap."
  },
  {
    "objectID": "rmethods2/session_4.html#the-oaxaca-way",
    "href": "rmethods2/session_4.html#the-oaxaca-way",
    "title": "Research Methods II",
    "section": "The oaxaca way",
    "text": "The oaxaca way\n\n\nCode\nssc install oaxaca\nqui:oaxaca lnwage educ exper age married, by(female) w(0)  \nest sto m1\nqui:oaxaca lnwage educ exper age married, by(female) w(1)  \nest sto m2\nesttab m1 m2, wide mtitle(bw_Xm Xm_bw)\n\n\nchecking oaxaca consistency and verifying not already installed...\nall files already exist and are up to date.\n\n----------------------------------------------------------------------\n                      (1)                          (2)                \n                    bw_Xm                        Xm_bw                \n----------------------------------------------------------------------\noverall                                                               \ngroup_1             3.440***     (196.70)        3.440***     (196.70)\ngroup_2             3.267***     (149.41)        3.267***     (149.41)\ndifference          0.173***       (6.20)        0.173***       (6.20)\nexplained          0.0446*         (2.50)       0.0222          (1.28)\nunexplained         0.129***       (4.63)        0.151***       (5.69)\n----------------------------------------------------------------------\nexplained                                                             \neduc               0.0496***       (4.15)       0.0315***       (4.09)\nexper              0.0150          (1.92)     -0.00960*        (-2.08)\nage              -0.00691         (-1.32)      -0.0185         (-1.46)\nmarried           -0.0132*        (-2.36)       0.0189***       (3.40)\n----------------------------------------------------------------------\nunexplained                                                           \neduc               -0.369**       (-2.96)       -0.351**       (-2.96)\nexper              -0.179**       (-3.17)       -0.154**       (-3.18)\nage                 0.521***       (4.00)        0.533***       (4.00)\nmarried             0.153***       (5.62)        0.121***       (5.54)\n_cons             0.00234          (0.02)      0.00234          (0.02)\n----------------------------------------------------------------------\nN                    1434                         1434                \n----------------------------------------------------------------------\nt statistics in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods2/session_4.html#choosing-the-counterfactual",
    "href": "rmethods2/session_4.html#choosing-the-counterfactual",
    "title": "Research Methods II",
    "section": "Choosing the Counterfactual",
    "text": "Choosing the Counterfactual\n\nWhat should be a good counterfactual?\n\nThe one that is most relevant to your research question.\nThe one that is most likely to be true.\nThe one that is not affected by discrimination.\n\nWhile most of the time, decomposition results do not change much respect to the counterfactual, some times they do.\nWhat to do?"
  },
  {
    "objectID": "rmethods2/session_4.html#section-11",
    "href": "rmethods2/session_4.html#section-11",
    "title": "Research Methods II",
    "section": "",
    "text": "Choosing the Counterfactual\n\nDefault counterfactual is to use predict wages for group A, using Wage structure of group B.\nSome times it may make more sense choosing something in between:\n\n\\[\\beta_c = \\omega \\beta_a + (1-\\omega) \\beta_b\n\\]\nThis is the meaning of w() in Oaxaca\n\nSome times, you may want to use \\(\\beta's\\) from a pool model (omega option in Oaxaca)"
  },
  {
    "objectID": "rmethods2/session_4.html#ob-cheat-sheet",
    "href": "rmethods2/session_4.html#ob-cheat-sheet",
    "title": "Research Methods II",
    "section": "OB Cheat Sheet",
    "text": "OB Cheat Sheet\n\nStata Implementation: oaxaca by Jann (2008)\nTypes of Decompositions\n\nTrifold Decomposition: oaxaca y x1 x2 x3 x4, by(group)\nStandard Decompositions: oaxaca y x1 x2 x3 x4, by(group) w(0)  [w(1) ]\nReimiers (1983) Decomposition: oaxaca y x1 x2 x3 x4, by(group) w(0.5)\nCotton(1983) Decomposition: oaxaca y x1 x2 x3 x4, by(group) w(#=Share)\nOaxaca Ransom (1988,1994) and Neumark (1988) Decomposition: oaxaca y x1 x2 x3 x4, by(group) omega\nCain (1986): oaxaca y x1 x2 x3 x4, by(group) pool"
  },
  {
    "objectID": "rmethods2/session_4.html#beyond-microdata---a-note",
    "href": "rmethods2/session_4.html#beyond-microdata---a-note",
    "title": "Research Methods II",
    "section": "Beyond Microdata - a note",
    "text": "Beyond Microdata - a note\n\nThe principles of OB decomposition can also be applied to other types of data, including Macro Data.\nConsider: Between 1990 and 2000 poverty rates fell from 20 to 10%.\nWhat factors explain this change?\n\nComposition changes (Populations with lower poverty rates grew faster)\nPovery rates within groups (Poverty rates within groups fell)\n\n\n\\[\\begin{aligned}\nP_t - P_s &= \\sum_{j=1}^K w_{jt} P_{jt} - \\sum_{j=1}^K w_{js} P_{js} \\\\\n\\Delta P &= \\sum_{j=1}^K w_{jt} ( P_{jt} -  P_{js}) + \\sum_{j=1}^K (w_{jt}-w_{js}) P_{js}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_4.html#ob-decomposition-extensions-1",
    "href": "rmethods2/session_4.html#ob-decomposition-extensions-1",
    "title": "Research Methods II",
    "section": "OB Decomposition: Extensions",
    "text": "OB Decomposition: Extensions\n\nOB Decompositions have two drawbacks\n\nIts meant to analyze differences in average differences\nIt uses differences in mean characteristics\nIts based on a linear model (OLS)\nStrong assumptions on error terms\n\nThere are various extensions (discussed in Firpo, Fortin, Lemieux (2010)) that can be used to address some of this limitations."
  },
  {
    "objectID": "rmethods2/session_4.html#linearity-assumption",
    "href": "rmethods2/session_4.html#linearity-assumption",
    "title": "Research Methods II",
    "section": "Linearity Assumption",
    "text": "Linearity Assumption\n\nBarsky at al (2002) and Dinardo Fortin and Limeux (1996)\nA strong assumption behind OB is that the model is linear in parameters.\n\nThis is important because the decomposition assumes we can make good “extrapolations” of the model.\nThis is a problem of model misspecification. (what to do?)\n\nOne option is to improve model specification\n\nConsider using quadratic terms, interactions, with CENTERED variables.\n\nHowever if Detailed decomposition is not of interest, one could use Re-weighting to get Counterfactuals"
  },
  {
    "objectID": "rmethods2/session_4.html#section-12",
    "href": "rmethods2/session_4.html#section-12",
    "title": "Research Methods II",
    "section": "",
    "text": "Re-weighting\n\nConsidered a more general model: \\(Y_k = G_k(X) + \\epsilon_k\\) and \\(E(Y|X,k) = G_k(X)\\)\nThe overall mean, in this case could be written as:\n\n\\[\\begin{aligned}\nE_k(Y)&=\\int y f_k(y) dy = \\iint (g_k(x) + \\epsilon) f_k(x,\\epsilon) dx d\\epsilon \\\\\n      &= \\int g_k(x) f_k(x) dx  \\\\\n      &= \\bar Y_{g=k, X=k}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_4.html#section-13",
    "href": "rmethods2/session_4.html#section-13",
    "title": "Research Methods II",
    "section": "",
    "text": "Now lets define a counterfactual: \\(\\bar Y^c_{x=A, g=B} = \\int g_B(x) f_A(x) dx\\)\n\nthis is defined as the average outcome of group A if they face the market of group B.\n\nThen, the nonlinear decomposition can be written as:\n\n\\[\\begin{aligned}\n\\Delta \\bar Y &= \\bar Y_{G=A, X=A} - \\bar Y_{G=B, X=B} \\\\\n&= \\bar Y_{G=A, X=A} - \\bar Y_{G=B, X=B} + \\bar Y^c_{G=A, X=B} - \\bar Y^c_{G=A, X=B} \\\\\n&= \\bar Y_{G=A, X=A} - \\bar Y^c_{G=A, X=B} + \\bar Y^c_{G=A, X=B} - \\bar Y_{G=B, X=B} \\\\\n&=                   \\Delta X + \\Delta G\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_4.html#section-14",
    "href": "rmethods2/session_4.html#section-14",
    "title": "Research Methods II",
    "section": "",
    "text": "But how are the counterfactuals weights identified?\nThen the counterfactual can be written as:\n\\[\n\\begin{aligned}\n\\bar Y^c_{G=A, X=B} &= \\int g_A(x) f_B(x) dx  = \\int g_A(x) \\frac{f_B(x)}{f_A(x)} f_A(x) dx \\\\\n&= \\int g_A(x) \\frac{1-P_A(X)}{P_A(X)} f_A(x) dx \\\\\n&= \\int g_A(x) \\widehat{IPW}(X) f_A(x) dx \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_4.html#section-15",
    "href": "rmethods2/session_4.html#section-15",
    "title": "Research Methods II",
    "section": "",
    "text": "Example\n\n\nCode\nfrause oaxaca, clear\ndrop if lnwage==.\nqui:logit female c.(educ exper age married)##c.(educ exper age married), nolog\npredict pr_b, pr\n\ngen ipw1 = (1-pr_b)/pr_b if female==1\ngen ipw2 = pr_b/(1-pr_b) if female==0\nqui:{\nmean lnwage educ exper age married if female==0\nest sto m1\nmean lnwage educ exper age married if female==0 [pw=ipw2] \nest sto m2a\nmean lnwage educ exper age married if female==1 [pw=ipw1]\nest sto m2b\nmean lnwage educ exper age married if female==1\nest sto m3\n \n}\nesttab m1 m2a m2b m3, nogaps se ///\nmtitle(Men Men_as_w Wmen_as_m Women)\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n(751 missing values generated)\n(683 missing values generated)\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                      Men        Men_as_w       Wmen_as_m           Women   \n----------------------------------------------------------------------------\nlnwage              3.440***        3.475***        3.277***        3.267***\n                 (0.0175)        (0.0247)        (0.0280)        (0.0218)   \neduc                11.81***        11.35***        11.82***        11.23***\n                 (0.0895)         (0.112)         (0.154)        (0.0900)   \nexper               14.08***        12.26***        15.23***        12.14***\n                  (0.408)         (0.432)         (1.621)         (0.319)   \nage                 38.43***        39.46***        39.47***        39.29***\n                  (0.413)         (0.725)         (1.160)         (0.411)   \nmarried             0.522***        0.393***        0.523***        0.413***\n                 (0.0182)        (0.0252)        (0.0408)        (0.0189)   \n----------------------------------------------------------------------------\nN                     751             751             683             683   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n\nDecomposition:\n(1,2,4): - \\(DX\\) = (1) vs (2)= 3.440-3.475=-0.035 - \\(DB\\) = (2) vs (4)= 3.475-3.267 =0.208 (1,3,4) - \\(DX\\) = (3) vs (4) = 3.277-3.267 = 0.010 - \\(DB\\) = (1) vs (3) = 3.440-3.277 = 0.163\n\n\nCode\nreplace ipw1=1 if ipw1==.\nreplace ipw2=1 if ipw2==.\nreg lnwage female [pw=ipw1] \nreg lnwage female [pw=ipw2] \n\n\n(751 real changes made)\n(683 real changes made)\n(sum of wgt is 1,553.12667637155)\n\nLinear regression                               Number of obs     =      1,434\n                                                F(1, 1432)        =      24.62\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.0249\n                                                Root MSE          =     .51221\n\n------------------------------------------------------------------------------\n             |               Robust\n      lnwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -.1635866   .0329659    -4.96   0.000    -.2282533   -.0989199\n       _cons |   3.440222   .0174631   197.00   0.000     3.405966    3.474478\n------------------------------------------------------------------------------\n(sum of wgt is 1,349.95921823604)\n\nLinear regression                               Number of obs     =      1,434\n                                                F(1, 1432)        =      39.96\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.0380\n                                                Root MSE          =     .52454\n\n------------------------------------------------------------------------------\n             |               Robust\n      lnwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |   -.208271   .0329455    -6.32   0.000    -.2728976   -.1436444\n       _cons |   3.475032   .0246922   140.73   0.000     3.426596    3.523469\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods2/session_4.html#section-16",
    "href": "rmethods2/session_4.html#section-16",
    "title": "Research Methods II",
    "section": "",
    "text": "Re-weighting and other functions\n\nThe first advantage of re-weighting is that it allows for non-linear relationships between X and y. (via IPW)\nIt also allows you to move away from focusing on differences in means. Any transformation of the outcome is now valid!\n\n\n\nCode\ngen wage = exp(lnwage)\nrifhdreg wage female [pw=ipw1] , rif(gini) over(female)\nrifhdreg wage female [pw=ipw2] , rif(gini) over(female)\n\n\n\nLinear regression                               Number of obs     =      1,434\n                                                F(1, 1432)        =       2.12\n                                                Prob &gt; F          =     0.1457\n                                                R-squared         =     0.0027\n                                                Root MSE          =     .24103\n\n------------------------------------------------------------------------------\n             |               Robust\n        wage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |   .0252941   .0173772     1.46   0.146    -.0087934    .0593816\n       _cons |   .2207253   .0067667    32.62   0.000     .2074516     .233999\n------------------------------------------------------------------------------\nDistributional Statistic: gini\nSample Mean    RIF gini :  .23379\n\nLinear regression                               Number of obs     =      1,434\n                                                F(1, 1432)        =      11.79\n                                                Prob &gt; F          =     0.0006\n                                                R-squared         =     0.0096\n                                                Root MSE          =     .24693\n\n------------------------------------------------------------------------------\n             |               Robust\n        wage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |   .0485792   .0141476     3.43   0.001     .0208269    .0763315\n       _cons |   .2187795   .0078984    27.70   0.000     .2032859    .2342732\n------------------------------------------------------------------------------\nDistributional Statistic: gini\nSample Mean    RIF gini :  .24336\n\n\n\nDrawback: requires correction of SE, and accounting for RW error and Specification error\nDoesn’t easily allow for detailed Decompositions."
  },
  {
    "objectID": "rmethods2/session_4.html#going-beyond-the-mean",
    "href": "rmethods2/session_4.html#going-beyond-the-mean",
    "title": "Research Methods II",
    "section": "Going Beyond the mean",
    "text": "Going Beyond the mean\n\nOne additional and useful extension of the OB Decomposition is to use it for analysis of statistics other than the mean.\nThis can be done by using the recentered influence function (RIF) (Firpo, Fortin, Lemieux (2009,2018))\nThe idea: Use RIFs of the statistic of interest to decompose instead as dependent variable.\nOB can then be applied as usual\nConsider Double Decomposition to avoid Reweighting errors"
  },
  {
    "objectID": "rmethods2/session_4.html#example-2",
    "href": "rmethods2/session_4.html#example-2",
    "title": "Research Methods II",
    "section": "Example",
    "text": "Example\n\nfrause oaxaca, clear\ndrop if lnwage==.\ngen wage = exp(lnwage)\nssc install rif\nqui:oaxaca_rif wage educ exper age married, by(female) rif(mean)\nest sto m1\nqui:oaxaca_rif wage educ exper age married, by(female) rif(gini)\nest sto m2\nqui:oaxaca_rif wage educ exper age married, by(female) rif(q(25))\nest sto m3\nqui:oaxaca_rif wage educ exper age married, by(female) rif(iqsr(40 90))\nest sto m4\n \nesttab m1 m2 m3 m4, se nogaps ///\nmtitle(Mean Gini 25th iqsr_4010)\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\nchecking rif consistency and verifying not already installed...\nall files already exist and are up to date.\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                     Mean            Gini            25th       iqsr_4010   \n----------------------------------------------------------------------------\noverall                                                                     \ngroup_1             34.34***        0.221***        25.36***        0.713***\n                  (0.518)       (0.00678)         (0.484)        (0.0244)   \ngroup_2             30.25***        0.267***        20.91***        0.942***\n                  (0.682)        (0.0118)         (0.507)        (0.0635)   \ndifference          4.083***      -0.0466***        4.448***       -0.230***\n                  (0.857)        (0.0136)         (0.701)        (0.0681)   \nexplained           0.455         -0.0238**         0.969**        -0.111** \n                  (0.516)       (0.00758)         (0.371)        (0.0403)   \nunexplained         3.628***      -0.0228           3.479***       -0.119   \n                  (0.878)        (0.0153)         (0.733)        (0.0782)   \n----------------------------------------------------------------------------\nexplained                                                                   \neduc                1.220***     -0.00539           0.943***      -0.0238   \n                  (0.312)       (0.00319)         (0.239)        (0.0169)   \nexper              0.0121         -0.0121*          0.313         -0.0560*  \n                  (0.216)       (0.00509)         (0.183)        (0.0260)   \nage                -0.283        -0.00297         -0.0508         -0.0154   \n                  (0.207)       (0.00243)        (0.0659)        (0.0128)   \nmarried            -0.494**      -0.00332          -0.236         -0.0154   \n                  (0.189)       (0.00280)         (0.125)        (0.0150)   \n----------------------------------------------------------------------------\nunexplained                                                                 \neduc               -5.229           0.107          -10.68**         0.528   \n                  (3.910)        (0.0695)         (3.346)         (0.349)   \nexper              -3.666*         0.0476          -2.788           0.317   \n                  (1.774)        (0.0317)         (1.475)         (0.162)   \nage                 14.13***       -0.130           13.76***       -0.745*  \n                  (4.070)        (0.0721)         (3.532)         (0.358)   \nmarried             4.804***     -0.00499           3.654***     -0.00311   \n                  (0.856)        (0.0149)         (0.731)        (0.0747)   \n_cons              -6.410         -0.0420          -0.469          -0.215   \n                  (4.582)        (0.0817)         (3.850)         (0.416)   \n----------------------------------------------------------------------------\nN                                                                           \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods2/session_6.html#missing-data",
    "href": "rmethods2/session_6.html#missing-data",
    "title": "Research Methods II",
    "section": "Missing Data",
    "text": "Missing Data\n\nAs we described before, missing data is a problem for micro data analysis.\n\nReduces sample size, statistical power, and may bias estimates. (depending on the type of missingness)\n\nWe have also discussed that there are few ways to deal with missing data.\n\nComplete case analysis\nReweighting\nImputation: Prediction\nImputation: Hotdecking\n\nThis methods allows you solve for missing data if data is MCAR or MAR.\n\nwith MNAR, dealing with missing data is difficult\n\nNevertheless, you can deal with Missing data, because you have some observed data that can be used to impute it."
  },
  {
    "objectID": "rmethods2/session_6.html#types-of-missing-data",
    "href": "rmethods2/session_6.html#types-of-missing-data",
    "title": "Research Methods II",
    "section": "Types of Missing data",
    "text": "Types of Missing data\n\nT1T2T3"
  },
  {
    "objectID": "rmethods2/session_6.html#missing-data-1",
    "href": "rmethods2/session_6.html#missing-data-1",
    "title": "Research Methods II",
    "section": "MISSING DATA",
    "text": "MISSING DATA\n\nWhat would happen if all data is missing?\nExample:\n\nYou are working with the CPS, but are interested in looking at the relationship between income and time use.\n\nCPS does NOT have time use data.\n\n\nWe are going for the -lion hunt-\n\nYou can’t impute time use data\nYou can’t use complete case analysis\nYou can’t use reweighting\nYou can’t use hotdecking\nwhat do we do?"
  },
  {
    "objectID": "rmethods2/session_6.html#section",
    "href": "rmethods2/session_6.html#section",
    "title": "Research Methods II",
    "section": "",
    "text": "What do we do?\n\nOne option would be using a different data set.\n\nIn the US, the American Time Use Survey (ATUS) could be a good option.\nBut…The data has no income information!\n\nWhat if we could combine the two data sets?\nThis changes the problem from Missing all data, to one of Missing Data by design.\n\nSome segment of the population was asked about income, and some other segment was asked about time use."
  },
  {
    "objectID": "rmethods2/session_6.html#imputation-and-statistical-matching",
    "href": "rmethods2/session_6.html#imputation-and-statistical-matching",
    "title": "Research Methods II",
    "section": "Imputation and Statistical Matching",
    "text": "Imputation and Statistical Matching\n\nIf you consider the idea of combining two data sets, you can treat the problem as one of imputation.\n\nYou have a sample (two) that represents the population of interest.\nWe can reasonably assume the data is MCAR. But the data of interest is not observed at the same time.\nThen, we can use the combine data to impute the missing data, using many of the approaches we have discussed before.\n\nAnd there is also another method that is more commonly used (at Levy) to deal with this problem.\n\nStatistical Matching (aka Data Fusion).\n\nWhat does this imply?:\n\nMatch individuals across datasets (“Donor” and “Recipient”)\nTransfer information based on the matching links"
  },
  {
    "objectID": "rmethods2/session_6.html#official-examples",
    "href": "rmethods2/session_6.html#official-examples",
    "title": "Research Methods II",
    "section": "Official examples:",
    "text": "Official examples:\nThere is a lot of work on this topic. Many statistical agencies use this approach to combine data Survey data with administrative data.\n\nAdministrative data is usually more accurate, but it is not collected for the purpose of research.\nSurvey data is collected for research purposes, but may not have accurate data in some areas (income)\nUnless Survey Data was collected with the purpose of being linked with administrative data, one requires methods similar to statistical matching to combine both data sets."
  },
  {
    "objectID": "rmethods2/session_6.html#in-house-some-examples",
    "href": "rmethods2/session_6.html#in-house-some-examples",
    "title": "Research Methods II",
    "section": "In house Some examples:",
    "text": "In house Some examples:\n\nAt Levy we have used this approach to produce relevant datasets:\n\nLIMEW: Levy Institute Measure of Economic Well-Being Combines Time use, wealtgh and Survey data (in addition to other aggregate data)\nLIMTIP: Levy Institute Measure of Time and Income Poverty Combines ATUS, with income/consumption data"
  },
  {
    "objectID": "rmethods2/session_6.html#what-do-we-need",
    "href": "rmethods2/session_6.html#what-do-we-need",
    "title": "Research Methods II",
    "section": "What do we need?",
    "text": "What do we need?\n\nConsider two data sets: \\(A\\) and \\(B\\).\n\n\\(A\\) has informtion on \\(X\\) and \\(Z\\)\n\\(B\\) has information on \\(Y\\) and \\(Z\\)\nWe want a file that has \\(X\\), \\(Y\\) and \\(Z\\)."
  },
  {
    "objectID": "rmethods2/session_6.html#assumptions",
    "href": "rmethods2/session_6.html#assumptions",
    "title": "Research Methods II",
    "section": "Assumptions",
    "text": "Assumptions\n\n(\\(X,Y,Z\\)) are multivariate random variables with joint distribution \\(f(x,y,z)\\), that represents the population of interest.\nBoth datasets are random samples from the same population of interest.\n\\(\\frac{P_w(D=A|X,Y,Z)}{P_w(D=B|X,Y,Z)} = \\frac{P(D=A)}{P(D=B)} = 1\\)\nConditional Independence assumption:\n\n\\(Y\\) and \\(Z\\) are independent from each other given \\(X\\). \\[f(x,y|z) = f(x|z)f(y|z)\\]\n\nThe goal is to combine the two data sets to produce a file that has data on \\(X\\), \\(Y\\) and \\(Z\\). by identifying \\(f(x,y,z)\\)."
  },
  {
    "objectID": "rmethods2/session_6.html#statistical-matching-limitations",
    "href": "rmethods2/session_6.html#statistical-matching-limitations",
    "title": "Research Methods II",
    "section": "Statistical Matching: Limitations",
    "text": "Statistical Matching: Limitations\n\nThe quality of this identification will depend on how well the conditional independence assumption holds.\nBecause of this, synthetic datasets can’t tell you much about covariances or causal relationships\n\n\\[Cov(z,y,z) = \\begin{pmatrix}\nV(X) & \\color{red}{V(X,Y)} & V(X,Z) \\\\\n\\color{red}{V(X,Y)'} & V(Y) & V(Y,Z) \\\\\nV(X,Z)' & V(Y,Z)' & V(Z)\n\\end{pmatrix}\n\\]\nalbeit, you can impose certain bounderies on the covariance matrix."
  },
  {
    "objectID": "rmethods2/session_6.html#matching-approaches",
    "href": "rmethods2/session_6.html#matching-approaches",
    "title": "Research Methods II",
    "section": "Matching Approaches:",
    "text": "Matching Approaches:\nThere are two types of statistical matching procedures:\n\nUnconstrained MatchingConstraints Matching\n\n\n\nRecords from \\(A\\) and \\(B\\) can be used multiple times (or none) in the matching.\n\nAbsurd case: One observation from \\(A\\) is matched with all observations from \\(B\\).\n\nThis is the most common approach in the literature for policy evaluation\nPros: Uses the “best” candidate for the matching. Cons: It may not transfer the uncoditional distribution of the data.\nDoes not necessarily required \\(A\\) and \\(B\\) to be from the same population. (weighted size)\n\n\n\n\nAll records from \\(A\\) and \\(B\\) are used once and only once in the matching. (without replacement)\nWhen using weighted samples, records are matched until the weights are exhausted.\n\nRequires \\(A\\) and \\(B\\) to be from the same population. (weighted size)\n\nPros: It transfers the unconditional distribution of the data. Cons: My not use “best” candidate for the matching."
  },
  {
    "objectID": "rmethods2/session_6.html#matching-records",
    "href": "rmethods2/session_6.html#matching-records",
    "title": "Research Methods II",
    "section": "Matching Records:",
    "text": "Matching Records:\n\nMatching records, requires defining a measure of similarity between records.\nThis measures can vary depending on the data type, and dimensionality of the data\n\n\\[\\begin{aligned}\n\\text{ Euclidian: } d(r^A,r^b) &= \\sqrt{\\sum_i^k(x^A_i-x^B_i)^2 } \\\\\n\\text{ SdEuclidian: } d(r^A,r^b) &= \\sqrt{\\sum_i^k\\left(\\frac{x^A_i-x^B_i}{\\sigma_j}\\right)^2 } \\\\\n\\text{ Mahalanobis: } d(r^A,r^b) &= \\sqrt{(x^A-x^B)'\\Sigma_x^{-1}(x^A-x^B)} \\\\\n\\end{aligned}\n\\]\n\nAll this measures are useful when one has high dimensional data."
  },
  {
    "objectID": "rmethods2/session_6.html#matching-reducing-dimensionality",
    "href": "rmethods2/session_6.html#matching-reducing-dimensionality",
    "title": "Research Methods II",
    "section": "Matching: Reducing Dimensionality",
    "text": "Matching: Reducing Dimensionality\n\nA second alternative is to reduce data dimensionality before estimating distances.\n\n\nPredictive mean matching:Propensity score matching:PCA\n\n\n\nModel \\(x = z\\beta + \\epsilon\\) using \\(A\\).\nMake predictions \\(z\\hat\\beta\\) for both samples.\nMatch records based on \\(z\\hat\\beta\\)\nGood results to match individuals with similar “predicted” income.\nPuts more “weight” on the variables used to predict the outcome.\n\n\n\n\nModel the likehood of an observation being in \\(A\\) using \\(Z\\). \\[P(D=A|Z) = G(Z\\gamma)\\]\nMake predictions \\(\\hat P\\) or \\(z\\hat\\gamma\\) for both samples.\nMatch records based on \\(\\hat\\pi\\)\nGeneral purpose score.\nMay be problematic if \\(A\\) and \\(B\\) have very similar distributions of \\(Z\\).\n\nPuts more “weight” on the variables with different distributions between \\(A\\) and \\(B\\).\n\n\n\n\nUse PCA to reduce dimensionality of \\(Z\\) into a single index.\n\nCan use either a single dataset or both\n\nMake predictions of the first principal component \\(PC1\\)\nMatch records based on \\(PC1\\)\nPuts more weight on variables that explain most of the variance in \\(Z\\)."
  },
  {
    "objectID": "rmethods2/session_6.html#matching-rank-matching",
    "href": "rmethods2/session_6.html#matching-rank-matching",
    "title": "Research Methods II",
    "section": "Matching: Rank Matching",
    "text": "Matching: Rank Matching\n\nMost of distance based matching is usually feasible with unconstrained matching.\n\nthus, best records are always matched.\n\nWhen considering constrained matching, distance based matching may not be adecuate\n\nWhile first records are matched the best, last records may be matched poorly match.\n\nA balance therefore is to use rank matching.\n\nRank observations based on a single variable (pscore, predicted mean, etc)\nMatch records based on rank.\n\nNo match would be “best”, but reduces changes of poor matches."
  },
  {
    "objectID": "rmethods2/session_6.html#data-harmonization",
    "href": "rmethods2/session_6.html#data-harmonization",
    "title": "Research Methods II",
    "section": "1. Data Harmonization",
    "text": "1. Data Harmonization\n\nBecause Data files come from different data sources, they may have different variables names, coding schemes, or definitions.\nWe need to set \\(Z\\) variables to be defined as identically as possible in both files\nBeyond definition harmonization, one must also be mindful of the distribution of the variables in both files.\n\nIf the distribution of \\(Z\\) is different in both files, the matching may not be adequate.\n\nThe weights schemes in both files should be adjusted to add up to the same population size (typically the “recipient” values)\n\nWeight adjustment could be done by selected strata"
  },
  {
    "objectID": "rmethods2/session_6.html#estimation-of-matching-score",
    "href": "rmethods2/session_6.html#estimation-of-matching-score",
    "title": "Research Methods II",
    "section": "2. Estimation of Matching Score",
    "text": "2. Estimation of Matching Score\n\nEither using full or sub (strata) samples, estimate a matching score\n\nThis could be a propensity score, predicted mean, or first principal component.\n\nYou may want to create “further cells” to improve matching. (not necessarily re estimate the matching score)\n\nFor example, You consider Gender as strata (two scores), but further create cells by “age” (5 groups)"
  },
  {
    "objectID": "rmethods2/session_6.html#perform-the-match",
    "href": "rmethods2/session_6.html#perform-the-match",
    "title": "Research Methods II",
    "section": "3. Perform the match",
    "text": "3. Perform the match\n\nUsing the finest definition of “cells”, rank observations based on Matching Scores\nUsing rank, match observations till all weights are exhausted.(from either Sample)\n“unmatched” observations are left for later rounds using coarser definitions of cells.\nMatching continues until all units (recipients) are matched."
  },
  {
    "objectID": "rmethods2/session_6.html#assessing-the-quality-of-the-match",
    "href": "rmethods2/session_6.html#assessing-the-quality-of-the-match",
    "title": "Research Methods II",
    "section": "4. Assessing the quality of the match",
    "text": "4. Assessing the quality of the match\n\nThe idea is to compare the distribution of the “transfered/imputed” data with the distribution from the “donor” data.\n\nOverall distribution of the data will be the same by construction.\n\nCompare distributions by Strata, smaller cells, or specific variables or interest.\nRule of thumb +/- 10% is acceptable (mean, median, Standard error).\n\nBut it may depend on the variable of interest.\n\nOne may also use other approaches like “regression” to compare all variables at once.\nIf the distribution of the data is not adequate, one may want re-do the matching, with different “cell” definitions or matching scores."
  },
  {
    "objectID": "rmethods2/session_6.html#example",
    "href": "rmethods2/session_6.html#example",
    "title": "Research Methods II",
    "section": "Example",
    "text": "Example\n\nfrause wage2, clear\nset seed 312\nxtile smp = runiform()\nreplace smp=smp==1\ngen wage_s = wage if smp==1\n**three Matching scores\n** Pmm\nreg wage_s hours iq kww educ exper tenure age married black south urban sibs \npredict wageh\n** pscore\nlogit smp hours iq kww educ exper tenure age married black south urban sibs \npredict pscore, xb\n** pca\npca hours iq kww educ exper tenure age married black south urban sibs , comp(1)\npredict pc1\n\nforeach i in wageh pscore pc1 {\n    qui:sum `i'\n    replace `i' = (`i'-r(mean))/r(sd)\n}\n\n\n\n\n(467 real changes made)\n(467 missing values generated)\n\n      Source |       SS           df       MS      Number of obs   =       468\n-------------+----------------------------------   F(12, 455)      =     14.83\n       Model |  24043666.1        12  2003638.84   Prob &gt; F        =    0.0000\n    Residual |  61493527.5       455   135150.61   R-squared       =    0.2811\n-------------+----------------------------------   Adj R-squared   =    0.2621\n       Total |  85537193.6       467  183163.155   Root MSE        =    367.63\n\n------------------------------------------------------------------------------\n      wage_s | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hours |  -4.080234   2.204689    -1.85   0.065    -8.412869    .2524015\n          iq |   3.090831   1.459171     2.12   0.035     .2232806    5.958381\n         kww |   5.637987   2.888128     1.95   0.052    -.0377362    11.31371\n        educ |   53.43222   10.41882     5.13   0.000     32.95724     73.9072\n       exper |   8.816439    5.30953     1.66   0.098    -1.617804    19.25068\n      tenure |   6.327233   3.534248     1.79   0.074    -.6182417    13.27271\n         age |   10.92113   7.195943     1.52   0.130    -3.220278    25.06253\n     married |    143.306   54.35023     2.64   0.009     36.49735    250.1146\n       black |  -144.0597   60.51784    -2.38   0.018    -262.9889    -25.1306\n       south |  -37.37316    37.7745    -0.99   0.323    -111.6073    36.86096\n       urban |   200.3258   38.93558     5.15   0.000       123.81    276.8417\n        sibs |   1.881618   8.468635     0.22   0.824    -14.76087    18.52411\n       _cons |  -842.6706     273.28    -3.08   0.002    -1379.718   -305.6231\n------------------------------------------------------------------------------\n(option xb assumed; fitted values)\n\nIteration 0:  Log likelihood = -648.09208  \nIteration 1:  Log likelihood = -642.21625  \nIteration 2:  Log likelihood = -642.21522  \nIteration 3:  Log likelihood = -642.21522  \n\nLogistic regression                                     Number of obs =    935\n                                                        LR chi2(12)   =  11.75\n                                                        Prob &gt; chi2   = 0.4657\nLog likelihood = -642.21522                             Pseudo R2     = 0.0091\n\n------------------------------------------------------------------------------\n         smp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hours |   .0153544   .0093201     1.65   0.099    -.0029127    .0336215\n          iq |   .0024677   .0057346     0.43   0.667    -.0087719    .0137072\n         kww |  -.0095774   .0113247    -0.85   0.398    -.0317735    .0126186\n        educ |  -.0255069   .0409702    -0.62   0.534     -.105807    .0547932\n       exper |   .0256009   .0205087     1.25   0.212    -.0145955    .0657973\n      tenure |   .0131078   .0137404     0.95   0.340     -.013823    .0400385\n         age |  -.0033883   .0281317    -0.12   0.904    -.0585256    .0517489\n     married |  -.2382151   .2167632    -1.10   0.272    -.6630632    .1866331\n       black |  -.1214461   .2276112    -0.53   0.594    -.5675559    .3246637\n       south |  -.0370746   .1457381    -0.25   0.799    -.3227161    .2485669\n       urban |  -.0371736   .1495076    -0.25   0.804    -.3302031     .255856\n        sibs |   -.041735   .0313008    -1.33   0.182    -.1030835    .0196135\n       _cons |  -.1246922   1.056515    -0.12   0.906    -2.195424    1.946039\n------------------------------------------------------------------------------\n\nPrincipal components/correlation                 Number of obs    =        935\n                                                 Number of comp.  =          1\n                                                 Trace            =         12\n    Rotation: (unrotated = principal)            Rho              =     0.2112\n\n    --------------------------------------------------------------------------\n       Component |   Eigenvalue   Difference         Proportion   Cumulative\n    -------------+------------------------------------------------------------\n           Comp1 |       2.5348      .622213             0.2112       0.2112\n           Comp2 |      1.91258      .821455             0.1594       0.3706\n           Comp3 |      1.09113     .0297471             0.0909       0.4615\n           Comp4 |      1.06138     .0497587             0.0884       0.5500\n           Comp5 |      1.01162     .0867533             0.0843       0.6343\n           Comp6 |      .924871     .0639678             0.0771       0.7114\n           Comp7 |      .860903     .0907871             0.0717       0.7831\n           Comp8 |      .770116      .144885             0.0642       0.8473\n           Comp9 |      .625231      .119791             0.0521       0.8994\n          Comp10 |       .50544     .0919858             0.0421       0.9415\n          Comp11 |      .413454      .124988             0.0345       0.9760\n          Comp12 |      .288466            .             0.0240       1.0000\n    --------------------------------------------------------------------------\n\nPrincipal components (eigenvectors) \n\n    --------------------------------------\n        Variable |    Comp1 | Unexplained \n    -------------+----------+-------------\n           hours |   0.1320 |       .9558 \n              iq |   0.4921 |       .3863 \n             kww |   0.4220 |       .5486 \n            educ |   0.4555 |        .474 \n           exper |  -0.2161 |       .8817 \n          tenure |   0.0463 |       .9946 \n             age |   0.0527 |        .993 \n         married |   0.0053 |       .9999 \n           black |  -0.3722 |       .6488 \n           south |  -0.2076 |       .8908 \n           urban |   0.0723 |       .9868 \n            sibs |  -0.3411 |       .7051 \n    --------------------------------------\n(score assumed)\n\nScoring coefficients \n    sum of squares(column-loading) = 1\n\n    ------------------------\n        Variable |    Comp1 \n    -------------+----------\n           hours |   0.1320 \n              iq |   0.4921 \n             kww |   0.4220 \n            educ |   0.4555 \n           exper |  -0.2161 \n          tenure |   0.0463 \n             age |   0.0527 \n         married |   0.0053 \n           black |  -0.3722 \n           south |  -0.2076 \n           urban |   0.0723 \n            sibs |  -0.3411 \n    ------------------------\n(935 real changes made)\n(935 real changes made)\n(935 real changes made)\n\n\nNext we create ranks for each observation, assuming no stratification.\n\n\nCode\nbysort smp (wageh) :gen rnk1=_n\nbysort smp (pscore):gen rnk2=_n\nbysort smp (pc1)   :gen rnk3=_n\n\n\nFinally, the imputation. Simply “matching” information from the donor to the recipient.\n\n* Imputation\nclonevar wage1 = wage_s\nclonevar wage2 = wage_s\nclonevar wage3 = wage_s\n\ngsort -smp rnk1\nreplace wage1 = wage_s[rnk1] if smp==0\n\ngsort -smp rnk2\nreplace wage2 = wage_s[rnk2] if smp==0\n\ngsort -smp rnk3\nreplace wage3 = wage_s[rnk3] if smp==0\n\n(467 missing values generated)\n(467 missing values generated)\n(467 missing values generated)\n(467 real changes made)\n(467 real changes made)\n(467 real changes made)\n\n\nSimple quality assessment.\n\nqui:reg wage hours iq kww educ exper tenure age married black south if smp==0\nest sto m1\nqui:reg wage1 hours iq kww educ exper tenure age married black south if smp==0\nest sto m2\nqui:reg wage2 hours iq kww educ exper tenure age married black south if smp==0\nest sto m3\nqui:reg wage3 hours iq kww educ exper tenure age married black south if smp==0\nest sto m4\nesttab m1 m2 m3 m4 , se mtitle(True Wageh pscore pca)\n\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                     True           Wageh          pscore             pca   \n----------------------------------------------------------------------------\nhours              -3.076          -5.508*          0.208          -0.971   \n                  (2.482)         (2.678)         (3.109)         (2.825)   \n\niq                  2.579           0.343          -2.775           3.653*  \n                  (1.411)         (1.522)         (1.767)         (1.605)   \n\nkww                 5.410*          8.750**         1.954           7.545*  \n                  (2.745)         (2.961)         (3.438)         (3.124)   \n\neduc                46.14***        77.82***        8.331           44.53***\n                  (10.18)         (10.98)         (12.75)         (11.58)   \n\nexper               11.66*          15.17**        -2.006           5.108   \n                  (4.980)         (5.372)         (6.237)         (5.667)   \n\ntenure              3.844           5.238          -0.511          0.0757   \n                  (3.332)         (3.595)         (4.174)         (3.792)   \n\nage                -0.945          -9.667          -10.91          -5.380   \n                  (6.905)         (7.449)         (8.648)         (7.858)   \n\nmarried             187.2***        165.6**         89.99           38.58   \n                  (54.26)         (58.54)         (67.96)         (61.75)   \n\nblack              -64.09          -99.07           46.48          -39.87   \n                  (52.43)         (56.57)         (65.67)         (59.67)   \n\nsouth              -80.79*         -74.14           9.854          -120.9** \n                  (35.19)         (37.96)         (44.07)         (40.04)   \n\n_cons              -254.2          -199.6          1349.4***       -104.9   \n                  (251.2)         (271.0)         (314.6)         (285.9)   \n----------------------------------------------------------------------------\nN                     467             467             467             467   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rm_class.html",
    "href": "rm_class.html",
    "title": "Research Methods: Econometrics I",
    "section": "",
    "text": "Class syllabus is here."
  },
  {
    "objectID": "rm_class.html#syllabus",
    "href": "rm_class.html#syllabus",
    "title": "Research Methods: Econometrics I",
    "section": "",
    "text": "Class syllabus is here."
  },
  {
    "objectID": "rm_class.html#zoom-link",
    "href": "rm_class.html#zoom-link",
    "title": "Research Methods: Econometrics I",
    "section": "Zoom Link",
    "text": "Zoom Link\nWhen there is need to attend a class online, and you have requested to attend online, use the following link:\nZoom class"
  },
  {
    "objectID": "rm_class.html#introduction",
    "href": "rm_class.html#introduction",
    "title": "Research Methods: Econometrics I",
    "section": "Introduction",
    "text": "Introduction\n\nChapter 1\nOnce Upon a time, why Econometrics?:"
  },
  {
    "objectID": "rm_class.html#part-i-basic-tools",
    "href": "rm_class.html#part-i-basic-tools",
    "title": "Research Methods: Econometrics I",
    "section": "Part I: Basic Tools",
    "text": "Part I: Basic Tools\n\n1. Chapter 2:\nThe Simple Regression Model\n\n\n2. Chapter 3:\nMultiple Regression Analysis: Estimation\n\n\n3. Chapter 4 - 5\nMRA: Inference and Asymptotics\n\n\nHomeWork 1 Github"
  },
  {
    "objectID": "rm_class.html#part-ii-addressing-problems-with-mra",
    "href": "rm_class.html#part-ii-addressing-problems-with-mra",
    "title": "Research Methods: Econometrics I",
    "section": "Part II: Addressing Problems with MRA",
    "text": "Part II: Addressing Problems with MRA\n\n4. Chapter 6 - 7\nMRA: Scaling, functional forms, Goodness of Fit, and Qualitative Information\n\n\n5. Chapter 8\nHeteroskedasticity\n\n\n6. Chapter 9\nFurther Problems: Functional form, Missing variables, measurement errors, missing data\n\n\nHomeWork 2 Github\n\n\nMIDTERM!\n\n7. Chapter 15\nInstrumental Variables and 2SLS\n\n\n8. Chapter 17\nLimited Dep Variables"
  },
  {
    "objectID": "rm_class.html#part-iii-panel-data-methods",
    "href": "rm_class.html#part-iii-panel-data-methods",
    "title": "Research Methods: Econometrics I",
    "section": "Part III: Panel Data Methods",
    "text": "Part III: Panel Data Methods\n\n9. Chapter 13\nPool Cross Section and Panel Data\n\n\n10. Chapter 14\nAdvanced Panel Data Methods\n\n\nHomeWork 3"
  },
  {
    "objectID": "rm_class.html#part-iv-time-series",
    "href": "rm_class.html#part-iv-time-series",
    "title": "Research Methods: Econometrics I",
    "section": "Part IV: Time Series",
    "text": "Part IV: Time Series\n\n11. Chapter 10 - 11\nBasics of Regression analysis with TSD\n\n\n12. Chapter 12 & 18\nAdvanced TSD Problems\n\n\nFinal TBD"
  },
  {
    "objectID": "rm-data/quizes/quiz-1 copy.html",
    "href": "rm-data/quizes/quiz-1 copy.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Quiz 5: Modeling Probabilities\nName: ____________________________ Grade: __/5\n\nWhat is one potential issue with the linear probability model?\n\nIt is always homoskedastic, requiring complex adjustments.\nIt may predict probabilities that are less than zero or greater than one.\nIt cannot handle more than two explanatory variables.\nIt is too computationally intensive for modern computers.\n\nWhat is the primary goal of “classification” in the context of binary y variables?\n\nPredicting the exact probability of y = 1 for target observations.\nVisualizing the relationship between predicted probabilities and actual outcomes using a calibration curve.\nAssigning target observations to either the y = 1 or y = 0 category based on a predicted probability.\nIdentifying potential outliers in the dataset that could skew the results of the analysis.\n\nIn the context of probability models, what are the “logit” and “probit” models designed to address?\n\nThe issue of non-linearity in the relationship between explanatory and dependent variables.\nThe need to account for heteroskedasticity in binary outcome data.\nThe challenge of interpreting the coefficients of linear probability models.\nThe problem of linear probability models potentially predicting probabilities outside the range of 0 to 1.**\n\nAccording to the excerpt, how do the predicted probabilities from logit and probit models typically compare?\n\nThey are usually very similar.\nLogit models tend to predict higher probabilities than probit models.\nProbit models are more accurate when the actual probabilities are close to 0 or 1.\nThe choice between logit and probit significantly impacts the interpretation of the results.\n\nHow do marginal differences relate to the coefficients of linear probability models?\n\nMarginal differences are the inverse of the corresponding LPM coefficients.\nMarginal differences are always smaller in magnitude than LPM coefficients.\nMarginal differences often have a similar interpretation to LPM coefficients.\nThere is no meaningful relationship between the two concepts. of the coefficients and marginal differences."
  }
]