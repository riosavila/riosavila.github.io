[
  {
    "objectID": "Stata_Basics.html",
    "href": "Stata_Basics.html",
    "title": "Stata-Basics",
    "section": "",
    "text": "You can use command window to type and excute commands directly into Stata.\nGreat for interactive exploration and analysis…\nBut highly recommended that Final analysis is “always” done in a “do-file”\n\n\n\n\n. display \"Hola\"\nHola\n\n. display \"2+2=\" 2+2\n2+2=4\n\n. display \"The probability that z &gt;1.95 is \" %5.3f 1-normal(1.95)\nThe probability that z &gt;1.95 is 0.026\n\n. \n\n\n\n\n\n\n99% of Stata commands come with Extensive help.\nIf you do not know how to use a command, or about a test just “ask for help”\n\nhelp help\n[R] help -- Display help in Stata\n-------------------------------------------------------------------------------\nStata's help system\n    There are several kinds of help available to the Stata user. For more\n    information, see Advice on getting help.  The information below is\n    technical details about Stata's help command.\n-------------------------------------------------------------------------------\nSyntax\n        help [command_or_topic_name] [, nonew name(viewername)\n              marker(markername)]\nMenu\n    Help &gt; Stata command...\nDescription\n    The help command displays help information about the specified command or\n    topic.  help launches a new Viewer to display help for the specified\n    command or topic or displays help on the console in Stata for\n    Unix(console).  If help is not followed by a command or a topic name,\n    Stata displays advice for using the help system and documentation.\nFor estimation commands, and specialized tests, help even provides links to the manuals.\nThe manuals have extensive detailed information on methods, formulas, references, and examples.\n\n\nOf course there is 1% that is “documented/undocumented” or truly undocumented.\nMost Community-contributed commands also have helpfiles, but are not always fully documented.\nYou could also ask for helps on “topics”: help sample selection\n\n\n\n\n\nStata, for all practical purposes, is self-contained.\n\nYou do not need outside sources to analyze your data, estimate models, create tables, etc.\n\nHowever, many users provide add-ons that may help to make your work “easier”\n\nMain Stata repository : Boston College Statistical Software Components (SSC) archive\n\n\n** For using Wooldridge Book Datasets\nssc install frause, replace \n** For Easiy tables\nnet install estout, replace from(https://raw.githubusercontent.com/benjann/estout/master/)\n** My own installer for extra utilities\nnet install fra, replace from(https://friosavila.github.io/stpackages) \nfra install fra_tools, replace\n\nIf at any point there is code that produces an error, and there is no help, let me know.\n\n\n\n\n\nStata Files have format dta.\nLoading Stata-data into Stata is very easy.\n\nDouble-click (opens a new Stata)\nDrag and Drop into your Stata instance\nLoad it from menu File&gt;open\nor using a do-file or command window\n\nOther Formats required extra work.\n\nUse other software to “translate” it into Stata\nMenu: File&gt;import&gt; many choices\n\n\n\n\n\n\n\n. ** Most Stata example files\n. ** Syntax:   sysuse  [filename], [clear]\n. sysuse dir\n  abortion.dta    citytemp.dta    nlsw88.dta      titanic.dta\n  auto.dta        citytemp4.dta   nlswide1.dta    tsline1.dta\n  auto16.dta      educ99gdp.dta   oaxaca.dta      tsline2.dta\n  auto2.dta       gapminder.dta   pop2000.dta     uslifeexp.dta\n  autornd.dta     gnp96.dta       ri.dta          uslifeexp2.dta\n  avocado.dta     lifeexp.dta     sandstone.dta   voter.dta\n  bplong.dta      mortgages.dta   scorecard.dta   xtline1.dta\n  bpwide.dta      mroz.dta        snow.dta        yule.dta\n  cancer.dta      network1.dta    sp500.dta\n  castle.dta      network1a.dta   surface.dta\n  census.dta      nhefs.dta       texas.dta\n\n. \n\n\n\n\n\n** Web data from Stata\nwebuse \"data-file-address\", clear\n** From other sites\nwebuse set [webaddress]\nwebuse data-file-address, clear \nwebuse set \n** from frause and Wooldrige\nfrause , dir\nfrause wage1, clear\n** from anyadress\nuse \"filename-adress\", clear\nuse \"https://friosavila.github.io/playingwithstata/data2/wage1.dta\", clear\n\n\n\n\n\nsysusewebusefrause\n\n\n\n\n\n. sysuse auto, clear\n(1978 automobile data)\n\n. des\n\nContains data from C:\\Program Files\\Stata17/ado\\base/a/auto.dta\n Observations:            74                  1978 automobile data\n    Variables:            12                  13 Apr 2020 17:45\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nmake            str18   %-18s                 Make and model\nprice           int     %8.0gc                Price\nmpg             int     %8.0g                 Mileage (mpg)\nrep78           int     %8.0g                 Repair record 1978\nheadroom        float   %6.1f                 Headroom (in.)\ntrunk           int     %8.0g                 Trunk space (cu. ft.)\nweight          int     %8.0gc                Weight (lbs.)\nlength          int     %8.0g                 Length (in.)\nturn            int     %8.0g                 Turn circle (ft.)\ndisplacement    int     %8.0g                 Displacement (cu. in.)\ngear_ratio      float   %6.2f                 Gear ratio\nforeign         byte    %8.0g      origin     Car origin\n-------------------------------------------------------------------------------\nSorted by: foreign\n\n. list in 1/3\n\n     +------------------------------------------------------------------------+\n  1. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Concord | 4,099 |  22 |     3 |      2.5 |    11 |  2,930 |    186 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       40     |          121     |         3.58     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n     +------------------------------------------------------------------------+\n  2. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Pacer   | 4,749 |  17 |     3 |      3.0 |    11 |  3,350 |    173 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       40     |          258     |         2.53     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n     +------------------------------------------------------------------------+\n  3. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Spirit  | 3,799 |  22 |     . |      3.0 |    12 |  2,640 |    168 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       35     |          121     |         3.08     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n. \n\n\n\n\n\n\n\n. webuse smoking, clear\n(Smoking and mortality data)\n\n. des\n\nContains data from https://www.stata-press.com/data/r17/smoking.dta\n Observations:        17,260                  Smoking and mortality data\n    Variables:            16                  27 Dec 2020 15:21\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nserno           int     %10.0g                Serial number\ncigs            byte    %10.0g                Daily cigarette consumption\nsysbp           int     %10.0g                Systolic blood pressure (mm Hg)\ndiasbp          float   %10.0g                Diastolic blood pressure (mm Hg)\nmap             float   %9.0g                 Mean arterial pressure (mm Hg)\nage             byte    %10.0g                Age (years)\nht              double  %10.0g                Height (cm)\nwt              double  %10.0g                Weight (kg)\nchol            double  %10.0g                Cholesterol (mmol/l)\ngradd1          byte    %8.0g                 Job grade 1\ngradd2          byte    %8.0g                 Job grade 2\ngradd3          byte    %8.0g                 Job grade 3\nall10           byte    %8.0g                 Ten year mortality\npyar            double  %10.0g                Years of follow-up\nchd             byte    %10.0g                Censoring (0 = censored, 1 = died\n                                                of CHD)\njobgrade        byte    %10.0g                Job grade\n-------------------------------------------------------------------------------\nSorted by: \n\n. list cigs map age ht gradd1 in 1/3\n\n     +-----------------------------------------+\n     | cigs        map   age       ht   gradd1 |\n     |-----------------------------------------|\n  1. |    0         97    46   154.94        0 |\n  2. |    0   97.66666    55   179.07        1 |\n  3. |    0         82    43   173.99        1 |\n     +-----------------------------------------+\n\n. \n\n\n\n\n\n\n\n. frause smoking, clear\n(A.C.Cameron & P.K.Trivedi (2022): Microeconometrics Using Stata, 2e)\n\n. des\n\nContains data from https://friosavila.github.io/playingwithstata/data2/smoking.\n&gt; dta\n Observations:         1,209                  A.C.Cameron & P.K.Trivedi\n                                                (2022): Microeconometrics Using\n                                                Stata, 2e\n    Variables:             7                  5 Oct 2022 13:08\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nstate           byte    %14.0g     state      State no\nyear            int     %9.0g                 Year\ncigsale         float   %9.0g                 Cigarette sale per capita (in\n                                                packs)\nlnincome        float   %9.0g                 Log state per capita GDP\nbeer            float   %9.0g                 Beer consumption per capita\nage15to24       float   %9.0g                 Percent of state population aged\n                                                15–24 years\nretprice        float   %9.0g                 Retail price of cigarettes\n-------------------------------------------------------------------------------\nSorted by: year\n\n. list in 1/3\n\n     +-----------------------------------------------------------------------+\n     |        state   year   cigsale   lnincome   beer   age15~24   retprice |\n     |-----------------------------------------------------------------------|\n  1. | Rhode Island   1970     123.9          .      .   .1831579       39.3 |\n  2. |    Tennessee   1970      99.8          .      .   .1780438       39.9 |\n  3. |      Indiana   1970     134.6          .      .   .1765159       30.6 |\n     +-----------------------------------------------------------------------+\n\n. \n\n\n\n\n\n\n\n\n\nSummary Statistics are essential before starting basic analysis. Stata gives you many options. Although not all of them are easy to export.\n\n\n\n\n. frause oaxaca, clear\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n. *summarize [varlist] [if] [in] [weight] [, options]\n. summarize if female==1, sep(0)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      lnwage |        683    3.266761    .5700236    .507681   5.259097\n        educ |        888    11.06025     2.26024          5       17.5\n       exper |        683    12.13769     8.32663          0   41.91667\n      tenure |        683    6.605051    6.727475          0   40.08333\n        isco |        683    3.920937    1.762983          1          9\n      female |        888           1           0          1          1\n         lfp |        888    .7691441    .4216179          0          1\n         age |        888    39.88401    10.72665         18         62\n       agesq |        888    1705.666    879.4667        324       3844\n      single |        888    .3018018    .4592984          0          1\n     married |        888    .5236486    .4997219          0          1\n    divorced |        888    .1745495    .3797953          0          1\n       kids6 |        888     .286036    .6726639          0          4\n     kids714 |        888        .375    .7538888          0          4\n          wt |        888    .9883364    .4030975   .5302977   3.181786\n\n. \n\n\n\n\n\n. *   tabstat varlist [if] [in] [weight] [, options]\n. tabstat educ exper tenure age married, by(female)\n\nSummary statistics: Mean\nGroup variable: female (sex of respondent (1=female))\n\n  female |      educ     exper    tenure       age   married\n---------+--------------------------------------------------\n       0 |   11.8004  14.07684  9.003107  38.51647  .5230567\n       1 |  11.06025  12.13769  6.605051  39.88401  .5236486\n---------+--------------------------------------------------\n   Total |  11.40134  13.15324  7.860937  39.25379  .5233758\n------------------------------------------------------------\n\n. tabstat educ exper tenure , by(female) stats(p10 p50 p90)\n\nSummary statistics: p10, p50, p90\nGroup variable: female (sex of respondent (1=female))\n\n  female |      educ     exper    tenure\n---------+------------------------------\n       0 |      10.5  1.416667  .4166667\n         |      10.5  11.08333      6.25\n         |      17.5  31.33333     23.25\n---------+------------------------------\n       1 |         9      2.25  .3333333\n         |      10.5     10.75      4.25\n         |      12.5    23.375     15.75\n---------+------------------------------\n   Total |         9  1.833333  .4166667\n         |      10.5  10.91667  5.291667\n         |        15  28.08333  19.41667\n----------------------------------------\n\n. \n\n\n\n\n\n. ssc install table1\nchecking table1 consistency and verifying not already installed...\nall files already exist and are up to date.\n\n. * see help table1\n. table1, by(female) vars(lnwage contn %3.2f \\ age contn %2.1f \\ married bin)\n  +--------------------------------------------------------------------+\n  | Factor                         female = 0    female = 1    p-value |\n  |--------------------------------------------------------------------|\n  | N                              759           888                   |\n  |--------------------------------------------------------------------|\n  | log hourly wages, mean (SD)    3.44 (0.48)   3.27 (0.57)    &lt;0.001 |\n  |--------------------------------------------------------------------|\n  | age of respondent, mean (SD)   38.5 (11.3)   39.9 (10.7)     0.012 |\n  |--------------------------------------------------------------------|\n  | married                        397 (52.3%)   465 (52.4%)      0.98 |\n  +--------------------------------------------------------------------+\n\n. qui:table1, by(female) vars(lnwage contn %3.2f \\ age contn %2.1f \\ married bi\n&gt; n) saving(m1.xls)\nfile m1.xls already exists\nr(602);\n\n\nYou can see the file here\n\nThere are other options from Stata as well. see help dtable and help table.\nOr you could construct some yourself with the help of estout and esttab.\nSee here for a quick guide on tables.\n\n\n\n\n\nTwo main commands:\n\ngenerate (or gen for short): Creates new variables as a function of others in the data. One can apply system functions. Example:\n\ngen var1 = 1\ngen var2 = _n\ngen wage = exp(lnwage)\ngen age_educ = age * educ\n\nreplace: replaces values in an already existing variable.\n\nreplace wage = 0 if wage==.\nreplace age_educ = . if female==1\n\negen: Advanced variable generating function. It applies a single function to a variable or list of variables to create a third one.\n\negen wage_mean=mean(exp(lnwage)), by(female)\negen wage_p10=pctile(lnwage), by(female) p(10)\n\n\nTo delete a variable, you can use drop varname/varlist or drop2 varname/varlist\n\ndrop is the official. Stops if the variable does not exist.\ndrop2 an addon. Will still work even if a variable name does not exist.\nRequires using full variable name.\n\n\n\n\n\n. gen var1 = exp(lnwage)\n(213 missing values generated)\n\n. gen xar2 = exp(lnwage)+married\n(213 missing values generated)\n\n. drop x\n\n. des var1 xar2\nvariable xar2 not found\nr(111);\n\n\n\n\n\n. gen xar2 = exp(lnwage)+married\n(213 missing values generated)\n\n. drop2 x\nvariable x not found\n\n. \n\n\n\n\n\n\nStata is case sensitive.\n\nYou can create variables with names one, One, OnE, ONE, etc.\n“file addresses” and commands are also case sensitive\n\nIn Stata, variable names cannot can only start with a letter or “_“. Otherwise, it will give you an error.\nOnce variables are created, you could “label” them\nlabel var variable_name \"Description\"\nYou can name other components of a dataset as well. See help label\n\n\n\n\n\nStata can create figures and plots for data exploration\n\n\n\n\n\n\n. webuse dui, clear\n(Fictional data on monthly drunk driving citations)\n\n. scatter citations fines\n\n. \n\n\n\n\n\n\n\n\n. two (scatter citations fines if csize==1)  ///\n&gt;     (scatter citations fines if csize==2), ///\n&gt;     legend(order(1 \"Small\" 2 \"Medium\"))\n\n. \n\n\n\n\n\n\nThe limitation. User written plotting commands do not interact well with Official plotting commands.\n\n\n\n\n\n\nCommand Window is effective to provide interactive analysis\nAt the end of your session, you can recover everything you did, clicking on the History Section, and save everythig, or just specific commands.\nThe best approach, however, is to ALWAYS use a do-file.\nFirst of all: Create a working directory. A folder in your computer that will hold your project, work, paper, homework, etc. (highly recommended)\nCreate a dofile: For simple projects a single file will suffice, but multiple may be needed for larger ones.\nTo start a dofile, simply type doedit \"filename\" in your command window.\n\nIf file exists in your “working directory” (type cd to see where you “are”), it will open it.\nOtherwise, a new file will be created\n\ndo-files are the best approach to save your work, and keep track of your analysis.\nGeneral Suggestion: Allways add comments to it, to know what you are doing\n\n\n*  You can always Start a command like this\n// Or like this\n/*\nBut you can always add a large comment using \"/*\" to start\nand \"*/\" to end it\n*/\n\n/* You could also add comments at the end of a command */\nsysuse auto, clear  // Loading Auto Dataset, after \"clearing\" the one currently in memory\n\n// Or as I did before, break a long command in various lines using \"///\"\n// Comments after \"///\" are possible\nregress price     /// Dep variable\n        mpg       /// indep variable \n        i.foreign, /// Foreign Dummy\n        robust     // Request Robust Standard errors.\n        \n** Last one has only two \"/\", because line ends there\n\n(1978 automobile data)\n\nLinear regression                               Number of obs     =         74\n                                                F(2, 71)          =      12.72\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.2838\n                                                Root MSE          =     2530.9\n\n------------------------------------------------------------------------------\n             |               Robust\n       price | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -294.1955   60.33645    -4.88   0.000     -414.503   -173.8881\n             |\n     foreign |\n    Foreign  |   1767.292   607.7385     2.91   0.005     555.4961    2979.088\n       _cons |   11905.42   1362.547     8.74   0.000     9188.573    14622.26\n------------------------------------------------------------------------------\n\n\n\n\n\n\nMost commands in Stata have the following syntax:\n\n[by varlist:/prefix] command [varlist]  [if exp] [in range] [weight] [using filename] [,options]\n\neverything in [] are optional.\n[by varlist:/prefix]: by is used to execute the command by groups.\nprefix to request additional manipulation (advanced use)\ncommand: The command itself that will process the data\nvarlist: For Estimation commands include the dependent (first) and independent variables (everything else)\n[if exp] [in range]: To restrict samples\n[weight]: Request the use of weights ie: [fw = wgt_var] or [pw = wgt_var]\n[using filename]: Some commands allow you to use this to work with not-yet loaded datasets or files.\n[options]: Options requesting specific behaivior, statistics, etc\n\n\n\n\n\nUse github, as an additional data-repository\nCombine Stata, python and nbstat to create Jupyter notebooks.\nYou can also use Quarto to create full dynamic reports."
  },
  {
    "objectID": "Stata_Basics.html#command-window",
    "href": "Stata_Basics.html#command-window",
    "title": "Stata-Basics",
    "section": "",
    "text": "You can use command window to type and excute commands directly into Stata.\nGreat for interactive exploration and analysis…\nBut highly recommended that Final analysis is “always” done in a “do-file”\n\n\n\n\n. display \"Hola\"\nHola\n\n. display \"2+2=\" 2+2\n2+2=4\n\n. display \"The probability that z &gt;1.95 is \" %5.3f 1-normal(1.95)\nThe probability that z &gt;1.95 is 0.026\n\n."
  },
  {
    "objectID": "Stata_Basics.html#help",
    "href": "Stata_Basics.html#help",
    "title": "Stata-Basics",
    "section": "",
    "text": "99% of Stata commands come with Extensive help.\nIf you do not know how to use a command, or about a test just “ask for help”\n\nhelp help\n[R] help -- Display help in Stata\n-------------------------------------------------------------------------------\nStata's help system\n    There are several kinds of help available to the Stata user. For more\n    information, see Advice on getting help.  The information below is\n    technical details about Stata's help command.\n-------------------------------------------------------------------------------\nSyntax\n        help [command_or_topic_name] [, nonew name(viewername)\n              marker(markername)]\nMenu\n    Help &gt; Stata command...\nDescription\n    The help command displays help information about the specified command or\n    topic.  help launches a new Viewer to display help for the specified\n    command or topic or displays help on the console in Stata for\n    Unix(console).  If help is not followed by a command or a topic name,\n    Stata displays advice for using the help system and documentation.\nFor estimation commands, and specialized tests, help even provides links to the manuals.\nThe manuals have extensive detailed information on methods, formulas, references, and examples.\n\n\nOf course there is 1% that is “documented/undocumented” or truly undocumented.\nMost Community-contributed commands also have helpfiles, but are not always fully documented.\nYou could also ask for helps on “topics”: help sample selection"
  },
  {
    "objectID": "Stata_Basics.html#installing-programs",
    "href": "Stata_Basics.html#installing-programs",
    "title": "Stata-Basics",
    "section": "",
    "text": "Stata, for all practical purposes, is self-contained.\n\nYou do not need outside sources to analyze your data, estimate models, create tables, etc.\n\nHowever, many users provide add-ons that may help to make your work “easier”\n\nMain Stata repository : Boston College Statistical Software Components (SSC) archive\n\n\n** For using Wooldridge Book Datasets\nssc install frause, replace \n** For Easiy tables\nnet install estout, replace from(https://raw.githubusercontent.com/benjann/estout/master/)\n** My own installer for extra utilities\nnet install fra, replace from(https://friosavila.github.io/stpackages) \nfra install fra_tools, replace\n\nIf at any point there is code that produces an error, and there is no help, let me know."
  },
  {
    "objectID": "Stata_Basics.html#loading-data",
    "href": "Stata_Basics.html#loading-data",
    "title": "Stata-Basics",
    "section": "",
    "text": "Stata Files have format dta.\nLoading Stata-data into Stata is very easy.\n\nDouble-click (opens a new Stata)\nDrag and Drop into your Stata instance\nLoad it from menu File&gt;open\nor using a do-file or command window\n\nOther Formats required extra work.\n\nUse other software to “translate” it into Stata\nMenu: File&gt;import&gt; many choices\n\n\n\n\n\n\n\n. ** Most Stata example files\n. ** Syntax:   sysuse  [filename], [clear]\n. sysuse dir\n  abortion.dta    citytemp.dta    nlsw88.dta      titanic.dta\n  auto.dta        citytemp4.dta   nlswide1.dta    tsline1.dta\n  auto16.dta      educ99gdp.dta   oaxaca.dta      tsline2.dta\n  auto2.dta       gapminder.dta   pop2000.dta     uslifeexp.dta\n  autornd.dta     gnp96.dta       ri.dta          uslifeexp2.dta\n  avocado.dta     lifeexp.dta     sandstone.dta   voter.dta\n  bplong.dta      mortgages.dta   scorecard.dta   xtline1.dta\n  bpwide.dta      mroz.dta        snow.dta        yule.dta\n  cancer.dta      network1.dta    sp500.dta\n  castle.dta      network1a.dta   surface.dta\n  census.dta      nhefs.dta       texas.dta\n\n. \n\n\n\n\n\n** Web data from Stata\nwebuse \"data-file-address\", clear\n** From other sites\nwebuse set [webaddress]\nwebuse data-file-address, clear \nwebuse set \n** from frause and Wooldrige\nfrause , dir\nfrause wage1, clear\n** from anyadress\nuse \"filename-adress\", clear\nuse \"https://friosavila.github.io/playingwithstata/data2/wage1.dta\", clear"
  },
  {
    "objectID": "Stata_Basics.html#basic-data-description",
    "href": "Stata_Basics.html#basic-data-description",
    "title": "Stata-Basics",
    "section": "",
    "text": "sysusewebusefrause\n\n\n\n\n\n. sysuse auto, clear\n(1978 automobile data)\n\n. des\n\nContains data from C:\\Program Files\\Stata17/ado\\base/a/auto.dta\n Observations:            74                  1978 automobile data\n    Variables:            12                  13 Apr 2020 17:45\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nmake            str18   %-18s                 Make and model\nprice           int     %8.0gc                Price\nmpg             int     %8.0g                 Mileage (mpg)\nrep78           int     %8.0g                 Repair record 1978\nheadroom        float   %6.1f                 Headroom (in.)\ntrunk           int     %8.0g                 Trunk space (cu. ft.)\nweight          int     %8.0gc                Weight (lbs.)\nlength          int     %8.0g                 Length (in.)\nturn            int     %8.0g                 Turn circle (ft.)\ndisplacement    int     %8.0g                 Displacement (cu. in.)\ngear_ratio      float   %6.2f                 Gear ratio\nforeign         byte    %8.0g      origin     Car origin\n-------------------------------------------------------------------------------\nSorted by: foreign\n\n. list in 1/3\n\n     +------------------------------------------------------------------------+\n  1. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Concord | 4,099 |  22 |     3 |      2.5 |    11 |  2,930 |    186 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       40     |          121     |         3.58     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n     +------------------------------------------------------------------------+\n  2. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Pacer   | 4,749 |  17 |     3 |      3.0 |    11 |  3,350 |    173 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       40     |          258     |         2.53     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n     +------------------------------------------------------------------------+\n  3. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Spirit  | 3,799 |  22 |     . |      3.0 |    12 |  2,640 |    168 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       35     |          121     |         3.08     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n. \n\n\n\n\n\n\n\n. webuse smoking, clear\n(Smoking and mortality data)\n\n. des\n\nContains data from https://www.stata-press.com/data/r17/smoking.dta\n Observations:        17,260                  Smoking and mortality data\n    Variables:            16                  27 Dec 2020 15:21\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nserno           int     %10.0g                Serial number\ncigs            byte    %10.0g                Daily cigarette consumption\nsysbp           int     %10.0g                Systolic blood pressure (mm Hg)\ndiasbp          float   %10.0g                Diastolic blood pressure (mm Hg)\nmap             float   %9.0g                 Mean arterial pressure (mm Hg)\nage             byte    %10.0g                Age (years)\nht              double  %10.0g                Height (cm)\nwt              double  %10.0g                Weight (kg)\nchol            double  %10.0g                Cholesterol (mmol/l)\ngradd1          byte    %8.0g                 Job grade 1\ngradd2          byte    %8.0g                 Job grade 2\ngradd3          byte    %8.0g                 Job grade 3\nall10           byte    %8.0g                 Ten year mortality\npyar            double  %10.0g                Years of follow-up\nchd             byte    %10.0g                Censoring (0 = censored, 1 = died\n                                                of CHD)\njobgrade        byte    %10.0g                Job grade\n-------------------------------------------------------------------------------\nSorted by: \n\n. list cigs map age ht gradd1 in 1/3\n\n     +-----------------------------------------+\n     | cigs        map   age       ht   gradd1 |\n     |-----------------------------------------|\n  1. |    0         97    46   154.94        0 |\n  2. |    0   97.66666    55   179.07        1 |\n  3. |    0         82    43   173.99        1 |\n     +-----------------------------------------+\n\n. \n\n\n\n\n\n\n\n. frause smoking, clear\n(A.C.Cameron & P.K.Trivedi (2022): Microeconometrics Using Stata, 2e)\n\n. des\n\nContains data from https://friosavila.github.io/playingwithstata/data2/smoking.\n&gt; dta\n Observations:         1,209                  A.C.Cameron & P.K.Trivedi\n                                                (2022): Microeconometrics Using\n                                                Stata, 2e\n    Variables:             7                  5 Oct 2022 13:08\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nstate           byte    %14.0g     state      State no\nyear            int     %9.0g                 Year\ncigsale         float   %9.0g                 Cigarette sale per capita (in\n                                                packs)\nlnincome        float   %9.0g                 Log state per capita GDP\nbeer            float   %9.0g                 Beer consumption per capita\nage15to24       float   %9.0g                 Percent of state population aged\n                                                15–24 years\nretprice        float   %9.0g                 Retail price of cigarettes\n-------------------------------------------------------------------------------\nSorted by: year\n\n. list in 1/3\n\n     +-----------------------------------------------------------------------+\n     |        state   year   cigsale   lnincome   beer   age15~24   retprice |\n     |-----------------------------------------------------------------------|\n  1. | Rhode Island   1970     123.9          .      .   .1831579       39.3 |\n  2. |    Tennessee   1970      99.8          .      .   .1780438       39.9 |\n  3. |      Indiana   1970     134.6          .      .   .1765159       30.6 |\n     +-----------------------------------------------------------------------+\n\n."
  },
  {
    "objectID": "Stata_Basics.html#summary-statistics",
    "href": "Stata_Basics.html#summary-statistics",
    "title": "Stata-Basics",
    "section": "",
    "text": "Summary Statistics are essential before starting basic analysis. Stata gives you many options. Although not all of them are easy to export.\n\n\n\n\n. frause oaxaca, clear\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n. *summarize [varlist] [if] [in] [weight] [, options]\n. summarize if female==1, sep(0)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      lnwage |        683    3.266761    .5700236    .507681   5.259097\n        educ |        888    11.06025     2.26024          5       17.5\n       exper |        683    12.13769     8.32663          0   41.91667\n      tenure |        683    6.605051    6.727475          0   40.08333\n        isco |        683    3.920937    1.762983          1          9\n      female |        888           1           0          1          1\n         lfp |        888    .7691441    .4216179          0          1\n         age |        888    39.88401    10.72665         18         62\n       agesq |        888    1705.666    879.4667        324       3844\n      single |        888    .3018018    .4592984          0          1\n     married |        888    .5236486    .4997219          0          1\n    divorced |        888    .1745495    .3797953          0          1\n       kids6 |        888     .286036    .6726639          0          4\n     kids714 |        888        .375    .7538888          0          4\n          wt |        888    .9883364    .4030975   .5302977   3.181786\n\n. \n\n\n\n\n\n. *   tabstat varlist [if] [in] [weight] [, options]\n. tabstat educ exper tenure age married, by(female)\n\nSummary statistics: Mean\nGroup variable: female (sex of respondent (1=female))\n\n  female |      educ     exper    tenure       age   married\n---------+--------------------------------------------------\n       0 |   11.8004  14.07684  9.003107  38.51647  .5230567\n       1 |  11.06025  12.13769  6.605051  39.88401  .5236486\n---------+--------------------------------------------------\n   Total |  11.40134  13.15324  7.860937  39.25379  .5233758\n------------------------------------------------------------\n\n. tabstat educ exper tenure , by(female) stats(p10 p50 p90)\n\nSummary statistics: p10, p50, p90\nGroup variable: female (sex of respondent (1=female))\n\n  female |      educ     exper    tenure\n---------+------------------------------\n       0 |      10.5  1.416667  .4166667\n         |      10.5  11.08333      6.25\n         |      17.5  31.33333     23.25\n---------+------------------------------\n       1 |         9      2.25  .3333333\n         |      10.5     10.75      4.25\n         |      12.5    23.375     15.75\n---------+------------------------------\n   Total |         9  1.833333  .4166667\n         |      10.5  10.91667  5.291667\n         |        15  28.08333  19.41667\n----------------------------------------\n\n. \n\n\n\n\n\n. ssc install table1\nchecking table1 consistency and verifying not already installed...\nall files already exist and are up to date.\n\n. * see help table1\n. table1, by(female) vars(lnwage contn %3.2f \\ age contn %2.1f \\ married bin)\n  +--------------------------------------------------------------------+\n  | Factor                         female = 0    female = 1    p-value |\n  |--------------------------------------------------------------------|\n  | N                              759           888                   |\n  |--------------------------------------------------------------------|\n  | log hourly wages, mean (SD)    3.44 (0.48)   3.27 (0.57)    &lt;0.001 |\n  |--------------------------------------------------------------------|\n  | age of respondent, mean (SD)   38.5 (11.3)   39.9 (10.7)     0.012 |\n  |--------------------------------------------------------------------|\n  | married                        397 (52.3%)   465 (52.4%)      0.98 |\n  +--------------------------------------------------------------------+\n\n. qui:table1, by(female) vars(lnwage contn %3.2f \\ age contn %2.1f \\ married bi\n&gt; n) saving(m1.xls)\nfile m1.xls already exists\nr(602);\n\n\nYou can see the file here\n\nThere are other options from Stata as well. see help dtable and help table.\nOr you could construct some yourself with the help of estout and esttab.\nSee here for a quick guide on tables."
  },
  {
    "objectID": "Stata_Basics.html#creating-variables",
    "href": "Stata_Basics.html#creating-variables",
    "title": "Stata-Basics",
    "section": "",
    "text": "Two main commands:\n\ngenerate (or gen for short): Creates new variables as a function of others in the data. One can apply system functions. Example:\n\ngen var1 = 1\ngen var2 = _n\ngen wage = exp(lnwage)\ngen age_educ = age * educ\n\nreplace: replaces values in an already existing variable.\n\nreplace wage = 0 if wage==.\nreplace age_educ = . if female==1\n\negen: Advanced variable generating function. It applies a single function to a variable or list of variables to create a third one.\n\negen wage_mean=mean(exp(lnwage)), by(female)\negen wage_p10=pctile(lnwage), by(female) p(10)\n\n\nTo delete a variable, you can use drop varname/varlist or drop2 varname/varlist\n\ndrop is the official. Stops if the variable does not exist.\ndrop2 an addon. Will still work even if a variable name does not exist.\nRequires using full variable name.\n\n\n\n\n\n. gen var1 = exp(lnwage)\n(213 missing values generated)\n\n. gen xar2 = exp(lnwage)+married\n(213 missing values generated)\n\n. drop x\n\n. des var1 xar2\nvariable xar2 not found\nr(111);\n\n\n\n\n\n. gen xar2 = exp(lnwage)+married\n(213 missing values generated)\n\n. drop2 x\nvariable x not found\n\n."
  },
  {
    "objectID": "Stata_Basics.html#variables-management",
    "href": "Stata_Basics.html#variables-management",
    "title": "Stata-Basics",
    "section": "",
    "text": "Stata is case sensitive.\n\nYou can create variables with names one, One, OnE, ONE, etc.\n“file addresses” and commands are also case sensitive\n\nIn Stata, variable names cannot can only start with a letter or “_“. Otherwise, it will give you an error.\nOnce variables are created, you could “label” them\nlabel var variable_name \"Description\"\nYou can name other components of a dataset as well. See help label"
  },
  {
    "objectID": "Stata_Basics.html#plots-in-stata",
    "href": "Stata_Basics.html#plots-in-stata",
    "title": "Stata-Basics",
    "section": "",
    "text": "Stata can create figures and plots for data exploration\n\n\n\n\n\n\n. webuse dui, clear\n(Fictional data on monthly drunk driving citations)\n\n. scatter citations fines\n\n. \n\n\n\n\n\n\n\n\n. two (scatter citations fines if csize==1)  ///\n&gt;     (scatter citations fines if csize==2), ///\n&gt;     legend(order(1 \"Small\" 2 \"Medium\"))\n\n. \n\n\n\n\n\n\nThe limitation. User written plotting commands do not interact well with Official plotting commands."
  },
  {
    "objectID": "Stata_Basics.html#saving-your-work",
    "href": "Stata_Basics.html#saving-your-work",
    "title": "Stata-Basics",
    "section": "",
    "text": "Command Window is effective to provide interactive analysis\nAt the end of your session, you can recover everything you did, clicking on the History Section, and save everythig, or just specific commands.\nThe best approach, however, is to ALWAYS use a do-file.\nFirst of all: Create a working directory. A folder in your computer that will hold your project, work, paper, homework, etc. (highly recommended)\nCreate a dofile: For simple projects a single file will suffice, but multiple may be needed for larger ones.\nTo start a dofile, simply type doedit \"filename\" in your command window.\n\nIf file exists in your “working directory” (type cd to see where you “are”), it will open it.\nOtherwise, a new file will be created\n\ndo-files are the best approach to save your work, and keep track of your analysis.\nGeneral Suggestion: Allways add comments to it, to know what you are doing\n\n\n*  You can always Start a command like this\n// Or like this\n/*\nBut you can always add a large comment using \"/*\" to start\nand \"*/\" to end it\n*/\n\n/* You could also add comments at the end of a command */\nsysuse auto, clear  // Loading Auto Dataset, after \"clearing\" the one currently in memory\n\n// Or as I did before, break a long command in various lines using \"///\"\n// Comments after \"///\" are possible\nregress price     /// Dep variable\n        mpg       /// indep variable \n        i.foreign, /// Foreign Dummy\n        robust     // Request Robust Standard errors.\n        \n** Last one has only two \"/\", because line ends there\n\n(1978 automobile data)\n\nLinear regression                               Number of obs     =         74\n                                                F(2, 71)          =      12.72\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.2838\n                                                Root MSE          =     2530.9\n\n------------------------------------------------------------------------------\n             |               Robust\n       price | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -294.1955   60.33645    -4.88   0.000     -414.503   -173.8881\n             |\n     foreign |\n    Foreign  |   1767.292   607.7385     2.91   0.005     555.4961    2979.088\n       _cons |   11905.42   1362.547     8.74   0.000     9188.573    14622.26\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "Stata_Basics.html#estimation-commands",
    "href": "Stata_Basics.html#estimation-commands",
    "title": "Stata-Basics",
    "section": "",
    "text": "Most commands in Stata have the following syntax:\n\n[by varlist:/prefix] command [varlist]  [if exp] [in range] [weight] [using filename] [,options]\n\neverything in [] are optional.\n[by varlist:/prefix]: by is used to execute the command by groups.\nprefix to request additional manipulation (advanced use)\ncommand: The command itself that will process the data\nvarlist: For Estimation commands include the dependent (first) and independent variables (everything else)\n[if exp] [in range]: To restrict samples\n[weight]: Request the use of weights ie: [fw = wgt_var] or [pw = wgt_var]\n[using filename]: Some commands allow you to use this to work with not-yet loaded datasets or files.\n[options]: Options requesting specific behaivior, statistics, etc"
  },
  {
    "objectID": "Stata_Basics.html#adv-options-for-saving-work.",
    "href": "Stata_Basics.html#adv-options-for-saving-work.",
    "title": "Stata-Basics",
    "section": "",
    "text": "Use github, as an additional data-repository\nCombine Stata, python and nbstat to create Jupyter notebooks.\nYou can also use Quarto to create full dynamic reports."
  },
  {
    "objectID": "rmethods/table/tab1.html",
    "href": "rmethods/table/tab1.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "asd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOz\n\nlbs\n\nKgs\n\nGr\n\n\n\n\n\nmale\n3.123\n\n0.195\n\n0.089\n\n88.605\n\n\n\n\n(1.071)\n[2.916]\n(0.067)\n[2.916]\n(0.030)\n[2.916]\n(30.389)\n[2.916]\n\n\nwhite\n5.404\n\n0.338\n\n0.153\n\n153.346\n\n\n\n\n(1.392)\n[3.882]\n(0.087)\n[3.882]\n(0.039)\n[3.882]\n(39.497)\n[3.882]\n\n\ncigs\n-0.480\n\n-0.030\n\n-0.014\n\n-13.628\n\n\n\n\n(0.091)\n[-5.288]\n(0.006)\n[-5.288]\n(0.003)\n[-5.288]\n(2.577)\n[-5.288]\n\n\nlfaminc\n1.053\n\n0.066\n\n0.030\n\n29.867\n\n\n\n\n(0.632)\n[1.664]\n(0.040)\n[1.664]\n(0.018)\n[1.664]\n(17.946)\n[1.664]\n\n\n_cons\n110.603\n\n6.913\n\n3.138\n\n3138.351\n\n\n\n\n(2.071)\n[53.410]\n(0.129)\n[53.410]\n(0.059)\n[53.410]\n(58.760)\n[53.410]\n\n\nN\n1388\n\n1388\n\n1388\n\n1388\n\n\n\nR2\n0.046\n\n0.046\n\n0.046\n\n0.046"
  },
  {
    "objectID": "rmethods/Question 1.html",
    "href": "rmethods/Question 1.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Question 1\nconsider the following matrixes\n\\(A=\\begin{pmatrix} 1 & 2 & 3 \\\\ 3 & 3 & 8 \\end{pmatrix}\\)\n\\(B=\\begin{pmatrix} 3 & 1 & 6 \\\\ 2 & 1 & 2 \\end{pmatrix}\\)\n\\(C=\\begin{pmatrix} 5 & 1 & 6 \\\\ 1 & 1 & 2 \\\\ 4 & 2 & 6 \\end{pmatrix}\\)\nConsider the following Operations:\n\\(A + B\\)\n\\(A + B'\\)\n\\(A * B\\)\n\\(Det(C)\\)\n\\(C^{-1}\\)\n\\(A' * B\\)\n\\((A * B')^{-1}\\)\nIndicate if they are valid operations. if not, explain why If they are obtain the resulting matrices.\nQuestion 2\nConsider the following functions:\n\\(y=1+x^2-3x^3\\)\n\\(y=ln(3x^2+1)\\)\n\\(y=exp(x+x*z)\\)\nFor each one estimate: \\(\\frac{\\partial y}{∂ x}\\) and \\(\\frac{\\partial^2 y}{∂ x^2}\\)\nQuestion 3 Solve the following set of equations. If not, indicate why they cant be solved:\nEq1\n\\(\\begin{aligned} 2x+y &= 3 \\\\ 2x+2y &= -3 \\end{aligned}\\)\nEq 2\n\\(\\begin{aligned} 2x+ y + z &= 1 \\\\ 3x+ 2y - z &= 2 \\\\  x+ y - 2 z &= 3 \\\\ \\end{aligned}\\)\nEq 3\n\\(x^2 + x - 2 =0\\)\nQuestion 4: Maximization\n\nConsider the following set of functions:\n\n\\(\\begin{aligned} y &= A x^α z ^\\beta \\\\ C &= p_x x + P_z z \\end{aligned}\\)\nSet up the constrained maximization problem.\n\nconsider the following function:\n\n\\(y = f(z)\\)\nIf you were to maximize this function, what are the first order conditions?\nIf there is a unique solution, what are the conditions to verify it is the maximum.\nQuestion 5\nP1)\nConsider the following set of numbers\n\\(x = \\{10, 4, 5, 7, 3, 6, 9\\}\\)\nEstimate the mean, median, and variance of this data.\np2)\nSay that X is a continuous random variable that ranges from -10 to 10.\n\nWhat is the probability that X=1 (\\(P(x=1)\\))\nTrue or False \\(P(X&gt;1) + p(X&lt;1)=1\\)\nTrue or False \\(P(X&gt;1) + p(-1&lt;X&lt;2)\\) can be larger than 1.\n\nP3)\nSay that Z1 and Z2 are two random variables that follow a bernulli distribution. Assume that the following probabilities are true:\n\np(z1=0 & z2=0) = 0.1\np(z1=1 & z2=1) = 0.2\np(z1=0) = 0.5\np(z2=1) = 0.6\n\nWhat is the probability for:\n\nProbability Z1=1 conditional on z2=0\nunconditional probability of Z1=1\n\np4. Given two random variables x_1 and x_2, with means and variances \\(\\mu_1\\), \\(\\mu_2\\) and \\(\\sigma^2_1\\) and \\(\\sigma^2_2\\).\nif z = x_1 - x_2, what is the mean and variance of Z\nwhat is the covariance between z and x_1."
  },
  {
    "objectID": "rmethods/homework_3.html",
    "href": "rmethods/homework_3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Not here yet"
  },
  {
    "objectID": "rmethods/homework_1.html",
    "href": "rmethods/homework_1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Read Chapter 2 of -The Effect- by Nick Hungtington-Klein. here\nWrite a small research proposal that will answer answer the following:\n\nResearch question,\nIdentify your main dependent variable, and the variable(s) you want to analyze the causal effect of.\nDescribe relevant factors that may need to be considered for the analysis (Economic model).\nDescribe how those factors may relate to the outcome, and your variable(s) of interest.\nDescribe an ideal Experiment you may run to identify the effect. Is it a feasible experiment?\nIs there any data you could use to answer this question?"
  },
  {
    "objectID": "rmethods/homework_1.html#note",
    "href": "rmethods/homework_1.html#note",
    "title": "Homework 1",
    "section": "Note",
    "text": "Note\nSearch among all datasets available in frause, and use the data from these datasets to decide what variables to analyze, and controls to use in your homework.\nExamples in the textbook can be useful as guides to consider here.\nYou are free to explore other sources. If so, include the data along with your homework.\nParticularly unique answers (based on completeness, detail, novelty) may receive extra points."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#the-problem-endogeneity",
    "href": "rmethods/8_iv2sls.html#the-problem-endogeneity",
    "title": "Instrumental Variables and 2SLS",
    "section": "The problem: Endogeneity",
    "text": "The problem: Endogeneity\n\nAs I mentioned at the beginning, one of the most important assumptions required to analyze data and obtain correct estimations and draw inference was A4: No endogeneity or \\(E(e|X)=0\\)\n\nEndogeneity is a problem that occurs because the error is related to \\(X\\).\nThis is a proble,, because we can no longer assume the error is, in average, constant when analyzing changes in \\(X's\\)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section",
    "href": "rmethods/8_iv2sls.html#section",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "Why did it happen?\n\nUsually because important variables are omitted\n\nAdd them back, or at least proxies?\n\nIncorrect functional form\n\nTry making it more flexible?\n\nData has measurement error\n\nGet better data?\n\nSample is endogenous (other treatments are necessary)\nReverse causality (you dont know which cause which)\nSimultenaity, similar to omitted variables. There is another factor that caused both the outcome and explanatory variable"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#a-pair-of-solutions",
    "href": "rmethods/8_iv2sls.html#a-pair-of-solutions",
    "title": "Instrumental Variables and 2SLS",
    "section": "A pair of Solutions",
    "text": "A pair of Solutions\n\nToday we will cover one approach that could help with many (not all) the situations that could cause endogeneity.\nTo apply this approach, however, we need:\n\nMore data (more variables with specific properties)\nMore information on how the “system” works.\n\nThese methods are:\n\nIV - Instrumental variable\n2sls - Two-stage Least squares\n\n\n\nNOTE: These two methods are almost interchangable.\n\nIV refers to cases with 1 endogenous variable and 1 “instrument”\n2sls refers to cases with 1+ endogenous variables and “instruments”"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#what-is-an-instrumental-variable",
    "href": "rmethods/8_iv2sls.html#what-is-an-instrumental-variable",
    "title": "Instrumental Variables and 2SLS",
    "section": "What is an “Instrumental Variable”",
    "text": "What is an “Instrumental Variable”\n\nI have mentioned a few times the word “instrument” But what are they really?\nInstruments: the heros/variables that will “save us” of endogeneity.\nThey have at least 2 properties:\n\nInstruments should be exogenous to the model\n\n\nThis means that they do not appear in the theoretical specification, thus there is no DIRECT conection between the instrument \\(Z\\) and the outcome \\(y\\).\n\nShould only be connected through the endogenous variable.\n\nAlso that there is no correlation between the model error and \\(Z\\).\n\n\nThe instrument is relevant and related to \\(x\\) (endogenous variable)\n\n\nPreferably, you need a variable that is not only correlated with \\(X\\) but determines changes in \\(X\\).\nWe also need this effect to be monotonic!\n\nIn many instances we even want an instrument that is just as good as [conditionally] random."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#how-instruments-work-the-math",
    "href": "rmethods/8_iv2sls.html#how-instruments-work-the-math",
    "title": "Instrumental Variables and 2SLS",
    "section": "How Instruments work: The math",
    "text": "How Instruments work: The math\nThe problem \\(corr(x_1,e) \\neq 0\\): \\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + e || \\tilde w = w-\\bar w  \\\\\n\\tilde \\beta_1 &=\\frac{\\sum \\tilde x_1 \\tilde y}{\\sum \\tilde x_1^2}=\n\\frac{\\sum \\tilde x_1 (\\beta_1 \\tilde x_1 + e)}{\\sum \\tilde x_1^2} \\\\\n& = \\beta_1 + \\frac{\\sum \\tilde x_1 e}{\\sum \\tilde x_1^2}\n\\end{aligned}\n\\]\nHow IV works \\(corr(z_1,e) \\neq 0\\):\n\\[\\begin{aligned}\n\\hat \\beta_1 &=\\frac{\\sum \\tilde z_1 \\tilde y}{\\sum \\tilde z_1 \\tilde x_1}=\n\\frac{\\sum \\tilde z_1 (\\beta_1 \\tilde x_1 + e)}{\\sum \\tilde z_1 \\tilde x_1} \\\\\n& = \\beta_1 + \\frac{\\sum \\tilde z_1 e}{\\sum \\tilde z_1 \\tilde x_1}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#how-instruments-work-the-intuition",
    "href": "rmethods/8_iv2sls.html#how-instruments-work-the-intuition",
    "title": "Instrumental Variables and 2SLS",
    "section": "How Instruments work: The intuition",
    "text": "How Instruments work: The intuition\n\nOne way of thinking about how IV works is by realizing not ALL changes in \\(X_1\\) are endogenous. Some are due to \\(e\\), but some are due other factors.\n\nwe just can’t differentiate them\nIf we could use (in the regression), only exogenous changes, (or omit endogenous ones), we could estimate our models correctly.\n\nWhat IV’s do is to identify Part of the exogenous component (the one related to \\(Z\\)), and use only THAT variation to identify coefficients.\n\nThis would do a reasonable work, as long as the instrument is relevant, and instrument exogenous."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#example",
    "href": "rmethods/8_iv2sls.html#example",
    "title": "Instrumental Variables and 2SLS",
    "section": "Example",
    "text": "Example\n\\[\\begin{aligned}\ne &\\sim chi(2)-2 ; z = chi(2)-2 ; x = chi(2)-2+z+e \\\\\ny &=1+x+e\n\\end{aligned}\n\\]\n\n\nCode\n** Montecarlo Simulation\nclear\nset seed 10101\nset obs 1000\nqui:mata:\nk = 1000; n=1000\nb1=bc = b = J(k,2,0)\nfor(i=1;i&lt;=k;i++){\n    e = rchi2(n,1,2):-2\n    e1 = rchi2(n,1,2):-2\n    z = rchi2(n,1,2):-2,J(n,1,1)\n    x = rchi2(n,1,2):-2:+z[,1]:+e ,J(n,1,1)\n    x1 = rchi2(n,1,2):-2:+z[,1]:+e1,J(n,1,1)\n    y = 1:+x[,1]:+e\n    y1 = 1:+x[,1]:+e1\n    xx = cross(x,x)\n    b[i,] = (invsym(xx)*cross(x,y))'\n    bc[i,] = (invsym(cross(z,x))*cross(z,y))'\n    b1[i,] = (invsym(xx)*cross(x,y1))'\n}\nend\ngetmata bb*=b\ngetmata bc*=bc\ngetmata b_*=b1\nset scheme white2\ncolor_style tableau\ntwo kdensity bb1 ||  kdensity bc1  || kdensity b_1, ///\nlegend(order(1 \"X Endogenous\" 2 \"IV\" 3 \"X Exogenous\"))"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#se-and-statistical-inference",
    "href": "rmethods/8_iv2sls.html#se-and-statistical-inference",
    "title": "Instrumental Variables and 2SLS",
    "section": "SE and Statistical Inference",
    "text": "SE and Statistical Inference\n\nSE have a different structure compared to OLS.\nIn the simplest case (one dep variable that is endogenous), and under the assumption of Homoskedasticity, SE for \\(\\beta_1\\) are given by:\n\n\\[\\begin{aligned}Var_{iv}(\\beta_1) = \\frac{\\hat\\sigma^2_e}{SST_x R^2_{x|z}} \\\\\n\\hat\\sigma^2_e =\\frac{ \\sum (y-\\hat\\beta_0-\\hat\\beta_1 x)^2}{n-2}\n\\end{aligned}\n\\]\n\nOnce they are obtained t-stats can be used as usual"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#examples-of-ivs",
    "href": "rmethods/8_iv2sls.html#examples-of-ivs",
    "title": "Instrumental Variables and 2SLS",
    "section": "Examples of IV’s",
    "text": "Examples of IV’s\n\n\nEducation:\n\nFathers Education\nDistance to School\n# Siblins\n\n\nVeteran Status:\n\nVietnam Lottery Ticket\n\nOther:\n\nRain\nShift-Share\nJudge FE\n\n\n\n\nIV SE will be necessarily larger than OLS, because there is less variation of \\(x\\) used to identify \\(\\beta\\).\nIf \\(x\\) is used as its own instrument, the estimator will be that of OLS."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#weak-and-endogenous-instruments",
    "href": "rmethods/8_iv2sls.html#weak-and-endogenous-instruments",
    "title": "Instrumental Variables and 2SLS",
    "section": "Weak and endogenous instruments:",
    "text": "Weak and endogenous instruments:\n\nWhile instruments can be used to address Endogeneity problems, finding good instruments can be hard.\n\nThey can be “weak-instruments”\nor not fully exogenous\n\nIf this happens, IV can be worse than endogeneity:\n\n\\[\\begin{aligned}\nplim \\beta_{ols} &= \\beta_1 + corr(x,u) \\frac{\\sigma_u}{\\sigma_x} \\\\\nplim \\beta_{iv}  &= \\beta_1 + \\frac{corr(z,u)}{corr(z,x)} \\frac{\\sigma_u}{\\sigma_x}\n\\end{aligned}\n\\]\n\nwe will talk about the “weak-instruments” later today."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#iv-with-mlr",
    "href": "rmethods/8_iv2sls.html#iv-with-mlr",
    "title": "Instrumental Variables and 2SLS",
    "section": "IV with MLR",
    "text": "IV with MLR\nAdding controls…is easy!\n\\[y = \\beta_0 + \\gamma_1 y_1 + X\\beta + e ; z \\text{ instrument for } y\n\\]\nWe still assume 1 endogenous variable (\\(y_1\\)) and one instrument (\\(z\\))\n\\[X =\\begin{bmatrix} 1 & y_1 & x_1 & x_2 & x_3 \\end{bmatrix} ;\nZ =\\begin{bmatrix} 1 & z & x_1 & x_2 & x_3 \\end{bmatrix}\n\\]\nthen \\(\\hat\\beta_{iv}\\) is given by\n\\[\\hat\\beta_{iv} = (Z'X)^{-1}{Z'y}\n\\]\n\nSame assumptions needed, except that instrument strength is measured by the \\(corr(y_1-E[y_1|X], z-E[z|X] )\\)\n\nMulticollinearity problem can be a problem here"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#iv-and-treatment-effects",
    "href": "rmethods/8_iv2sls.html#iv-and-treatment-effects",
    "title": "Instrumental Variables and 2SLS",
    "section": "IV and Treatment Effects",
    "text": "IV and Treatment Effects\nOne small note:\n\nWhen analyzing the model of interest, we could also try to analyze the reduced form model:\n\n\\[y = \\beta_0 + \\lambda z + X \\beta + e\\]\n\nThis model will not give you the effect of \\(y_1\\) (endogenous variable), but can be just as interesting, specially when instrument and endogenous variables are dummies.\n\nIn such case \\(\\lambda\\) will represent the “intention-to-treat” effect, rather than “treatment” effect."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#sls-many-ys-many-zs",
    "href": "rmethods/8_iv2sls.html#sls-many-ys-many-zs",
    "title": "Instrumental Variables and 2SLS",
    "section": "2SLS: Many Ys many Zs",
    "text": "2SLS: Many Ys many Zs\n\nThere could be situations where you have not one, but many instruments.\n\nThis may be rare, as even finding a single instrument can be hard\n\nYou may also have the situation where there is more than one endogenous variable!\n\nIn such case, you need at least as many IVs as endogenous variables\n\n\n\nThe same assumptions as before apply. IV’s need to be exogenous, but relevant to explain the endogenous variables.\nContrary to intuition, ALL instruments are used to analyze ALL exogenous variables"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#sls-estimation",
    "href": "rmethods/8_iv2sls.html#sls-estimation",
    "title": "Instrumental Variables and 2SLS",
    "section": "2SLS: Estimation",
    "text": "2SLS: Estimation\nConsider the following:\n\n\\(y\\) is the variable of interest\n\\(x1, x2\\) set of exogenous variables\n\\(y1, y2\\) set of endogenous variables\n\\(z1,z2,z3\\) set of instruments for \\(y1\\) and \\(y2\\)\n\n\\(X's\\) and \\(Z's\\) are exogenous:"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section-1",
    "href": "rmethods/8_iv2sls.html#section-1",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "Model of interesed: \\[y = a_0 + a_1 y_1 + a_2 y_2 + b_1 x_1 + b_2 x_2 +  b_3 x_3 + e\n\\]\nFirst Stage\n\\[\\begin{aligned}\ny_1 = \\gamma^1_0 + \\gamma^1_1 z_1+ \\gamma^1_2 z_2+ \\gamma^1_3 z_3+\\lambda^1_1 x_1 + \\lambda^1_2 x_2 +  \\lambda^1_3 x_3  + v_1 \\rightarrow \\hat y_1 \\\\\ny_2 = \\gamma^2_0 + \\gamma^2_1 z_1+ \\gamma^2_2 z_2+ \\gamma^2_3 z_3+\\lambda^2_1 x_1 + \\lambda^2_2 x_2 +  \\lambda^2_3 x_3  + v_2 \\rightarrow \\hat y_1\n\\end{aligned}\n\\]\nSecond Stage:\n\\[y = a_0 + a_1 \\hat y_1 + a_2 \\hat y_2 + b_1 x_1 + b_2 x_2 +  b_3 x_3 + e\n\\]\nThis last model should work, because \\(\\hat y_1\\) and \\(\\hat y_2\\) are exogenous (if \\(Zs\\) are).\nStandard Errors, however, need to be adjusted appropietly. (all two-step estimators need this)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#matrix-math",
    "href": "rmethods/8_iv2sls.html#matrix-math",
    "title": "Instrumental Variables and 2SLS",
    "section": "Matrix Math",
    "text": "Matrix Math\n\\[\\begin{aligned}\nX &= \\begin{bmatrix} 1 & y_k & x  \\end{bmatrix} \\\\\nZ &=\\begin{bmatrix} 1 & z   & x \\end{bmatrix}\n\\end{aligned}\n\\]\nThen\n\\[\\begin{aligned}\n\\beta_{2sls} &= [X'Z(Z'Z)^{-1}Z'X]^{-1}[X'Z(Z'Z)^{-1}Z'y] \\\\\n\\beta_{2sls} &= [X'P_z X]^{-1}[X'P_z y] \\\\\n\\beta_{2sls} &= [\\hat X'X]^{-1}[\\hat X'y]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#multicolinearity-and-2sls",
    "href": "rmethods/8_iv2sls.html#multicolinearity-and-2sls",
    "title": "Instrumental Variables and 2SLS",
    "section": "Multicolinearity and 2sls",
    "text": "Multicolinearity and 2sls\n\nWhen Appplying 2sls, the problem of Multicolinearity could be stronger.\n\nBecause of MCL, Standard errors will increase (less individual variation.)\nBecause of IV, only a fraction of the variation is used for the analysis.\nBotton line: Combined have conisderate results."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#weak-instruments",
    "href": "rmethods/8_iv2sls.html#weak-instruments",
    "title": "Instrumental Variables and 2SLS",
    "section": "Weak Instruments",
    "text": "Weak Instruments\n\nWeak Instruments create problems when using IV’s and 2SLS.\n\nA weak instrument is one that doesnt have much explanatory power on dep variable, once all other controls are taken into account.\nIf the instrument is too weak, the bias it generates could larger than OLS.\nThe distribution of coefficent is no longer normal, so its harder to make inference.\n\n\n\nHow do we test for it?"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section-2",
    "href": "rmethods/8_iv2sls.html#section-2",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "We usually test for weak instruments when analyzing the “first-stage” regression.\n\\[y_2 = \\gamma_0 + \\gamma_1 z_1 + \\gamma_2 z_2 + \\gamma_3 x_1 + \\gamma_4 x_2 + e\n\\]\n\nThe null is: \\(H_0: \\gamma_1 = \\gamma_2 =0\\), or the instruments are weak.\nBased on Stock and Yogo (2005), the general recommendation is to get an F&gt;10, to reject the Null.\nHowever, new evidence and research suggest that F=10 is not large enough.\n\nOne should either use F&gt;104 (To keep the same t) (Lee McCrary Moreira Porter, 2020)\nor use an alternative t-critica (3.4 for an F=10)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#examples",
    "href": "rmethods/8_iv2sls.html#examples",
    "title": "Instrumental Variables and 2SLS",
    "section": "Examples",
    "text": "Examples\n\n\nCode\n/*\n* This code is only to show the process. Too long to run in quarto\ncapture program drop simx \nprogram simx, eclass\nclear\nlocal N `1'\nlocal F `2'\nlocal sig = sqrt(`N'/ `F')\nset obs `N'\ngen e = rnormal()\ngen z = rnormal() \ngen x = 1 + z + (e+rnormal())*sqrt(.5)*`sig'\n*sqrt(10) \ngen y = 1 + (e+rnormal())*sqrt(.5)\nreg x z, \nmatrix b=(_b[z]/_se[z])^2\nivreg y (x=z), \nmatrix b=b,_b[x],_b[x]/_se[x]\nereturn post b\nend\n\ntempfile f1 f2 f3 f4 f5\nparallel initialize 14\nparallel sim, reps(5000): simx 500 10\ngen F=10\nsave `f1'\nparallel sim, reps(5000): simx 500 20\ngen F=20\nsave `f2'\nparallel sim, reps(5000): simx 500 40\ngen F=40\nsave `f3'\nparallel sim, reps(5000): simx 500 80\ngen F=80\nsave `f4'\nparallel sim, reps(5000): simx 500 160\ngen F=160\nsave `f5'\n\nclear \nappend using `f1'\nappend using `f2'\nappend using `f3'\nappend using `f4'\nappend using `f5'\n\nren (*) (f_stat b_coef t_stat)\n*/\nuse mdata/ivweak, clear\nset scheme white2\ncolor_style tableau\n\njoy_plot t_stat, over(F) xline(-1.96 1.96) xtitle(t-Stat) dadj(2)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#iv-for-measurement-errors-two-wrongs-make-one-right",
    "href": "rmethods/8_iv2sls.html#iv-for-measurement-errors-two-wrongs-make-one-right",
    "title": "Instrumental Variables and 2SLS",
    "section": "IV for Measurement errors: Two wrongs make one right",
    "text": "IV for Measurement errors: Two wrongs make one right\n\nAs described previously when independent variables have measurement errors (Classical), using the variable with errors will produced biased coefficients.\nIf you have multiple variables with Mesurement error, however, its possible to use IV to correct the problem.\n\nThis is done by using one variable as the instrument of the other.\n\nFOr this strategy to work, we need the error to be classical. That is, Uncorrelated across each other, and unrelated to the model error.\n\nConsider the following:\n\\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + e \\\\\n\\check x_1 &= x_1 + u_1 \\\\\n\\tilde x_1 &= x_1 + u_2 \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section-3",
    "href": "rmethods/8_iv2sls.html#section-3",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "If we use each model, independently, we will have biased coefficients\nIf we combined the, Biases will remain present (albeit lower)\nIf we use one as instrument of the other tho:\n\n\\[\n\\begin{aligned}\n\\beta_{1,iv} &= \\frac{cov(\\check x_1, y)}{cov(\\check x_1, \\tilde x_1)} or \\beta_{1,iv} = \\frac{cov(\\tilde x_1, y)}{cov(\\check x_1, \\tilde x_1)} \\\\\n& = \\frac{cov(x_1 + u_1, \\beta_0 + \\beta_1 x_1 + e)}{cov(x_1 + u_2, x_1 + u_1)} \\\\\n& = \\frac{\\beta_1 cov(x_1,x_1) + cov(x_1, e) + \\beta_1 cov(u_1, x_1) + cov(x_1,e)}{cov(x_1 , x_1 )} \\\\\n& = \\beta_1\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#example-1",
    "href": "rmethods/8_iv2sls.html#example-1",
    "title": "Instrumental Variables and 2SLS",
    "section": "Example",
    "text": "Example\n\n\nCode\nclear\nset obs 1000\ngen x = rnormal()\ngen y = 1 + x + rnormal() \ngen x1 = x + rnormal()\ngen x2 = x + rnormal()\nqui: regress y x\nest sto m1\nqui: regress y x1\nest sto m2\nqui: regress y x2\nest sto m3\ngen x1_x2 = (x1 + x2)/2\nqui: regress y x1_x2\nest sto m3b\nqui:ivregress 2sls  y (x1=x2)\nest sto m4\nqui:ivregress 2sls  y (x2=x1)\nest sto m5\nesttab m1 m2 m3 m3b m4 m5, se\n\n\nNumber of observations (_N) was 0, now 1,000.\n\n-------------------------------------------------------------------------------\n&gt; -----------------------------\n                      (1)             (2)             (3)             (4)      \n&gt;        (5)             (6)   \n                        y               y               y               y      \n&gt;          y               y   \n-------------------------------------------------------------------------------\n&gt; -----------------------------\nx                   1.006***                                                   \n&gt;                              \n                 (0.0320)                                                      \n&gt;                              \n\nx1                                  0.520***                                   \n&gt;      1.018***                \n                                 (0.0277)                                      \n&gt;   (0.0640)                   \n\nx2                                                  0.502***                   \n&gt;                      1.038***\n                                                 (0.0277)                      \n&gt;                   (0.0654)   \n\nx1_x2                                                               0.682***   \n&gt;                              \n                                                                 (0.0301)      \n&gt;                              \n\n_cons               1.010***        0.996***        0.988***        0.983***   \n&gt;      0.974***        0.956***\n                 (0.0313)        (0.0380)        (0.0384)        (0.0360)      \n&gt;   (0.0438)        (0.0451)   \n-------------------------------------------------------------------------------\n&gt; -----------------------------\nN                    1000            1000            1000            1000      \n&gt;       1000            1000   \n-------------------------------------------------------------------------------\n&gt; -----------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#testing-for-endogeneity-and-overidentifying-restrictions",
    "href": "rmethods/8_iv2sls.html#testing-for-endogeneity-and-overidentifying-restrictions",
    "title": "Instrumental Variables and 2SLS",
    "section": "Testing for Endogeneity and Overidentifying restrictions",
    "text": "Testing for Endogeneity and Overidentifying restrictions\n\nIn general, Instrumental variables are a great tool to deal with A4 violations.\n\nbut, it can be hard to find the perfect IV, and you may still have problems if its Weak.\n\nIn that case, it may be useful to asnwer…Do you have an Endogeneity problem? (empirically rather than theoretically)\n\nTest:\n\nEstimate first stage, and save predicted residuals \\(\\hat v\\):\n\n\\[y_2 = \\gamma_0 + \\gamma_1 z_1 + \\gamma_2 x_1 + \\gamma_3 x_2 + v\n\\]\nYou expect residuals to be endogenous\n\nEstimate main model “adding” the residuals first stage\n\n\\[y = \\beta_0 + \\beta_1 y_2 + \\beta_2 x_1 + \\beta_3 x_2 + \\theta \\hat v + e\n\\]\nTest for \\(H_0: \\theta=0\\)."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section-4",
    "href": "rmethods/8_iv2sls.html#section-4",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "If the model was endogenous, Then \\(\\theta\\) will be different from zero, and \\(\\beta's\\) different from the case without controlling for it.\nOtherwise, we reject presence of endogeneity.\nThis method has the added advantage:\nFor the simple and exactly identified case, 2sls and adding residuals provide the same solution, except for SE. (its called Control function approach)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#overidentifying-restrictions",
    "href": "rmethods/8_iv2sls.html#overidentifying-restrictions",
    "title": "Instrumental Variables and 2SLS",
    "section": "overidentifying restrictions",
    "text": "overidentifying restrictions\n\nSome times, you may have access to multiple potential instruments.\n\nBut what if this instruments give you different results?\nIf you expect/believe a single effect exists, then there may be a problem. (one of they may be endogenous)\n\nSo how do we test if the instruments are exogenous?\n\nFirst you need more instruments than endogenous variables."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section-5",
    "href": "rmethods/8_iv2sls.html#section-5",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "S1. Estimate Structural Equation \\[y=\\beta_0 + \\gamma y_2 + \\beta_1 x_1 + e | y_2 \\sim z_1, z_2\n\\]\nS2. Auxiliary equation \\[\n\\hat e = \\delta_0 + \\delta_1 z_1 + \\delta_2 z2 + \\delta_3 x_1 + v\n\\]\nS3. Test for Overall Fitness. \\(nR^2\\sim \\chi^2(q_{iv})\\) with \\(q_{iv} = \\# over IVs\\)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#ivs-as-lates",
    "href": "rmethods/8_iv2sls.html#ivs-as-lates",
    "title": "Instrumental Variables and 2SLS",
    "section": "IV’s as LATES",
    "text": "IV’s as LATES\nNOTE\n\n\n\n\n\n\n\n\n\nWhen describing this test, I mentioned that one would be typically worried if observing multiple coefficients when using different IV’s.\nHowever, IV’s can identify different effects, because different IV’s may affect different people differently.\n\\(Z1\\) may affect, say, only men. \\(z_2\\) only women, \\(z_3\\) only highly educated, etc.\nWhen analyzing IV’s will be important to undertand who would be affected by the instrument the most, because that may explain why effects vary."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#example-2",
    "href": "rmethods/8_iv2sls.html#example-2",
    "title": "Instrumental Variables and 2SLS",
    "section": "Example",
    "text": "Example\nIgnoring Potential endogeneity\n\nfrause mroz, clear\ndrop if lwage==.\n** But with Endogeneity\nreg lwage educ exper expersq\n\n(325 observations deleted)\n\n      Source |       SS           df       MS      Number of obs   =       428\n-------------+----------------------------------   F(3, 424)       =     26.29\n       Model |  35.0222967         3  11.6740989   Prob &gt; F        =    0.0000\n    Residual |  188.305144       424  .444115906   R-squared       =    0.1568\n-------------+----------------------------------   Adj R-squared   =    0.1509\n       Total |  223.327441       427  .523015084   Root MSE        =    .66642\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .1074896   .0141465     7.60   0.000     .0796837    .1352956\n       exper |   .0415665   .0131752     3.15   0.002     .0156697    .0674633\n     expersq |  -.0008112   .0003932    -2.06   0.040    -.0015841   -.0000382\n       _cons |  -.5220406   .1986321    -2.63   0.009    -.9124667   -.1316144\n------------------------------------------------------------------------------\n\n\nFirst Stage for different IVs\n\nqui: reg educ fatheduc exper expersq\npredict r1 , res\ntest fatheduc\nadde scalar fiv = r(F)\nadde scalar pfiv = r(p)\nest sto m1\nqui: reg educ motheduc exper expersq\npredict r2 , res\ntest motheduc\nadde scalar fiv = r(F)\nadde scalar pfiv = r(p)\nest sto m2\nqui: reg educ fatheduc motheduc exper expersq\npredict r3 , res\ntest motheduc fatheduc\nadde scalar fiv = r(F)\nadde scalar pfiv = r(p)\nest sto m3\n\nesttab m1 m2 m3 , scalar(fiv pfiv) sfmt(%5.2f %5.3f) order(fatheduc motheduc exper expersq) se ///\nstar(* 0.1 ** 0.05 *** 0.01)\n\n\n ( 1)  fatheduc = 0\n\n       F(  1,   424) =   87.74\n            Prob &gt; F =    0.0000\n\n ( 1)  motheduc = 0\n\n       F(  1,   424) =   73.95\n            Prob &gt; F =    0.0000\n\n ( 1)  motheduc = 0\n ( 2)  fatheduc = 0\n\n       F(  2,   423) =   55.40\n            Prob &gt; F =    0.0000\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                     educ            educ            educ   \n------------------------------------------------------------\nfatheduc            0.271***                        0.190***\n                 (0.0289)                        (0.0338)   \n\nmotheduc                            0.268***        0.158***\n                                 (0.0311)        (0.0359)   \n\nexper              0.0468          0.0489          0.0452   \n                 (0.0411)        (0.0417)        (0.0403)   \n\nexpersq          -0.00115        -0.00128        -0.00101   \n                (0.00123)       (0.00124)       (0.00120)   \n\n_cons               9.887***        9.775***        9.103***\n                  (0.396)         (0.424)         (0.427)   \n------------------------------------------------------------\nN                     428             428             428   \nfiv                 87.74           73.95           55.40   \npfiv                0.000           0.000           0.000   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01\n\n\nUsing parents education as instruments\n\n* SMALL requests Df adjustment\nqui:ivregress 2sls lwage (educ= fatheduc ) exper expersq, small\nest sto m1\nqui:ivregress 2sls lwage (educ= motheduc ) exper expersq, small\nest sto m2\nqui:ivregress 2sls lwage (educ= fatheduc motheduc) exper expersq, small\nest sto m3\nesttab m1 m2 m3 , se mtitle(IV:Father IV:Mother IV:Parents) ///\nstar(* 0.1 ** 0.05 *** 0.01)\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                IV:Father       IV:Mother      IV:Parents   \n------------------------------------------------------------\neduc               0.0702**        0.0493          0.0614*  \n                 (0.0344)        (0.0374)        (0.0314)   \n\nexper              0.0437***       0.0449***       0.0442***\n                 (0.0134)        (0.0136)        (0.0134)   \n\nexpersq         -0.000882**     -0.000922**     -0.000899** \n               (0.000401)      (0.000406)      (0.000402)   \n\n_cons             -0.0611           0.198          0.0481   \n                  (0.436)         (0.473)         (0.400)   \n------------------------------------------------------------\nN                     428             428             428   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01\n\n\n** Testing for endogeneity\n\n* Instrumenting education\nqui:reg  lwage educ  exper expersq r1\nest sto m1\nqui:reg  lwage educ  exper expersq r2\nest sto m2\nqui:reg  lwage educ  exper expersq r3\nest sto m3\n\nesttab m1 m2 m3 , se mtitle(IV:Father IV:Mother IV:Parents) ///\nstar(* 0.1 ** 0.05 *** 0.01)\n\n*see -estat endogenous- after ivregress 2sls\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                IV:Father       IV:Mother      IV:Parents   \n------------------------------------------------------------\neduc               0.0702**        0.0493          0.0614** \n                 (0.0341)        (0.0366)        (0.0310)   \n\nexper              0.0437***       0.0449***       0.0442***\n                 (0.0133)        (0.0133)        (0.0132)   \n\nexpersq         -0.000882**     -0.000922**     -0.000899** \n               (0.000397)      (0.000398)      (0.000396)   \n\nr1                 0.0450                                   \n                 (0.0375)                                   \n\nr2                                 0.0684*                  \n                                 (0.0397)                   \n\nr3                                                 0.0582*  \n                                                 (0.0348)   \n\n_cons             -0.0611           0.198          0.0481   \n                  (0.433)         (0.463)         (0.395)   \n------------------------------------------------------------\nN                     428             428             428   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01\n\n\nOverid Test\n\nqui:ivregress 2sls lwage (educ= fatheduc motheduc) exper expersq, small\npredict resid, res\nestat overid \nreg resid fatheduc motheduc exper expersq, notable robust\n\n\n  Tests of overidentifying restrictions:\n\n  Sargan (score) chi2(1) =  .378071  (p = 0.5386)\n  Basmann chi2(1)        =  .373985  (p = 0.5408)\n\nLinear regression                               Number of obs     =        428\n                                                F(4, 423)         =       0.11\n                                                Prob &gt; F          =     0.9791\n                                                R-squared         =     0.0009\n                                                Root MSE          =     .67521"
  },
  {
    "objectID": "rmethods/6_Heteros.html#what-is-heteroskedasticity",
    "href": "rmethods/6_Heteros.html#what-is-heteroskedasticity",
    "title": "Multiple Regression Analysis",
    "section": "What is Heteroskedasticity?",
    "text": "What is Heteroskedasticity?\n\nMathematically: \\[Var(e|x=c_1)\\neq Var(e|x=c_2)\n\\]\nThis means: the conditional variance of the errors is not constant across control characteristics."
  },
  {
    "objectID": "rmethods/6_Heteros.html#consequences",
    "href": "rmethods/6_Heteros.html#consequences",
    "title": "Multiple Regression Analysis",
    "section": "Consequences",
    "text": "Consequences\nWhat happens when you have heteroskedastic errors?\n\nIn terms of \\(\\beta's\\) and \\(R^2\\) and \\(R^2_{adj}\\), nothing. Coefficients and Goodness of fit are still unbiased and consistent.\nBut, Coefficients standard errors are based on the simplifying assumption of normality. Thus Variances will be bias!.\n\nIf variances are biased, then all statistics will be wrong."
  },
  {
    "objectID": "rmethods/6_Heteros.html#section",
    "href": "rmethods/6_Heteros.html#section",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "How bad can it be?\nSetup:\n\\(y = e\\) where \\(e~N(0,\\sigma_e^2h(x))\\)\n\\(x = uniform(-1,1)\\)\n\n\nCode\n/*capture program drop sim_het\nprogram sim_het, eclass\n    clear\n    set obs 500 \n    gen x = runiform(-1,1)\n    gen u = rnormal()\n    ** Homoskedastic\n    gen y_1 = u*2\n    ** increasing first, decreasing later\n    gen y_4 = u*sqrt(9*abs(x))\n    replace x = x-2\n    reg y_1 x\n    matrix b=_b[x],_b[x]/_se[x]\n    reg y_4 x\n    matrix b=b,_b[x],_b[x]/_se[x]\n    matrix coleq   b = h0 h0 h3 h3 \n    matrix colname b = b  t  b  t \n    ereturn post b\nend\nqui:simulate , reps(1000) dots(100):sim_het\nsave mdata/simulate.dta, replace*/\nuse mdata/simulate.dta, replace\ntwo (kdensity h0_b_t) (kdensity h3_b_t) ///\n    (function y = normalden(x), range(-4 4) lw(2) color(gs5%50)), ///\n    legend(order(3 \"Normal\" 1 \"With Homoskedasticty\" 2 \"with Heteroskedasticity\"))\ngraph export images/fig6_1.png, replace height(1000)"
  },
  {
    "objectID": "rmethods/6_Heteros.html#what-to-do-about-it-1",
    "href": "rmethods/6_Heteros.html#what-to-do-about-it-1",
    "title": "Multiple Regression Analysis",
    "section": "What to do about it?",
    "text": "What to do about it?\n\nSo, If errors are heteroskedastic, then all statistics (t-stats, F-stats, chi2’s) are wrong.\nBut, there are solutions…many solutions\n\nGLS: Generalized Least Squares\nWLS: Weighted Least Squares\nFGLS: Feasible Generealized Least Squares\nWFLS: Weighted FGLS\nHC0-HC3: Heteroskedasticity consistent SE\n\nSome of them are more involved than others.\nBut before trying to do that, lets first ask…do we have a problem?"
  },
  {
    "objectID": "rmethods/6_Heteros.html#detecting-the-problem",
    "href": "rmethods/6_Heteros.html#detecting-the-problem",
    "title": "Multiple Regression Analysis",
    "section": "Detecting the Problem",
    "text": "Detecting the Problem\n\nConsider the model:\n\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +\\beta_3 x_3 +e\n\\]\n\nWe usually start with the assumption that errors are homoskedastic \\(Var(e|x's)=\\sigma^2_c\\).\nHowever, now we want to allow for the possibility of heteroskedasiticity. ie, that variance is some function of X.\n\nWe have to model is variance is a function that varies with \\(x\\):\n\n\n\\[Var(e|x)=f(x_1,x_2,\\dots,x_k) \\sim a_0+a_1x_1 + a_2 x_2 + \\dots + a_k x_k+v\\]"
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-1",
    "href": "rmethods/6_Heteros.html#section-1",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "\\[Var(e|x)=f(x_1,x_2,\\dots,x_k) \\sim a_0+a_1x_1 + a_2 x_2 + \\dots + a_k x_k+v\\]\n\nThis expression says the onditional variance can vary with \\(X's\\).\nIt could be as flexible as needed, but linear is usually enough.\n\nWith this the Null hypothesis is: \\[H_0: a_1 = a_2 = \\dots = a_k=0 \\text{ vs } H_1: H_0 \\text{ is false}\n\\]\nEasy enough, but do we KNOW \\(Var(e|x)\\) ? can we model the equation?"
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-2",
    "href": "rmethods/6_Heteros.html#section-2",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "We don’t!.\n\nBut we can use \\(e^2\\) instead. This implies we using the assumption that \\(e^2\\) is a good enough approximation for the condional variance \\(Var(e|x)\\).\nWith this, the test for heteroskedasticty can be implemented using the following recipe.\n\n\nEstimate \\(y=x\\beta+e\\) and obtain predicted model errors \\(\\hat e\\).\nModel \\(\\hat e^2 = \\color{green}{h(x)}+v\\), as a proxy for the variance model.\n\n\\(h(x)\\) could be estimated using some linear or nonlinear functional forms.\n\nTest if variance changes with respect to any explanatory variables.\n\nIn this case, you can assume the heteroskedastic model is homoskedastic.\n\n\nNote: Depending on Model specification, and test used, there are various Heteroskedasticity tests."
  },
  {
    "objectID": "rmethods/6_Heteros.html#heteroskedasticity-tests",
    "href": "rmethods/6_Heteros.html#heteroskedasticity-tests",
    "title": "Multiple Regression Analysis",
    "section": "Heteroskedasticity tests",
    "text": "Heteroskedasticity tests\n\\[\\begin{aligned}\n\\text{Model}: y &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + e \\\\\n\\hat e & = y - (\\hat \\beta_0 + \\hat\\beta_1 x_1 +\\hat \\beta_2 x_2 +\\hat \\beta_3 x_3)\n\\end{aligned}\n\\]\nBreush-Pagan:\n\\[\\begin{aligned}\n\\hat e^2 & = \\gamma_0 + \\gamma_1 x_1 +\\gamma_2 x_2 +\\gamma_3 x_3 + v \\\\\nH_0 &: \\gamma_1=\\gamma_2=\\gamma_3=0 \\\\\nF &= \\frac{R^2_{\\hat e^2}/k}{(1-R^2_{\\hat e^2})/(n-k-1)} \\\\\nLM &=N R^2_{\\hat e^2} \\sim \\chi^2(k) \\leftarrow BP-test\n\\end{aligned}\n\\]\n\nEasy and simple, but only considers “linear” Heteroskedasticity"
  },
  {
    "objectID": "rmethods/6_Heteros.html#heteroskedasticity-tests-1",
    "href": "rmethods/6_Heteros.html#heteroskedasticity-tests-1",
    "title": "Multiple Regression Analysis",
    "section": "Heteroskedasticity tests:",
    "text": "Heteroskedasticity tests:\n\\[\\begin{aligned}\n\\text{Model}: y &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + e \\\\\n\\hat e & = y - (\\hat \\beta_0 + \\hat\\beta_1 x_1 +\\hat \\beta_2 x_2 +\\hat \\beta_3 x_3)\n\\end{aligned}\n\\]\nWhite:\n\\[\\begin{aligned}\n\\hat e^2 & = \\gamma_0 + \\sum \\gamma_{1,k} x_k + \\sum \\gamma_{2,k} x_k^2 + \\sum_k \\sum_{j\\neq k} \\gamma_{3,j,k} x_j x_k + v \\\\\nH_0 &: \\text{ All } \\gamma's =0 \\\\\nF &= \\frac{R^2_{\\hat e^2}/q}{(1-R^2_{\\hat e^2})/(n-q-1)} \\\\\nLM &=N R^2_{\\hat e^2} \\sim \\chi^2(q)\n\\end{aligned}\n\\]\n\\(q\\) is the total number of coefficients in the model (not counting the intercept.)\n\nAccounts for nonlinearities, but gets “messy” with more variables."
  },
  {
    "objectID": "rmethods/6_Heteros.html#heteroskedasticity-tests-2",
    "href": "rmethods/6_Heteros.html#heteroskedasticity-tests-2",
    "title": "Multiple Regression Analysis",
    "section": "Heteroskedasticity tests:",
    "text": "Heteroskedasticity tests:\n\\[\\begin{aligned}\n\\text{Model}: y &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + e \\\\\n\\hat e & = y - (\\hat \\beta_0 + \\hat\\beta_1 x_1 +\\hat \\beta_2 x_2 +\\hat \\beta_3 x_3)\n\\end{aligned}\n\\]\nModified White:\n\\[\\begin{aligned}\n\\hat y &= y - \\hat e \\\\\n\\hat e^2 & = \\gamma_0 + \\gamma_1 \\hat y + \\gamma_2 \\hat y^2 + \\dots + v \\\\\nH_0 &: \\gamma_1 = \\gamma_2 = \\dots =0 \\\\\nF &= \\frac{R^2_{\\hat e^2}/ h }{(1-R^2_{\\hat e^2})/(n-h-1)} \\\\\nLM &=N R^2_{\\hat e^2} \\sim \\chi^2(h)\n\\end{aligned}\n\\]\n\\(h\\) is the total number of coefficients in the model (not counting the intercept.)\n\nAccounts for nonlinearities (because of how \\(\\hat y\\) is constructed), and is simpler to implement.\nBut, nonlinearity is restricted."
  },
  {
    "objectID": "rmethods/6_Heteros.html#example",
    "href": "rmethods/6_Heteros.html#example",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\nHousing prices:\n\\[\\begin{aligned}\nprice &= \\beta_0 + \\beta_1 lotsize + \\beta_2 sqft + \\beta_3 bdrms + e_1 \\\\\nlog(price) &= \\beta_0 + \\beta_1 log(lotsize) + \\beta_2 log(sqft) + \\beta_3 bdrms + e_2 \\\\\n\\end{aligned}\n\\]\n\nfrause hprice1, clear\nreg price lotsize sqrft bdrms \npredict res, res\npredict price_hat\ngen res2=res^2\ndisplay \"BP-test\"\nreg res2  lotsize sqrft bdrms, notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\ndisplay \"White Test\"\nreg res2  c.(lotsize sqrft bdrms)##c.(lotsize sqrft bdrms), notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\ndisplay \"MWhite Test\"\nreg res2  price_hat c.price_hat#c.price_hat, notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(3, 84)        =     57.46\n       Model |  617130.701         3  205710.234   Prob &gt; F        =    0.0000\n    Residual |  300723.805        84   3580.0453   R-squared       =    0.6724\n-------------+----------------------------------   Adj R-squared   =    0.6607\n       Total |  917854.506        87  10550.0518   Root MSE        =    59.833\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     lotsize |   .0020677   .0006421     3.22   0.002     .0007908    .0033446\n       sqrft |   .1227782   .0132374     9.28   0.000     .0964541    .1491022\n       bdrms |   13.85252   9.010145     1.54   0.128    -4.065141    31.77018\n       _cons |  -21.77031   29.47504    -0.74   0.462    -80.38466    36.84405\n------------------------------------------------------------------------------\n(option xb assumed; fitted values)\nBP-test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(3, 84)        =      5.34\n       Model |   701213780         3   233737927   Prob &gt; F        =    0.0020\n    Residual |  3.6775e+09        84  43780003.5   R-squared       =    0.1601\n-------------+----------------------------------   Adj R-squared   =    0.1301\n       Total |  4.3787e+09        87  50330276.7   Root MSE        =    6616.6\n\nnR^2: 14.092386\np(chi2) 0.003\nWhite Test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(9, 78)        =      5.39\n       Model |  1.6784e+09         9   186492378   Prob &gt; F        =    0.0000\n    Residual |  2.7003e+09        78    34619265   R-squared       =    0.3833\n-------------+----------------------------------   Adj R-squared   =    0.3122\n       Total |  4.3787e+09        87  50330276.7   Root MSE        =    5883.8\n\nnR^2: 33.731659\np(chi2) 0.000\nMWhite Test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(2, 85)        =      9.64\n       Model |   809489395         2   404744697   Prob &gt; F        =    0.0002\n    Residual |  3.5692e+09        85  41991113.9   R-squared       =    0.1849\n-------------+----------------------------------   Adj R-squared   =    0.1657\n       Total |  4.3787e+09        87  50330276.7   Root MSE        =    6480.1\n\nnR^2: 16.268416\np(chi2) 0.000\n\n\n\nfrause hprice1, clear\nreg lprice llotsize lsqrft bdrms \npredict res, res\npredict price_hat\ngen res2=res^2\ndisplay \"BP-test\"\nreg res2  llotsize lsqrft bdrms, notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\ndisplay \"White Test\"\nreg res2  c.(llotsize lsqrft bdrms)##c.(llotsize lsqrft bdrms), notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\ndisplay \"MWhite Test\"\nreg res2  price_hat c.price_hat#c.price_hat, notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(3, 84)        =     50.42\n       Model |  5.15504028         3  1.71834676   Prob &gt; F        =    0.0000\n    Residual |  2.86256324        84  .034078134   R-squared       =    0.6430\n-------------+----------------------------------   Adj R-squared   =    0.6302\n       Total |  8.01760352        87  .092156362   Root MSE        =     .1846\n\n------------------------------------------------------------------------------\n      lprice | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    llotsize |   .1679667   .0382812     4.39   0.000     .0918404     .244093\n      lsqrft |   .7002324   .0928652     7.54   0.000     .5155597    .8849051\n       bdrms |   .0369584   .0275313     1.34   0.183    -.0177906    .0917074\n       _cons |  -1.297042   .6512836    -1.99   0.050    -2.592191    -.001893\n------------------------------------------------------------------------------\n(option xb assumed; fitted values)\nBP-test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(3, 84)        =      1.41\n       Model |  .022620168         3  .007540056   Prob &gt; F        =    0.2451\n    Residual |  .448717194        84  .005341871   R-squared       =    0.0480\n-------------+----------------------------------   Adj R-squared   =    0.0140\n       Total |  .471337362        87  .005417671   Root MSE        =    .07309\n\nnR^2: 4.2232484\np(chi2) 0.238\nWhite Test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(9, 78)        =      1.05\n       Model |  .051147864         9  .005683096   Prob &gt; F        =    0.4053\n    Residual |  .420189497        78  .005387045   R-squared       =    0.1085\n-------------+----------------------------------   Adj R-squared   =    0.0057\n       Total |  .471337362        87  .005417671   Root MSE        =     .0734\n\nnR^2: 9.5494489\np(chi2) 0.388\nMWhite Test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(2, 85)        =      1.73\n       Model |  .018464046         2  .009232023   Prob &gt; F        =    0.1830\n    Residual |  .452873315        85  .005327921   R-squared       =    0.0392\n-------------+----------------------------------   Adj R-squared   =    0.0166\n       Total |  .471337362        87  .005417671   Root MSE        =    .07299\n\nnR^2: 3.4472889\np(chi2) 0.178\n\n\nCan you do this in Stata? Yes, estat hettest. But look into the options. There are many more options in that command."
  },
  {
    "objectID": "rmethods/6_Heteros.html#what-do-you-do-when-you-have-heteroskedasticity",
    "href": "rmethods/6_Heteros.html#what-do-you-do-when-you-have-heteroskedasticity",
    "title": "Multiple Regression Analysis",
    "section": "What do you do when you have Heteroskedasticity?",
    "text": "What do you do when you have Heteroskedasticity?\nWe need to fix!\n\nRecall, the problem is that \\(Var(e|X)\\neq c\\)\nThis affects how standard errors are estimated (we required homoskedasticity). But what happens when Homoskedasticity doesnt hold?\n\nWe can “fix/change” the model, so its no longer heteroskedastic, and Standard Inference works. (FGLS, WLS)\nWe neec to account for heteroskedasticity when estimating the variance covariance model.\n\n\nSo lets learn to Fix it first"
  },
  {
    "objectID": "rmethods/6_Heteros.html#how-do-we-fix-heteroskedasticity",
    "href": "rmethods/6_Heteros.html#how-do-we-fix-heteroskedasticity",
    "title": "Multiple Regression Analysis",
    "section": "How do we Fix Heteroskedasticity?",
    "text": "How do we Fix Heteroskedasticity?\n\nIn order to address the problem of heteroskedasticity, we require knowledge of why the model is heteroskedastic, or what is generating it.\n\n\\[Var(e|X)=h(x)\\sigma^2_e\n\\]\n\nWhere \\(h(x)\\) is the “source” of heteroskedasticity, which may be a known or estimated function of \\(x\\).\n\nWhich should be an strictly possitive function of \\(x's\\)."
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-4",
    "href": "rmethods/6_Heteros.html#section-4",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Knowledge is power\n\nIf you know \\(h(x)\\), correcting heteroskedasticity is “easy”. Consider the following:\n\n\\[\\begin{aligned}\ny &= b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3 +e \\\\\nVar(e|x) &=x_1 \\sigma_e^2 || h(x)=x_1\n\\end{aligned}\n\\]\nYou can correct Heteroskedasticity in two ways:\n\nTransform model by dividing everything by \\(\\sqrt{h(x)}\\): \\[\\begin{aligned}\n\\frac{y}{\\sqrt{x_1}} &= b_0 \\frac{1}{\\sqrt{x_1}}+ b_1 \\sqrt{x_1} + b_2 \\frac{x_2}{\\sqrt{x_1}} + b_3 \\frac{x_3}{\\sqrt{x_1}} +\\frac{e}{\\sqrt{x_1}} \\\\\nVar\\left(\\frac{e}{\\sqrt{x_1}}|x\\right) &= \\frac{1}{x_1} x_1\\sigma_e^2=\\sigma_e^2\n\\end{aligned}\n\\]\n\nThe new error is Homoskedastic (but has no constant)!"
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-5",
    "href": "rmethods/6_Heteros.html#section-5",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Estimate the model using by \\(\\frac{1}{h(x)}\\) as weights: \\[\\begin{aligned}\n\\beta=\\min_\\beta \\sum \\frac{1}{h(x)} (y-(b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3))^2\n\\end{aligned}\n\\]\n\n\nSame solution as before, and there is no need to “transform” data, or keep track of a constant.\nThis is often called WLS (weighted least squares) or GLS (Generalized Least Squares)."
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-6",
    "href": "rmethods/6_Heteros.html#section-6",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Interestingly: These approaches are more efficient than Standard OLS.\n\nUses more information (heteroskedasticity)\nMakes better use of information (More weight to better data) Standard errors are smaller.\n\nt-stats, F-stats, etc now are valid.\nCoefficients will NOT be the same as before.\n\\(R^2\\) is less useful\nHeteroskedasticty test on transformed data may required added work."
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-7",
    "href": "rmethods/6_Heteros.html#section-7",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "FGLS: We do not know \\(h(x)\\), but we can guess\n\nIf \\(h(x)\\) is not known, we can use an auxiliary model to estimate it:\n\n\\[\\begin{aligned}\nVar(e|x) &= \\sigma^2 \\exp(\\delta_0 + \\delta_1 x_1 + \\delta_2 x_2 +\\dots) \\exp v \\\\\nlog(\\hat e^2) &= \\delta_0 + \\delta_1 x_1 + \\delta_2 x_2 +\\dots+ v \\\\\nlog(\\hat e^2) &= \\delta_0 + \\delta_1 \\hat y + \\delta_2 \\hat y^2 + \\dots+ v \\\\\nh(x) &= \\widehat{log(\\hat e^2)}\n\\end{aligned}\n\\]\n\nProceed as before (weighted or transformed)\nIts call Feasible GLS, because we need to estimate \\(h(x)\\)."
  },
  {
    "objectID": "rmethods/6_Heteros.html#do-not-correct-account-for-it-gls-and-fgls",
    "href": "rmethods/6_Heteros.html#do-not-correct-account-for-it-gls-and-fgls",
    "title": "Multiple Regression Analysis",
    "section": "Do not Correct, account for it: GLS and FGLS",
    "text": "Do not Correct, account for it: GLS and FGLS\nRecall “Long” variance formula:\n\\[Var(\\beta)=\\color{brown}{(X'X)^{-1}}\\color{green}{X}'\\color{red}{Var(e|X)}\\color{green}{X}\\color{brown}{(X'X)^{-1}}\n\\]\n\nThe red part is a \\(N\\times N\\) VCOV matrix of ALL erros. It can be Simplified with what we know!\n\n\\[\\begin{aligned}\nVar_{gls/fgls}(\\beta)&=\\sigma^2_{\\tilde e} \\color{brown}{(X'X)^{-1}}\\color{green}{X}'\\color{red}{ \\Omega_h(x) }\\color{green}{X}\\color{brown}{(X'X)^{-1}} \\\\\n\\sigma^2_{\\tilde e} &= \\frac{1}{N-k-1} \\sum \\frac{\\hat e^2}{h(x)} \\\\\n\\Omega_h(x) [i,j] &= h(x_i) & \\text{ if } i=j \\\\\n& = 0 & \\text{ if } i\\neq j \\\\\n\\end{aligned}\n\\]\n\nSE are corrected, but coefficients remain the same!"
  },
  {
    "objectID": "rmethods/6_Heteros.html#do-not-correct-account-for-it-white-sandwich-formula",
    "href": "rmethods/6_Heteros.html#do-not-correct-account-for-it-white-sandwich-formula",
    "title": "Multiple Regression Analysis",
    "section": "Do not Correct, account for it: White Sandwich Formula",
    "text": "Do not Correct, account for it: White Sandwich Formula\n\nWhat if we do not want to even try guessing \\(h(x)\\)?\nyou can use Robust Standard errors!\n\nHeteroskedastic Consistent SE to Heterosedasticity of unknown form.\n\n\nLet me present to you, the Sandwitch Formula: \\[Var(\\beta)=c \\color{brown}{(X'X)^{-1}}\\color{green}{X}'\\color{red}{\\Omega}\\color{green}{X}\\color{brown}{(X'X)^{-1}}\n\\]\n\\[\\begin{aligned}\n\\Omega [i,j] &= \\hat e_i^2 & \\text{ if } i=j \\\\\n& = 0 & \\text{ if } i\\neq j \\\\\n\\end{aligned}\n\\]\n\nThe best approximation to conditional variance is equal to \\(\\hat e_i^2\\). (plus assuming no correlation)\nValid in large samples, but can be really bad in smaller ones.\nThere are other versions. See HC0 HC1 HC2 HC3."
  },
  {
    "objectID": "rmethods/6_Heteros.html#what-if-did-hx-and-it-was-wrong",
    "href": "rmethods/6_Heteros.html#what-if-did-hx-and-it-was-wrong",
    "title": "Multiple Regression Analysis",
    "section": "What if did \\(h(x)\\), and it was wrong",
    "text": "What if did \\(h(x)\\), and it was wrong\n\nUsing FGLS will change coefficients a bit. If they change a lot, It could indicate other assumptions in the model are incorrect. (functional form or exogeneity)\nIn either case, you could always combine FGLS with Robust Standard Errors!"
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-8",
    "href": "rmethods/6_Heteros.html#section-8",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Extra: Prediction and SE\nPrediction SE:\n\nIf you are using GLS, Formulas seen before apply with the following modification: \\(Var(e|X=x_0)=\\sigma^2_{\\tilde e} h(x_0)\\)\nIf you are using FGLS, its not that simple because of the two-step modeling\n\nFor Prediction with Logs\n\nYou need to take into account Heteroskedasticity\n\n\\[\\hat y_i = \\exp \\left( \\widehat{log y_i}+\\hat \\sigma_{\\tilde e}^2 \\hat h_i /2 \\right)\n\\]"
  },
  {
    "objectID": "rmethods/6_Heteros.html#example-1",
    "href": "rmethods/6_Heteros.html#example-1",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\n\nfrause smoke, clear\ngen age_40sq=(age-40)^2\n** Default\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn\nest sto m1\npredict cig_hat\npredict cig_res,res\n** GLS: h(x)=lincome Weighted\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn [aw=lincome]\nest sto m2\n** FGLS: h(x) = f(cigs_hat)\ngen lcres=log(cig_res^2)\nqui:reg lcres c.cig_hat##c.cig_hat##c.cig_hat \npredict aux\ngen hx=exp(aux)\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn [aw=hx]\nest sto m3\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn , robust\nest sto m4\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn [aw=lincome], robust\nest sto m5\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn [aw=hx], robust\nest sto m6\nset linesize 255\n\n\n\nCode\nesttab m1 m2 m3 m4 m5 m6, gaps mtitle(default GLS FGLS Rob GLS-Rob FGLS-Rob) ///\nnonum cell( b( fmt( 3) star ) se( par(( )) ) p( par([ ]) ) ) ///\nstar(* .1 ** 0.05 *** 0.01  )\n\n\n\n------------------------------------------------------------------------------------------------------------\n                  default             GLS            FGLS             Rob         GLS-Rob        FGLS-Rob   \n                   b/se/p          b/se/p          b/se/p          b/se/p          b/se/p          b/se/p   \n------------------------------------------------------------------------------------------------------------\nlincome             0.880           0.851           1.207           0.880           0.851           1.207   \n                  (0.728)         (0.783)         (0.937)         (0.596)         (0.637)         (0.897)   \n                  [0.227]         [0.277]         [0.198]         [0.140]         [0.182]         [0.179]   \n\nlcigpric           -0.751          -0.045          -0.947          -0.751          -0.045          -0.947   \n                  (5.773)         (5.840)         (6.095)         (6.035)         (6.051)         (7.412)   \n                  [0.897]         [0.994]         [0.877]         [0.901]         [0.994]         [0.898]   \n\neduc               -0.501***       -0.526***       -0.465**        -0.501***       -0.526***       -0.465** \n                  (0.167)         (0.168)         (0.200)         (0.162)         (0.166)         (0.223)   \n                  [0.003]         [0.002]         [0.021]         [0.002]         [0.002]         [0.037]   \n\nage                 0.049           0.049           0.033           0.049*          0.049*          0.033   \n                  (0.034)         (0.034)         (0.041)         (0.030)         (0.030)         (0.040)   \n                  [0.146]         [0.144]         [0.432]         [0.099]         [0.097]         [0.413]   \n\nage_40sq           -0.009***       -0.009***       -0.010***       -0.009***       -0.009***       -0.010***\n                  (0.002)         (0.002)         (0.003)         (0.001)         (0.001)         (0.002)   \n                  [0.000]         [0.000]         [0.001]         [0.000]         [0.000]         [0.000]   \n\nrestaurn           -2.825**        -2.878**        -2.417*         -2.825***       -2.878***       -2.417*  \n                  (1.112)         (1.115)         (1.451)         (1.008)         (1.023)         (1.385)   \n                  [0.011]         [0.010]         [0.096]         [0.005]         [0.005]         [0.081]   \n\n_cons              10.797           8.521           8.866          10.797           8.521           8.866   \n                 (24.145)        (24.604)        (26.340)        (25.401)        (25.536)        (32.622)   \n                  [0.655]         [0.729]         [0.736]         [0.671]         [0.739]         [0.786]   \n------------------------------------------------------------------------------------------------------------\nN                     807             807             807             807             807             807   \n------------------------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/6_Heteros.html#lpm-revised",
    "href": "rmethods/6_Heteros.html#lpm-revised",
    "title": "Multiple Regression Analysis",
    "section": "LPM revised",
    "text": "LPM revised\n\nWhat was wrong with LPM?\n\nFixed marginal effects (depends on functional form)\nMay predict p&gt;1 or p&lt;0\nIt is Heteroskedastic by construction\n\nBut now we know how to deal with this! GLS (why not FGLS) and Robust\nIn LPM: \\(Var(y|x)=p(x)(1-p(x)) = \\hat y (1-\\hat y)\\)\n\nWe can use this to transform or weight the data!\nOnly works if \\(0&lt;p(x)&lt;1\\)."
  },
  {
    "objectID": "rmethods/6_Heteros.html#lpm-example",
    "href": "rmethods/6_Heteros.html#lpm-example",
    "title": "Multiple Regression Analysis",
    "section": "LPM Example",
    "text": "LPM Example\n\nfrause gpa1, clear\n** LPM\ngen parcoll = (fathcoll | mothcoll)\nreg pc hsgpa act parcoll\npredict res_1, res\n\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =      1.98\n       Model |  1.40186813         3  .467289377   Prob &gt; F        =    0.1201\n    Residual |  32.3569971       137  .236182461   R-squared       =    0.0415\n-------------+----------------------------------   Adj R-squared   =    0.0205\n       Total |  33.7588652       140  .241134752   Root MSE        =    .48599\n\n------------------------------------------------------------------------------\n          pc | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .0653943   .1372576     0.48   0.635    -.2060231    .3368118\n         act |   .0005645   .0154967     0.04   0.971    -.0300792    .0312082\n     parcoll |   .2210541    .092957     2.38   0.019      .037238    .4048702\n       _cons |  -.0004322   .4905358    -0.00   0.999     -.970433    .9695686\n------------------------------------------------------------------------------\n\n\n\npredict pchat\ngen hx = pchat*(1-pchat)\nsum pchat hx\n\n(option xb assumed; fitted values)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       pchat |        141    .3971631    .1000667   .1700624   .4974409\n          hx |        141    .2294822    .0309768   .1411412   .2499934\n\n\n\nreg pc hsgpa act parcoll [w=1/hx]\npredict res_2, res\n\n(analytic weights assumed)\n(sum of wgt is 628.1830743667746)\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =      2.22\n       Model |  1.54663033         3  .515543445   Prob &gt; F        =    0.0882\n    Residual |  31.7573194       137  .231805251   R-squared       =    0.0464\n-------------+----------------------------------   Adj R-squared   =    0.0256\n       Total |  33.3039497       140  .237885355   Root MSE        =    .48146\n\n------------------------------------------------------------------------------\n          pc | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .0327029   .1298817     0.25   0.802    -.2241292     .289535\n         act |    .004272   .0154527     0.28   0.783    -.0262847    .0348286\n     parcoll |   .2151862   .0862918     2.49   0.014       .04455    .3858224\n       _cons |   .0262099   .4766498     0.05   0.956    -.9163323    .9687521\n------------------------------------------------------------------------------\n\n\n\n** Testing for Heteroskedasticity\nreplace res_1 = res_1^2\nreplace res_2 = res_2^2/hx\ndisplay \"Default\"\nreg res_1 hsgpa act parcoll, notable\ndisplay \"Weighted\"\nreg res_2 hsgpa act parcoll, notable\n\n(141 real changes made)\n(141 real changes made)\nDefault\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =      2.82\n       Model |  .133163365         3  .044387788   Prob &gt; F        =    0.0412\n    Residual |  2.15497574       137   .01572975   R-squared       =    0.0582\n-------------+----------------------------------   Adj R-squared   =    0.0376\n       Total |  2.28813911       140  .016343851   Root MSE        =    .12542\n\nWeighted\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =      0.63\n       Model |  .874194807         3  .291398269   Prob &gt; F        =    0.5980\n    Residual |  63.5472068       137  .463848225   R-squared       =    0.0136\n-------------+----------------------------------   Adj R-squared   =   -0.0080\n       Total |  64.4214016       140  .460152868   Root MSE        =    .68106"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#how-do-you-know-if-what-you-see-is-relevant",
    "href": "rmethods/4_MLRM_IA.html#how-do-you-know-if-what-you-see-is-relevant",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "How do you know if what you see is relevant?",
    "text": "How do you know if what you see is relevant?\n\nLast time, we talk a bit about the estimation of MLRM. For those who do not remember:\n\n\\[\\hat\\beta=(X'X)^{-1}X'y\n\\]\n\nWe also defined how, under A5 (homoskedasticity), we can estimate the variance covariance of coefficients:\n\n\\[Var(\\beta) = \\frac{\\sum \\hat e^2}{N-K-1} (X'X)^{-1}\n\\]\n\nThe next question: how to know how precise your estimates are?\nThat should be simple, just divide coefficient by its Standard error. The larger this is, the more precise, and more significant.\nIs this enough to say something about the population coefficients?\n\n(lets assume A1-A5 holds)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#distribution-of-coefficients",
    "href": "rmethods/4_MLRM_IA.html#distribution-of-coefficients",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Distribution of coefficients",
    "text": "Distribution of coefficients\n\nThe right answer is…Perhaps.\nUnless you know something about the distribution of \\(\\beta's\\), it would be hard to make any inferences from the estimates. Why?\nBecause not all distributions are made equal!\n\n\n\nCode\nclear\nrange x -4 4 1000\ngen funiform = 0 \nreplace funiform = 1/(2*sqrt(3)) if inrange(x,-sqrt(3),sqrt(3))\n\ngen fnormal = 0 \nreplace fnormal = normalden(x)\n\ngen fchi2 = 0 \nreplace fchi2 = sqrt(8)*chi2den(4,x*sqrt(8)+4)\n\ninteg funiform x, gen(F1)\ninteg fnormal x, gen(F2)\ninteg fchi2 x, gen(F3)\n\nset scheme white2\ncolor_style egypt\nreplace x = x + 1.5\ntwo (area funiform x           , pstyle(p1) color(%20)) ///\n    (area funiform x if F1&lt;0.05, pstyle(p1) color(%80)) /// \n    (area funiform x if F1&gt;0.95, pstyle(p1) color(%80)) /// \n    (area fnormal  x           , pstyle(p2) color(%20)) ///  \n    (area fnormal  x if F2&lt;0.05, pstyle(p2) color(%80)) /// \n    (area fnormal  x if F2&gt;0.95, pstyle(p2) color(%80)) /// \n    (area fchi2    x           , pstyle(p3) color(%20)) /// \n    (area fchi2    x if F3&gt;0.95, pstyle(p3) color(%80)) /// \n    (area fchi2    x if F3&lt;0.05, pstyle(p3) color(%80)), ///\n    xlabel(-4 / 4) legend(order(2 \"Uniform\" 5 \"Normal\" 8 \"C-Chi2\")) /// \n    xtitle(\"Beta hat Distribution\") ///\n    xline( 0, lstyle(1) lwidth(1)) xline(1.5)\n\ngraph export images/f4_1.png, replace width(1200)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#not-all-distributions-are-the-same",
    "href": "rmethods/4_MLRM_IA.html#not-all-distributions-are-the-same",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Not all Distributions are the Same",
    "text": "Not all Distributions are the Same"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#new-assumption",
    "href": "rmethods/4_MLRM_IA.html#new-assumption",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "New Assumption",
    "text": "New Assumption\n\nA6: Errors are normal \\(e\\sim N(0,\\sigma^2_e)\\).\n\nA1-A6 are the Classical Linear Model Assumption\nThis assumes the outcome is “conditionally” normal. \\(y|X \\sim N(X\\beta,\\sigma^2_e)\\)\nAnd with this assumption OLS is no longer blue. Its now BUE!"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-1",
    "href": "rmethods/4_MLRM_IA.html#section-1",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Why does it matter?\n\nIf you combine two variables with the same distributions, the combined variable will not have the same distributions as the “parents”\nExcept with normals! if you add two -normal- distributions together. The outcome will also be normal. (Dont believe me try it)\n\nRecall:\n\\[\\hat \\beta=\\beta + (X'X)^{-1}X'e\n\\]\nIf \\(e\\) is normal, then \\(\\beta's\\) will also be normal\nAnd this works for ANY Sample size!"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-2",
    "href": "rmethods/4_MLRM_IA.html#section-2",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "If \\(e\\) normal then \\(\\beta\\) is normal\n\nIf \\(\\hat \\beta's\\) are normal, then we can use this distribution to make inferences about \\(\\beta's\\) using normal distribution.\nThis is good, because we know how to do math with Normal distributions. And can used the modified Ratio:\n\n\\[z_j = \\frac{\\hat \\beta_j - \\beta_j}{sd(\\hat\\beta)}\\sim N(0,1)\n\\]\n\nWhere \\(\\beta_j\\) is what you think the True Population parameter is (your hypothesis), and \\(\\hat\\beta_j\\) is what you estimate in your data.\nDepending on the size of this, you can either reject your hypothesis, or not Reject it.\n\nbut do we “know” \\(sd(\\beta)\\)?"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-3",
    "href": "rmethods/4_MLRM_IA.html#section-3",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Do we “know” \\(sd(\\beta)\\)?\nWe don’t, which is why we can use a normal directly. Instead we use a t-distribution, which uses \\(se(\\hat\\beta )\\)\n\\[t_j = \\frac{\\hat \\beta_j - \\beta_j}{se(\\hat\\beta)}\\sim t_{N-k-1}\n\\]\nThen\n\nIf \\(e\\) is normal, \\(\\beta\\) will be normal.\nWhen Samples are “small” Standardized \\(\\beta\\) will follow a t-distribution\nBut, as \\(N\\rightarrow \\infty\\), \\(t_{N-k-1}\\sim N(0,1)\\)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#testing-hypothesis",
    "href": "rmethods/4_MLRM_IA.html#testing-hypothesis",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Testing Hypothesis",
    "text": "Testing Hypothesis\n\nThe idea of hypothesis testing is contrasting the “evidence” from your data (estimates) with the beliefs we have about the population.\n\n\\[y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + e\n\\]\nSay I have two hypothesis.\n\n\\(x_2\\) has no effect on \\(y\\). ie \\(H_0: \\beta_2 = 0\\)\n\\(x_1\\) has an effect equal to 1. ie \\(H_0: \\beta_1 = 1\\)\n\nNotice we make hypothesis about the population coefficients not the estimates\n\n\nI can “test” each hypothesis separately using a “t-statistic”\n\\[ \\color{green}{t_2=\\frac{\\hat \\beta_2 - 0}{se(\\hat \\beta_2)}} ;\nt_1=\\frac{\\hat \\beta_1 - 1}{se(\\hat \\beta_1)}\n\\]"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#types-of-hypothesis",
    "href": "rmethods/4_MLRM_IA.html#types-of-hypothesis",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Types of Hypothesis:",
    "text": "Types of Hypothesis:\nWhen talking about hypothesis testing there are two types:\n\nOne sided: when your alternative hypothesis compares your null to something either strictly larger, or strictly smaller than your hypothesis.\n\nEducation has no effect on wages vs Returns to education are positive.\nSkipping class has no effect on grades vs Skiping class reduces grades.\n\nTwo sided: When your alternative hypothesis is to say, “its different than”\n\nReturns to education is 10%, vs is not 10%\nSkipping class reduces grades in 0.5 points, vs not 0.5points\n\n\nIn both cases, you use the same t-statistic.\n\\[t_\\beta=\\frac{\\hat\\beta - \\beta_{hyp}}{se(\\hat \\beta)} \\sim t_{N-k-1}\n\\]"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-4",
    "href": "rmethods/4_MLRM_IA.html#section-4",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "What changes are the “thresholds” to Judge something significant or not.\nOne sided test:\n\\[\\begin{aligned}\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k&gt;\\beta^{hyp}_k \\\\\n& t_{\\beta_k}&gt;t_{N-k-1}(1-\\alpha) \\\\\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k&lt;\\beta^{hyp}_k \\\\\n& t_{\\beta_k}&lt;-t_{N-k-1}(1-\\alpha)\n\\end{aligned}\n\\]\n\nWhere \\(\\alpha\\) is your level of significance, and \\(t_{N-k-1}(1-\\alpha)\\) is the critical value.\n\\(\\alpha\\) determines the “risk” of commiting an error type I: Rejecting the Null when its true.\nIntuitively, the smaller \\(\\alpha\\) is, the more possitive (negative) “t” needs to be reject the Null."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-5",
    "href": "rmethods/4_MLRM_IA.html#section-5",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Two sided test:\n\\[\\begin{aligned}\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k \\neq \\beta^{hyp}_k \\\\\n& | t_{\\beta_k} | &gt;t_{N-k-1}(1-\\alpha/2)\n\\end{aligned}\n\\]\n\nSimilar to before, except the one needs to consider both tails of the distribution to determine critical values (see \\(t_{N-k-1}(1-\\alpha/2)\\))\nIntuitively, the smaller \\(\\alpha\\) is, the larger the absolute value of “t” needs to be reject the Null.\n\nBut, what is an error type I? and why we don’t we “accept” \\(H_0\\) s?"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#why-we-never-accept",
    "href": "rmethods/4_MLRM_IA.html#why-we-never-accept",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Why we never accept?:",
    "text": "Why we never accept?:\n\nAs stated few times before, \\(\\hat \\beta\\) are just approximations to the true \\(\\beta\\) coefficients. Its the “evidence” you have based on the data available.\nWith this evidence, you can reject some hypothesis. (Some more strongly than others)\nHowever, there could exists many scenarios that would fit the evidence."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-7",
    "href": "rmethods/4_MLRM_IA.html#section-7",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Code\nclear\nrange x -5 5 1000\ngen fx = normalden(x) \nset scheme white2\ncolor_style tableau\ngen xx = x+1\ntwo (area fx x , pstyle(p1) color(%20)) ///\n    (area fx x if x&lt;invnormal(.025), pstyle(p1) color(%80) ) ///\n    (area fx x if x&gt;invnormal(.975), pstyle(p1) color(%80) ) ///\n    (area fx xx , pstyle(p2) color(%20)) ///\n    (area fx xx if x&lt;invnormal(.025), pstyle(p2) color(%80) ) /// \n    (area fx xx if x&gt;invnormal(.975), pstyle(p2) color(%80) ) ///\n    , xline(1.8) legend(order(1 \"H0: b=0\" 4 \"H0: b=1\"))  ///\n    xlabel(-4(2)4)  ylabel(0(.1).5)  xsize(8) ysize(4)\ngraph export images/f4_2.png, height(1000)  replace"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#what-about-type-error-i-and-ii",
    "href": "rmethods/4_MLRM_IA.html#what-about-type-error-i-and-ii",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "What about Type error I and II?",
    "text": "What about Type error I and II?\n\n\nBecause we do not know the truth, we are bound to commit errors in our assessment of the data.\nSo given the data evidence and the hypothesis, there could be 2 scenarios:\n\nGOOD: You either reject when \\(H_0\\) is false, or not reject when \\(H_0\\) is true.\n\\(TE-I\\): You reject \\(H_0\\) when it is true,\n\\(TE-II\\): Not reject \\(H_0\\) when it is false (Something else was true)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-8",
    "href": "rmethods/4_MLRM_IA.html#section-8",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Code\nclear\nrange x -5 5 1000\ngen fx = normalden(x) \n\ngen xxx=x+3 \ntwo (area fx x , pstyle(p1) color(%20)) ///\n    (area fx xxx, pstyle(p2) color(%20)) ///\n    (area fx x if x&gt;2, pstyle(p1) color(%80)) ///\n    (area fx xxx if xxx &lt;2, pstyle(p2) color(%80))  ///\n    ,legend(order(1 \"H0\"3 \"Type I \" 2 \"H1\"  4 \"Type II \") cols(2))   ///\n    xlabel(-4(2)7)  ylabel(0(.1).5) xsize(8) ysize(4)  \ngraph export images/f4_3.png, height(1000)  replace"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#example-determinants-of-college-gpa",
    "href": "rmethods/4_MLRM_IA.html#example-determinants-of-college-gpa",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Example: Determinants of College GPA",
    "text": "Example: Determinants of College GPA\n\n\nCode\nfrause gpa1, clear\nreg colgpa hsgpa act skipped\n\n\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =     13.92\n       Model |  4.53313314         3  1.51104438   Prob &gt; F        =    0.0000\n    Residual |  14.8729663       137  .108561798   R-squared       =    0.2336\n-------------+----------------------------------   Adj R-squared   =    0.2168\n       Total |  19.4060994       140  .138614996   Root MSE        =    .32949\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .4118162   .0936742     4.40   0.000     .2265819    .5970505\n         act |   .0147202   .0105649     1.39   0.166    -.0061711    .0356115\n     skipped |  -.0831131   .0259985    -3.20   0.002    -.1345234   -.0317028\n       _cons |   1.389554   .3315535     4.19   0.000     .7339295    2.045178\n------------------------------------------------------------------------------\n\n\n\nHypothesis: Skipping classes has no effect on College GPA.\n\n\\[H_0: \\beta_{skip} = 0 \\text{ vs } H_1: \\beta_{skip} \\neq 0\n\\]\n\nTest, \\(a=95%\\), \\(|t_{skip}|=3.2\\) vs \\(t_{n-k-1}(0.975)\\):\n\n\nCode\ndisplay invt(141-4,0.975)\n\n1.9774312\n\nConclusion: \\(H_0\\) is rejected."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-9",
    "href": "rmethods/4_MLRM_IA.html#section-9",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Hyp: Skipping college has no effect on College GPA vs has a negative effect\n\n\\[H_0: \\beta_{skip} = 0 \\text{ vs } H_1: \\beta_{skip}&lt;0\n\\]\n\nTest, \\(a=95\\%\\), \\(|t_{skip}|=3.2\\) vs \\(t_{n-k-1}(0.95)=1.6560\\)\nAlso Reject \\(H_0\\)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-10",
    "href": "rmethods/4_MLRM_IA.html#section-10",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "\\(t_{ACT}=1.39\\)\nHyp: ACT has no effect on College GPA vs It has a non-zero effect\nHyp: ACT has no effect on College GPA vs it has a positive effect\nCritical:\n\n\\(t_{137}(0.95)=1.6560\\) Donot Reject\\(H_0\\) with \\(\\alpha = 5\\%\\)\n\\(t_{137}(0.90)=1.2878\\) Reject \\(H_0\\) with \\(\\alpha = 10\\%\\)\n\nEach GPA point in highschool translates into half a point in College GPA. vs Is less than .5\n\n\ntest hsgpa = 0.5\nlincom hsgpa - 0.5\n\n\n ( 1)  hsgpa = .5\n\n       F(  1,   137) =    0.89\n            Prob &gt; F =    0.3482\n\n ( 1)  hsgpa = .5\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |  -.0881838   .0936742    -0.94   0.348    -.2734181    .0970505\n------------------------------------------------------------------------------\n\n\n\nCritical at 5%: \\(t=-1.6560\\)\nCannot Reject \\(H_0\\)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#p-values",
    "href": "rmethods/4_MLRM_IA.html#p-values",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "p-values",
    "text": "p-values\n\nSomething you may or may not have noticed. The significance level \\(\\alpha\\) can be choosen by the researcher.\n\nConventional levels are 10%, 5% and 1%.\n\nThis may lead to researchers choosing any value that would make their theory fit.\n\nThere is a better alternative. Using \\(p-values\\) to capture the smallest significance level that you could use to reject your Null.\n\n\\[p-value = P(|t|&gt;|t-stat|) \\text{ or } p-value = 2*P(|t|&gt;|t-stat|)\n\\]\n\nThe smallest the better! (for rejection)\nHow?\n\nOne tail : display 1-t(df = n-k-1, |t-stat|)\ntwo tails: display 2-2*t(df = n-k-1, |t-stat|)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-11",
    "href": "rmethods/4_MLRM_IA.html#section-11",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Code\nclear\nrange x -5 5 1000\ngen fx = normalden(x) \nlocal p1:display %5.1f 100*2*(1-normal(1.5)) \nlocal p2:display %5.1f 100*2*(1-normal(2.5)) \ntwo (area fx x , pstyle(p1) color(%20)) ///\n    (area fx x if x&lt;-1.5, pstyle(p3) color(%80) ) ///\n    (area fx x if x&gt;+1.5, pstyle(p3) color(%80) ) ///\n    (area fx x if x&lt;invnormal(.025), pstyle(p1) color(%80) ) ///\n    (area fx x if x&gt;invnormal(.975), pstyle(p1) color(%80) ) ///\n    (area fx x if x&lt;-2.5, pstyle(p2) color(%80) ) ///\n    (area fx x if x&gt;+2.5, pstyle(p2) color(%80) ) , ///\n    xline(-1.5 2.5) legend(order(4 \"{&alpha}=5%\" 6 \"p-value = `p2'%\" 2 \"p-value = `p1'%\")) ///\n    ylabel(0(.1).5) xlabel(-5 0 5 -1.5 2.5)\ngraph export images/f4_4.png, replace width(1200)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#note-on-statistical-significance",
    "href": "rmethods/4_MLRM_IA.html#note-on-statistical-significance",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Note on Statistical Significance",
    "text": "Note on Statistical Significance\n\nStatistically significant doesnt mean meaninful. And lack of it, doesnt mean is not important\n\nKeep in mind that SE may be larger or smaller due to other factors (N or Mcollinearity)\n\nBe careful of discussing the effect size. (a 1US increase in min wage is different from 1chp in min Wage)\nIf non-significant, pay attention to the magnitude and relevance for your research. Does it have the correct sign?\nIncorrect signs with significant results. Either there is something wrong, or you found something interesting."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#confidence-intervals",
    "href": "rmethods/4_MLRM_IA.html#confidence-intervals",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nThis is the third approach to assess how precise or significant an estimate is. You provide a Range of possible values, given the level of coverage, and SE.\n\n\\[CI(\\beta_i) = [\\hat \\beta_i - t_{n-k-1}(1-\\alpha),\\hat \\beta_i + t_{n-k-1}(1-\\alpha)]\n\\]\n\nInterpretation:\n\nIf we were to draw M samples, the true beta would be in this interval \\(1-\\alpha\\%\\) of the time.\n\nIt allows you to see what other “hypothesis” would be consistent with the evidence of the estimate (you wouldnt be able to reject the Null)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-13",
    "href": "rmethods/4_MLRM_IA.html#section-13",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Code\nclear\nrange x -5 5 1000\ngen fx = normalden(x) \nlocal p1:display %5.1f 100*2*(1-normal(1.5)) \nlocal p2:display %5.1f 100*2*(1-normal(2.5)) \ngen xx = x+2\n\ntwo (area fx x , pstyle(p1) color(%10)) ///\n    (area fx x if x&lt;invnormal(.025), pstyle(p1) color(%60) ) ///\n    (area fx x if x&gt;invnormal(.975), pstyle(p1) color(%60) ) ///\n    (area fx xx , color(gs1%10) ) ///\n    (area fx xx if x&lt;invnormal(.005),  color(gs1%80) ) ///\n    (area fx xx if x&gt;invnormal(.995),  color(gs1%80) ) ///\n    (area fx xx if x&lt;invnormal(.025),  color(gs1%60) ) ///\n    (area fx xx if x&gt;invnormal(.975),  color(gs1%60) ) ///\n    (area fx xx if x&lt;invnormal(.05),  color(gs1%40) ) ///\n    (area fx xx if x&gt;invnormal(.95),  color(gs1%40) ), ///\n    xline(2) legend(order(5 \"CI-1%\" 7 \"CI-5%\" 9 \"CI-10%\")) ///\n    ylabel(0(.1).5) xlabel(-5 0 5 )  \ngraph export images/f4_5.png, replace width(1000)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#testing-linear-combinations",
    "href": "rmethods/4_MLRM_IA.html#testing-linear-combinations",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Testing Linear Combinations:",
    "text": "Testing Linear Combinations:\n\nYou may be interested in testing particular linear combinations of coefficients:\n\n\\(b_1 - b_2 =0 ; b_2+b_3=1 ; 2*b_4-b_5=b_6\\)\n\nDoing this is “simple”. Because is a single linear combination, you can still use “t-stat”.\n\n\\(t-stat = \\frac{2*\\hat b_4 -\\hat b_5 -\\hat b_6}{se(2*\\hat b_4 -\\hat b_5 -\\hat b_6)}\\)\n\nJust need SE for combined coefficients (requires knowing Variances and Covariances)\nEasy way, you could use Stata:\n\nreg y x1 x2 x3 x4 x5 x6\nlincom x1-x2 or lincom 2*x4-x5-x6\ntest (x1-x2=0) (x2+x3=1) (2*x4-x5=x6), mtest"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-14",
    "href": "rmethods/4_MLRM_IA.html#section-14",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Harder Way: (if you dare)\nMatrix Multiplication\nAssume Constant is the last coefficient: \\[V( 2*b_4-b_5- b_6) = R' V R ; R = [0,0,0,2,-1,-1]\n\\]\nwhere R are the restrictions, and V is the variance covariance matrix of \\(\\beta's\\).\nThen your t-stat\n\\[t-stat = \\frac{2*b_4-b_5- b_6}{\\sqrt{V(2*b_4-b_5- b_6)}}\n\\]"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-15",
    "href": "rmethods/4_MLRM_IA.html#section-15",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Alternative: Substitution\n\nOne can manipulate the regression model to consider a model with the contrained coefficient.\nOnce model is estimated, it simplifies testing:\n\n\\[\\begin{aligned}\n& y = b_0 + b_1 x_1 + b_2  x_2 + b_3 x_3 + e  \\\\\nh0: & b_1 - 2b_2 +b_3=0 \\rightarrow \\theta = b_1 - 2b_2 +b_3 \\rightarrow b_1 = \\theta + 2b_2 - b_3 \\\\\n& y = b_0 + ( \\theta + 2b_2 - b_3) x_1 + b_2 x_2 + b_3 x_3 + e \\\\\n& y = b_0 +  \\theta x_1 + b_2( x_2 +2 x_1) + b_3 (x_3-x_1) + e \\\\\n& y = b_0 +  \\theta x_1 + b_2 \\tilde x_2 + b_3 \\tilde x_3 + e \\\\\n\\end{aligned}\n\\]\nHere testing for \\(\\theta=0\\) is the same as testing for \\(b_1 - 2b_2 +b_3\\) in the original model.\n\n\n\n\n\n\n\n\n\nAlways ask something like this in Midterm, so brush up your math."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#testing-multiple-restrictions",
    "href": "rmethods/4_MLRM_IA.html#testing-multiple-restrictions",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Testing Multiple Restrictions",
    "text": "Testing Multiple Restrictions\nWhat if you are interested in testing multiple restrictions:\n\\[\\begin{aligned}\ny &= b_0 + b_1 x_1 + b_2 x_2 +b_3 x_3 + e \\\\\n& H_0: b_1 = 0 ; b_2 - b_3 =0 \\\\\n& H_1: H_0 \\text{ is false}\n\\end{aligned}\n\\]\nEasy way: Stata command test allows you to do this\nOtherwise, you can do it by hand:"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-16",
    "href": "rmethods/4_MLRM_IA.html#section-16",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Estimate unrestricted model (original) and “save” \\(SSR_{ur}\\) or \\(R_{ur}^2\\)\nImpose restrictions on the model and “save” \\(SSR_r\\) or \\(R_{r}^2\\)\nEstimate F-stat:\n\n\\[F_{q,n-k-1} = \\frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(n-k-1)}  \\text{ or }\n\\frac{(R^2_{ur}-R^2_r)/q}{(1-R^2_{ur})/(n-k-1)} \\sim F(q,n-k-1)\n\\]\n\\(SSR\\) Sum of Squared Residuals, \\(q\\) number of restrictions\n\nIdea, you are comparing how the overall fitness of the model changes with restrictions.\nIf restrictions slightly decreases the model Fitness, you cannot be rejected them.\nOtherwise, They are rejected! (you just dont know which)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#overall-model-significance",
    "href": "rmethods/4_MLRM_IA.html#overall-model-significance",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Overall Model Significance",
    "text": "Overall Model Significance\nOne test, we often don’t do anymore, is testing the overall fitness of a model:\n\\[H_0: x_1, x_2, \\dots , x_k \\text{ do not explain y}\n\\]\n\\[H_0: \\beta_1=\\beta_2=\\dots=\\beta_k =0\n\\]\nWhere we kind of suggest that a model with only an intercept is better than the one with covariates.\n\\[F_{q,n-k-1} = \\frac{(R^2_{ur}-\\color{red}{R^2_r})/q}{(1-R^2_{ur})/(n-k-1)} \\sim F(q,n-k-1)\n\\]\nIn this case \\(\\color{red}{R^2_r}=0\\)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#not-for-the-faint-for-heart",
    "href": "rmethods/4_MLRM_IA.html#not-for-the-faint-for-heart",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Not For the faint for heart",
    "text": "Not For the faint for heart\nMatrix form for F-Stat! Restrictions:\n\\[H_0: R_{q,k+1}\\beta_{k+1,1}=c_{q,1}\n\\]\nFirst. Define matrix with all Matrix Restriction \\[\n\\Sigma_R = R_{q,k+1} V_\\beta R'_{q,k+1}\n\\]\nSecond: F-statistic\n\\[\nF-stat = \\frac 1 q (R\\beta-c)' \\Sigma_R^{-1} (R\\beta-c)\n\\]"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#example",
    "href": "rmethods/4_MLRM_IA.html#example",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Example",
    "text": "Example\n\nfrause hprice1, clear\nreg lprice lasses bdrms llotsize lsqrft\n\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(4, 83)        =     70.58\n       Model |  6.19607473         4  1.54901868   Prob &gt; F        =    0.0000\n    Residual |  1.82152879        83   .02194613   R-squared       =    0.7728\n-------------+----------------------------------   Adj R-squared   =    0.7619\n       Total |  8.01760352        87  .092156362   Root MSE        =    .14814\n\n------------------------------------------------------------------------------\n      lprice | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     lassess |   1.043065    .151446     6.89   0.000     .7418453    1.344285\n       bdrms |   .0338392   .0220983     1.53   0.129    -.0101135    .0777918\n    llotsize |   .0074379   .0385615     0.19   0.848    -.0692593    .0841352\n      lsqrft |  -.1032384   .1384305    -0.75   0.458     -.378571    .1720942\n       _cons |    .263743   .5696647     0.46   0.645    -.8692972    1.396783\n------------------------------------------------------------------------------\n\n\n\ntest (lasses=1)\ntest (lasses=1) (bdrms=llotsize=lsqrft=0)\n\n\n ( 1)  lassess = 1\n\n       F(  1,    83) =    0.08\n            Prob &gt; F =    0.7768\n\n ( 1)  lassess = 1\n ( 2)  bdrms - llotsize = 0\n ( 3)  bdrms - lsqrft = 0\n ( 4)  bdrms = 0\n\n       F(  4,    83) =    0.67\n            Prob &gt; F =    0.6162\n\n\n\nfrause mlb1, clear\nreg lsalary years gamesyr bavg hrunsyr rbisy\n\n\n      Source |       SS           df       MS      Number of obs   =       353\n-------------+----------------------------------   F(5, 347)       =    117.06\n       Model |  308.989208         5  61.7978416   Prob &gt; F        =    0.0000\n    Residual |  183.186327       347  .527914487   R-squared       =    0.6278\n-------------+----------------------------------   Adj R-squared   =    0.6224\n       Total |  492.175535       352  1.39822595   Root MSE        =    .72658\n\n------------------------------------------------------------------------------\n     lsalary | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       years |   .0688626   .0121145     5.68   0.000     .0450355    .0926898\n     gamesyr |   .0125521   .0026468     4.74   0.000     .0073464    .0177578\n        bavg |   .0009786   .0011035     0.89   0.376    -.0011918     .003149\n     hrunsyr |   .0144295    .016057     0.90   0.369    -.0171518    .0460107\n      rbisyr |   .0107657    .007175     1.50   0.134    -.0033462    .0248776\n       _cons |   11.19242   .2888229    38.75   0.000     10.62435    11.76048\n------------------------------------------------------------------------------\n\n\n\ntest bavg hrunsyr rbisy\n\n\n ( 1)  bavg = 0\n ( 2)  hrunsyr = 0\n ( 3)  rbisyr = 0\n\n       F(  3,   347) =    9.55\n            Prob &gt; F =    0.0000"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#introduction",
    "href": "rmethods/4_MLRM_IA.html#introduction",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Introduction",
    "text": "Introduction\n\nWhen considering the topic of asymptotic theory, there are few concepts that are important ton consider.\n\nAsymtotics refer to properties of OLS when \\(N\\rightarrow \\infty\\)\nWhen samples grow, we are more concern about consistency rather than “just” unbiased estimators.\nWe are also concern with how flexible is the normality assumption when samples grow large."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#what-is-consistency",
    "href": "rmethods/4_MLRM_IA.html#what-is-consistency",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "What is consistency?",
    "text": "What is consistency?\n\nUp until now, we have been concerned with Unbiased estimates\n\n\\[E(\\hat\\beta)=\\beta\n\\]\n\nIn large samples, this is no longer enough. One requires Consistency!\n\nConsistency says that as \\(N\\rightarrow \\infty\\) then \\(plim \\hat \\beta = \\beta\\).\n\\(p(|\\hat \\beta - \\beta|&lt;\\varepsilon) = 1\\) or that The variance shrinks to zero, or we can estimate \\(\\beta\\) almost surely.\nThis is also known as asymptotic unbiasness.\n\nIn linear regression analysis, consistency can be achieved with a weaker A4’: \\(Cov(e,x)=0\\), assuming that we require only linear independence."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#consistency-vs-bias",
    "href": "rmethods/4_MLRM_IA.html#consistency-vs-bias",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Consistency vs Bias",
    "text": "Consistency vs Bias\n\nConsistent and UnbiasedConsistent and Biased"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#what-about-normality-assumption",
    "href": "rmethods/4_MLRM_IA.html#what-about-normality-assumption",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "What about Normality Assumption?",
    "text": "What about Normality Assumption?\n\nEverything we have seen so far was possible under the normality assumption of the errors.\n\nif \\(e\\) is normal, then \\(b\\) is normal (even in small samples), thus we can use \\(t\\), \\(F\\), etc\n\nBut what if this assumption fails? would we care?\n\n\n\nPerhaps. If your sample is small, \\(b\\) will not be normal, and standard procedures will not work.\nIn large Samples, however, \\(\\beta's\\) will be normal, even if \\(e\\) is not. Thanks to CLT"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-18",
    "href": "rmethods/4_MLRM_IA.html#section-18",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Good news\n\nBottom line, when \\(N\\) is large, you do not need \\(e\\) to be normal.\nif A1-A5 hold, you can rely on asymptotic normality!\nThus you can still use t’s and F’s, but you can also use LM"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#lm-lagrange-multiplier",
    "href": "rmethods/4_MLRM_IA.html#lm-lagrange-multiplier",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "LM-Lagrange Multiplier",
    "text": "LM-Lagrange Multiplier\n\nWhile you can still use t-stat and F-stat to draw inference from your model, there is a better test (given the large sample): Lagrange Multiplier Statistic\nThe idea: Does impossing restrictions affect the model Fitness?\n\n\nRegress \\(y\\) on restricted \\(x_1,\\dots,x_{k-q}\\), and obtain \\(\\tilde e\\)\nRegress \\(\\tilde e\\) on all \\(x's\\), and obtain \\(R^2_e\\).\n\nIf the excluded regressors were not significant, the \\(R^2_e\\) should be very small.\n\nCompare \\(nR^2_e\\) with \\(\\chi^2(n,1-\\alpha)\\), and draw conclusions."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#example-1",
    "href": "rmethods/4_MLRM_IA.html#example-1",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Example:",
    "text": "Example:\n\nfrause crime1, clear\nqui: reg narr86 pcnv avgsen tottime ptime86 qemp86\n** H0: avgsen=0 and tottime=0\ntest (avgsen=0) (tottime=0)\n\n\n ( 1)  avgsen = 0\n ( 2)  tottime = 0\n\n       F(  2,  2719) =    2.03\n            Prob &gt; F =    0.1310\n\n\n\nqui: reg narr86 pcnv                ptime86 qemp86\n* Predict residuals of constrained model\npredict u_tilde , res\n* regress residuals againts all variables\nreg u_tilde  pcnv avgsen tottime ptime86 qemp86\n\n\n      Source |       SS           df       MS      Number of obs   =     2,725\n-------------+----------------------------------   F(5, 2719)      =      0.81\n       Model |  2.87904835         5  .575809669   Prob &gt; F        =    0.5398\n    Residual |  1924.39392     2,719  .707757969   R-squared       =    0.0015\n-------------+----------------------------------   Adj R-squared   =   -0.0003\n       Total |  1927.27297     2,724  .707515773   Root MSE        =    .84128\n\n------------------------------------------------------------------------------\n     u_tilde | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        pcnv |  -.0012971    .040855    -0.03   0.975    -.0814072    .0788129\n      avgsen |  -.0070487   .0124122    -0.57   0.570     -.031387    .0172897\n     tottime |   .0120953   .0095768     1.26   0.207    -.0066833     .030874\n     ptime86 |  -.0048386   .0089166    -0.54   0.587    -.0223226    .0126454\n      qemp86 |   .0010221   .0103972     0.10   0.922    -.0193652    .0214093\n       _cons |  -.0057108   .0331524    -0.17   0.863    -.0707173    .0592956\n------------------------------------------------------------------------------\n\n\n\nCode\ndisplay \"Chi2(2)=\" `=e(N)*e(r2)'\ndisplay \"Its p-value=\" %5.4f `=1-chi2(2, `=e(N)*e(r2)' )'\n\nChi2(2)=4.0707294 Its p-value=0.1306\nTry making it a program if you “dare”"
  },
  {
    "objectID": "rmethods/2_SRA.html#the-simple-regression-model",
    "href": "rmethods/2_SRA.html#the-simple-regression-model",
    "title": "Simple Regression Model",
    "section": "The Simple Regression Model",
    "text": "The Simple Regression Model\n\nAs we saw in the previous slides, one of the important steps when doing empirical analysis is to develop a model that describes reality.\nThis model is quite abstract, as it rarely provides guidance regarding on How should you build your econometric model.\nIn this chapter, we introduce the first (boring) tool to solve this problem. The simple Regression model or SRL"
  },
  {
    "objectID": "rmethods/2_SRA.html#what-is-a-simple-regression-model-srm",
    "href": "rmethods/2_SRA.html#what-is-a-simple-regression-model-srm",
    "title": "Simple Regression Model",
    "section": "What is a Simple Regression Model (SRM) ?",
    "text": "What is a Simple Regression Model (SRM) ?\n\nA Simple regression model is known as such because it aims to capture the relationship between two variables.\nIt does not mean it ignores other factors, but rather, bundles them together as part of a Bag of Holding or error. In its most flexible setup, a simple regression model can be written as:\n\n\\[y = f(x,u)\n\\]\nThis model simply says that there is some relationship between:\n\n\\(y\\), your outcome, dependent, explained, response, variable\nand \\(x\\), your independent, explanatory, regression, variable\n\nwhereas everything else not considered is assumed to be part of the unobserved \\(u\\)."
  },
  {
    "objectID": "rmethods/2_SRA.html#from-abstract-to-concrete",
    "href": "rmethods/2_SRA.html#from-abstract-to-concrete",
    "title": "Simple Regression Model",
    "section": "From Abstract to Concrete",
    "text": "From Abstract to Concrete\n\nA good reason why one should start thinking about the model as shown earlier is to acknowledge that we Do not know the functional form between \\(x\\) and \\(y\\).\nFurther, we don’t even know how \\(u\\) interacts with \\(x\\).\n\nThis brings us to the first step one should do (almost always) when analyzing data…Create a plot to see if there is any relationship in the data"
  },
  {
    "objectID": "rmethods/2_SRA.html#simple-scatter-1",
    "href": "rmethods/2_SRA.html#simple-scatter-1",
    "title": "Simple Regression Model",
    "section": "Simple Scatter 1",
    "text": "Simple Scatter 1\n\n\nCode\n** To download all Wooldrige Files\nqui: ssc install frause, replace\n** for some additional color schemes\nqui: ssc install color_style\nset scheme white2\ncolor_style tableau\n** Loads file wage1\nfrause wage1, clear\nscatter wage educ\n\n\n\n\nWe observe a positive relationship between Wages and years of education\nThis relationship does not seem to be linear"
  },
  {
    "objectID": "rmethods/2_SRA.html#simple-scatter-2",
    "href": "rmethods/2_SRA.html#simple-scatter-2",
    "title": "Simple Regression Model",
    "section": "Simple Scatter 2",
    "text": "Simple Scatter 2"
  },
  {
    "objectID": "rmethods/2_SRA.html#even-more-concrete",
    "href": "rmethods/2_SRA.html#even-more-concrete",
    "title": "Simple Regression Model",
    "section": "Even more Concrete",
    "text": "Even more Concrete\n\nThis first “model” provides little guidance for the modeling itself.\nThe Simple Linear Regression Model corrects for that, establishing a specific relationship between the variables of interest and the error: \\[y = \\beta_0 + \\beta_1 x + u\\]\n\nThis model has a lot packed in.\n\nIt imposes a relationship between \\(y\\) and \\(x\\) (linear)\nAnd addresses the fact that there could be other factors not considered \\(u\\). Impossing the assumption they are additive errors.\n\nIt also assumes the population relationships: \\[E(y|x) = \\beta_0 + \\beta_1 x\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#what-can-we-learn-from-it",
    "href": "rmethods/2_SRA.html#what-can-we-learn-from-it",
    "title": "Simple Regression Model",
    "section": "What can we learn from it?",
    "text": "What can we learn from it?\n\\[E(y|x) = \\beta_0 + \\beta_1 x\\]\nThis is your Population Regresson function. To interpret it, we need to assume \\(u\\) is fixed (ceteris paribus). This implies that \\[E(u|x)=c=0\\]\nWhich says that the errors are mean independent of \\(x\\). Thus, for all practical purposes, when \\(x\\) changes, we will assume \\(u\\) is as good as fixed.\nUnder these conditions, we can interpret the coefficients:\n\n\\(\\beta_0\\) is the constant, or expected outcome when \\(x=0\\).\n\\(\\beta_1\\) is the slope of \\(x\\), or the expected change in \\(y\\) when \\(x\\) changes in 1 unit:\n\n\\[\\Delta y = \\beta_1 \\Delta x \\rightarrow \\frac{\\Delta y}{\\Delta x} = \\beta_1\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#example",
    "href": "rmethods/2_SRA.html#example",
    "title": "Simple Regression Model",
    "section": "Example",
    "text": "Example\n\nSoybean and Yield Fertilizer:\n\n\\[yield = \\beta_0 + \\beta_1 fertilizer + u\\]\n\\(\\beta_1\\) Effect of Fertilizer (an additional dosage) on Soybean Yield\n\nSimple wage equation\n\n\\[wage = \\beta_0 + \\beta_1 educ + u\n\\]\n\\(\\beta_1\\) Change in wages given an additional year of education."
  },
  {
    "objectID": "rmethods/2_SRA.html#deriving-coefficients-ordinary-least-squares---ols",
    "href": "rmethods/2_SRA.html#deriving-coefficients-ordinary-least-squares---ols",
    "title": "Simple Regression Model",
    "section": "Deriving Coefficients: Ordinary Least Squares - OLS",
    "text": "Deriving Coefficients: Ordinary Least Squares - OLS\n\nThere are an infinite number of candiates for \\(\\beta_0 \\& \\beta_1\\).\nOLS, is one of the multiple methods that allows us to estimate the coefficients of a SLRM1.\nThe goal is to Choose parameters \\(\\beta={\\beta_0,\\beta_1}\\) that “minimizes” the Squared of the residuals.\n\nIn other words, OLS aims to maximize Explantion power by minimizing errors.\n\n\nSimple Linear Regression Model"
  },
  {
    "objectID": "rmethods/2_SRA.html#visualization",
    "href": "rmethods/2_SRA.html#visualization",
    "title": "Simple Regression Model",
    "section": "Visualization",
    "text": "Visualization\n\n\nCode\nset seed 10\nclear\nrange x -2 2 20\ngen y = 1 + x + rnormal()\ncolor_style tableau\ntwo (scatter  y x) ///\n    (function y = 0.5 + 2*x, range(-2 2)) ///\n    (function y = 2 + 0.5*x, range(-2 2)) ///\n    (function y = 1 + 1*x  , range(-2 2)) , ///\n    legend(order(2 \"y=0.5+2x\" 3 \"y=2+0.5x\" 4 \"y=1+1x\"))\ngraph export images/fig2_1.png , replace width(1000)\n\ngen y1=.5+2*x\ngen y2=2+0.5*x\ngen y3=+1+1*x\n\n\negen u21 = sum((y-y1)^2)\negen u22 = sum((y-y2)^2)\negen u23 = sum((y-y3)^2)\nreg y x\negen u24 = sum((y-_b[_cons] - _b[x]*x)^2)\nlabel var u21 \"SSR-model 1\"\nlabel var u22 \"SSR-model 2\"\nlabel var u23 \"SSR-model 3\"\nlabel var u24 \"SSR-model 4\"\n\ntwo (scatter y x) ///\n    (function y = 0.5 + 2*x, range(-2 2)) ///\n    (rspike y y1 x), ylabel(-4/6) ytitle(y) ///\n    legend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))\ngraph export images/fig2_2.png , replace width(1000) \n    \ntwo (scatter y x) ///\n    (function y = 2 + 0.5*x, range(-2 2)) ///\n    (rspike y y2 x), ylabel(-4/6) ytitle(y) ///\n    legend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))\ngraph export images/fig2_3.png , replace width(1000) \n    \ntwo (scatter y x) ///\n    (function y = 1 + 1*x, range(-2 2)) ///\n    (rspike y y3 x) , ylabel(-4/6) ytitle(y) ///\n    legend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))\ngraph export images/fig2_4.png , replace width(1000)    \n\nreg y x\npredict yh\ntwo (scatter y x) ///\n    (function y = _b[_cons] + _b[x]*x, range(-2 2)) ///\n    (rspike y yh x) , ylabel(-4/6) ytitle(y) ///\n    legend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))    \ngraph export images/fig2_5.png , replace width(1000)    \nclonevar u24x=u24\ngraph bar u24 u21 u22 u23 u24x , ///\n   legend(order( 2 \"SSR-Model 1\" 3 \"SSR-Model 2\" 4 \"SSR-Model 3\" 5 \"Sample Fit\") ///\n   ring(0) pos(2)) xsize(5) ysize(8) scale(1.5) bar(1, color(gs0%0))\n   \ngraph export images/fig2_5x.png , replace width(300)    \n\n\n\nOptionsOpt1Opt2Opt3Opt4"
  },
  {
    "objectID": "rmethods/2_SRA.html#just-a-minimization-problem",
    "href": "rmethods/2_SRA.html#just-a-minimization-problem",
    "title": "Simple Regression Model",
    "section": "Just a Minimization Problem",
    "text": "Just a Minimization Problem\n\\[y_i =\\beta_0 + \\beta_1 x_i + u_i \\rightarrow u_i = y_i - \\beta_0 - \\beta_1 x_i\n\\]\n\\[{\\hat\\beta_0,\\hat\\beta_1} = \\min_{\\beta_0,\\beta_1} = SSR =\\sum_{i=1}^N u_i^2 = \\sum_{i=1}^N (y-\\beta_0 - \\beta_1 x_i)^2 \\\\\n\\]\nFirst Order Conditions:\n\\[\n\\begin{aligned}\n\\frac{\\partial SSR}{\\partial \\beta_0} &= -2 \\sum (y_i-\\beta_0 - \\beta_1 x_i) = -2 \\sum u_i =0 \\\\\n\\frac{\\partial SSR}{\\partial \\beta_1} &= -2 \\sum x_i (y_i-\\beta_0 - \\beta_1 x_i) =- 2 \\sum x_i u_i =0\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#just-a-minimization-problem-1",
    "href": "rmethods/2_SRA.html#just-a-minimization-problem-1",
    "title": "Simple Regression Model",
    "section": "Just a Minimization Problem",
    "text": "Just a Minimization Problem\nSimilar conditions as before (but now Mathematically):\n\\[\\begin{aligned}\n\\sum u_i &=0 \\rightarrow nE(e) = 0 \\\\\n\\sum x_i u_i &=0 \\rightarrow nE(x*e) \\rightarrow  n Cov(x,e) =0  \n\\end{aligned}\n\\]\nAnd the First Order Conditions simply provide a system of \\(k+1\\) equations with \\(k+1\\) unknowns.\n\\[\\begin{aligned}\n\\hat\\beta_0 &= \\bar y - \\beta_1 \\bar x \\\\\n\\hat\\beta_1 &= \\frac{\\sum (x_i-\\bar x)(y_i-\\bar y)}{\\sum (x_i-\\bar x)^2}\n= \\frac{\\hat \\rho \\hat \\sigma_x \\hat \\sigma_y}{\\hat \\sigma_x^2}\n= \\frac{\\hat \\rho \\hat \\sigma_y}{\\hat \\sigma_y}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#interpretation",
    "href": "rmethods/2_SRA.html#interpretation",
    "title": "Simple Regression Model",
    "section": "Interpretation?",
    "text": "Interpretation?\n\\[\\hat\\beta_0 = \\bar y - \\hat \\beta_1 \\bar x\\]\n\n\\(\\beta_0\\) is usually estimated as a “residual”, thus is often of little of no interest.\n\nExpected outcome when \\(X=0\\)\n\n\n\\[\n\\hat\\beta_1 = \\frac {cov(x,y)}{var(x)}= \\hat \\rho\\frac{ \\hat \\sigma_x \\hat \\sigma_y}{\\hat \\sigma_x^2}\n= \\hat \\rho\\frac{ \\hat \\sigma_y}{\\hat \\sigma_x}\n\\]\n\n\\(\\beta_1\\) is a slope, which is directly related to the correlation between \\(y\\) and \\(x\\).\n\nIt can only be estimated if \\(\\sigma_x\\)&gt;&gt;0\n\n\nAlso, this \\(\\hat y = \\hat \\beta_0 + \\hat \\beta_1 x\\) becomes your sample regression function\n\nwhere \\(\\hat y\\) is the fitted value of \\(y\\) (proyection or prediction), given some value of \\(x\\)."
  },
  {
    "objectID": "rmethods/2_SRA.html#visualization-1",
    "href": "rmethods/2_SRA.html#visualization-1",
    "title": "Simple Regression Model",
    "section": "Visualization",
    "text": "Visualization\n\n\nCode\ngen y0 = 0\ntwo (scatter y x) ///\n    (function y = _b[_cons] + _b[x]*x, range(-2.1 2.1)) ///\n    (rspike y y0 x, color(gs9%50) lw(1)) ///\n    , ylabel(-4/6) ytitle(y) yline(0) ///\n    title(\"Data\")   legend(off)\ngraph export images/fig2_6.png, replace width(1000)\ntwo (scatter y x) ///\n    (function y = _b[_cons] + _b[x]*x, range(-2.1 2.1)) ///\n    (rspike yh y0 x, color(gs9%50) lw(1)) ///\n    , ylabel(-4/6) ytitle(y) yline(0) ///\n    title(\"Prediction\") legend(off)\ngraph export images/fig2_7.png, replace width(1000) \ntwo (scatter y x) ///\n    (function y = _b[_cons] + _b[x]*x, range(-2.1 2.1)) ///\n    (rspike y yh x, color(gs9%50) lw(1)) ///\n    , ylabel(-4/6) ytitle(y) yline(0) ///\n    title(\"Residual\")   legend(off)\ngraph export images/fig2_8.png, replace width(1000) \n\n\n\nDataPredictionResiduals"
  },
  {
    "objectID": "rmethods/2_SRA.html#properties-of-the-estimator",
    "href": "rmethods/2_SRA.html#properties-of-the-estimator",
    "title": "Simple Regression Model",
    "section": "Properties of the Estimator",
    "text": "Properties of the Estimator\n\nBased on F.O.C., we know the following:\n\n\\[\n\\sum_i^n \\hat u_i = 0 \\ \\& \\ \\sum_i^n x_i \\hat u_i = 0\n\\]\nIn average \\(u_i\\) is zero, and uncorrelated with \\(x\\), and \\(\\bar y , \\bar x\\) is on the regression line\n\nBy construction \\(y_i = \\hat y_i + \\hat u_i\\), so that\n\n\\[\n\\begin{aligned}\n    \\sum_{i=1}^n(y_i-\\bar y)^2 &=\n    \\sum_{i=1}^n(y_i-\\hat y)^2  +\n    \\sum_{i=1}^n(\\hat y-\\bar y)^2  \\\\\n    SST &= SSE + SSR\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#properties-of-the-estimator-1",
    "href": "rmethods/2_SRA.html#properties-of-the-estimator-1",
    "title": "Simple Regression Model",
    "section": "Properties of the Estimator",
    "text": "Properties of the Estimator\n\nGoodness of FIT is defined as\n\n\\[R^2= 1-\\frac {SSR} {SST}=\\frac {SSE} {SST}\\]\n\nHow much of the Data variation (SST) is explained by the model (SSE), or\n1-amount not explained by the model"
  },
  {
    "objectID": "rmethods/2_SRA.html#some-discussion",
    "href": "rmethods/2_SRA.html#some-discussion",
    "title": "Simple Regression Model",
    "section": "Some Discussion",
    "text": "Some Discussion\nWe now know how to estimate coefficients given some data, but we need to ask the questions:\n\nHow do we know if the estimated coefficients are indeed appropriate for the population parameters?\nHow can we know the precision (or lack there off) of the estimates\nRemember, \\(\\hat \\beta's\\) depend on the sample. Different Samples will lead to different estimates. Thus \\(\\hat \\beta's\\) are random.\nIn repeated sampling scenarios, we could empirically obtain the distribution of the estimated parameters, and verify if estimations are unbiased.\nHowever, we can also do that based on analytical solutions. Lets see those assumptions"
  },
  {
    "objectID": "rmethods/2_SRA.html#assumption-1",
    "href": "rmethods/2_SRA.html#assumption-1",
    "title": "Simple Regression Model",
    "section": "Assumption 1:",
    "text": "Assumption 1:\n\n\n\n\n\n\nLinear in Parameters\n\n\nWe need to assume the population model is linear in parameters:\n\\[y_i = \\beta_0 + \\beta_1 x_i + u_i\\]\n\n\n\nIn other words, we need to assume that the model we chose is a good representation of what the true population model is.\n\nAdditive error, with a linear relationship between \\(x_i\\) on \\(y_i\\).\nWe can make it more flexible using some transformations of \\(x_i\\)."
  },
  {
    "objectID": "rmethods/2_SRA.html#assumption-2",
    "href": "rmethods/2_SRA.html#assumption-2",
    "title": "Simple Regression Model",
    "section": "Assumption 2:",
    "text": "Assumption 2:\n\n\n\n\n\n\nRandom Sampling\n\n\nThe data we are using is collected from a Random sample of the population, for which the linear model is valid.\n\n\n\n\nData should be representative from the population (for whom the Linear model Holds)\nThe Data Sampling should not depend the data collected, specially the dependent variable.\nAlso helps to ensure units “unobservables” \\(u's\\) are independent from each other."
  },
  {
    "objectID": "rmethods/2_SRA.html#assumption-3",
    "href": "rmethods/2_SRA.html#assumption-3",
    "title": "Simple Regression Model",
    "section": "Assumption 3:",
    "text": "Assumption 3:\n\n\n\n\n\n\nThere is variation in the explanatory variable\n\n\n\\[\\sum_{i=1}^n (x_i - \\bar x)^2 &gt;0\n\\]\n\n\n\nIf there is no variation in the data, there are no slopes to estmate, and a solution cannot be found to the linear model."
  },
  {
    "objectID": "rmethods/2_SRA.html#assumption-4",
    "href": "rmethods/2_SRA.html#assumption-4",
    "title": "Simple Regression Model",
    "section": "Assumption 4:",
    "text": "Assumption 4:\n\n\n\n\n\n\nZero Conditional Mean\n\n\n\\[E(u_i)= E(u_i|x_i) = 0\n\\]\n\n\n\n\nWe expect unobserved factors \\(u_i\\) to have a zero average effect on the outcome. This helps identify the constant \\(\\beta_0\\).\nWe also expect that the expected value of \\(u_i\\) to be zero for any value of \\(x\\)."
  },
  {
    "objectID": "rmethods/2_SRA.html#unbiased-coefficients",
    "href": "rmethods/2_SRA.html#unbiased-coefficients",
    "title": "Simple Regression Model",
    "section": "Unbiased Coefficients:",
    "text": "Unbiased Coefficients:\nIf Assumptions 1-4 Hold, then OLS allows you to estimate the coefficents of the linear Regression model.\n\\[\\hat \\beta_1 = \\frac {\\sum \\tilde x_i \\tilde y_i}{\\sum \\tilde x_i^2} , \\tilde x_i=x_i - \\bar x\n\\]\n\\[\\begin{aligned}\n\\hat \\beta_1 &= \\frac {\\sum \\tilde x_i (\\beta_1 \\tilde x_i +e)}{\\sum \\tilde x_i^2} = \\beta_1 \\frac {\\sum  \\tilde x_i^2 }{\\sum \\tilde x_i^2} + \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2}  \\\\  \nE(\\hat \\beta_1) &= \\beta_1\n\\end{aligned}\n\\]\nWhile coefficients can be different for each sample, In average, they will be the same as the true parameters."
  },
  {
    "objectID": "rmethods/2_SRA.html#variance-of-ols-estimators",
    "href": "rmethods/2_SRA.html#variance-of-ols-estimators",
    "title": "Simple Regression Model",
    "section": "Variance of OLS Estimators",
    "text": "Variance of OLS Estimators\nHow precise are the estimates?\n\\[\\hat \\beta_1 = \\beta_1 + \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2}\n\\]\n\nIf we assume \\(x's\\) are assume fixed, the distribution from \\(\\beta's\\) will depend only on the variation of the error \\(u_i\\).\nThus we need to impose an additional assumption on this errors, to estimate the variance of \\(\\beta's\\). (at least for convinience)"
  },
  {
    "objectID": "rmethods/2_SRA.html#assumption-5",
    "href": "rmethods/2_SRA.html#assumption-5",
    "title": "Simple Regression Model",
    "section": "Assumption 5:",
    "text": "Assumption 5:\n\n\n\n\n\n\nErrors are Homoskedastic\n\n\n\\[E(u_i^2)= E(u_i ^2 | x_i) = \\sigma_u ^2\n\\]\n\n\n\n\nThis simplifying assumption states that the “distribution” of the errors is constant, regardless of \\(x\\)."
  },
  {
    "objectID": "rmethods/2_SRA.html#visualization-2",
    "href": "rmethods/2_SRA.html#visualization-2",
    "title": "Simple Regression Model",
    "section": "Visualization",
    "text": "Visualization\n\n\nCode\nclear\nset scheme white2\nset obs 1000\ngen x = runiform(-2 , 2)    \ngen u = rnormal()\ngen y1 = x + u\ngen y2 = x + u*abs(x)\ngen y3 = x + u*(2-abs(x))\ngen y4 = x + u*(sin(2*x))\n\n\ntwo scatter y1 x, name(m1, replace)\nscatter y2 x, name(m2, replace)\nscatter y3 x, name(m3, replace)\nscatter y4 x, name(m4, replace)\ngraph combine m1 m2 m3 m4\ngraph export images/fig2_9.png, replace width(1000)"
  },
  {
    "objectID": "rmethods/2_SRA.html#sampling-variance-of-ols",
    "href": "rmethods/2_SRA.html#sampling-variance-of-ols",
    "title": "Simple Regression Model",
    "section": "Sampling Variance of OLS",
    "text": "Sampling Variance of OLS\nWe Start with:\n\\[\n\\hat \\beta_1 - \\beta_1 = \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2}\n\\]\nAnd apply the Variance operator:\n\\[\\begin{aligned}\nVar(\\hat \\beta_1 - \\beta_1) &= Var \\left( \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2} \\right)=\n            \\frac{\\tilde x_1 Var(u_i)}{(\\sum x_i^2)^2}+\\frac{\\tilde x_2^2 Var(u_i)}{(\\sum x_i^2)^2}+...+\\frac{\\tilde x_n^2 Var(u_i)}{(\\sum x_i^2)^2} \\\\\n            &= \\frac {\\sum  \\tilde x_i^2 Var( u_i) }{(\\sum \\tilde x_i^2)^2} =\\frac {\\sum  \\tilde x_i^2 \\sigma_u^2 }{(\\sum \\tilde x_i^2)^2} \\\\\n            &= \\sigma_u^2 \\frac {\\sum  \\tilde x_i^2 }{(\\sum \\tilde x_i^2)^2} = \\frac{\\sigma_u^2}{\\sum \\tilde x_i^2}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#last-piece-of-the-puzze-sigma2_u",
    "href": "rmethods/2_SRA.html#last-piece-of-the-puzze-sigma2_u",
    "title": "Simple Regression Model",
    "section": "Last Piece of the Puzze \\(\\sigma^2_u\\)",
    "text": "Last Piece of the Puzze \\(\\sigma^2_u\\)\nTo estimate \\(Var(\\beta's)\\) we also need \\(\\sigma^2_u\\). But \\(u\\) is not observed, since we only observe \\(\\hat u\\).\n\\[\\hat u_i = u_i + (\\beta_0 - \\hat \\beta_0) + (\\beta_1 - \\hat \\beta_1)*x_i\n\\]\nAnd since to estimate \\(\\hat u_i\\) we need to estimate \\(\\beta_0\\) and \\(\\beta_1\\), we “lose” degrees of freedom that will require adjustment.\nSo, we use the following:\n\\[\\hat \\sigma^2_u = \\frac {1}{N-2} \\sum_{i=1}^N \\hat u_i^2\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#examples",
    "href": "rmethods/2_SRA.html#examples",
    "title": "Simple Regression Model",
    "section": "Examples",
    "text": "Examples\nDeriving OLS \\(\\beta's\\):\n\n** Wage and Education: Example 2.7\n\nfrause wage1, clear\nmata: y = st_data(.,\"wage\")\nmata: x = st_data(.,\"educ\")\nmata: b1 = sum( (x :- mean(x)) :* (y :- mean(y)) ) / sum( (x :- mean(x)):^2 ) \nmata: b0 = mean(y)-mean(x)*b1\nmata: b1, b0\n\n                 1             2\n    +-----------------------------+\n  1 |  .5413592547   -.904851612  |\n    +-----------------------------+\n\n\nSST = SSE + SSR\n\nmata: yh  = b0:+b1*x\nmata: sst = sum( (y:-mean(y)):^2 )\nmata: sse = sum( (yh:-mean(y)):^2 )\nmata: ssr = sum( (y:-yh):^2 )\nmata: sst, sse, ssr, sse + ssr\n\n                 1             2             3             4\n    +---------------------------------------------------------+\n  1 |  7160.414291   1179.732036   5980.682255   7160.414291  |\n    +---------------------------------------------------------+\n\n\n\\(R^2\\):\n\nmata: sse/sst , 1-ssr/sst\n\n                1            2\n    +---------------------------+\n  1 |  .164757511   .164757511  |\n    +---------------------------+\n\n\n\\(\\hat\\sigma_\\beta\\)\n\nmata: sig2_u = ssr / (rows(y)-2)\nmata: sst_x  = sum( (x:-mean(x)):^2 )\nmata: sig_b1 = sqrt( sig2_u / sst_x )\nmata: sig_b0 = sqrt( sig2_u * mean(x:^2) / sst_x ) \nmata: sig_b1 , sig_b0\n\n                 1             2\n    +-----------------------------+\n  1 |  .0532480368   .6849678211  |\n    +-----------------------------+\n\n\nStata command:\n\n\nCode\nregress wage educ\n\n\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =    103.36\n       Model |  1179.73204         1  1179.73204   Prob &gt; F        =    0.0000\n    Residual |  5980.68225       524  11.4135158   R-squared       =    0.1648\n-------------+----------------------------------   Adj R-squared   =    0.1632\n       Total |  7160.41429       525  13.6388844   Root MSE        =    3.3784\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .5413593    .053248    10.17   0.000     .4367534    .6459651\n       _cons |  -.9048516   .6849678    -1.32   0.187    -2.250472    .4407687\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/2_SRA.html#expanding-on-slrm-units-of-measure",
    "href": "rmethods/2_SRA.html#expanding-on-slrm-units-of-measure",
    "title": "Simple Regression Model",
    "section": "Expanding on SLRM: Units of Measure",
    "text": "Expanding on SLRM: Units of Measure\nFirst thing you should always consider doing is obtaining some summary statistics.\nwithout that its difficult o understand the magnitud of the coefficients and their effects.\n\n\nCode\nfrause ceosal1, clear\ndisplay \"***Variables Description***\"\ndes salary roe\ndisplay \"***Summary Statistics***\"\nsum salary roe\ndisplay \"***Simple Regression***\"\nreg salary roe\n\n\n***Variables Description***\n\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nsalary          int     %9.0g                 1990 salary, thousands $\nroe             float   %9.0g                 return on equity, 88-90 avg\n***Summary Statistics***\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      salary |        209     1281.12    1372.345        223      14822\n         roe |        209    17.18421    8.518509         .5       56.3\n***Simple Regression***\n\n      Source |       SS           df       MS      Number of obs   =       209\n-------------+----------------------------------   F(1, 207)       =      2.77\n       Model |  5166419.04         1  5166419.04   Prob &gt; F        =    0.0978\n    Residual |   386566563       207  1867471.32   R-squared       =    0.0132\n-------------+----------------------------------   Adj R-squared   =    0.0084\n       Total |   391732982       208  1883331.64   Root MSE        =    1366.6\n\n------------------------------------------------------------------------------\n      salary | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         roe |   18.50119   11.12325     1.66   0.098    -3.428196    40.43057\n       _cons |   963.1913   213.2403     4.52   0.000     542.7902    1383.592\n------------------------------------------------------------------------------\n\n\nNow, Changing scales has no effect on \\(R^2\\), nor the coefficient to Standard error ratio (\\(t-stat\\))\nIt could allow for easier interpretation of the results.\n\ngen saldol=salary*1000\ngen roedec=roe / 100\nqui: reg salary roe\nest sto m1\nqui: reg saldol roe\nest sto m2\nqui: reg salary roedec\nest sto m3\nesttab m1 m2 m3, se r2\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                   salary          saldol          salary   \n------------------------------------------------------------\nroe                 18.50         18501.2                   \n                  (11.12)       (11123.3)                   \n\nroedec                                             1850.1   \n                                                 (1112.3)   \n\n_cons               963.2***     963191.3***        963.2***\n                  (213.2)      (213240.3)         (213.2)   \n------------------------------------------------------------\nN                     209             209             209   \nR-sq                0.013           0.013           0.013   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/2_SRA.html#expanding-on-slrm-nonlinearities",
    "href": "rmethods/2_SRA.html#expanding-on-slrm-nonlinearities",
    "title": "Simple Regression Model",
    "section": "Expanding on SLRM: Nonlinearities",
    "text": "Expanding on SLRM: Nonlinearities\nIt is possible to incorporate some nonlinearities by using “log” transformations:\n\\[\n\\begin{aligned}\nlog(y) &= \\beta_0 + \\beta_1 x + e \\rightarrow & 100\\beta_1 \\simeq \\frac{\\% \\Delta y}{\\Delta x} \\\\\ny &= \\beta_0 + \\beta_1 log(x) + e \\rightarrow & \\frac {\\beta_1}{100} \\simeq \\frac{\\Delta y}{\\% \\Delta x} \\\\\nlog(y) &= \\beta_0 + \\beta_1 log(x) + e \\rightarrow & \\beta_1 =\\frac{\\% \\Delta y}{\\% \\Delta x}\n\\end{aligned}\n\\]\n\nThis allows us to estimate other interesting relationships with a the SRM. Specifically Semi-elasticities and Elasticities.\nThis transformation compresses the distribution of a variable, potentially addressing problems of Heteroskedasticity (non-constant variance)\n\n\n\nCode\n*** Example 2.10\nfrause wage1, clear\ngen lnwage = log(wage)\ngen lneduc = log(educ)\ntwo scatter wage educ     || lfit wage educ    , ///\n    name(m1, replace) title(lin-lin) legend(off)\ntwo scatter lnwage educ   || lfit lnwage educ  , ///\n    name(m2, replace) title(log-lin) legend(off)\ntwo scatter wage lneduc   || lfit wage lneduc  , ///\n    name(m3, replace) title(lin-log) legend(off)\ntwo scatter lnwage lneduc || lfit lnwage lneduc, ///\n    name(m4, replace) title(log-log) legend(off)\ngraph combine m1 m2 m3 m4, \ngraph export images/fig2_10.png, width(1000) replace\n\n\n\n\n\n\n\nCode\nset linesize 255\nqui: reg wage educ\nest sto m1\nqui: reg lnwage educ\nest sto m2\nqui: reg wage lneduc\nest sto m3\nqui: reg lnwage lneduc\nest sto m4\nesttab m1 m2 m3 m4, se r2 nonumber  compress nostar nogaps\n\n\n\n--------------------------------------------------\n                wage    lnwage      wage    lnwage\n--------------------------------------------------\neduc           0.541    0.0827                    \n            (0.0532) (0.00757)                    \nlneduc                             5.330     0.825\n                                 (0.608)  (0.0864)\n_cons         -0.905     0.584    -7.460    -0.445\n             (0.685)  (0.0973)   (1.532)   (0.218)\n--------------------------------------------------\nN                526       526       524       524\nR-sq           0.165     0.186     0.128     0.149\n--------------------------------------------------\nStandard errors in parentheses\n\n\n\n\nAn additional year of education\n\n1. Increases hourly wages in 54cnts\n\n2. Increases hourly wages in 8.3%\n\n3. A 1% increase in years of education (about 1.5months) increases wages in 5.3cnts\n\n4. A 1% increase in years of education would increase wages in 0.82%."
  },
  {
    "objectID": "rmethods/2_SRA.html#expanding-on-slrm-using-dummies",
    "href": "rmethods/2_SRA.html#expanding-on-slrm-using-dummies",
    "title": "Simple Regression Model",
    "section": "Expanding on SLRM: Using Dummies",
    "text": "Expanding on SLRM: Using Dummies\n\nA SLRM can also be done using Dummy variables. (Those that take only two values: 0 or 1)\nThis type of modeling may be observed when evaluating programs (Were you treated?(Tr=1) or not (Tr=0))\nAnd can be used to Easily compare means across two groups:\n\n\\[wage = \\beta_0 + \\beta_1 female + e\n\\]\nIn this particular case, both \\(\\beta_0 \\& \\beta_1\\) have clear interpretation:\n\\[\n\\begin{aligned}\n\\beta_0 &= E(wage|female=0) \\\\\n\\beta_1 &= E(wage|female=1) - E(wage|female=0)\n\\end{aligned}\n\\]\nIn most Software, you need to either Create the new variable explicitly, or use internal code to make it for you:\n\nfrause wage1, clear\n** verify Coding\nssc install fre, replace\nfre female\n** create your own\ngen is_male = female==0\n** Regression using Newly created variable\nreg wage is_male\n** Regression using Stata \"factor notation\"\nreg wage i.female\n\nchecking fre consistency and verifying not already installed...\nall files already exist and are up to date.\n\nfemale -- =1 if female\n-----------------------------------------------------------\n              |      Freq.    Percent      Valid       Cum.\n--------------+--------------------------------------------\nValid   0     |        274      52.09      52.09      52.09\n        1     |        252      47.91      47.91     100.00\n        Total |        526     100.00     100.00           \n-----------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =     68.54\n       Model |  828.220467         1  828.220467   Prob &gt; F        =    0.0000\n    Residual |  6332.19382       524  12.0843394   R-squared       =    0.1157\n-------------+----------------------------------   Adj R-squared   =    0.1140\n       Total |  7160.41429       525  13.6388844   Root MSE        =    3.4763\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     is_male |    2.51183   .3034092     8.28   0.000     1.915782    3.107878\n       _cons |   4.587659   .2189834    20.95   0.000     4.157466    5.017852\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =     68.54\n       Model |  828.220467         1  828.220467   Prob &gt; F        =    0.0000\n    Residual |  6332.19382       524  12.0843394   R-squared       =    0.1157\n-------------+----------------------------------   Adj R-squared   =    0.1140\n       Total |  7160.41429       525  13.6388844   Root MSE        =    3.4763\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    1.female |   -2.51183   .3034092    -8.28   0.000    -3.107878   -1.915782\n       _cons |   7.099489   .2100082    33.81   0.000     6.686928     7.51205\n------------------------------------------------------------------------------\n\n\nif the Dummy is a treatment, and Assumption 4 Holds, then you can use this to estimate Average Treatment Effects (ATE) aka Average Casual Effects. (Usually requires randomization)\n\\[\n\\begin{aligned}\ny_i &= y_i(0)(1-D) + y_i(1)D  \\\\\ny_i &= y_i(0) + (y_i(1)-y_i(0))*D \\\\\ny_i &= \\bar y_0 + u_i(0) + (\\bar y_1 - \\bar y_0)*D + (u_i(1)-u_i(0))*D \\\\\ny_i &= \\alpha_0 + \\tau_{ate} D + u_i\n\\end{aligned}\n\\]\nBut we expect \\(u_i(1)-u_i(0)=0\\)\n\n** Example 2.14\nfrause jtrain2, clear\n** Training was Randomly assigned\nreg re78 i.train\n\n\n      Source |       SS           df       MS      Number of obs   =       445\n-------------+----------------------------------   F(1, 443)       =      8.04\n       Model |  348.013451         1  348.013451   Prob &gt; F        =    0.0048\n    Residual |  19177.6432       443  43.2903909   R-squared       =    0.0178\n-------------+----------------------------------   Adj R-squared   =    0.0156\n       Total |  19525.6566       444  43.9767041   Root MSE        =    6.5795\n\n------------------------------------------------------------------------------\n        re78 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     1.train |   1.794343   .6328536     2.84   0.005     .5505748    3.038111\n       _cons |   4.554802    .408046    11.16   0.000     3.752856    5.356749\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rm2_return.html",
    "href": "rm2_return.html",
    "title": "Research Methods II: Applied Micro-methods",
    "section": "",
    "text": "This page is still under construction.\nThis site will cover provide all material for the Micro-based research methods course (Second part of the course)\nAs always, here, you will have access to all documents, slides, and code used in class."
  },
  {
    "objectID": "quizes/quiz_1.html",
    "href": "quizes/quiz_1.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Name: ________________________________________________________\n\nHousehold Budget surveys are an example of:\n\n\npanel data\nexperimental data\ntime series data\ncross-sectional data\n\n\nA dependent variable is also known as a(n) _____.\n\n\nexplanatory variable\ncontrol variable\npredictor variable\nresponse variable\n\n\nThe Zero conditional mean assumption means\n\n\n\\(E(u│x)=0\\)\n\\(E(\\hat u | x)=0\\)\n\\(E(u^2│x)=0\\)\n\\(E(\\hat u ^2│x)=0\\)\n\n\nThe explained sum of squares (SSE) for the regression function, \\(y=\\beta_0 + \\beta_1 x + u\\), is defined as _____.\n\n\n\\(\\sum(\\hat y_i - \\bar{y})^2\\)\n\\(\\sum(y_i - \\hat y_i )^2\\)\n\\(\\sum \\hat u\\)\n\\(\\sum \\hat u^2\\)\n\n\nThe error term in a regression equation is said to exhibit homoskedasticty if _____.\n\n\nit has zero conditional mean\nit has the same variance for all values of the explanatory variable\nit has the same value for all values of the explanatory variable\nif the error term has a value of one given any value of the explanatory variable\n\n\nBonus: Why do we care about the Zero Conditional mean assumption?"
  },
  {
    "objectID": "quizes/quiz_1.html#quiz-1",
    "href": "quizes/quiz_1.html#quiz-1",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Name: ________________________________________________________\n\nHousehold Budget surveys are an example of:\n\n\npanel data\nexperimental data\ntime series data\ncross-sectional data\n\n\nA dependent variable is also known as a(n) _____.\n\n\nexplanatory variable\ncontrol variable\npredictor variable\nresponse variable\n\n\nThe Zero conditional mean assumption means\n\n\n\\(E(u│x)=0\\)\n\\(E(\\hat u | x)=0\\)\n\\(E(u^2│x)=0\\)\n\\(E(\\hat u ^2│x)=0\\)\n\n\nThe explained sum of squares (SSE) for the regression function, \\(y=\\beta_0 + \\beta_1 x + u\\), is defined as _____.\n\n\n\\(\\sum(\\hat y_i - \\bar{y})^2\\)\n\\(\\sum(y_i - \\hat y_i )^2\\)\n\\(\\sum \\hat u\\)\n\\(\\sum \\hat u^2\\)\n\n\nThe error term in a regression equation is said to exhibit homoskedasticty if _____.\n\n\nit has zero conditional mean\nit has the same variance for all values of the explanatory variable\nit has the same value for all values of the explanatory variable\nif the error term has a value of one given any value of the explanatory variable\n\n\nBonus: Why do we care about the Zero Conditional mean assumption?"
  },
  {
    "objectID": "quarto/stata_basics.html",
    "href": "quarto/stata_basics.html",
    "title": "Stata-output",
    "section": "",
    "text": "Analyzing Oaxaca dataset\nsmaller\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n&gt; (2)\n\n\n\n\n\nyears of education\n0.0885***\n0 &gt; .0794***\n\n\n\n(0.00519)\n(0.0 &gt; 0522)\n\n\n \n\n\nyears of work\n0.0153***\n0. &gt; 00399*\n\n\nexperience\n(0.00126)\n(0.0 &gt; 0188)\n\n\n \n\n\nyears of job tenure\n\n0. &gt; 00407*\n\n\n\n\n(0.0 &gt; 0196)\n\n\n \n\n\nage of respondent\n\n0 &gt; .0114***\n\n\n\n\n(0.0 &gt; 0174)\n\n\n \n\n\nConstant\n2.136***\n&gt; 1.915***\n\n\n\n(0.0654)\n(0. &gt; 0727)\n\n\n\n\n\nObservations\n1434\n&gt; 1434"
  },
  {
    "objectID": "Notes.html",
    "href": "Notes.html",
    "title": "Untitled",
    "section": "",
    "text": "Econometrics\n\nChapt 1 and 2\nchapt 3\nChapt 4 and 5? &lt;- no emphasis on 5\nChapt 6 - 7\nChapt 8\nchapt 9\nChapt 15 - IV\nchapt 17 - logit/probit/tobit/poisson/\nChapt 13 - Pool CS and Panel\nChapt 14 - Panel\nChapt 10 - 11 Time series\nchatp 12 - 18 SCorr Heteroskedasiticy"
  },
  {
    "objectID": "mathref/math_2.html",
    "href": "mathref/math_2.html",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "",
    "text": "A vector is a list of numbers. We can think of a vector as a point in space, or as an arrow pointing from the origin to that point. For example, the vector\n\\[\\vec{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\] is a vector in \\(\\mathbb{R}^3\\) (three-dimensional space) that points from the origin to the point \\((1, 2, 3)\\).\nWe can add vectors together by adding their corresponding elements. For example,\n\\[\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 7 \\\\ 9 \\end{bmatrix}\\]\nWe can also multiply a vector by a scalar (a single number) by multiplying each element of the vector by that number. For example,\n\\[2 \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "mathref/math_2.html#vectors",
    "href": "mathref/math_2.html#vectors",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "",
    "text": "A vector is a list of numbers. We can think of a vector as a point in space, or as an arrow pointing from the origin to that point. For example, the vector\n\\[\\vec{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\] is a vector in \\(\\mathbb{R}^3\\) (three-dimensional space) that points from the origin to the point \\((1, 2, 3)\\).\nWe can add vectors together by adding their corresponding elements. For example,\n\\[\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 7 \\\\ 9 \\end{bmatrix}\\]\nWe can also multiply a vector by a scalar (a single number) by multiplying each element of the vector by that number. For example,\n\\[2 \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "mathref/math_2.html#matrices",
    "href": "mathref/math_2.html#matrices",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Matrices",
    "text": "Matrices\nA matrix is a two-dimensional array of numbers. We can think of a matrix as a list of vectors. For example, the matrix:\n\\[A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} =\n\\begin{bmatrix} 1 \\\\ 4  \\end{bmatrix},\n\\begin{bmatrix} 2 \\\\ 5  \\end{bmatrix},\n\\begin{bmatrix} 3 \\\\ 6  \\end{bmatrix} \\]\nis a matrix that concatenates 3 \\(\\mathbb{R}^2\\) vectors together.\nMatrices can have different dimensions. For example, the matrix:\n\\[B = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\]\nis a square matrix that concatenates 3 \\(\\mathbb{R}^3\\) vectors together."
  },
  {
    "objectID": "mathref/math_2.html#matrix-dimensions",
    "href": "mathref/math_2.html#matrix-dimensions",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Matrix Dimensions",
    "text": "Matrix Dimensions\nMatrices are often denoted by their dimensions. For example, the matrix \\(A\\) above is a \\(2 \\times 3\\) matrix, because it has 2 rows and 3 columns. The matrix \\(B\\) above is a \\(3 \\times 3\\) matrix, because it has 3 rows and 3 columns.\nIn general, we can denote a matrix \\(M\\) with \\(r\\) rows and \\(c\\) columns as an \\(r \\times c\\) matrix. For Notation, I will usually refer to this like \\(M_{r \\times c}\\). In this case we have \\(A_{2 \\times 3}\\) and \\(B_{3 \\times 3}\\).\nWe can denote the element in the \\(i\\)th row and \\(j\\)th column of \\(M\\) as \\(M_{ij}\\). For example, the element in the 2nd row and 3rd column of \\(B\\) is \\(B_{23} = 6\\)."
  },
  {
    "objectID": "mathref/math_2.html#special-matrices",
    "href": "mathref/math_2.html#special-matrices",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Special Matrices",
    "text": "Special Matrices\nThere are a few special matrices that we will use often. The zero matrix is a matrix where all of the elements are 0. For example, the zero matrix with 2 rows and 3 columns is:\n\\[Zero=\\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\\]\nA square matrix is a matrix where the number of rows is equal to the number of columns. For example, \\(B\\) is a square matrix.\nThe identity matrix is a square matrix where all of the elements are 0, except for the elements along the diagonal, which are 1. For example, the identity matrix with 3 rows and 3 columns is:\n\\[I_{3}=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\nFor simplicitly, we will use the subscript to denote the size of the identity matrix. For example, \\(I_{3}\\) is a 3x3 identity matrix, and \\(I_{5}\\) is a 5x5 identity matrix.\nA \\(1\\times c\\) matrix is called a row vector. Wheras a \\(r \\times 1\\) matrix is called a column vector.\nA diagonal matrix is a square matrix where all of the elements off the diagonal are 0. For example, the following matrix is a diagonal matrix:\n\\[\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix}\\]\nThe identify matrix is a special case of a diagonal matrix."
  },
  {
    "objectID": "mathref/math_2.html#matrix-operations",
    "href": "mathref/math_2.html#matrix-operations",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Matrix Operations",
    "text": "Matrix Operations\nWe can add matrices together by adding their corresponding elements. For example,\n\\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} + \\begin{bmatrix} 7 & 8 & 9 \\\\ 10 & 11 & 12 \\end{bmatrix} = \\begin{bmatrix} 8 & 10 & 12 \\\\ 14 & 16 & 18 \\end{bmatrix}\\]\nHowever, both matrices must have the same dimensions. For example, we cannot add the following matrices together:\n\\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}_{2\\times 3} + \\begin{bmatrix} 7 & 8 \\\\ 10 & 11 \\end{bmatrix}_{2\\times 2}\\]"
  },
  {
    "objectID": "mathref/math_2.html#matrix-scalar-multiplication",
    "href": "mathref/math_2.html#matrix-scalar-multiplication",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Matrix Scalar Multiplication",
    "text": "Matrix Scalar Multiplication\nWe can multiply a matrix by a scalar by multiplying each element of the matrix by that scalar. For example,\n\\[ a \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} = \\begin{bmatrix} 2a & 4a & 6a \\\\ 8a & 10a & 12a \\end{bmatrix}\\]"
  },
  {
    "objectID": "mathref/math_2.html#matrix-multiplication",
    "href": "mathref/math_2.html#matrix-multiplication",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication\nWe can multiple two matrices together by taking the dot product of each row of the first matrix with each column of the second matrix. For example:\n\\[\\begin{aligned}\n\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}_{2\\times 3} \\begin{bmatrix} 7 & 8 \\\\ 10 & 11 \\\\ 13 & 14 \\end{bmatrix}_{3\\times 2} &= \\begin{bmatrix} 1*7 + 2*10 + 3*13 & 1*8 + 2*11 + 3*14 \\\\ 4*7 + 5*10 + 6*13 & 4*8 + 5*11 + 6*14 \\end{bmatrix}_{2\\times 2} \\\\\n&= \\begin{bmatrix} 66 & 82 \\\\ 156 & 199 \\end{bmatrix}\n\\end{aligned}\n\\]\nA good way of remembering this is to follow the flow: \\(\\rightarrow \\times \\downarrow\\).\nNote that the number of columns in the first matrix must be equal to the number of rows in the second matrix. For example, we cannot multiply the following matrices together:\n\\[\\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{pmatrix}_{2\\times 3} \\begin{pmatrix} 7 & 8 \\\\ 10 & 11 \\end{pmatrix}_{2\\times 2}\\]\nIn general, given two matrixes \\(A_{a\\times b}\\) and \\(B_{c\\times d}\\), we can multiply them together if and only if \\(b=c\\). The resulting matrix will be \\(AB_{a\\times d}\\).\nSome properties of matrix multiplication:\n\nMatrix multiplication is not commutative. That is, \\(AB \\neq BA\\) in general.\nMatrix multiplication is associative. That is, \\(A(BC) = (AB)C\\).\nAny matrix multiplied by \\(I\\) is equal to itself. That is, \\(AI = IA = A\\)."
  },
  {
    "objectID": "mathref/math_2.html#transpose",
    "href": "mathref/math_2.html#transpose",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Transpose",
    "text": "Transpose\nThe transpose of a matrix is a matrix where the rows and columns are swapped. For example, if the matrix \\(A\\) is defined as:\n\\[A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\]\nthen the transpose of \\(A\\), denoted \\(A^T\\), is:\n\\[A^T = \\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}\\]\nNote that if \\(A_{a\\times b}\\), then \\(A^T_{b\\times a}\\).\nSome properties of the transpose:\n\n\\((A^T)^T = A\\)\n\\((AB)^T = B^TA^T\\)\n\\((A+B)^T = A^T + B^T\\)\n\\((aA)^T = aA^T\\)\n\\((A^T)^{-1} = (A^{-1})^T\\)"
  },
  {
    "objectID": "mathref/math_2.html#inverse",
    "href": "mathref/math_2.html#inverse",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Inverse",
    "text": "Inverse\nThe inverse of a square matrix is a matrix that, when multiplied by the original matrix (\\(A A^{-1} = I\\)), results in the identity matrix. For example, if the matrix \\(A\\) is defined as:\n\\[A = \\begin{bmatrix} 1 & 2 \\\\ 4   & 6 \\end{bmatrix}\\]\nthen the inverse of \\(A\\), denoted \\(A^{-1}\\), is:\n\\[A^{-1} = \\begin{bmatrix} -3 & 1 \\\\ 2 & -.5 \\end{bmatrix}\\]\nFor a \\(2 \\times 2\\) matrix, the inverse is defined as:\n\\[\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\\]\nIf a matrix has determinant 0, then it is not invertible.\nSome properties of the inverse:\n\n\\((A^{-1})^{-1} = A\\)\n\\((AB)^{-1} = B^{-1}A^{-1}\\)\n\\((A^T)^{-1} = (A^{-1})^T\\)\n\\((aA)^{-1} = \\frac{1}{a}A^{-1}\\)"
  },
  {
    "objectID": "mathref/math_2.html#determinant",
    "href": "mathref/math_2.html#determinant",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Determinant",
    "text": "Determinant\nThe determinant of a square matrix is a scalar value that is a function of the elements of the matrix. The determinant of a \\(2 \\times 2\\) matrix is defined as:\n\\[\\begin{vmatrix} a & b \\\\ c & d \\end{vmatrix} = ad - bc\\]\nThe determinant of a \\(3 \\times 3\\) matrix is defined as:\n\\[\\begin{vmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{vmatrix} = aei+dhc+gbf-ceg-fha-ibd\\]"
  },
  {
    "objectID": "mathref/math_2.html#rank-and-linear-independence",
    "href": "mathref/math_2.html#rank-and-linear-independence",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Rank and linear independence",
    "text": "Rank and linear independence\nThe rank of a matrix is the number of linearly independent rows or columns in the matrix. In a rectangular matrix, the rank cannot be larger than the smaller of the rows or columns.\nIf we consider each column, or rows, of a matrix as a vector, then the rank of the matrix is the number of linearly independent vectors in the matrix. If a set of vectors are not linearly independent, then one of the vectors can be expressed as a linear combination of the other vectors. For example, the following vectors are not linearly independent:\n\\[a_1 \\vec x_1 + a_2 \\vec x_2 + a_3 \\vec x_3 = 0\\]"
  },
  {
    "objectID": "mathref/math_2.html#eigenvalues-and-eigenvectors",
    "href": "mathref/math_2.html#eigenvalues-and-eigenvectors",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Eigenvalues and eigenvectors",
    "text": "Eigenvalues and eigenvectors\nThe eigenvalues and eigenvectors of a matrix are scalars and vectors that satisfy the following equation:\n\\[A \\vec x = \\lambda \\vec x\\]\nwhere \\(A\\) is a square matrix, \\(\\vec x\\) is the an eigen a vector, and \\(\\lambda\\) is a scalar. In other words, multiplying a vector by a matrix is the same as multiplying the vector by a scalar.\nThe eigenvalues of a matrix are the values of \\(\\lambda\\) that satisfy this equation. The eigenvectors of a matrix are the vectors \\(\\vec x\\) that satisfy this equation.\nThe eigenvalues and eigenvectors of a matrix can be found by solving the following equation:\n\\[det(A - \\lambda I) = 0\\]\nwhere \\(I\\) is the identity matrix.\nIf the matrix A is of dimension \\(n \\times n\\), then there are \\(n\\) eigenvalues and \\(n\\) eigenvectors."
  },
  {
    "objectID": "mathref/math_2.html#system-of-linear-equations",
    "href": "mathref/math_2.html#system-of-linear-equations",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "System of linear equations",
    "text": "System of linear equations\nA system of linear equations is a set of equations that can be expressed in the form:\n\\[\\begin{aligned}\na_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &= b_1 \\\\\na_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &= b_2 \\\\\n\\vdots \\\\\na_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nn}x_n &= b_n \\\\\n\\end{aligned}\\]\nwhere \\(a_{ij}\\) and \\(b_i\\) are constants, and \\(x_i\\) are variables.\nThis system of equations can be written in matrix form as:\n\\[A_{n\\times n}   X_{n\\times 1} =   b_{n\\times 1}\\]\nif the system has a unique solution, then the matrix \\(A\\) is invertible, and the solution is given by:\n\\[X = A^{-1}b\\]\nThus if there is no solution, then \\(A\\) is not invertible. If the determinant of \\(A\\) is 0, then \\(A\\) is not invertible."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Econometrics",
    "section": "",
    "text": "Greetings, valiant students of the realm!\nPrepare yourselves, for you stand upon the precipice of a grand and extraordinary odyssey, one that will propel you through the cosmic tapestry of knowledge and understanding. Behold, the epic adventure that awaits you: the realm of Econometrics, where the forces of knowledge converge!\nAs you step into this wondrous expedition, envision yourselves as intrepid explorers, traversing vast galaxies of data and unlocking the secrets of economic phenomena that lie hidden amidst the stars. With quills and scrolls in hand, you shall decipher the arcane languages of equations and statistical models, unraveling the very fabric of reality itself.\nLike brave knights donning their armor, you shall equip yourselves with the tools of econometric analysis, harnessing the powers of regression, causality, and inference. Through the warp and weft of datasets, you shall chart a course through treacherous terrains of multicollinearity and endogeneity, boldly navigating the uncharted territories of economic relationships.\nBut beware, dear scholars, for this adventure is not for the faint of heart. Along your path, you shall encounter formidable challenges that test your mettle and determination. Like cosmic anomalies distorting the very laws of causality, confounding variables shall seek to cloud your vision. Yet fear not, for within your arsenal lies the mighty sword of identification strategies, capable of cleaving through the veils of confusion and revealing the true essence of causation.\nAs you traverse the celestial landscapes of panel data and time series analysis, immerse yourselves in the symphony of econometric theories, their harmonies resonating across the realms of academia and policy. Delve into the arcane mysteries of heteroscedasticity, autocorrelation, and simultaneity, unearthing the hidden treasures of robustness and efficiency.\nTogether, as a fellowship of scholars, you shall brave this extraordinary journey. Engage in spirited debates, exchange insights, and sharpen your intellects as you sail upon the vast sea of econometric inquiry. Embrace the camaraderie of fellow explorers, as you forge alliances and embark upon group projects, unraveling the enigmas of econometric phenomena that defy conventional wisdom.\nAnd when you emerge from this magnificent expedition, equipped with the knowledge and skills of econometrics, you shall be heralded as true heroes of economic analysis, ready to shape the destinies of nations and unlock the frontiers of progress. The legacy you create shall reverberate throughout the annals of scholarly lore, forever remembered as the brave souls who dared to venture into the realm of discovery.\nSo, my esteemed students, with hearts aflame and minds afire, let us embark upon this epic adventure together. May your econometric quest be filled with triumphs and revelations, as you transcend the boundaries of the ordinary and ascend to the echelons of intellectual greatness. Embrace the grandeur of this cosmic odyssey and let the realm of econometrics unveil its splendor before you!\nOnward, intrepid scholars, to the boundless vistas of glory! And let the path take you as far as you can.\nF.G.R.P.A.T.\n\n\nYour browser does not support the audio element."
  },
  {
    "objectID": "imewld/chapter3.html",
    "href": "imewld/chapter3.html",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "Model:\n\\[colGPA = \\beta_0 + \\beta_1hsGPA + \\beta_2ACT + u\\]\n\nfrause gpa1, clear\nregress colgpa hsgpa act\nregress colgpa act\n\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(2, 138)       =     14.78\n       Model |  3.42365506         2  1.71182753   Prob &gt; F        =    0.0000\n    Residual |  15.9824444       138  .115814814   R-squared       =    0.1764\n-------------+----------------------------------   Adj R-squared   =    0.1645\n       Total |  19.4060994       140  .138614996   Root MSE        =    .34032\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .4534559   .0958129     4.73   0.000     .2640047    .6429071\n         act |    .009426   .0107772     0.87   0.383    -.0118838    .0307358\n       _cons |   1.286328   .3408221     3.77   0.000      .612419    1.960237\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(1, 139)       =      6.21\n       Model |  .829558811         1  .829558811   Prob &gt; F        =    0.0139\n    Residual |  18.5765406       139  .133644177   R-squared       =    0.0427\n-------------+----------------------------------   Adj R-squared   =    0.0359\n       Total |  19.4060994       140  .138614996   Root MSE        =    .36557\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         act |    .027064   .0108628     2.49   0.014     .0055862    .0485417\n       _cons |   2.402979   .2642027     9.10   0.000     1.880604    2.925355\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter3.html#example-3.1-determinants-of-college-gpa",
    "href": "imewld/chapter3.html#example-3.1-determinants-of-college-gpa",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "Model:\n\\[colGPA = \\beta_0 + \\beta_1hsGPA + \\beta_2ACT + u\\]\n\nfrause gpa1, clear\nregress colgpa hsgpa act\nregress colgpa act\n\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(2, 138)       =     14.78\n       Model |  3.42365506         2  1.71182753   Prob &gt; F        =    0.0000\n    Residual |  15.9824444       138  .115814814   R-squared       =    0.1764\n-------------+----------------------------------   Adj R-squared   =    0.1645\n       Total |  19.4060994       140  .138614996   Root MSE        =    .34032\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .4534559   .0958129     4.73   0.000     .2640047    .6429071\n         act |    .009426   .0107772     0.87   0.383    -.0118838    .0307358\n       _cons |   1.286328   .3408221     3.77   0.000      .612419    1.960237\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(1, 139)       =      6.21\n       Model |  .829558811         1  .829558811   Prob &gt; F        =    0.0139\n    Residual |  18.5765406       139  .133644177   R-squared       =    0.0427\n-------------+----------------------------------   Adj R-squared   =    0.0359\n       Total |  19.4060994       140  .138614996   Root MSE        =    .36557\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         act |    .027064   .0108628     2.49   0.014     .0055862    .0485417\n       _cons |   2.402979   .2642027     9.10   0.000     1.880604    2.925355\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter3.html#example-3.2-hourly-wage-equation",
    "href": "imewld/chapter3.html#example-3.2-hourly-wage-equation",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Example 3.2: Hourly Wage Equation",
    "text": "Example 3.2: Hourly Wage Equation\nModel:\n\\[log(wage)=\\beta_0 + \\beta_1educ + \\beta_2exper + \\beta_3tenure + u\\]\n\nfrause wage1, clear\ngen logwage = log(wage)\nreg logwage educ exper tenure\n\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(3, 522)       =     80.39\n       Model |  46.8741776         3  15.6247259   Prob &gt; F        =    0.0000\n    Residual |  101.455574       522  .194359337   R-squared       =    0.3160\n-------------+----------------------------------   Adj R-squared   =    0.3121\n       Total |  148.329751       525   .28253286   Root MSE        =    .44086\n\n------------------------------------------------------------------------------\n     logwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |    .092029   .0073299    12.56   0.000     .0776292    .1064288\n       exper |   .0041211   .0017233     2.39   0.017     .0007357    .0075065\n      tenure |   .0220672   .0030936     7.13   0.000     .0159897    .0281448\n       _cons |   .2843595   .1041904     2.73   0.007     .0796756    .4890435\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter3.html#partialling-out-interpretation-of-multiple-regression",
    "href": "imewld/chapter3.html#partialling-out-interpretation-of-multiple-regression",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Partialling Out Interpretation of Multiple Regression",
    "text": "Partialling Out Interpretation of Multiple Regression\nModel:\n\\[log(wage)=\\beta_0 + \\beta_1educ + \\beta_2exper + \\beta_3tenure + u\\]\nWe could estimate the same models with the followin:\n\\[educ=\\gamma_0 + \\gamma_1exper + \\gamma_2tenure + v\\]\n\\[log(wage)=\\beta_0 + \\beta_1 \\hat v + u\\]\n\nqui:reg logwage exper tenure\npredict logwage_res, resid\nqui:reg educ exper tenure\npredict educ_res, resid\nreg logwage_res educ_res\n\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =    158.24\n       Model |  30.6376772         1  30.6376772   Prob &gt; F        =    0.0000\n    Residual |  101.455574       524  .193617507   R-squared       =    0.2319\n-------------+----------------------------------   Adj R-squared   =    0.2305\n       Total |  132.093251       525  .251606192   Root MSE        =    .44002\n\n------------------------------------------------------------------------------\n logwage_res | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    educ_res |    .092029   .0073159    12.58   0.000     .0776568    .1064011\n       _cons |  -6.25e-10   .0191858    -0.00   1.000    -.0376905    .0376905\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter3.html#example-3.4-determinants-of-college-gpa",
    "href": "imewld/chapter3.html#example-3.4-determinants-of-college-gpa",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Example 3.4 : Determinants of College GPA",
    "text": "Example 3.4 : Determinants of College GPA\nSee example 3.1"
  },
  {
    "objectID": "imewld/chapter3.html#example-3.5-explaining-arrest-records",
    "href": "imewld/chapter3.html#example-3.5-explaining-arrest-records",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Example 3.5 : Explaining Arrest Records",
    "text": "Example 3.5 : Explaining Arrest Records\n\nfrause crime1, clear\nregress narr86 pcnv  ptime86 qemp86\n\n\n      Source |       SS           df       MS      Number of obs   =     2,725\n-------------+----------------------------------   F(3, 2721)      =     39.10\n       Model |  83.0741941         3   27.691398   Prob &gt; F        =    0.0000\n    Residual |  1927.27296     2,721  .708295833   R-squared       =    0.0413\n-------------+----------------------------------   Adj R-squared   =    0.0403\n       Total |  2010.34716     2,724  .738012906   Root MSE        =     .8416\n\n------------------------------------------------------------------------------\n      narr86 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        pcnv |  -.1499274   .0408653    -3.67   0.000    -.2300576   -.0697973\n     ptime86 |  -.0344199    .008591    -4.01   0.000    -.0512655   -.0175744\n      qemp86 |   -.104113   .0103877   -10.02   0.000    -.1244816   -.0837445\n       _cons |   .7117715   .0330066    21.56   0.000      .647051     .776492\n------------------------------------------------------------------------------\n\n\n\nregress narr86 pcnv avgsen ptime86 qemp86\n\n\n      Source |       SS           df       MS      Number of obs   =     2,725\n-------------+----------------------------------   F(4, 2720)      =     29.96\n       Model |  84.8242895         4  21.2060724   Prob &gt; F        =    0.0000\n    Residual |  1925.52287     2,720  .707912819   R-squared       =    0.0422\n-------------+----------------------------------   Adj R-squared   =    0.0408\n       Total |  2010.34716     2,724  .738012906   Root MSE        =    .84138\n\n------------------------------------------------------------------------------\n      narr86 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        pcnv |  -.1508319   .0408583    -3.69   0.000    -.2309484   -.0707154\n      avgsen |   .0074431   .0047338     1.57   0.116    -.0018392    .0167254\n     ptime86 |  -.0373908   .0087941    -4.25   0.000    -.0546345   -.0201471\n      qemp86 |   -.103341   .0103965    -9.94   0.000    -.1237268   -.0829552\n       _cons |   .7067565   .0331515    21.32   0.000     .6417519     .771761\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter3.html#variance-inflation-factors",
    "href": "imewld/chapter3.html#variance-inflation-factors",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Variance Inflation Factors",
    "text": "Variance Inflation Factors\n\nqui:regress narr86 pcnv avgsen ptime86 qemp86\nestat vif\n\n\n    Variable |       VIF       1/VIF  \n-------------+----------------------\n     ptime86 |      1.13    0.883693\n      qemp86 |      1.08    0.927081\n      avgsen |      1.06    0.942363\n        pcnv |      1.00    0.996771\n-------------+----------------------\n    Mean VIF |      1.07\n\n\n\nqui:regress pcnv avgsen ptime86 qemp86\ndisplay \"VIF for pcnv:   \" 1/(1-e(r2))\nqui:regress avgsen ptime86 qemp86 pcnv\ndisplay \"VIF for avgsen: \" 1/(1-e(r2))\n\nVIF for pcnv: 1.003239\nVIF for avgsen: 1.0611622"
  },
  {
    "objectID": "imewld/chapter3.html#example-3.7-evaluating-a-job-training-program",
    "href": "imewld/chapter3.html#example-3.7-evaluating-a-job-training-program",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Example 3.7 : Evaluating a Job Training Program",
    "text": "Example 3.7 : Evaluating a Job Training Program\n\nfrause jtrain98, clear\nregress earn98 train\n\n\n      Source |       SS           df       MS      Number of obs   =     1,130\n-------------+----------------------------------   F(1, 1128)      =     17.91\n       Model |  1054.41369         1  1054.41369   Prob &gt; F        =    0.0000\n    Residual |  66408.4778     1,128   58.872764   R-squared       =    0.0156\n-------------+----------------------------------   Adj R-squared   =    0.0148\n       Total |  67462.8915     1,129   59.754554   Root MSE        =    7.6729\n\n------------------------------------------------------------------------------\n      earn98 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       train |  -2.050053   .4844142    -4.23   0.000    -3.000507   -1.099599\n       _cons |    10.6099    .279429    37.97   0.000     10.06164    11.15816\n------------------------------------------------------------------------------\n\n\n\nregress earn98 train earn96 educ age married  \n\n\n      Source |       SS           df       MS      Number of obs   =     1,130\n-------------+----------------------------------   F(5, 1124)      =    152.99\n       Model |  27320.1797         5  5464.03593   Prob &gt; F        =    0.0000\n    Residual |  40142.7118     1,124  35.7141564   R-squared       =    0.4050\n-------------+----------------------------------   Adj R-squared   =    0.4023\n       Total |  67462.8915     1,129   59.754554   Root MSE        =    5.9761\n\n------------------------------------------------------------------------------\n      earn98 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       train |   2.410547   .4352625     5.54   0.000     1.556528    3.264565\n      earn96 |   .3725384   .0186262    20.00   0.000     .3359923    .4090845\n        educ |   .3628329    .064047     5.67   0.000     .2371678    .4884979\n         age |   -.181046    .018875    -9.59   0.000    -.2180803   -.1440118\n     married |   2.481719   .4262625     5.82   0.000      1.64536    3.318079\n       _cons |   4.667042   1.145283     4.08   0.000     2.419908    6.914176\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "data_resources.html",
    "href": "data_resources.html",
    "title": "Data-Set Handbook",
    "section": "",
    "text": "Source: L.E. Papke (1995), “Participation in and Contributions to 401(k) Pension Plans: Evidence from Plan Data,” Journal of Human Resources 30, 311-325.\nProfessor Papke, of Michigan State University, kindly provided these data. She gathered them from the Internal Revenue Service’s Form 5500 tapes.\nUsed in Text: pages 62, 76, 133, 169, 212-213, 656-657\nNotes: This data set is used in a variety of ways in the text. One additional possibility is to investigate whether the coefficients from the regression of prate on mrate, log(totemp) differ by whether the plan is a sole plan. The Chow test (see Section 7.4), and the less restrictive version that allows different intercepts, can be used.\n\n\n\nSource: A. Abadie (2003), “Semiparametric Instrumental Variable Estimation of Treatment Response Models,” Journal of Econometrics 113, 231-263.\nProfessor Abadie, now at MIT, kindly provided these data. He obtained them from the 1991 Survey of Income and Program Participation (SIPP).\nUsed in Text: pages 160-161, 178, 217-218, 258-259, 276, 292, 528\nNotes: This data set can also be used to illustrate the binary response models, probit and logit, in Chapter 17, where, say, pira (an indicator for having an individual retirement account) is the dependent variable, and e401k [the 401(k) eligibility indicator] is the key explanatory variable."
  },
  {
    "objectID": "data_resources.html#k",
    "href": "data_resources.html#k",
    "title": "Data-Set Handbook",
    "section": "",
    "text": "Source: L.E. Papke (1995), “Participation in and Contributions to 401(k) Pension Plans: Evidence from Plan Data,” Journal of Human Resources 30, 311-325.\nProfessor Papke, of Michigan State University, kindly provided these data. She gathered them from the Internal Revenue Service’s Form 5500 tapes.\nUsed in Text: pages 62, 76, 133, 169, 212-213, 656-657\nNotes: This data set is used in a variety of ways in the text. One additional possibility is to investigate whether the coefficients from the regression of prate on mrate, log(totemp) differ by whether the plan is a sole plan. The Chow test (see Section 7.4), and the less restrictive version that allows different intercepts, can be used."
  },
  {
    "objectID": "data_resources.html#ksubs",
    "href": "data_resources.html#ksubs",
    "title": "Data-Set Handbook",
    "section": "",
    "text": "Source: A. Abadie (2003), “Semiparametric Instrumental Variable Estimation of Treatment Response Models,” Journal of Econometrics 113, 231-263.\nProfessor Abadie, now at MIT, kindly provided these data. He obtained them from the 1991 Survey of Income and Program Participation (SIPP).\nUsed in Text: pages 160-161, 178, 217-218, 258-259, 276, 292, 528\nNotes: This data set can also be used to illustrate the binary response models, probit and logit, in Chapter 17, where, say, pira (an indicator for having an individual retirement account) is the dependent variable, and e401k [the 401(k) eligibility indicator] is the key explanatory variable."
  },
  {
    "objectID": "data_resources.html#admnrev",
    "href": "data_resources.html#admnrev",
    "title": "Data-Set Handbook",
    "section": "ADMNREV",
    "text": "ADMNREV\nSource: Data from the National Highway Traffic Safety Administration: “A Digest of State Alcohol-Highway Safety Related Legislation,” U.S. Department of Transportation, NHTSA. I used the third (1985), eighth (1990), and 13th (1995) editions.\nUsed in Text: not used\nNotes: This is not so much a data set as a summary of so-called “administrative per se” laws at the state level, for three different years. It could be supplemented with drunk-driving fatalities for a nice econometric analysis. In addition, the data for 2000 or later years can be added, forming the basis for a term project. Many other explanatory variables could be included. Unemployment rates, state-level tax rates on alcohol, and membership in MADD are just a few possibilities."
  },
  {
    "objectID": "data_resources.html#affairs",
    "href": "data_resources.html#affairs",
    "title": "Data-Set Handbook",
    "section": "AFFAIRS",
    "text": "AFFAIRS\nSource: R.C. Fair (1978), “A Theory of Extramarital Affairs,” Journal of Political Economy 86, 45-61, 1978.\nI collected the data from Professor Fair’s web cite at the Yale University Department of Economics. He originally obtained the data from a survey by Psychology Today.\nUsed in Text: not used\nNotes: This is an interesting data set for problem sets starting in Chapter 7. Even though naffairs (number of extramarital affairs a woman reports) is a count variable, a linear model can be used to model its conditional mean as an approximation. Or, you could ask the students to estimate a linear probability model for the binary indicator affair, equal to one of the woman reports having any extramarital affairs. One possibility is to test whether putting the single marriage rating variable, ratemarr, is enough, against the alternative that a full set of dummy variables is needed; see pages 239-240 for a similar example. This is also a good data set to illustrate Poisson regression (using naffairs) in Section 17.3 or probit and logit (using affair) in Section 17.1."
  },
  {
    "objectID": "data_resources.html#airfare",
    "href": "data_resources.html#airfare",
    "title": "Data-Set Handbook",
    "section": "AIRFARE",
    "text": "AIRFARE\nSource: Jiyoung Kwon, a former doctoral student in economics at MSU, kindly provided these data, which she obtained from the Domestic Airline Fares Consumer Report by the U.S. Department of Transportation.\nUsed in Text: pages 476, 488, 557, 557\nNotes: This data set nicely illustrates the different estimates obtained when applying pooled OLS, random effects, and fixed effects."
  },
  {
    "objectID": "data_resources.html#alcohol",
    "href": "data_resources.html#alcohol",
    "title": "Data-Set Handbook",
    "section": "ALCOHOL",
    "text": "ALCOHOL\nSource: Terza, J.V. (2002), “Alcohol Abuse and Employment: A Second Look,” Journal of Applied Econometrics 17, 393-404.\nI obtained these data from the Journal of Applied Econometrics data archive at http://qed.econ.queensu.ca/jae/.\nUsed in Text: page 600"
  },
  {
    "objectID": "data_resources.html#apple",
    "href": "data_resources.html#apple",
    "title": "Data-Set Handbook",
    "section": "APPLE",
    "text": "APPLE\nSource: These data were used in the doctoral dissertation of Jeffrey Blend, Department of Agricultural Economics, Michigan State University, 1998. The thesis was supervised by Professor Eileen van Ravensway. Drs. Blend and van Ravensway kindly provided the data, which were obtained from a telephone survey conducted by the Institute for Public Policy and Social Research at MSU.\nUsed in Text: pages 195, 217, 259-260, 598\nNotes: This data set is close to a true experimental data set because the price pairs facing a family were randomly determined. In other words, the family head was presented with prices for the eco-labeled and regular apples, and then asked how much of each kind of apple the family would buy at the given prices. As predicted by basic economics, the own price effect is negative (and strong) and the cross price effect is positive (and strong). While the main dependent variable, ecolbs, piles up at zero, estimating a linear model is still worthwhile. Interestingly, because the survey design induces a strong positive correlation between the prices of eco-labeled and regular apples, there is an omitted variable problem if either of the price variables is dropped from the demand equation. A good exam question is to show a simple regression of ecolbs on ecoprc and then a multiple regression on both prices, and ask students to decide whether the price variables must be positively or negatively correlated."
  },
  {
    "objectID": "data_resources.html#approval",
    "href": "data_resources.html#approval",
    "title": "Data-Set Handbook",
    "section": "APPROVAL",
    "text": "APPROVAL\nSource: Harbridge, L., J. Krosnick, and J.M. Wooldridge (2016), “Presidential Approval and Gas Prices: Sociotropic or Pocketbook Influence?” In New Explorations in Political Psychology, ed. J. Krosnick. New York: Psychology Press (Taylor and Francis Group)\nProfessor Harbridge kindly provided the data, of which I have used a subset.\nUsed in Text: 365, 393, 424"
  },
  {
    "objectID": "data_resources.html#athlet1",
    "href": "data_resources.html#athlet1",
    "title": "Data-Set Handbook",
    "section": "ATHLET1",
    "text": "ATHLET1\nSources:: Peterson’s Guide to Four Year Colleges, 1994 and 1995 (24th and 25th editions). Princeton University Press. Princeton, NJ.\nThe Official 1995 College Basketball Records Book, 1994, NCAA.\n1995 Information Please Sports Almanac (6th edition). Houghton Mifflin. New York, NY.\nUsed in Text: page 661\nNotes: These data were collected by Patrick Tulloch, an MSU economics major, for a term project. The “athletic success” variables are for the year prior to the enrollment and academic data. Updating these data to get a longer stretch of years, and including appearances in the “Sweet 16” NCAA basketball tournaments, would make for a more convincing analysis. With the growing popularity of women’s sports, especially basketball, an analysis that includes success in women’s athletics would be interesting."
  },
  {
    "objectID": "data_resources.html#athlet2",
    "href": "data_resources.html#athlet2",
    "title": "Data-Set Handbook",
    "section": "ATHLET2",
    "text": "ATHLET2\nSources:: Peterson’s Guide to Four Year Colleges, 1995 (25th edition). Princeton University Press.\n1995 Information Please Sports Almanac (6th edition). Houghton Mifflin. New York, NY\nUsed in Text: page 661\nNotes: These data were collected by Paul Anderson, an MSU economics major, for a term project. The score from football outcomes for natural rivals (Michigan-Michigan State, California-Stanford, Florida-Florida State, to name a few) is matched with application and academic data. The application and tuition data are for Fall 1994. Football records and scores are from 1993 football season. Extended these data to obtain a long stretch of panel data and other “natural” rivals could be very interesting."
  },
  {
    "objectID": "data_resources.html#attend",
    "href": "data_resources.html#attend",
    "title": "Data-Set Handbook",
    "section": "ATTEND",
    "text": "ATTEND\nSource: These data were collected by Professors Ronald Fisher and Carl Liedholm during a term in which they both taught principles of microeconomics at Michigan State University. Professors Fisher and Liedholm kindly gave me permission to use a random subset of their data, and their research assistant at the time, Jeffrey Guilfoyle, who completed his Ph.D. in economics at MSU, provided helpful hints.\nUsed in Text: pages 110, 146, 193-194, 216\nNotes: The attendance figures were obtained by requiring students to slide their ID cards through a magnetic card reader, under the supervision of a teaching assistant. You might have the students use final, rather than the standardized variable, so that they can see the statistical significance of each variable remains exactly the same. The standardized variable is used only so that the coefficients measure effects in terms of standard deviations from the average score."
  },
  {
    "objectID": "data_resources.html#audit",
    "href": "data_resources.html#audit",
    "title": "Data-Set Handbook",
    "section": "AUDIT",
    "text": "AUDIT\nSource: These data come from a 1988 Urban Institute audit study in the Washington, D.C. area. I obtained them from the article “The Urban Institute Audit Studies: Their Methods and Findings,” by James J. Heckman and Peter Siegelman. In Fix, M. and Struyk, R., eds., Clear and Convincing Evidence: Measurement of Discrimination in America. Washington, D.C.: Urban Institute Press, 1993, 187-258.\nUsed in Text: pages 732, 738, 741"
  },
  {
    "objectID": "data_resources.html#barium",
    "href": "data_resources.html#barium",
    "title": "Data-Set Handbook",
    "section": "BARIUM",
    "text": "BARIUM\nSource: C.M. Krupp and P.S. Pollard (1999), “Market Responses to Antidumpting Laws: Some Evidence from the U.S. Chemical Industry,” Canadian Journal of Economics 29, 199-227.\nDr. Krupp kindly provided the data. They are monthly data covering February 1978 through December 1988.\nUsed in Text: pages 349-350, 359, 363, 407, 410-411, 422, 424, 631-632, 639\nNote: Rather than just having intercept shifts for the different regimes, one could conduct a full Chow test across the different regimes."
  },
  {
    "objectID": "data_resources.html#beauty",
    "href": "data_resources.html#beauty",
    "title": "Data-Set Handbook",
    "section": "BEAUTY",
    "text": "BEAUTY\nSource: Hamermesh, D.S. and J.E. Biddle (1994), “Beauty and the Labor Market,” American Economic Review 84, 1174-1194.\nProfessor Hamermesh kindly provided me with the data. For manageability, I have included only a subset of the variables, which results in somewhat larger sample sizes than reported for the regressions in the Hamermesh and Biddle paper.\nUsed in Text: pages 231, 259"
  },
  {
    "objectID": "data_resources.html#benefits",
    "href": "data_resources.html#benefits",
    "title": "Data-Set Handbook",
    "section": "BENEFITS",
    "text": "BENEFITS\nSource: I collected these data from the old Michigan Department of Education web site, which used to have an annual Michigan Schools Report. I used data on most elementary schools in the state of Michigan for 1993. I dropped some schools that had suspicious-looking data.\nUsed in Text: 218\nNotes: This is an elementary school-level version of MEAP93, which contains data for high schools."
  },
  {
    "objectID": "data_resources.html#big9salary",
    "href": "data_resources.html#big9salary",
    "title": "Data-Set Handbook",
    "section": "BIG9SALARY",
    "text": "BIG9SALARY\nSource: O. Baser and E. Pema (2003), “The Return of Publications for Economics Faculty,” Economics Bulletin 1, 1-13.\nProfessors Baser and Pema kindly provided the data.\nUsed in Text: not used\nNotes: This is an unbalanced panel data set in the sense that as many as three years of data are available for each faculty member but where some have fewer than three years. It is not clear that something like a fixed effects or first differencing analysis makes sense: in effect, approaches that remove the heterogeneity control for too much by controlling for unobserved heterogeneity – which, in this case, includes faculty intelligence, talent, and motivation. Presumably, these factors enter into the publication index. It is hard to think we want to hold the main factors driving productivity fixed when trying to measure the effect of productivity on salary. Pooled OLS regression with “cluster robust” standard errors seems more natural.\nOn the other hand, if we want to measure the return to having a degree from a top 20 Ph.D. program then we would want to control for factors that cause selection into a top 20 program. Unfortunately, this variable does not change over time, and so FD and FE are not applicable. One could include the top 20 dummy variable in a correlated random effects analysis."
  },
  {
    "objectID": "data_resources.html#bwght",
    "href": "data_resources.html#bwght",
    "title": "Data-Set Handbook",
    "section": "BWGHT",
    "text": "BWGHT\nSource: J. Mullahy (1997), “Instrumental-Variable Estimation of Count Data Models: Applications to Models of Cigarette Smoking Behavior,” Review of Economics and Statistics 79, 596-593.\nProfessor Mullahy kindly provided the data. He obtained them from the 1988 National Health Interview Survey.\nUsed in Text: pages 16, 58, 109, 145, 172, 178, 181-182, 251-252, 504"
  },
  {
    "objectID": "data_resources.html#bwght2",
    "href": "data_resources.html#bwght2",
    "title": "Data-Set Handbook",
    "section": "BWGHT2",
    "text": "BWGHT2\nSource: Dr. Zhehui Luo, a professor of epidemiology and biostatistics at MSU, kindly provided these data. She obtained them from state files linking birth and infant death certificates, and from the National Center for Health Statistics natality and mortality data.\nUsed in Text: pages 178, 217\nNotes: There are many possibilities with this data set. In addition to number of prenatal visits, smoking and alcohol consumption (during pregnancy) are included as explanatory variables. These can be added to equations of the kind found in Exercise C6.10. In addition, the one- and five-minute APGAR scores are included. These are measures of the well being of infants just after birth. An interesting feature of the score is that it is bounded between zero and 10, making a linear model less than ideal. Still, a linear model would be informative, and you might ask students about predicted values less than zero or greater than 10."
  },
  {
    "objectID": "data_resources.html#campus",
    "href": "data_resources.html#campus",
    "title": "Data-Set Handbook",
    "section": "CAMPUS",
    "text": "CAMPUS\nSource: These data were collected by Daniel Martin, a former MSU undergraduate, for a final project. They come from the FBI Uniform Crime Reports and are for the year 1992.\nUsed in Text: pages 128-129\nNotes: Colleges and universities are now required to provide much better, more detailed crime data. A very rich data set can now be obtained, even a panel data set for colleges across different years. Statistics on male/female ratios, fraction of men/women in fraternities or sororities, policy variables – such as a “safe house” for women on campus, as was started at MSU in 1994 – could be added as explanatory variables. The crime rate in the host town would be a good control."
  },
  {
    "objectID": "data_resources.html#card",
    "href": "data_resources.html#card",
    "title": "Data-Set Handbook",
    "section": "CARD",
    "text": "CARD\nSource: D. Card (1995), “Using Geographic Variation in College Proximity to Estimate the Return to Schooling,” in Aspects of Labour Market Behavior: Essays in Honour of John Vanderkamp. Ed. L.N. Christophides, E.K. Grant, and R. Swidinsky, 201-222. Toronto: University of Toronto Press.\nProfessor Card kindly provided these data.\nUsed in Text: pages 507-508, 527\nNotes: Computer Exercise C15.3 is important for analyzing these data. There, it is shown that the instrumental variable, nearc4, is actually correlated with IQ, at least for the subset of men for which an IQ score is reported. However, the correlation between nearc4 and IQ, once the other explanatory variables are netted out, is arguably zero. (At least, it is not statistically different from zero.) In other words, nearc4 fails the exogeneity requirement in a simple linear model but it passes – at least using the crude test described above – if controls are added to the wage equation.\nFor a more advanced course, a nice extension of Card’s analysis is to allow the return to education to differ by race. A relatively simple extension is to include blackeduc as an additional explanatory variable; its natural instrument is blacknearc4."
  },
  {
    "objectID": "data_resources.html#catholic",
    "href": "data_resources.html#catholic",
    "title": "Data-Set Handbook",
    "section": "CATHOLIC",
    "text": "CATHOLIC\nSource: Altonji, J.G., T.E. Elder, and C.R. Taber (2005), “An Evaluation of Instrumental Variable Strategies for Estimating the Effects of Catholic Schooling,” Journal of Human Resources 40, 791-821.\nProfessor Elder kindly provided a subset of the data, with some variables stripped away for confidentiality reasons.\nUsed in Text: pages 260-261, 530\n\nCEMENT\nSource: J. Shea (1993), “The Input-Output Approach to Instrument Selection,” Journal of Business and Economic Statistics 11, 145-156.\nProfessor Shea kindly provided these data.\nUsed in Text: pages 556\nNotes: Compared with Shea’s analysis, the producer price index (PPI) for fuels and power has been replaced with the PPI for petroleum. The data are monthly and have not been seasonally adjusted."
  },
  {
    "objectID": "data_resources.html#census2000",
    "href": "data_resources.html#census2000",
    "title": "Data-Set Handbook",
    "section": "CENSUS2000",
    "text": "CENSUS2000\nSource: Obtained from the United States Census Bureau by Professor Alberto Abadie of the Harvard Kennedy School of Government.\nProfessor Abadie kindly provided the data.\nUsed in Text: pages 485"
  },
  {
    "objectID": "data_resources.html#ceosal1",
    "href": "data_resources.html#ceosal1",
    "title": "Data-Set Handbook",
    "section": "CEOSAL1",
    "text": "CEOSAL1\nSource: I took a random sample of data reported in the May 6, 1991 issue of Businessweek.\nUsed in Text: pages 29, 32-33, 35, 154, 211, 252-253, 257, 662\nNotes: This kind of data collection is relatively easy for students just learning data analysis, and the findings can be interesting. A good term project is to have students collect a similar data set using a more recent issue of Businessweek, and to find additional variables that might explain differences in CEO compensation. My impression is that the public is still interested in CEO compensation.\nAn interesting question is whether the list of explanatory variables included in this data set now explain less of the variation in log(salary) than they used to."
  },
  {
    "objectID": "data_resources.html#ceosal2",
    "href": "data_resources.html#ceosal2",
    "title": "Data-Set Handbook",
    "section": "CEOSAL2",
    "text": "CEOSAL2\nSource: See CEOSAL1\nUsed in Text: pages 62, 110, 154, 207, 324, 662\nNotes: Compared with CEOSAL1, in this CEO data set more information about the CEO, rather than about the company, is included."
  },
  {
    "objectID": "data_resources.html#charity",
    "href": "data_resources.html#charity",
    "title": "Data-Set Handbook",
    "section": "CHARITY",
    "text": "CHARITY\nSource: P.H. Franses and R. Paap (2001), Quantitative Models in Marketing Research. Cambridge: Cambridge University Press.\nProfessor Franses kindly provided the data.\nUsed in Text: pages 63, 111, 260, 599\nNotes: This data set can be used to illustrate probit and Tobit models, and to study the linear approximations to them."
  },
  {
    "objectID": "data_resources.html#consump",
    "href": "data_resources.html#consump",
    "title": "Data-Set Handbook",
    "section": "CONSUMP",
    "text": "CONSUMP\nSource: I collected these data from the 1997 Economic Report of the President. Specifically, the data come from Tables B71, B15, B29, and B32.\nUsed in Text: pages 363-364, 391, 422, 548-549, 555, 640\nNotes: For a student interested in time series methods, updating this data set and using it in a manner similar to that in the text could be acceptable as a final project."
  },
  {
    "objectID": "data_resources.html#corn",
    "href": "data_resources.html#corn",
    "title": "Data-Set Handbook",
    "section": "CORN",
    "text": "CORN\nSource: G.E. Battese, R.M. Harter, and W.A. Fuller (1988), “An Error-Components Model for Prediction of County Crop Areas Using Survey and Satellite Data,” Journal of the American Statistical Association 83, 28-36.\nThis small data set is reported in the article.\nUsed in Text: pages 745\nNotes: You could use these data to illustrate simple regression when the population intercept should be zero: no corn pixels should predict no corn planted. The same can be done with the soybean measures in the data set."
  },
  {
    "objectID": "data_resources.html#countymurders",
    "href": "data_resources.html#countymurders",
    "title": "Data-Set Handbook",
    "section": "COUNTYMURDERS",
    "text": "COUNTYMURDERS\nSource: Compiled by J. Monroe Gamble for a Summer Research Opportunities Program (SROP) at Michigan State University, Summer 2014. Monroe obtained data from the U.S. Census Bureau, the FBI Uniform Crime Reports, and the Death Penalty Information Center.\nUsed in Text: pages 17, 64, 458, 490"
  },
  {
    "objectID": "data_resources.html#cps78_85",
    "href": "data_resources.html#cps78_85",
    "title": "Data-Set Handbook",
    "section": "CPS78_85",
    "text": "CPS78_85\nSource: Professor Henry Farber, now at Princeton University, compiled these data from the 1978 and 1985 Current Population Surveys. Professor Farber kindly provided these data when we were colleagues at MIT.\nUsed in Text: pages 427, 429-430, 454\nNotes: Obtaining more recent data from the CPS allows one to track, over a long period of time, the changes in the return to education, the gender gap, black-white wage differentials, and the union wage premium."
  },
  {
    "objectID": "data_resources.html#cps91",
    "href": "data_resources.html#cps91",
    "title": "Data-Set Handbook",
    "section": "CPS91",
    "text": "CPS91\nSource: Professor Daniel Hamermesh, at the University of Texas, compiled these data from the May 1991 Current Population Survey. Professor Hamermesh kindly provided these data.\nUsed in Text: page 599\nNotes: This is much bigger than the other CPS data sets even though the sample is restricted to married women. (CPS91 contains many more observations than MROZ, too.) In addition to the usual human capital variables for the women in the sample, we have information on the husband. Therefore, we can estimate a labor supply function as in Chapter 16, although the validity of potential experience as an IV for log(wage) is questionable. (MROZ contains an actual experience variable.) Perhaps more convincing is to add hours to the wage offer equation, and instrument hours with indicators for young and old children. This data set also contains a union membership indicator.\nThe web site for the National Bureau of Economic Research makes it very easy now to download CPS data files in a variety of formats. Go to http://www.nber.org/data/cps_basic.html."
  },
  {
    "objectID": "data_resources.html#crime1",
    "href": "data_resources.html#crime1",
    "title": "Data-Set Handbook",
    "section": "CRIME1",
    "text": "CRIME1\nSource: J. Grogger (1991), “Certainty vs. Severity of Punishment,” Economic Inquiry 29, 297-309.\nProfessor Grogger kindly provided a subset of the data he used in his article.\nUsed in Text: pages 78, 169, 174, 243, 268, 291, 295-296, 581-582, 597"
  },
  {
    "objectID": "data_resources.html#crime2",
    "href": "data_resources.html#crime2",
    "title": "Data-Set Handbook",
    "section": "CRIME2",
    "text": "CRIME2\nSource: These data were collected by David Dicicco, a former MSU undergraduate, for a final project. They came from various issues of the County and City Data Book, and are for the years 1982 and 1985. Unfortunately, I do not have the list of cities.\nUsed in Text: pages 303-304, 439-440\nNotes: Very rich crime data sets, at the county, or even city, level, can be collected using the FBI’s Uniform Crime Reports. These data can be matched up with demographic and economic data, at least for census years. The County and City Data Book contains a variety of statistics, but the years do not always match up. These data sets can be used investigate issues such as the effects of casinos on city or county crime rates."
  },
  {
    "objectID": "data_resources.html#crime3",
    "href": "data_resources.html#crime3",
    "title": "Data-Set Handbook",
    "section": "CRIME3:",
    "text": "CRIME3:\nSource: E. Eide (1994), Economics of Crime: Deterrence of the Rational Offender. Amsterdam: North Holland. The data come from Tables A3 and A6.\nUsed in Text: pages 443-444, 455\nNotes: These data are for the years 1972 and 1978 for 53 police districts in Norway. Much larger data sets for more years can be obtained for the United States, although a measure of the “clear-up” rate is needed."
  },
  {
    "objectID": "data_resources.html#crime4",
    "href": "data_resources.html#crime4",
    "title": "Data-Set Handbook",
    "section": "CRIME4",
    "text": "CRIME4\nSource: From C. Cornwell and W. Trumball (1994), “Estimating the Economic Model of Crime with Panel Data,” Review of Economics and Statistics 76, 360-366.\nProfessor Cornwell kindly provided the data.\nUsed in Text: pages 449-450, 456, 486, 556\nNotes: Computer Exercise C16.7 shows that variables that might seem to be good instrumental variable candidates are not always so good, especially after applying a transformation such as differencing across time. You could have the students do an IV analysis for just, say, 1987."
  },
  {
    "objectID": "data_resources.html#discrim",
    "href": "data_resources.html#discrim",
    "title": "Data-Set Handbook",
    "section": "DISCRIM",
    "text": "DISCRIM\nSource: K. Graddy (1997), “Do Fast-Food Chains Price Discriminate on the Race and Income Characteristics of an Area?” Journal of Business and Economic Statistics 15, 391-401.\nProfessor Graddy kindly provided the data set.\nUsed in Text: pages 111, 161, 663\nNotes: If you want to assign a common final project, this would be a good data set. There are many possible dependent variables, namely, prices of various fast-food items. The key variable is the fraction of the population that is black, along with controls for poverty, income, housing values, and so on. These data were also used in a famous study by David Card and Alan Krueger on estimation of minimum wage effects on employment. See the book by Card and Krueger, Myth and Measurement, 1997, Princeton University Press, for a detailed analysis."
  },
  {
    "objectID": "data_resources.html#driving",
    "href": "data_resources.html#driving",
    "title": "Data-Set Handbook",
    "section": "DRIVING",
    "text": "DRIVING\nSource: Freeman, D.G. (2007), “Drunk Driving Legislation and Traffic Fatalities: New Evidence on BAC 08 Laws,” Contemporary Economic Policy 25, 293–308.\nProfessor Freeman kindly provided the data.\nUsed in Text: page 489; see also the general discussion on page 659\nNotes: Several more years of data are now available and may further shed light on the effectiveness of several traffic laws."
  },
  {
    "objectID": "data_resources.html#earns",
    "href": "data_resources.html#earns",
    "title": "Data-Set Handbook",
    "section": "EARNS",
    "text": "EARNS\nSource: Economic Report of the President, 1989, Table B47. The data are for the non-farm business sector.\nUsed in Text: pages 382, 390\nNotes: These data could be usefully updated, but changes in reporting conventions in more recent ERPs may make that difficult."
  },
  {
    "objectID": "data_resources.html#econmath",
    "href": "data_resources.html#econmath",
    "title": "Data-Set Handbook",
    "section": "ECONMATH",
    "text": "ECONMATH\nSource: Compiled by Professor Charles Ballard, Michigan State University Department of Economics.\nProfessor Ballard kindly provided the data.\nUsed in Text: 162, 177"
  },
  {
    "objectID": "data_resources.html#elem94_95",
    "href": "data_resources.html#elem94_95",
    "title": "Data-Set Handbook",
    "section": "ELEM94_95",
    "text": "ELEM94_95\nSource: Culled from a panel data set used by Professor Leslie Papke in her paper “The Effects of Spending on Test Pass Rates: Evidence from Michigan” (2005), Journal of Public Economics 89, 821-839.\nUsed in Text: pages 161, 330-331\nNotes: Starting in 1995, the Michigan Department of Education stopped reporting average teacher benefits along with average salary. This data set includes both variables, at the school level, and can be used to study the salary-benefits tradeoff, as in Chapter 4. There are a few suspicious benefits/salary ratios, and so this data set makes a good illustration of the impact of outliers in Chapter 9."
  },
  {
    "objectID": "data_resources.html#expendshares",
    "href": "data_resources.html#expendshares",
    "title": "Data-Set Handbook",
    "section": "EXPENDSHARES",
    "text": "EXPENDSHARES\nSource: Blundell, R., A. Duncan, and K. Pendakur (1998), “Semiparametric Estimation and Consumer Demand,” Journal of Applied Econometrics 13, 435-461.\nI obtained these data from the Journal of Applied Econometrics data archive at http://qed.econ.queensu.ca/jae/.\nUsed in Text: pages 557-558\nNotes: The dependent variables in this data set – the expenditure shares – are necessarily bounded between zero and one. The linear model is at best an approximation, but the usual IV estimator likely gives good estimates of the average partial effects."
  },
  {
    "objectID": "data_resources.html#engin",
    "href": "data_resources.html#engin",
    "title": "Data-Set Handbook",
    "section": "ENGIN",
    "text": "ENGIN\nSource: Thada Chaisawangwong, a former graduate student at MSU, obtained these data for a term project in applied econometrics. They come from the Material Requirement Planning Survey carried out in Thailand during 1998.\nUsed in Text: not used\nNotes: This is a nice change of pace from wage data sets for the United States. These data are for engineers in Thailand, and represents a more homogeneous group than data sets that consist of people across a variety of occupations. Plus, the starting salary is also provided in the data set, so factors affecting wage growth – and not just wage levels at a given point in time – can be studied. This is a good data set for a common term project that tests basic understanding of multiple regression and the interpretation of models with a logarithm for a dependent variable."
  },
  {
    "objectID": "data_resources.html#ezanders",
    "href": "data_resources.html#ezanders",
    "title": "Data-Set Handbook",
    "section": "EZANDERS",
    "text": "EZANDERS\nSource: L.E. Papke (1994), “Tax Policy and Urban Development: Evidence from the Indiana Enterprise Zone Program,” Journal of Public Economics 54, 37-49.\nProfessor Papke kindly provided these data.\nUsed in Text: page 363\nNotes: These are actually monthly unemployment claims for the Anderson enterprise zone. Papke used annualized data, across many zones and non-zones, in her original analysis."
  },
  {
    "objectID": "data_resources.html#ezunem",
    "href": "data_resources.html#ezunem",
    "title": "Data-Set Handbook",
    "section": "EZUNEM",
    "text": "EZUNEM\nSource: See EZANDERS\nUsed in Text: pages 449, 486-487\nNotes: A very good project is to have students analyze enterprise, empowerment, renaissance, or opportunity zone policies in their home states. Many states now have such programs. Or, there are also national programs. A few years of panel data straddling periods of zone designation, at the city or zip code level, could make a nice study."
  },
  {
    "objectID": "data_resources.html#fair",
    "href": "data_resources.html#fair",
    "title": "Data-Set Handbook",
    "section": "FAIR",
    "text": "FAIR\nSource: R.C. Fair (1996), “Econometrics and Presidential Elections,” Journal of Economic Perspectives 10, 89-102.\nThe data set is provided in the article.\nUsed in Text: pages 350-351, 420, 422\nNotes: An updated version of this data set, through the 2004 election, is available at Professor Fair’s web site at Yale University: http://fairmodel.econ.yale.edu/rayfair/pdf/2001b.htm. Students might want to try their own hands at predicting the most recent election outcome, but they should be restricted to no more than a handful of explanatory variables because of the small sample size."
  },
  {
    "objectID": "data_resources.html#fertil1",
    "href": "data_resources.html#fertil1",
    "title": "Data-Set Handbook",
    "section": "FERTIL1",
    "text": "FERTIL1\nSource: W. Sander, “The Effect of Women’s Schooling on Fertility,” Economics Letters 40, 229-233.\nProfessor Sander kindly provided the data, which are a subset of what he used in his article. He compiled the data from various years of the National Opinion Resource Center’s General Social Survey.\nUsed in Text: pages 428-429, 453, 521, 597, 646\nNotes: (1) Much more recent data can be obtained from the National Opinion Research Center website, http://www.norc.org/GSS+Website/Download/. Very rich pooled cross sections can be constructed to study a variety of issues – not just changes in fertility over time.\n\nIt would be interesting to analyze a similar data set for a developing country, especially where efforts have been made to emphasize birth control. Some measure of access to birth control could be useful if it varied by region. Sometimes, one can find policy changes in the advertisement or availability of contraceptives."
  },
  {
    "objectID": "data_resources.html#fertil2",
    "href": "data_resources.html#fertil2",
    "title": "Data-Set Handbook",
    "section": "FERTIL2",
    "text": "FERTIL2\nSource: These data were obtained by James Heakins, a former MSU undergraduate, for a term project. They come from Botswana’s 1988 Demographic and Health Survey.\nUsed in Text: page 526-527 Notes: Currently, this data set is used only in one computer exercise. Since the dependent variable of interest – number of living children or number of children every born – is a count variable, the Poisson regression model discussed in Chapter 17 can be used. However, some care is required to combine Poisson regression with an endogenous explanatory variable (educ). I refer you to Chapter 19 of my book Econometric Analysis of Cross Section and Panel Data. Even in the context of linear models, much can be done beyond Computer Exercise C15.2. At a minimum, the binary indicators for various religions can be added as controls. One might also interact the schooling variable, educ, with some of the exogenous explanatory variables."
  },
  {
    "objectID": "data_resources.html#fertil3",
    "href": "data_resources.html#fertil3",
    "title": "Data-Set Handbook",
    "section": "FERTIL3",
    "text": "FERTIL3\nSource: L.A. Whittington, J. Alm, and H.E. Peters (1990), “Fertility and the Personal Exemption: Implicit Pronatalist Policy in the United States,” American Economic Review 80, 545-556.\nThe data are given in the article.\nUsed in Text: pages 346-347, 355, 362, 363, 363, 381, 384-385, 421, 634, 639-640"
  },
  {
    "objectID": "data_resources.html#fish",
    "href": "data_resources.html#fish",
    "title": "Data-Set Handbook",
    "section": "FISH",
    "text": "FISH\nSource: K Graddy (1995), “Testing for Imperfect Competition at the Fulton Fish Market,” RAND Journal of Economics 26, 75-92.\nProfessor Graddy’s collaborator on a later paper, Professor Joshua Angrist at MIT, kindly provided me with these data.\nUsed in Text: pages 422-423, 556-557\nNotes: This is a nice example of how to go about finding exogenous variables to use as instrumental variables. Often, weather conditions can be assumed to affect supply while having a negligible effect on demand. If so, the weather variables are valid instrumental variables for price in the demand equation. It is a simple matter to test whether prices vary with weather conditions by estimating the reduced form for price."
  },
  {
    "objectID": "data_resources.html#fringe",
    "href": "data_resources.html#fringe",
    "title": "Data-Set Handbook",
    "section": "FRINGE",
    "text": "FRINGE\nSource: F. Vella (1993), “A Simple Estimator for Simultaneous Models with Censored Endogenous Regressors,” International Economic Review 34, 441-457.\nProfessor Vella kindly provided the data.\nUsed in Text: page 596-597\nNotes: Currently, this data set is used in only one Computer Exercise – to illustrate the Tobit model. It can be used much earlier. First, one could just ignore the pileup at zero and use a linear model where any of the hourly benefit measures is the dependent variable. Another possibility is to use this data set for a problem set in Chapter 4, after students have read Example 4.10. That example, which uses teacher salary/benefit data at the school level, finds the expected tradeoff, although it appears to less than one-to-one. By contrast, if you do a similar analysis with FRINGE, you will not find a tradeoff. A positive coefficient on the benefit/salary ratio is not too surprising because we probably cannot control for enough factors, especially when looking across different occupations. The Michigan school-level data is more aggregated than one would like, but it does restrict attention to a more homogeneous group: high school teachers in Michigan."
  },
  {
    "objectID": "data_resources.html#gpa1",
    "href": "data_resources.html#gpa1",
    "title": "Data-Set Handbook",
    "section": "GPA1",
    "text": "GPA1\nSource: Christopher Lemmon, a former MSU undergraduate, collected these data from a survey he took of MSU students in Fall 1994.\nUsed in Text: pages 72, 74, 77, 113. 127. 155. 162. 225. 256. 286. 291\nNotes: This is a nice example of how students can obtain an original data set by focusing locally and carefully composing a survey."
  },
  {
    "objectID": "data_resources.html#gpa2",
    "href": "data_resources.html#gpa2",
    "title": "Data-Set Handbook",
    "section": "GPA2",
    "text": "GPA2\nSource: For confidentiality reasons, I cannot provide the source of these data. I can say that they come from a midsize research university that also supports men’s and women’s athletics at the Division I level.\nUsed in Text: pages 104, 178, 202-203, 204-205, 215, 252, 256-257"
  },
  {
    "objectID": "data_resources.html#gpa3",
    "href": "data_resources.html#gpa3",
    "title": "Data-Set Handbook",
    "section": "GPA3",
    "text": "GPA3\nSource: See GPA2\nUsed in Text: pages 237-238, 266-267, 287-288, 444, 455"
  },
  {
    "objectID": "data_resources.html#happiness",
    "href": "data_resources.html#happiness",
    "title": "Data-Set Handbook",
    "section": "HAPPINESS",
    "text": "HAPPINESS\nSource: Subset of data collected by Kevin Williams for a McNair Scholars project in Summer 2008 at Michigan State University. The data come from several waves of the General Social Survey, and is therefore a pooled cross sectional data set. Professor Williams, now at Yale University, kindly provided the data.\nUsed in Text: not used\nNotes: This data set can be used to estimate models of self-reported “happiness,” including studying whether the effects of certain variables – such as education, gender, race, and having children –changed in importance from the mid-1990s to the mid-2000s. For a similar example, see how FERTIL1 is used in Example 13.1 in the text."
  },
  {
    "objectID": "data_resources.html#hprice1",
    "href": "data_resources.html#hprice1",
    "title": "Data-Set Handbook",
    "section": "HPRICE1",
    "text": "HPRICE1\nSource: Collected from the real estate pages of the Boston Globe during 1990. These homes sold in the Boston, MA area.\nUsed in Text: pages 109-110, 148-149, 155-156, 160, 205, 216, 226-227, 270-271, 273, 290, 297-298\nNotes: Typically, it is very easy to obtain data on selling prices and characteristics of homes, using publicly available databases. It is interesting to match the information on houses with other information – such as local crime rates, quality of the local schools, pollution levels, and so on – and estimate the effects of such variables on housing prices."
  },
  {
    "objectID": "data_resources.html#hprice2",
    "href": "data_resources.html#hprice2",
    "title": "Data-Set Handbook",
    "section": "HPRICE2",
    "text": "HPRICE2\nSource: D. Harrison and D.L. Rubinfeld (1978), “Hedonic Housing Prices and the Demand for Clean Air,” by Harrison, D. and D.L.Rubinfeld, Journal of Environmental Economics and Management 5, 81-102.\nDiego Garcia, a former Ph.D. student in economics at MIT, kindly provided these data, which he obtained from the book Regression Diagnostics: Identifying Influential Data and Sources: of Collinearity, by D.A. Belsey, E. Kuh, and R. Welsch, 1990. New York: Wiley.\nUsed in Text: pages 106, 130, 185, 186-187, 190-191\nNotes: The census contains rich information on variables such as median housing prices, median income levels, average family size, and so on, for fairly small geographical areas. If such data can be merged with pollution data, one can update the Harrison and Rubinfeld study. Presumably, this has been done in academic journals."
  },
  {
    "objectID": "data_resources.html#hseinv",
    "href": "data_resources.html#hseinv",
    "title": "Data-Set Handbook",
    "section": "HSEINV",
    "text": "HSEINV\nSource: D. McFadden (1994), “Demographics, the Housing Market, and the Welfare of the Elderly,” in D.A. Wise (ed.), Studies in the Economics of Aging. Chicago: University of Chicago Press, 225-285.\nThe data are contained in the article.\nUsed in Text: pages 354-355, 358, 390, 609-610, 638, 783"
  },
  {
    "objectID": "data_resources.html#htv",
    "href": "data_resources.html#htv",
    "title": "Data-Set Handbook",
    "section": "HTV",
    "text": "HTV\nSource: J.J. Heckman, J.L. Tobias, and E. Vytlacil (2003), “Simple Estimators for Treatment Parameters in a Latent-Variable Framework,” Review of Economics and Statistics 85, 748-755.\nProfessor Tobias kindly provided the data, which were obtained from the 1991 National Longitudinal Survey of Youth. All people in the sample are males age 26 to 34. For confidentiality reasons, I have included only a subset of the variables used by the authors.\nUsed in Text: pages 529, 599-600\nNotes: Because an ability measure is included in this data set, it can be used as another illustration of including proxy variables in regression models. See Chapter 9. Also, one can try the IV procedure with the ability measure included as an exogenous explanatory variable."
  },
  {
    "objectID": "data_resources.html#infmrt",
    "href": "data_resources.html#infmrt",
    "title": "Data-Set Handbook",
    "section": "INFMRT",
    "text": "INFMRT\nSource: Statistical Abstract of the United States, 1990 and 1994. (For example, the infant mortality rates come from Table 113 in 1990 and Table 123 in 1994.)\nUsed in Text: pages 320-321, 328\nNotes: An interesting exercise is to add the percentage of the population on AFDC (afdcper) to the infant mortality equation. Pooled OLS and first differencing can give very different estimates. Adding the years 1998 and 2002 and applying fixed effects seems natural. Intervening years can be added, too, although variation in the key variables from year to year might be minimal."
  },
  {
    "objectID": "data_resources.html#injury",
    "href": "data_resources.html#injury",
    "title": "Data-Set Handbook",
    "section": "INJURY",
    "text": "INJURY\nSource: B.D. Meyer, W.K. Viscusi, and D.L. Durbin (1995), “Workers’ Compensation and Injury Duration: Evidence from a Natural Experiment,” American Economic Review 85, 322-340.\nProfessor Meyer kindly provided the data.\nUsed in Text: pages 435-436, 453\nNotes: This data set also can be used to illustrate the Chow test in Chapter 7. In particular, students can test whether the regression functions differ between Kentucky and Michigan. Or, allowing for different intercepts for the two states, do the slopes differ? A good lesson from this example is that a small R-squared is compatible with the ability to estimate the effects of a policy. Of course, for the Michigan data, which has a smaller sample size, the estimated effect is much less precise (but of virtually identical magnitude)."
  },
  {
    "objectID": "data_resources.html#intdef",
    "href": "data_resources.html#intdef",
    "title": "Data-Set Handbook",
    "section": "INTDEF",
    "text": "INTDEF\nSource: Economic Report of the President, 2004, Tables B64, B73, and B79.\nUsed in Text: pages 345, 363, 415, 527"
  },
  {
    "objectID": "data_resources.html#intqrt",
    "href": "data_resources.html#intqrt",
    "title": "Data-Set Handbook",
    "section": "INTQRT",
    "text": "INTQRT\nSource: From Salomon Brothers, Analytical Record of Yields and Yield Spreads, 1990. The folks at Salomon Brothers kindly provided the Record at no charge when I was an assistant professor at MIT.\nUsed in Text: pages 388-389, 612, 617, 620, 621, 639, 640\nNotes: A nice feature of the Salomon Brothers data is that the interest rates are not averaged over a month or quarter – they are end-of-month or end-of-quarter rates. Asset pricing theories apply to such “point-sampled” data, and not to averages over a period. Most other sources report monthly or quarterly averages. This is a good data set to update and test whether current data are more or less supportive of basic asset pricing theories."
  },
  {
    "objectID": "data_resources.html#inven",
    "href": "data_resources.html#inven",
    "title": "Data-Set Handbook",
    "section": "INVEN",
    "text": "INVEN\nSource: Economic Report of the President, 1997, Tables B4, B20, B61, and B71.\nUsed in Text: pages 391, 421-422, 423, 614, 783"
  },
  {
    "objectID": "data_resources.html#jtrain",
    "href": "data_resources.html#jtrain",
    "title": "Data-Set Handbook",
    "section": "JTRAIN",
    "text": "JTRAIN\nSource: H. Holzer, R. Block, M. Cheatham, and J. Knott (1993), “Are Training Subsidies Effective? The Michigan Experience,” Industrial and Labor Relations Review 46, 625-636.\nThe authors kindly provided the data.\nUsed in Text: pages 133-134, 156, 226, 244-245, 328, 444-446, 456, 464-465, 469, 486, 521-522, 730-731, 740-741, 742"
  },
  {
    "objectID": "data_resources.html#jtrain2",
    "href": "data_resources.html#jtrain2",
    "title": "Data-Set Handbook",
    "section": "JTRAIN2",
    "text": "JTRAIN2\nSource: R.J. Lalonde (1986), “Evaluating the Econometric Evaluations of Training Programs with Experimental Data,” American Economic Review 76, 604-620.\nProfessor Jeff Biddle, at MSU, kindly passed the data set along to me. He obtained it from Professor Lalonde.\nUsed in Text: pages 16, 329-330\nNotes: Professor Lalonde obtained the data from the National Supported Work Demonstration job-training program conducted by the Manpower Demonstration Research Corporation in the mid 1970s. Training status was randomly assigned, so this is essentially experimental data. Computer Exercise C17.8 looks only at the effects of training on subsequent unemployment probabilities. For illustrating the more advanced methods in Chapter 17, a good exercise would be to have the students estimate a Tobit of re78 on train, and obtain estimates of the expected values for those with and without training. These can be compared with the sample averages."
  },
  {
    "objectID": "data_resources.html#jtrain3",
    "href": "data_resources.html#jtrain3",
    "title": "Data-Set Handbook",
    "section": "JTRAIN3",
    "text": "JTRAIN3\nSource: R.H. Dehejia and S. Wahba (1999), “Causal Effects in Nonexperimental Studies: Reevaluating the Evaluation of Training Programs,” Journal of the American Statistical Association 94, 1053-1062.\nProfessor Sergio Firpo, at Insper Institute of Education and Research in São Paulo, has used this data set in his work. He kindly provided it to me. This data set is a subset of that originally used by Lalonde in the study cited for JTRAIN2.\nUsed in Text: pages 329-330, 457"
  },
  {
    "objectID": "data_resources.html#jtrain98",
    "href": "data_resources.html#jtrain98",
    "title": "Data-Set Handbook",
    "section": "JTRAIN98",
    "text": "JTRAIN98\nSource: This is a data set I created many years ago intended as an update to the files JTRAIN2 and JTRAIN3. While the data were partly generated by me, the data attributes are similar to data sets used to evaluate job training programs.\nUsed in Text: 101-102, 248, 601\nNotes: The response variables, earn98 and unem98, both have discreteness: the former is a corner solutions (takes on the value zero and then a range of strictly positive values) and the latter is binary. One could use these in an exercise using methods in Chapter 17. unem98 can be used in a probit or logit model, earn98 in a Tobit model, or in Poisson regression (without assuming, of course, that the Poisson distribution is correct)."
  },
  {
    "objectID": "data_resources.html#kielmc",
    "href": "data_resources.html#kielmc",
    "title": "Data-Set Handbook",
    "section": "KIELMC",
    "text": "KIELMC\nSource: K.A. Kiel and K.T. McClain (1995), “House Prices During Siting Decision Stages: The Case of an Incinerator from Rumor through Operation,” Journal of Environmental Economics and Management 28, 241-255.\nProfessors Kiel and McClain kindly provided the data, of which I used only a subset.\nUsed in Text: pages 214, 431-434, 452, 454"
  },
  {
    "objectID": "data_resources.html#labsup",
    "href": "data_resources.html#labsup",
    "title": "Data-Set Handbook",
    "section": "LABSUP",
    "text": "LABSUP\nSource: The subset of data for black or Hispanic women used in J.A. Angrist and W.E. Evans (1998), “\nUsed in Text: pages 530-531\nNotes: This example can promote an interesting discussion of instrument validity, and in particular, how a variable that is beyond our control – for example, whether the first two children have the same gender – can, nevertheless, affect subsequent economic choices. Students are asked to think about such issues in Computer Exercise C13 in Chapter 15. A more egregious version of this mistake would be to treat a variable such as age as a suitable instrument because it is beyond our control: clearly age has a direct effect on many economic outcomes that would play the role of the dependent variable."
  },
  {
    "objectID": "data_resources.html#lawsch85",
    "href": "data_resources.html#lawsch85",
    "title": "Data-Set Handbook",
    "section": "LAWSCH85",
    "text": "LAWSCH85\nSource: Collected by Kelly Barnett, an MSU economics student, for use in a term project. The data come from two sources: The Official Guide to U.S. Law Schools, 1986, Law School Admission Services, and The Gourman Report: A Ranking of Graduate and Professional Programs in American and International Universities, 1995, Washington, D.C.\nUsed in Text: pages 105, 108, 159-160, 231-232\nNotes: More recent versions of both cited documents are available. One could try a similar analysis for, say, MBA programs or Ph.D. programs in economics. Quality of placements may be a good dependent variable, and measures of business school or graduate program quality could be included among the explanatory variables. Of course, one would want to control for factors describing the incoming class so as to isolate the effect of the program itself."
  },
  {
    "objectID": "data_resources.html#loanapp",
    "href": "data_resources.html#loanapp",
    "title": "Data-Set Handbook",
    "section": "LOANAPP",
    "text": "LOANAPP\nSource: W.C. Hunter and M.B. Walker (1996), “The Cultural Affinity Hypothesis and Mortgage Lending Decisions,” Journal of Real Estate Finance and Economics 13, 57-70.\nProfessor Walker kindly provided the data.\nUsed in Text: pages 257-258, 291, 329, 596\nNotes: These data were originally used in a famous study by researchers at the Boston Federal Reserve Bank. See A. Munnell, G.M.B. Tootell, L.E. Browne, and J. McEneaney (1996), “Mortgage Lending in Boston: Interpreting HMDA Data,” American Economic Review 86, 25-53."
  },
  {
    "objectID": "data_resources.html#lowbrth",
    "href": "data_resources.html#lowbrth",
    "title": "Data-Set Handbook",
    "section": "LOWBRTH",
    "text": "LOWBRTH\nSource: Source: Statistical Abstract of the United States, 1990, 1993, and 1994.\nUsed in Text: not used\nNotes: This data set can be used very much like INFMRT. It contains two years of state-level panel data. In fact, it is a superset of INFMRT. The key is that it contains information on low birth weights, as well as infant mortality. It also contains state identifies, so that several years of more recent data could be added for a term project. Putting in the variable afcdprc and its square leads to some interesting findings for pooled OLS and fixed effects (first differencing). After differencing, you can even try using the change in the AFDC payments variable as an instrumental variable for the change in afdcprc."
  },
  {
    "objectID": "data_resources.html#mathpnl",
    "href": "data_resources.html#mathpnl",
    "title": "Data-Set Handbook",
    "section": "MATHPNL",
    "text": "MATHPNL\nSource: Dr. Leslie Papke, an economics professor at MSU, collected these data from Michigan Department of Education web site, www.michigan.gov/mde. These are district-level data, which Professor Papke kindly provided. She has used building-level data in “The Effects of Spending on Test Pass Rates: Evidence from Michigan” (2005), Journal of Public Economics 89, 821-839.\nUsed in Text: pages 456, 487-488"
  },
  {
    "objectID": "data_resources.html#meap00",
    "href": "data_resources.html#meap00",
    "title": "Data-Set Handbook",
    "section": "MEAP00",
    "text": "MEAP00\nSource: Michigan Department of Education, www.michigan.gov/mde\nUsed in Text: pages 218, 292"
  },
  {
    "objectID": "data_resources.html#meap01",
    "href": "data_resources.html#meap01",
    "title": "Data-Set Handbook",
    "section": "MEAP01",
    "text": "MEAP01\nSource: Michigan Department of Education, www.michigan.gov/mde\nUsed in Text: page 16\nNotes: This is another good data set to compare simple and multiple regression estimates. The expenditure variable (in logs, say) and the poverty measure (lunch) are negatively correlated in this data set. A simple regression of math4 on lexppp gives a negative coefficient. Controlling for lunch makes the spending coefficient positive and significant."
  },
  {
    "objectID": "data_resources.html#meap93",
    "href": "data_resources.html#meap93",
    "title": "Data-Set Handbook",
    "section": "MEAP93",
    "text": "MEAP93\nSource: I collected these data from the old Michigan Department of Education web site. See MATHPNL for the current web site. I used data on most high schools in the state of Michigan for 1993. I dropped some high schools that had suspicious-looking data.\nUsed in Text: pages 44-45, 63, 110-111, 125-126, 149-150, 158, 212, 325, 329, 329, 660\nNotes: Many states have data, at either the district or building level, on student performance and spending. A good exercise in data collection and cleaning is to have students find such data for a particular state, and to put it into a form that can be used for econometric analysis."
  },
  {
    "objectID": "data_resources.html#meapsingle",
    "href": "data_resources.html#meapsingle",
    "title": "Data-Set Handbook",
    "section": "MEAPSINGLE",
    "text": "MEAPSINGLE\nSource: Collected by Professor Leslie Papke, an economics professor at MSU, from the Michigan Department of Education web site, www.michigan.gov/mde, and the U.S. Census Bureau. Professor Papke kindly provided the data.\nUsed in Text: 110-111, 158-159, 213"
  },
  {
    "objectID": "data_resources.html#minwage",
    "href": "data_resources.html#minwage",
    "title": "Data-Set Handbook",
    "section": "MINWAGE",
    "text": "MINWAGE\nSource: P. Wolfson and D. Belman (2004), “The Minimum Wage: Consequences for Prices and Quantities in Low-Wage Labor Markets,” Journal of Business & Economic Statistics 22, 296-311.\nProfessor Belman kindly provided the data.\nUsed in Text: pages 365, 393, 424, 641\nNotes: The sectors corresponding to the different numbers in the data file are provided in the Wolfson and Bellman and article."
  },
  {
    "objectID": "data_resources.html#mlb1",
    "href": "data_resources.html#mlb1",
    "title": "Data-Set Handbook",
    "section": "MLB1",
    "text": "MLB1\nSource: Collected by G. Mark Holmes, a former MSU undergraduate, for a term project. The salary data were obtained from the New York Times, April 11, 1993. The baseball statistics are from The Baseball Encyclopedia, 9th edition, and the city population figures are from the Statistical Abstract of the United States.\nUsed in Text: pages 140-143, 160, 229, 235-236, 256-257\nNotes: The baseball statistics are career statistics through the 1992 season. Players whose race or ethnicity could not be easily determined were not included. It should not be too difficult to obtain the city population and racial composition numbers for Montreal and Toronto for 1993. Of course, the data can be pretty easily obtained for more recent players."
  },
  {
    "objectID": "data_resources.html#mroz",
    "href": "data_resources.html#mroz",
    "title": "Data-Set Handbook",
    "section": "MROZ",
    "text": "MROZ\nSource: T.A. Mroz (1987), “The Sensitivity of an Empirical Model of Married Women’s Hours of Work to Economic and Statistical Assumptions,” Econometrica 55, 765-799.\nProfessor Ernst R. Berndt, of MIT, kindly provided the data, which he obtained from Professor Mroz.\nUsed in Text: pages 240, 253, 285, 501, 511, 516, 518, 543-544, 555, 568-569, 575-576, 591-592, 597"
  },
  {
    "objectID": "data_resources.html#murder",
    "href": "data_resources.html#murder",
    "title": "Data-Set Handbook",
    "section": "MURDER",
    "text": "MURDER\nSource: From the Statistical Abstract of the United States, 1995 (Tables 310 and 357), 1992 (Table 289). The execution data originally come from the U.S. Bureau of Justice Statistics, Capital Punishment Annual.\nUsed in Text: pages 457, 487, 527-528\nNotes: The data set COUNTYMURDERS includes information on executions and murder rates at the county level, and provides more variation."
  },
  {
    "objectID": "data_resources.html#nbasal",
    "href": "data_resources.html#nbasal",
    "title": "Data-Set Handbook",
    "section": "NBASAL",
    "text": "NBASAL\nSource: Collected by Christopher Torrente, a former MSU undergraduate, for a term project. He obtained the salary data and the career statistics from The Complete Handbook of Pro Basketball, 1995, edited by Zander Hollander. New York: Signet. The demographic information (marital status, number of children, and so on) was obtained from the teams’ 1994-1995 media guides.\nUsed in Text: pages 216-217, 258\nNotes: A panel version of this data set could be useful for further isolating productivity effects of marital status. One would need to obtain information on enough different players in at least two years, where some players who were not married in the initial year are married in later years. Fixed effects (or first differencing, for two years) is the natural estimation method."
  },
  {
    "objectID": "data_resources.html#ncaa_rpi",
    "href": "data_resources.html#ncaa_rpi",
    "title": "Data-Set Handbook",
    "section": "NCAA_RPI",
    "text": "NCAA_RPI\nSource: Data on NCAA men’s basketball teams, collected by Weizhao Sun for a senior seminar project in sports economics at Michigan State University, Spring 2017. He used various sources, including www.espn.com and www.teamrankings.com/ncaa-basketball/rpi-ranking/rpi-rating-by-team.\nUsed in Text: not used\nNotes: This is a nice example of how multiple regression analysis can be used to determine whether rankings compiled by experts – the so-called pre-season RPI in this case – provide additional information beyond what we can obtain from widely available data bases. A simple and interesting question is whether, once the previous year’s post-season RPI is controlled for, does the pre-season RPI – which is supposed to add information on recruiting and player development – help to predict performance (such as win percentage or making it to the NCAA men’s basketball tournament). For the binary outcome that indicates making it to the NCAA tournament, a probit or logit model can be used for courses that introduce more advanced methods. There are some other interesting variables, such as coaching experience, that can be included, too."
  },
  {
    "objectID": "data_resources.html#nyse",
    "href": "data_resources.html#nyse",
    "title": "Data-Set Handbook",
    "section": "NYSE",
    "text": "NYSE\nSource: These are Wednesday closing prices of value-weighted NYSE average, available in many publications. I do not recall the particular source I used when I collected these data at MIT. Probably the easiest way to get similar data is to go to the NYSE web site, www.nyse.com.\nUsed in Text: pages 352-353, 368, 393, 398, 399, 595"
  },
  {
    "objectID": "data_resources.html#okun",
    "href": "data_resources.html#okun",
    "title": "Data-Set Handbook",
    "section": "OKUN",
    "text": "OKUN\nSource: Economic Report of the President, 2007, Tables B4 and B42.\nUsed in Text: 392, 423-424"
  },
  {
    "objectID": "data_resources.html#openness",
    "href": "data_resources.html#openness",
    "title": "Data-Set Handbook",
    "section": "OPENNESS",
    "text": "OPENNESS\nSource: D. Romer (1993), “Openness and Inflation: Theory and Evidence,” Quarterly Journal of Economics 108, 869-903.\nThe data are included in the article.\nUsed in Text: pages 544-545, 555"
  },
  {
    "objectID": "data_resources.html#pension",
    "href": "data_resources.html#pension",
    "title": "Data-Set Handbook",
    "section": "PENSION",
    "text": "PENSION\nSource: L.E. Papke (2004), “Individual Financial Decisions in Retirement Saving: The Role of Participant-Direction,” Journal of Public Economics 88, 39-61.\nProfessor Papke kindly provided the data. She collected them from the National Longitudinal Survey of Mature Women, 1991.\nUsed in Text: page 488"
  },
  {
    "objectID": "data_resources.html#phillips",
    "href": "data_resources.html#phillips",
    "title": "Data-Set Handbook",
    "section": "PHILLIPS",
    "text": "PHILLIPS\nSource: Economic Report of the President, 2004, Tables B42 and B64.\nUsed in Text: pages 344-345, 364-365, 375, 390-391, 392, 392, 403, 404, 412, 423, 528, 613, 625, 628, 639, 770"
  },
  {
    "objectID": "data_resources.html#pntsprd",
    "href": "data_resources.html#pntsprd",
    "title": "Data-Set Handbook",
    "section": "PNTSPRD",
    "text": "PNTSPRD\nSource: Collected by Scott Resnick, a former MSU undergraduate, from various newspaper sources.\nUsed in Text: pages 271, 560, 623\nNotes: The data are for the 1994-1995 men’s college basketball seasons. The spread is for the day before the game was played. One might collect more recent data and determine whether the spread has become a less accurate predictor of the actual outcome in more recent years. In other words, in the simple regression of the actual score differential on the spread, is the variance larger in more recent years. (We should fully expect the slope coefficient not to be statistically different from one.)"
  },
  {
    "objectID": "data_resources.html#prison",
    "href": "data_resources.html#prison",
    "title": "Data-Set Handbook",
    "section": "PRISON",
    "text": "PRISON\nSource: S.D. Levitt (1996), “The Effect of Prison Population Size on Crime Rates: Evidence from Prison Overcrowding Legislation,” Quarterly Journal of Economics 111, 319-351.\nProfessor Levitt kindly provided me with the data, of which I used a subset.\nUsed in Text: pages 551"
  },
  {
    "objectID": "data_resources.html#prminwge",
    "href": "data_resources.html#prminwge",
    "title": "Data-Set Handbook",
    "section": "PRMINWGE",
    "text": "PRMINWGE\nSource: A.J. Castillo-Freeman and R.B. Freeman (1992), “When the Minimum Wage Really Bites: The Effect of the U.S.-Level Minimum Wage on Puerto Rico,” in Immigration and the Work Force, edited by G.J. Borjas and R.B. Freeman, 177-211. Chicago: University of Chicago Press.\nThe data are reported in the article.\nUsed in Text: pages 345-346, 356-357, 400, 405\nNotes: Given the ongoing debate on the employment effects of the minimum wage, this would be a great data set to try to update. The coverage rates are the most difficult variables to construct."
  },
  {
    "objectID": "data_resources.html#recid",
    "href": "data_resources.html#recid",
    "title": "Data-Set Handbook",
    "section": "RECID",
    "text": "RECID\nSource: C.-F. Chung, P. Schmidt, and A.D. Witte (1991), “Survival Analysis: A Survey,” Journal of Quantitative Criminology 7, 59-98.\nProfessor Chung kindly provided the data.\nUsed in Text: pages 584-585, 597"
  },
  {
    "objectID": "data_resources.html#rdchem",
    "href": "data_resources.html#rdchem",
    "title": "Data-Set Handbook",
    "section": "RDCHEM",
    "text": "RDCHEM\nSource: From Businessweek R&D Scoreboard, October 25, 1991.\nUsed in Text: pages 63, 135-136, 154-155, 198, 211, 317-318, 328-329\nNotes: It would be interesting to collect more recent data and see whether the R&D/firm size relationship has changed over time."
  },
  {
    "objectID": "data_resources.html#rdtelec",
    "href": "data_resources.html#rdtelec",
    "title": "Data-Set Handbook",
    "section": "RDTELEC",
    "text": "RDTELEC\nSource: See RDCHEM\nUsed in Text: not used\nNotes: According to these data, the R&D/firm size relationship is different in the telecommunications industry than in the chemical industry: there is pretty strong evidence that R&D intensity decreases with firm size in telecommunications. Of course, that was in 1991. The data could easily be updated, and a panel data set could be constructed for more advanced courses."
  },
  {
    "objectID": "data_resources.html#rental",
    "href": "data_resources.html#rental",
    "title": "Data-Set Handbook",
    "section": "RENTAL",
    "text": "RENTAL\nSource: David Harvey, a former MSU undergraduate, collected the data for 64 “college towns” from the 1980 and 1990 United States censuses.\nUsed in Text: pages 155, 444, 454-455, 486\nNotes: These data can be used in a somewhat crude simultaneous equations analysis, either focusing on one year or pooling the two years. (In the latter case, in an advanced class, you might have students compute the standard errors robust to serial correlation across the two time periods.) The demand equation would have ltothsg as a function of lrent, lavginc, and lpop. The supply equation would have ltothsg as a function of lrent, pctst, and lpop. Thus, in estimating the demand function, pctstu is used as an IV for lrent. Clearly one can quibble with excluding pctstu from the demand equation, but the estimated demand function gives a negative price effect.\nGetting information for 2000 and 2010, and adding many more college towns, would make for a much better analysis. Information on number of spaces in on-campus dormitories would be a big improvement, too."
  },
  {
    "objectID": "data_resources.html#return",
    "href": "data_resources.html#return",
    "title": "Data-Set Handbook",
    "section": "RETURN",
    "text": "RETURN\nSource: Collected by Stephanie Balys, a former MSU undergraduate, from the New York Stock Exchange and Compustat.\nUsed in Text: page 157\nNotes: More can be done with this data set. Recently, I discovered that lsp90 does appear to predict return (and the log of the 1990 stock price works better than sp90). I am a little suspicious, but you could use the negative coefficient on lsp90 to illustrate “reversion to the mean.”"
  },
  {
    "objectID": "data_resources.html#saving",
    "href": "data_resources.html#saving",
    "title": "Data-Set Handbook",
    "section": "SAVING",
    "text": "SAVING\nSource: Unknown\nUsed in Text: not used\nNotes: I remember entering this data set in the late 1980s, and I am pretty sure it came directly from an introductory econometrics text. But so far my search has been fruitless. If anyone runs across this data set, I would appreciate knowing about it."
  },
  {
    "objectID": "data_resources.html#school93_98",
    "href": "data_resources.html#school93_98",
    "title": "Data-Set Handbook",
    "section": "SCHOOL93_98",
    "text": "SCHOOL93_98\nSource: L.E. Papke (2005), “The Effects of Spending on Test Pass Rates: Evidence from Michigan,” Journal of Public Economics 89, 821-839.\nUsed in Text: page 491\nNotes: This is closer to the data actually used in the Papke paper as it is at the school (building) level. It is unbalanced because data on scores and some of the spending and other variables is missing for some schools. While the usual RE and FE methods can be applied directly, obtaining the correlated random effects version of the Hausman test is more advance. Computer Exercise 17 in Chapter 14 walks the reader through it."
  },
  {
    "objectID": "data_resources.html#sleep75",
    "href": "data_resources.html#sleep75",
    "title": "Data-Set Handbook",
    "section": "SLEEP75",
    "text": "SLEEP75\nSource: J.E. Biddle and D.S. Hamermesh (1990), “Sleep and the Allocation of Time,” Journal of Political Economy 98, 922-943.\nProfessor Biddle kindly provided the data.\nUsed in Text: pages 62, 105, 156-157, 251, 257, 290\nNotes: In their article, Biddle and Hamermesh include an hourly wage measure in the sleep equation. An econometric problem that arises is that the hourly wage is missing for those who do not work. Plus, the wage offer may be endogenous (even if it were always observed). Biddle and Hamermesh employ extensions of the sample selection methods in Section 17.5. See their article for details."
  },
  {
    "objectID": "data_resources.html#slp75_81",
    "href": "data_resources.html#slp75_81",
    "title": "Data-Set Handbook",
    "section": "SLP75_81",
    "text": "SLP75_81\nSource: See SLEEP75\nUsed in Text: pages 442-443"
  },
  {
    "objectID": "data_resources.html#smoke",
    "href": "data_resources.html#smoke",
    "title": "Data-Set Handbook",
    "section": "SMOKE",
    "text": "SMOKE\nSource: J. Mullahy (1997), “Instrumental-Variable Estimation of Count Data Models: Applications to Models of Cigarette Smoking Behavior,” Review of Economics and Statistics 79, 596-593.\nProfessor Mullahy kindly provided the data.\nUsed in Text: pages 177, 280-281, 288, 291-292, 555, 598-599\nNotes: If you want to do a “fancy” IV version of Computer Exercise C16.1, you could estimate a reduced form count model for cigs using the Poisson regression methods in Section 17.3, and then use the fitted values as an IV for cigs. Presumably, this would be for a fairly advanced class."
  },
  {
    "objectID": "data_resources.html#traffic1",
    "href": "data_resources.html#traffic1",
    "title": "Data-Set Handbook",
    "section": "TRAFFIC1",
    "text": "TRAFFIC1\nSource: I collected these data from two sources, the 1992 Statistical Abstract of the United States (Tables 1009, 1012) and A Digest of State Alcohol-Highway Safety Related Legislation, 1985 and 1990, published by the U.S. National Highway Traffic Safety Administration.\nUsed in Text: pages 444, 446\nNotes: In addition to adding recent years, this data set could really use state-level tax rates on alcohol. Other important law changes include defining driving under the influence as having a blood alcohol level of .08 or more, which many states have adopted since the 1980s. The trend really picked up in the 1990s and continued through the 2000s. The data set DRIVING is more complete and more recent, but it is also more complicated."
  },
  {
    "objectID": "data_resources.html#traffic2",
    "href": "data_resources.html#traffic2",
    "title": "Data-Set Handbook",
    "section": "TRAFFIC2",
    "text": "TRAFFIC2\nSource: P.S. McCarthy (1994), “Relaxed Speed Limits and Highway Safety: New Evidence from California,” Economics Letters 46, 173-179.\nProfessor McCarthy kindly provided the data.\nUsed in Text: pages 364, 392, 422, 641, 659\nNotes: Many states have changed maximum speed limits and imposed seat belt laws over the past 25 years. Data similar to those in TRAFFIC2 should be fairly easy to obtain for a particular state. One should combine this information with changes in a state’s blood alcohol limit and the passage of per se and open container laws."
  },
  {
    "objectID": "data_resources.html#twoyear",
    "href": "data_resources.html#twoyear",
    "title": "Data-Set Handbook",
    "section": "TWOYEAR",
    "text": "TWOYEAR\nSource: T.J. Kane and C.E. Rouse (1995), “Labor-Market Returns to Two- and Four-Year Colleges,” American Economic Review 85, 600-614.\nWith Professor Rouse’s kind assistance, I obtained the data from her web site at Princeton University.\nUsed in Text: pages 137-139, 160, 254, 329\nNotes: As possible extensions, students can explore whether the returns to two-year or four-year colleges depend on race or gender. This is partly done in Problem 7.9 but where college is aggregated into one number. Also, should experience appear as a quadratic in the wage specification?"
  },
  {
    "objectID": "data_resources.html#volat",
    "href": "data_resources.html#volat",
    "title": "Data-Set Handbook",
    "section": "VOLAT",
    "text": "VOLAT\nSource: J.D. Hamilton and L. Gang (1996), “Stock Market Volatility and the Business Cycle,” Journal of Applied Econometrics 11, 573-593.\nI obtained these data from the Journal of Applied Econometrics data archive at http://qed.econ.queensu.ca/jae/\nUsed in Text: pages 364, 637-640"
  },
  {
    "objectID": "data_resources.html#vote1",
    "href": "data_resources.html#vote1",
    "title": "Data-Set Handbook",
    "section": "VOTE1",
    "text": "VOTE1\nSource: M. Barone and G. Ujifusa, The Almanac of American Politics, 1992. Washington, DC: National Journal.\nUsed in Text: pages 31, 36, 159-160, 215-216, 290, 663"
  },
  {
    "objectID": "data_resources.html#vote2",
    "href": "data_resources.html#vote2",
    "title": "Data-Set Handbook",
    "section": "VOTE2",
    "text": "VOTE2\nSource: See VOTE1\nUsed in Text: pages 324-325, 444, 455-456, 663\nNotes: These are panel data, at the Congressional district level, collected for the 1988 and 1990 U.S. House of Representative elections. Of course, much more recent data are available, possibly even in electronic form."
  },
  {
    "objectID": "data_resources.html#voucher",
    "href": "data_resources.html#voucher",
    "title": "Data-Set Handbook",
    "section": "VOUCHER",
    "text": "VOUCHER\nSource: Rouse, C.E. (1998), “Private School Vouchers and Student Achievement: An Evaluation of the Milwaukee Parental Choice Program,” Quarterly Journal of Economics 113, 553-602.\nProfessor Rouse kindly provided the original data set from her paper.\nUsed in Text: pages 529-530\nNotes: This is a condensed version of the data set used by Professor Rouse. The original data set had missing information on many variables, including pre-program and post-program test scores. I did not impute any missing data and have dropped observations that were unusable without filling in missing data. There are 990 students in the current data set but pre-program test scores are available for only 328 of them.\nThis is a good example of where eligibility for a program is randomized but participation need not be. In addition, even if we look at just the effect of eligibility (captured in the variable selectyrs) on the math test score (mnce), we need to confront the fact that attrition (students leaving the district) can bias the results. Controlling for the pre-policy test score, mnce90, can help – but at the cost of losing two-thirds of the observations. A simple regression of mnce on selectyrs followed by a multiple regression that adds mnce90 as a control is informative.\nThe selectyrs dummy variables can be used as instrumental variables for the choiceyrs variable to try to estimate the effect of actually participating in the program (rather than estimating the so-called intention-to-treat effect). Computer Exercise C15.11 steps through the details."
  },
  {
    "objectID": "data_resources.html#wage1",
    "href": "data_resources.html#wage1",
    "title": "Data-Set Handbook",
    "section": "WAGE1",
    "text": "WAGE1\nSource: These are data from the 1976 Current Population Survey, collected by Henry Farber when he and I were colleagues at MIT in 1988.\nUsed in Text: pages 6, 30-31, 33, 73, 86, 123, 178, 189, 214, 222, 224, 227, 228-229, 232-233, 235, 257, 265-266, 316, 644\nNotes: Barry Murphy, of the University of Portsmouth in the UK, has pointed out that for several observations the values for exper and tenure are in logical conflict. In particular, for some workers the number of years with current employer (tenure) is greater than overall work experience (exper). At least some of these conflicts are due to the definition of exper as “potential” work experience, but probably not all. Nevertheless, I am using the data set as it was supplied to me."
  },
  {
    "objectID": "data_resources.html#wage2",
    "href": "data_resources.html#wage2",
    "title": "Data-Set Handbook",
    "section": "WAGE2",
    "text": "WAGE2\nSource: M. Blackburn and D. Neumark (1992), “Unobserved Ability, Efficiency Wages, and Interindustry Wage Differentials,” Quarterly Journal of Economics 107, 1421-1436.\nProfessor Neumark kindly provided the data, of which I used just the data for 1980.\nUsed in Text: pages 63, 104-105, 110, 160, 212, 215, 256, 301-302, 328, 502, 515, 526, 528-529, 644\nNotes: As with WAGE1, there are some clear inconsistencies among the variables tenure, exper, and age. I have not been able to track down the source of the inconsistency, and so any changes would be effectively arbitrary. Instead, I am using the data as provided by the authors of the above QJE article."
  },
  {
    "objectID": "data_resources.html#wagepan",
    "href": "data_resources.html#wagepan",
    "title": "Data-Set Handbook",
    "section": "WAGEPAN",
    "text": "WAGEPAN\nSource: F. Vella and M. Verbeek (1998), “Whose Wages Do Unions Raise? A Dynamic Model of Unionism and Wage Rate Determination for Young Men,” Journal of Applied Econometrics 13, 163-183.\nI obtained the data from the Journal of Applied Econometrics data archive at http://qed.econ.queensu.ca/jae/. The JAE data archive is generally a nice resource for undergraduates looking to replicate or extend a published study.\nUsed in Text: pages 457, 465, 472"
  },
  {
    "objectID": "data_resources.html#wageprc",
    "href": "data_resources.html#wageprc",
    "title": "Data-Set Handbook",
    "section": "WAGEPRC",
    "text": "WAGEPRC\nSource: Economic Report of the President, various years.\nUsed in Text: pages 388, 421, 638\nNotes: These monthly data run from January 1964 through October 1987. The consumer price index averages to 100 in 1967. An updated set of data can be obtained electronically from http://www.gpo.gov/fdsys/browse/collection.action?collectionCode=ERP."
  },
  {
    "objectID": "data_resources.html#wine",
    "href": "data_resources.html#wine",
    "title": "Data-Set Handbook",
    "section": "WINE",
    "text": "WINE\nSource: These data were reported in a New York Times article, December 28, 1994.\nUsed in Text: not used\nNotes: The dependent variables deaths, heart, and liver each can be regressed on alcohol as nice simple regression examples. The conventional wisdom is that wine is good for the heart but not for the liver, something that is apparent in the regressions. Because the number of observations is small, this can be a good data set to illustrate calculation of the OLS estimates and statistics."
  },
  {
    "objectID": "adv_class/PO_RCT.html",
    "href": "adv_class/PO_RCT.html",
    "title": "RCT Implementation",
    "section": "",
    "text": "In this excecise, I’ll provide some simple examples of how to implement a basic RCT analysis.\nBefore we start, however, we need to create some data.\n\nEstimate two wages models. For men and women, using an heteroskedastic regression model.\n\nThe goal: estimate two potential outcomes for wages. One following Womens wage structure, and the other using Men wage structure.\n\n\nCode\nset linesize 255 \nfrause oaxaca, clear\nset seed 101\nqui:hetreg lnwage age agesq married divorced kids6 kids714 if female==0, het(age agesq married divorced kids6 kids714)\nest sto m1\npredict xb1, xb\npredict s1, sigma\nqui:hetreg lnwage age agesq married divorced kids6 kids714 if female==1, het(age agesq married divorced kids6 kids714)\nest sto m2\npredict xb0, xb\npredict s0, sigma\n\n\n\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n\nHere, as I did for UQregression, I obtain the predicted mean and predicted standard errors. and simulate two wages. I will use wage differences between simulated men and women predicted log wages as the treated effect\n\\[\n\\begin{aligned}\nTE= ln\\hat w_1 - ln\\hat w_0  \\\\\nln\\hat w_1 \\sim N(xb_1,s^2_1) \\\\\nln\\hat w_0 \\sim N(xb_0,s_0^2)\n\\end{aligned}\n\\]\n\n\nCode\ngen lnwage1 = rnormal(xb0,s0)\ngen lnwage0 = rnormal(xb1,s1)\ngen teff=lnwage1-lnwage0\n\n\nI will create a randomzed treatment, and assume those treated get the treatment effect\n\\[\nY_i = Y_i(0) + trt_i * \\delta_i\n\\]\n\n\nCode\ngen trt = runiform()&lt;.5\nreplace lnwage = lnwage0 + trt * teff\n\n\n(1,647 real changes made)"
  },
  {
    "objectID": "adv_class/PO_RCT.html#randomized-control-trial",
    "href": "adv_class/PO_RCT.html#randomized-control-trial",
    "title": "RCT Implementation",
    "section": "",
    "text": "In this excecise, I’ll provide some simple examples of how to implement a basic RCT analysis.\nBefore we start, however, we need to create some data.\n\nEstimate two wages models. For men and women, using an heteroskedastic regression model.\n\nThe goal: estimate two potential outcomes for wages. One following Womens wage structure, and the other using Men wage structure.\n\n\nCode\nset linesize 255 \nfrause oaxaca, clear\nset seed 101\nqui:hetreg lnwage age agesq married divorced kids6 kids714 if female==0, het(age agesq married divorced kids6 kids714)\nest sto m1\npredict xb1, xb\npredict s1, sigma\nqui:hetreg lnwage age agesq married divorced kids6 kids714 if female==1, het(age agesq married divorced kids6 kids714)\nest sto m2\npredict xb0, xb\npredict s0, sigma\n\n\n\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n\nHere, as I did for UQregression, I obtain the predicted mean and predicted standard errors. and simulate two wages. I will use wage differences between simulated men and women predicted log wages as the treated effect\n\\[\n\\begin{aligned}\nTE= ln\\hat w_1 - ln\\hat w_0  \\\\\nln\\hat w_1 \\sim N(xb_1,s^2_1) \\\\\nln\\hat w_0 \\sim N(xb_0,s_0^2)\n\\end{aligned}\n\\]\n\n\nCode\ngen lnwage1 = rnormal(xb0,s0)\ngen lnwage0 = rnormal(xb1,s1)\ngen teff=lnwage1-lnwage0\n\n\nI will create a randomzed treatment, and assume those treated get the treatment effect\n\\[\nY_i = Y_i(0) + trt_i * \\delta_i\n\\]\n\n\nCode\ngen trt = runiform()&lt;.5\nreplace lnwage = lnwage0 + trt * teff\n\n\n(1,647 real changes made)"
  },
  {
    "objectID": "adv_class/PO_RCT.html#visual-exploration",
    "href": "adv_class/PO_RCT.html#visual-exploration",
    "title": "RCT Implementation",
    "section": "Visual Exploration",
    "text": "Visual Exploration\nNow that we have a randomized treatment, we could start exploring the data:\n\n\nCode\ntwo (kdensity lnwage if trt == 1) (kdensity lnwage if trt == 0) , ///\n    legend(order(1 \"Treated\" 2 \"Untreated\"))\n\n\n\n\n\nLog wage distribution between Treated and untreated\n\n\n\n\nIn order to estimate the treatment effects, we could simple estimate a regression model of the outcome. Compare it to the treatment effect"
  },
  {
    "objectID": "adv_class/PO_RCT.html#estimation-of-ate-effect",
    "href": "adv_class/PO_RCT.html#estimation-of-ate-effect",
    "title": "RCT Implementation",
    "section": "Estimation of ATE Effect",
    "text": "Estimation of ATE Effect\n\n\nCode\n** True Effect\nsum teff\n** Simple Regression\nset linesize 255\nreg lnwage  trt, robust\nest sto m0\n\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        teff |      1,647   -.2121816    .6613419  -3.024343   2.704082\n\nLinear regression                               Number of obs     =      1,647\n                                                F(1, 1645)        =      79.85\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.0464\n                                                Root MSE          =      .5017\n\n------------------------------------------------------------------------------\n             |               Robust\n      lnwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         trt |  -.2213292   .0247679    -8.94   0.000    -.2699092   -.1727492\n       _cons |   3.465982   .0150123   230.88   0.000     3.436537    3.495427\n------------------------------------------------------------------------------\n\n\nBecause treatment is randomized, we could also add other controls to the model, and improve on precision\n\nCode\nqui:reg lnwage  trt age agesq , robust\nest sto m1\nqui:reg lnwage  trt age agesq married divorced , robust\nest sto m2\nqui:reg lnwage  trt age agesq married divorced kids6 kids714 , robust\nest sto m3\n\nesttab m0 m1 m2 m3, se nonum mtitle(\"m0\" \"m1\" \"m2\" \"m3\") keep(trt) md\n\n\n\n\n\n\n\n\n\n\n\n\nm0\nm1\nm2\nm3\n\n\n\n\ntrt\n-0.221***\n-0.214***\n-0.213***\n-0.212***\n\n\n\n(0.0248)\n(0.0225)\n(0.0225)\n(0.0224)\n\n\nN\n1647\n1647\n1647\n1647\n\n\n\nStandard errors in parentheses * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "adv_class/PO_RCT.html#falsification",
    "href": "adv_class/PO_RCT.html#falsification",
    "title": "RCT Implementation",
    "section": "Falsification",
    "text": "Falsification\nWe could just use other outcomes that shouldnt be affected by the treatment. You expect they have no impact on outcome\n\nCode\nqui:reg exper  trt age agesq married divorced kids6 kids714 , robust\nest sto m0\nqui:reg tenure trt age agesq married divorced kids6 kids714 , robust\nest sto m1\nesttab m0 m1 , se nonum mtitle(\"m0\" \"m1\") keep(trt) md\n\n\n\n\n\n\n\n\n\n\nm0\nm1\n\n\n\n\ntrt\n-0.104\n-0.458\n\n\n\n(0.366)\n(0.335)\n\n\nN\n1434\n1434\n\n\n\nStandard errors in parentheses * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "adv_class/PO_RCT.html#balance-test",
    "href": "adv_class/PO_RCT.html#balance-test",
    "title": "RCT Implementation",
    "section": "Balance test",
    "text": "Balance test\nYou should also try to create balance tables, where you compare and test if characteristics are similar across treated and control groups:\n\n\nCode\ntabstat age agesq married divorced kids6 kids714 , by(trt)\nsureg age agesq married divorced kids6 kids714 =trt, \n\n\n\nSummary statistics: Mean\nGroup variable: trt \n\n     trt |       age     agesq   married  divorced     kids6   kids714\n---------+------------------------------------------------------------\n       0 |  39.14475   1649.63    .53076  .1206273  .2979493  .3365501\n       1 |   39.3643  1675.521  .5158924  .1466993  .2713936  .3215159\n---------+------------------------------------------------------------\n   Total |  39.25379  1662.489  .5233758  .1335762  .2847602  .3290832\n----------------------------------------------------------------------\n\nSeemingly unrelated regression\n------------------------------------------------------------------------------\nEquation             Obs   Params         RMSE  \"R-squared\"      chi2   P&gt;chi2\n------------------------------------------------------------------------------\nage                1,647        1     11.02798      0.0001       0.16   0.6862\nagesq              1,647        1     893.7224      0.0002       0.35   0.5566\nmarried            1,647        1     .4993979      0.0002       0.36   0.5458\ndivorced           1,647        1     .3399466      0.0015       2.42   0.1197\nkids6              1,647        1     .6626276      0.0004       0.66   0.4161\nkids714            1,647        1     .7071256      0.0001       0.19   0.6662\n------------------------------------------------------------------------------\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nage          |\n         trt |   .2195505   .5434865     0.40   0.686    -.8456636    1.284765\n       _cons |   39.14475   .3830175   102.20   0.000     38.39405    39.89545\n-------------+----------------------------------------------------------------\nagesq        |\n         trt |   25.89111   44.04489     0.59   0.557    -60.43528    112.2175\n       _cons |    1649.63   31.04026    53.14   0.000     1588.792    1710.467\n-------------+----------------------------------------------------------------\nmarried      |\n         trt |  -.0148675   .0246116    -0.60   0.546    -.0631054    .0333703\n       _cons |     .53076   .0173448    30.60   0.000     .4967648    .5647552\n-------------+----------------------------------------------------------------\ndivorced     |\n         trt |    .026072   .0167534     1.56   0.120    -.0067641    .0589081\n       _cons |   .1206273   .0118068    10.22   0.000     .0974863    .1437682\n-------------+----------------------------------------------------------------\nkids6        |\n         trt |  -.0265557    .032656    -0.81   0.416    -.0905602    .0374488\n       _cons |   .2979493    .023014    12.95   0.000     .2528427     .343056\n-------------+----------------------------------------------------------------\nkids714      |\n         trt |  -.0150342   .0348489    -0.43   0.666    -.0833368    .0532685\n       _cons |   .3365501   .0245595    13.70   0.000     .2884143    .3846858\n------------------------------------------------------------------------------\n\n\nHere, the goal is just to see if trt is not-significant across groups"
  },
  {
    "objectID": "adv_class/12did.html",
    "href": "adv_class/12did.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "title: “Differences in Differences” subtitle: “1 is good, 2 is twice as good” author: Fernando Rios-Avila format: revealjs: slide-number: true reveal-js-url: ppt_files width: 1400 height: 900 code-overflow: wrap"
  },
  {
    "objectID": "adv_class/10matching.html#recap-potential-outcomes-and-identification",
    "href": "adv_class/10matching.html#recap-potential-outcomes-and-identification",
    "title": "Matching and Re-weighting",
    "section": "Recap: Potential outcomes and Identification",
    "text": "Recap: Potential outcomes and Identification\nTo identify treatment effects one could just compare potential outcomes in two states:\n\nwith treatment\nwithout treatment\n\nMathematically, average treatment effects would be: \\[\nATE = E(Y_i(1)-Y_i(0))\n\\]\nthe problem: with real data, we are only able to see one outcome. The counter factual is not observed:\n\\[\nY_i = Y_i(1)*D + Y_i(0)*(1-D)\n\\]\nand simple differences may not capture ATE, because of selection bias and heterogeneity in effects."
  },
  {
    "objectID": "adv_class/10matching.html#recap-gold-standard---rct",
    "href": "adv_class/10matching.html#recap-gold-standard---rct",
    "title": "Matching and Re-weighting",
    "section": "Recap: Gold Standard - RCT",
    "text": "Recap: Gold Standard - RCT\nThe easiest, but most expensive, way to deal with the problem is using Randomized Control Trials.\nEffectively, you randomize Treatment, so that potential outcomes are independent of treatment:\n\\[\nY(1),Y(0) \\perp D\n\\]\nIn other words, the distribution of potential outcomes is the same for those treated or untreated units.\n\\[\n\\begin{aligned}\nE(Y,D=1)&=E(Y(1),D=1)=E(Y(1),D=0) \\\\\nE(Y,D=0)&=E(Y(0),D=1)=E(Y(0),D=0) \\\\\nATT&=E(Y,D=1) - E(Y,D=0)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/10matching.html#when-unconditional-fails",
    "href": "adv_class/10matching.html#when-unconditional-fails",
    "title": "Matching and Re-weighting",
    "section": "When unconditional fails",
    "text": "When unconditional fails\nMore often than not, specially if we didn’t construct the data, it would be impossible to find that unconditional independence assumption holds.\nFor example, treatment (say having health insurance) may vary by age, gender, race, location, etc.\nThis is similar to the selection bias: Outcomes across treated and untreated groups will be different because:\n\nComposition: Characteristics of people among the treated could be different than those among the untreated For example, they could be older, more educated, mostly men, etc.\nOther factors: There could be factors we cannot control for, that also affect outcomes."
  },
  {
    "objectID": "adv_class/10matching.html#there-is-conditional",
    "href": "adv_class/10matching.html#there-is-conditional",
    "title": "Matching and Re-weighting",
    "section": "There is conditional",
    "text": "There is conditional\nWhen unconditional independence assumption fails, we can call on Conditional independence assumption:\n\\[\nY(1),Y(0) \\perp D | X\n\\]\nIn other words, If we can look into specific groups (given \\(X\\)), it may be possible to impose the Independence assumption.\nThis relaxes the independence condition, but assumes selection is due to observable characteristics only. (it still needs to be as good as randomized given \\(X\\))\nImplications:\n\\[\n\\begin{aligned}\nE(Y|D=1,X) =E(Y(1)|D=1,X)=E(Y(1)|D=0,X)  \\\\\nE(Y|D=0,X) =E(Y(0)|D=1,X)=E(Y(0)|D=0,X)  \n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/10matching.html#intuition",
    "href": "adv_class/10matching.html#intuition",
    "title": "Matching and Re-weighting",
    "section": "Intuition",
    "text": "Intuition\nMatching is a methodology that falls within quasi-experimental designs. You cannot or could not decide the assignment rules, so now are using data as given.\nThe idea is to construct an artificial control and use it as a counter-factual, so that both treated and control groups “look similar” in terms of observables.\nOnce a group of synthetic controls has been constructed, treatment effects can be calculated for the whole population:\n\\[\n\\begin{aligned}\nATE(X) &= E(Y|D=1,X) -E(Y|D=0,X) \\\\\nATE &= \\int ATE(X) dFx\n\\end{aligned}\n\\]\nHow can we do this?\nwe just need to find observational twins!"
  },
  {
    "objectID": "adv_class/10matching.html#matching-twins",
    "href": "adv_class/10matching.html#matching-twins",
    "title": "Matching and Re-weighting",
    "section": "Matching Twins",
    "text": "Matching Twins\n\nMatching on Observables"
  },
  {
    "objectID": "adv_class/10matching.html#subclassification-or-stratification",
    "href": "adv_class/10matching.html#subclassification-or-stratification",
    "title": "Matching and Re-weighting",
    "section": "Subclassification or stratification",
    "text": "Subclassification or stratification\nConsider the following dataset:\n\n\nCode\nfrause titanic, clear\nexpand freq\ndrop if freq==0\ngen class1=class==1\ntab survived class1 , nofreq col\n\n\n\n\n\n(Data downloaded from R base)\n(8 zero counts ignored; observations not deleted)\n(2,177 observations created)\n(8 observations deleted)\n\n           |        class1\n  Survived |         0          1 |     Total\n-----------+----------------------+----------\n        No |     72.92      37.54 |     67.70 \n       Yes |     27.08      62.46 |     32.30 \n-----------+----------------------+----------\n     Total |    100.00     100.00 |    100.00 \n\n\nIf we assume full Independence assumption we would believe that being in first class increased chance of survival in 35.4%. but is that the case?\nWhat if the composition of individuals differs across classes (women and children)\n\n\nCode\ntab age class1, nofreq col\ntab sex class1, nofreq col\n\n\n\n           |        class1\n       Age |         0          1 |     Total\n-----------+----------------------+----------\n     Child |      5.49       1.85 |      4.95 \n     Adult |     94.51      98.15 |     95.05 \n-----------+----------------------+----------\n     Total |    100.00     100.00 |    100.00 \n\n           |        class1\n       Sex |         0          1 |     Total\n-----------+----------------------+----------\n      Male |     82.68      55.38 |     78.65 \n    Female |     17.32      44.62 |     21.35 \n-----------+----------------------+----------\n     Total |    100.00     100.00 |    100.00 \n\n\nThere were fewer children, but more women in first class. Perhaps that explains the difference in survival rates"
  },
  {
    "objectID": "adv_class/10matching.html#section",
    "href": "adv_class/10matching.html#section",
    "title": "Matching and Re-weighting",
    "section": "",
    "text": "A better approach would be to look into the survival probabilities stratifying the data:\n\n\nCode\ngen surv=survived==2\nbysort age sex class1:egen sr_mean=mean(survived==2)\ntable (age sex) (class1), stat(mean surv) nototal\n\n\n\n-----------------------------------\n             |         class1      \n             |         0          1\n-------------+---------------------\nAge          |                     \n  Child      |                     \n    Sex      |                     \n      Male   |  .4067797          1\n      Female |  .6136364          1\n  Adult      |                     \n    Sex      |                     \n      Male   |  .1883378   .3257143\n      Female |  .6263345   .9722222\n-----------------------------------\n\n\nSo even within each group, the survival probability is larger in first class. What about Average?\n\n\nCode\nbysort age sex:egen sr_mean_class1=max(sr_mean*(class1==1))\nbysort age sex:egen sr_mean_class0=max(sr_mean*(class1==0))\ngen teff = sr_mean_class1-sr_mean_class0\nsum teff if class1==1 // ATT\nsum teff if class1==0 // ATU\nsum teff  // ATE\n\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        teff |        325    .2375421    .1125033   .1373765   .5932204\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        teff |      1,876    .1887847    .1089261   .1373765   .5932204\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        teff |      2,201    .1959842    .1107948   .1373765   .5932204"
  },
  {
    "objectID": "adv_class/10matching.html#what-did-we-do",
    "href": "adv_class/10matching.html#what-did-we-do",
    "title": "Matching and Re-weighting",
    "section": "What did we do?",
    "text": "What did we do?\nThe procedure above is a simple stratification approach, aka matching, to analyze the true impact of the treatment (being a 1st class passenger).\n\nStratified the sample in groups by age and gender.\n\nIdentify the shares of each group by class1\n\nPredict probability of survival per strata and class1\nObtain the Strata level Effects\nAggregate as needed.\n\nHere, we could estimate ATE, ATT or ATU!\n\n\nWhere could things go wrong?"
  },
  {
    "objectID": "adv_class/10matching.html#overlapping",
    "href": "adv_class/10matching.html#overlapping",
    "title": "Matching and Re-weighting",
    "section": "Overlapping",
    "text": "Overlapping\nThe procedure describe above works well whenever there is data overlapping.\n\nFor every combination of X, you see data on the control and treated group \\(0&lt;P(D|X)&lt;1\\)\n\nWhen this fails, you wont be able to estimate ATE’s, although ATT’s or ATU’s might still be possible:\n\nfor ATT: \\(P(D|X)&lt;1\\)\nfor ATU: \\(0&lt;P(D|X)\\)\n\nFor example:\n\n\nCode\nfrause hhprice, clear\nkeep price rooms type_h\ntab rooms type_h\n\n\n\n           |    =0 if house, =1\n Number of |       TownHouse\n     rooms |         0          1 |     Total\n-----------+----------------------+----------\n         1 |        37         72 |       109 \n         2 |     1,134        751 |     1,885 \n         3 |     4,634        648 |     5,282 \n         4 |     2,465        115 |     2,580 \n         5 |       465          2 |       467 \n         6 |        46          0 |        46 \n         7 |         7          0 |         7 \n-----------+----------------------+----------\n     Total |     8,788      1,588 |    10,376 \n\n\nWould not be able to estimate ATE nor ATU. Only ATT for townhouses."
  },
  {
    "objectID": "adv_class/10matching.html#curse-of-dimensionality",
    "href": "adv_class/10matching.html#curse-of-dimensionality",
    "title": "Matching and Re-weighting",
    "section": "Curse of dimensionality",
    "text": "Curse of dimensionality\nThere is a second problem in terms of stratification. How would we deal with Multiple dimensions? Would it be possible to find “twins” for every observation?\nThe answer is, probably no. Too many groups to track, to many micro cells to make use of:\n\n\nCode\nfrause oaxaca, clear\ndrop if lnwage==.\negen strata=group(educ isco)\nbysort strata:egen flag=mean(female)\nlist educ isco female if (flag==0 | flag==1) & educ == 10, sep(0)\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n\n      +----------------------+\n      | educ   isco   female |\n      |----------------------|\n 158. |   10      1        0 |\n 159. |   10      1        0 |\n 197. |   10      7        0 |\n 198. |   10      7        0 |\n 199. |   10      9        1 |\n 200. |   10      9        1 |\n      +----------------------+"
  },
  {
    "objectID": "adv_class/10matching.html#alternative-matching-as-a-weighted",
    "href": "adv_class/10matching.html#alternative-matching-as-a-weighted",
    "title": "Matching and Re-weighting",
    "section": "Alternative: Matching as a weighted",
    "text": "Alternative: Matching as a weighted\nThe problem of curse of dimensional states that as the number of desired characteristics to match increase, fewer “twins” will be available in the data. At the end…no one will be like you!\nThe alternative, is to look into People that are sufficiently close so they can be used for matching.\n\\[\n\\begin{aligned}\nATT_i &= Y_i -  \\sum_{j \\in C} w(x_j,x_i) Y_j \\\\\nATT   &= \\frac{1}{N_T}\\sum(ATT_i)  \\\\\nATT   &=E(Y|D=1) - E_i\\left( \\sum_{j \\in C} w(x_j,x_i) Y_j  \\Big| D=0 \\right)\n\\end{aligned}\n\\]\nDepending how \\(w(.)\\) is defined, we would be facing different kinds of matching estimators."
  },
  {
    "objectID": "adv_class/10matching.html#matching-on-covariates",
    "href": "adv_class/10matching.html#matching-on-covariates",
    "title": "Matching and Re-weighting",
    "section": "Matching on covariates",
    "text": "Matching on covariates\nThe first decision to take is whether one should find matches based on covariates, or based on scores (propensity scores).\nUsing covariates implies that will aim to find the closest “twin” possible, based on multiple dimensions: \\[\n\\begin{aligned}\nEclidean=d(x_i,x_j) &=\\sqrt{ (x_i-x_j)'(x_i-x_j)} \\\\\nWEclidean=d(x_i,x_j) &=\\sqrt{ (x_i-x_j)'W (x_i-x_j)} \\\\\nMaha =d(x_i,x_j) &=\\sqrt{(x_i-x_j)'S^{-1}(x_i-x_j)}\n\\end{aligned}\n\\]\nDistance measures are used to identify the closest matches to a given observation, and thus the weight assigned to that observation.\nHas the advantage of looking at individuals who are indeed close to each other, but becomes more difficult as the dimensionality of X’s increase. (you will not find close matches)"
  },
  {
    "objectID": "adv_class/10matching.html#matching-on-scores",
    "href": "adv_class/10matching.html#matching-on-scores",
    "title": "Matching and Re-weighting",
    "section": "Matching on Scores",
    "text": "Matching on Scores\nA second approach is to match individuals based on some summary index that condenses the information in \\(X\\) into a single scalar \\(h(x)\\), reducing the dimensionality problem fron K to 1.\nFew candidates:\n\nPropensity Score: \\(P(D|X)\\) based on a logit/probit/binomial model. Most common approach!\nPredicted Mean: \\(X\\beta\\) if there is information on outcome to be predicted\nPCA: Using Principal components to reduce dimensionality before Matching\n\nSince there is only 1 dimension to consider, multiple distance measures are possible:\n\nnearest neighbors, kernel weight matching, radious matching.\n\nBut one has to be careful with the approach. King and Nielsen (2019) Argue about the risks of PSM"
  },
  {
    "objectID": "adv_class/10matching.html#vs-k-matching-with-and-without-replacement",
    "href": "adv_class/10matching.html#vs-k-matching-with-and-without-replacement",
    "title": "Matching and Re-weighting",
    "section": "1 vs K matching; With and without replacement",
    "text": "1 vs K matching; With and without replacement\nTwo additional questions remain regarding matching. How many “twins” to use, and if twins will be obtained with/without replacement.\n\nFewer matches reduce bias (choosing only the closest observation), but increase variance.\nMore matches increase bias, but reduce variance. (because of less optimal matches)\nwith replacement: control units may be used more than once. This will improve matching quality reducing bias. But by using the same units multiple times, it will increase variance.\nwithout replacement: Control units are used once, potentially reducing matching quality, but reducing variance. It will be order dependent.\n\nsee Caliendo and Kopeing (2008)"
  },
  {
    "objectID": "adv_class/10matching.html#what-about-se-and-statistical-inference",
    "href": "adv_class/10matching.html#what-about-se-and-statistical-inference",
    "title": "Matching and Re-weighting",
    "section": "What about SE? and Statistical inference?",
    "text": "What about SE? and Statistical inference?\nWell….this is one of the few cases where Bootstrapping WON’T work!\nStandard errors are more cumbersome. So we will just rely on software results"
  },
  {
    "objectID": "adv_class/10matching.html#other-considerations",
    "href": "adv_class/10matching.html#other-considerations",
    "title": "Matching and Re-weighting",
    "section": "Other considerations",
    "text": "Other considerations\nOnce you have chosen your matching method, find your “statistical twins”, and estimate your differences you are done! (or are you)\nNot yet…common practice: Evaluate the balance of your data\n\nMatching aims to reduce or eliminate differences in characteristics between treatment and control units. Thus, one should evaluate the differences (before and after match) of your characteristis\n\n\nCheck for overlapping condition.\n\n\neither variable by variable or with pscore\n\n\nAssess Matching Quality: Have differences across groups vanished?\n\n\nCheck Standardized differences \\(\\frac{\\mu_1 - \\mu_2}{\\sqrt{0.5*(V_1 + V_2)}}\\)\nt-tests\nPR2 of regression with matched data"
  },
  {
    "objectID": "adv_class/10matching.html#implementation",
    "href": "adv_class/10matching.html#implementation",
    "title": "Matching and Re-weighting",
    "section": "Implementation",
    "text": "Implementation\nIn Stata, there are at least two approaches that can be used for matching:\n\npsmatch2 (from ssc)\nteffects (Official Stata command)\n\nWe will use this to answer a simple question:\n\nWhat is the impact of Traing Jobs on Earnings?"
  },
  {
    "objectID": "adv_class/10matching.html#example",
    "href": "adv_class/10matching.html#example",
    "title": "Matching and Re-weighting",
    "section": "Example",
    "text": "Example\nThis file contains information on experimental and observed data for the analysis of training on earnings program:\n\n\nCode\nuse https://friosavila.github.io/playingwithstata/drdid/lalonde.dta, clear\nkeep if year==1978 \ndrop if dwincl==0\nlabel define sample 1 \"exper\"  2 \"CPS\" 3 \"PSID\"\nlabel values sample sample\ntab sample treated,m\n\n\n(19,204 observations deleted)\n(277 observations deleted)\n\n           |             treated\n    sample |         0          1          . |     Total\n-----------+---------------------------------+----------\n     exper |       260        185          0 |       445 \n       CPS |         0          0     15,992 |    15,992 \n      PSID |         0          0      2,490 |     2,490 \n-----------+---------------------------------+----------\n     Total |       260        185     18,482 |    18,927 \n\n\nFirst Experimental design - RCT\n\n\nCode\nreg re treated\ntabstat age educ black married nodegree , by(treated)\nlogit treated age educ black hisp married nodegree \n\n\n\n      Source |       SS           df       MS      Number of obs   =       445\n-------------+----------------------------------   F(1, 443)       =      8.04\n       Model |   348013183         1   348013183   Prob &gt; F        =    0.0048\n    Residual |  1.9178e+10       443  43290369.3   R-squared       =    0.0178\n-------------+----------------------------------   Adj R-squared   =    0.0156\n       Total |  1.9526e+10       444  43976681.9   Root MSE        =    6579.5\n\n------------------------------------------------------------------------------\n          re | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     treated |   1794.342   632.8534     2.84   0.005     550.5745     3038.11\n       _cons |   4554.801   408.0459    11.16   0.000     3752.855    5356.747\n------------------------------------------------------------------------------\n\nSummary statistics: Mean\nGroup variable: treated \n\n treated |       age      educ     black   married  nodegree\n---------+--------------------------------------------------\n       0 |  25.05385  10.08846  .8269231  .1538462  .8346154\n       1 |  25.81622  10.34595  .8432432  .1891892  .7081081\n---------+--------------------------------------------------\n   Total |  25.37079  10.19551  .8337079  .1685393  .7820225\n------------------------------------------------------------\n\nIteration 0:  Log likelihood =     -302.1  \nIteration 1:  Log likelihood = -294.72908  \nIteration 2:  Log likelihood = -294.71464  \nIteration 3:  Log likelihood = -294.71464  \n\nLogistic regression                                     Number of obs =    445\n                                                        LR chi2(6)    =  14.77\n                                                        Prob &gt; chi2   = 0.0221\nLog likelihood = -294.71464                             Pseudo R2     = 0.0244\n\n------------------------------------------------------------------------------\n     treated | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0059171   .0142668     0.41   0.678    -.0220452    .0338794\n        educ |  -.0639597    .071354    -0.90   0.370     -.203811    .0758916\n       black |  -.2543689   .3639735    -0.70   0.485    -.9677438    .4590061\n        hisp |  -.8291587   .5042305    -1.64   0.100    -1.817432     .159115\n     married |   .2342415   .2661824     0.88   0.379    -.2874665    .7559495\n    nodegree |  -.8385524   .3093833    -2.71   0.007    -1.444933   -.2321722\n       _cons |   1.053028   1.047384     1.01   0.315    -.9998064    3.105862\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/10matching.html#section-1",
    "href": "adv_class/10matching.html#section-1",
    "title": "Matching and Re-weighting",
    "section": "",
    "text": "Then using PScore Matching CPS\n\n\nCode\nkeep if treated == 1 | sample ==2\nreplace treated=0 if treated==.\nreg re treated\ntabstat age educ black hisp married nodegree , by(treated)\n\n\n(2,750 observations deleted)\n(15,992 real changes made)\n\n      Source |       SS           df       MS      Number of obs   =    16,177\n-------------+----------------------------------   F(1, 16175)     =    142.43\n       Model |  1.3206e+10         1  1.3206e+10   Prob &gt; F        =    0.0000\n    Residual |  1.4997e+12    16,175  92717515.8   R-squared       =    0.0087\n-------------+----------------------------------   Adj R-squared   =    0.0087\n       Total |  1.5129e+12    16,176  93528158.4   Root MSE        =      9629\n\n------------------------------------------------------------------------------\n          re | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     treated |  -8497.516   712.0207   -11.93   0.000    -9893.156   -7101.877\n       _cons |   14846.66   76.14292   194.98   0.000     14697.41    14995.91\n------------------------------------------------------------------------------\n\nSummary statistics: Mean\nGroup variable: treated \n\n treated |       age      educ     black      hisp   married  nodegree\n---------+------------------------------------------------------------\n       0 |  33.22524  12.02751  .0735368   .072036  .7117309  .2958354\n       1 |  25.81622  10.34595  .8432432  .0594595  .1891892  .7081081\n---------+------------------------------------------------------------\n   Total |  33.14051  12.00828  .0823391  .0718922  .7057551  .3005502\n----------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/10matching.html#section-2",
    "href": "adv_class/10matching.html#section-2",
    "title": "Matching and Re-weighting",
    "section": "",
    "text": "We need to do trimming\n\n\nCode\nbysort educ black hisp married:egen n11=sum(treated==1)\nbysort age  black hisp married:egen n22=sum(treated==1)\ndrop if n11==0 | n22 ==0\ntabstat age educ black hisp married nodegree , by(treated)\nreg re treated\n\n\n(13,536 observations deleted)\n\nSummary statistics: Mean\nGroup variable: treated \n\n treated |       age      educ     black      hisp   married  nodegree\n---------+------------------------------------------------------------\n       0 |  24.24145  11.69788   .252443  .0260586  .3346906  .2569218\n       1 |  25.81622  10.34595  .8432432  .0594595  .1891892  .7081081\n---------+------------------------------------------------------------\n   Total |  24.35176  11.60318  .2938281  .0283983  .3244983  .2885271\n----------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     2,641\n-------------+----------------------------------   F(1, 2639)      =     73.89\n       Model |  5.7607e+09         1  5.7607e+09   Prob &gt; F        =    0.0000\n    Residual |  2.0575e+11     2,639  77964783.1   R-squared       =    0.0272\n-------------+----------------------------------   Adj R-squared   =    0.0269\n       Total |  2.1151e+11     2,640  80117339.3   Root MSE        =    8829.8\n\n------------------------------------------------------------------------------\n          re | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     treated |  -5786.584   673.1834    -8.60   0.000    -7106.605   -4466.564\n       _cons |   12135.73   178.1702    68.11   0.000     11786.36     12485.1\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/10matching.html#section-3",
    "href": "adv_class/10matching.html#section-3",
    "title": "Matching and Re-weighting",
    "section": "",
    "text": "Lets do some matching\n\n\nCode\nteffects nnmatch (re age educ black   married nodegree  ) (treated)\ntebalance summarize\nteffects nnmatch (re age educ black   married nodegree  ) (treated), nn(2)\ntebalance summarize\nteffects psmatch (re) (treated age educ black   married nodegree  )\ntebalance summarize\nteffects psmatch (re) (treated age educ black   married nodegree  ) ,  nn(2)\ntebalance summarize\n\n\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : nearest-neighbor matching     Matches: requested =          1\nOutcome model  : matching                                     min =          1\nDistance metric: Mahalanobis                                  max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |  -3685.665   1188.666    -3.10   0.002    -6015.407   -1355.923\n------------------------------------------------------------------------------\n(refitting the model using the generate() option)\n\nCovariate balance summary\n\n                         Raw      Matched\n-----------------------------------------\nNumber of obs =        2,641        5,282\nTreated obs   =          185        2,641\nControl obs   =        2,456        2,641\n-----------------------------------------\n\n-----------------------------------------------------------------\n                |Standardized differences          Variance ratio\n                |        Raw     Matched           Raw    Matched\n----------------+------------------------------------------------\n            age |   .2342346    -.015417      1.305844   .8410946\n           educ |  -.7684118   -.0812288      1.881909   .8598207\n          black |   1.473105           0      .7039609          1\n        married |  -.3351313   -.0008087      .6923501    .999393\n       nodegree |   1.010393           0      1.088086          1\n-----------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : nearest-neighbor matching     Matches: requested =          2\nOutcome model  : matching                                     min =          2\nDistance metric: Mahalanobis                                  max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |  -5166.888   1107.653    -4.66   0.000    -7337.848   -2995.929\n------------------------------------------------------------------------------\n(refitting the model using the generate() option)\n\nCovariate balance summary\n\n                         Raw      Matched\n-----------------------------------------\nNumber of obs =        2,641        5,282\nTreated obs   =          185        2,641\nControl obs   =        2,456        2,641\n-----------------------------------------\n\n-----------------------------------------------------------------\n                |Standardized differences          Variance ratio\n                |        Raw     Matched           Raw    Matched\n----------------+------------------------------------------------\n            age |   .2342346   -.0209048      1.305844   .7345997\n           educ |  -.7684118   -.0385284      1.881909   .8978301\n          black |   1.473105    .0074673      .7039609   1.006716\n        married |  -.3351313    -.004586      .6923501   .9965432\n       nodegree |   1.010393    .0016705      1.088086   1.001557\n-----------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : propensity-score matching     Matches: requested =          1\nOutcome model  : matching                                     min =          1\nTreatment model: logit                                        max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |  -4278.549   1135.847    -3.77   0.000    -6504.768   -2052.331\n------------------------------------------------------------------------------\n(refitting the model using the generate() option)\n\nCovariate balance summary\n\n                         Raw      Matched\n-----------------------------------------\nNumber of obs =        2,641        5,282\nTreated obs   =          185        2,641\nControl obs   =        2,456        2,641\n-----------------------------------------\n\n-----------------------------------------------------------------\n                |Standardized differences          Variance ratio\n                |        Raw     Matched           Raw    Matched\n----------------+------------------------------------------------\n            age |   .2342346    .0014058      1.305844   .9313458\n           educ |  -.7684118   -.1308249      1.881909   .9665937\n          black |   1.473105   -.0926638      .7039609     .90999\n        married |  -.3351313   -.0973289      .6923501   .9197524\n       nodegree |   1.010393    .0821105      1.088086    1.07103\n-----------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : propensity-score matching     Matches: requested =          2\nOutcome model  : matching                                     min =          2\nTreatment model: logit                                        max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |  -4380.078   1158.019    -3.78   0.000    -6649.754   -2110.403\n------------------------------------------------------------------------------\n(refitting the model using the generate() option)\n\nCovariate balance summary\n\n                         Raw      Matched\n-----------------------------------------\nNumber of obs =        2,641        5,282\nTreated obs   =          185        2,641\nControl obs   =        2,456        2,641\n-----------------------------------------\n\n-----------------------------------------------------------------\n                |Standardized differences          Variance ratio\n                |        Raw     Matched           Raw    Matched\n----------------+------------------------------------------------\n            age |   .2342346     -.06133      1.305844   .8834346\n           educ |  -.7684118   -.1321518      1.881909   1.021302\n          black |   1.473105   -.0698339      .7039609    .933348\n        married |  -.3351313   -.0414439      .6923501   .9674741\n       nodegree |   1.010393    .0939209      1.088086   1.080951\n-----------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/10matching.html#section-4",
    "href": "adv_class/10matching.html#section-4",
    "title": "Matching and Re-weighting",
    "section": "",
    "text": "A missing variable? Earnings in previous year. May capture information of Need to do treatment (selection)\n\n\nCode\ntabstat age educ black hisp married nodegree re74, by(treated)\ngen dre = re-re74\nteffects nnmatch (dre age educ black   married nodegree  ) (treated)\n\nteffects nnmatch (dre age educ black   married nodegree  ) (treated), nn(2)\n\nteffects psmatch (dre) (treated age educ black   married nodegree  )\n\nteffects psmatch (dre) (treated age educ black   married nodegree  ) ,  nn(2)\n\n\n\nSummary statistics: Mean\nGroup variable: treated \n\n treated |       age      educ     black      hisp   married  nodegree\n---------+------------------------------------------------------------\n       0 |  24.24145  11.69788   .252443  .0260586  .3346906  .2569218\n       1 |  25.81622  10.34595  .8432432  .0594595  .1891892  .7081081\n---------+------------------------------------------------------------\n   Total |  24.35176  11.60318  .2938281  .0283983  .3244983  .2885271\n----------------------------------------------------------------------\n\n treated |      re74\n---------+----------\n       0 |  9347.406\n       1 |  2095.574\n---------+----------\n   Total |  8839.421\n--------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : nearest-neighbor matching     Matches: requested =          1\nOutcome model  : matching                                     min =          1\nDistance metric: Mahalanobis                                  max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |   2616.653   1803.172     1.45   0.147    -917.4997    6150.806\n------------------------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : nearest-neighbor matching     Matches: requested =          2\nOutcome model  : matching                                     min =          2\nDistance metric: Mahalanobis                                  max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |   730.2925    1674.91     0.44   0.663     -2552.47    4013.055\n------------------------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : propensity-score matching     Matches: requested =          1\nOutcome model  : matching                                     min =          1\nTreatment model: logit                                        max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |   2162.311    1740.12     1.24   0.214    -1248.262    5572.884\n------------------------------------------------------------------------------\n\nTreatment-effects estimation                   Number of obs      =      2,641\nEstimator      : propensity-score matching     Matches: requested =          2\nOutcome model  : matching                                     min =          2\nTreatment model: logit                                        max =        138\n------------------------------------------------------------------------------\n             |              AI robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |    1833.03   1739.496     1.05   0.292    -1576.318    5242.379\n------------------------------------------------------------------------------\n\n\nIn this case, Matching alone could not get the right answer. Who were the most likely to “go to the training?”\nSo instead we change the question: How much the change in earnings compare across groups."
  },
  {
    "objectID": "adv_class/10matching.html#wait-what-about-reweighting",
    "href": "adv_class/10matching.html#wait-what-about-reweighting",
    "title": "Matching and Re-weighting",
    "section": "Wait: What about Reweighting?",
    "text": "Wait: What about Reweighting?\nAn alternative method to Matching is to do Re-weighting.\nWe have seen this!\nYour control group has a distribution \\(g(x)\\) and your treatment \\(f(x)\\). We can use some weighting factors \\(h(x)\\) that reshapes \\(g(x)\\rightarrow \\hat f(x)\\).\nHow? Using Propensity scores\nWhy does it work? Just as matching, your goal is to compare distributions of outcomes, forcing differences in observed characteristics to be the same.\nIPW, does this by reweighting the distribution! (rather than matching)"
  },
  {
    "objectID": "adv_class/10matching.html#inverse-probability-weightingipw",
    "href": "adv_class/10matching.html#inverse-probability-weightingipw",
    "title": "Matching and Re-weighting",
    "section": "Inverse Probability Weighting:IPW",
    "text": "Inverse Probability Weighting:IPW\ns1: Estimate Pscore\n\\[\np(D=1|X)=F(X\\beta)\n\\] S2: Estimate IPW\nFor ATT: \\(W(D=1,x)=1 \\ \\& \\ W(D=0,X)=\\frac{\\hat p(x)}{1-\\hat p(x)}\\)\nFor ATU: \\(W(D=0,x)=1 \\ \\& \\ W(D=1,X)=\\frac{1-\\hat p(x)}{\\hat p(x)}\\)\nFor ATE: \\(W(D=0,x)=\\frac{1}{1-\\hat p(x)} \\ \\& \\ W(D=1,X)=\\frac{1}{\\hat p(x)}\\)\ns3: Estimate Treatment effect:\n\\[\nTE = \\sum_{i \\in D=1} w_i^s(1) Y_i - \\sum_{i \\in D=0} w_i^s(0) Y_i\n\\]"
  },
  {
    "objectID": "adv_class/10matching.html#even-better-go-dr",
    "href": "adv_class/10matching.html#even-better-go-dr",
    "title": "Matching and Re-weighting",
    "section": "Even Better: Go DR!",
    "text": "Even Better: Go DR!\nAn interesting advantage of IPW approach is that you can gain efficiency using Doubly Robust Methods. Namely, instead of comparing outcomes directly, you could compare predicted outcomes!\n\\[\n\\begin{aligned}\nATT &= \\frac{1}{N_t}\\sum(Y_1-X'\\hat\\beta_w^0) \\\\\nATU &= \\frac{1}{N_c}\\sum(X'\\hat\\beta_w^1-Y_0) \\\\\nATE &= \\frac{1}{N}\\sum(X'\\hat\\beta_w^1-X'\\hat\\beta_w^0)\n\\end{aligned}\n\\] where \\(\\hat \\beta^k_w\\) can be modeled using weighted least squares"
  },
  {
    "objectID": "adv_class/10matching.html#comparing-to-matching",
    "href": "adv_class/10matching.html#comparing-to-matching",
    "title": "Matching and Re-weighting",
    "section": "Comparing to Matching",
    "text": "Comparing to Matching\nteffects ipw (re) (treated age educ black   married nodegree) , iter(3) nolog\nteffects ipwra (re age educ black   married nodegree) (treated age educ black married nodegree), iter(3) nolog\nteffects ipw (dre) (treated age educ black   married nodegree), iter(3) nolog\nteffects ipwra (dre age educ black   married nodegree) (treated age educ black   married nodegree), iter(3) nolog\n\nTreatment-effects estimation                    Number of obs     =      2,641\nEstimator      : inverse-probability weights\nOutcome model  : weighted mean\nTreatment model: logit\n------------------------------------------------------------------------------\n             |               Robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |  -4833.352   1088.667    -4.44   0.000    -6967.101   -2699.603\n-------------+----------------------------------------------------------------\nPOmean       |\n     treated |\n          0  |   11979.19   179.1903    66.85   0.000     11627.99     12330.4\n------------------------------------------------------------------------------\n\nTreatment-effects estimation                    Number of obs     =      2,641\nEstimator      : IPW regression adjustment\nOutcome model  : linear\nTreatment model: logit\n------------------------------------------------------------------------------\n             |               Robust\n          re | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |   -4835.38   1012.598    -4.78   0.000    -6820.036   -2850.724\n-------------+----------------------------------------------------------------\nPOmean       |\n     treated |\n          0  |   11976.52   179.0958    66.87   0.000     11625.49    12327.54\n------------------------------------------------------------------------------\nWarning: Convergence not achieved.\n\nTreatment-effects estimation                    Number of obs     =      2,641\nEstimator      : inverse-probability weights\nOutcome model  : weighted mean\nTreatment model: logit\n------------------------------------------------------------------------------\n             |               Robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |    1475.71   1792.427     0.82   0.410    -2037.382    4988.802\n-------------+----------------------------------------------------------------\nPOmean       |\n     treated |\n          0  |   2746.475   161.2845    17.03   0.000     2430.363    3062.587\n------------------------------------------------------------------------------\nWarning: Convergence not achieved.\n\nTreatment-effects estimation                    Number of obs     =      2,641\nEstimator      : IPW regression adjustment\nOutcome model  : linear\nTreatment model: logit\n------------------------------------------------------------------------------\n             |               Robust\n         dre | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATE          |\n     treated |\n   (1 vs 0)  |   1286.605   1516.493     0.85   0.396    -1685.666    4258.875\n-------------+----------------------------------------------------------------\nPOmean       |\n     treated |\n          0  |   2754.756   161.4406    17.06   0.000     2438.338    3071.173\n------------------------------------------------------------------------------\nWarning: Convergence not achieved."
  },
  {
    "objectID": "adv_class/08panel_FE.html#re-cap-potential-outcome-model",
    "href": "adv_class/08panel_FE.html#re-cap-potential-outcome-model",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Re-Cap: Potential outcome Model",
    "text": "Re-Cap: Potential outcome Model\nIn the ideal world, where we can see all possible outcomes and scenarios of your potential treatments, it will be very simple to estimate treatment effects:\n\\[\n\\delta_i = Y_i(1)-Y_i(0)\n\\]\nWhy does this work??\nOne way to understand this it to imagine that potential outcomes are a function of all observed and unobserved individual characteristics, plust the treatment Status.\n\\[y_i(D)=y_i(X,u,D)\n\\]\nSo when comparing a person with himself (clones or parallel worlds), we know (or at least expect) that everything else is the same, except for the Treatment Status.\nDifferences between the two states are explained only by the treatment!"
  },
  {
    "objectID": "adv_class/08panel_FE.html#the-problem-and-first-solution-rct",
    "href": "adv_class/08panel_FE.html#the-problem-and-first-solution-rct",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "The Problem and first Solution RCT",
    "text": "The Problem and first Solution RCT\nWe do not observe both States at the same time. People will either be treated or untreated, not both.\nSo what can we do?\n\nWe need to find good counterfactuals!\n\nOne way to do so is via RCT, for example using a lottery!\n\nWhy does it work?\n\nPotential outcomes will be unrelated to treatment, because treatmet is assigned at random.\nHere, it also means that \\(X's\\) and \\(u's\\) will be similar across groups (because of random assigment)\nBut…you cannot estimatate individual effects, but at least estimate aggregate effects (ATE = ATT = ATU)"
  },
  {
    "objectID": "adv_class/08panel_FE.html#other-solutions",
    "href": "adv_class/08panel_FE.html#other-solutions",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Other Solutions?",
    "text": "Other Solutions?\nSo RCTs can be very expensive, and difficult to implement after the fact. In those situations, however, you can use observed data to try answering the same questions!.\nOne option? Something we have done before…Regression Analysis!\n\\[y_i = a_0 + \\delta D_i + X_i\\beta + e_i\\]\nThe idea is that you directly control for all confounding factors that could be related to \\(y_i\\) and \\(d_i\\).\nIn other words, you add controls until \\(D_i\\) is exogenous! \\(E(e_i|D)=0\\)"
  },
  {
    "objectID": "adv_class/08panel_FE.html#implications-to-the-po-model",
    "href": "adv_class/08panel_FE.html#implications-to-the-po-model",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Implications to the PO model?",
    "text": "Implications to the PO model?\n\nAssumes all individuals have the same outcome structure (\\(\\beta s\\)), except for the TE\nThe treatment is effect is homogenous (no heterogeneity)\nand that functional form is correct (for extrapolation)\n\nHowever, explicitly controlling for covariates, balances characteristics (FWL):\n\\[\n\\begin{aligned}\nD_i &= X\\gamma + v_i \\\\\ny_i &= a_0 + \\delta v_i + u_i \\\\\n\\delta &=\\frac{1}{N} \\sum \\frac{D_i - X\\gamma}{var(v_i)} y_i\n\\end{aligned}\n\\]\n\nTreated units will get positive weights, and controls negative weights, with exceptions because of the LPM.\nWeights will “balance Samples” to estimate ATE."
  },
  {
    "objectID": "adv_class/08panel_FE.html#controlling-for-unobservables",
    "href": "adv_class/08panel_FE.html#controlling-for-unobservables",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Controlling for Unobservables",
    "text": "Controlling for Unobservables"
  },
  {
    "objectID": "adv_class/08panel_FE.html#what-if-you-can-see-xs",
    "href": "adv_class/08panel_FE.html#what-if-you-can-see-xs",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "What if you can See X’s",
    "text": "What if you can See X’s\nSome times, you may have situations where some covariates cannot be observed (Z_i): \\[\ny_i = \\delta D_i +X_i \\beta + Z_i \\gamma + e_i\n\\]\nIf \\(Z_i\\) is unrelated to \\(D_i\\), you are on the clear. If its unrelated to \\(Y_i\\) you are also ok. But what if that doesn happen?\n\nThen you have a problem!\n\nYou no longer can use regression, because the potential outcomes will no longer be independent of the treatment.\nyou are dooomed!\n(when would this happen)"
  },
  {
    "objectID": "adv_class/08panel_FE.html#having-access-to-more-data",
    "href": "adv_class/08panel_FE.html#having-access-to-more-data",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Having access to More Data",
    "text": "Having access to More Data\nSolution?: Say you have access to panel data: Same individuals across time:\n\\[\ny_{it} = \\delta D_{it} +X_{it} \\beta + Z_{it} \\gamma + e_{it}\n\\]\nIf we can’t measure \\(Z_{it}\\), and you estimate this using Pool OLS (just simple OLS), you still need the assumption that:\n\\[E(Z_{it}\\gamma + e_{it}|D_it)=0\n\\]\nBut that doesnt solve the problem if \\(Z_{it}\\) is related to \\(D_it\\).\nOne option, in cases like this, is assuming that individual unobservables are fixed across time:\n\\[\ny_{it} = \\delta D_{it} +X_{it} \\beta + Z_{i} \\gamma + e_{it}\n\\]\nin which case, it may be possible to estimate Treatment effects"
  },
  {
    "objectID": "adv_class/08panel_FE.html#fixed-effects",
    "href": "adv_class/08panel_FE.html#fixed-effects",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Fixed Effects",
    "text": "Fixed Effects\nWith panel data and assuming unobservables are fixed across time, estimating TE is “Easy”. Just add Dummies for each individual!\n\\[\ny_{it}= \\delta D_{it} +X_{it} \\beta + \\sum d_i \\gamma_i + e_{it}\n\\]\nHere \\(d_i \\gamma_i\\) is our proxy for ALL unobserved factors. OLS can be used to estimate ATEs\nThis happens because we can estimate potential outcome under the same assumptions as before.\nyou could, in fact, consider adding fixed effects for all dimensions you consider important to account for:\n\nCity, school, region, age, industry, etc\n\nThe only limitation…how many dummies can your computer handle? What happens internally?"
  },
  {
    "objectID": "adv_class/08panel_FE.html#fixed-effects-estimation---the-variation-within",
    "href": "adv_class/08panel_FE.html#fixed-effects-estimation---the-variation-within",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Fixed Effects: Estimation - The variation within",
    "text": "Fixed Effects: Estimation - The variation within\nThe obvious approach is using dummies. But that can take you only so far (why?), and may create other problems! (excluded variables)\nThe alternative is using the within estimator. Say we take the means across individuals, and use that to substract information from the original regression:\n\\[\n\\begin{aligned}\ny_{it} &= \\delta D_{it} +X_{it} \\beta + Z_{i} \\gamma + e_{it} \\\\\n\\bar y_i &= \\delta \\bar D_i + \\bar X_i \\beta +  Z_{i} \\gamma + \\bar e_i \\\\\ny_{it}-\\bar y_i = \\tilde y_{it} &=\\delta \\tilde D_{it} + \\tilde X_{it}\\beta+\\tilde e_{it}\n\\end{aligned}\n\\]\nLast equation is easier to estimate (no dummies!) however you need within variation. IF unobserved factors are fixed, they will be “absorbed”.\nAlso, the SE will have to be adjusted for degrees of freedom. (but nothing else)\nThis is nothing else but the use of FWL and regression on residuals."
  },
  {
    "objectID": "adv_class/08panel_FE.html#dont-forget-random-effects",
    "href": "adv_class/08panel_FE.html#dont-forget-random-effects",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Dont Forget Random Effects",
    "text": "Dont Forget Random Effects\nThis approach is more efficient than Fixed effects because you don’t estimate fixed effects, just the distribution.\nSo how does this affect the estimation:\n\nErrors have two components. One time fixed \\(e_i\\), and one time variant \\(u_{it}\\). Then total errors will be correlated with themselves across time \\[corr(v_{it}, v_{is}) =  corr(e_i+u_{it}, e_i+u_{is}) = \\sigma^2_e\n\\]\nApply FGLS to eliminating this source of auto-correlation! \\[y_{it}-\\lambda \\bar y_i = (X_{it}-\\lambda \\bar X_it) + v_{it}\\]\n\nBut, you need the assumption that unobservables \\(e_i\\) are unrelated to \\(X's\\). (because we are not directly controlling for them).\nThe advantage, however, is that you do no need within variation!"
  },
  {
    "objectID": "adv_class/08panel_FE.html#fe-vs-re",
    "href": "adv_class/08panel_FE.html#fe-vs-re",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "FE vs RE",
    "text": "FE vs RE\nSo there are two ways to Analyze data Panel data.\n\nFE: Uses only within variation, is more consistent, but less efficient (too many dummies)\nRE: Uses all variation in data, is less consistent (stronger assumptions), but more efficient!\n\nHow to choose?\nThe Standard approach is to apply a Hausan Test:\n\nH0:\\(\\beta^{FE} = \\beta^{RE}\\) using Chi2\n\nIf they are not different (H0 cannot be rejected), then choose RE (efficient). If they are different then choose FE (consistent)"
  },
  {
    "objectID": "adv_class/08panel_FE.html#more-fixed-effects-twfe---nwfe",
    "href": "adv_class/08panel_FE.html#more-fixed-effects-twfe---nwfe",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "More Fixed effects: TWFE - NWFE?",
    "text": "More Fixed effects: TWFE - NWFE?\nWith multiple sets of fixed effects (individual, time, cohort, region, etc), you can still use dummies to add them to the model.\nBut, you can apply something similar to the previous approach:\n\\[\n\\begin{aligned}\ny_{it} &= \\delta D_{it}+x_{it}\\beta + \\gamma_i + \\gamma_t + e_{it} \\\\\n\\bar y_i &= \\bar D_i +\\bar x_{i}\\beta + \\gamma_i + E(\\gamma_t|i) + \\bar e_i \\\\\n\\bar y_t &= \\bar D_t +\\bar x_{t}\\beta + E(\\gamma_i|t) + \\gamma_t + \\bar e_t \\\\\n\\bar y &= \\bar D +\\bar x\\beta + E(\\gamma_i) + E(\\gamma_t)+ \\bar e \\\\\n\\tilde y_{it} &= y_{it}-\\bar y_i - \\bar y_t + \\bar y\n\\end{aligned}\n\\]\nSo one can estimate the following:\n\\[\n\\tilde y_{it} = \\delta \\tilde D_{it} + \\tilde X_{it} \\beta + \\tilde e_{it}\n\\]\nThis eliminates FE for both time and individual (if panel is balanced)"
  },
  {
    "objectID": "adv_class/08panel_FE.html#second-option",
    "href": "adv_class/08panel_FE.html#second-option",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Second Option:",
    "text": "Second Option:\nAlternatively, you can just run regressions of residuals:\n\\[\nw_{it} = \\gamma^w_i+\\gamma^w_t+rw_{it}\n\\]\nand make regressions using the residuals. (Demeaning also works, but its an iterative process)"
  },
  {
    "objectID": "adv_class/08panel_FE.html#stata-example",
    "href": "adv_class/08panel_FE.html#stata-example",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Stata Example",
    "text": "Stata Example\n\n\n\n\n\n\n\nCode\n#delimit;\nfrause school93_98, clear;\nxtset schid year;\nqui:reg math4 lunch lenrol lrexpp                     ; est sto m1;\nqui:xtreg math4 lunch lenrol lrexpp                   ; est sto m2;\nqui:xtreg math4 lunch lenrol lrexpp, fe               ; est sto m3;\nqui:reghdfe math4 lunch lenrol lrexpp, abs(schid)     ; est sto m4;\nqui:reghdfe math4 lunch lenrol lrexpp, abs(schid year); est sto m5;\nesttab m1 m2 m3 m4 m5, mtitle(ols re fe refe1 refe2) compress se b(3);\nhausman m3 m2;\n\n\n\nPanel variable: schid (strongly balanced)\n Time variable: year, 1993 to 1998\n         Delta: 1 unit\n\n---------------------------------------------------------------------------\n                 (1)          (2)          (3)          (4)          (5)   \n                 ols           re           fe        refe1        refe2   \n---------------------------------------------------------------------------\nlunch         -0.413***    -0.370***     0.057        0.057       -0.062*  \n             (0.007)      (0.011)      (0.031)      (0.031)      (0.026)   \n\nlenrol        -0.121        0.936        8.766***     8.766***     0.297   \n             (0.425)      (0.616)      (1.704)      (1.704)      (1.468)   \n\nlrexpp        28.887***    39.161***    46.450***    46.450***     2.799*  \n             (0.860)      (0.878)      (1.006)      (1.006)      (1.265)   \n\n_cons       -162.292***  -254.864***  -377.338***  -377.423***    37.398*  \n             (7.960)      (8.681)     (14.913)     (14.918)     (15.847)   \n---------------------------------------------------------------------------\nN               9369         9369         9369         9328         9328   \n---------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n                 ---- Coefficients ----\n             |      (b)          (B)            (b-B)     sqrt(diag(V_b-V_B))\n             |       m3           m2         Difference       Std. err.\n-------------+----------------------------------------------------------------\n       lunch |     .056932    -.3703211        .4272531        .0287753\n      lenrol |    8.766051     .9357725        7.830279        1.588902\n      lrexpp |    46.44966     39.16107        7.288595        .4915896\n------------------------------------------------------------------------------\n                          b = Consistent under H0 and Ha; obtained from xtreg.\n           B = Inconsistent under Ha, efficient under H0; obtained from xtreg.\n\nTest of H0: Difference in coefficients not systematic\n\n    chi2(3) = (b-B)'[(V_b-V_B)^(-1)](b-B)\n            = 627.26\nProb &gt; chi2 = 0.0000"
  },
  {
    "objectID": "adv_class/08panel_FE.html#correlated-random-effects",
    "href": "adv_class/08panel_FE.html#correlated-random-effects",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Correlated Random Effects",
    "text": "Correlated Random Effects\nRandom effects model may produce inconsistent results, because it assumes unobserved factors are uncorrelated to characteristics.\nFixed effects controls for individual effects explicitly, or via demeaning.\nA 3rd approach is known as CRE model. A more explicit modeling of the unobserved but fixed components.\n\nCall the unobserved component \\(a_i\\), and say we suspect it may be related with individual characteristics.\nBecause \\(a_i\\) is constant over time, it may be reasonable assuming its correlated with individual average characteristics: \\[a_i = a + \\bar X_i \\gamma + r_i\n\\qquad(1)\\]\n\nBy construction, \\(r_i\\) and \\(X_{it} \\& \\bar X_i\\) will be uncorrelated. So lets just add that to the main model"
  },
  {
    "objectID": "adv_class/08panel_FE.html#cre",
    "href": "adv_class/08panel_FE.html#cre",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "CRE",
    "text": "CRE\nLets add Equation 1 to our main equation. \\[\ny_i = \\beta X_{it} +  \\theta Z_{i} + \\bar X_i \\gamma + r_i + e_{it}\n\\]\nThis equation can now be estimated using RE, because it already allows controls for the correlation of unobserved factors and the individual effects.\nYou can also estimate the model using pool OLS, clustering errors at individual level.\nResult:\n\nyou now have a model that allows for time variant and time fixed components, that is consistent as FE (same coefficients).\n\nUses:\n\nSimpler way to test for FE vs RE (are the \\(\\gamma 's\\) significant?)\nthere is no need for within variation for any variable! (just overall variation)"
  },
  {
    "objectID": "adv_class/08panel_FE.html#cre-in-stata",
    "href": "adv_class/08panel_FE.html#cre-in-stata",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "cre in Stata",
    "text": "cre in Stata\n\n\nCode\n#delimit cr\nfrause school93_98, clear\nreghdfe math4 lunch lenrol lrexpp, abs(schid year) cluster(schid)\n** Experimental\ncre, abs(schid year): reg math4 lunch lenrol lrexpp, cluster(schid)\n\n\n(dropped 41 singleton observations)\n(MWFE estimator converged in 5 iterations)\n\nHDFE Linear regression                            Number of obs   =      9,328\nAbsorbing 2 HDFE groups                           F(   3,   1734) =       2.59\nStatistics robust to heteroskedasticity           Prob &gt; F        =     0.0515\n                                                  R-squared       =     0.7548\n                                                  Adj R-squared   =     0.6985\n                                                  Within R-sq.    =     0.0014\nNumber of clusters (schid)   =      1,735         Root MSE        =    11.5747\n\n                              (Std. err. adjusted for 1,735 clusters in schid)\n------------------------------------------------------------------------------\n             |               Robust\n       math4 | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       lunch |  -.0620863   .0324188    -1.92   0.056    -.1256705    .0014978\n      lenrol |   .2966956   1.484868     0.20   0.842    -2.615625    3.209017\n      lrexpp |   2.798777   1.410581     1.98   0.047     .0321579    5.565397\n       _cons |   37.39798    16.8327     2.22   0.026     4.383449     70.4125\n------------------------------------------------------------------------------\n\nAbsorbed degrees of freedom:\n-----------------------------------------------------+\n Absorbed FE | Categories  - Redundant  = Num. Coefs |\n-------------+---------------------------------------|\n       schid |      1735        1735           0    *|\n        year |         6           0           6     |\n-----------------------------------------------------+\n* = FE nested within cluster; treated as redundant for DoF computation\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n_reghdfe_r~d |      9,328    3.88e-17    .1015168  -1.180599   1.214313\n\n\nLinear regression                               Number of obs     =      9,328\n                                                F(9, 1734)        =     821.90\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.4595\n                                                Root MSE          =     15.505\n\n                              (Std. err. adjusted for 1,735 clusters in schid)\n------------------------------------------------------------------------------\n             |               Robust\n       math4 | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       lunch |  -.0620863   .0327488    -1.90   0.058    -.1263177     .002145\n      lenrol |   .2966948   1.509296     0.20   0.844    -2.663538    3.256927\n      lrexpp |   2.798777   1.435068     1.95   0.051    -.0158696    5.613423\n    m1_lunch |  -.3799049   .0343728   -11.05   0.000    -.4473213   -.3124884\n    m2_lunch |  -2.750266   .5058811    -5.44   0.000    -3.742467   -1.758065\n   m1_lenrol |  -2.394415   1.633657    -1.47   0.143    -5.598561     .809731\n   m2_lenrol |  -273.8924   11.25589   -24.33   0.000    -295.9689   -251.8158\n   m1_lrexpp |   5.667779   2.276245     2.49   0.013     1.203305    10.13225\n   m2_lrexpp |    73.4047   3.276595    22.40   0.000      66.9782    79.83119\n       _cons |   37.39799   17.10933     2.19   0.029     3.840901    70.95507\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/08panel_FE.html#caveats-not-everything-is-solved-using-fe",
    "href": "adv_class/08panel_FE.html#caveats-not-everything-is-solved-using-fe",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Caveats: Not everything is solved using FE",
    "text": "Caveats: Not everything is solved using FE\n\nWhile FE allows you do control for unobserve but time fixed factors, it will Not help you if those factors are time varying.\nif \\(e_{it}\\) is different across treated and control groups \\(D_{it}=0,1\\) then TE cannot be estimated.\nThis could happen if cases of reverse causality or\nBecause it depends strongly on within variation, it will be more sensitive to measurement errors. Specifically:\n\n\\[\\beta^{fe} = \\beta * \\left(1-\\frac{\\sigma_v^2}{(\\sigma^2_v+\\sigma^2_x)(1-\\rho_x)}\\right)\n\\]\nIn other words. when \\(X\\) has strong autocorrelation (Stable treatment), the measurement error effect is far larger!"
  },
  {
    "objectID": "adv_class/08panel_FE.html#caveats-fe-makes-things-harder-to-analyze",
    "href": "adv_class/08panel_FE.html#caveats-fe-makes-things-harder-to-analyze",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Caveats: FE makes things harder to analyze",
    "text": "Caveats: FE makes things harder to analyze\n\nWhen using a single FE, OLS using within variation to identify the slope coefficients. How does a change in X’s (compared to the average) affect changes in the outcomes (respect to averages)\nWhen using Two Fixed effects (individuals and time) identification becomes tricky: \\[\\tilde y_{it} = y_{it}-\\bar y_i - \\bar y_t + \\bar y  \\]\n\nWe are looking for variation across time but also across individuals.\n\nwe are using changes in outcome that are different from the average changes in the sample.\n\n\nwith Multiple FE, same story…we are trying to exploit variation across multiple dimensions! Difficult to understand"
  },
  {
    "objectID": "adv_class/08panel_FE.html#caveats-some-times-the-variation-may-be-wrong",
    "href": "adv_class/08panel_FE.html#caveats-some-times-the-variation-may-be-wrong",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Caveats: Some times, the variation may be wrong:",
    "text": "Caveats: Some times, the variation may be wrong:\nConsider:\n\\[y_{it} = a_i + a_t + \\delta D_{it} + e_{it}\\]\nIf \\(D_it\\) changes only for some people at the same time, we are good.\n\nThe variation comes from comparing individuals (before and after) (time variation), who were treated and untreated (individual effects)\n\nBut if \\(D_{it}\\) changes at different times for different people, we have a problem.\n\nWho is being compared???\n\nThose before and after (fine) to those with Status change (D=0 -&gt; D=1) to those whos status do not change! (D=0 to D=0) or (D=1 to D=1)\n\n\nWe will discuss this problem again when talking about DID"
  },
  {
    "objectID": "adv_class/08panel_FE.html#income-schooling-and-ability",
    "href": "adv_class/08panel_FE.html#income-schooling-and-ability",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Income, Schooling, and Ability:",
    "text": "Income, Schooling, and Ability:\nEvidence from a New Sample of Identical Twins\nby\nOrley Ashenfelter and Cecilia Rouse"
  },
  {
    "objectID": "adv_class/08panel_FE.html#motivation",
    "href": "adv_class/08panel_FE.html#motivation",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Motivation:",
    "text": "Motivation:\nIn search of Returns to Education\nThis paper aims to identify returns of education abstracting from the impact of innate ability.\nIn their framework, ability is mostly explained by genetics, thus to control for it, the authors use a sample of identical twins, to “absorb” unobserved genetics using FE.\nThey address some of the problems inherited to FE estimation"
  },
  {
    "objectID": "adv_class/08panel_FE.html#the-model",
    "href": "adv_class/08panel_FE.html#the-model",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "The model",
    "text": "The model\n\nThe theoretical model described states that all individuals have an optimal level of Schooling, such that maximizes the his/her returns.\nHowever, Total schooling may be affected by measurement or optimization errors.\nSchooling will be directly affected by returns to education, but also by the ability of students.\n\nIn their framework, for the twins setup, (log)earnings will be determined by:\n\\[\n\\begin{aligned}\ny_{1j}=A_j + b_j S_{1j} + \\gamma X_j + e_{1j} \\\\\ny_{2j}=A_j + b_j S_{2j} + \\gamma X_j + e_{2j}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/08panel_FE.html#the-model-1",
    "href": "adv_class/08panel_FE.html#the-model-1",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "The model",
    "text": "The model\nBecause ability is related to schooling, they suggest using the following:\n\\[\ny_{ij}=\\lambda(0.5(S_{1j}+S_{2j}) + b_j S_{1j} + \\gamma X_j + v_j+ e_{ij}\n\\]\nWhich is the equivalent to CRE. Or estimate the fixed effects equivalent:\n\\[y_{1j} - y_{2j} = b(S_{2j}-S_{1j}) + e_{2j} - e_{2j}\n\\]\nThe later is a First difference, rather than FE estimator, but they both identical when T=2.\n\nAn additional model the authors use is one where returns to education could be related to ability.\nOr where ability is measured/proxied by parents education. (which is fixed across twins)"
  },
  {
    "objectID": "adv_class/08panel_FE.html#data",
    "href": "adv_class/08panel_FE.html#data",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "adv_class/08panel_FE.html#ols",
    "href": "adv_class/08panel_FE.html#ols",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "OLS",
    "text": "OLS"
  },
  {
    "objectID": "adv_class/08panel_FE.html#fe-re-cre",
    "href": "adv_class/08panel_FE.html#fe-re-cre",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "FE-RE-CRE?",
    "text": "FE-RE-CRE?"
  },
  {
    "objectID": "adv_class/08panel_FE.html#heterogeneity",
    "href": "adv_class/08panel_FE.html#heterogeneity",
    "title": "Panel data and Fixed Effects (Many FE)",
    "section": "Heterogeneity",
    "text": "Heterogeneity"
  },
  {
    "objectID": "adv_class/06nlreg.html#sowhat-is-non-linear",
    "href": "adv_class/06nlreg.html#sowhat-is-non-linear",
    "title": "NLS, IRLS, and MLE",
    "section": "So…what is non-linear?",
    "text": "So…what is non-linear?\nOptions:\n\\[\n\\begin{aligned}\ny = b_0 + b_1 x^{b_2}+e \\\\\ny = exp(b_0+b_1 x)+e \\\\\ny = exp(b_0+b_1 x+e) \\\\\ny = h(x\\beta)+e \\\\\ny = h(x\\beta+e)\n\\end{aligned}\n\\]\nAll of them are Nonlinear, but some of them are linearizable.\n\nA Linearizable model is one you can apply a transformation and make it linear.\n\nFor models #2 and #4, you could apply (logs) or \\(h^{-1}()\\) (if functions), and use OLS. For the others you need other methods."
  },
  {
    "objectID": "adv_class/06nlreg.html#how-do-you-do-nl",
    "href": "adv_class/06nlreg.html#how-do-you-do-nl",
    "title": "NLS, IRLS, and MLE",
    "section": "How do you do, NL?",
    "text": "How do you do, NL?\nNonlinear models are tricky. In contrast with our good old OLS, there no “close form” solution we can plug in:\n\\[\n\\beta = (X'X)^{-1}X'y\n\\]\nWe already saw this! For Quantile regressions, we never did it by-hand (requires linear programming). Because, Qregressions are also nolinear.\nIn this section, we will cover some of the few methodologies that are available for the estimation of Nonlinear models. We start with the first, an extension to OLS, we will call NLS.\n\\[\ny = h(x,\\beta) + e\n\\]\nWhat makes this model NLS, is that the error adds to the outcome (or CEF)! However, the CEF is modeled as a nolinear function of \\(X's\\) and \\(\\beta's\\). (but we still aim to MIN SSR)"
  },
  {
    "objectID": "adv_class/06nlreg.html#some-assumptions",
    "href": "adv_class/06nlreg.html#some-assumptions",
    "title": "NLS, IRLS, and MLE",
    "section": "Some Assumptions",
    "text": "Some Assumptions\nFor identification and estimation NLS requires similar assumptions as OLS:\n\nFunctional form: \\(E(y|X)\\) is given by \\(h(x,\\beta)\\), which is continuous and differentiable.\nThere is a unique solution! (like no-multicolinearity). if \\(\\beta\\) Minimizes the errors, then there is no other \\(\\beta_0\\) that will give the same solution.\nThe expected value of the error is zero \\(E(e)=0\\), and \\(E(e|h(x,\\beta))=0\\) . Similar to No endogeneity, but constrained by functional form.\nData is well behaved. (no extreme distributions, so that mean and variance exists)\n\nUnder this assumptions, its possible to Estimate the coefficients of interest."
  },
  {
    "objectID": "adv_class/06nlreg.html#but-how",
    "href": "adv_class/06nlreg.html#but-how",
    "title": "NLS, IRLS, and MLE",
    "section": "But How?",
    "text": "But How?\nNLS aims to choose \\(\\beta's\\) to minimize the sum of squared residuals:\n\\[\nSSR(\\beta) = \\sum(y-h(x,\\beta))^2 = [y-h(x,\\beta)]'[y-h(x,\\beta)]\n\\]\nThe FOC of this model are a non-linear system of equations.\n\\[\n\\frac{\\partial SSR(\\beta)}{\\partial \\beta}=-2 \\Bigg[\\frac{\\partial h(x,\\beta)}{\\partial \\beta}\\Bigg]' [y-h(x,\\beta)] =-2\\color{red}{\\tilde X'} e\n\\]\nSo how? Lets Start with a 2nd Order Taylor Expansion:\n\\[\n\\begin{aligned}\nSSR(\\beta) &\\simeq  SSR(\\beta_0) + g(\\beta_0)' (\\beta-\\beta_0)+\\frac{1}{2}(\\beta-\\beta_0)'H(\\beta_0)(\\beta-\\beta_0) \\\\\ng(\\beta) &=-2 \\tilde X'e ;H(\\beta)=2 \\tilde X'\\tilde X ; \\tilde X=\n\\Bigg[\\frac{\\partial h(x,\\beta)}{\\partial \\beta}\\Bigg]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/06nlreg.html#foc-again..",
    "href": "adv_class/06nlreg.html#foc-again..",
    "title": "NLS, IRLS, and MLE",
    "section": "FOC again..",
    "text": "FOC again..\n\\[\n\\begin{aligned}\ng(\\beta_0)+H(\\beta_0)(\\beta-\\beta_0)&=0 \\\\\n\\beta-\\beta_0 &= -H(\\beta_0)^{-1}g(\\beta_0) \\\\\n\\beta_t &=\\beta_{t-1}-H(\\beta_{t-1})^{-1}g(\\beta_{t-1})\n\\end{aligned}\n\\]\nThis simply says, In order to solve the system, you need to use a recursive system, so that \\(\\beta's\\) are updated until there is no longer a change.\nThis Iterative process is also known as a Newton Raphson method to solve nonlinear equations (if a solution exists).\nWhy does this work?\n\nYou change \\(\\beta\\) in the direction that should minimize SSR. (that direction is \\(g(.,.)\\).\nThat get to that change the “fastest” way possible using the Hessian\n\nThis is the most basic numerical optimization method."
  },
  {
    "objectID": "adv_class/06nlreg.html#small-example",
    "href": "adv_class/06nlreg.html#small-example",
    "title": "NLS, IRLS, and MLE",
    "section": "Small Example",
    "text": "Small Example\nConsider the function \\(y = x^4 -18 x^2 + 15x\\) , find the Minimum.\nS1. Gradient: \\(\\frac{\\partial y}{\\partial x}=4x^3-36*x+15\\)\nS2. Hessian: \\(\\frac{\\partial y ^2}{\\partial^2 x}=12*x^2-36\\)\nSolution:\n\\[x_t = x_{t-1} - \\frac{dy/dx}{dy^2/d^2x}\\]\nmata:\n     // function to obtain the Value, the gradient and Hessian \n     real matrix  fgh_x(real matrix x, real scalar g){\n        real matrix y\n        if (g==0)      y =    -x:^4 :- 18*x:^2 :+ 15*x\n        else if (g==1) y =  -4*x:^3 :- 36*x    :+ 15\n        else if (g==2) y = -12*x:^2 :- 36   \n        return(y)\n     }\n    // Some initial values\n     x = -2 , 2 ,0 \n     xt = x,fgh_x(x,0),fgh_x(x,1)\n    for(i=1;i&lt;8;i++) {\n         x = x :- fgh_x(x,1):/fgh_x(x,2)\n         xt = xt \\ x,fgh_x(x,0),fgh_x(x,1)\n    }\n    xt[,(1,4,7)] \n    xt[,(2,5,8)] \n    xt[,(3,6,9)] \n    end\n\n        +----------------------------------------------+\n      1 |            -2            -86             55  |\n      2 |  -6.583333333    999.5046779   -889.2939815  |\n      3 |  -4.746265374    30.78669521   -241.8115913  |\n      4 |  -3.714313214   -113.7119057   -56.25720696  |\n      5 |  -3.280073929   -127.1074326   -8.077091068  |\n      6 |  -3.193322943   -127.4662895   -.2936080906  |\n      7 |  -3.189923432   -127.4667891   -.0004426933  |\n      8 |  -3.189918291   -127.4667891   -1.01177e-09  |\n        +----------------------------------------------+\n                      1              2              3\n        +----------------------------------------------+\n      1 |             2            -26            -25  |\n      2 |   4.083333333    39.13430748    140.3356481  |\n      3 |    3.22806275   -30.56155349    33.34042084  |\n      4 |   2.853639205   -37.46140286    5.220655055  |\n      5 |   2.769051829    -37.6890608    .2425933896  |\n      6 |   2.764720715   -37.68958705    .0006229957  |\n      7 |   2.764709535   -37.68958705    4.14680e-09  |\n      8 |   2.764709535   -37.68958705    1.42109e-14  |\n        +----------------------------------------------+\n                     1             2             3\n        +-------------------------------------------+\n      1 |            0             0            15  |\n      2 |  .4166666667   3.155140818   .2893518519  |\n      3 |  .4251979252   3.156376126   .0003663956  |\n      4 |  .4252087555   3.156376128   5.98494e-10  |\n      5 |  .4252087556   3.156376128   1.77636e-15  |\n      6 |  .4252087556   3.156376128             0  |\n      7 |  .4252087556   3.156376128             0  |\n      8 |  .4252087556   3.156376128             0  |\n        +-------------------------------------------+"
  },
  {
    "objectID": "adv_class/06nlreg.html#why-so-many-solutions",
    "href": "adv_class/06nlreg.html#why-so-many-solutions",
    "title": "NLS, IRLS, and MLE",
    "section": "Why So many Solutions?",
    "text": "Why So many Solutions?\n\nStata"
  },
  {
    "objectID": "adv_class/06nlreg.html#nls",
    "href": "adv_class/06nlreg.html#nls",
    "title": "NLS, IRLS, and MLE",
    "section": "NLS",
    "text": "NLS\nThe same principle (as above) can be used for Regression:\n\\[\ny = b_0 + b_1 x ^{b_2} + e = h(x,b) + e\n\\]\nPseudo Regressors and gradients\n\\[\n\\begin{aligned}\n\\tilde X(b) = \\frac{\\partial h(x,b)}{\\partial b_0},\n\\frac{\\partial h(x,b)}{\\partial b_1},\n\\frac{\\partial h(x,b)}{\\partial b_2} \\\\\n\\tilde X(b) = 1, x^{b_2},{b_1} x^{b_2}\\ log\\ b_2 \\\\\n\\beta_t =\\beta_{t-1}-(\\tilde X ' \\tilde X) ^{-1} \\tilde X' \\hat e \\\\\n\\hat e=y-h(x,b_{t-1})\n\\end{aligned}\n\\]\nIt turns out that:\n\\[\n\\begin{aligned}\nb^* \\sim N(b, (\\tilde X' \\tilde X)^{-1}\\tilde X' \\Omega \\tilde X (\\tilde X' \\tilde X)^{-1} )\\\\\n\\Omega = f(\\hat e)\n\\end{aligned}\n\\]\nSo…you can 🥪 just like with regular OLS!"
  },
  {
    "objectID": "adv_class/06nlreg.html#nls-in-stata-long-example",
    "href": "adv_class/06nlreg.html#nls-in-stata-long-example",
    "title": "NLS, IRLS, and MLE",
    "section": "NLS in Stata: Long Example",
    "text": "NLS in Stata: Long Example\nclear\nset seed 101\nset obs 100\n** Generating fake data\ngen x = runiform()\ngen y = 1+0.5*x^0.5+rnormal()*.1\n*** Load data in Mata...to make things quick\nmata\nx=st_data(.,\"x\")\ny=st_data(.,\"y\")\nend\n\nmata\n// Initial data\nb=1\\1\\1\nb0=999\\999\\999\nbb=b'\n// and a loop to see when data converges\nwhile (sum(abs(b0:-b))&gt; 0.000001 ) { \n    b0=b    \n    // residuals\n    e=y:-(b[1]:+b[2]*x:^b[3])\n    // pseudo regressors\n    sx=J(100,1,1),x:^b[3],b[2]*x:^b[3]:*ln(x)\n    // gradient and Hessian\n    g=-cross(sx,e)\n    H=cross(sx,sx)\n    // updating B\n    b=b-invsym(H)*g\n    // Storing results\n    bb=bb\\b'\n}\n/// Now STD ERR (for fun 😃 )\nvcv = e'e / (100-3) * invsym(H)\nb , diagonal(vcv):^0.5\n\nend\n: b , diagonal(vcv):^0.5\n                 1             2\n    +-----------------------------+\n  1 |   1.06407084   .0527603468  |\n  2 |  .4228891942   .0477125017  |\n  3 |  .5788053878    .159931095  |\n    +-----------------------------+"
  },
  {
    "objectID": "adv_class/06nlreg.html#nls-in-stata-short-example-nl-function",
    "href": "adv_class/06nlreg.html#nls-in-stata-short-example-nl-function",
    "title": "NLS, IRLS, and MLE",
    "section": "NLS in Stata: Short Example: NL function",
    "text": "NLS in Stata: Short Example: NL function\n** Stata has the function -nl- (nonlinear)\n** it can be used to estimate this types of models\n** see help nl\n** Be careful about Initial values\nnl ( y = {b0=1} + {b1=1} * x ^ {b2=1})\n\nIteration 0:  residual SS =   .854919\nIteration 1:  residual SS =  .7742535\nIteration 2:  residual SS =   .766106\nIteration 3:  residual SS =  .7660948\nIteration 4:  residual SS =  .7660947\nIteration 5:  residual SS =  .7660947\nIteration 6:  residual SS =  .7660947\n\n\n      Source |      SS            df       MS\n-------------+----------------------------------    Number of obs =        100\n       Model |  1.2706842          2  .635342093    R-squared     =     0.6239\n    Residual |  .76609469         97  .007897883    Adj R-squared =     0.6161\n-------------+----------------------------------    Root MSE      =     .08887\n       Total |  2.0367789         99  .020573524    Res. dev.     =  -203.3743\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         /b0 |   1.064071   .0527608    20.17   0.000     .9593554    1.168786\n         /b1 |   .4228891   .0477129     8.86   0.000     .3281923     .517586\n         /b2 |   .5788057   .1599306     3.62   0.000     .2613878    .8962236\n------------------------------------------------------------------------------\nNote: Parameter b0 is used as a constant term during estimation.\nSo, you could now estimate many nonlinear models! (logits, probits, poissons, etc) or can you?"
  },
  {
    "objectID": "adv_class/06nlreg.html#nls-for-logit",
    "href": "adv_class/06nlreg.html#nls-for-logit",
    "title": "NLS, IRLS, and MLE",
    "section": "NLS for logit",
    "text": "NLS for logit\nThe model (as NLS)\n\\[\nP(y=1|x) = \\frac{exp(x\\beta)}{1+exp(x\\beta)}+e\n\\]\nThis guaranties the predicted value is between 0 and 1. But, still assumes errors are homoskedastic!\nfrause oaxaca, clear\nnl (lfp = logistic({b0}+{b1:female age educ}))\n\n      Source |      SS            df       MS\n-------------+----------------------------------    Number of obs =      1,647\n       Model |  1272.8283          4  318.207079    R-squared     =     0.8876\n    Residual |  161.17168       1643  .098095973    Adj R-squared =     0.8873\n-------------+----------------------------------    Root MSE      =   .3132028\n       Total |       1434       1647  .870673953    Res. dev.     =   845.9593\n\n------------------------------------------------------------------------------\n         lfp | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         /b0 |   3.432866   .9601508     3.58   0.000     1.549617    5.316114\n  /b1_female |  -3.056149   .8625563    -3.54   0.000    -4.747975   -1.364324\n     /b1_age |  -.0205121   .0054815    -3.74   0.000    -.0312635   -.0097607\n    /b1_educ |   .1522987   .0329513     4.62   0.000     .0876679    .2169296\n------------------------------------------------------------------------------\nlogit lfp female age educ\n\n\nLogistic regression                                     Number of obs =  1,647\n                                                        LR chi2(3)    = 251.69\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -508.42172                             Pseudo R2     = 0.1984\n\n------------------------------------------------------------------------------\n         lfp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |   -3.21864    .365329    -8.81   0.000    -3.934672   -2.502609\n         age |  -.0233149   .0072746    -3.20   0.001    -.0375729   -.0090569\n        educ |   .1719904   .0411498     4.18   0.000     .0913383    .2526425\n       _cons |   3.507185   .6550208     5.35   0.000     2.223367    4.791002\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/06nlreg.html#glm-and-interative-reweighted-ls-irls",
    "href": "adv_class/06nlreg.html#glm-and-interative-reweighted-ls-irls",
    "title": "NLS, IRLS, and MLE",
    "section": "GLM and Interative Reweighted LS (IRLS)",
    "text": "GLM and Interative Reweighted LS (IRLS)\nGeneralized Linear Model are a natural extension to LR models. It changes how LR models are estimated.\n\nPuts more emphasis in modeling the CEF (conditional mean) of the distribution\nAllows for different transformations that relate the index \\(xb\\) to \\(E(y|x)\\) (links)\nConsiders data can come from different distributions. ( family )\n\n\\[\nE(y|X) = \\eta ^{-1}(x\\beta) ; Var(E(y|X)) = \\sigma^2(x)\n\\]\nFor example:\nLogit model: Family \\(\\rightarrow\\) binomial, Link function logistic function is logistic \\(p(y|x) = \\frac {e^{x\\beta} }{1+e^{x\\beta}} \\rightarrow x\\beta=log \\left( \\frac{p}{1-p} \\right)\\)\nFamily: Binomial, so variance is given by \\(Var(y|X) = p(y|x)(1-p(y|x))\\)"
  },
  {
    "objectID": "adv_class/06nlreg.html#how-does-this-change-estimation-nls",
    "href": "adv_class/06nlreg.html#how-does-this-change-estimation-nls",
    "title": "NLS, IRLS, and MLE",
    "section": "How does this change Estimation? NLS??",
    "text": "How does this change Estimation? NLS??\n\nFusion"
  },
  {
    "objectID": "adv_class/06nlreg.html#how-does-this-change-estimation-nls-1",
    "href": "adv_class/06nlreg.html#how-does-this-change-estimation-nls-1",
    "title": "NLS, IRLS, and MLE",
    "section": "How does this change Estimation? NLS??",
    "text": "How does this change Estimation? NLS??\nRecall GLS?\n\nHeteroskedasticity was addressed by either using “weights” to estimate the model, or by transforming the data!\nHere, when we choose a family, we are also choosing a particular source of heteroskedasticy. (Logit, probit, poisson, have very specific heteroskedastic errors)\nThus, you just need to apply NLS to transformed data!\n\n\\[\n\\begin{aligned}\nSSR(\\beta) = \\sum \\left( \\frac{y-h(x,\\beta)}{\\sigma(x,\\beta)} \\right)^2 \\\\\\\n\\tilde X = \\frac{1}{\\sigma(x,\\beta)} \\frac{\\partial h(x,\\beta)}{\\partial \\beta} ; \\tilde y = \\frac{y}{\\sigma(x,\\beta)} ; \\tilde e=\\frac{1}{\\sigma(x,\\beta)}(y-h(x,\\beta))\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/06nlreg.html#how-does-this-change-estimation-nls-2",
    "href": "adv_class/06nlreg.html#how-does-this-change-estimation-nls-2",
    "title": "NLS, IRLS, and MLE",
    "section": "How does this change Estimation? NLS??",
    "text": "How does this change Estimation? NLS??\nThen, just apply the iterative process we saw before, until it converges!\n\\[\n\\beta_t =\\beta_{t-1}-(\\tilde X'\\tilde X)^{-1} (\\tilde X' \\tilde e)\n\\]\nThen simply derive Standard errors from here.\nfrause oaxaca, clear\nnl (lfp = logistic({b0}+{b1:female age educ}))   \nlogit lfp female age educ\n\n** IRSL\ngen one =1\nmata\ny = st_data(.,\"lfp\")\nx = st_data(.,\"female age educ one\")\nb = 0\\0\\0\\0\n end\n\n\nmata:\nb0=999\\999\\999\\999\nbb=b'\nwhile (sum(abs(b0:-b))&gt; 0.000001 )  {\n    b0=b    \n    yh = logistic(x*b)\n    err = y:-yh\n    se = sqrt(yh:*(1:-yh))\n    wsx =  yh:*(1:-yh):*x:/se\n    werr=  err:/se\n    g = -cross(wsx,werr)\n    h = cross(wsx,wsx)\n    b = b:-invsym(h)*g;b'\n    }\nb,diagonal(cross(werr,werr)/1643*invsym(h)):^.5\nend\n    +-------------------------------+\n  1 |  -3.218640936    .3458501096  |\n  2 |  -.0233149268    .0068867539  |\n  3 |   .1719904055    .0389557282  |\n  4 |   3.507185231    .6200958697  |\n  +-------------------------------+"
  },
  {
    "objectID": "adv_class/06nlreg.html#glm-in-stata",
    "href": "adv_class/06nlreg.html#glm-in-stata",
    "title": "NLS, IRLS, and MLE",
    "section": "GLM in Stata",
    "text": "GLM in Stata\nThis one is easy as 🥧\nhelp glm\n* see for advanced options if interested\n* syntax\nglm y x1 x2 x3 , family( family ) link(function) method\nfrause oaxaca\nglm lfp female age educ, family(binomial)  link(probit)\nglm lfp female age educ, family(binomial)  link(identity)\nglm lfp female age educ, family(binomial)  link(logit)\n\n\nGeneralized linear models                         Number of obs   =      1,647\nOptimization     : ML                             Residual df     =      1,643\n                                                  Scale parameter =          1\nDeviance         =   1016.84343                   (1/df) Deviance =   .6188944\nPearson          =  1472.464769                   (1/df) Pearson  =    .896205\n\nVariance function: V(u) = u*(1-u)                 [Bernoulli]\nLink function    : g(u) = ln(u/(1-u))             [Logit]\n\n                                                  AIC             =   .6222486\nLog likelihood   = -508.4217152                   BIC             =  -11152.38\n\n------------------------------------------------------------------------------\n             |                 OIM\n         lfp | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -3.218641    .365329    -8.81   0.000    -3.934672   -2.502609\n         age |  -.0233149   .0072746    -3.20   0.001    -.0375729   -.0090569\n        educ |   .1719904   .0411498     4.18   0.000     .0913383    .2526425\n       _cons |   3.507185   .6550209     5.35   0.000     2.223368    4.791002\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/06nlreg.html#what-is-mle",
    "href": "adv_class/06nlreg.html#what-is-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "What is MLE",
    "text": "What is MLE\nMLE is an estimation method that allows you to find asymptotically efficient estimators of the parameters of interest \\(\\beta's\\).\nHow?\n\nUnder the assumption that you know something about the data conditional distribution, MLE will find the set of parameters that maximizes the probability (likelihood)that data “comes” from the chosen distribution.\n\nok….but HOW???\nLets do this by example"
  },
  {
    "objectID": "adv_class/06nlreg.html#poisson-regression-via-mle",
    "href": "adv_class/06nlreg.html#poisson-regression-via-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "Poisson Regression via MLE",
    "text": "Poisson Regression via MLE\nConsider variable \\(y={1,1,2,2,3,3,3,3,4,5}\\)\nAnd say, we assume it comes from a poisson distribution:\n\\[\np(y|\\lambda) = \\frac{e^{-\\lambda} \\lambda ^y}{y!}\n\\]\nThis function depends on the value of \\(\\lambda\\) .\n\nWhen \\(\\lambda\\) is known, this is the probability \\(y=\\#\\), assuming a poisson distribution.\nWhen \\(\\lambda\\) is unknown, this function (now \\(f(y|\\lambda)\\) gives the likelihood that we observe \\(y=\\#\\) , for any given \\(\\lambda\\)."
  },
  {
    "objectID": "adv_class/06nlreg.html#the-likelihood-of-lambda",
    "href": "adv_class/06nlreg.html#the-likelihood-of-lambda",
    "title": "NLS, IRLS, and MLE",
    "section": "The Likelihood of \\(\\lambda\\)",
    "text": "The Likelihood of \\(\\lambda\\)"
  },
  {
    "objectID": "adv_class/06nlreg.html#section",
    "href": "adv_class/06nlreg.html#section",
    "title": "NLS, IRLS, and MLE",
    "section": "",
    "text": "Previous figure only considers the likelihood function of a single observation. And every curve suggests its own parameter. (care to guess?)\nWhat if we want one that Maximizes the likelihood of ALL observations at once!. For this we need to impose an additional assumption: Independent distribution.\nThis means that JOINT density is defined as:\n\n\\[\nL(\\lambda|y_1,y_2,...,y_{10})=\\prod f(y_i|\\lambda)  \n\\]"
  },
  {
    "objectID": "adv_class/06nlreg.html#the-likelihood-of-lambda-1",
    "href": "adv_class/06nlreg.html#the-likelihood-of-lambda-1",
    "title": "NLS, IRLS, and MLE",
    "section": "The Likelihood of \\(\\lambda\\)",
    "text": "The Likelihood of \\(\\lambda\\)"
  },
  {
    "objectID": "adv_class/06nlreg.html#what-mle-does",
    "href": "adv_class/06nlreg.html#what-mle-does",
    "title": "NLS, IRLS, and MLE",
    "section": "What MLE does",
    "text": "What MLE does\nWhat MLE does, then, is to find the parameter \\(\\lambda\\) that maximizes the Joint probability of observing the dataset \\(Y\\). Simple as that….\nWith one more difference. Because products are Hard, MLE aims to maximize the Log-Likelihood of observing the data:\n\\[\n\\begin{aligned}\nLnL(\\lambda|y_1,y_2,...,y_{10}) &=\\sum ln f(y_i|\\lambda) \\\\\\\n&= \\sum (-\\lambda + y_i ln(\\lambda) - ln(y_i!))\n\\end{aligned}\n\\]\nAnd just two more changes."
  },
  {
    "objectID": "adv_class/06nlreg.html#section-1",
    "href": "adv_class/06nlreg.html#section-1",
    "title": "NLS, IRLS, and MLE",
    "section": "",
    "text": "When you have \\(X's\\), you can further modify this model, to allow for covariates. For example:\n\n\\[\n\\lambda = e^{x\\beta}\n\\]Which guaranties the mean, or more specifically conditional mean \\(\\lambda = E(y|X)\\) , is positive.\n\nWe divide LnL by \\(N\\). (Number of observations)\n\n\\[\n\\begin{aligned}\n\\beta = \\min_\\beta LnL(\\beta|Y,X) \\\\\n\\beta = \\min_\\beta \\frac{1}{N}\\sum -e^{x'\\beta}+y_i  x_i'\\beta - ln(y_i!)\n\\end{aligned}\n\\]\nWhich arrives to the Same solution"
  },
  {
    "objectID": "adv_class/06nlreg.html#how-to-solve-this",
    "href": "adv_class/06nlreg.html#how-to-solve-this",
    "title": "NLS, IRLS, and MLE",
    "section": "How to solve this?",
    "text": "How to solve this?\nWe actually already covered this…We use Iterative methods!\n\\[\n\\begin{aligned}\nLL &=  \\sum ln \\ f(y|x,\\beta) \\\\\nFOC&: \\\\\n\\frac{\\partial LL}{\\partial \\beta}&= \\sum \\frac{\\partial ln \\ f(y|x,\\beta)}{\\partial \\beta} = \\sum g_i=n E[g_i] =0 \\\\\nSOC&: \\\\\n\\frac{\\partial^2 LL}{\\partial \\beta \\partial \\beta'}&=\n\\sum\\frac{\\partial^2 ln f(y|x,\\beta)}{\\partial \\beta \\partial \\beta'} =\\sum H_i = n E(H_i)\n\\end{aligned}\n\\]\nThus, \\(\\beta's\\) can be estimated using the iterative process (or other more efficient process)\n\\[\n\\beta_t = \\beta_{t-1} - E(H)^{-1} E(g_i)\n\\]"
  },
  {
    "objectID": "adv_class/06nlreg.html#why-do-we-like-mle",
    "href": "adv_class/06nlreg.html#why-do-we-like-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "Why do we like MLE?",
    "text": "Why do we like MLE?\nProperties of MLE:\n\nThe estimates are consistent \\(plim \\ \\hat \\theta = \\theta\\)\nIts MLE estimates are asymptically normal \\(\\hat \\theta\\sim N(\\theta,\\sigma^2_\\theta)\\)\nAnd asymptotically efficient (smallest variance)\n\nWait…What about Variances? How do you estimate them!"
  },
  {
    "objectID": "adv_class/06nlreg.html#regularity-conditions-and-mle",
    "href": "adv_class/06nlreg.html#regularity-conditions-and-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "Regularity Conditions and MLE ⚠️",
    "text": "Regularity Conditions and MLE ⚠️\nThe variance of estimated coefficients has three estimators for its variance:\n\nWe can 🥪(Sandwich) the variance:\n\n\\[\nVar\\left(N^{-1/2}(\\hat\\beta - \\beta) \\right) = H^{-1}g'gH^{-1} = A^{-1}BA^{-1}\n\\]\n\nOr you can use of of the following: \\[\n\\begin{aligned}\nVar\\left(N^{-1/2}(\\hat\\beta - \\beta) \\right) = -H^{-1} \\\\\nVar\\left(N^{-1/2}(\\hat\\beta - \\beta) \\right) = \\left(\\frac{1}{N}\\sum g_i g_i'\\right)^{-1}\n\\end{aligned}\n\\]\n\nBut if all is well (Regularity conditions), They are all equivalent. Otherwise Opt1 is similar to HC1, and Option 2a is closer to non-robust Standard Errors"
  },
  {
    "objectID": "adv_class/06nlreg.html#regularity-conditions",
    "href": "adv_class/06nlreg.html#regularity-conditions",
    "title": "NLS, IRLS, and MLE",
    "section": "Regularity Conditions ⚠️",
    "text": "Regularity Conditions ⚠️\n\nThe LogLikelihood function is build on a well behaved distribution function. Which implies FOC:\n\n\\[\n\\begin{aligned}\n\\int f(y|\\theta) dy =1 \\\\\n\\int \\frac{\\partial f(y|\\theta)}{\\partial \\theta}dy=\\int  \\frac{\\partial ln f(y|\\theta)}{\\partial \\theta} f(y|\\theta)=0 \\\\\nE(g_i)=0\n\\end{aligned}\n\\]\n\nOrder of Diff and Integration is interchangeable. We obtain SOC: \\[\n\\int \\left( \\frac{\\partial^2 ln f(y|\\theta)}{\\partial \\theta \\partial \\theta'}f(y|\\theta) +\n\\frac{\\partial ln f(y|\\theta)}{\\partial \\theta} \\frac{\\partial ln f(y|\\theta)}{\\partial \\theta'} \\right)dy = 0 \\\\\nE(H_i) = -E(g_ig_i')\n\\]"
  },
  {
    "objectID": "adv_class/06nlreg.html#lr-as-mle",
    "href": "adv_class/06nlreg.html#lr-as-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "LR as MLE",
    "text": "LR as MLE\nUnder the assumption of normality, LR can be estimated using ML:\n\\[y_i = x_i'\\beta + e_i \\ | \\ e_i\\sim N(0,\\sigma^2) \\rightarrow y|x \\sim N(x'\\beta, \\sigma^2)\n\\]\nHow does the MLE look? \\[\n\\begin{aligned}\nL_i = f(y_i|x_i,\\beta,\\sigma^2) = \\frac{1}{ \\sqrt{2\\pi \\sigma^2 }} e^{-\\frac{1}{2} \\frac{ (y_i-x_i'\\beta)^2}{\\sigma^2} } \\\\\nLL_i = -\\frac{1}{2}\\frac{(y_i-x_i'\\beta)^2}{\\sigma^2} - \\frac{1}{2}ln(2\\pi)-\\frac{1}{2}ln(\\sigma^2)\n\\end{aligned}\n\\]\nTotal LL?\n\\[\nLL = -\\frac{1}{2\\sigma^2} \\sum (y_i-x_i'\\beta)^2-\\frac{N ln(2\\pi)}{2}-\\frac{N}{2} ln(\\sigma^2)\n\\]"
  },
  {
    "objectID": "adv_class/06nlreg.html#lr-as-mle-pii",
    "href": "adv_class/06nlreg.html#lr-as-mle-pii",
    "title": "NLS, IRLS, and MLE",
    "section": "LR as MLE PII",
    "text": "LR as MLE PII\nFOC:\n\\[\n\\begin{aligned}\n\\frac{\\partial LL}{\\partial \\beta} = -\\frac{1}{\\sigma^2}\\sum x'(y_i-x'\\beta)=0 \\rightarrow \\hat\\beta=(X'X)^{-1}X'y \\\\\n\\frac{\\partial LL}{\\partial \\sigma^2} = \\frac{\\sum e^2}{2\\sigma^4}-\\frac{N}{2\\sigma^2}=0 \\rightarrow \\hat \\sigma^2 = \\frac{\\sum e^2}{N}\n\\end{aligned}\n\\]\nSOC:\n\\[\n\\begin{aligned}\n\\frac{\\partial^2 LL}{\\partial \\beta \\partial \\beta'} = - \\frac{X'X}{\\sigma^2} ;\n\\frac{\\partial^2 LL}{\\partial \\beta \\partial \\sigma} = - \\frac{X'y-X'X\\beta }{\\sigma^2} \\\\\n\\frac{\\partial^2 LL}{\\partial \\sigma \\partial \\beta'}=0;\n\\frac{\\partial^2 LL}{\\partial \\sigma ^2}=-\\frac{\\sum e^2}{\\sigma^6}+\\frac{N}{2\\sigma^4}=-\\frac{N}{2\\sigma^4}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/06nlreg.html#soc",
    "href": "adv_class/06nlreg.html#soc",
    "title": "NLS, IRLS, and MLE",
    "section": "SOC",
    "text": "SOC\n\\[\n\\begin{aligned}\nH = \\Bigg[\\begin{matrix} -\\frac{X'X}{\\sigma^2}& 0 \\\\\n0 & -\\frac{N}{2\\sigma^4}\n\\end{matrix} \\Bigg] \\\\\n& \\rightarrow Var(\\beta,\\sigma^2)=-H^{-1}=\n\\Bigg[\\begin{matrix}\n\\sigma^2 (X'X)^{-1} & 0 \\\\\n0 & \\frac{2\\sigma^4}{N}\n\\end{matrix} \\Bigg]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/06nlreg.html#other-considerations",
    "href": "adv_class/06nlreg.html#other-considerations",
    "title": "NLS, IRLS, and MLE",
    "section": "Other considerations",
    "text": "Other considerations\nMLE is consistent if the model is correctly Specified.\nThis means that one correctly specifies the conditional distribution of \\(y\\).\n\nOften, its possible to especify the CEF, but specify the variance correctly, may be difficult\n\nUsually, this would be grounds of inconsistency. However if the distribution function is part of the exponential family (normal, bernulli, poisson, etc), one only needs to correctly specify the CEF correctly!\n\nIn this case, the model is no longer estimated using MLE but QMLE\n\nIn this cases, use Robust!"
  },
  {
    "objectID": "adv_class/06nlreg.html#mle-in-stata",
    "href": "adv_class/06nlreg.html#mle-in-stata",
    "title": "NLS, IRLS, and MLE",
    "section": "MLE in Stata",
    "text": "MLE in Stata\nMany native commands in Stata actually estimate models using MLE on the background.\n\nlogit; probit; poisson; ologit; mlogit, etc\n\nSome multiple equation models as well.\n\nmovestay , craggit , ky_fit\n\nAre just a few user written commands that also rely on MLE.\nSo how do you estimate this models?\nyou do it by hand!"
  },
  {
    "objectID": "adv_class/06nlreg.html#this-is-the-way",
    "href": "adv_class/06nlreg.html#this-is-the-way",
    "title": "NLS, IRLS, and MLE",
    "section": "This is the way",
    "text": "This is the way"
  },
  {
    "objectID": "adv_class/06nlreg.html#programming-mle",
    "href": "adv_class/06nlreg.html#programming-mle",
    "title": "NLS, IRLS, and MLE",
    "section": "Programming MLE",
    "text": "Programming MLE\nStata has one feature that would allow you to estimate almost any model via MLE. the -ml- suit.\nThis has many levels of programming (lf, d0, d1, d2, etc), but we will concentrate on the easiest one : lf (linear function)\n\nThis only requires you to provide the individual level likelihood function!\n\nBut how do we start?"
  },
  {
    "objectID": "adv_class/06nlreg.html#section-2",
    "href": "adv_class/06nlreg.html#section-2",
    "title": "NLS, IRLS, and MLE",
    "section": "",
    "text": "You need three pieces of information:\n\nIdentify distribution or objective function to maximize.\nIdentify the parameters that the distribution depends on, and how will they be affected by characteristics\nIf more than one equation exists, Identify possible connections across variables\nWrap it all in a program\n\nFor details on a few examples see Rios-Avila & Canavire-Bacarreza 2018"
  },
  {
    "objectID": "adv_class/06nlreg.html#programming-mle-pi",
    "href": "adv_class/06nlreg.html#programming-mle-pi",
    "title": "NLS, IRLS, and MLE",
    "section": "Programming MLE PI",
    "text": "Programming MLE PI\nOLS via MLE:\n\n\\(y\\) distributes as normal distribution, which depends on the mean \\(\\mu\\) and variance \\(\\sigma^2\\). We assume that only the mean depends on \\(X\\).\n\n** Define Program\ncapture program drop my_ols\nprogram define my_ols\n  args lnf ///   &lt;- Stores the LL for obs i\n       xb  ///   &lt;- Captures the Linear combination of X's\n       lnsigma // &lt;- captures the Log standard error\n  ** Start creating all aux variables and lnf     \n  qui {\n    tempvar sigma // Temporary variable\n    gen double `sigma' = exp(`lnsigma')\n    tempvar y\n    local y $ML_y1\n    replace `lnf'      = log( normalden(`y',`xb',`sigma' ) )\n  }     \nend\nNow Just Call on the program\n* load some data\nfrause oaxaca, clear\nml model lf   /// Ask to use -ml- to estimate a model with method -lf-\n   my_ols     /// your ML program\n   (xb: lnwage = age educ female  ) /// 1st Eq (only one in this case)\n   (lnsig: = female ) // Empty, (no other Y, but could add X's)\n   // I could haave added weights, or IF conditions\nml check     // checks if the code is correct\nml maximize  // maximizes\nml display   // shows results\n\n* Short version \nml model lf  my_ols /// Model and method\n   (xb: lnwage = age educ female) (lnsig: = female ) /// model Parms\n   , robust maximize // Other SE options, and maximize\n   \n\nml display\n\n                                                        Number of obs =  1,434\n                                                        Wald chi2(3)  = 393.05\nLog pseudolikelihood = -870.41117                       Prob &gt; chi2   = 0.0000\n\n------------------------------------------------------------------------------\n             |               Robust\n      lnwage | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nxb           |\n         age |   .0177272   .0012221    14.51   0.000      .015332    .0201224\n        educ |   .0685501   .0056712    12.09   0.000     .0574347    .0796655\n      female |  -.1487184   .0247333    -6.01   0.000    -.1971948   -.1002421\n       _cons |   1.949072   .0888074    21.95   0.000     1.775012    2.123131\n-------------+----------------------------------------------------------------\nlnsig        |\n      female |   .3440266   .0691673     4.97   0.000     .2084611     .479592\n       _cons |  -.9758137   .0488944   -19.96   0.000    -1.071645   -.8799825\n------------------------------------------------------------------------------\n\n. reg  lnwage age educ female\n\n      Source |       SS           df       MS      Number of obs   =     1,434\n-------------+----------------------------------   F(3, 1430)      =    167.12\n       Model |  104.907056         3  34.9690188   Prob &gt; F        =    0.0000\n    Residual |  299.212747     1,430  .209239683   R-squared       =    0.2596\n-------------+----------------------------------   Adj R-squared   =    0.2580\n       Total |  404.119804     1,433  .282009633   Root MSE        =    .45743\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0161424   .0010978    14.70   0.000      .013989    .0182959\n        educ |   .0719322   .0050365    14.28   0.000     .0620524     .081812\n      female |  -.1453936   .0243888    -5.96   0.000    -.1932352    -.097552\n       _cons |   1.970021   .0725757    27.14   0.000     1.827654    2.112387\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/06nlreg.html#other-considerations-1",
    "href": "adv_class/06nlreg.html#other-considerations-1",
    "title": "NLS, IRLS, and MLE",
    "section": "Other Considerations",
    "text": "Other Considerations\nWith MLE, as with logit probit tobit, etc, you cannot interpret the models directly!\nThen what?\n\nIdentify what is the Statistic of interest (most of the time its the expected value, conditional mean, or predicted mean). But others may be something related to other parameters (we care about \\(\\sigma\\) not \\(ln \\ \\sigma\\)).\nSee margins, and option expression\nIdentify if you are interested in Average effects, or effects at the average.\nSome times, you may need to use your own predictions!\n\nsee example for probit"
  },
  {
    "objectID": "adv_class/06nlreg.html#margins-in-action",
    "href": "adv_class/06nlreg.html#margins-in-action",
    "title": "NLS, IRLS, and MLE",
    "section": "Margins in action",
    "text": "Margins in action\n**Estimate Logit model\nlogit lfp educ female age married divorced\n** use margins to estimate predicted probability\nmargins\n** or use expression\nmargins, expression(exp(xb())/(1+exp(xb())))\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _cons |    .870674   .0071023   122.59   0.000     .8567537    .8845942\n------------------------------------------------------------------------------\n\n** marginal effects\nmargins, dydx(educ) \n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0161004   .0037127     4.34   0.000     .0088236    .0233772\n------------------------------------------------------------------------------\n\n\n** Marginal effect of the marginal effect?\nmargins, expression( logistic(xb())*(1-logistic(xb()))*_b[educ] )\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _cons |   .0161004   .0037127     4.34   0.000     .0088236    .0233772\n------------------------------------------------------------------------------\nmargins, dydx(*) expression( logistic(xb())*(1-logistic(xb()))*_b[educ] )\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n\n-------------+----------------------------------------------------------------\n        educ |  -.0010759   .0004973    -2.16   0.031    -.0020507   -.0001012\n    1.female |   .0246023   .0054734     4.49   0.000     .0138747    .0353299\n         age |   5.41e-06   .0000502     0.11   0.914     -.000093    .0001039\n   1.married |    .021062    .004861     4.33   0.000     .0115346    .0305893\n  1.divorced |   .0037604   .0013899     2.71   0.007     .0010362    .0064846\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/06nlreg.html#going-beyond",
    "href": "adv_class/06nlreg.html#going-beyond",
    "title": "NLS, IRLS, and MLE",
    "section": "Going Beyond?",
    "text": "Going Beyond?\nMLE can also be used in more Advanced models.\n\nMulti equation model that may or may not depend on each other.\nFor example, Oaxaca-Blinder Decomposition, requires using Multiple Equations\nIV - CF, or IV TSLS are also multi equation models but which depend on each other.\nOne could also estimate nonlinear versions of Standard models. For example Nonlinear Tobit model (See Ransom 1987)\nEstimation of latent groups of finite mixture models . This combine models that are otherwise unobserved (See Kapteyn and Ypma 2007)\n\nBasically, if you can figure out \\(f(.)\\)’s and how are they connected, you can estimate any model via MLE (with few exceptions)"
  },
  {
    "objectID": "adv_class/04cqreg.html#introduction",
    "href": "adv_class/04cqreg.html#introduction",
    "title": "Conditional Quantile Regressions",
    "section": "Introduction",
    "text": "Introduction\nQuestion: What are quantiles? and why do we care??\n\nQuantiles are statistics that have the purpose of providing a better characterization of distributions.\n\nThis is possible, because it provides you with more information than standard summary statistics (means and variance)\n\nHow so? In general, there are 3 ways you can use to know “everything” about a distribution.\n\nYou either have access to every single \\(y_i\\)\nOr you know the distribution function \\(f(y)\\) (or probability density function pdf)\nOr you know the cumulative distribution function \\(F(y)=\\int_\\infty^y f(t) dt = P(Y\\leq y)\\)\n\nHowever, there is an additional way. Quantile:\n\n\\[\nQ(\\theta) = F^{-1}(p)\n\\]\n\nWhich in principle, is nothing but the inverse cumulative density function."
  },
  {
    "objectID": "adv_class/04cqreg.html#section",
    "href": "adv_class/04cqreg.html#section",
    "title": "Conditional Quantile Regressions",
    "section": "",
    "text": "\\(Q(\\theta) = F^{-1}(\\theta)\\)"
  },
  {
    "objectID": "adv_class/04cqreg.html#other-advantages-yes",
    "href": "adv_class/04cqreg.html#other-advantages-yes",
    "title": "Conditional Quantile Regressions",
    "section": "Other advantages? Yes!",
    "text": "Other advantages? Yes!\n\nQuantiles are far more stable in the presence of outliers. Because of this, they are particularly useful as measures of central tendency (perhaps superior to the mean) (🤔?)\n\nSimple “test”. In the small town of Troy-NY one of the residents wins the 2B$ lottery. How much has welfare increase for the average resident?\n\nScaled IQR can be used as an alternative measure of dispersion.\n\n\\[\nse2 = \\frac{Q_{75}-Q_{25}}{1.34898}\n\\]\n\nThey are also “function-transformation” resistant: \\(exp(Q_{log(y)} (.10)) = Q_y(.10)\\)\nAnd are also very easy to estimate:\n\nSort data by y \\(\\rightarrow\\) Obtain weighted ranks \\(\\rightarrow\\) choose the lowest value so that \\(\\theta\\) % of the data is less of equal to that number\n\n\n\\[\nF^{-1} (tau) = inf(x: F(x)\\geq t)\n\\]\nThis “just” requires obtaining an approximation for \\(F(\\theta)\\), which can be approximated using nonparametric methods!\n\\[\\hat F(x) = \\frac{1}{N}\\sum (K_F(x,x_i,h)) =\n\\frac{1}{N}\\sum 1(x_i&lt;x)\n\\]\nthen we simply “invert” the function for whichever quantile we are interested in."
  },
  {
    "objectID": "adv_class/04cqreg.html#statistical-inference",
    "href": "adv_class/04cqreg.html#statistical-inference",
    "title": "Conditional Quantile Regressions",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nAs with the mean, sampling quantiles are measured with sampling error. Thus its important to recognize its sampling distribution.\nHowever, because of the nature of how quantiles are defined, their standard errors are not as intuitive to obtain, although they can be derived using the delta Method. We start from:\n\\[Q_y(\\tau) = F_y^{-1}(\\tau) \\rightarrow F_y(Q_y(\\tau)) = \\tau\n\\]\n\\[1 = f_y(Q_y(\\tau)) \\frac{dQ}{d\\tau} \\rightarrow \\frac{dQ}{d\\tau} =  \\frac{1}{f(Q_y(\\tau))}\n\\]\nSo we have:\n\\[\n\\begin{aligned}\n\\hat Q_y(\\tau) - Q_y(\\tau) \\simeq \\frac{1}{f(Q_y(\\tau)}(\\hat \\tau-\\tau) \\\\\nVar(\\hat Q_y(\\tau)) = \\frac{Var(\\hat \\tau - \\tau) }{f^2(Q_y(\\tau))} = \\frac{N^{-1} \\tau(1-\\tau)}{f^2(Q_y(\\tau))}\n\\end{aligned}\n\\]\nLets understand this elements"
  },
  {
    "objectID": "adv_class/04cqreg.html#quantile-se",
    "href": "adv_class/04cqreg.html#quantile-se",
    "title": "Conditional Quantile Regressions",
    "section": "Quantile SE",
    "text": "Quantile SE\n\\[\nVar(\\hat Q_y(\\tau)) = \\frac{Var(\\hat \\tau - \\tau) }{f^2(Q_y(\\tau))} = \\frac{N^{-1} \\tau(1-\\tau)}{f^2(Q_y(\\tau))}\n\\]\n\nthe variance of a quantile depends on the distribution of \\(\\tau\\) which is nothing else that the distribution of a Bernoulli experiment: Is \\(y\\geq Q_y\\) or \\(y&lt;Q_y\\).\n\nThis is the largest near the center of the distribution (50%-50%) but smaller (more precise) near the tails of the distribution (more certainty that something will be larger or smaller.\n\nBut also depends on the density of the distribution.\n\nMore precise estimates when the density is high (center), but less precise near tails of the distribution.\n\nAnd as usual, it depends on the sample size (N) (for more precision one needs more data)\n\nA minor problem. This depends on \\(f()\\). Unless this is known, is another source of variation! (that we usually ignore\n\nOf course, you also have the alternative method. Bootstrap!"
  },
  {
    "objectID": "adv_class/04cqreg.html#example",
    "href": "adv_class/04cqreg.html#example",
    "title": "Conditional Quantile Regressions",
    "section": "Example",
    "text": "Example\nfrause wage2, clear\nbootstrap q10=r(r1) q25=r(r2) q50=r(r3) q75=r(r4) q90=r(r5), reps(1000): _pctile wage  , p(10 25 50 75 90)\n\nBootstrap results                                        Number of obs =   935\n                                                         Replications  = 1,000\n\n      Command: _pctile wage, p(10 25 50 75 90)\n\n------------------------------------------------------------------------------\n             |   Observed   Bootstrap                         Normal-based\n             | coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         q10 |        500   8.720396    57.34   0.000     482.9083    517.0917\n         q25 |        668   14.49296    46.09   0.000     639.5943    696.4057\n         q50 |        905   14.61303    61.93   0.000      876.359     933.641\n         q75 |       1160   20.18665    57.46   0.000     1120.435    1199.565\n         q90 |       1444   33.10919    43.61   0.000     1379.107    1508.893\n------------------------------------------------------------------------------\n\n* Analytical\nsort wage\ngen w1 = _n\ngen w0 = _n-1\nby wage:gen p=0.5*(w1[_N]+w0[1])/935\nkdensity wage, at(wage) gen(fwage)\nreplace se = sqrt(p*(1-p)/935)/fwage\ntabstat wage se if inlist(wage,500,668,905,1160,1444), by(wage)\n\n\n    wage |      wage        se\n---------+--------------------\n     500 |       500  13.27634\n     668 |       668  14.78419\n     905 |       905  14.69217\n    1160 |      1160   19.3574\n    1444 |      1444  29.32711\n---------+--------------------\n   Total |  730.7619  15.88035\n------------------------------"
  },
  {
    "objectID": "adv_class/04cqreg.html#from-q_y-to-q_yx",
    "href": "adv_class/04cqreg.html#from-q_y-to-q_yx",
    "title": "Conditional Quantile Regressions",
    "section": "From \\(Q_Y\\) to \\(Q_{Y|X}\\)",
    "text": "From \\(Q_Y\\) to \\(Q_{Y|X}\\)\nThe previous approaches used to identify a particular quantile are not the only ones.\nJust like we can use OLS to estimate Means (Can you prove it?), we could also use a similar method to estimate the median. We only need to change the loss function \\(L()\\) from an \\(L^2\\) to a \\(|L|\\).\nConsider this:\n\\[\nmedian(Y) = min_\\mu \\frac{1}{N}\\sum |y-\\mu|=\\frac{2}{N}\\sum (y-u)(0.5-I([y-u]&lt;0)\n\\]"
  },
  {
    "objectID": "adv_class/04cqreg.html#section-1",
    "href": "adv_class/04cqreg.html#section-1",
    "title": "Conditional Quantile Regressions",
    "section": "",
    "text": "Q and Loss functions\nWhy does it matter?\n\nThe loss function for Quantiles does not penalize “errors” as much as \\(L^2\\) does. This is why its more robust to outliers (almost not affected by them). (the R2 will also need to be changed)\nHowever, the loss function is no longer differentiable (is discontinuous). So requires other methods to find the solution. Even tho it may not look like that:"
  },
  {
    "objectID": "adv_class/04cqreg.html#section-2",
    "href": "adv_class/04cqreg.html#section-2",
    "title": "Conditional Quantile Regressions",
    "section": "",
    "text": "From B to XB\nKoenker and Bassett (1978) extended this last approximation in two ways:\n\nAllows for Covariates (\\(X's\\)) variation\nAllows to identify other quantiles in the distribution:\n\n\\[ \\beta(\\tau) = \\underset {b}{min} \\ N^{-1} \\sum \\rho_\\tau(y_i-X_i'b) \\\\\n\\rho_\\tau (u) = u (\\tau-I(u&lt;0))\n\\]\n\nThis implicitly states that you want to find a combination of \\(X's\\) such that \\(\\tau\\) proportion of \\(y_i\\) are lower than the \\(X_i'\\beta(\\tau)\\) .\n\nBut because we are using controls, we also need that, conditional on \\(x^c\\), \\(\\tau\\)% is lower than \\(x'^{c}\\beta\\)\n\n\n\\[Q_{Y|X}(\\tau) = \\beta_0(\\tau) + \\beta_1(\\tau)x_1 +...+ \\beta_k(\\tau)x_k\n\\]"
  },
  {
    "objectID": "adv_class/04cqreg.html#interpretation-why-is-it-so-different-from-ols",
    "href": "adv_class/04cqreg.html#interpretation-why-is-it-so-different-from-ols",
    "title": "Conditional Quantile Regressions",
    "section": "Interpretation: Why is it so different from OLS?",
    "text": "Interpretation: Why is it so different from OLS?\n\nIn Rios-Avila and Maroto(2022?) we stress that OLS can be interpreted at different “levels”. So consider the following:\n\n\\[\ny_i = b_0 + b_1 x_1 + b_2 x_2  + e\n\\]\nIf there errors are exogenous, and there is no heteroskedasticty, you can “obtain” marginal effects at many levels:\n\\[\n\\begin{aligned}\nInd &: \\frac{dy_i}{dx_{1i}}=b_1 \\\\\nCond &: E(y_i|X=x)=b_0 + b_1 x_1 + b_2 x_2 \\\\\n&: \\frac{dE(y|x)}{dx_1} =b_1 \\\\\nUcond &: E(y_i) = b_0 + b_1 E(x_1) + b_2 E(x_2) \\\\\n&:  \\frac{dE(y)}{dE(x_1)} =b_1\n\\end{aligned}\n\\]\n\nSo in OLS, assuming a linear model in parameters, Nothing changes. The effect is the same! (although magnitude of the “experiment” changes)"
  },
  {
    "objectID": "adv_class/04cqreg.html#but-cqreg",
    "href": "adv_class/04cqreg.html#but-cqreg",
    "title": "Conditional Quantile Regressions",
    "section": "But CQreg?",
    "text": "But CQreg?\nFor quantile regressions, things are not that simple.\n\nThere is no “individual” level quantile effect, because we do not observe individual ranks \\(\\tau\\) . If we could observe them, and we assume they are fixed, then one can obtain individual level effects.\nBecause \\(\\tau\\) is unobserved, all Qregression coefficients, should be interpreted as effects on Conditional Distributions (thus the name CQREG).\n\nIn other words, effects are just expected changes in some points in the distribution.\n\nYou cannot use it for unconditional effects either (not easily), because\n\n\\[\nE(Q_{Y|X}(\\tau)) \\neq Q_Y(\\tau)\n\\]\nand you cannot “simply” average the CQREG effects to get unconditional.\nBut what does it mean? This means that CQREG interpretation are percentile \\(\\tau\\) and covariate \\(X\\) specific. But that is not all:\n\nFixed rank. If you happen to be on the top of the distribution (and stay there), the quantile effect is given by the \\(\\beta(\\tau)\\)\nChanges in Conditional distributions: What we see are how distribution changes along characteristics\n\nSo this must be kept in mind, whenever one interpret results"
  },
  {
    "objectID": "adv_class/04cqreg.html#example-wages",
    "href": "adv_class/04cqreg.html#example-wages",
    "title": "Conditional Quantile Regressions",
    "section": "Example: Wages…",
    "text": "Example: Wages…\nfrause oaxaca, clear\nqreg  lnwage educ exper tenure female, nolog q(10)\nest sto m1\nqreg  lnwage educ exper tenure female, nolog q(50)\nest sto m2\nqreg  lnwage educ exper tenure female, nolog q(90)\nest sto m3\n* ssc install estout\nesttab m1 m2 m3, nogaps mtitle(q10 q50 q90)\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                      q10             q50             q90   \n------------------------------------------------------------\neduc                0.103***       0.0694***       0.0639***\n                   (6.21)         (16.03)          (7.09)   \nexper              0.0200***      0.00758***      0.00402   \n                   (4.06)          (5.91)          (1.50)   \ntenure           0.000669         0.00657***      0.00774*  \n                   (0.11)          (4.19)          (2.37)   \nfemale             -0.151         -0.0689**       -0.0543   \n                  (-1.87)         (-3.29)         (-1.24)   \n_cons               1.462***        2.474***        2.984***\n                   (6.67)         (43.36)         (25.10)   \n------------------------------------------------------------\nN                    1434            1434            1434   \n------------------------------------------------------------\nt statistics in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n** Also\n** ssc install qregplot\nqregplot educ exper tenure female, cons q(5/95)"
  },
  {
    "objectID": "adv_class/04cqreg.html#example-wages-1",
    "href": "adv_class/04cqreg.html#example-wages-1",
    "title": "Conditional Quantile Regressions",
    "section": "Example: Wages…",
    "text": "Example: Wages…"
  },
  {
    "objectID": "adv_class/04cqreg.html#random-coefficents",
    "href": "adv_class/04cqreg.html#random-coefficents",
    "title": "Conditional Quantile Regressions",
    "section": "Random coefficents",
    "text": "Random coefficents\nOne approach to both understanding, and simulating QREG is by also understanding the intuition behind the data generating process.\n\\[\n\\begin{aligned}\ny = b_0(\\tau)+b_1(\\tau)x_1 + +b_2(\\tau)x_2+...+b_k(\\tau) x_k \\\\\n\\tau \\sim runiform(0,1)\n\\end{aligned}\n\\]\nwhere all coefficients are some function (preferably monotonically increasing or decreasing) of \\(\\tau\\) .\n\nWe want them to be monotonically increasing or decreasing because we want that \\[\nX B(\\tau_1 ) \\geq\nX B(\\tau_2 ) \\  if \\ \\tau_1 &gt; \\tau_2\\]\n\nThis specification suggest that the unobserved component \\(\\tau\\) is a random indicator a kind to luck. If you are lucky and get a high \\(\\tau\\) then you will have better outcomes than anyone of your peers.\nAlso notice that this setup assumes that \\(\\tau\\) is the only random factor, and should be uncorrelated with \\(X\\) (you do not make your luck!)\nCan you create data with these characteristics?"
  },
  {
    "objectID": "adv_class/04cqreg.html#svc-model-with-a-latent-running-variable",
    "href": "adv_class/04cqreg.html#svc-model-with-a-latent-running-variable",
    "title": "Conditional Quantile Regressions",
    "section": "SVC model with a latent running variable",
    "text": "SVC model with a latent running variable\nAnother way you can think of Qreg is to align it to the -semiparametric- method we introduced ealier. SVC model.\nThe difference here is that the running variable is unknown. Given the outcome, and characteristics, however, we can identify something akin to the presence of a “latent” component. (but not really estimating it).\nThere are a few (recent) papers that focus on estimation and identification of these models. The general intuition is that the qreg model is identified by the following moment condition:\n\\[\nE\\Big( 1[x\\beta(\\tau) - y &gt; 0 ] - \\tau \\Big) = 0\n\\]\nbut substitute the indicator function with a smooth function. CDF\n\\[\nE\\Big( F(x\\beta(\\tau) - y) - \\tau \\Big) = 0\n\\]\nBeing differentiable, this problem is relatively easier to solve (given good initial values)"
  },
  {
    "objectID": "adv_class/04cqreg.html#example-1",
    "href": "adv_class/04cqreg.html#example-1",
    "title": "Conditional Quantile Regressions",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "adv_class/04cqreg.html#scale-and-location-model",
    "href": "adv_class/04cqreg.html#scale-and-location-model",
    "title": "Conditional Quantile Regressions",
    "section": "Scale and Location Model",
    "text": "Scale and Location Model\nAnother approach that can be used to understand Quantile regressions (and elaborate the interpretation) is to assume that the coefficients are in fact capturing two components:\n\\[y = Xb + Xg(\\tau)\n\\]\n\nLocation: which indicates what is the average relationship between X and Y. \\({b}\\)\nScale: which indicates how far one could be from the average effect, given a relative to its position \\(g(\\tau)\\)\n\nThe model Still assumes random coefficients\nEstimation of this model is not standard. But can be manually implemented:\n\nEstimate OLS and get residuals\nEstimate QREG using those residuals\n\nRequires additional care for the estimation of SE, or just bootstrap"
  },
  {
    "objectID": "adv_class/04cqreg.html#scale-and-location-2-heteroskedasticity",
    "href": "adv_class/04cqreg.html#scale-and-location-2-heteroskedasticity",
    "title": "Conditional Quantile Regressions",
    "section": "Scale and Location 2: Heteroskedasticity",
    "text": "Scale and Location 2: Heteroskedasticity\nA second approach that is useful to understand and interpret CQreg is to consider a parametric version of the LS model:\n\\[y = Xb + \\gamma (X) * e\n\\]\nWhere we assume \\(\\gamma(X)&gt;0\\) . which directly shows the relationship between a quantile regressions and heteroskedasticity in the error term. (typically model as \\(X\\gamma\\))\nBecause Heteroskedasticity is parametric, it constrains the relationship across all quantile coefficients:\n\\[y = X(b+\\gamma F^{-1}(\\tau)) \\rightarrow b(\\tau)=b+\\gamma(\\tau)\n\\]\nMaking it more efficient, albeit imposing constrains of the relationship.\nIt shows more clearly the nature of the overall inequality increasing or decreasing effect."
  },
  {
    "objectID": "adv_class/04cqreg.html#visual-loc-vs-scale",
    "href": "adv_class/04cqreg.html#visual-loc-vs-scale",
    "title": "Conditional Quantile Regressions",
    "section": "Visual Loc vs Scale",
    "text": "Visual Loc vs Scale"
  },
  {
    "objectID": "adv_class/04cqreg.html#visual-loc-vs-scale-1",
    "href": "adv_class/04cqreg.html#visual-loc-vs-scale-1",
    "title": "Conditional Quantile Regressions",
    "section": "Visual Loc vs Scale",
    "text": "Visual Loc vs Scale"
  },
  {
    "objectID": "adv_class/04cqreg.html#estimation-and-statistical-inference",
    "href": "adv_class/04cqreg.html#estimation-and-statistical-inference",
    "title": "Conditional Quantile Regressions",
    "section": "Estimation and Statistical Inference",
    "text": "Estimation and Statistical Inference\nAs hinted previously, there are many approaches that can be used for the estimation of Conditional Quantile regressions.\n\nStata: qreg, sqreg, bsqreg, qreg2, qrprocess, mmqreg, smqreg, sivqr\n\nEach one with its own assumptions. For Standard errors, however, there are 3 options. Under the assumption of iid error. Non iid error (robust), and assuming clustered standard errors.\n\\[\n\\begin{aligned}\niid: \\Sigma_\\beta &=\\frac{\\tau(1-\\tau)}{f^2_y(F^{-1}(\\tau))}(X'X)^{-1} \\\\\nniid: \\Sigma_\\beta &= \\tau(1-\\tau) (X'f(0|x)X)^{-1} \\ (X'X) \\ (X'f(0|x)X)^{-1} \\\\\nalt: \\Sigma_\\beta &= (IF_\\beta \\ ' IF_\\beta) N^{-2}\n\\end{aligned}\n\\]\nOr simply Bootstrap"
  },
  {
    "objectID": "adv_class/04cqreg.html#problems-and-considerations",
    "href": "adv_class/04cqreg.html#problems-and-considerations",
    "title": "Conditional Quantile Regressions",
    "section": "Problems and Considerations",
    "text": "Problems and Considerations\n\nUnless otherwise specified, quantile regressions are linear in variables (and parameters?)\nWith few exceptions, quantile regressions are quantile specific. Comparisons across quantiles require joint estimation (to construct VCV matrix)\nBecause they are “local” estimators, there is risk of crossing quantiles. (Violation of Monotonicity)\nNon-linear effects will be present if either the location or scale components are nonlinear.\nQuantile regressions are very sensitive to measurement errors in both dependent and independent variables\nThey can be difficult to interpret (see references)"
  },
  {
    "objectID": "adv_class/02OLS.html#introduction",
    "href": "adv_class/02OLS.html#introduction",
    "title": "Linear Regression Model",
    "section": "Introduction",
    "text": "Introduction\n\nLinear Regression (usually estimated via OLS) is the most basic, and still most useful, tool for analyzing data.\nThe goal is to find what the relationship between the outcome \\(y\\) and explanatory variables \\(X's\\) is.\nSay that we start with a very simple “model” that states tries to describe the population function as the following:\n\n\\[\ny = h(X,\\varepsilon)\n\\]\nHere, \\(X\\) represents a set of observed covariates and \\(\\varepsilon\\) the set of unobserved characteristics, and for now, we assume that there is no pre-define relationship between these components.\n\nFor now, we will make standard exogeneity assumptions for the identification of the model"
  },
  {
    "objectID": "adv_class/02OLS.html#estimation",
    "href": "adv_class/02OLS.html#estimation",
    "title": "Linear Regression Model",
    "section": "Estimation",
    "text": "Estimation\n\nThe functional form, however, is unknowable. However, under the small assumption that \\(X\\) and \\(\\varepsilon\\) are unrelated, if we would have access to the population data, we could instead consider the Conditional Expectation function (CEF):\n\n\\[\nE(y_i|X_i=x) = \\int t f_y(t|X_i=x)dx\n\\]\n\nNotice that this implies a fully non-parametric estimation of the Linear function (because it does not impose any functional form).\nWith this, we can “decompose” the outcome \\(y\\) into two components, one that depends on observation characteristics (CEF) and one that depends on the error \\(\\varepsilon\\).\n\n\\[\ny = E(y|X) + \\varepsilon\n\\]\n\nThis has the nice property that the error is unrelated to any functional form of \\(X\\), while providing a summary of the relationship between \\(X\\) and \\(y\\)."
  },
  {
    "objectID": "adv_class/02OLS.html#section",
    "href": "adv_class/02OLS.html#section",
    "title": "Linear Regression Model",
    "section": "",
    "text": "The CEF is a convenient abstract, but to estimate it, we require assumptions. (Recall the assumptions for unbiased OLS?)\nNamely, we need to impose a linearity assumption, namely:\n\n\\[\nE(y_i|X_i=x) = \\beta_0 + \\beta_1 x_1 +\\beta_2 x_2 + ... +\n\\beta_k x_k = X_i'\\beta\n\\]\n\nAnd the solution for \\(\\beta\\) is given by:\n\n\\[\n\\beta = \\underset{b}{arg} \\ E(L(y_i-X'_i b))\n\\]\nWhere the loss function \\(L(x)=x^2\\). (Square loss function)\n\nThis implies the following condition:\n\n\\[\nE[X_i (y_i-X_i'b)]=0 \\rightarrow \\beta = E[X_i'X_i]^{-1}E[X_i'y_i]\n\\]\n\nThis population terms must be substituted by the sample equivalent: \\(E(X_i) =\\frac{1}{N} \\sum_i^NX_i\\)"
  },
  {
    "objectID": "adv_class/02OLS.html#mata-ols-estimator",
    "href": "adv_class/02OLS.html#mata-ols-estimator",
    "title": "Linear Regression Model",
    "section": "Mata: OLS Estimator",
    "text": "Mata: OLS Estimator\nThe estimator using Sample equivalents become:\n\\[\n\\hat \\beta =\n\\left(\\frac{1}{N} \\sum_i X_i'X_i \\right)^{-1}\n\\frac{1}{N} \\sum_i X_i'y_i=(X'X)^{-1}X'y\n\\]\n\n\nCode\nfrause oaxaca, clear\nkeep if lnwage !=.\nmata:\n  y = st_data(.,\"lnwage\")\n  n = rows(y)\n  x = st_data(.,\"female age educ\"),J(n,1,1)\n  exx = cross(x,x)/n\n  exy = cross(x,y)/n\n  b   = invsym(exx)*exy\n  b\nend  \n\n\n\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:   y = st_data(.,\"lnwage\")\n\n:   n = rows(y)\n\n:   x = st_data(.,\"female age educ\"),J(n,1,1)\n\n:   exx = cross(x,x)/n\n\n:   exy = cross(x,y)/n\n\n:   b = invsym(exx)*exy\n\n:   b\n                 1\n    +---------------+\n  1 |  -.145393595  |\n  2 |  .0161424301  |\n  3 |  .0719321873  |\n  4 |  1.970020725  |\n    +---------------+\n\n: end\n-------------------------------------------------------------------------------\n\n."
  },
  {
    "objectID": "adv_class/02OLS.html#inference---distribution-of-betas",
    "href": "adv_class/02OLS.html#inference---distribution-of-betas",
    "title": "Linear Regression Model",
    "section": "Inference - Distribution of \\(\\beta's\\)",
    "text": "Inference - Distribution of \\(\\beta's\\)\nso: \\[\n\\begin{aligned}\ny &= X\\beta + \\varepsilon \\\\\n\\sqrt N (\\hat\\beta - \\beta) &=\\frac{1}{N}\\Big[\\sum (X_iX_i')\\Big]^{-1} \\frac{1}{\\sqrt N} \\sum(X_i\\varepsilon_i)\n\\end{aligned}\n\\]\n\nHere \\(\\varepsilon\\) is the true population error. \\(\\hat\\beta\\) is unbiased if the second term has an expectation of Zero. (the error is independent from \\(X\\)).\nAsymptotically, the first term is assumed fixed \\(E(X_i X_i')\\). And, because \\(E(X_i\\varepsilon)=0\\), and \\(\\frac{1}{\\sqrt N} \\sum(X_i\\varepsilon)\\) is normalized, by CLT we have that:\n\n\\[\n\\sqrt N (\\hat\\beta-\\beta)\\sim N(0,E(X_iX_i')^{-1} \\ E(X_iX_i'\\varepsilon ^2) \\ E(X_iX_i')^{-1})\n\\]\n\nFrom here, the main question is : How do we estimate \\(E(X_iX'\\varepsilon_i^2)\\)?"
  },
  {
    "objectID": "adv_class/02OLS.html#inference-estimating-se",
    "href": "adv_class/02OLS.html#inference-estimating-se",
    "title": "Linear Regression Model",
    "section": "Inference: Estimating SE",
    "text": "Inference: Estimating SE\n\nLets First Rewrite the last expression:\n\n\\[\nVar(\\hat\\beta)=(X'X)^{-1} X'\\Omega X (X'X)^{-1}\n\\]\nwhere:\n\\[\n\\Omega=\n\\left(\n\\begin{matrix}\n\\sigma_1^2 & \\sigma_{12} &  ... & \\sigma_{1N}\\\\\n\\sigma_{21} & \\sigma_{2}^2 & ... & \\sigma_{2N} \\\\\n...&...&...&...\\\\\n\\sigma_{N1} & \\sigma_{N2} & ... & \\sigma_{NN}^2 \\\\\n\\end{matrix}\n\\right)\n\\]\nIn other words, the variance of \\(\\hat\\beta\\) allows for arbitrary relationship among the errors, as well as heteroskedasticity. This, however is impossible to estimate!, thus we require assumptions"
  },
  {
    "objectID": "adv_class/02OLS.html#homoskedasticity-and-independent-samples",
    "href": "adv_class/02OLS.html#homoskedasticity-and-independent-samples",
    "title": "Linear Regression Model",
    "section": "Homoskedasticity and independent samples",
    "text": "Homoskedasticity and independent samples\nThe easiest route is to assume homoskedastic errors \\(\\sigma^2 = \\sigma_i^2 \\ \\forall i \\in 1,...,N\\) . (the error is spread equally around the mean)\nWith independent samples \\(\\sigma_{ij}=0 \\ \\forall \\ i\\neq j\\) . (A persons unobserved is completely independent from anybody else)\n\\[\n\\Omega_00=\n\\left(\n\\begin{matrix}\n\\sigma_1^2 & \\sigma_{12} &  ... & \\sigma_{1N}\\\\\n\\sigma_{21} & \\sigma_{2}^2 & ... & \\sigma_{2N} \\\\\n...&...&...&...\\\\\n\\sigma_{N1} & \\sigma_{N2} & ... & \\sigma_{NN}^2 \\\\\n\\end{matrix}\n\\right)=I(N)*\\sigma^2\n\\]\nThus \\[\n\\begin{aligned}\nVar(\\hat\\beta)_{00} &=(X'X)^{-1} X'I(N)\\sigma^2 X (X'X)^{-1} \\\\\n&=\\sigma^2 (X'X)^{-1} \\\\\n\\sigma^2 &= E(\\varepsilon^2)\n\\end{aligned}\n\\]\n\n\nCode\nmata: e=err = y:-x*b\nmata: var_b_000 = mean(err:^2) * invsym(x'x)\nmata: b,sqrt(diagonal(var_b_000))\n\n\n                 1             2\n    +-----------------------------+\n  1 |  -.145393595   .0243547399  |\n  2 |  .0161424301   .0010962465  |\n  3 |  .0719321873    .005029506  |\n  4 |  1.970020725   .0724744138  |\n    +-----------------------------+"
  },
  {
    "objectID": "adv_class/02OLS.html#section-1",
    "href": "adv_class/02OLS.html#section-1",
    "title": "Linear Regression Model",
    "section": "",
    "text": "But, \\(\\sigma^2\\) is not known, so we have to use \\(\\hat\\sigma^2\\) instead, which depends on the sample residuals: \\[\n\\hat\\sigma^2 = \\frac{1}{N-k-1}\\sum \\hat e^2\n\\]\nWhere we account for the fact true errors are not observed, but rather residuals are estimated, adjusting the degrees of freedom.\n\n\nCode\nmata:\n    N = rows(y); k = cols(x)\n    var_b_00 = sum(err:^2)/(N-k) * invsym(x'x)\n    b,sqrt(diagonal(var_b_00))\nend\n\n\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     N = rows(y); k = cols(x)\n\n:     var_b_00 = sum(err:^2)/(N-k) * invsym(x'x)\n\n:     b,sqrt(diagonal(var_b_00))\n                 1             2\n    +-----------------------------+\n  1 |  -.145393595   .0243887787  |\n  2 |  .0161424301   .0010977786  |\n  3 |  .0719321873   .0050365354  |\n  4 |  1.970020725   .0725757058  |\n    +-----------------------------+\n\n: end\n-------------------------------------------------------------------------------\n\n."
  },
  {
    "objectID": "adv_class/02OLS.html#lifting-assumptions-heteroscedasticity",
    "href": "adv_class/02OLS.html#lifting-assumptions-heteroscedasticity",
    "title": "Linear Regression Model",
    "section": "Lifting Assumptions: Heteroscedasticity",
    "text": "Lifting Assumptions: Heteroscedasticity\n\nWe start by lifting this assumption, which implies the following:\n\n\\[\n\\sigma^2_i \\neq \\sigma^2_j \\  \\forall \\ i\\neq j\n\\]\nBut to estimate this, we need an approximation for \\(\\sigma^2_i = E(\\varepsilon_i^2) = \\varepsilon_i^2\\).\n\nWith this, we can obtain what is known as th White or Eicker-White or Heteroskedasiticy Robust Standard errors.\n\n\\[\n\\begin{aligned}\nVar(\\hat\\beta)_{0} &= (X'X)^{-1} (X\\hat e)'(\\hat eX) (X'X)^{-1} \\\\\n&=(X'X)^{-1} \\sum(X_iX_i'\\hat e^2) (X'X)^{-1}\n\\end{aligned}\n\\]\nWhich imposes NO penalty to the fact that we are using residuals not errors. If we account for that however, we obtain what is known as HC1, SE, the standard in stata. (when you type robust)\n\\[\nVar(\\hat\\beta)_{1}=\\frac{N}{N-K-1}Var(\\hat\\beta)_{0}\n\\]\n\n\nCode\nmata:\n    ixx = invsym(x'x)\n    var_b_0 = ixx * (x:*e)'(x:*e) * ixx\n    var_b_1 = N/(N-k)*var_b_0\n    b,sqrt(diagonal(var_b_0)),sqrt(diagonal(var_b_1))\nend\n\n\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     ixx = invsym(x'x)\n\n:     var_b_0 = ixx * (x:*e)'(x:*e) * ixx\n\n:     var_b_1 = N/(N-k)*var_b_0\n\n:     b,sqrt(diagonal(var_b_0)),sqrt(diagonal(var_b_1))\n                 1             2             3\n    +-------------------------------------------+\n  1 |  -.145393595   .0243162137   .0243501986  |\n  2 |  .0161424301   .0013544849   .0013563779  |\n  3 |  .0719321873    .005690214   .0056981668  |\n  4 |  1.970020725   .0875757052   .0876981032  |\n    +-------------------------------------------+\n\n: end\n-------------------------------------------------------------------------------\n\n."
  },
  {
    "objectID": "adv_class/02OLS.html#but-error-is-not-the-same-as-residual",
    "href": "adv_class/02OLS.html#but-error-is-not-the-same-as-residual",
    "title": "Linear Regression Model",
    "section": "But error is not the same as residual!",
    "text": "But error is not the same as residual!\nA residual is model dependent, and should not be confused with the model error \\(\\hat \\varepsilon \\neq \\varepsilon\\). Because of this, additional corrections are needed to obtained unbiased \\(var(\\hat\\beta)\\) estimates. (Degrees of freedom). But other options exists.\nRedefine the Variance Formula:\n\\[\nVar(\\hat\\beta)=(X'X)^{-1} (\\sum X_iX_i \\psi_i )  (X'X)^{-1}\n\\]\nFrom here Mackinnon and White (1985) suggest few other options: \\[\n\\begin{matrix}\nHC0: \\psi_i = \\hat e^2 &\nHC1: \\psi_i = \\frac{N}{N-K}  \\hat e^2 \\\\\nHC2: \\psi_i =   \\hat e^2 \\frac{1}{1-h_{ii}} &\nHC3: \\psi_i =   \\hat e^2 \\frac{1}{(1-h_{ii})^2}\n\\end{matrix}\n\\]\nWhere \\(h_{ii}\\) is the ith diagonal element of \\(X(X'X)^{-1}X'\\) and allows you to see how dependent a model is to a single observation.\nHC2 and HC3 Standard errors are better than HC1 SE, specially when Samples are small.\n\nNOTE: this \\(h_{ii}\\) element is also used to measure the degrees of freedom of a model. Sum it up, and you will see!."
  },
  {
    "objectID": "adv_class/02OLS.html#coding-robust-se",
    "href": "adv_class/02OLS.html#coding-robust-se",
    "title": "Linear Regression Model",
    "section": "Coding Robust SE",
    "text": "Coding Robust SE\n\n\nCode\nmata:\n    // h = diagonal(X invsym(X'x) X') Wrong Way, too many calculations\n    h = rowsum(x*invsym(x'x):*x)\n    psi0 = e:^2           ;   psi1 = e:^2*N/(N-k)\n    psi2 = e:^2:/(1:-h)   ;   psi3 = e:^2:/((1:-h):^2)\n    var_b_0 = ixx * cross(x,psi0,x) * ixx\n    var_b_1 = ixx * cross(x,psi1,x) * ixx\n    var_b_2 = ixx * cross(x,psi2,x) * ixx\n    var_b_3 = ixx * cross(x,psi3,x) * ixx\n    b,sqrt(diagonal(var_b_0)),sqrt(diagonal(var_b_1)),\n    sqrt(diagonal(var_b_2)),sqrt(diagonal(var_b_3))\nend  \n\n\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     // h = diagonal(X invsym(X'x) X') Wrong Way, too many calculations\n:     h = rowsum(x*invsym(x'x):*x)\n\n:     psi0 = e:^2 ; psi1 = e:^2*N/(N-k)\n\n:     psi2 = e:^2:/(1:-h) ; psi3 = e:^2:/((1:-h):^2)\n\n:     var_b_0 = ixx * cross(x,psi0,x) * ixx\n\n:     var_b_1 = ixx * cross(x,psi1,x) * ixx\n\n:     var_b_2 = ixx * cross(x,psi2,x) * ixx\n\n:     var_b_3 = ixx * cross(x,psi3,x) * ixx\n\n:     b,sqrt(diagonal(var_b_0)),sqrt(diagonal(var_b_1)),\n&gt;     sqrt(diagonal(var_b_2)),sqrt(diagonal(var_b_3))\n                 1             2             3             4             5\n    +-----------------------------------------------------------------------+\n  1 |  -.145393595   .0243162137   .0243501986   .0243568124   .0243975204  |\n  2 |  .0161424301   .0013544849   .0013563779   .0013573922   .0013603079  |\n  3 |  .0719321873    .005690214   .0056981668   .0057079191    .005725691  |\n  4 |  1.970020725   .0875757052   .0876981032   .0878131672   .0880514838  |\n    +-----------------------------------------------------------------------+\n\n: end\n-------------------------------------------------------------------------------\n\n. \n\n\nOr in Stata:\nregress y x1 x2 x3, vce(robust)\nregress y x1 x2 x3, vce(hc2)\nregress y x1 x2 x3, vce(hc3)"
  },
  {
    "objectID": "adv_class/02OLS.html#lifting-even-more-assumptions-correlation",
    "href": "adv_class/02OLS.html#lifting-even-more-assumptions-correlation",
    "title": "Linear Regression Model",
    "section": "Lifting Even more Assumptions: Correlation",
    "text": "Lifting Even more Assumptions: Correlation\n\nOne assumption we barely consider last semester was the possibility that errors could be correlated within groups. (except for time series and serial correlation)\nFor example, families may share similar unobserved factors, So would people interviewed from the same classroom, cohort, city, etc. There could be many dimensions to consider possible correlations!\nIn that situation, we may be missmeasuring the magnitude of the errors (probably downward), because the \\(\\Omega\\) is no longer diagonal: \\(\\sigma_{ij} \\neq 0\\) for some \\(i\\neq j\\).\n\nBut, estimate all parameters in an NxN matrix is unfeasible. We need assumptions!\n\nSay we have \\(G\\) groups \\(g=(1…G)\\) . We can rewrite the expression for \\(\\hat\\beta\\) as follows:\n\n\\[\n\\begin{aligned}\n\\hat\\beta-\\beta &= (X'X)^{-1}\\sum_{g=1}^G X'_g \\varepsilon_g \\\\\\\n&=(X'X)^{-1}\\sum_{g=1}^G s_g\n\\end{aligned}\n\\]\n\nWe can assume that individuals are correlated within groups \\(E(s_g's_g) =\\Sigma_g\\) , but they are uncorrelated across groups \\(E(s_g s_g')=0 \\ \\forall \\ g \\neq g'\\) .\nThese groups are typically known as “clusters”"
  },
  {
    "objectID": "adv_class/02OLS.html#addressing-correlation",
    "href": "adv_class/02OLS.html#addressing-correlation",
    "title": "Linear Regression Model",
    "section": "Addressing Correlation",
    "text": "Addressing Correlation\n\nThe idea of correcting for clusters is pretty simple. We just need to come up with an estimator for \\(\\Sigma_g\\) for every cluster, so that:\n\n\\[\n\\begin{aligned}\nVar(\\hat\\beta) &= (X'X)^{-1} \\left( \\sum_{g=1}^N \\Sigma_g \\right) (X'X)^{-1} \\\\\n\\Sigma_g &= E( X_g' \\Omega_g X_g)\n\\end{aligned}\n\\]\n\nHere \\(\\Omega_g\\) should be an approximation of the variance covariance matrix among the errors of ALL individuals that belong to the same cluster. But how do we approximate it?\nAs with the EW - HC standard errors, there are many ways to estimate Clustered Standard errors. See MacKinnon et al (2023) for reference. We will refer only to the simpler ones CV0 and CV1.\n\n\nStill How?\n\n\nRecall we approximate \\(\\sigma^2_i\\) with \\(\\varepsilon_i^2\\). Then we can approximate \\(\\sigma_{ij}\\) with \\(\\varepsilon_j \\varepsilon_i\\). More specifically:\n\n\\[\n\\Omega_g \\simeq \\varepsilon \\varepsilon' \\ or \\ \\Sigma_g = X'_g \\varepsilon \\varepsilon' X_g = (X'_g \\varepsilon) (\\varepsilon' X_g)\n\\]\n\nChange \\(\\varepsilon\\) with \\(\\hat\\varepsilon\\), do that for every group, and done! (almost)."
  },
  {
    "objectID": "adv_class/02OLS.html#section-2",
    "href": "adv_class/02OLS.html#section-2",
    "title": "Linear Regression Model",
    "section": "",
    "text": "As mentioned earlier, there are many CCSE (clustered consistent SE).\n\n\\[\n\\begin{aligned}\nCV_0 &= (X'X)^{-1} \\sum_{g=1}^G \\hat \\Sigma_g (X'X)^{-1} \\\\\nCV_1 &= \\frac{G(N-1)}{(G-1)(N-k-1)}(X'X)^{-1} \\sum_{g=1}^G \\hat \\Sigma_g (X'X)^{-1}\n\\end{aligned}\n\\]\n\nSimilar to HC. CV0 does not correct for degrees of freedom. CV1, however, accounts for Degrees of freedom in the model, and clusters.\n\n\n\nCode\nmata:\n    // 1st Sort Data (easier in Stata rather than Mata) and reload\n    y   = st_data(.,\"lnwage\")\n    x   = st_data(.,\"educ exper female\"),J(1434,1,1) \n    cvar= st_data(.,\"isco\")\n    ixx = invsym(cross(x,x)); xy = cross(x,y)\n    b   = ixx * xy\n    e   = y:-x*b\n    // Set the panel info\n    info = panelsetup(cvar,1); g=rows(info); n=rows(y)\n    // get X_g'e for all groups: \n    s_xg_e = panelsum(x:*e,info)\n    // Sum Sigma_g\n    sigma_g = s_xg_e's_xg_e\n    cv0 = ixx*sigma_g*ixx\n    cv1 =g/(g-1)*(n-1)/(n-k)*ixx*sigma_g*ixx\n    b,sqrt(diagonal(cv0)),sqrt(diagonal(cv1))\nend    \n\n\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     // 1st Sort Data (easier in Stata rather than Mata) and reload\n:     y = st_data(.,\"lnwage\")\n\n:     x = st_data(.,\"educ exper female\"),J(1434,1,1)\n\n:     cvar= st_data(.,\"isco\")\n\n:     ixx = invsym(cross(x,x)); xy = cross(x,y)\n\n:     b = ixx * xy\n\n:     e = y:-x*b\n\n:     // Set the panel info\n:     info = panelsetup(cvar,1); g=rows(info); n=rows(y)\n\n:     // get X_g'e for all groups:\n:     s_xg_e = panelsum(x:*e,info)\n\n:     // Sum Sigma_g\n:     sigma_g = s_xg_e's_xg_e\n\n:     cv0 = ixx*sigma_g*ixx\n\n:     cv1 =g/(g-1)*(n-1)/(n-k)*ixx*sigma_g*ixx\n\n:     b,sqrt(diagonal(cv0)),sqrt(diagonal(cv1))\n                  1              2              3\n    +----------------------------------------------+\n  1 |   .0858251775    .0140570765    .0149254126  |\n  2 |   .0147342796    .0014534593    .0015432426  |\n  3 |  -.0949227416    .0525121234    .0557559112  |\n  4 |   2.218849962    .1947497649    .2067798804  |\n    +----------------------------------------------+\n\n: end\n-------------------------------------------------------------------------------\n\n. \n\n\nor compare it to\n\n\nCode\nreg lnwage educ exper female, cluster(isco)\n\n\n\nLinear regression                               Number of obs     =      1,434\n                                                F(3, 8)           =      59.13\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.2217\n                                                Root MSE          =     .46897\n\n                                   (Std. err. adjusted for 9 clusters in isco)\n------------------------------------------------------------------------------\n             |               Robust\n      lnwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0858252   .0149254     5.75   0.000     .0514071    .1202432\n       exper |   .0147343   .0015432     9.55   0.000     .0111756     .018293\n      female |  -.0949227   .0557559    -1.70   0.127    -.2234961    .0336506\n       _cons |    2.21885   .2067799    10.73   0.000     1.742015    2.695685\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/02OLS.html#beware-of-over-clustering",
    "href": "adv_class/02OLS.html#beware-of-over-clustering",
    "title": "Linear Regression Model",
    "section": "Beware of over-clustering",
    "text": "Beware of over-clustering\nWhile clustering helps address a problem of “intragroup” correlation, it can/should be done with care. It is important to be aware about some unintended problems of using Correlation.\n\nCV0 and CV1 work well when you have a large number of Clusters. How many? MHE(2009) says…42 (this is like having large enough samples for Asymptotic variance). If # clusters are small, you would do better with other approaches (including CV2 and CV3).\nWhen you cluster your standard errors, you will “most-likely” generate larger standard errors in your model. Standard recommendation (MHE) is to cluster at the level that makes sense (based on data) and produces largest SE (to be conservative)."
  },
  {
    "objectID": "adv_class/02OLS.html#role-of-clusters",
    "href": "adv_class/02OLS.html#role-of-clusters",
    "title": "Linear Regression Model",
    "section": "Role of clusters",
    "text": "Role of clusters\n\nStandard Errors"
  },
  {
    "objectID": "adv_class/02OLS.html#section-3",
    "href": "adv_class/02OLS.html#section-3",
    "title": "Linear Regression Model",
    "section": "",
    "text": "You may also consider that clustering does not work well when sample sizes within cluster are to diverse (micro vs macro clusters)\nAnd there is the case where clustering is required among multiple dimensions (see vcemway). Where the unobserved correlation could be present in different dimensions.\n\nSo what to cluster and how?\n\nMackinnon et al (2023) provides a guide on how and when to cluster your standard errors. (some are quite advanced)\nGeneral practice, At least use Robust SE (HC2 or HC3 if sample is small), but use clustered SE for robustness.\nYou may want to cluster SE based on some theoretical expectations. Choose -broader- groups for conservative analysis.\nIn treatment-causal effect analysis, you may want to cluster at the “treatment” level.\n\n\nBut…Beyond hc0/1 and CV0/1 there is not much out there for correcting Standard errors in nonlinear models."
  },
  {
    "objectID": "adv_class/02OLS.html#if-you-cant-sandwich-you-can-re-sample",
    "href": "adv_class/02OLS.html#if-you-cant-sandwich-you-can-re-sample",
    "title": "Linear Regression Model",
    "section": "If you can’t Sandwich 🥪, you can re-Sample",
    "text": "If you can’t Sandwich 🥪, you can re-Sample\n\nThe discussion above refereed to the estimation of SE using \\(Math\\). In other words, it was based on the asymptotic properties of the data. Which may not work in small samples.\nAn alternative, often used by practitioners, is using re-sampling methods to obtain approximations to the coefficient distributions of interest.\n\nBut… How does it work?🤔\nFirst ask yourself, how does Asymptotic theory work (and econometrics)? 😱\n\nNote: I recommend reading the -simulation- chapter in The effect, and simulation methods chapter in CT."
  },
  {
    "objectID": "adv_class/02OLS.html#a-brief-reviewagain",
    "href": "adv_class/02OLS.html#a-brief-reviewagain",
    "title": "Linear Regression Model",
    "section": "A Brief Review…again 😇",
    "text": "A Brief Review…again 😇\nIf I were to summarize most of the methodologies (ok all) we used last semester, and this one, the properties that have been derived and proofed are based on the assumption that we “could” always get more data (frequentist approach).\nThere is population (or supper population) from where we can get samples of data.\n\nWe get a sample (\\(y,X\\)) (of size N)\nEstimate our model : method(\\(y,X\\))\\(\\rightarrow\\) \\(\\beta's\\)\nRepeat to infinitum\nCollect all \\(\\beta's\\) and summarize. (Mean and Standard deviations)\n\nDone.\nThe distributions you get from the above exercise should be the same as what your estimation method produces. (if not, there there is something wrong with the estimation method)"
  },
  {
    "objectID": "adv_class/02OLS.html#but-we-only-get-1-sample",
    "href": "adv_class/02OLS.html#but-we-only-get-1-sample",
    "title": "Linear Regression Model",
    "section": "But we only get 1 Sample!",
    "text": "But we only get 1 Sample!\nThe truth is we do not have access to multiple samples. Getting more data, is in fact, very expensive. So what to do ?\n\nRely on Asymptotic theory\nlearn Bayesian Econometrics 🥺\nor-resample? and do Bootstrap!\n\nBasic idea of Bootstrapping\n\nIn the ideal scenario, you get multiple samples from your population, Estimate parameters, and done.\nIf not possible you do the next best thing. You get your sample (assume is your mini-population),\n\nDraw subsamples of same size (with replacement) (\\(y_i^s,X_i^s\\))\nestimate your model and obtain parameters \\(\\beta^s_i\\)\nSummarize those parameters…and done, you get \\(Var(\\hat\\beta)\\) for 🆓. (or is it?)"
  },
  {
    "objectID": "adv_class/02OLS.html#bootstrapping",
    "href": "adv_class/02OLS.html#bootstrapping",
    "title": "Linear Regression Model",
    "section": "Bootstrapping",
    "text": "Bootstrapping\n\n👢Bootstrapping is a methodology that allows you to obtain empirical estimations of standard errors making use of the data in hand, and without even knowing about Asymptotic theory (other than how to get means and variances).\n\n\nBootstrap Sample\nAnd of course, it comes in different flavors."
  },
  {
    "objectID": "adv_class/02OLS.html#bootstrap-types",
    "href": "adv_class/02OLS.html#bootstrap-types",
    "title": "Linear Regression Model",
    "section": "Bootstrap Types:",
    "text": "Bootstrap Types:\n\nNon-parametric Bootstrap: You draw subsamples from the main sample. Each observation has the same pr of being selected.\n\nEasiest to implement ( see bootstrap:)\nWorks in almost all cases, but you may have situations when some covariates are rare.\nCan be extended to allow “clusters” using “block bootstrapping”. Works best if re-sampling “follows” the same sampling structure as your sample.\n\nParametric Bootstrap: You estimate your model, make assumptions of your model error.\n\nYou need to implement it on your own. \\(y^s=x\\hat b+\\tilde e\\) for \\(\\tilde e \\sim f(\\hat \\theta)\\)\nIt will not work well if the assumptions of the error modeling are wrong.\n\nResidual bootstrap: Estimate your model, obtain residuals. Re-sample residuals\n\nAgain, implement it on your own. \\(y^s = x\\hat b+\\tilde e\\) for \\(\\tilde e \\sim {\\hat e_1 , ... , \\hat e_N}\\)\nIt depends even more on the assumptions of the error modeling."
  },
  {
    "objectID": "adv_class/02OLS.html#section-4",
    "href": "adv_class/02OLS.html#section-4",
    "title": "Linear Regression Model",
    "section": "",
    "text": "UWild bootstrap: Estimate your model, obtain residuals, and re-sample residual weights.\n\nAgain…on your own: \\(y^s = x\\hat b +\\hat e * v\\) , where \\(v \\sim ff()\\) where \\(ff()\\) is a “good” distribution function. \\(E(v)=0 \\ \\& \\ Var(v)=1\\)\nActually quite flexible, and works well under heteroskedasticity!\nIt can also allow clustered standard errors. The error \\(v\\) no longer changes by individual, but by group. It also works well with weights.\n\nUWild bootstrap-2 : Estimate your model, obtain Influence functions 😱 , and re-sample residual weights.\n\nThis is an extension to the previous option. But with advantages\n\nyou do not need to -reestimate- the model. Just look into how the the mean of IF’s change.\nit can be applied to linear and nonlinear model (if you know how to build the IF’s)\n\nWorks well with clustered and weights.\n\nCWild bootstrap: Similar UWild Bootstrap, Obtain Influence functions under the Null (imposing restrictions), and use that to test the NULL.\n\nNo, you do not need to do it on your own. see bootest in Stata.\nWorks pretty well with small samples and small # clusters. Probably the way to go if you really care about Standard errors."
  },
  {
    "objectID": "adv_class/02OLS.html#how-to-bootstrap-in-stata",
    "href": "adv_class/02OLS.html#how-to-bootstrap-in-stata",
    "title": "Linear Regression Model",
    "section": "How to Bootstrap? in Stata",
    "text": "How to Bootstrap? in Stata\nI have a few notes on Bootstrapping here Bootstrapping in Stata. But let me give you the highlights for the most general case.\n\nMost (if not all commands) in Stata allow you to obtain bootstrap standard errors, by default. see:help [cmd]\nthey usually have the following syntax:\n[cmd] y x1 x2 x3, vce(bootstrap, options)\nregress lnwage educ exper female, vce(bootstrap, reps(100))\nHowever, you can also Bootstrap that commands that do not have their own bootstrap option.\nbootstrap:[cmd] y x1 x2 x3, \nbootstrap, reps(100):regress lnwage educ exper female\nbootstrap, reps(100) cluster(isco):regress lnwage educ exper female"
  },
  {
    "objectID": "adv_class/02OLS.html#section-5",
    "href": "adv_class/02OLS.html#section-5",
    "title": "Linear Regression Model",
    "section": "",
    "text": "This last command may allow you to bootstrap multiple models at the same time, although it does require a bit of programming. (and a do file)\n\n\n\nCode\ngen tchild = kids6 + kids714\ncapture program drop bs_wages_children\nprogram bs_wages_children, eclass // eclass is for things like equations\n    ** Estimate first model\n    reg lnwage educ exper female\n    matrix b1 = e(b)\n    matrix coleq b1 = lnwage\n    ** Estimate second model\n    reg tchild educ exper female\n    matrix b2 = e(b)\n    matrix coleq b2 = tchild\n    ** Put things together and post\n    matrix b = b1 , b2\n    ereturn post b\nend\nbootstrap: bs_wages_children\n\n\n\n(running bs_wages_children on estimation sample)\n\nwarning: bs_wages_children does not set e(sample), so no observations will be\n         excluded from the resampling because of missing values or other\n         reasons. To exclude observations, press Break, save the data, drop\n         any observations that are to be excluded, and rerun bootstrap.\n\nBootstrap replications (50): .........10.........20.........30.........40......\n&gt; ...50 done\n\nBootstrap results                                        Number of obs = 1,434\n                                                         Replications  =    50\n\n------------------------------------------------------------------------------\n             |   Observed   Bootstrap                         Normal-based\n             | coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nlnwage       |\n        educ |   .0858252   .0058256    14.73   0.000     .0744072    .0972431\n       exper |   .0147343   .0011288    13.05   0.000      .012522    .0169466\n      female |  -.0949227   .0283363    -3.35   0.001     -.150461   -.0393845\n       _cons |    2.21885   .0826592    26.84   0.000     2.056841    2.380859\n-------------+----------------------------------------------------------------\ntchild       |\n        educ |   .0177854   .0091641     1.94   0.052     -.000176    .0357468\n       exper |  -.0047747   .0017288    -2.76   0.006     -.008163   -.0013864\n      female |  -.1306332   .0457432    -2.86   0.004    -.2202883   -.0409781\n       _cons |   .4163459   .1156959     3.60   0.000     .1895861    .6431058\n------------------------------------------------------------------------------\n\n\nWhy does it matter? because you may want to test coefficients individually, or across models. This is only possible if the FULL system is estimated jointly"
  },
  {
    "objectID": "adv_class/02OLS.html#final-words-on-bootstrap",
    "href": "adv_class/02OLS.html#final-words-on-bootstrap",
    "title": "Linear Regression Model",
    "section": "Final words on Bootstrap:",
    "text": "Final words on Bootstrap:\nSo bootstrap (and its many flavors) are convenient approaches to estimate standard errors and elaborate statistical Inference, but its not infallible.\n\nIf the re-sampling process does not simulate the true sampling design, we may miss important information when constructing SE.\nWhen the parameters are estimated using “hard” cutoffs or restricted distributions, it may not produce good approximations for SE.\nYou usually require MANY repetitions (standard = 50, but you probably want 999 or more). The more the better, but has some computational costs. (specially simple bs)\nSome methods play better with weighted samples, clusters, and other survey designs than others. And some require more know-how than others.\n\nSo choose your 🔫weapon wisely!"
  },
  {
    "objectID": "adv_class/02OLS.html#variance-of-nonlinear-functions",
    "href": "adv_class/02OLS.html#variance-of-nonlinear-functions",
    "title": "Linear Regression Model",
    "section": "Variance of nonlinear functions",
    "text": "Variance of nonlinear functions\n\n\nSome times (perhaps not with simple OLS) you many need to estimate Standard errors for transformations of your main coefficient of interest, or combinations of those coefficients.\nSay that you estimated \\(\\theta \\sim N(\\mu_\\theta, \\sigma^2_\\theta)\\) but are interested in the distribution of \\(g(\\theta)\\). How do you do this?\nTwo options:\n\nyou re estimate \\(g(\\theta\\)) instead, or\nyou make an approximation, using the Delta Method\n\nHow does it work?"
  },
  {
    "objectID": "adv_class/02OLS.html#section-6",
    "href": "adv_class/02OLS.html#section-6",
    "title": "Linear Regression Model",
    "section": "",
    "text": "The Delta method uses the linear approximations to approximate the distribution of otherwise not known distributions.\nFurther, It relies on the fact that linear transformations a normal distribution, is on itself normal. For example:\n\n\\[\ng(\\hat \\theta) \\simeq g(\\theta) + g'(\\hat\\theta) (\\hat \\theta-\\theta)\n\\]\n\nThis states that the nonlinear function \\(g(\\theta)\\) can be “locally” approximated as a linear function in the neighborhood of \\(g(\\theta)\\).\nPredictions above or below are approximated using the slope of the function. \\(g'(\\theta)\\).\nSo, if we take the variance, we get:\n\n\\[\nVar(g(\\hat \\theta)) \\simeq  Var \\left(g(\\theta)+ g'(\\hat\\theta) (\\hat \\theta-\\theta)\\right)\n=g'(\\hat\\theta)^2 Var(\\theta)\n\\]"
  },
  {
    "objectID": "adv_class/02OLS.html#delta-method-visualization",
    "href": "adv_class/02OLS.html#delta-method-visualization",
    "title": "Linear Regression Model",
    "section": "Delta Method: Visualization",
    "text": "Delta Method: Visualization"
  },
  {
    "objectID": "adv_class/02OLS.html#section-7",
    "href": "adv_class/02OLS.html#section-7",
    "title": "Linear Regression Model",
    "section": "",
    "text": "It can go multivariate as well:\n\\[\n\\begin{aligned}\ng(\\hat \\theta, \\hat \\gamma)-g(\\theta,\\gamma) &\\simeq N(0,\\nabla g ' \\Sigma \\nabla g) \\\\\n\\nabla g ' &=   [\\begin{matrix}\n    dg/d\\theta & dg/d\\gamma\n  \\end{matrix}]\n\\end{aligned}  \n\\]"
  },
  {
    "objectID": "adv_class/02OLS.html#so-why-do-we-care",
    "href": "adv_class/02OLS.html#so-why-do-we-care",
    "title": "Linear Regression Model",
    "section": "So why do we care:",
    "text": "So why do we care:\nTwo reasons:\n\nNonlinear models need this kind of approximations to do statistical inference (probit/logit)\nRecall that when using Robust Standard errors Joint hypothesis Should be done with Care…\n\nConsider a linear set of restrictions imposed by the \\(H_0: R\\beta = r\\).\n\nEstimate the Variance of \\(R\\beta\\)\n\n\\[\nVar(R\\beta)  = \\nabla (R\\beta)' Var(\\beta) R \\nabla (R\\beta)'= R' Var(\\beta) R\n\\]\n\nEstimate the F value for the Linear Hypothesis (Wald Test)\n\n\\[\n(R\\hat \\beta-r)' Var(R\\beta)^{-1} (R\\hat \\beta-r)/Q \\sim F(Q,N-K)\n\\]"
  },
  {
    "objectID": "adv_class/02OLS.html#what-happens-when-k-is-too-big",
    "href": "adv_class/02OLS.html#what-happens-when-k-is-too-big",
    "title": "Linear Regression Model",
    "section": "What happens when K is too big?",
    "text": "What happens when K is too big?\n\n\nHow many variables (max) can you use in a model?\n\n\\[max \\ k = rank(X'X)\\]\n\nWhat happens when you add too many variables in a model?\n\nIncrease Multicolinearity and coefficient variance (too much noise)\nR2 overly large (without explaining much)\nFar more difficult to interpret (too many factors)\nMay introduce endogeneity (when it wasnt a problem before)\n\nHow can you solve the problem?\n\nYou select only a few of the variables, based on theory, and contribution to the model\n\nWhat if you can’t choose?"
  },
  {
    "objectID": "adv_class/02OLS.html#ml-we-let-the-choose-for-you",
    "href": "adv_class/02OLS.html#ml-we-let-the-choose-for-you",
    "title": "Linear Regression Model",
    "section": "ML: We let the 💻Choose for you",
    "text": "ML: We let the 💻Choose for you\n\nBefore we start. The methodology we will discuss are usually meant to get models with “good” predictive power, and some times better interpretability, not so much stat-inference (although its possible)\n\nWhen you do not know how to choose, you could try select a subset of variables from your model such that you maximize the predictive power of the model.\nThis should go beyond IN sample predictive power, but instead maximize Out of sample predictive power.\nThis is typically achieved using the following:\n\\[\nAR^2 = 1-\\frac{SSR}{SST}\\frac{n-1}{n-k-1} \\\\\nAIC = n^{-1}(SSR + 2k\\hat\\sigma^2) \\\\\nBIC = n^{-1}(SSR + ln(n) k\\hat\\sigma^2)\n\\]\nOr using a method known as cross-validation (Comparing predictive power using data not used for model estimation)\nHowever, we can always try to estimate a model with all variables!"
  },
  {
    "objectID": "adv_class/02OLS.html#ridge-and-lasso-and-elasticnet",
    "href": "adv_class/02OLS.html#ridge-and-lasso-and-elasticnet",
    "title": "Linear Regression Model",
    "section": "Ridge and Lasso and ElasticNet",
    "text": "Ridge and Lasso and ElasticNet\n\nRecall that when using OLS to obtain \\(\\beta's\\), we try to minimize the following:\n\n\\[\nSSR = \\sum_i(y_i - X_i \\beta)^2\n\\]\n\nThis has the restrictions of mentioned before (\\(k &lt; N\\)). In addition to letting coefficents vary “too much”\nAn alternative is to use Ridge regression, which instead Minimizes the following:\n\n\\[\nrSS = \\sum_i(y_i - X_i \\beta)^2+ \\lambda \\sum_{k=1}^K\\beta_k^2\n\\]\n\nThis essentially aims to find parameters that reduces SSR, but also “controls” for how large \\(\\beta's\\) can be, using a shrinkage penalty that depends on \\(\\lambda\\).\nIf \\(\\lambda = 0\\) you get Standard OLS, and if \\(\\lambda \\rightarrow \\infty\\) , you get a situation where all betas (but the constant) are zero. For intermediate values, you may have better models than OLS, because you can balance Bias (when \\(\\beta's\\) are zero) with increase variance (when all \\(\\beta's\\) vary as they “please”)"
  },
  {
    "objectID": "adv_class/02OLS.html#section-8",
    "href": "adv_class/02OLS.html#section-8",
    "title": "Linear Regression Model",
    "section": "",
    "text": "We usually start with Ridge, because is relatively Easy to implement, since it has a close form Solution:\n\n\\[\n\\beta = (X'X + \\lambda I)^{-1}{X'y}\n\\]\n\n\nCode\nfrause oaxaca, clear\nkeep if lnwage!=.\ngen male = 1-female\nmata:\n    y = st_data(.,\"lnwage\")\n    x = st_data(.,\"educ exper female male\"),J(1434,1,1)\n    i0 = I(5);i0[5,5]=0\n    xx = (cross(x,x)) ; xy = (cross(x,y))\n    bb0 = invsym(xx)*xy \n    bb1 = invsym(xx:+i0*1)*xy \n    bb10 = invsym(xx:+i0*10)*xy \n    bb100 = invsym(xx:+i0*100)*xy \n    bb1000 = invsym(xx:+i0*1000)*xy \n    bb0,bb1,bb10,bb100,bb1000\nend \n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     y = st_data(.,\"lnwage\")\n\n:     x = st_data(.,\"educ exper female male\"),J(1434,1,1)\n\n:     i0 = I(5);i0[5,5]=0\n\n:     xx = (cross(x,x)) ; xy = (cross(x,y))\n\n:     bb0 = invsym(xx)*xy\n\n:     bb1 = invsym(xx:+i0*1)*xy\n\n:     bb10 = invsym(xx:+i0*10)*xy\n\n:     bb100 = invsym(xx:+i0*100)*xy\n\n:     bb1000 = invsym(xx:+i0*1000)*xy\n\n:     bb0,bb1,bb10,bb100,bb1000\n                  1              2              3              4\n    +-------------------------------------------------------------\n  1 |   .0858251775    .0858183338    .0857563567    .0851046501\n  2 |   .0147342796    .0147345813    .0147372042    .0147554544\n  3 |  -.0949227416    -.047396817   -.0468240416    -.041806663\n  4 |             0     .047396817    .0468240416     .041806663\n  5 |   2.218849962    2.171466638    2.172174327    2.179690914\n    +-------------------------------------------------------------\n                  5\n     ----------------+\n  1     .0778292498  |\n  2     .0146298058  |\n  3    -.0208062854  |\n  4     .0208062854  |\n  5     2.266275433  |\n     ----------------+\n\n: end\n-------------------------------------------------------------------------------\n\n."
  },
  {
    "objectID": "adv_class/02OLS.html#lasso-and-elastic-net",
    "href": "adv_class/02OLS.html#lasso-and-elastic-net",
    "title": "Linear Regression Model",
    "section": "Lasso and Elastic Net",
    "text": "Lasso and Elastic Net\n\nRidge is a relatively easy model to understand and estimate, since it has a close form solution. It has the slight disadvantage that you still estimate a coefficient for “every” variable (tho some are very small)\nAnother approach, that overcomes this advantage is known as Lasso.\n\n\\[\nLSS = \\sum_i(y_i - X_i \\beta)^2+ \\lambda \\sum_{k=1}^K |\\beta_k|\n\\]\n\nand the one known as Elastic net\n\n\\[\neSS = \\sum_i(y_i - X_i \\beta)^2+ \\lambda_L \\sum_{k=1}^K |\\beta_k| +\n\\lambda_r \\sum_{k=1}^K \\beta_k^2\n\\]\n\nLasso has the advantage of forcing some coefficients exactly to zero, when \\(\\lambda\\) is sufficiently large.\nElastic net tries to use the benefits from both approaches."
  },
  {
    "objectID": "adv_class/02OLS.html#lasso-vs-ridge",
    "href": "adv_class/02OLS.html#lasso-vs-ridge",
    "title": "Linear Regression Model",
    "section": "Lasso vs Ridge",
    "text": "Lasso vs Ridge"
  },
  {
    "objectID": "adv_class/02OLS.html#considerations",
    "href": "adv_class/02OLS.html#considerations",
    "title": "Linear Regression Model",
    "section": "Considerations:",
    "text": "Considerations:\nAs with many methodologies, the benefits from this approaches is not free.\n\nYou need to choose tuning parameters “wisely” using approaches such as AIC, BIC, or cross validation.\nThe model you get may improve prediction, but inference is not as straight forward.\nIt also requires working with Standardized coefficients. (so the same penalty can be used for all variables in the model.\n\nNevertheless, they can be used as starting point for model selection.\nif interested, look into Stata introduction to Lasso regression. help Lasso intro"
  },
  {
    "objectID": "adv_class/02OLS.html#brief-example",
    "href": "adv_class/02OLS.html#brief-example",
    "title": "Linear Regression Model",
    "section": "Brief Example:",
    "text": "Brief Example:\n\n\nCode\nfrause oaxaca, clear\nkeep if lnwage!=.\nqui:reg lnwage i.age\npredict p_ols\nqui:elasticnet linear lnwage i.age, selection(cv, alllambdas)  alpha(0)\npredict p_ridge\nqui:lasso linear lnwage i.age, selection(cv, alllambdas)  \npredict p_lasso\nqui:elasticnet linear lnwage i.age, selection(cv, alllambdas)   \npredict p_elastic\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n(option xb assumed; fitted values)\n(options xb penalized assumed; linear prediction with penalized coefficients)\n(options xb penalized assumed; linear prediction with penalized coefficients)\n(options xb penalized assumed; linear prediction with penalized coefficients)"
  },
  {
    "objectID": "adv_class/02OLS.html#shrinking-coefficients",
    "href": "adv_class/02OLS.html#shrinking-coefficients",
    "title": "Linear Regression Model",
    "section": "Shrinking Coefficients",
    "text": "Shrinking Coefficients\n\nLasso vs Ridge"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site was created with the only purpose of sharing the class slides with all Students.\nIt will also (at some point) contain the homeworks and class assigments."
  },
  {
    "objectID": "adv_class/01Introduction.html#introduction",
    "href": "adv_class/01Introduction.html#introduction",
    "title": "Introduction",
    "section": "Introduction",
    "text": "Introduction\nFirst of all, thank you for joining me this semester, we I expect we all will earn something new.\nWhy? well, while I have learned and implemented many of the methodologies we will see here today, there are a few I have yet to dive into.\nNevertheless, I hope you will enjoy, and learn as much as you can from this course, which has the purpose of:\n\nExpose you to a large set of empirical econometric analysis techniques\nExpose you to the application of some of this techniques to the analysis of Causal effects\n\nBut first, lets lay out the Rules of the game…"
  },
  {
    "objectID": "adv_class/01Introduction.html#grades",
    "href": "adv_class/01Introduction.html#grades",
    "title": "Introduction",
    "section": "Grades",
    "text": "Grades\nYour grade will consist on 4 elements\n\nParticipation 10%: As before, active participation is encourage, so reading the material before class is highly recommended.\nPresentations 15%: Students will have two presentations during the semester (second half), based on suggested material (Syllabus) or other papers the students may be interested in.\nThe main requirement. The paper should implement any of the methodologies we will be covering in class.\nThe presentation should emphasize the Research question, assumptions used, methodology, and results. If possible also provide criticism to the paper.\nHomework 25%: Homeworks will be provided for you to practice and implement the different methodologies discussed in class. They can be carried out individually or in groups (of 2). This will include making a brief description of the results."
  },
  {
    "objectID": "adv_class/01Introduction.html#grades-1",
    "href": "adv_class/01Introduction.html#grades-1",
    "title": "Introduction",
    "section": "Grades",
    "text": "Grades\n\nPaper Project 50:\n\n\nWrite a term paper that can be of two types:\n\n\nPaper Replication: You can choose to write a replication paper on a methodological paper, or applied empirical paper.\nIn either case, the replication will have to extend the analysis of the original paper to a different setup (empirical paper), different software (if replication paper), or other extensions to the original analysis/methodology.\nResearch: A 20-25 pages paper where students answer a research question of their choice, using any of the methodologies presented in class. Standard structure of the paper applies.\n\n\nPresentation of the paper in class"
  },
  {
    "objectID": "adv_class/01Introduction.html#course-content",
    "href": "adv_class/01Introduction.html#course-content",
    "title": "Introduction",
    "section": "Course Content",
    "text": "Course Content\nThe course content will consist of two parts:\n\nPart I: I will review and cover many of empirical methodologies that expand on the Linear Regression analysis we cover in Econometrics 1. This include:\n\nLinear Regression: OLS (again), but with SE emphasis, and allowing for (too)many variables.\nSemi- and non-parametric regressions: When you need things to be Flexible (but not too flexible)\nCQuantile regressions: When you are interested in people beyond the mean (distributions)\nUQuantile-Regressions, RIF-Regressions: When you are interested in the whole distribution\nNonlinear Models: MLE and GMM: When your models are nonlinear (in coefficients)"
  },
  {
    "objectID": "adv_class/01Introduction.html#course-content-1",
    "href": "adv_class/01Introduction.html#course-content-1",
    "title": "Introduction",
    "section": "Course Content",
    "text": "Course Content\n\n\n\n\n\\[MLE: Y|x \\sim f(\\theta)\\] \\[GMM: E(y-m(x))= 0\\]"
  },
  {
    "objectID": "adv_class/01Introduction.html#course-content-2",
    "href": "adv_class/01Introduction.html#course-content-2",
    "title": "Introduction",
    "section": "Course Content",
    "text": "Course Content\nThe second part of the course aims to introduce Methodologies that have the goal of identifying Causal effects.\nWhat do we mean with that? We specifically focus on cases when:\n\nA change in T(reatment) (\\(0 \\rightarrow 1\\)) has an impact in Y from \\(y(0) \\rightarrow y(1)\\), because we manage use a design that makes everything else (\\(X's \\ \\& \\ \\varepsilon's\\)) constant.\n\n\nParallel WorldsIdeally you want to observe the same unit under two different Status! (Multiverse!)"
  },
  {
    "objectID": "adv_class/01Introduction.html#course-content-3",
    "href": "adv_class/01Introduction.html#course-content-3",
    "title": "Introduction",
    "section": "Course Content",
    "text": "Course Content\nHowever, as we have discussed last semester, achieving this is hard. There are many factors that we may not be able to control. Thus we need to come-up with “cleaver” strategies to achieve something Similar.\n\nRandomization: When Treatment is generated at “random”. You can’t see All worlds, but its the closest.\nPanel Data and Fixed Effects: Some things are fixed, and you may be able to get “rid” of them if you see them often: Panel data, family effect, twins, etc.\nInstrumental Variables: Searching for Exogenous Variation to “simulate” random assignment. Even tho it may only capture “local” effects\nMatching and Re-weighting: If true twins do not exist, find people who are “observational twins”. Same life, same characteristics, different treatment and outcome. (or instead of people, distributions)"
  },
  {
    "objectID": "adv_class/01Introduction.html#course-content-4",
    "href": "adv_class/01Introduction.html#course-content-4",
    "title": "Introduction",
    "section": "Course Content",
    "text": "Course Content\n\nRDD-Regression Discontinuity Design: Use Jumps, and assignment rules. Use the fact that some times treatment is assigned around a threshold.\nDifferences in Differences: Use changes across groups and time. DiD: Accounts for group differences and trends (but be aware of TWFE - when we absorb too much)\nSynthetic Control: Similar to Matching, you can create “synthetic” units, combining the information of multiple units. at the same time. And like DID, you can use that to identify effects."
  },
  {
    "objectID": "adv_class/01Introduction.html#reading",
    "href": "adv_class/01Introduction.html#reading",
    "title": "Introduction",
    "section": "Reading",
    "text": "Reading\n\nI have assigned many readings! But you do not need to read them all (but, you may benefit from it).\nAt the very least, read one paper/chapter of the assigned readings.\nThe main books: Casual Inference:The mixtape, The Effect, Mostly Harmless Econometrics, are all available online.\nOtherwise, I ll provide the corresponding -pdfs- on the class website.\n\n\n\nte"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#introduction",
    "href": "adv_class/03par_spar_npar.html#introduction",
    "title": "Semi- and Non- Parametric regression",
    "section": "Introduction",
    "text": "Introduction\nWhat exactly do we mean with non parametric??\n\nFirst of all, everything we have done in the last class, concerned to the analysis of parametric relationships between \\(y\\) and \\(X's\\) .\nWhy parametric? Because we assume that the relationship between those variables is linear, so we just need to estimate the parameters of that relationship. (\\(\\beta's\\)). Even tho the CEF was on itself non-parametric.\nThis was just a matter of convince. Instead of trying to estimate all possible conditional means (impossible task?) we impose functional form conditions, to identify the relationship of interest.\nWhen we covered MLE (last semester) we even imposed functional forms assumptions on relationships and distribution!"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#so-what-about-non-parametric",
    "href": "adv_class/03par_spar_npar.html#so-what-about-non-parametric",
    "title": "Semi- and Non- Parametric regression",
    "section": "So what about non-parametric?",
    "text": "So what about non-parametric?\n\nNon-parametric is on the other side of the spectrum. There are no “single” parameters to estimate, but rather it tries to be as flexible as possible, to identify all possible relationships in the data.\nIn terms of distributions, it no longer assumes data distributes as normal, poisson, exponential, etc. Instead, it simply assumes it distributes, however it does. 🤔 But isnt that a problem?\nYes it can be.\n\nOn the one hand Parametric modeling is very “strict” regarding functional forms. (linear quadratic, logs, etc).\nOn the other, Non-parametric can be too flexible. Making the problem almost impossible to solve.\n\nHowever, the benefits of letting your data “speak” for itself, would allow you to avoid some problems with parametric models. At least is some balance can be set on the “flexibility”"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#ok-but-what-about-semi-parametric",
    "href": "adv_class/03par_spar_npar.html#ok-but-what-about-semi-parametric",
    "title": "Semi- and Non- Parametric regression",
    "section": "Ok but what about Semi-parametric!",
    "text": "Ok but what about Semi-parametric!\n\nSemi-parametric models try to establish a mid point between parametric and non-parametric models, attempting to draw from the benefit of both.\n\nIt also helps that it has a smaller computational burden (we will see what do i mean with this.\n\nWhat about an example? Say we are trying to explain “wages” as a function of age and education. (assume exogeneity)\nTheoretical framework : \\[wage = g(age, education, \\varepsilon)\n\\]\nParametric model: \\[wage = b_0 + b_1 age + b_2 education +\\varepsilon\n\\]\nNon-parametric model: \\[\nwage = g(age,education) +\\varepsilon\n\\]\nSemi-parametric model:\n\n\\[wage = b_0 + g_1(age) + g_2(education) +\\varepsilon \\\\ wage = g_0(age)+b1 education+\\varepsilon \\\\ wage = g_0(age)+g_1 (age)education+\\varepsilon\n\\]"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#step1-estimation-of-density-functions",
    "href": "adv_class/03par_spar_npar.html#step1-estimation-of-density-functions",
    "title": "Semi- and Non- Parametric regression",
    "section": "Step1: Estimation of Density functions",
    "text": "Step1: Estimation of Density functions\n\nThe first step towards learning non-parmetric analysis, is by learning to use the most basic task of all.\nEstimating distributions (PDFs) : why? in economics, and other social sciences, we care about distributions!\nDistribution of income, how many live under poverty, how much is concentrated among the rich, how skew the distribution is, what is the level of inequality, etc, etc\nThe parametric approach to estimating distribution, is by using some predefined functional form (say normal), and use the data to estimate the parameters that define that distribution:\n\n\\[\n\\hat f(x) = \\frac{1}{\\sqrt{2\\pi\\hat\\sigma^2}}exp \\left(-\\frac{1}{2}\\left(\\frac{x-\\hat \\mu}{\\hat \\sigma}\\right)^2 \\right)\n\\]"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#section",
    "href": "adv_class/03par_spar_npar.html#section",
    "title": "Semi- and Non- Parametric regression",
    "section": "",
    "text": "Which can be done rather easy in Stata\n\nfrause oaxaca, clear\ndrop if lnwage==.\nsum lnwage\ngen f = normalden(lnwage, r(mean), r(sd))\nhistogram wages\n\nBut as you can see, it does not fit well."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#histogram-and-kernel-density",
    "href": "adv_class/03par_spar_npar.html#histogram-and-kernel-density",
    "title": "Semi- and Non- Parametric regression",
    "section": "Histogram and Kernel Density",
    "text": "Histogram and Kernel Density\nHistograms and Kernel densities (you probably have used a lot) are a type of non-parametric estimators, because they impose no functional form restrictions to estimate probability density functions (PDFs).\nConstruction histograms, is in fact, a fairly Straight forward task:\n\nYou select the width of bins, \\(h\\) , and starting value \\(x_0\\)\n\n\\[if \\ x_i \\in [x_0 + m * h, x_0 + (m+1)h ) \\rightarrow\nbin(x)=m+1\n\\]\n\nAnd the Histogram estimator for density, is given by:\n\n\\[\\hat f (x) = \\frac{1}{nh} \\sum_i 1(bin(x)=bin(x_i))\n\\]\nSimple yet powerful approach to estimate and visualize distributions. But with lots of room for improvement. It may provide very different pictures based on “h”"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#histograms-with-varying-h",
    "href": "adv_class/03par_spar_npar.html#histograms-with-varying-h",
    "title": "Semi- and Non- Parametric regression",
    "section": "Histograms with Varying h",
    "text": "Histograms with Varying h"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#kernel-density",
    "href": "adv_class/03par_spar_npar.html#kernel-density",
    "title": "Semi- and Non- Parametric regression",
    "section": "Kernel density",
    "text": "Kernel density\nAn alternative that overcomes some of the limitations of the Histogram is known as the kernel density estimator. This is defined as:\n\\[\n\\hat f(x) = \\frac{1}{nh}\\sum_i K\\left(\\frac{X_i-x}{h}\\right)\n\\]\nwhere \\(K\\) is what is known as a kernel function.\nThis function is such that has the following properties:\n\\[\n\\int K(z)dz = 1 ; \\int zK(z)dz = 0 ; \\int z^2K(z)dz &lt; \\infty\n\\]\nIs a well behaved pdf on its own, that is symmetric, with defined second moment.\n\nas with the histogram estimator, the Kden is just an average of functions, that has the advantage of being smooth.\nAlthough it also depends strongly, on the choice of bandwidth."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#kernel-density-visualization",
    "href": "adv_class/03par_spar_npar.html#kernel-density-visualization",
    "title": "Semi- and Non- Parametric regression",
    "section": "Kernel density: Visualization",
    "text": "Kernel density: Visualization"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#trade-off-bias-vs-variance",
    "href": "adv_class/03par_spar_npar.html#trade-off-bias-vs-variance",
    "title": "Semi- and Non- Parametric regression",
    "section": "Trade off: Bias vs variance",
    "text": "Trade off: Bias vs variance\nWhile this estimators are “flexible” in the sense that we impose very simple assumptions for estimation, there is still one parameter that needs attention.\nThe bandwidth \\(h\\)\nThis does not (or cannot) be estimated, rather, needs to be calibrated to balance two problems in Non-parametric analysis. Bias vs Variance:\n\nwhen \\(h\\rightarrow 0\\) , the bias of your estimator goes to zero ( in average). Intuitively \\(\\hat f(x)\\) is constructed based on information that comes from \\(x\\) alone.\nBut the variance increases! Because things will vary for every \\(x\\).\nwhen \\(h \\rightarrow \\infty\\) , the bias increases, because you start using data that is very different to \\(x\\) to estimate \\(\\hat f(x)\\).\nBut variance decreases. Since the “function” is now very smooth (a line?)\n\nThus, special attention is needed to choose the right h, which minimizes the problems (bias and variance)."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#kdensity-bias-vs-variance",
    "href": "adv_class/03par_spar_npar.html#kdensity-bias-vs-variance",
    "title": "Semi- and Non- Parametric regression",
    "section": "Kdensity, Bias vs Variance",
    "text": "Kdensity, Bias vs Variance"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#other-considerations",
    "href": "adv_class/03par_spar_npar.html#other-considerations",
    "title": "Semi- and Non- Parametric regression",
    "section": "Other Considerations",
    "text": "Other Considerations\n\nAs shown above, one needs to choose the bandwidth \\(h\\) carefully, balancing the bias-variance trade off. Common approach is to simply use rule-of-thumb approaches to select this parameter:\n\n\\[\nh = 1.059 \\sigma n ^ {-1/5} \\\\ h = 1.3643 * d * n ^ {-1/5} * min(\\sigma,iqr\\sigma)\n\\]\nBut other approaches may work better.\n\nA second consideration is the choice of Kernel function! (see help kdensity -&gt; kernel)\n\nAlthough, except in few cases, the choice of bandwidth matters more than the kernel function.\n\nThis method works well when your data is smooth and continuous. But for so much for discrete data.\n\nNevertheless, it is still possible to use it with discrete data, and kernel weights!\n\nCan be “easily” extended to multiple dimensions \\(f(x,y,z,...)\\), including mixture of continuous and discrete data. You just multiple Kernels!\n\nBut, beware of Curse of dimensionality.\nBut still better than just Subsampling!"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#kfunctions",
    "href": "adv_class/03par_spar_npar.html#kfunctions",
    "title": "Semi- and Non- Parametric regression",
    "section": "Kfunctions",
    "text": "Kfunctions"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#np---regression",
    "href": "adv_class/03par_spar_npar.html#np---regression",
    "title": "Semi- and Non- Parametric regression",
    "section": "NP - Regression",
    "text": "NP - Regression\n\nAs hinted from the beginning, the idea of non-parametric regressions is related to estimate a model that is as flexible as it can probably be.\nThis relates to the CEF, where we want to estimate a conditional mean for every combination of X’s. In other words, you aim to estimate models that are valid “locally”. A very difficult task.\n\nYou have a limited sample size\nYou may not see all possible X’s combinations\nand for some, you may have micro-samples (n=1) Can you really do something with this?\n\nYes, make your model flexible, but not overly flexible! but how?\n\nKernel regression ; Spline regression\nPolynomial regression; Smoothed Spline regression."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#univariate-case",
    "href": "adv_class/03par_spar_npar.html#univariate-case",
    "title": "Semi- and Non- Parametric regression",
    "section": "Univariate case",
    "text": "Univariate case\n\nConsider a univariate case \\(y,X\\) where you only have 1 indep variable, which are related as follows:\n\n\\[\ny = m(x) + e\n\\]\nwhich imposes the simplifying assumption that error is additive.\n\nIn the parametric case:\n\n\\[\ny =b_0 + b_1 x + b_2 x^2 +b_3 x^3 +...+e\n\\]\n(this is, in fact, starting to become less parametric)\n\nBut in the full (unconstrained) model it would just be (simple conditional mean:\n\n\\[\nE(y|X) = \\hat m(x) = \\frac{\\sum y_i 1(x_i=x)}{\\sum 1(x_i=x)}\n\\]\nProblems? Impossible to do out out sample predictions, and if \\(n&lt;42\\) inference would be extremely unreliable."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#local-constant-regression",
    "href": "adv_class/03par_spar_npar.html#local-constant-regression",
    "title": "Semi- and Non- Parametric regression",
    "section": "Local Constant Regression",
    "text": "Local Constant Regression\nWe can improve over the Unconstrained mean using the following connection:\n\n\\(1(x_i=x)\\) is a non smooth indicator that shows if an observation is included (counted towards the mean).\nIt can be substituted with \\(K_h(x_i,x)\\), which is the standardized kernel function. (ie \\(K_h(x_i,x) = \\frac{1}{h} K(\\frac{x_i-x}{h})\\)\nDepending on \\(h\\) , it gives the most weight the closer \\(x_i\\) is to \\(x\\).\nThis gives what is known as the Nadaraya-Watson or Local constant estimator:\n\n\\[\n\\hat m(x) = \\frac{\\sum y_i K_h(x_i,x)}{\\sum K_h(x_i,x)} = \\sum y_i w_i\n\\]\nWhich, on its core, is simply a weighted regression, with weights given by \\(\\frac{K_h(x_i,x)}{\\sum K_h(x_i,x)}\\)\n\nKernel Regressions “borrows” info from neighboring observations to obtain a smooth estimator."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#visuals",
    "href": "adv_class/03par_spar_npar.html#visuals",
    "title": "Semi- and Non- Parametric regression",
    "section": "Visuals",
    "text": "Visuals"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#considerations",
    "href": "adv_class/03par_spar_npar.html#considerations",
    "title": "Semi- and Non- Parametric regression",
    "section": "Considerations",
    "text": "Considerations\n\nLocal Constant estimator is simple to estimate with a single variable. And so with multiple variables:\n\n\\[\n\\hat m(x,z) = \\frac{\\sum y_i K_h(x_i,x) \\times K_h(z_i,z)}{\\sum K_h(x_i,x) \\times K_h(z_i,z)}\n\\]\nThe problem, however, lies on the curse of dimensionality. - More dimensions, less data per \\((x,z)\\) point, unless you “increase” bandwidths.\n\nAs Before, it all depends on the Bandwidth \\(h\\).It determines the “flexibility” of the model.\nThe local constant tends to have considerable bias (specially near limits of the distribution, or when \\(g\\) has too much curvature)"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#choosing-h",
    "href": "adv_class/03par_spar_npar.html#choosing-h",
    "title": "Semi- and Non- Parametric regression",
    "section": "Choosing h",
    "text": "Choosing h\nThe quality of the NPK regression depends strongly on the choice of \\(h\\). And as with density estimation, the choice translates into a tradeoff between bias and variance of the estimation.\nThere are various approaches to choose \\(h\\). Some which depend strongly on the dimensionality of the model.\nFor Example, Stata command lpoly estimates local constant models, using the following:\n\nBut that is not the only approach.\nAn alternative (used for regularization) is using Cross-Validaton. (a method to evaluate the predictive power of a model)"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#cross-validation-intuition",
    "href": "adv_class/03par_spar_npar.html#cross-validation-intuition",
    "title": "Semi- and Non- Parametric regression",
    "section": "Cross Validation: Intuition",
    "text": "Cross Validation: Intuition\n\nSeparate your data in two parts: Training and testing Sample.\nEstimate your model in the TrainS, and evaluate predictive power in TestS.\nTo obtain a full view of predictive power, Repeat the process rotating the training set\n\n\\[\nmse = \\frac{1}{N}\\sum(y_i - g_{-k}(x))^2\n\\]\nThis should give you a better idea of the predictive power of the model."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#cross-validation-in-stata",
    "href": "adv_class/03par_spar_npar.html#cross-validation-in-stata",
    "title": "Semi- and Non- Parametric regression",
    "section": "Cross-validation in Stata",
    "text": "Cross-validation in Stata\nfrause oaxaca, clear\nssc isntall cv_kfold\n\nqui:reg lnwage educ exper tenure female age\n\ncv_kfold\nk-fold Cross validation\nNumber of Folds     :          5\nNumber of Repetions :          1\nAvg Root Mean SE    :    0.45838\n\nqui:reg lnwage c.(educ exper tenure female age)\n               ##c.(educ exper tenure female age)\n\ncv_kfold\nk-fold Cross validation\nNumber of Folds     :          5\nNumber of Repetions :          1\nAvg Root Mean SE    :    0.42768\n\n. qui:reg lnwage c.(educ exper tenure female age)\n                 ##c.(educ exper tenure female age)\n                 ##c.(educ exper tenure female age)\n\n. cv_kfold\nk-fold Cross validation\nNumber of Folds     :          5\nNumber of Repetions :          1\nAvg Root Mean SE    :    0.43038\n\nssc install cv_regress\n* Does lOOCV for regression\ncv_regress\n\nLeave-One-Out Cross-Validation Results \n-----------------------------------------\n         Method          |    Value\n-------------------------+---------------\nRoot Mean Squared Errors |       0.4244\nLog Mean Squared Errors  |      -1.7144\nMean Absolute Errors     |       0.2895\nPseudo-R2                |      0.36344\n-----------------------------------------"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#loocv",
    "href": "adv_class/03par_spar_npar.html#loocv",
    "title": "Semi- and Non- Parametric regression",
    "section": "LOOCV",
    "text": "LOOCV\nBecause the “choice” of “folds” and Repetitions, and the randomness, may produce different results every-time, one also has the option of using the “leave-one-out” approach.\nThis means, leave one observation out, and use the rest to make the predictions.\n\\[\nCV(h) = n^{-1}\\sum_{i=1}^n(y_i - \\hat g_{-i}(x_i))^2\n\\]\nThis is not as bad as it looks, since we can use the shortcut\n\\[\nCV(h) = n^{-1}\\sum_{i=1}^n\\left(\\frac{y_i - \\hat g(x_i)}{1-w_i/\\Sigma w_j}\\right)^2\n\\]\nIn Stata, the command npregress kernel uses this type of cross-validation to determine “optimal” \\(h\\)\nlpoly y x, kernel(gaussian) nodraw\ndisplay r(bwidth)\n.23992564\nnpregress kernel y x, estimator(constant) noderiv\n. Bandwidth\n-------------------------\n             |      Mean \n-------------+-----------\n           x |  .4064052 \n-------------------------"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#extending-from-constant-to-polynomial",
    "href": "adv_class/03par_spar_npar.html#extending-from-constant-to-polynomial",
    "title": "Semi- and Non- Parametric regression",
    "section": "Extending from constant to Polynomial",
    "text": "Extending from constant to Polynomial\nAn alternative way to understanding the simple NW (local constant) regressions, is to understand it as a local regression model with anything but a constant:\n\\[\n\\hat m(x)=min\\sum(y_i - \\beta_0)^2 w(x,h)_i\n\\]\nThis means that you could extend the analogy and include “centered” polynomials to the model.\n\\[\n\\begin{aligned}\nmin &\\sum(y_i - \\beta_0 - \\beta_1 (x_i -x) -\\beta_2 (x_i - x) ^2 - ...-\\beta_k(x_i-x)^k)^2 w(x,h)_i \\\\\n\\hat m(x) &= \\hat \\beta_0\n\\end{aligned}\n\\]\nThis is called the local polynomial regression.\n\nBecause its more flexible, it shows less bias when the true function shows a lot of variation.\nBecause of added polynomials, it requires more information (larger \\(h\\))\nIt can be used to easily obtain local marginal effects.\nAnd can also be used with multinomial models (local planes)\n\n\\[min \\sum (y_i - \\beta_0 - \\beta_1 (x_i-x) - \\beta_2 (z_i-z))^2 w(x,z,h)\n\\]"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#local-constant-to-local-polynomial",
    "href": "adv_class/03par_spar_npar.html#local-constant-to-local-polynomial",
    "title": "Semi- and Non- Parametric regression",
    "section": "Local Constant to Local Polynomial",
    "text": "Local Constant to Local Polynomial\n\n\nwebuse motorcycle\ntwo scatter accel time || ///\nlpoly accel time , degree(0) n(100) || ///\nlpoly accel time , degree(1) n(100) || ///\nlpoly accel time , degree(2) n(100) || ///\nlpoly accel time , degree(3) n(100) , ///\nlegend(order(2 \"LConstant\" 3 \"Local Linear\" ///\n4 \"Local Cubic\" 5 \"Local Quartic\"))"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#statistical-inference",
    "href": "adv_class/03par_spar_npar.html#statistical-inference",
    "title": "Semi- and Non- Parametric regression",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nFor Statistical Inference, since each regression is just a linear model, standard errors can be obtained using the criteria as in Lecture 1. (Robust, Clustered, bootstrapped).\n\nWith perhaps one caveat. Local estimation and standard errors may need to be estimated “globally”, rather than locally.\n\nThe estimation of marginal effects becomes a bit more problematic.\n\nLocal marginal effects are straightforward (when local linear or higher local polynomial is used)\nGlobal marginal effects, can be obtained averaging all local marginal effects.\nHowever, asymptotic standard errors are difficult to obtain (consider the multiple correlated components), but bootstrapping is still possible."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#stata-example",
    "href": "adv_class/03par_spar_npar.html#stata-example",
    "title": "Semi- and Non- Parametric regression",
    "section": "Stata Example",
    "text": "Stata Example\nfrause oaxaca\nnpregress kernel lnwage age exper\n\nComputing mean function\n  \nMinimizing cross-validation function:\n  \nIteration 0:   Cross-validation criterion = -1.5904753  \nIteration 1:   Cross-validation criterion = -1.5906158  \nIteration 2:   Cross-validation criterion = -1.5907243  \nIteration 3:   Cross-validation criterion = -1.5911389  \nIteration 4:   Cross-validation criterion = -1.5911389  \nIteration 5:   Cross-validation criterion = -1.5911855  \nIteration 6:   Cross-validation criterion = -1.5912075  \n  \nComputing optimal derivative bandwidth\n  \nIteration 0:   Cross-validation criterion =  .01378252  \nIteration 1:   Cross-validation criterion =   .0019967  \nIteration 2:   Cross-validation criterion =  .00196967  \nIteration 3:   Cross-validation criterion =  .00196371  \n\nBandwidth\n------------------------------------\n             |      Mean     Effect \n-------------+----------------------\n         age |  2.843778   15.10978 \n       exper |  3.113587   16.54335 \n------------------------------------\n\nLocal-linear regression                    Number of obs      =          1,434\nKernel   : epanechnikov                    E(Kernel obs)      =          1,434\nBandwidth: cross-validation                R-squared          =         0.3099\n------------------------------------------------------------------------------\n      lnwage |   Estimate\n-------------+----------------------------------------------------------------\nMean         |\n      lnwage |   3.339269\n-------------+----------------------------------------------------------------\nEffect       |\n         age |   .0169326\n       exper |  -.0010196\n------------------------------------------------------------------------------\nNote: Effect estimates are averages of derivatives.\nNote: You may compute standard errors using vce(bootstrap) or reps()."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#other-types-of-non-parametric-models",
    "href": "adv_class/03par_spar_npar.html#other-types-of-non-parametric-models",
    "title": "Semi- and Non- Parametric regression",
    "section": "Other types of “non-parametric” models",
    "text": "Other types of “non-parametric” models\nWe have explored the basic version of non-parametric modeling. But its not the only one.\nThere are at least two others that are easy to implement.\n\nNonparametric Series Regression (we will see this)\nSmoothing series/splines: Which borrows from Series regression and Ridge Regression."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#non-parametric-series",
    "href": "adv_class/03par_spar_npar.html#non-parametric-series",
    "title": "Semi- and Non- Parametric regression",
    "section": "Non-parametric series",
    "text": "Non-parametric series\nThis approach assumes that model flexibility can achieve by using “basis” functions in combination with Interactions, but using “global” regressions (OLS)\nBut what are “basis” functions? They are a collection of terms that approximates smooth functions arbitrarily well.\n\\[\n\\begin{aligned}\ny &= m(x,z)+e  \\\\\nm(x,z)  &= B(x)+ B(z)+B(x)*B(z)  \\\\\nB(x)  &= (x, x^2, x^3,...) \\\\\nB(x)  & = fracPoly \\\\\nB(x)  &= (x, max(0,x-c_1), max(0,x-c_2), ... \\\\\nB(x)  &= (x,x^2,max(0,x-c_1)^2, max(0,x-c_2)^2, ... \\\\\nB(x)  &= B-splines\n\\end{aligned}\n\\]\n\nPolynomials can be used, but there may be problems with high order polynomials. (Runge’s phenomenon,multiple-co-linearity). They are “global” estimators.\nFractional polynomials: More flexible than polynomials, without producing “waves” on the predictions\nNatural Splines, are better at capturing smooth transitions (depending on degree). Require choosing Knots appropriately.\nB-splines are similar to N-splines, but with better stat properties. Also require choosing knots\n\nExcept for correctly estimating the Bases functions (fracpoly and Bsplines are not straight forward), estimation requires simple OLS."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#np-series---tuning",
    "href": "adv_class/03par_spar_npar.html#np-series---tuning",
    "title": "Semi- and Non- Parametric regression",
    "section": "NP series - tuning",
    "text": "NP series - tuning\n\nWhile NP-series are easy to estimate, we also need to address problems of over-fitting.\nWith Polynomial: What degree of polynomial is correct? What about the degree of the interactions?\nFractional Polynomials: How many terms are needed, what would their “degrees” be.\nNsplines, Bsplines: How to choose degree? and where to set the knots?\n\nThese questions are similar to the choosing \\(h\\) in kernel regressions. However, model choice is simple…Cross validation.\n\nEstimate a model under different specifications (cut offs), and compare the out-of-sample predictive power. (see Stata: cv_kfold or cv_regress)\n\nOne more problem left. Statistical Inference"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#np-series---se-and-mfx",
    "href": "adv_class/03par_spar_npar.html#np-series---se-and-mfx",
    "title": "Semi- and Non- Parametric regression",
    "section": "NP series - SE and Mfx",
    "text": "NP series - SE and Mfx\nLecture 1 applies here. Once the model has been chosen, you can estimate SE using appropriate methods. There is only one caveat\n\nStandard SE estimation ignores the uncertainty of choosing cut-offs or polynomial degrees. In principle, cut-offs uncertainty can be modeled. But requires non-linear estimation.\n\nMarginal effects are somewhat easier for some basis. Just take derivatives:\n\\[\ny = b_0 + b_1 x + b_2 x^2 +b_3 max(0,x-c)^2 + e \\\\\n\\frac{dy}{dx}=b_1 + 2 b_2 x + 2 b_3 (x-c) 1(x&gt;c)\n\\]\n\nBut keeping track of derivatives in a multivariate model is difficult, and often, the functions are hard to track down. so how to implement it?"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#np-series-implementation-marginal-effects",
    "href": "adv_class/03par_spar_npar.html#np-series-implementation-marginal-effects",
    "title": "Semi- and Non- Parametric regression",
    "section": "NP series: Implementation marginal effects",
    "text": "NP series: Implementation marginal effects\nAs always, it all depends on how are the models estimated.\n\nStata command npregress series allows you to estimate this type of models using polynomials, splines and B-splines. And also allows estimates marginal effects for you. (can be slow)\nfp estimates fractional polynomials, but does not estimate marginal effects for you.\nR is a bit more flexible in terms of tracking down functions as variables (~x+I(x*x)+ln(x)). So it may be “easy” to estimate those effects.\nIn Stata, you can use the package f_able to estimate those marginal effects, however. see here for details. and SSC for the latest update.\n\nfrause oaxaca, clear\ndrop agesq\nf_spline age = age, nk(1) degree(3)\nf_spline exper = exper, nk(1) degree(3)\nqui:regress lnwage c.(age*)##c.(exper*)\nf_able age? exper?, auto \nmargins, dydx(age exper) noestimcheck \n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0360234   .0033909    10.62   0.000     .0293775    .0426694\n       exper |   .0082594   .0050073     1.65   0.099    -.0015547    .0180735\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#semiparametric-regressions",
    "href": "adv_class/03par_spar_npar.html#semiparametric-regressions",
    "title": "Semi- and Non- Parametric regression",
    "section": "Semiparametric Regressions",
    "text": "Semiparametric Regressions\n\nFull non-parametric estimations are powerful to identify very flexible functional forms. To avoid overfitting, however, one must choose tuning parameters appropriately (\\(h\\) and \\(cutoffs\\) ).\nA disadvantage: Curse of dimensionality. More variables need more data to provide good results. But, the more data you have, the more difficult to estimate (computing time).\nIt also becomes extremly difficult to interpret. (too much flexibility)\nAn alternative, Use the best of both worlds: Semiparametric regression\n\nFlexiblity when needed with the structure of standard regressions, to avoid the downfalls of fully nonparametric models"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#partially-linear-model",
    "href": "adv_class/03par_spar_npar.html#partially-linear-model",
    "title": "Semi- and Non- Parametric regression",
    "section": "Partially Linear model",
    "text": "Partially Linear model\n\\[\ny = x\\beta +g(z) +e\n\\]\nThis model assumes that only a smaller set of covariates need to be estimated non-parametrically in the model.\nEstimators:\n\nnpregress series: Use BasisF to estimate \\(g(z)\\) . Or other series regressions, fractional polynomial fp\nYatchew 1997: For a single z, sort variables by it, and estimate: \\(\\Delta y=\\Delta X\\beta+ \\Delta g(z) + \\Delta e\\). This works because \\(\\Delta g(z)\\rightarrow 0\\)\nEstimate \\(g(z)\\) regressing \\(y-x\\hat \\beta\\) on \\(z\\). See plreg\nRobinson 1988: Application of FWL. Estimate \\(y = g_y(z)+e_y\\) and \\(x = g_x(z)+e_x\\) and estimate \\(\\beta = (e_x ' e_x)^{-1} e_x ' e_y\\) . For \\(g(z)\\) same as before. See semipar.\nOther methods available see semi_stata"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#generalized-additive-model",
    "href": "adv_class/03par_spar_npar.html#generalized-additive-model",
    "title": "Semi- and Non- Parametric regression",
    "section": "Generalized Additive model",
    "text": "Generalized Additive model\n\\[\ny = g(x) +g(z)+e\n\\]\nThis model assumes the effect of X and Z (or any other variables) are additive separable, and may have a nonlinear effect on y.\n\nnpregress series: with non-interaction option. Fractional polynomials mfp, cubic splines mvrs (see mvrs) , or manual implementation.\nKernel regression possible. (as in Robinson 1988), but requires an iterative method. (back fitting algorithm)\n\n\\(g(x) = smooth (y-g(z)|x)\\), center \\(g(x)\\) , and \\(g(z) = smooth (y-g(x)|z)\\), center \\(g(z)\\) until convergence\n\nIn general, it can be easy to apply, but extra work required for marginal effects."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#smooth-coefficient-model",
    "href": "adv_class/03par_spar_npar.html#smooth-coefficient-model",
    "title": "Semi- and Non- Parametric regression",
    "section": "Smooth Coefficient model",
    "text": "Smooth Coefficient model\n\\[\ny = g_0(z)+g_1(z)x + e\n\\]\nThis model assumes that \\(X's\\) have a locally linear effect on $y$, but that effect varies across values of \\(z\\), in a non-parametric way.\n\nfp or manual implementation of basis functions, with interaction. May allow for multiple variables in \\(z\\)\nOne can also use Local Kernel regressions. locally weighted regression where All X variables are considered fixed, or interacted with polynomials of Z. Choice of bandwidth problematic, but doable (LOOCV).\nvc_pack can estimate this models with a single z, as well as test it. Overall marginal effects still difficult to obtain."
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#example",
    "href": "adv_class/03par_spar_npar.html#example",
    "title": "Semi- and Non- Parametric regression",
    "section": "Example",
    "text": "Example\n\nfrause oaxaca\nnpregress kernel lnwage age exper\nmargins, dydx(*)\n\nvc_bw lnwage educ exper tenure female married divorced, vcoeff(age)\nvc_reg lnwage educ exper tenure female married divorced, vcoeff(age) k(20)\nssc install addplot\nvc_graph educ exper tenure female married divorced, rarea\naddplot grph1:, legend(off) title(Education)\naddplot grph2:, legend(off) title(Experience)\naddplot grph3:, legend(off) title(Tenure)\naddplot grph4:, legend(off) title(Female)\naddplot grph5:, legend(off) title(Married)\naddplot grph6:, legend(off) title(Divorced)\n\ngraph combine grph1 grph2 grph3 grph4 grph5 grph6"
  },
  {
    "objectID": "adv_class/03par_spar_npar.html#example-1",
    "href": "adv_class/03par_spar_npar.html#example-1",
    "title": "Semi- and Non- Parametric regression",
    "section": "Example",
    "text": "Example\n\nWage Profile across years"
  },
  {
    "objectID": "adv_class/05uqreg.html#introduction",
    "href": "adv_class/05uqreg.html#introduction",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Introduction",
    "text": "Introduction\nAs we saw last class, conditional quantile regressions have only one purpose:\n\nAnalyze relationships between conditional distributions.\n\nThis is a very useful tool!. As it allows you to move beyond Average relationships.\n\nHow do people (who are not all average) would be affected by changes in \\(Xs\\)\n\nThere is a limitation, however. The effects you may estimate, will depend strongly on model specification.\n\nThis is similar to OVB. Changing covariates could drastically change the conditional distributions and associated coefficients\n\nWhat if, you are interested in distributional effects across the whole population! Not only a subsample?"
  },
  {
    "objectID": "adv_class/05uqreg.html#eqyx-is-not-qy",
    "href": "adv_class/05uqreg.html#eqyx-is-not-qy",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "E(q(y|X)) is not Q(y)",
    "text": "E(q(y|X)) is not Q(y)\nOne common mistake one makes when analyzing QRegressions is to make interpretations as if the average effects on the \\(qth\\) conditional quantiles would be the same as the effect on the “overall” \\(qth\\) quantile.\nExcept for few cases (when Quantile regressions are not relevant), CQ effects do not translate directly into Changes into the unconditional quantile.\nHowever, as a policy maker, this would be the most relevant estimand you may be interested in :\n\nHow does improving education affect inequality?\nWould eliminating Unionization would increase wage inequality?\nIs there heterogeneity in consumption expenditure?\n\nHowever, going from Conditional to unconditional statistics (not only Q) is not always straight forward."
  },
  {
    "objectID": "adv_class/05uqreg.html#waitwhat-do-we-mean-unconditional",
    "href": "adv_class/05uqreg.html#waitwhat-do-we-mean-unconditional",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "⌚Wait…What do we mean unconditional?",
    "text": "⌚Wait…What do we mean unconditional?\nOne of the questions I read a lot regarding UQR is what do we mean unconditional?\n\nThis is perhaps a someone poor choice of words.\nAnytime we estimate ANY statistic, we condition on something.\n\nWe condition on all individual characteristics (including errors)\nWe condition on groups characteristics (CQREG and CEF)\nor, We condition on all characteristics (distributions). We happen to call this, unconditional statistics.\n\nThis, however, does make a big difference in interpretation."
  },
  {
    "objectID": "adv_class/05uqreg.html#from-condition-on-individuals",
    "href": "adv_class/05uqreg.html#from-condition-on-individuals",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "From Condition on Individuals,",
    "text": "From Condition on Individuals,\nto conditioning on Distributions\n\\[\\begin{aligned}\ny_i &= b_0 + b_1 x_i + e_i + x_i e_i \\\\\n\\frac{dy_i}{dx_i}&=b_1 + e_i \\\\\nE(y_i|x_i=x) &= b_0 + b_1 x  \\\\\n\\frac{dE(y_i|x)}{dx}&=b_1 \\\\\nE(E(y_i|x_i=x))=E(y_i) &= b_0 + b_1 E(x_i)    \\\\\n\\frac{dE(y_i)}{dE(x_i)}&=b_1\n\\end{aligned}\n\\]\nSame effects, but different interpretations (specially last one)"
  },
  {
    "objectID": "adv_class/05uqreg.html#how-are-unconditional-effects-estimated",
    "href": "adv_class/05uqreg.html#how-are-unconditional-effects-estimated",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "How are Unconditional effects Estimated?",
    "text": "How are Unconditional effects Estimated?\nConsider any distributional statistic \\(v\\), which takes as arguments, all observations, density distributions \\(f()\\), or cumulative distributions \\(F()\\).\n\\[\nv = v(F_y) \\ or \\ v(f_y) \\ or \\ v(y_1, y_2, ...,y_n)\n\\]\nAnd to simplify notation, lets say this function is defined as follows:\n\\[\nv(f_y) = \\int_{-\\infty}^\\infty h(y,\\theta) f(y)dy\n\\]\nThis simply considers distributional statistics \\(v\\) that can be estimated by simply integrating a transformation of \\(h(y,\\theta)\\) given a set of parameters \\(\\theta\\).\nBut for now, lets consider only the Identify function \\(h(y,\\theta)=y\\)\nbut…What about Controls??"
  },
  {
    "objectID": "adv_class/05uqreg.html#introducing-controls",
    "href": "adv_class/05uqreg.html#introducing-controls",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Introducing controls",
    "text": "Introducing controls\nAssume there is a joint distribution of function \\(f(y,x)\\), then\n\\[\n\\begin{aligned}\nf(y,x)&=f(y|x)f(x) \\\\\nf(y) &= \\int f(y|x) f(x) dx\n\\end{aligned}\n\\]\nAnd all together:\n\\[\n\\begin{aligned}\nv(f_y) &= \\int y \\int f(y|x) f(x) dx \\ dy \\\\\nv(f_y) &= \\iint y f(y|x) dy \\ f(x) dx  \\\\\nv(f_y) &= \\int  E(y|X) f(x) dx  \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/05uqreg.html#or-a-bit-more-general",
    "href": "adv_class/05uqreg.html#or-a-bit-more-general",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Or a bit more General",
    "text": "Or a bit more General\n\\[\nv(f_y) = \\iint h(y,\\theta) f(y|x)f(x)dxdy\n\\]\nSo, the statistic \\(v\\) will change if:\n- We change the function \\(h\\) or its parameters \\(\\theta\\).\n- Assume some shocks that change the conditional \\(f(y|x)\\)\n- or the distribution of characteristics change!\nNote: \\[f(y|x) \\sim \\beta \\text{ and }\nf(x) \\sim x\n\\]"
  },
  {
    "objectID": "adv_class/05uqreg.html#againhow-are-unconditional-effects-estimated",
    "href": "adv_class/05uqreg.html#againhow-are-unconditional-effects-estimated",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Again…How are Unconditional effects Estimated?",
    "text": "Again…How are Unconditional effects Estimated?\nIn an ideal scenario, you simple get the data under to regimes (before and after changes in \\(x\\)), and do the following:\n\\[\\Delta v = v(f'_y)-v(f_y)\n\\]\nThat is, just estimate the statistic in two scenarios (\\(f'\\) and \\(f\\)), and calculate the difference. (impossible!)\nBut there are (at least) three alternatives:\n\nUsing Reweighting approaches to “reshape” the data: \\(f(x)\\) (non-parametric)\nIdentify \\(f(y|x)\\) so one can simulate how \\(\\Delta X\\) affect y\nFocus on the statistic \\(v\\) and indirectly identify the effects of interest. (RIF!)"
  },
  {
    "objectID": "adv_class/05uqreg.html#op1-re-weighting",
    "href": "adv_class/05uqreg.html#op1-re-weighting",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Op1: Re-weighting",
    "text": "Op1: Re-weighting\nConsider the following. there is a policy such that you plan to improve education in a country. Every single person will have at least 7 years of education, and will have free access to two additional years of education if they want to.\nIn other words, characteristics change from \\(f(x) \\rightarrow g(x)\\) . But you do not see this! \\[ v(g_y) = \\iint h(y,\\theta) f(y|x) \\color{red}{g(x)}dxdy \\]\nbut perhaps, we could see this:\n\\[\\hat v(g_y) = \\iint h(y,\\theta) f(y|x) \\color{red}{\\hat w(x)}f(x) dxdy\n\\]\nif we can come up with a set of weights \\(\\color{red}{\\hat w(x)}\\) such that \\(f(x)\\hat w(x)=g(x)\\)\n\\[\n\\hat w(x) = \\frac{\\hat g(x)}{\\hat f(x)}\n\\]"
  },
  {
    "objectID": "adv_class/05uqreg.html#op1-re-weighting-1",
    "href": "adv_class/05uqreg.html#op1-re-weighting-1",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Op1: Re-weighting",
    "text": "Op1: Re-weighting\nSimple, yet hard. Estimation of multivariate densities can be a difficult task. So assume the following\n\\[\nf(x) = h(x|s=0) ; g(x) = h(x|s=1)\n\\]\nThis makes things “easier”.\n\nBayes: \\[\n\\begin{aligned}\nh(x|s=k) &= \\frac{h(x)p(s=k|x)}{p(s=k)} \\\\\n\\hat w(x)  \n     &= \\frac{h(x)p(s=1|x)}{h(x)p(s=0|x)}\\frac{p(s=0)}{p(s=1)} \\\\\n     &=\\frac{p(s|x)}{1-p(s|x)} \\frac{1-p(s)}{p(s)}\n\\end{aligned}\n\\]\n\nEasier to estimate conditional probabilities, (logit probit or other) than Densities"
  },
  {
    "objectID": "adv_class/05uqreg.html#example",
    "href": "adv_class/05uqreg.html#example",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Example",
    "text": "Example\n. *** UQR\n. *** Reweighting\n. webuse dui, clear\n(Fictional data on monthly drunk driving citations)\n\n. ** Create Fake Sample\n. gen id = _n\n\n. expand 2\n(500 observations created)\n\n. bysort id:gen smp = _n ==2\n\n. ** Now you have two of ever person. So lets do some Policy\n. ** Fines increase lower fines more than higher ones, up to 12\n. replace fines = 0.1*(12-fines)+fines if smp==1\n(498 real changes made)\n\n. ** Estimate logit \n. logit smp c.fines##c.fines taxes i.csize college\n\nIteration 0:   log likelihood = -693.14718  \nIteration 1:   log likelihood = -680.74735  \nIteration 2:   log likelihood = -680.68931  \nIteration 3:   log likelihood = -680.68931  \n\nLogistic regression                                     Number of obs =  1,000\n                                                        LR chi2(6)    =  24.92\n                                                        Prob &gt; chi2   = 0.0004\nLog likelihood = -680.68931                             Pseudo R2     = 0.0180\n\n---------------------------------------------------------------------------------\n            smp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n----------------+----------------------------------------------------------------\n          fines |   4.231032   1.762476     2.40   0.016     .7766421    7.685422\n                |\nc.fines#c.fines |  -.1923691   .0877831    -2.19   0.028    -.3644209   -.0203173\n                |\n          taxes |     .00044   .1407545     0.00   0.998    -.2754338    .2763139\n                |\n          csize |\n        Medium  |   .0549913    .161242     0.34   0.733    -.2610373    .3710198\n         Large  |   .0399068   .1526637     0.26   0.794    -.2593086    .3391222\n                |\n        college |  -.0218305   .1488361    -0.15   0.883    -.3135439    .2698829\n          _cons |  -22.99551   8.826624    -2.61   0.009    -40.29537   -5.695646\n---------------------------------------------------------------------------------\n\n. predict pr_smp\n(option pr assumed; Pr(smp))\n\n. gen wgt = pr_smp / (1-pr_smp) \n\n. replace wgt = 1 if smp==0\n(500 real changes made)\n\n.  xi:tabstat fines i.csize college  taxes [w=wgt],  by(smp)\ni.csize           _Icsize_1-3         (naturally coded; _Icsize_1 omitted)\n(analytic weights assumed)\n\nSummary statistics: Mean\nGroup variable: smp \n\n     smp |     fines  _Icsiz~2  _Icsiz~3   college     taxes\n---------+--------------------------------------------------\n       0 |    9.8952       .29      .358      .248      .704\n       1 |  10.24873  .2926133  .3584506  .2474627  .7045357\n---------+--------------------------------------------------\n   Total |  10.07894  .2913582  .3582342  .2477208  .7042784\n------------------------------------------------------------\n\ntwo (kdensity citations if smp==0 ) ///\n    (kdensity citations if smp==1 [w=wgt]) /// \n    , legend(order(1 \"Before Policy\" 2 \"After Policy\"))\n\ntabstat citations [w=wgt], by(smp) stats(p10 p25 p50 mean p75 p90  )\n(analytic weights assumed)\n\nSummary for variables: citations\nGroup variable: smp \n\n     smp |       p10       p25       p50      Mean       p75       p90\n---------+------------------------------------------------------------\n       0 |      11.5        15        20    22.018        27      34.5\n       1 |        11        15        19  20.69419        26        32\n---------+------------------------------------------------------------\n   Total |        11        15        20  21.32998        27        33\n----------------------------------------------------------------------\n    \nIncreasing fines, may reduce citations in about 1.3., but have almost no effect at the bottom of the distribution.\nWhat about Standard errors? Bootstrap! (logit and estimation, probably clustering at individual level)\nEasy to extend to other Statistics, but, can only provide results “within” support."
  },
  {
    "objectID": "adv_class/05uqreg.html#op2-model-conditional-distribution",
    "href": "adv_class/05uqreg.html#op2-model-conditional-distribution",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Op2: Model Conditional Distribution",
    "text": "Op2: Model Conditional Distribution\nSay that you are interested in the same Policy, but do not trust re-weighting. Instead you want to model the Outcome, using some parametric or nonparametric analysis\n\nDefine your model. Should be feasible enough to accommodate changes in the conditional distribution. (one “model” for each \\(X's\\) combination?)\nUse the model to make predictions of your outcome (quite a few times). and summarize all results.\n\nOptions for flexible mode?\n\nYou can use Heteroskedastic OLS \\(y\\sim N(x\\beta,x\\gamma)\\) and predict from here\nYou can use CQregressions to simulate the results.\n\nOne of this is similar to what we do in simulation analysis, and imputation. The other is similar to the work of Machado Mata (2005) and Melly(2005). Where you invert the whole distribution “globally”"
  },
  {
    "objectID": "adv_class/05uqreg.html#example-1-hetregress",
    "href": "adv_class/05uqreg.html#example-1-hetregress",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Example #1: Hetregress",
    "text": "Example #1: Hetregress\n** Example for OPT2\nwebuse dui, clear\n** Modeling OLS with heteroskedastic errors\n    hetregress citations fines i.csize college taxes ,  het(fines i.csize college taxes )\n    \n    \n------------------------------------------------------------------------------\n   citations | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\ncitations    |\n       fines |   -6.18443   .3018298   -20.49   0.000    -6.776006   -5.592855\n             |\n       csize |\n     Medium  |   4.683941   .5028377     9.32   0.000     3.698397    5.669484\n      Large  |   9.655742   .5261904    18.35   0.000     8.624428    10.68706\n             |\n     college |   4.495635   .5283579     8.51   0.000     3.460072    5.531197\n       taxes |  -3.640864   .4938209    -7.37   0.000    -4.608735   -2.672993\n       _cons |   79.48011   3.118008    25.49   0.000     73.36892    85.59129\n-------------+----------------------------------------------------------------\nlnsigma2     |\n       fines |  -.5261208    .082495    -6.38   0.000     -.687808   -.3644337\n             |\n       csize |\n     Medium  |    .331204   .1681709     1.97   0.049     .0015952    .6608129\n      Large  |   .5578834   .1662309     3.36   0.001     .2320768    .8836899\n             |\n     college |   .3186815   .1539424     2.07   0.038     .0169599    .6204032\n       taxes |  -.3988692   .1437708    -2.77   0.006    -.6806548   -.1170836\n       _cons |   8.257714   .8201063    10.07   0.000     6.650335    9.865093\n------------------------------------------------------------------------------\nLR test of lnsigma2=0: chi2(5) = 75.42                    Prob &gt; chi2 = 0.0000\n\n\n**  make Policy\nclonevar fines_copy = fines\nreplace fines = 0.1*(12-fines)+fines \n\npredict xb, xb\npredict xbs, sigma\n\n** Simulate results\ncapture program drop sim1\nprogram sim1, eclass\n    capture drop cit_hat \n    gen cit_hat = rnormal(xb,xbs)   \n    qui:sum citations, d \n    local lp10 = r(p10)\n    local lp25 = r(p25)\n    local lp50 = r(p50) \n    local lpmn = r(mean)\n    local lp75 = r(p75)\n    local lp90 = r(p90)\n    qui:sum cit_hat, d \n    matrix b = r(p10)-`lp10',r(p25)-`lp25', r(p50)-`lp50' , r(mean) -`lpmn',r(p75)-`lp75',r(p90)-`lp90'\n    matrix colname b = p10 p25 p50 mean p75 p90\n    ereturn post b\nend\n\nsimulate, reps(1000): sim1\nsum\n\n-------------+---------------------------------------------------------\n      _b_p10 |      1,000    -1.08147    .3913698   -2.31713   .1689796\n      _b_p25 |      1,000   -.3262908    .3230118  -1.817808   .6465259\n      _b_p50 |      1,000   -.2085465     .316455   -1.09237   .7785921\n     _b_mean |      1,000   -1.675626    .2234377  -2.400322   -1.03909\n      _b_p75 |      1,000   -1.541725    .4210822  -2.857586  -.2505198\n-------------+---------------------------------------------------------\n      _b_p90 |      1,000   -3.543298    .6079578  -5.464802  -1.682991\nEffects larger than Reweigthing. Statistical inference here may be flawed. (first stage error not carried over)"
  },
  {
    "objectID": "adv_class/05uqreg.html#example-2-qregress",
    "href": "adv_class/05uqreg.html#example-2-qregress",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Example #2: Qregress",
    "text": "Example #2: Qregress\nwebuse dui, clear\ngen id = _n\n** Expand to 99 quantiles\nexpand 99 \nbysor id:gen q=_n\n** make policy\ngen fines_policy=0.1*(12-fines)+fines \ngen fines_copy  =fines \n** Estimate 99 quantiles (in theory one should do more..but choose at random)\nssc install qrprocess // Faster than qreg\n** Save Cit hat (prediction)\n** cit policy (with policy)\ngen cit_hat=.\ngen cit_pol=.\n\nforvalues  i = 1 / 99 {\n    if `i'==1   _dots 0 0\n    _dots `i' 0\n    qui {\n        local i100=`i'/100\n        capture drop aux\n        qrprocess citations c.fines##c.fines  (i.csize college taxes) if q==1, q(`i100')\n        ** predicts the values as if they were in q100\n        predict aux\n        replace cit_hat=aux if q==`i'\n        drop aux\n        replace fines = fines_policy\n        predict aux\n        replace cit_pol=aux if q==`i'   \n        replace fines = fines_copy \n    }   \n}\n\n tabstat citations cit_hat cit_pol, stats(p10 p25 p50 mean p75 p90)\n \n   Stats |  citati~s   cit_hat   cit_pol\n---------+------------------------------\n     p10 |      11.5  10.70744  9.911633\n     p25 |        15  15.42857  14.27302\n     p50 |        20  21.15557  19.68303\n    Mean |    22.018   22.0002  20.31824\n     p75 |        27  27.65936  25.56173\n     p90 |      34.5  34.03413  31.39192\n----------------------------------------\nVery demanding (computationally) and may only capture effects to the extend that we have good coverage of the distribution.\nStandard Errors…Bootstrapping. Perhaps use random quantile assignment, and may have problems near boundaries."
  },
  {
    "objectID": "adv_class/05uqreg.html#opt-1-and-2-comments",
    "href": "adv_class/05uqreg.html#opt-1-and-2-comments",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Opt 1 and 2: Comments",
    "text": "Opt 1 and 2: Comments\nThe first two options allow you to estimate effects of changes in \\(f(x)\\) on the unconditional distribution of \\(y\\), and in consequence, the distributional statistics of interest.\nThey have limitations:\n\nThey both are limited to a single experiment. A different policy requires a change in the setup.\nReweighing is simple to apply, but has limitation on the type of policies. They all need to be within the support of \\(X\\)\nModeling the conditional distribution is a more direct approach, but more computationally intensive, specially for obtaining Stand errors."
  },
  {
    "objectID": "adv_class/05uqreg.html#opt-3.-local-approximation-rif-regression",
    "href": "adv_class/05uqreg.html#opt-3.-local-approximation-rif-regression",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Opt 3. Local Approximation: RIF regression",
    "text": "Opt 3. Local Approximation: RIF regression\nThe third approach was first introduced by Firpo, Fortin and Lemieux 2009, as a computationally simple way to analyze how changes in \\(X's\\) affect the unconditional quantiles of \\(y\\).\nThis strategy was later extended to analyze the effects on a myriad of distributional statistics and rank dependent indices. as well as an approach to estimate distributional treatment effects. See Rios-Avila (2020).\nIn contrast with other approaches, it can be used to analyze multiple types of policies without re-estimating the model. However the identification and interpretation needs particular attention.\nIt also allows you to easily make Statistical inference. (except for quantiles…)"
  },
  {
    "objectID": "adv_class/05uqreg.html#opt-3.-from-ground-up",
    "href": "adv_class/05uqreg.html#opt-3.-from-ground-up",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Opt 3. From ground up",
    "text": "Opt 3. From ground up\nReconsider the Original question. How do you capture the effect of changes of distribution of \\(x\\) on the distribution of \\(y\\).\n\\[\n\\Delta v=v(G_y) - v(F_y)\n\\]\nNow, assume that \\(G_y\\) is just marginally different from \\(F_y\\) (different in a very particular way)\n\\[\nG_y(y_i) = (1-\\epsilon)F_y+ \\epsilon 1(y&gt;y_i)\n\\]\nThis function puts just a bit more weight on observation \\(y_i\\). Think of it as “dropping” a new person in the pool.\nIf this is the case, the \\(\\Delta v(y_i)\\) Captures how would the Statistic \\(v\\) changes if the distribution puts just a bit extra weight on 1 observation. (this would be very small)"
  },
  {
    "objectID": "adv_class/05uqreg.html#opt-3.-one-more-thing",
    "href": "adv_class/05uqreg.html#opt-3.-one-more-thing",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Opt 3. One more thing",
    "text": "Opt 3. One more thing\nLets Rescale it:\n\\[\nIF(v,F_y,y_i) =lim_{\\epsilon \\rightarrow 0} \\frac{v(G_y(y_i))-v(F_y)}{\\epsilon}\n\\]\nThe influence function is a measure of direction of change, we should expect the statistic \\(v\\) will have as we change \\(F_y \\rightarrow G_y\\) .\nFrom here the RIF is just \\(RIF(v,F_y,y_i) = v + IF(v,F_y,y_i)\\)\nWhich has some properties:\n\\[\n\\begin{aligned}\n\\int IF(v,F_y,y_i)f_ydy=0 &;\n\\int RIF(v,F_y,y_i)f_y dy=v \\\\\nv(F_y) \\sim N \\left(v(F_y),\\frac{\\sigma^2_{IF}}{N} \\right) &;\n\\int IF^2f_ydy =\\sigma^2_{IF}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/05uqreg.html#opt-3.-rif-regression",
    "href": "adv_class/05uqreg.html#opt-3.-rif-regression",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Opt 3. RIF Regression",
    "text": "Opt 3. RIF Regression\nFirst:\n\\[\nv(F_y) = \\iint RIF(v,F_Y,y_i) f(y|x)f(x)dy = \\int E(RIF(.)|x) f(x)\n\\]\nFrom here is similar to Opt 3. Use some econometric model to estimate \\(E(RIF(.)|X)\\), and use that to make predictions on how \\(v(F_y)\\) would change, when there is a distributional change in \\(X\\).\nRIF-OLS: Unconditional effect!\n\\[\nRIF(v,F_y,y_i) = X\\beta+e  \\ \\rightarrow\\\nE(RIF) = v(F_y) = \\bar X \\beta \\\\\n\\frac{dv(F_y)}{d\\bar X}=\\beta\n\\]\nLogic. When \\(F_x\\) changes, it will change the distribution of \\(F_y\\), which will affect how the statistic \\(v\\) will change. But, we can only consider changes in means! (and Var)"
  },
  {
    "objectID": "adv_class/05uqreg.html#why-it-works-and-why-it-may-not",
    "href": "adv_class/05uqreg.html#why-it-works-and-why-it-may-not",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Why it works, and why it may not",
    "text": "Why it works, and why it may not\nRIF regressions works by using a linear approximation of the statistic \\(v\\) with the changes in \\(F_y\\) which are caused by changes in \\(F_x\\), proxied by changes in \\(\\bar X\\).\n\nChanges at the individual \\(x_i\\) are not interesting (in a population of 1million, what happens to person 99 may not be large enough to matter)\n\nDepending on the model specification, however, we may only be able to identify changes in first and second moments of the distribution of \\(x\\). (Mean and variance).\n-\nHowever, as any linear approximation to a non-linear function, the approximations are BAD when the changes in \\(F_x\\) are too large. The most relevant example…Dummies and treatment!"
  },
  {
    "objectID": "adv_class/05uqreg.html#rif-reg-and-dummies",
    "href": "adv_class/05uqreg.html#rif-reg-and-dummies",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "RIF-Reg and dummies",
    "text": "RIF-Reg and dummies\nDummies are a challenge. At individual or conditional level, we usually consider changes from 0 to 1 (off or on).\n\nFor unconditional effects this is not correct (too large of a change) (No-one treated vs All treated). Thus you need to change the question…Not on and off changes, but Changes in proportion of treated!\n\nVery important. a 1% increase in pop treated is different if current treatment is 10% or 90%.\n\nHowever, its possible to restructure RIF regressions to be partially conditional (Rios-Avila and Maroto 2023) (Combines CQREG with UQREG)\nSimilar problems are experienced if the change in continuous variables is large!\n\nMinor point. How do you construct RIFs? (analytically and Empirically)"
  },
  {
    "objectID": "adv_class/05uqreg.html#example-1",
    "href": "adv_class/05uqreg.html#example-1",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Example",
    "text": "Example\nwebuse dui, clear\n**  Consider the policy change\ngen change_fines= 0.1*(12-fines)\n**  consider average change in fines.Since we are only considering this effect\nsum change_fines\n\nrifhdreg citations fines i.csize college taxes, rif(q(10)) \nest sto m1\nrifhdreg citations fines i.csize college taxes, rif(q(50)) \nest sto m2\nrifhdreg citations fines i.csize college taxes, rif(q(90)) \nest sto m3\n** This are Rescaled to show true effect\nrifhdreg citations fines i.csize college taxes, rif(q(10)) scale(.21048)\nest sto m4\nrifhdreg citations fines i.csize college taxes, rif(q(50)) scale(.21048)\nest sto m5\nrifhdreg citations fines i.csize collegetaxes, rif(q(90)) scale(.21048)\nest sto m6\n\n. esttab m1 m2 m3 m4 m5 m6, se mtitle(q10 q50 q90 r-q10 r-q50 r-q90) compress nogaps\n\n----------------------------------------------------------------------------------------\n                 (1)          (2)          (3)          (4)          (5)          (6)   \n                 q10          q50          q90        r-q10        r-q50        r-q90   \n----------------------------------------------------------------------------------------\nfines         -4.476***    -6.700***    -9.887***    -0.942***    -1.410***    -2.081***\n             (0.491)      (0.493)      (0.978)      (0.103)      (0.104)      (0.206)   \n1.csize            0            0            0            0            0            0   \n                 (.)          (.)          (.)          (.)          (.)          (.)   \n2.csize        4.603***     7.325***     6.370***     0.969***     1.542***     1.341***\n             (0.963)      (0.966)      (1.917)      (0.203)      (0.203)      (0.404)   \n3.csize        6.504***     13.54***     12.97***     1.369***     2.851***     2.729***\n             (0.914)      (0.917)      (1.820)      (0.192)      (0.193)      (0.383)   \ncollege        2.922**      5.948***     9.973***     0.615**      1.252***     2.099***\n             (0.890)      (0.892)      (1.771)      (0.187)      (0.188)      (0.373)   \ntaxes         -3.279***    -3.303***    -8.319***    -0.690***    -0.695***    -1.751***\n             (0.842)      (0.844)      (1.676)      (0.177)      (0.178)      (0.353)   \n_cons          53.71***     81.04***     129.2***     11.30***     17.06***     27.20***\n             (4.964)      (4.977)      (9.880)      (1.045)      (1.048)      (2.080)   \n----------------------------------------------------------------------------------------\nN                500          500          500          500          500          500   \n----------------------------------------------------------------------------------------\nConsider the basic change. Fines increases in 1 unit, Cntys with taxes, increase 10%, etc\nOr consider rescaled effects"
  },
  {
    "objectID": "adv_class/05uqreg.html#how-do-they-compare",
    "href": "adv_class/05uqreg.html#how-do-they-compare",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "How Do they Compare",
    "text": "How Do they Compare"
  },
  {
    "objectID": "adv_class/05uqreg.html#other-considerations",
    "href": "adv_class/05uqreg.html#other-considerations",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Other Considerations",
    "text": "Other Considerations\nRIF Regressions are useful, but again, one must use them with care.\n\nOnly Small changes! Larger changes may be meaningless\n\nExcept for Stata (see rif and rifhdreg), the applications of RIF regressions outside Mean, Variance and Quantiles are non-existent. (paper?)\n\nFor most Common Statistics, RIF’s automatically provide correct Standard errors (which can be Robustized!). In fact, a simple LR can be considered as a special case of RIF’s\n\n\\[\n\\begin{aligned}\nRIF(mean,y_i,F_y) &= y_i \\\\\nRIF(variance,y_i,F_y) &= (y_i-\\bar y)^2 \\\\\nRIF(Q,y_i,F_Y) &= Q_y(\\tau) + \\frac{\\tau-1(y_i \\leq Q_y(\\tau))}{f_Y(y_i)}\n\\end{aligned}\n\\]\nExcept for quantile related functions! (\\(f_y\\) also needs estimation, thus errors!)\n\nAccounting for “local” unconditional effects beyond means require Center Polynomials:\n\n\\[\nRIF(.,y) = b_0 + b_1 x + b_2 (x-\\bar x)^2+\\varepsilon\n\\]\n\nQuantile treatment effects (on and off) are possible using PC-RIF (When you condition the distribution on just 1 variable)\n\n\\[\nRIF(.,F_{Y|D},y) = b_0 + b_1 D+b_2 x + b_3 (x-\\bar x)^2+\\varepsilon\n\\]"
  },
  {
    "objectID": "adv_class/05uqreg.html#final-words-on-rif",
    "href": "adv_class/05uqreg.html#final-words-on-rif",
    "title": "Unconditional Quantile Regressions (and RIF’s)",
    "section": "Final words on RIF",
    "text": "Final words on RIF\nBecause this implementation uses LR, you can add Multiple Fixed effects as well. (with limitations)\nAnd you can skip LR all together, and model RIF using Other approaches! (which may be even better than OLS)."
  },
  {
    "objectID": "adv_class/07po_ci.html#introduction-what-if",
    "href": "adv_class/07po_ci.html#introduction-what-if",
    "title": "Potential outcomes and Causal Models",
    "section": "Introduction: What if?",
    "text": "Introduction: What if?\nFrom here on, the whole purpose of the methodologies we will dicusess is the analysis of causal effects of some:\n\nPolicy, treatment, experiment, or otherwise event\n\nBut, How is it different from what we did before?\nIt isn’t.\nUnder Exogeneity assumption \\(E(e|X)=0\\), one can make causal effect claims.\n\n\n\n\n\n\nWe seek the truth\n\n\nHow much of the change in outcome is caused by the program alone?\n\n\n\nBut this is not as easy."
  },
  {
    "objectID": "adv_class/07po_ci.html#exampleswhere-is-the-challenge",
    "href": "adv_class/07po_ci.html#exampleswhere-is-the-challenge",
    "title": "Potential outcomes and Causal Models",
    "section": "Examples…Where is the challenge?",
    "text": "Examples…Where is the challenge?\nA few examples for Causal effect questions:\n\nDo minimum wages increase unemployment ?\nDo Conditional cash transfers improve health outcomes in children?\nDo Covid Vaccines help reduce the Spread of Covid?\n\nThese questions are, however, difficult to answer.\n\nHow do you make sure the “treatment” is the Only factor that explains the difference in outcome across groups??\n\nTo do this we need strategies that rule out any other explanation that could “take away” the connection we seek.\n\nWe need to close all back doors, block all alternative explanations, or nuisanse factors"
  },
  {
    "objectID": "adv_class/07po_ci.html#potential-outcomes",
    "href": "adv_class/07po_ci.html#potential-outcomes",
    "title": "Potential outcomes and Causal Models",
    "section": "Potential Outcomes",
    "text": "Potential Outcomes\n\nChoice"
  },
  {
    "objectID": "adv_class/07po_ci.html#the-path-not-taken",
    "href": "adv_class/07po_ci.html#the-path-not-taken",
    "title": "Potential outcomes and Causal Models",
    "section": "The Path not taken",
    "text": "The Path not taken\nThe way we express the \\(TE_i\\), compares counterfactuals. Two possible States of the world, where an allknowing researcher can perfectly identify TE.\nUnfortunately, the same person cannot take both paths, and we cannot see both options. A person is either Treated or Untreated. Thus, the first approach to Casual effects is impossible.\n…\nBut it does provide us with a clue of how to go ahead and analyze Causal effects. We “simply” need to estimate the counterfactual!\nBut before going deeper into how to estimate the counterfactuals And treatment effects some notation"
  },
  {
    "objectID": "adv_class/07po_ci.html#potential-outcomes-notation",
    "href": "adv_class/07po_ci.html#potential-outcomes-notation",
    "title": "Potential outcomes and Causal Models",
    "section": "Potential Outcomes: Notation",
    "text": "Potential Outcomes: Notation\n\n\\(i\\) will represent a unit. Person, city, country, school, classroom, etc\n\\(D\\) will indicate the treatment Status of a unit. \\(D=1\\) means is treated, and \\(D=0\\) is untreated.\nEach unit has two potential outcomes \\(Y_i(D=1)\\) and \\(Y_i(D=0)\\)\nAll units have only one effective or realized outcome: \\(Y_i\\), which is what we observe, and depends on their treatment status:\n\n\\[Y_i=(1-D_i)*Y_i(0)+D_i*Y_i(1)\\]\n\nUnit Specific causal effect is the difference between potential outcomes: \\[\\delta_i = Y_i(1)-Y_i(0)\\]\n\\(\\pi\\) is the proportion of treated units"
  },
  {
    "objectID": "adv_class/07po_ci.html#parameters-of-interest",
    "href": "adv_class/07po_ci.html#parameters-of-interest",
    "title": "Potential outcomes and Causal Models",
    "section": "Parameters of Interest",
    "text": "Parameters of Interest\nAssume we can see the true USCE (unit specific casual effect) for all units. In addition to their Treatment Status.\nThere are three parameters one might be interested in analyzing:\n\\[\n\\begin{aligned}\nATE &= E(\\delta_i) \\\\\nATT &= E(\\delta_i|D_i=1) \\\\\nATU &= E(\\delta_i|D_i=0)\n\\end{aligned}\n\\]\nIn general, this three estimands may tell very different stories.\nLets Put some numbers here"
  },
  {
    "objectID": "adv_class/07po_ci.html#simulating-effects-stata",
    "href": "adv_class/07po_ci.html#simulating-effects-stata",
    "title": "Potential outcomes and Causal Models",
    "section": "Simulating effects Stata",
    "text": "Simulating effects Stata\n\n\nCode\nclear\nset linesize 255\nset seed 101\nset obs 1000\ngen y0 = rnormal(5)\ngen t  = rnormal(0.0,0.5)\ngen y1 = y0+t\n** Assume only those with t&gt;0 take treatment\ngen trt =(t&gt;0)\ngen y = y0 * (1-trt) +  y1 * (trt)\nformat y0 y1 y t %4.3f \nlist in 1/10, sep(0) clean\n** For Everyone 100 obs\ntabstat t, by(trt)\n\n\n\n\n\nNumber of observations (_N) was 0, now 1,000.\n\n          y0        t      y1   trt       y  \n  1.   5.254   -0.801   4.453     0   5.254  \n  2.   4.997    0.039   5.035     1   5.035  \n  3.   2.608   -0.490   2.118     0   2.608  \n  4.   6.280   -0.579   5.700     0   6.280  \n  5.   5.761    0.090   5.850     1   5.850  \n  6.   6.132    0.211   6.342     1   6.342  \n  7.   4.928    0.153   5.081     1   5.081  \n  8.   4.377   -0.073   4.304     0   4.377  \n  9.   3.142    0.427   3.569     1   3.569  \n 10.   6.050   -0.332   5.718     0   6.050  \n\nSummary for variables: t\nGroup variable: trt \n\n     trt |      Mean\n---------+----------\n       0 | -.3984745\n       1 |  .3748617\n---------+----------\n   Total | -.0187664\n--------------------"
  },
  {
    "objectID": "adv_class/07po_ci.html#but-there-is-only-1",
    "href": "adv_class/07po_ci.html#but-there-is-only-1",
    "title": "Potential outcomes and Causal Models",
    "section": "But there is only 1",
    "text": "But there is only 1\nBut, we never see potential outcomes, nor unit specific effects.\nThe most naive estimator is to just estimate the mean difference in “post-treatment” outcome after treatment was in place. But that would be very biased!\n\n\nCode\ngen yy0 = y if trt==0\ngen yy1 = y if trt==1\nlist y yy1 yy0 trt in 1/10\nreg  y trt\n\n\n(491 missing values generated)\n(509 missing values generated)\n\n     +-----------------------------------+\n     |     y        yy1        yy0   trt |\n     |-----------------------------------|\n  1. | 5.254          .   5.254051     0 |\n  2. | 5.035   5.035442          .     1 |\n  3. | 2.608          .   2.608155     0 |\n  4. | 6.280          .    6.27964     0 |\n  5. | 5.850   5.850478          .     1 |\n     |-----------------------------------|\n  6. | 6.342    6.34237          .     1 |\n  7. | 5.081   5.080995          .     1 |\n  8. | 4.377          .   4.376629     0 |\n  9. | 3.569   3.568727          .     1 |\n 10. | 6.050          .   6.049989     0 |\n     +-----------------------------------+\n\n      Source |       SS           df       MS      Number of obs   =     1,000\n-------------+----------------------------------   F(1, 998)       =     49.49\n       Model |  49.4759442         1  49.4759442   Prob &gt; F        =    0.0000\n    Residual |  997.714103       998   .99971353   R-squared       =    0.0472\n-------------+----------------------------------   Adj R-squared   =    0.0463\n       Total |  1047.19005       999  1.04823829   Root MSE        =    .99986\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         trt |   .4449359   .0632467     7.03   0.000      .320824    .5690477\n       _cons |   4.988793   .0443179   112.57   0.000     4.901826     5.07576\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/07po_ci.html#bias-direction",
    "href": "adv_class/07po_ci.html#bias-direction",
    "title": "Potential outcomes and Causal Models",
    "section": "Bias Direction",
    "text": "Bias Direction\n\\[\n\\begin{aligned}\nE(Y_i|D_i=1)-E(Y_i|D_i=0) &= ATE  \\\\\n&  +E(Y(0)|D=1)-E(Y(0)|D=0) \\\\\n& +(1-\\pi)(ATT-ATU)\n\\end{aligned}\n\\]\nIntuition: Simple Difference will be biases because\n\nThere could be a selection bias (one group baseline outcome is different from the other)\nTreatment Heterogeneity. Some groups are affected differently from others\n\nFrom these two problems, the second one is easier to handle (either concentrate on ATT or ATU).\nThe first one, however, requires using strategies to be able to account for selection bias."
  },
  {
    "objectID": "adv_class/07po_ci.html#independence-assumption",
    "href": "adv_class/07po_ci.html#independence-assumption",
    "title": "Potential outcomes and Causal Models",
    "section": "Independence assumption",
    "text": "Independence assumption\nWhile the Simple mean estimator is most likely to be biased, under Independence assumption, it may still work:\n\\[\nY(1),Y(0) \\perp D\n\\]\nThis means that Treatment Status should NOT depend on the potential outcomes.\nIn other words, there shouldnt be any differneces in the potential outcomes before or after treatment takes place.\nThis eliminates the selection bias. And group Heterogeneity.\n\\[\n\\begin{aligned}\nATT - ATU &= E(Y|D=1) -  E(Y(0)|D=1) \\\\\n&- E(Y(1)|D=0) -  E(Y|D=0)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "adv_class/07po_ci.html#sutva",
    "href": "adv_class/07po_ci.html#sutva",
    "title": "Potential outcomes and Causal Models",
    "section": "SUTVA",
    "text": "SUTVA\nStable Unit Treatment value assumption\nThis is a strong assumption that is still required to estimate of treatment effects.\nIt assumes that:\n\nTreatment is Homogenous. Same intensity, quality, type of treatment among all treated.\nThere are no spill overs. Your Treatment Status effects you and you only, and you are only affected by your treatment status. No externalities nor Spillovers.\nAlso, there are no general equibrium effects\nAnd NO Anticipation.\n\nThis assumptions namely guaranties that when a unit is not treated, its/his/her outcome will not change."
  },
  {
    "objectID": "adv_class/07po_ci.html#narrowing-down-the-problem",
    "href": "adv_class/07po_ci.html#narrowing-down-the-problem",
    "title": "Potential outcomes and Causal Models",
    "section": "Narrowing down the problem",
    "text": "Narrowing down the problem\n\nIndividual level effects are impossible to identify. We only observe one outcome at a time. (not both)\nIt is possible to identify causal effects on groups (treated, not-treated, kind-of-treated). But..\nSimple Mean difference will not identify causal effects, unless Independence and Sutva assumptions hold\n\nThis however, suggests a path. Constructing good counterfactuals can help idenfiying the Causal effects.\nGoal:\n\nIdentify a control/comparison group that is statistically identical to the treated group, except for the Treatment Status"
  },
  {
    "objectID": "adv_class/07po_ci.html#the-gold-standard-randomized-control-trials",
    "href": "adv_class/07po_ci.html#the-gold-standard-randomized-control-trials",
    "title": "Potential outcomes and Causal Models",
    "section": "The Gold Standard: Randomized Control Trials",
    "text": "The Gold Standard: Randomized Control Trials\n\n\n\n\n\n\nTo keep in mind\n\n\nSearching for good controls doesnt require having access to perfect “clones”. However, in average, we need groups (T vs UT) that are very similar to each other.\n\n\n\nIn general, research designed is guided by the rules of program or treatment assignment on participants.\nWhen researchers have control on the assingment rules, the best approach is to design a randomized control trial.\nIn an RCT, randomized assigment, eliminates any selection-bias problems (although SUTVA remains as an assumption)"
  },
  {
    "objectID": "adv_class/07po_ci.html#rct-and-selection-problems",
    "href": "adv_class/07po_ci.html#rct-and-selection-problems",
    "title": "Potential outcomes and Causal Models",
    "section": "RCT and Selection Problems",
    "text": "RCT and Selection Problems\nConsider the example of the Health Impacts of Hospitals.\n\nHospitals (or health care) should improve health of individuals. but\nOnly unhealthy people will use Health care services. Selection bias\nThus It may look that Hospitals Hurt people’s health becuase those who used it have lower health than those who dont: \\[\nE(Y|D=1)-E(Y|D=0) = ATT + E(Y(0)|D=1)-E(Y(0)|D=0)\n\\]\n\nSo even if Health services help those in need (ATT). If the selection bias is large, Naive estimations may suggest Hospitals are harmful.\nThis is a problem caused because Treatment(going to the Hospital) is affected by pre-conditions or potential treatment outcomes."
  },
  {
    "objectID": "adv_class/07po_ci.html#stata-example",
    "href": "adv_class/07po_ci.html#stata-example",
    "title": "Potential outcomes and Causal Models",
    "section": "Stata Example",
    "text": "Stata Example\n\n\nCode\ngen trt2=rnormal()&gt;0\ngen yrct = y0 * (1-trt2) +  y1 * (trt2)\ntabstat t y0 y1 yrct, by(trt2)\nreg yrct trt2\n\n\n\nSummary statistics: Mean\nGroup variable: trt2 \n\n    trt2 |         t        y0        y1      yrct\n---------+----------------------------------------\n       0 | -.0097507  5.061626  5.051875  5.061626\n       1 |  -.027891  4.984309  4.956418  4.956418\n---------+----------------------------------------\n   Total | -.0187664  5.023199  5.004433  5.009338\n--------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     1,000\n-------------+----------------------------------   F(1, 998)       =      2.64\n       Model |  2.76706003         1  2.76706003   Prob &gt; F        =    0.1046\n    Residual |  1046.22695       998   1.0483236   R-squared       =    0.0026\n-------------+----------------------------------   Adj R-squared   =    0.0016\n       Total |  1048.99401       999  1.05004406   Root MSE        =    1.0239\n\n------------------------------------------------------------------------------\n        yrct | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        trt2 |  -.1052076   .0647568    -1.62   0.105    -.2322827    .0218675\n       _cons |   5.061626   .0456524   110.87   0.000      4.97204    5.151212\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "adv_class/07po_ci.html#internal-vs-external-validity",
    "href": "adv_class/07po_ci.html#internal-vs-external-validity",
    "title": "Potential outcomes and Causal Models",
    "section": "Internal vs External Validity",
    "text": "Internal vs External Validity\nRCTs are not the only strategy that allows you to control the Rules of Treatment. In Experimental Economics this is done quite often\n\nyou select your sample (of students), and randomly assigned treatments of interest (Experimental design).\n\nThere are, however, further considerations to be taken:\n\n\n\n\n\n\nInternal Validity\n\n\nThe estimated Impact is net of all other confunding factors (Random assigment)\n\n\n\n\n\n\n\n\n\nExternal Validity\n\n\nThe estimated impact can be generalized to the population in general. (Random Sample)\n\n\n\n\n\n\n\n\n\nHow to Randomize\n\n\nDepends mostly on how reasonable is to mantain SUTVA assumption"
  },
  {
    "objectID": "adv_class/07po_ci.html#examples",
    "href": "adv_class/07po_ci.html#examples",
    "title": "Potential outcomes and Causal Models",
    "section": "Examples",
    "text": "Examples\n\n\n\n\n\n\nCCT and Education in Mexico\n\n\nProgresa/Prospera is a CCT program for poor mothers based on children school enrollment to suport education attainment.\nEligibility was based on Census data on Poverty levels and Baseline Data collection. But during phase-in period, only 2/3 if localities were selected to receive the Transfer\n\n\n\n\n\n\n\n\n\nWater and Sanitation Intervention in Bolivia\n\n\nIn 2012 IADB and Bolivian Goverment implemented a random assigment of water/sanitation interventions in Small Rural Communities.\nFrom 369 eliginle communities, 182 were selected at random for the program implementation, via public lotteries, constraining on Community Size"
  },
  {
    "objectID": "adv_class/07po_ci.html#how-to-analyze-the-data",
    "href": "adv_class/07po_ci.html#how-to-analyze-the-data",
    "title": "Potential outcomes and Causal Models",
    "section": "How to Analyze the Data",
    "text": "How to Analyze the Data\n\nVerify Data Balance: Even if treatment was assigned at random, it is important to verify of groups remain comparable. (Thus avoid compossition effects)\nMean Difference: Because of Random Assigment, one could use simple mean differences to estimate ATE\nConsider using controls: Under RA, controls will not affect the outcome, but may improve precision: \\(y_i = \\alpha_0 + \\tau D_i + x_i\\beta + \\varepsilon_i\\).\n\nBut controls should not be affected by the treatment itself (thus should be pre-treatment)\n\nMay consider falsification tests\nAnd Other Robustness test: Outliers, or distributional impacts may be of interest"
  },
  {
    "objectID": "adv_class/07po_ci.html#background",
    "href": "adv_class/07po_ci.html#background",
    "title": "Potential outcomes and Causal Models",
    "section": "Background",
    "text": "Background\nProgresa is a Cash Transfer Program designed to increase School Enrollment among the poor, minimizing desincentives to work.\nThis program provided Grants to families whos children attended school for atleast 85% of the school year, covering between 50/75% of school cost.\nWhile there were 495 localities that were eligible to benefit from the program, only 314 were randomly selected to start reciving resources for the first 2 years. With the unselected localities being treated a couple of years later.\n\nThe program continued beyond the original scope of the policy, now its known as Progresa/Oportunidades"
  },
  {
    "objectID": "adv_class/07po_ci.html#method",
    "href": "adv_class/07po_ci.html#method",
    "title": "Potential outcomes and Causal Models",
    "section": "Method",
    "text": "Method\nGoal. Estimate the impact of Progresa (P) on Enrollment (S) \\[\nS_i = a_0 +a_1 P_i + a_2 E_i + a_3 P_iE_i + \\delta Enrolled_c + \\beta X + e_i\n\\]\n\\(P_i=1\\) if the comunity is eligible\n\\(E_i=1\\) if the child is Poor\n\\(P_i * E_i\\) the impact on Poor Chilren in eligible communities.\nModel can be estimated Separately (5 years), or using pooled data\nThis is a kind of DIDID model. However, we could consider it as a simple mean comparison between those Effectively treated and those untreated."
  },
  {
    "objectID": "adv_class/07po_ci.html#differences-in-characteristics",
    "href": "adv_class/07po_ci.html#differences-in-characteristics",
    "title": "Potential outcomes and Causal Models",
    "section": "Differences in Characteristics",
    "text": "Differences in Characteristics"
  },
  {
    "objectID": "adv_class/07po_ci.html#raw-differences",
    "href": "adv_class/07po_ci.html#raw-differences",
    "title": "Potential outcomes and Causal Models",
    "section": "Raw Differences",
    "text": "Raw Differences\n\nPoor HH"
  },
  {
    "objectID": "adv_class/07po_ci.html#with-controls",
    "href": "adv_class/07po_ci.html#with-controls",
    "title": "Potential outcomes and Causal Models",
    "section": "With Controls",
    "text": "With Controls"
  },
  {
    "objectID": "adv_class/07po_ci.html#other-outcomes",
    "href": "adv_class/07po_ci.html#other-outcomes",
    "title": "Potential outcomes and Causal Models",
    "section": "Other Outcomes",
    "text": "Other Outcomes"
  },
  {
    "objectID": "adv_class/09iv.html#recap-pos-and-rcts",
    "href": "adv_class/09iv.html#recap-pos-and-rcts",
    "title": "Instrumental Variables",
    "section": "Recap: PO’s and RCT’s",
    "text": "Recap: PO’s and RCT’s\n\nQuick Recap. The goal of the methodologies we are covering is to identify treatment effects.\nIn the PO framework, that is done by simply comparing a group with itself, in two different States (treated vs untreated)\nSince this is impossible, the next best solution is using RCT. Individuals are randomized, and assuming every body follows directions, we can identify treatment effects of the experiments.\nBut only if the RCT is well executed! Sometimes even that may fail"
  },
  {
    "objectID": "adv_class/09iv.html#instrumental-variables",
    "href": "adv_class/09iv.html#instrumental-variables",
    "title": "Instrumental Variables",
    "section": "Instrumental Variables",
    "text": "Instrumental Variables\n\nWhile here discussed 3rd, the second best approach to identify Treatment effects is by using Instrumental variables.\nIn fact with a Good-Enough instrument, one should be able to identify ANY causal effect. Assuming such IV exists.\n\nbut how?\n\nIf the instrument is good, it may create an exogenous variation, which will allow us to identify Treatment effects by looking ONLY at those affected by the treatment!\nUsing the external variation, we can Estimate TE comparing two groups who are identical in every aspect, except being expose to the Instrument, because they were exposed to the instrument. The randomization comes Because of the IV!"
  },
  {
    "objectID": "adv_class/09iv.html#cannonical-iv",
    "href": "adv_class/09iv.html#cannonical-iv",
    "title": "Instrumental Variables",
    "section": "Cannonical IV",
    "text": "Cannonical IV\nAs we have mentioned, the estimation of TE require that we identify two groups of individuals with mostly similar (if not identical) characteristics. This include unobserved characteristics.\nIf the latter is not true, we have a problem of confunders or Endogeneity. But why?\nConsider the following diagram\n\n\n\nHere the effect of \\(D\\) on \\(Y\\) is direct, because there is nothing else that would get people confuse why treatment affects outcome\n\n\n\n\n\nflowchart LR\n  e(error) --&gt; Y(Outcome)  \n  D(Treatment) --&gt; Y(Outcome)  \n\n\n\n\n\n\n\n\n\n\nHere the effect of \\(D\\) on \\(Y\\) is not as clear, because there is an additional factor \\(v\\) that affects \\(D\\) and \\(Y\\) (is in the way)\n\n\n\n\n\nflowchart LR\n  e( error ) --&gt; Y(Outcome)  \n  D( Treatment ) --&gt; Y(Outcome)\n  v(unobserved) -.-&gt; D(Treatment)  \n  v(unobserved) -.-&gt; Y(Outcome)"
  },
  {
    "objectID": "adv_class/09iv.html#cannonical-iv-1",
    "href": "adv_class/09iv.html#cannonical-iv-1",
    "title": "Instrumental Variables",
    "section": "Cannonical IV",
    "text": "Cannonical IV\nHere is where a good instrument comes into play.\n\nNot everything in \\(D\\) is affected by \\(v\\). Some may, but some may be trully exogenous. What if we have an instrument that helps you ID this:\n\n\n\n\n\nflowchart LR\ne( error ) --&gt; Y(Outcome)  \n  subgraph Treatment\n    D1(Exog) \n    D2(Endog)\n  end\n  \n  D1( Exog ) --&gt; Y(Outcome)\n  D2( Endog ) --&gt; Y(Outcome)\n  Z(Instrument) --&gt; D1  \n  v(unobserved) -.-&gt; D2\n  \n  v(unobserved) -.-&gt; Y(Outcome)  \n\n\n\n\n\n\nBy Isolating those affected by the Instrument Alone, we do not need to worry about endogeneity anymore."
  },
  {
    "objectID": "adv_class/09iv.html#properties",
    "href": "adv_class/09iv.html#properties",
    "title": "Instrumental Variables",
    "section": "Properties",
    "text": "Properties\nInstrumental variables should have at the very list 2 Properties\n\nThe instrumental variable \\(Z\\) should not be correlated with the model error (Validity).\nBut, it should explain the treatment Itself \\(D\\) (Relevance).\n\nFailure of (1) may reintroduce problems of endogeneity. Faiture of (2) will make the instrument Irrelevant."
  },
  {
    "objectID": "adv_class/09iv.html#how-does-it-work",
    "href": "adv_class/09iv.html#how-does-it-work",
    "title": "Instrumental Variables",
    "section": "How does it work",
    "text": "How does it work\nConsider the following.\n\nPeople who study more, tend to earn higher wages\nPeople with high ability tend to study more.\nPeople with high ability, also earn higher wages.\n\nDoes Studying more generate higher wages?\nInstrument. We create a lottery that provides some people resources to pay for their education. This gives them a chance to study more (regardless of ability). \\[Z \\rightarrow D\\]"
  },
  {
    "objectID": "adv_class/09iv.html#section",
    "href": "adv_class/09iv.html#section",
    "title": "Instrumental Variables",
    "section": "",
    "text": "So, we know the instrument was Random. We can analyze how much outcome increases among those benefited by the Lottery.\n\\[E(W|Z=1)-E(W|Z=0)\\]\n\nThis is often called the reduced form effect.\nIn principle, \\(Z\\) only affects wages because of education. So looking at this differences should be similar to a treatment effect of Lotteries.\nThese are also known as Intention to treatment effect. Which will bias towards zero, because not everyone will effectively make use of the opporunities"
  },
  {
    "objectID": "adv_class/09iv.html#section-1",
    "href": "adv_class/09iv.html#section-1",
    "title": "Instrumental Variables",
    "section": "",
    "text": "In othe words, not everyone will Study more…So we can see if the lotery had that effect.\n\\[E(S|Z=1)-E(S|Z=0)\\]\nThis is the equivalent to the first stage. Where we measure the impact of the “instrument/lottery” on Education (to see, say, relevance)\nFinally, the TE is given by the Ratio of thes two\n\\[TE=\\frac{E(W|Z=1)-E(W|Z=0)}{E(S|Z=1)-E(S|Z=0)}\n\\]\nThis is also known as the Wald Estimator. How much of the changes in wages is due to changes in the “# treated”"
  },
  {
    "objectID": "adv_class/09iv.html#some-commnents",
    "href": "adv_class/09iv.html#some-commnents",
    "title": "Instrumental Variables",
    "section": "Some commnents",
    "text": "Some commnents\n\nThis was an example of a binary instrument, which was assigned at random.\nIn fact, this particular scenario is typical byproduct of “failed” RCTs!\n\nPartially failed RCTs: Not every body selected WAS treated\n\n\nConsider the following:\n\nWe make the RCT above giving bouchers to People so they Study more.\nBut, not everybody uses the bouchers:\n\nSome use them and study more.\nSome decide to not use them.\n\n\nComparing Wages among those who receive will only provide you the “intention to treat” effect. (Reduced form)\nBecause of imperfect compliance we need to “readjust/inflate” our TE estimate."
  },
  {
    "objectID": "adv_class/09iv.html#more-comments",
    "href": "adv_class/09iv.html#more-comments",
    "title": "Instrumental Variables",
    "section": "More Comments",
    "text": "More Comments\n\nIn this scenario the Reduced form and second stage can be estimated by just comparing means, because the treatment was randonmized.\n\nIn other words, something you really want is an instrument that is as good as random.\n\nThe effect we capture is a LOCAL treatment effect (LATE).\n\nHowever, it could be an ATE if:\n\nThe effect is homogenous for everyone.\nThe people affected by the treatment is a representative of the population\n\nIt all boils down to identifying who is or might be affected by the treatment.\nFor now, lets assume effects are Homogenous (So we get ATEs)"
  },
  {
    "objectID": "adv_class/09iv.html#who-are-affected-and-who-are-not",
    "href": "adv_class/09iv.html#who-are-affected-and-who-are-not",
    "title": "Instrumental Variables",
    "section": "Who Are Affected and who are not?",
    "text": "Who Are Affected and who are not?\nEven if we are able to identify ATEs, its important to understand who can be affected by the instrument, because the population is generally selected in 3 groups\n\nnever takers & always takers: These are the individuals who would have never done anything different than their normal.\n\nPerhaps their likelihood was already too low (or high) to be affected.\n\nCompliers: These are the ones who, given they receive “instrument”, they comply and follow up. We use their variation for analysis.\nDefiers: These are the ones who, given Z, will do the oposite. We cannot differentiate them from Compliers, so they will affect how treatment is estimated.\n\nWe do not want to have defiers!"
  },
  {
    "objectID": "adv_class/09iv.html#extension-1-continuous-instrument",
    "href": "adv_class/09iv.html#extension-1-continuous-instrument",
    "title": "Instrumental Variables",
    "section": "Extension 1: Continuous Instrument",
    "text": "Extension 1: Continuous Instrument\nThe Wald estimator is for the simplest case of binary treatment. However, if the treatment is continuous, one could modify the IV estimator as follows:\n\\[\n\\delta_{IV} = \\frac{cov(y,z)}{cov(d,z)}\n\\]\nThe logic remains. We are trying to see how variation in the outcome related to Z reates to changes in treatment because of Z.\nThe treatment here is very small (Small changes in d). The intuition is that we are averaging the variation in the outcome across all Zs to estimate the effect."
  },
  {
    "objectID": "adv_class/09iv.html#extension-2-controls",
    "href": "adv_class/09iv.html#extension-2-controls",
    "title": "Instrumental Variables",
    "section": "Extension 2: Controls",
    "text": "Extension 2: Controls\nAdding controls to the model is also straight forward, and you have quite a few options for it\n\nAdding exogenous controls may help improving model precision, even if instrument was randomized. The easiest way to do this is by applying the 2sls procedure (among others)\n\n\\[\n\\begin{aligned}\n1st: d = z\\gamma_z + x\\gamma_x + e_1  \\\\\n2nd: y = x\\beta_x + \\delta \\hat d+ e_2\n\\end{aligned}\n\\]\n\nThe 1st stage “randomizes” instrument to measure the effect on treatment.\nThe 2nd stage uses predicted values of the first to see what the impact on the outcome will be.\nThis works because \\(\\hat d\\) is exogenous, “carrying over” exogenous changes in the treatment."
  },
  {
    "objectID": "adv_class/09iv.html#extension-2-controls-1",
    "href": "adv_class/09iv.html#extension-2-controls-1",
    "title": "Instrumental Variables",
    "section": "Extension 2: Controls",
    "text": "Extension 2: Controls\nOne can also think of the approach as a pseudo Wald Estimator, with continuous variables:\n\\[\n\\begin{aligned}\n1st: d &= \\gamma_z * z + x\\gamma_x + e_1  \\\\\nrd:  y &= \\beta_z  * z+ x\\beta_x + e_2 \\\\\nATE &=\\frac{\\beta_z}{\\gamma_z}=\\frac{cov(y,\\tilde z)}{cov(d,\\tilde z)}\n\\end{aligned}\n\\]\nThis compares average changes in the outcome to average changes in the treatment."
  },
  {
    "objectID": "adv_class/09iv.html#extension-3-multiple-endogenous-variables",
    "href": "adv_class/09iv.html#extension-3-multiple-endogenous-variables",
    "title": "Instrumental Variables",
    "section": "Extension 3: Multiple Endogenous Variables",
    "text": "Extension 3: Multiple Endogenous Variables\nAlthough less common in Causal Analysis perspective, in other frameworks one may to consider more than 1 instrument or using instrument interactions. In these cases one still has two alternatives\n\n2SLS: One can model more than one endogenous variable at the same time, simply substituting the predicted values in the main regression. When using interactions, or polynomials, each will need its own first stage regression.\nControl function approach. In contrast with the “prediction subtstitution” approach, this method suggests using a “residual inclusion approach”. This controls for endogeneity directly. If there is only one endogenous variable (with interactions of polynomials) only one model is needed.\n\nIn the first case, you need at least 1 instrument per regression. Even if its just a transformation of the original variable\nIn the second case, you need at least 1 instrument per endogenous variable."
  },
  {
    "objectID": "adv_class/09iv.html#instrument-validity",
    "href": "adv_class/09iv.html#instrument-validity",
    "title": "Instrumental Variables",
    "section": "Instrument Validity",
    "text": "Instrument Validity\nAs mentioned earlier, Instruments require to fullfill two conditions:\n\nRelevant. They need to be Strongly related to the endogenous variable\nExogenous. instruments should not and cannot be endogenous. In fact, you want instruments that are as good as random, thus not defined by the “system” in anyway."
  },
  {
    "objectID": "adv_class/09iv.html#iv-validity-exogeneity",
    "href": "adv_class/09iv.html#iv-validity-exogeneity",
    "title": "Instrumental Variables",
    "section": "IV Validity: Exogeneity",
    "text": "IV Validity: Exogeneity\nUnfortunately, for most cases, this assumption is not testable, because we do not observe the model unobservables, thus dont know if \\(z\\) is related to those unobserved components.\nWhile most efforts for these are done through model design, or argumentation, there are at least 2 options to verify the exogeneity\n\nIf truly exogenous, the instrument should be as good as random. Thus controls shouldnt be affected by the instrument. (Balance test)\nOtherwise, one could test for exogeneity only by comparing Estimates across different IV’s. Different results may suggest instruments are invalid.\n\nRun a regression of Residuals from the main model against all exogenous variables plus other instruments.\n\n\nNote: Unless the instrument was randomized, assumed is going to be slighly endogenous."
  },
  {
    "objectID": "adv_class/09iv.html#iv-validity-strength",
    "href": "adv_class/09iv.html#iv-validity-strength",
    "title": "Instrumental Variables",
    "section": "IV Validity: Strength",
    "text": "IV Validity: Strength\nThe only thing we could probably do is try to analyze model strength. How much does the instrument affect treatment take up? is the effect marginal? or a large effect?\nWeaker instruments may create larger problems on the analysis because:\n\nWith weaker instruments, the precision of the estimator drops substantially.\nWith weaker instruments, any “endogeneity” problem (even due to randomness) will generate a bias\n\n\nStock and Yogo (2005) suggest and F~13.9 (or higher) for a 5% bias\nLee, et al (2020) suggest you need even higher F’s if you want to avoid problems with CI\n\n\nWith weak instruments, distribution of beta coefficients will no longer be normal!"
  },
  {
    "objectID": "adv_class/09iv.html#iv-strength",
    "href": "adv_class/09iv.html#iv-strength",
    "title": "Instrumental Variables",
    "section": "IV Strength",
    "text": "IV Strength\n\n\nCode\ncapture program drop simx\nprogram simx, eclass\n    clear\n    set obs 500\n    gen z=rnormal()&gt;0\n    gen u1=rnormal()\n    gen u2=rnormal()\n    gen u3=rnormal()\n    forvalues i = 1/5 {\n        gen d`i' = ((-0.5+z) + (u1 + u2)*0.5*`i')&gt;0\n        gen y`i' = 1 + d`i'+u3+u2\n    }\n    forvalues i = 1/5 {\n        reg d`i' z\n        matrix b`i' =(_b[z]/_se[z])^2\n        ivregress  2sls y`i' (d`i'=z)\n        matrix b`i' = b`i',_b[d`i'],_se[d`i'],_b[d`i']/_se[d`i']\n        matrix colname b`i'=f_stat beta beta_se beta_t\n        matrix coleq b`i'=md`i'\n    }\n    matrix b=b1\n    forvalues i = 2/5 {\n        matrix b=b,b`i'\n    }\n    ereturn post b\nend\n\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nmd1_b_f_stat |        494    189.9262    38.63693   101.8865   337.4117\n  md1_b_beta |        494    .9912474    .2289997   .1758122    1.64053\nmd1_b_beta~e |        494    .2440663    .0235266   .1867024   .3293121\n-------------+---------------------------------------------------------\nmd2_b_f_stat |        494    43.33591    13.76347   12.07967   101.6624\n  md2_b_beta |        494    .9717824    .4421832  -.8390987   2.327243\nmd2_b_beta~e |        494    .4711093    .0945389   .3032212   .9873207\n-------------+---------------------------------------------------------\nmd3_b_f_stat |        494    19.25043     8.81203   2.051613   61.55846\n  md3_b_beta |        494    .9354647    .6820819  -1.964163   2.996998\nmd3_b_beta~e |        494    .7430147    .2493365    .383426    2.16554\n-------------+---------------------------------------------------------\nmd4_b_f_stat |        494    11.19082    6.428525   .0483399   38.44057\n  md4_b_beta |        494    .8711808    .9987467  -4.383311   5.634943\nmd4_b_beta~e |        494    1.135114    1.244907   .4609722   25.16503\n-------------+---------------------------------------------------------\nmd5_b_f_stat |        494    7.522731    5.144063   .0544148   28.23535\n  md5_b_beta |        494    .7696057    1.443986  -6.293723   8.951643\nmd5_b_beta~e |        494     1.69479    1.866909    .530196    18.3837"
  },
  {
    "objectID": "adv_class/09iv.html#iv-strength-bias-distribution",
    "href": "adv_class/09iv.html#iv-strength-bias-distribution",
    "title": "Instrumental Variables",
    "section": "IV Strength: Bias distribution",
    "text": "IV Strength: Bias distribution\n\n\nCode\nuse resources/simiv.dta, clear\nforvalues i = 1/5 {\n  qui:sum md`i'_b_beta\n  gen new`i'=(md`i'_b_beta-1)/r(sd)\n}\nset scheme white2\ncolor_style tableau\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new1, name(m1, replace) , legend(off)\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new2, name(m2, replace) , legend(off)\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new3, name(m3, replace) , legend(off)\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new4, name(m4, replace) , legend(off)\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new5, name(m5, replace) , legend(off)\ngraph combine m1 m2 m3 m4 m5, col(3) xcommon ycommon\ngraph export resources/cmb.png, width(1500)  replace"
  },
  {
    "objectID": "adv_class/09iv.html#iv-strength-solution-weakiv",
    "href": "adv_class/09iv.html#iv-strength-solution-weakiv",
    "title": "Instrumental Variables",
    "section": "IV Strength Solution: weakiv",
    "text": "IV Strength Solution: weakiv\n\nWeak IV’s are a problem in the sense that it may induce bias on the estimated coefficients, but also that it may affect how Standard Errors are estimated.\n\nThe distribution of the Statistic is no longer normal\n\nOne solution, in this case, is at least adjusting SE and CI So they better reflect the problem.\nIn Stata, this can be done with weakiv (ssc install weakiv)\nAt the end, however, if you weak instruments, you may be able to correct of potential biases, but you may need to get more data, or better instruments"
  },
  {
    "objectID": "adv_class/09iv.html#late-local-average-treatement-effect",
    "href": "adv_class/09iv.html#late-local-average-treatement-effect",
    "title": "Instrumental Variables",
    "section": "LATE: Local Average Treatement Effect",
    "text": "LATE: Local Average Treatement Effect\nUp to this point, we imposed the assumption that TE were homogenous. Thus, IV could identify Treatment effects for everyone. (Average Treatment effect)\nHowever, not everyone may be affected by the instrument, only by the compliers.\nTwo ways of thinking about it:\n\nNot everybody is affected by the instrument. (you have the always and never takers)\nthe instrument was never suppoused to affect certain groups!\n\nSo, IV will identify TE for the compliers only.\nBecause of this, using different instruments may actually identify different effects, based on which population was affected.\nOverid tests may fail in this case."
  },
  {
    "objectID": "adv_class/09iv.html#simulation-example",
    "href": "adv_class/09iv.html#simulation-example",
    "title": "Instrumental Variables",
    "section": "Simulation Example:",
    "text": "Simulation Example:\n\n\nCode\nclear\nset obs 10000\ngen sex = rnormal()&gt;0\ngen z1 = rnormal()&gt;0\ngen z2 = rnormal()&gt;0\ngen e_1 =rnormal()\ngen e_2 =rnormal()\ngen e_3 =rnormal()\ngen D =(z1*(sex==0) + z2*(sex==1) + (e_1 + e_2)*.5)&gt;0\ngen Ds =(  (e_1 + e_2)*.5)&gt;0\ngen y = 1 + D*(sex==0) +2*D*(sex==1)+e_3+e_2\n\n\nNumber of observations (_N) was 0, now 10,000.\n\n\n\n\nCode\n%%echo\nivregress 2sls y (D=z1)\nivregress 2sls y (D=z2)\nivregress 2sls y (D=z1 z2)\n\n\n\n. ivregress 2sls y (D=z1)\n\nInstrumental variables 2SLS regression            Number of obs   =     10,000\n                                                  Wald chi2(1)    =      68.61\n                                                  Prob &gt; chi2     =     0.0000\n                                                  R-squared       =     0.2706\n                                                  Root MSE        =     1.5526\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           D |   1.170842   .1413543     8.28   0.000     .8937923    1.447891\n       _cons |   1.233464   .1015275    12.15   0.000     1.034474    1.432455\n------------------------------------------------------------------------------\nEndogenous: D\nExogenous:  z1\n\n. ivregress 2sls y (D=z2)\n\nInstrumental variables 2SLS regression            Number of obs   =     10,000\n                                                  Wald chi2(1)    =     211.38\n                                                  Prob &gt; chi2     =     0.0000\n                                                  R-squared       =     0.3558\n                                                  Root MSE        =     1.4591\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           D |   1.948051   .1339876    14.54   0.000      1.68544    2.210662\n       _cons |   .6818014   .0962172     7.09   0.000     .4932192    .8703836\n------------------------------------------------------------------------------\nEndogenous: D\nExogenous:  z2\n\n. ivregress 2sls y (D=z1 z2)\n\nInstrumental variables 2SLS regression            Number of obs   =     10,000\n                                                  Wald chi2(1)    =     257.90\n                                                  Prob &gt; chi2     =     0.0000\n                                                  R-squared       =     0.3223\n                                                  Root MSE        =     1.4966\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           D |   1.556085   .0968958    16.06   0.000     1.366172    1.745997\n       _cons |   .9600188   .0703862    13.64   0.000     .8220643    1.097973\n------------------------------------------------------------------------------\nEndogenous: D\nExogenous:  z1 z2\n\n."
  },
  {
    "objectID": "adv_class/09iv.html#canonical-designs",
    "href": "adv_class/09iv.html#canonical-designs",
    "title": "Instrumental Variables",
    "section": "Canonical Designs",
    "text": "Canonical Designs\n\nThe general message about using IV’s is, and has always been, that they are hard to come by.\nApplied research spends a quite good amount of time explaining why a particular instrument IS valid. (exogenous and relevant)\nRelevance is generally easy to test, but exogeneity is difficult. Little can be done other than relying in other papers, and circumstances.\nThere are also those “clever” IVs, that tend to be case specific\n\nScott Cunningham talks about Instruments being “weird”, because you wouldnt expect them to be in the context of the research\n\nThere are, however, some designs that are used quite often, because they apply to different circumstances."
  },
  {
    "objectID": "adv_class/09iv.html#cd-lotteries",
    "href": "adv_class/09iv.html#cd-lotteries",
    "title": "Instrumental Variables",
    "section": "CD: Lotteries",
    "text": "CD: Lotteries\n\nIn RCT, Lotteries are commonly used to decide who gets or doesnt get treatment among participants. Once treatment is assigned, however, not everyone will effectively taking up the treatment.\n\nFurthermore, some people may still end up being effectively treated because of other factors.\n\nThis is a case of imperfect compliance.\nIn cases like this, the lottery itself (which is randomized) can be used as instrument to identify the effect of being effectively treated.\n\nExamples:\n\nVietnam Draft Lottery\nOregon Medicaid Expansion Lottery"
  },
  {
    "objectID": "adv_class/09iv.html#cd-judge-fixed-effects",
    "href": "adv_class/09iv.html#cd-judge-fixed-effects",
    "title": "Instrumental Variables",
    "section": "CD: Judge Fixed Effects",
    "text": "CD: Judge Fixed Effects\nThis design is also partially based on a kind of randomized assigment.\n\nIndividuals are “allocated” to work, or be judge, under different officers “judges”, at random.\nJudges are consistent among each other, with only difference being the severity of the judgment.\nThen Judge fixed effect can be used as an instrument on the judgment (treatment), and the final impact on the outcome of interest.\n\nThe idea here is that “judgment-severity” varies by judge. This difference in taste creates exogenous variation on some treatment, which is analyzed on some treatment.\nExample:\n\nTeachers Grading? Driving test officers? Performance tests?"
  },
  {
    "objectID": "adv_class/09iv.html#cd-shift-share-bartik-instrument",
    "href": "adv_class/09iv.html#cd-shift-share-bartik-instrument",
    "title": "Instrumental Variables",
    "section": "CD: Shift-Share Bartik Instrument",
    "text": "CD: Shift-Share Bartik Instrument\nOriginally used in a study of regional labor market effects, this kind of instruments have also been used widely in other areas, such as imigration and trade.\nThe instrument was developed to analyze how changes in economic growth would affect market outcomes. (reverse Causality)\nTo do this, Bartik (1991) suggests, that it could be possible to create an instrument, making use of only exogenous variations, to first predict Potential local growth.\n\nEstimate industry shares by local region, based on some Ex ante information.\nEstimate national growth by industry (which should be exogenous to local growth)\nEstimate Potential Local growth using Shares x growth\n\nThis last one should represent the instrument to be used on actual local growth\nThis instrument depends strongly on the assumption that Shares are exogenous, and states are small compare to the national experience."
  },
  {
    "objectID": "adv_class/11rdd.html#re-cap-potential-outcome-model",
    "href": "adv_class/11rdd.html#re-cap-potential-outcome-model",
    "title": "Regression Discontinuity Design",
    "section": "Re-Cap: Potential outcome Model",
    "text": "Re-Cap: Potential outcome Model\nIn the ideal world, where we can see all possible outcomes and scenarios of your potential treatments, it will be very simple to estimate treatment effects:\n\\[\n\\delta_i = Y_i(1)-Y_i(0)\n\\]\nThis works because all observed and unobserved individual characteristics are kept fixed, except for the treatment Status.\n\\[y_i(D)=y_i(X,u,D)\\]\nSo when comparing a person with himself (clones or parallel worlds), we know (or at least expect) that everything else is the same, and that differences between the two states are explained only by the treatment."
  },
  {
    "objectID": "adv_class/11rdd.html#the-problem",
    "href": "adv_class/11rdd.html#the-problem",
    "title": "Regression Discontinuity Design",
    "section": "The Problem",
    "text": "The Problem\nWe do not observe both ALL States at the same time. People will either be treated or untreated, not both.\nSo what can we do?\n\nWe need to find good counterfactuals!\n\nThis means finding people are very similar to the ones treated, so they can be used as the examples of the “what if” question.\nBut there is a problem. Even in the best scenarios, we can never be asure about how to control for unobservables…or can we?"
  },
  {
    "objectID": "adv_class/11rdd.html#section",
    "href": "adv_class/11rdd.html#section",
    "title": "Regression Discontinuity Design",
    "section": "",
    "text": "You can always RCT But it can be expensive\nYou can IV the problem but its hard to justify\nYou can add FE, but you have time varying errors\n\nThen what?\n\nYou could RDD the problem (if you have the right data!)"
  },
  {
    "objectID": "adv_class/11rdd.html#what-is-rdd",
    "href": "adv_class/11rdd.html#what-is-rdd",
    "title": "Regression Discontinuity Design",
    "section": "What is RDD?",
    "text": "What is RDD?\nRDD or Regression Discontinuity design is methodology that is known for its clean identification and with a relatively easy visualization tool to understand the identification, and solve the problem of unobserved distributions. (see here for a recent paper on how to make graphs on this).\nIn fact, the treatment Status has a very clear Rule!\nConsider the following problem:\n\nYou want to study the role of college on earnings.\nYou have data on people who are applying to go to school. They all take some standardized tests. Their grade will determine if they get into College or not.\nPeople with high skills will get a higher grades in the GRE, go to college, and probably get higher salaries.\nBut, there is a problem. How can you figure out if wages are due to College or skill?"
  },
  {
    "objectID": "adv_class/11rdd.html#possible-solution",
    "href": "adv_class/11rdd.html#possible-solution",
    "title": "Regression Discontinuity Design",
    "section": "Possible Solution",
    "text": "Possible Solution\nSay that we actually have access to the grades, which range from 100 to 200. And assume that you say, every one with grades higher than 170 will go to college.\nCan you estimate the effect now?\n\nYou can’t compare people with more than 170 to those with less than 170. Because skill or ability will be different across groups.\nHowever, what if you compare individuals with 170-172 vs 167-169?\n\nThese individuals are so close togeter they problably have very similar characteristics as well!\nyou have a Localized randomization.\n\n\nIn this case, your analysis is those individuals just above the thresholds to those just below (counterfactual)\nUnless you think grades near the threshold are as good as random, then you have a design to identify treatment effects!"
  },
  {
    "objectID": "adv_class/11rdd.html#rdd-how-it-works.",
    "href": "adv_class/11rdd.html#rdd-how-it-works.",
    "title": "Regression Discontinuity Design",
    "section": "RDD: How it works.",
    "text": "RDD: How it works.\nSelection and Index:\n\nThe first thing you need to see if you can use and RDD is to see if you have access to a variable that “ranks” units.\n\n\nThis variable should be smooth and preferibly continuous.\n\nage, distance from boarder, test score, poverty index\n\n\n\nAssignment into treatment is a function of this index only, with a clear threshold for the index to have an impact the treatment.\n\n\nThose under the Threshold are not treated. Those above are.\nThis is called a Sharp RDD design."
  },
  {
    "objectID": "adv_class/11rdd.html#section-1",
    "href": "adv_class/11rdd.html#section-1",
    "title": "Regression Discontinuity Design",
    "section": "",
    "text": "The threshold should be unique to the Treatment of interest (nothing else happens around that point)\n\n\nIn the College Case, we assume 170 triggers acceptance to School. But if it also triggers Scholarships??\n\n\nPerhaps the Most important: The score cannot be manipulated\n\nOnly then we have true local randomization.\n\nYou want the potential outcomes to be smooth functions of Z. (so we do not mix treatment effects with Nonsmooth changes in outcomes)"
  },
  {
    "objectID": "adv_class/11rdd.html#sharp-rdd",
    "href": "adv_class/11rdd.html#sharp-rdd",
    "title": "Regression Discontinuity Design",
    "section": "Sharp RDD",
    "text": "Sharp RDD\n\n\nCode\nclear\nset scheme white2\ncolor_style bay\nset seed 1\nqui:set obs 100\ngen e=rnormal()\ngen z=runiform()+e\ngen t = z&gt;0\ngen y = 1 + z + e + rnormal() + (z&gt;0)\ntwo line t z, sort title(\"Treatment\") ylabel(0 \"Not Treated\" 1 \"Treated\") name(m1, replace) ytitle(\"\")\ngraph export resources\\rdd1.png,  width(1000) height(1000) replace\ntwo scatter y z, sort title(\"Outcome\") pstyle(p1) || lfit y z, lw(1) pstyle(p2) ///\n    || lfit y z if z&lt;0, lw(0.5) || lfit y z if z&gt;0, lw(0.5) , legend(off) name(m2, replace)\ngraph export resources\\rdd2.png,  width(1000) height(1000)  replace"
  },
  {
    "objectID": "adv_class/11rdd.html#how-it-works-p2",
    "href": "adv_class/11rdd.html#how-it-works-p2",
    "title": "Regression Discontinuity Design",
    "section": "How it works : p2",
    "text": "How it works : p2\nRecall that in an RCT (or under randomization) treatment effects are estimated by comparing those treated and those not treated.\n\\[E(y|D=1)-E(y|D=0)\\]\nUnder SRDD, you can also think about the same experiment, except that we would need to compare individuals AT the theshold.\n\\[\\begin{aligned}\n\\lim_{z\\downarrow c} E(y|Z=z) &- \\lim_{z\\uparrow c} E(y|Z=z) \\\\\nE(y(1)|Z=c) &-E(y(0)|Z=c)\n\\end{aligned}\n\\]\n\nIn this case, the overlapping assumption is violated. So we need to attemp obtaining effects for groups AT the limit when \\(Z=c\\)."
  },
  {
    "objectID": "adv_class/11rdd.html#estimation",
    "href": "adv_class/11rdd.html#estimation",
    "title": "Regression Discontinuity Design",
    "section": "Estimation",
    "text": "Estimation\nThe most simple way to proceed is to estimate the model using a parametric approach (OLS)\n\\[y = a_0 + \\delta D_{z&gt;c} + f(z-c) + e\\]\nThe idea here is to identify a “jump” in the outcome (treatment effect) at the point where \\(z\\) crosses the threshold.\nBut to identify the jump only, we also need to model the trend observe before and after that threshold (\\(f(z-c)\\)), which can be modelled as flexible as possible. (this include interactions with the jump)\nAlternatively, we could use smaller bandwidths (nonparametric)"
  },
  {
    "objectID": "adv_class/11rdd.html#example",
    "href": "adv_class/11rdd.html#example",
    "title": "Regression Discontinuity Design",
    "section": "Example",
    "text": "Example\n\n\nCode\n qui: {\nclear\nset seed 1\nset obs 200\ngen e=rnormal()\ngen z=runiform()+e\nsum z\nreplace z=(z-r(mean))/r(sd)\ngen t = z&gt;0\ngen y = 1 + 0.5*z + e + rnormal() + (z&gt;0)\n\nqui:reg y t z \npredict yh1\nlocal b1:display %3.2f _b[t]\nqui:reg y t c.z##c.z  \npredict yh2\nlocal b2:display %3.2f _b[t]\nqui:reg y t c.z##c.z##c.z\npredict yh3\nlocal b3:display %3.2f _b[t]\nsort z\n}\ntwo (scatter y z, sort title(\"Sharp RDD\") pstyle(p1) color(%20)) ///\n    (line yh1 z if z&lt;0, pstyle(p2) lw(0.5)) (line yh1 z if z&gt;0, pstyle(p2) lw(0.5)) ///\n    (line yh2 z if z&lt;0, pstyle(p3) lw(0.5)) (line yh2 z if z&gt;0, pstyle(p3) lw(0.5)) ///\n    (line yh3 z if z&lt;0, pstyle(p4) lw(0.5)) (line yh3 z if z&gt;0, pstyle(p4) lw(0.5)) , ///\n    legend(order(2 \"Linear ATT: `b1'\" 3 \"Quadratic ATT: `b2'\" 4 \"Cubic ATT: `b3'\")) name(m1, replace)"
  },
  {
    "objectID": "adv_class/11rdd.html#example-1",
    "href": "adv_class/11rdd.html#example-1",
    "title": "Regression Discontinuity Design",
    "section": "Example",
    "text": "Example\n\n\nCode\nqui: {\nqui:reg y t c.z#t \npredict yh11\nlocal b1:display %3.2f _b[t]\nqui:reg y t (c.z##c.z)#t  \npredict yh21\nlocal b2:display %3.2f _b[t]\nqui:reg y t (c.z##c.z##c.z)#t\npredict yh31\nlocal b3:display %3.2f _b[t]\n}\ntwo (scatter y z, sort title(\"Sharp RDD\") pstyle(p1) color(%20)) ///\n    (line yh11 z if z&lt;0, pstyle(p2) lw(0.5)) (line yh11 z if z&gt;0, pstyle(p2) lw(0.5)) ///\n    (line yh21 z if z&lt;0, pstyle(p3) lw(0.5)) (line yh21 z if z&gt;0, pstyle(p3) lw(0.5)) ///\n    (line yh31 z if z&lt;0, pstyle(p4) lw(0.5)) (line yh31 z if z&gt;0, pstyle(p4) lw(0.5)) , ///\n    legend(order(2 \"Linear ATT: `b1'\" 3 \"Quadratic ATT: `b2'\" 4 \"Cubic ATT: `b3'\")) name(m2, replace)"
  },
  {
    "objectID": "adv_class/11rdd.html#fuzzy-rd-imperfect-compliance",
    "href": "adv_class/11rdd.html#fuzzy-rd-imperfect-compliance",
    "title": "Regression Discontinuity Design",
    "section": "Fuzzy RD: Imperfect compliance",
    "text": "Fuzzy RD: Imperfect compliance\nWhile the Idea Scenario happens when there is perfect compliance (above the threshold you are treated), this doesnt happen all the time.\nIn the education example:\n\nSome people with low grades may be “legacy” or have “contacts” (or took a second exam later) and manage to go to college\nSome decided not to go, even after entering to college\n\nSounds Familiar? (Never takers vs always takers)\nWhen this happens, you can still do RDD, but you need more steps"
  },
  {
    "objectID": "adv_class/11rdd.html#fuzzy-rd",
    "href": "adv_class/11rdd.html#fuzzy-rd",
    "title": "Regression Discontinuity Design",
    "section": "Fuzzy RD",
    "text": "Fuzzy RD\n\nEstimate the impact of Discontinuity on Treatment\nEstimate the impact of Discontinuity on Outcome\nEstimate the ratio between (1) and (2)\n\nSounds Familiar?\n\nIts a kind of wald/IV estimator.\n\nThe instrument is the discontinuity\nThe the endogenous variable is the treament\n\n\nYou still need to estimate the effect as close to the Discontinuity as possible\nivregress may still do most of this for you"
  },
  {
    "objectID": "adv_class/11rdd.html#example-2",
    "href": "adv_class/11rdd.html#example-2",
    "title": "Regression Discontinuity Design",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\nTreatment\n\n\n\n\n\n\n\nOutcome\n\n\n\n\n\nEffect:\n\nCode\nqui: gen dz = z&gt;0\nqui: reg y dz c.z##c.z#i.dz\nlocal b1 = _b[dz]\nqui: reg t dz c.z##c.z#i.dz\nlocal b2 = _b[dz]\n\ndisplay \"There is a \" %3.2f `b1' \" effect on the outcome\"\ndisplay \"and a \" %3.2f `b2' \" effect on the treatment\"\ndisplay \"which imply a LATE of \" %3.2f `=`b1'/`b2''\n\nThere is a 0.45 effect on the outcome and a 0.44 effect on the treatment which imply a LATE of 1.03"
  },
  {
    "objectID": "adv_class/11rdd.html#things-to-consider",
    "href": "adv_class/11rdd.html#things-to-consider",
    "title": "Regression Discontinuity Design",
    "section": "Things to consider",
    "text": "Things to consider\nTheoretical:\n\nYou need to identify “jumps” caused by a running variable. (depends on knowing how things works)\nThe potential outcomes have to be smooth functions of the running variable\n\nEmpirical:\n\nThe running variable shouldnt be manipulated. (random)\n\nImplies Assignment rules are not known, are exogenous, and there is no random heaping\n\nControls should be balanced around the threshold"
  },
  {
    "objectID": "adv_class/11rdd.html#testing-empirical-assumptions",
    "href": "adv_class/11rdd.html#testing-empirical-assumptions",
    "title": "Regression Discontinuity Design",
    "section": "Testing Empirical Assumptions",
    "text": "Testing Empirical Assumptions\nManipulation of running variable may cause a non-smooth density in the running variable:\n\nIf there is no manipulation, you may expect density round threshold to be smooth.\n\nIn Stata: ssc install rddensity. In r install.packages(c(‘rdd’,‘rddensity’))\n\n\n\n\nCode\nset linesize 100\nqui:ssc install  lpdensity, replace\nqui:ssc install  rddensity, replace\nrddensity z, c(0) plot\ngraph export resources\\frdd3.png, width(1000) replace"
  },
  {
    "objectID": "adv_class/11rdd.html#section-2",
    "href": "adv_class/11rdd.html#section-2",
    "title": "Regression Discontinuity Design",
    "section": "",
    "text": "In cases of nonrandom heaping, it may be possible to avoid the problem by restricting the data.\nThis is an example of measurement error, when individuals may “round-up/down” answers. And may occure near threshold.\nThis does not necessarily mean there is manipulation.\nPossible Solution? Estimate RDD excluding observations around (excluding) threshold."
  },
  {
    "objectID": "adv_class/11rdd.html#covariate-balance-and-placebo-tests",
    "href": "adv_class/11rdd.html#covariate-balance-and-placebo-tests",
    "title": "Regression Discontinuity Design",
    "section": "Covariate balance and Placebo tests",
    "text": "Covariate balance and Placebo tests\n\nIf treatment is locally randomized, then covariates should not be affected by discontinuity.\nAlternatively, one could estimate effects on variables you know CANNOT be affected by the treatment\nOne could also implement a placebo test, checking the impact on a different threholds.\n\nNo effect should be observed on the outcome, (but some on the treatment)"
  },
  {
    "objectID": "adv_class/11rdd.html#example-3",
    "href": "adv_class/11rdd.html#example-3",
    "title": "Regression Discontinuity Design",
    "section": "Example",
    "text": "Example\nImpact of Scores on Scholarship recipiency\n\n\nCode\nuse resources\\fuzzy, clear\ncolor_style bay\nqui:rdplot d x1, graph_options(legend( pos(6)))"
  },
  {
    "objectID": "adv_class/11rdd.html#manipulation-test",
    "href": "adv_class/11rdd.html#manipulation-test",
    "title": "Regression Discontinuity Design",
    "section": "Manipulation test",
    "text": "Manipulation test\n\n\nCode\nqui:rddensity x1, plot"
  },
  {
    "objectID": "adv_class/11rdd.html#intention-to-treat",
    "href": "adv_class/11rdd.html#intention-to-treat",
    "title": "Regression Discontinuity Design",
    "section": "Intention to treat",
    "text": "Intention to treat\nImpact on Enrollment\n\n\nCode\nqui:rdplot y x1, graph_options(legend( pos(6)))"
  },
  {
    "objectID": "adv_class/11rdd.html#estimation-of-the-effect",
    "href": "adv_class/11rdd.html#estimation-of-the-effect",
    "title": "Regression Discontinuity Design",
    "section": "Estimation of the effect:",
    "text": "Estimation of the effect:\n\n\nCode\ngen dx1 = x1&gt;0\nivregress 2sls y (d = dx1) c.x1##c.x1#dx1 \n\n\n\nInstrumental variables 2SLS regression            Number of obs   =     23,132\n                                                  Wald chi2(5)    =    1645.56\n                                                  Prob &gt; chi2     =     0.0000\n                                                  R-squared       =     0.2845\n                                                  Root MSE        =     .39876\n\n-------------------------------------------------------------------------------\n            y | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n--------------+----------------------------------------------------------------\n            d |   .4432824   .0215111    20.61   0.000     .4011214    .4854434\n              |\n     dx1#c.x1 |\n           0  |   .0002949   .0019179     0.15   0.878    -.0034642    .0040539\n           1  |  -.0034238    .000779    -4.39   0.000    -.0049507   -.0018969\n              |\ndx1#c.x1#c.x1 |\n           0  |   .0000741   .0000703     1.05   0.292    -.0000637    .0002119\n           1  |   .0000616   .0000164     3.76   0.000     .0000295    .0000936\n              |\n        _cons |   .5103635    .010876    46.93   0.000     .4890469    .5316801\n-------------------------------------------------------------------------------\nEndogenous: d\nExogenous:  0b.dx1#c.x1 1.dx1#c.x1 0b.dx1#c.x1#c.x1 1.dx1#c.x1#c.x1 dx1"
  },
  {
    "objectID": "adv_class/11rdd.html#some-sensitivity",
    "href": "adv_class/11rdd.html#some-sensitivity",
    "title": "Regression Discontinuity Design",
    "section": "Some Sensitivity",
    "text": "Some Sensitivity\n\n\nCode\nqui:{   \nmatrix b1 = 0,0\nforvalues i = 1/15 {\n    ivregress 2sls y (d = dx1) c.x1#dx1  if abs(x1)&lt;`i'\n    matrix b1=b1\\[_b[d],_se[d]]\n}\n\nmatrix b2 = 0,0\nforvalues i = 1/15 {\n    ivregress 2sls y (d = dx1) c.x1##c.x1#dx1  if abs(x1)&lt;`i'\n    matrix b2=b2\\[_b[d],_se[d]]\n}\n\nmatrix b3 = 0,0\nforvalues i = 1/15 {\n    ivregress 2sls y (d = dx1) c.x1##c.x1##c.x1#dx1  if abs(x1)&lt;`i'\n    matrix b3=b3\\[_b[d],_se[d]]\n}\n\nlbsvmat b1\nlbsvmat b2\nlbsvmat b3\nforvalues i = 1/3 {\n  gen ll`i'=b`i'1-b`i'2*1.96\n  gen ul`i'=b`i'1+b`i'2*1.96\n}\ngen z = _n-1 if b11!=.\nreplace z=. in 1\n\n}\ngen z1 = z + 0.25\ngen z2 = z + 0.5\ntwo (rspike ll1 ul1 z, lw(1) pstyle(p1) color(%50) ) (scatter b11 z, pstyle(p1) ) ///\n    (rspike ll2 ul2 z1, lw(1) pstyle(p2) color(%50) ) (scatter b21 z1, pstyle(p2) ) ///\n    (rspike ll3 ul3 z2, lw(1) pstyle(p3) color(%50) ) (scatter b31 z2, pstyle(p3) ), ///\n    legend(order(1 \"Linear\" 3 \"Quadratic\" 5 \"Cubic\")) xtitle(\"Bandwidth\")\n\n\n(23,117 missing values generated)\n(23,117 missing values generated)"
  },
  {
    "objectID": "adv_class/11rdd.html#flasification-test",
    "href": "adv_class/11rdd.html#flasification-test",
    "title": "Regression Discontinuity Design",
    "section": "Flasification test",
    "text": "Flasification test\n\n\nCode\nqui {\nivregress 2sls icfes_female (d = dx1) c.x1#dx1    if abs(x1)&lt;20\nest sto m1\nivregress 2sls icfes_age (d = dx1) c.x1#dx1       if abs(x1)&lt;20\nest sto m2\nivregress 2sls icfes_urm (d = dx1) c.x1#dx1       if abs(x1)&lt;20\nest sto m3\nivregress 2sls icfes_famsize (d = dx1) c.x1#dx1       if abs(x1)&lt;20\nest sto m4\n}\nesttab m1 m2 m3 m4, keep(d)\n\n\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n             icfes_female       icfes_age       icfes_urm    icfes_fams~e   \n----------------------------------------------------------------------------\nd                  0.0199           0.128         0.00791          0.0439   \n                   (0.80)          (1.05)          (0.65)          (0.63)   \n----------------------------------------------------------------------------\nN                   14841           14799           14841           14801   \n----------------------------------------------------------------------------\nt statistics in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "adv_class/13SC.html#introduction",
    "href": "adv_class/13SC.html#introduction",
    "title": "Synthetic Control",
    "section": "Introduction",
    "text": "Introduction\n\nOne more last time. What is the Goal of Causal Analysis?\n\n\n\n\n\n\n\n\n\n\nThe goal of Causal Analysis is to identify how a treatment affects the outcome by itself, once all other factors are kept constant or controlled for.\n\n\n\nFrom a theoretical point of view, that is very easy. You simply compare two Potential outcomes:\n\\[\nTE_i = y_i(1)- y_i(0)\n\\]\nand aggregate those outcomes as needed:\n\\[\nATT=E(TE_i|D=1);ATU=E(TE_i|D=0);ATE=E(TE_i);ATX=E(TE_i|X)\n\\]"
  },
  {
    "objectID": "adv_class/13SC.html#section",
    "href": "adv_class/13SC.html#section",
    "title": "Synthetic Control",
    "section": "",
    "text": "Unfortunately, we only observe one outcome. You are either treated or untreated…So how do we fix this?\nYou need to find counterfactuals so both observed (\\(X\\)) and unobserved (\\(e\\)) are the same (or close) between treated and contro group.\n\nRCT: Gold Standard, You randomize treatment and compare means. If correctly done, \\(X's\\) and \\(e's\\) will be comparable across groups, and ATE’s can be identified.\nReg + FE: For other cases, we just work with observational data. First method, Regression (OLS?). Adding covariates controls for their presence, working as a pseudo balancing approach.\nYou could also add fixed effects, to control for factors that are fixed (across time), but you do not observe. (requires Panel data).\nIt works if Treatment occurs at the same time for everyone treated. and if Unobserved are “fixed”"
  },
  {
    "objectID": "adv_class/13SC.html#section-1",
    "href": "adv_class/13SC.html#section-1",
    "title": "Synthetic Control",
    "section": "",
    "text": "Instrumental variables: 2nd Best to RCT. It uses IV to generate a small randomization process that can be used for estimating ATT. Technically it compares the effect among those potentially affected by the random instrument. Requires Randome instrument, and no-defiers. Its a Local ATE\nMatching and Reweigthing. Similar to Regression, but better to balance characteristics. The goal is to find units with similar characteristics for all treated units. You can estimate ATE, ATT or ATU. Depends on how well Matching is done\nRDD. If you have data where treatment depends on a single variable and a threshold, you can use this to identify TE for those “Near” the threshold. They Key assumption, treatment assigment is as good at random at the threshold."
  },
  {
    "objectID": "adv_class/13SC.html#section-2",
    "href": "adv_class/13SC.html#section-2",
    "title": "Synthetic Control",
    "section": "",
    "text": "DD. Differences in differences uses variation across time and across individual to identify treatment effects. Under PTA, and SUTVA\nDif. within individuals eliminates common time trends, Dif across time, eliminates individual fixed effects. DD provide you with ATT’s for the treated, after treatment.\n\\[ATT=(Y_{g=1,t=1}-Y_{g=1,t=0}) - [(Y_{g=0,t=1}-Y_{g=0,t=0})]\\]\nCan be generalized to Many periods and many groups, but requires stronger assumptions (no anticipation and no change in treatment status), and further aggregation.\nOr combined with Matching for even better results."
  },
  {
    "objectID": "adv_class/13SC.html#synthetic-control-special-case",
    "href": "adv_class/13SC.html#synthetic-control-special-case",
    "title": "Synthetic Control",
    "section": "Synthetic Control: Special case",
    "text": "Synthetic Control: Special case\nAs previous Cases, Synthetic control aims to identify treatment constructing appropriate “counterfactuals”.\nIt is said that Synthethic controls may be even MORE credible methodology, because the treated group is by construction Exogenous…but how?\n\nThe treated group is a Case Study.\nAn isolated event or unit that is affected by a treatment, and should not affect other units !\n\nIn this sense, the treatment is exogenous, because it affected a single unit.\nBut what about the counterfactual?"
  },
  {
    "objectID": "adv_class/13SC.html#section-3",
    "href": "adv_class/13SC.html#section-3",
    "title": "Synthetic Control",
    "section": "",
    "text": "In other methods (in particular Matching), our “conterfactual” mean to look for observations that had the same characteristics as the treated observation.\nSome times, we needed to settle to use a single “bad” control, because we couldnt find one better. (people are very different).\n\nUsing Stricter criteria would make it unfeasible.\nMore relax and we have lots of biases.\n\nSC is different. You have MANY controls, so why settle with only one?\nSC is like Dr Frankenstein, where you “build” a single comparison group by averaging information of all controls.\nYou build the synthetic control getting “weighted averages”.\nBut…we assume you can see all units across time (panel data)"
  },
  {
    "objectID": "adv_class/13SC.html#this-is-a-very-popular-method",
    "href": "adv_class/13SC.html#this-is-a-very-popular-method",
    "title": "Synthetic Control",
    "section": "This is a very popular method",
    "text": "This is a very popular method\nWhere has this method been used:\n\neffects of right-to-carry laws (Donohue et al., 2019),\nlegalized prostitution (Cunningham and Shah, 2018),\nimmigration policy (Bohn et al., 2014),\ncorporate political connections (Acemoglu et al., 2016),\ntaxation (Kleven et al., 2013),\norganized crime (Pinotti, 2015)\n\nJust to name a few."
  },
  {
    "objectID": "adv_class/13SC.html#assumptions",
    "href": "adv_class/13SC.html#assumptions",
    "title": "Synthetic Control",
    "section": "Assumptions:",
    "text": "Assumptions:\n\nThe Donor Pool should be a good match for the treated unit. Thus, the synthethic control should be Zero before treatment.\n\nThis is similar to PTA, but stronger. Before treatment, there should be no difference between Treated and synthetic control\n\nSUTVA. Only the treated group is affected by treatment. The control group should be unaffected (no spill over effects).\nThere should be NO other “event” in the period of analysis. (Thus we only capture treatment impact)"
  },
  {
    "objectID": "adv_class/13SC.html#how-does-it-work.",
    "href": "adv_class/13SC.html#how-does-it-work.",
    "title": "Synthetic Control",
    "section": "How does it work.",
    "text": "How does it work.\nRecall, we want to estimate TE for the single untreated unit:\n\\[ATT_{1t} = Y_{1t}- Y(0)_{1t}\n\\]\nbut we do not observe \\(Y(0)_{1t}\\). We only know that before treatment\n\\[ATT_{1t} = Y_{1t}- Y(0)_{1t}=0\\]\nWe could construct a synthetic control:\n\\[\\hat Y_{1t}(0) = \\sum_{i = 2}^N w_i Y_{it}\n\\]\nAt the very least, the weights \\(w\\) should be such that before treatment (\\(G\\)):\n\\[Y_{1t} = \\sum_{i \\neq 1} w_i Y_{it} \\ \\forall \\ t&lt;G \\]"
  },
  {
    "objectID": "adv_class/13SC.html#section-4",
    "href": "adv_class/13SC.html#section-4",
    "title": "Synthetic Control",
    "section": "",
    "text": "At the very least, the weights \\(w\\) should be such that before treatment (\\(G\\)):\n\\[Y_{1t} = \\sum_{i \\neq 1} w_i Y_{it} \\ \\forall \\ t&lt;G \\]\nHavent we seen seen something like this Before? OLS:\n\\[y = x\\beta + e\\] \\[y^t_{1} = \\color{red}{a_0} +  y_i^t w + e\\]\n\\[\n\\begin{bmatrix}\ny^1_1 \\\\ y^1_2 \\\\ ... \\\\ y^1_{G-1} \\\\\n\\end{bmatrix} = \\color{red}{a_0} +\n\\begin{bmatrix}\ny^2_1 & y^3_1 & ... & y^k_1  \\\\\ny^2_2 & y^3_2 & ... & y^k_2 \\\\\n...  & ... & ... & ...\\\\\ny^2_{G-1} & y^3_{G-1} & ... & y^k_{G-1}\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nw_2 \\\\ w_3 \\\\ ... \\\\ w_k\n\\end{bmatrix}\n+ e\n\\]"
  },
  {
    "objectID": "adv_class/13SC.html#section-5",
    "href": "adv_class/13SC.html#section-5",
    "title": "Synthetic Control",
    "section": "",
    "text": "\\[y^t_{1} = \\color{red}{a_0} +  y_i^t w + e\\]\n\nIn this Specification, each row (observation) is a “pre-treatment” period of observed data.\nand each control unit (from the many controls) will be a variable.\n\nOLS can help you find the weights, which can then be used for obtaining the “Synthetic” control"
  },
  {
    "objectID": "adv_class/13SC.html#small-example",
    "href": "adv_class/13SC.html#small-example",
    "title": "Synthetic Control",
    "section": "Small Example",
    "text": "Small Example\n\n\nCode\nqui:frause smoking, clear\ncolor_style tableau\nbysort year:egen mean_cig=mean(cigsale) if state!=3\ntwo (line cigsale year if state ==3) (line mean_cig year if state==1), ///\n    legend(order(1 \"California\" 2 \"Avg Other States\") pos(6) col(2)) xline(1988)\n\n\n\n\n\n(31 missing values generated)"
  },
  {
    "objectID": "adv_class/13SC.html#section-6",
    "href": "adv_class/13SC.html#section-6",
    "title": "Synthetic Control",
    "section": "",
    "text": "Code\ndrop mean_cig\nqui:reshape wide cigsale lnincome beer age15to24 retprice , i(year) j(state)\nren cigsale1 mcigsale\n\n\n\nNow we have…38 variables, (other States but California)\nAnd 31 periods (only 19 Before treatment)\nCan we estimate the weights using OLS?\n\n…\n\nNop. N&lt;K !"
  },
  {
    "objectID": "adv_class/13SC.html#section-7",
    "href": "adv_class/13SC.html#section-7",
    "title": "Synthetic Control",
    "section": "",
    "text": "Code\nqui:reg mcigsale cigsale* if year&lt;=1988, nocons\npredict mcigh1\nqui: lasso linear  mcigsale cig* if year&lt;=1988, nocons\npredict mcigh2\n two (line mcigsale year, lw(0.5)  ) ///\n  (line mcigh1 mcigh2   year, lw(0.5)  ) , ///\n  legend(order(1 \"California\" 2 \"OLS Synthetic\" 3 \"LASSO Synthetic\")) xline(1988)\n\n\n(option xb assumed; fitted values)\n(options xb penalized assumed; linear prediction with penalized coefficients)\n\n\n\n\nOLS Not appropriate (specially if N&lt;K)\nLasso Better, because of regularization, but not great.\nWe are not controlling for other factors either (controls)\nOther Details we cover next"
  },
  {
    "objectID": "adv_class/13SC.html#allowing-for-covariates",
    "href": "adv_class/13SC.html#allowing-for-covariates",
    "title": "Synthetic Control",
    "section": "Allowing for Covariates:",
    "text": "Allowing for Covariates:\n\nAs with other methodologies, one should also considered controlling for covariates.\nSpecifically, more covariates can be allowed by Stacking them:\n\n\\[\\begin{bmatrix} y^t_{1} \\\\ x^t_{1} \\\\ z^t_{1} \\end{bmatrix}\n= \\color{red}{(a_0=0)} +\n\\begin{bmatrix} y^t_{2} & y^t_{3} ... & y^t_{k} \\\\\n                x^t_{2} & x^t_{3} ... & x^t_{k} \\\\\n                z^t_{2} & z^t_{3} ... & z^t_{k} \\end{bmatrix}\n\\begin{bmatrix} w_2 \\\\ w_3 \\\\ ... \\\\ w_k \\end{bmatrix}\n+ e\n\\]"
  },
  {
    "objectID": "adv_class/13SC.html#section-8",
    "href": "adv_class/13SC.html#section-8",
    "title": "Synthetic Control",
    "section": "",
    "text": "Code\nqui:frause smoking, clear\nren (cigsale lnincome beer age15to24 retprice) ///\n      (var1    var2     var3 var4      var5)\nqui: reshape long var, i(state year)    j(new)\nqui: reshape wide var, i(year new)  j(state)\nlabel define new 1 \"cigsale\" ///\n                 2 \"lnincome\" ///\n                 3 \"beer\" ///\n                 4 \"age15to24\" /// \n                 5 \"retprice\", modify\nlabel values new new    \nren var3 cal_out\nqui:reg cal_out var* if year&lt;=1988, nocons\npredict mcigh1\nqui: lasso linear   cal_out var* if year&lt;=1988, nocons\npredict mcigh2\n two (line cal_out year if new==1, lw(0.5)  ) ///\n  (line mcigh1 mcigh2   year if new==1, lw(0.5)  ) , ///\n  legend(order(1 \"California\" 2 \"OLS Synthetic\" 3 \"LASSO Synthetic\")) xline(1988)\n\n\n(option xb assumed; fitted values)\n(32 missing values generated)\n(options xb penalized assumed; linear prediction with penalized coefficients)"
  },
  {
    "objectID": "adv_class/13SC.html#what-else-to-keep-in-mind",
    "href": "adv_class/13SC.html#what-else-to-keep-in-mind",
    "title": "Synthetic Control",
    "section": "What else to keep in mind",
    "text": "What else to keep in mind\n\nWith More Variables, the goal is still to be able to choose \\(w's\\) that best explain the observed outcomes (and characteristics) of the “treated unit”.\n\n\\[w = \\min_w \\sum_{m=1}^K \\left[ v_m \\left( X_{1t}-\\sum_{j=2}^J w_j X_{jt}  \\right)^2 \\right]\n\\]\nHowever, we also need to impose restrictions on Weights:\n\n\\(w_j \\geq 0\\) Weights cannot be negative.\n\\(\\sum w_j =1\\) They should sum up to 1.\n\\(v_m\\) can be used to increase, or reduce the relative importance of factors in the model. (lower bound at 0) The constant is zero.\n\nThis is a maximization problem with constrains. Restrictions ensure the prediction is based on a “convex” set, avoiding extrapolation."
  },
  {
    "objectID": "adv_class/13SC.html#is-it-noise-or-causal",
    "href": "adv_class/13SC.html#is-it-noise-or-causal",
    "title": "Synthetic Control",
    "section": "Is it noise? or Causal?",
    "text": "Is it noise? or Causal?\n\nWhen using SC, you essentially have a sample \\(n=1\\) to estimate an effect. How do you know that effect is significant? and not just noise?\n\nYou can do a randomization experiment! and answer:\n\n\n\n“how unusual is this estimate under the null hypothesis of no policy effect?”.\n\n\nHow does this work?"
  },
  {
    "objectID": "adv_class/13SC.html#randomization",
    "href": "adv_class/13SC.html#randomization",
    "title": "Synthetic Control",
    "section": "Randomization",
    "text": "Randomization\n\nExcluding the treated unit, estimate the pseudo effect of every other unit in the dataset. These are placebos, and you should expect the effect to be zero for them…but you may see some positive and negative effects.\n\nThis may be consider the sampling distribution of the estimated effect.\n\nCalculate the pre- and post- treament Root mean squared prediction error for all units (treated and placebos).\n\nPre-RMSPE provides a statistic of how well the model fits before treatment.\nPost-RMSPE provides a statistic of how unusual is the outcome after the “treatment date”. The largest it is, the more unpredictable (or stronger treatment effect) it would be."
  },
  {
    "objectID": "adv_class/13SC.html#section-9",
    "href": "adv_class/13SC.html#section-9",
    "title": "Synthetic Control",
    "section": "",
    "text": "\\[\n  \\begin{aligned}\n  RMSPE_i^{pre} &= \\sqrt{ \\frac{1}{g-1}\\sum_{t=1}^{g-1}(y_{i,t}-\\sum_{j\\neq i}w_j^i y_{j,t})^2 } \\\\\n  RMSPE_i^{post} &= \\sqrt{ \\frac{1}{T-g+1}\\sum_{t=g}^{T}(y_{i,t}-\\sum_{j\\neq i}w_j^i y_{j,t})^2 }\n  \\end{aligned}\n  \\]\n\nEstimate the ratio between Pre and Post RMSPE, and rank them. \\[Ratio_i = \\frac{RMSPE_i^{post}}{RMSPE_i^{pre}}\n\\]\nThe p-value for the treatment is proportional to the Rank:\n\\[pvalue_i = \\frac{rank(i)}{Tot}\\]"
  },
  {
    "objectID": "adv_class/13SC.html#lets-continue-the-example",
    "href": "adv_class/13SC.html#lets-continue-the-example",
    "title": "Synthetic Control",
    "section": "Lets continue the example:",
    "text": "Lets continue the example:\n\n\nCode\nqui:frause smoking, clear\nxtset state year\ntempfile sc3\n** For California\nsynth cigsale cigsale(1970) cigsale(1975) cigsale(1980) cigsale(1985) cigsale(1988), trunit(3) trperiod(1989) keep(`sc3') replace\n** Same Specification for All other States excluding California\nforvalues i =1/39{\n    if `i'!=3 {\n        local pool\n        foreach j of local stl {\n            if `j'!=3 & `j'!=`i' local pool `pool' `j'\n        }\n        tempfile sc`i'\n        synth cigsale cigsale(1970) cigsale(1975) cigsale(1980) cigsale(1985) cigsale(1988), ///\n        trunit(`i') trperiod(1989) keep(`sc`i'') replace counit(`pool')\n    }\n}\n** Some data cleaning and prepration\nforvalues i =1/39{\n    use `sc`i'' , clear\n    gen tef`i' = _Y_treated - _Y_synthetic\n    egen sef`i'a =mean( (_Y_treated - _Y_synthetic)^2) if _time&lt;=1988\n    egen sef`i'b =mean( (_Y_treated - _Y_synthetic)^2) if _time&gt;1988\n  replace sef`i'a=sqrt(sef`i'a[1])\n    replace sef`i'b=sqrt(sef`i'b[_N])\n    drop if _time==.\n    keep tef`i' sef`i'* _time\n    save `sc`i'', replace\n}\n**\n** Merging all together, and getting ready to plot\n** \n\nuse `sc1', clear\nforvalues i = 2/39 {\n    merge 1:1 _time using `sc`i'', nogen\n}\nglobal toplot\nglobal toplot2\n\nforvalues i = 1/39 {\n    global toplot $toplot (line tef`i' _time, color(gs11) )\n  if (sef`i'a[1])&lt;(2*sef3a[1]) {\n        global toplot2 $toplot2 (line tef`i' _time, color(gs11) )\n    }\n}\n\n\nAll Cases\n\n\nCode\ntwo $toplot (line tef3 _time, lw(1) color(navy*.8)), xline(1989) legend(off)"
  },
  {
    "objectID": "adv_class/13SC.html#section-10",
    "href": "adv_class/13SC.html#section-10",
    "title": "Synthetic Control",
    "section": "",
    "text": "Good Cases Restricts to States with Good RMSEP (less than 2 California)\n\n\nCode\ntwo $toplot2 (line tef3 _time, lw(1) color(navy*.8)), xline(1989) legend(off)"
  },
  {
    "objectID": "adv_class/13SC.html#rmse-ratio",
    "href": "adv_class/13SC.html#rmse-ratio",
    "title": "Synthetic Control",
    "section": "RMSE Ratio",
    "text": "RMSE Ratio\n\n\nCode\nforvalues i = 1/39 {\n  if (sef`i'a[1])&lt;(2*sef3a[1]) {\n        matrix rt=nullmat(rt)\\[`i',sef`i'b[1]/sef`i'a[1]]\n    }\n}\nsvmat rt\negen rnk=rank(rt2)\n \ntwo bar rt2 rnk || bar rt2 rnk if rt1==3 , ///\n legend(order( 2 \"California\")) ///\n  ytitle(RMSE ratio)  xtitle(RMSE rank)\n\n\nnumber of observations will be reset to 32\nPress any key to continue, or Break to abort\nNumber of observations (_N) was 31, now 32."
  },
  {
    "objectID": "adv_class/13SC.html#p-values",
    "href": "adv_class/13SC.html#p-values",
    "title": "Synthetic Control",
    "section": "p-values",
    "text": "p-values\n\n\nCode\n gen rnk2=0\nforvalues i = 1/39 {\n    if   (sef`i'a[1])&lt;(2*sef3a[1]) {\n        local t = `t'+1\n        replace rnk2=rnk2+(tef`i'&lt;=tef3)    \n    }\n} \ngen pv=rnk2*100/`t'\n \ntwo bar pv _time if _time&gt;1988 & rnk2&lt;32, ylabel(0(2)15) xlabel(1989/2000)\n\n\n(10 real changes made)\n(6 real changes made)\n(32 real changes made)\n(5 real changes made)\n(9 real changes made)\n(3 real changes made)\n(5 real changes made)\n(4 real changes made)\n(6 real changes made)\n(4 real changes made)\n(4 real changes made)\n(8 real changes made)\n(5 real changes made)\n(4 real changes made)\n(8 real changes made)\n(3 real changes made)\n(5 real changes made)\n(8 real changes made)\n(5 real changes made)\n(5 real changes made)\n(7 real changes made)\n(2 real changes made)\n(4 real changes made)\n(4 real changes made)\n(2 real changes made)\n(6 real changes made)\n(3 real changes made)\n(6 real changes made)\n(8 real changes made)\n(5 real changes made)\n(4 real changes made)\n(6 real changes made)"
  },
  {
    "objectID": "adv_class/13SC.html#other-falsification-tests",
    "href": "adv_class/13SC.html#other-falsification-tests",
    "title": "Synthetic Control",
    "section": "Other Falsification Tests",
    "text": "Other Falsification Tests\n\nChange of treatment Year.\n\nIn the manual implementation you may want to change the treatment year (to an earler point). One should see no effect between false treatment date and the true to be zero.\nUsing synth (Stata) you may want to drop some of the controls, so only “pre-false” treatment data is used.\n\nLook also into synth_runner\n\n\nChange of Outcome.\n\nOne can estimate the effect on alternative outcomes. No effect should be estimated."
  },
  {
    "objectID": "adv_class/13SC.html#conclusions",
    "href": "adv_class/13SC.html#conclusions",
    "title": "Synthetic Control",
    "section": "Conclusions",
    "text": "Conclusions\n\nThe basic methodology presented here differs from other strategies because one uses a single treated unit, with pletora of treated groups.\nInstead of comparing single units with the treated group, it aims to compare a weighted average “synthetic control” to do so.\nIt will work better than matching because you are focusing on getting the best “weighted” group for a single unit.\nBut this methodology is still under development, with extensions toward using dissagregated data, or a combination with DD approaches.\nThis may change how much more one can do with the method"
  },
  {
    "objectID": "adv_class.html",
    "href": "adv_class.html",
    "title": "Advance Econometrics: Causal Effects",
    "section": "",
    "text": "Introduction: Even the longest journey starts somewhere"
  },
  {
    "objectID": "adv_class.html#part-i-the-tools",
    "href": "adv_class.html#part-i-the-tools",
    "title": "Advance Econometrics: Causal Effects",
    "section": "Part I: The tools",
    "text": "Part I: The tools\n\n1. Unveiling the Tapestry of Truth: The Grand Saga of Linear Regression and its Statistical Inference\nLinear Regression Model: Statistical Inference and Extensions: html or pdf\n\n\n2. Unleashing the Power of Infinite Flexibility: Exploring the Cosmos of Semi- and Non-Parametric Regression\nSemi- and Non-Parametric Regression: How Flexible is Flexible Enough?: html or pdf\n\n\n3. Beyond the Ordinary: Embarking on the Quest of Conditional Quantile Regressions\nConditional Quantile Regressions: Because No One is Average html or pdf\n\n\n4. Ascending the Ladder of Equality: Studying the Mysteries of Unconditional Quantile Regressions\nUnconditional Quantile Regressions: When We Care About Everyone html or pdf\n\n\n5. Breaking the Chains of Linearity: A Transcendent Expedition into NLS, IRLS, and MLE\nNLS, IRLS, and MLE: Going Truly Nonlinear html or pdf"
  },
  {
    "objectID": "adv_class.html#part-ii-the-methods",
    "href": "adv_class.html#part-ii-the-methods",
    "title": "Advance Econometrics: Causal Effects",
    "section": "Part II: The methods",
    "text": "Part II: The methods\n\n6. Chronicles of Fate: Exploring the Mystical What-Ifs in the Web of Causal Models\nPotential outcomes and Causal Models html or pdf\n\n\n7. The Mirror’s Embrace: Taming Unobservables with the Power of Fixed Effects\nPanel Data and Fixed Effects (Many FE) html or pdf\n\n\n8. The Summoner’s Call: Using Instrumental Variables to estimate LATE’s\nInstrumental Variables html or pdf\n\n\n9. The Blade of Equivalence: Unleashing the Art of Matching in the Realm of Divergence\nMatching and Re-weighting html or pdf\n\n\n10. Beyond the Discontinuity Veil: Crossing thresholds to Illuminate Secrets\nRegression Discontinuity Design html or pdf\n\n\n11. Twofold Wisdom: Unraveling Truths with the Dual Forces of Differences in Differences\nDifferences in Differences pdf\n\n\n12. Cosmic Recreations: Forging New Realities through Synthetic Control\nSynthetic Control html or pdf"
  },
  {
    "objectID": "imewld/chapter2.html",
    "href": "imewld/chapter2.html",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "",
    "text": "Model:\n\\[salary = \\beta_0 + \\beta_1 roe + u\\]\n\n\n\n\n\n\n\n\n\n\n\nfrause ceosal1, clear\nregress salary roe    \n\ndisplay \"Prediction: \" _b[_cons] \" +\" _b[roe] \"*30\" \"=\" _b[_cons] + _b[roe] *30\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =       209\n-------------+----------------------------------   F(1, 207)       =      2.77\n       Model |  5166419.04         1  5166419.04   Prob &gt; F        =    0.0978\n    Residual |   386566563       207  1867471.32   R-squared       =    0.0132\n-------------+----------------------------------   Adj R-squared   =    0.0084\n       Total |   391732982       208  1883331.64   Root MSE        =    1366.6\n\n------------------------------------------------------------------------------\n      salary | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         roe |   18.50119   11.12325     1.66   0.098    -3.428196    40.43057\n       _cons |   963.1913   213.2403     4.52   0.000     542.7902    1383.592\n------------------------------------------------------------------------------\nPrediction: 963.19134 +18.501186*30=1518.2269"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.3-ceo-salary-and-return-on-equity",
    "href": "imewld/chapter2.html#example-2.3-ceo-salary-and-return-on-equity",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "",
    "text": "Model:\n\\[salary = \\beta_0 + \\beta_1 roe + u\\]\n\n\n\n\n\n\n\n\n\n\n\nfrause ceosal1, clear\nregress salary roe    \n\ndisplay \"Prediction: \" _b[_cons] \" +\" _b[roe] \"*30\" \"=\" _b[_cons] + _b[roe] *30\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =       209\n-------------+----------------------------------   F(1, 207)       =      2.77\n       Model |  5166419.04         1  5166419.04   Prob &gt; F        =    0.0978\n    Residual |   386566563       207  1867471.32   R-squared       =    0.0132\n-------------+----------------------------------   Adj R-squared   =    0.0084\n       Total |   391732982       208  1883331.64   Root MSE        =    1366.6\n\n------------------------------------------------------------------------------\n      salary | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         roe |   18.50119   11.12325     1.66   0.098    -3.428196    40.43057\n       _cons |   963.1913   213.2403     4.52   0.000     542.7902    1383.592\n------------------------------------------------------------------------------\nPrediction: 963.19134 +18.501186*30=1518.2269"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.4-wage-and-education",
    "href": "imewld/chapter2.html#example-2.4-wage-and-education",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.4: Wage and Education",
    "text": "Example 2.4: Wage and Education\n\n\n\n\n\n\n\n\n\n\n\nfrause wage1, clear\nregress wage educ\n\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =    103.36\n       Model |  1179.73204         1  1179.73204   Prob &gt; F        =    0.0000\n    Residual |  5980.68225       524  11.4135158   R-squared       =    0.1648\n-------------+----------------------------------   Adj R-squared   =    0.1632\n       Total |  7160.41429       525  13.6388844   Root MSE        =    3.3784\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .5413593    .053248    10.17   0.000     .4367534    .6459651\n       _cons |  -.9048516   .6849678    -1.32   0.187    -2.250472    .4407687\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.5-voting-outcomes-and-campaign-expenditures",
    "href": "imewld/chapter2.html#example-2.5-voting-outcomes-and-campaign-expenditures",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.5: Voting Outcomes and Campaign Expenditures",
    "text": "Example 2.5: Voting Outcomes and Campaign Expenditures\n\n\n\n\n\n\n\n\n\n\n\nfrause vote1, clear\nregress votea sharea\n\n\n      Source |       SS           df       MS      Number of obs   =       173\n-------------+----------------------------------   F(1, 171)       =   1017.66\n       Model |  41486.2307         1  41486.2307   Prob &gt; F        =    0.0000\n    Residual |  6971.01783       171  40.7661862   R-squared       =    0.8561\n-------------+----------------------------------   Adj R-squared   =    0.8553\n       Total |  48457.2486       172  281.728189   Root MSE        =    6.3848\n\n------------------------------------------------------------------------------\n       votea | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      sharea |   .4638269   .0145397    31.90   0.000     .4351266    .4925272\n       _cons |   26.81221   .8872146    30.22   0.000     25.06091    28.56352\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.6-ceo-salary-and-return-on-equity",
    "href": "imewld/chapter2.html#example-2.6-ceo-salary-and-return-on-equity",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.6: CEO Salary and Return on Equity",
    "text": "Example 2.6: CEO Salary and Return on Equity\n\n\n\n\n\n\n\n\n\n\n\nfrause ceosal1, clear\nqui:regress salary roe  \npredict salaryhat, xb\npredict uhat , resid\n\nlist roe salary salaryhat uhat  in 1/10\n\n\n     +--------------------------------------+\n     |  roe   salary   salary~t        uhat |\n     |--------------------------------------|\n  1. | 14.1     1095   1224.058   -129.0581 |\n  2. | 10.9     1001   1164.854   -163.8543 |\n  3. | 23.5     1122   1397.969   -275.9692 |\n  4. |  5.9      578   1072.348   -494.3483 |\n  5. | 13.8     1368   1218.508    149.4923 |\n     |--------------------------------------|\n  6. |   20     1145   1333.215   -188.2151 |\n  7. | 16.4     1078   1266.611   -188.6108 |\n  8. | 16.3     1094   1264.761   -170.7607 |\n  9. | 10.5     1237   1157.454     79.5462 |\n 10. | 26.3      833   1449.773   -616.7725 |\n     +--------------------------------------+"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.7-wage-and-education",
    "href": "imewld/chapter2.html#example-2.7-wage-and-education",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.7: Wage and Education",
    "text": "Example 2.7: Wage and Education\n\n\n\n\n\n\n\n\n\n\n\nfrause wage1, clear\nqui:regress wage educ\nsum wage educ\ndisplay \"b0+b1*E(educ)= \" _b[_cons] + _b[educ]*r(mean)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        wage |        526    5.896103    3.693086        .53      24.98\n        educ |        526    12.56274    2.769022          0         18\nb0+b1*E(educ)= 5.8961027"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.8-ceo-salary-and-return-on-equity",
    "href": "imewld/chapter2.html#example-2.8-ceo-salary-and-return-on-equity",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.8: CEO Salary and Return on Equity",
    "text": "Example 2.8: CEO Salary and Return on Equity\n\nfrause ceosal1, clear\ngen one=1\nmata:y = st_data(., \"salary\")\nmata:x = st_data(., \"one roe\")\nmata:b = invsym(x'*x)*x'*y\nmata:yhat = x*b\nmata:uhat = y - yhat\nmata:sst = sum((y :- mean(y)):^2)\nmata:sse = sum((yhat :- mean(y)):^2)\nmata:ssr = sum((y :- yhat):^2)\nmata:rsq = 1 - ssr/sst;rsq\nmata:rsq = sse/sst;rsq\n\n  .0131886241\n  .0131886241"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.10-a-log-wage-model",
    "href": "imewld/chapter2.html#example-2.10-a-log-wage-model",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.10: a log wage model",
    "text": "Example 2.10: a log wage model\nModel\n\\[log(wage) = \\beta_0 + \\beta_1 educ + u\\]\n\n\n\n\n\n\n\n\n\n\n\nfrause wage1, clear\ngen logwage = log(wage)\nregress logwage educ\n\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =    119.58\n       Model |  27.5606288         1  27.5606288   Prob &gt; F        =    0.0000\n    Residual |  120.769123       524  .230475425   R-squared       =    0.1858\n-------------+----------------------------------   Adj R-squared   =    0.1843\n       Total |  148.329751       525   .28253286   Root MSE        =    .48008\n\n------------------------------------------------------------------------------\n     logwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0827444   .0075667    10.94   0.000     .0678796    .0976091\n       _cons |   .5837727   .0973358     6.00   0.000     .3925563    .7749891\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.11-ceo-salary-and-firms-sales",
    "href": "imewld/chapter2.html#example-2.11-ceo-salary-and-firms-sales",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.11: CEO Salary and Firms Sales",
    "text": "Example 2.11: CEO Salary and Firms Sales\nModel:\n\\[log(salary) = \\beta_0 + \\beta_1 log(sales) + u\\]\n\n\n\n\n\n\n\n\n\n\n\nfrause ceosal1, clear\ngen logsalary = log(salary)\ngen logsales = log(sales)\nreg logsalary logsales\n\n\n      Source |       SS           df       MS      Number of obs   =       209\n-------------+----------------------------------   F(1, 207)       =     55.30\n       Model |  14.0661688         1  14.0661688   Prob &gt; F        =    0.0000\n    Residual |  52.6559944       207  .254376785   R-squared       =    0.2108\n-------------+----------------------------------   Adj R-squared   =    0.2070\n       Total |  66.7221632       208  .320779631   Root MSE        =    .50436\n\n------------------------------------------------------------------------------\n   logsalary | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    logsales |   .2566717   .0345167     7.44   0.000     .1886224    .3247209\n       _cons |   4.821997   .2883396    16.72   0.000     4.253538    5.390455\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.14-evaluating-a-job-training-program",
    "href": "imewld/chapter2.html#example-2.14-evaluating-a-job-training-program",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.14: Evaluating a Job Training Program",
    "text": "Example 2.14: Evaluating a Job Training Program\n\n\n\n\n\n\n\n\n\n\n\nfrause jtrain2, clear\nreg re78 train\n\n\n      Source |       SS           df       MS      Number of obs   =       445\n-------------+----------------------------------   F(1, 443)       =      8.04\n       Model |  348.013451         1  348.013451   Prob &gt; F        =    0.0048\n    Residual |  19177.6432       443  43.2903909   R-squared       =    0.0178\n-------------+----------------------------------   Adj R-squared   =    0.0156\n       Total |  19525.6566       444  43.9767041   Root MSE        =    6.5795\n\n------------------------------------------------------------------------------\n        re78 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       train |   1.794343   .6328536     2.84   0.005     .5505748    3.038111\n       _cons |   4.554802    .408046    11.16   0.000     3.752856    5.356749\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter4.html",
    "href": "imewld/chapter4.html",
    "title": "Chapter 4: Multiple Regression Analysis: Inference",
    "section": "",
    "text": "Model:\n\\[log(wage)=\\beta_0 +\\beta_1 jc + \\beta_2 univ + \\beta_3 exper + u\\]\nHypothesis\n\\[H_0: \\beta_1 = \\beta_2 \\rightarrow \\beta_1 - \\beta_2 = 0\\] \\[H_1: \\beta_1 &lt; \\beta_2 \\rightarrow \\beta_1 - \\beta_2 &lt; 0\\]\n\nfrause twoyear, clear\nreg lwage jc univ exper\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =     6,763\n-------------+----------------------------------   F(3, 6759)      =    644.53\n       Model |  357.752575         3  119.250858   Prob &gt; F        =    0.0000\n    Residual |  1250.54352     6,759  .185019014   R-squared       =    0.2224\n-------------+----------------------------------   Adj R-squared   =    0.2221\n       Total |  1608.29609     6,762  .237843255   Root MSE        =    .43014\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          jc |   .0666967   .0068288     9.77   0.000     .0533101    .0800833\n        univ |   .0768762   .0023087    33.30   0.000     .0723504    .0814021\n       exper |   .0049442   .0001575    31.40   0.000     .0046355    .0052529\n       _cons |   1.472326   .0210602    69.91   0.000     1.431041     1.51361\n------------------------------------------------------------------------------\n\n\n\ntest jc = univ\n\n\n ( 1)  jc - univ = 0\n\n       F(  1,  6759) =    2.15\n            Prob &gt; F =    0.1422\n\n\nManual transformation:\n\\[\\theta = \\beta_1 - \\beta_2 \\rightarrow \\beta_1 = \\theta + \\beta_2 \\]\n\\[log(wage)=\\beta_0 +(\\theta + \\beta_2) jc + \\beta_2 univ + \\beta_3 exper + u\\] \\[log(wage)=\\beta_0 +\\theta jc + \\beta_2 (univ+jc) + \\beta_3 exper + u\\]\n\ngen univjc = univ + jc\nreg lwage jc univjc exper\n\n\n      Source |       SS           df       MS      Number of obs   =     6,763\n-------------+----------------------------------   F(3, 6759)      =    644.53\n       Model |  357.752575         3  119.250858   Prob &gt; F        =    0.0000\n    Residual |  1250.54352     6,759  .185019014   R-squared       =    0.2224\n-------------+----------------------------------   Adj R-squared   =    0.2221\n       Total |  1608.29609     6,762  .237843255   Root MSE        =    .43014\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          jc |  -.0101795   .0069359    -1.47   0.142    -.0237761     .003417\n      univjc |   .0768762   .0023087    33.30   0.000     .0723504    .0814021\n       exper |   .0049442   .0001575    31.40   0.000     .0046355    .0052529\n       _cons |   1.472326   .0210602    69.91   0.000     1.431041     1.51361\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter4.html#testing-hypotheses-about-single-linear-combinations-of-the-parameters",
    "href": "imewld/chapter4.html#testing-hypotheses-about-single-linear-combinations-of-the-parameters",
    "title": "Chapter 4: Multiple Regression Analysis: Inference",
    "section": "",
    "text": "Model:\n\\[log(wage)=\\beta_0 +\\beta_1 jc + \\beta_2 univ + \\beta_3 exper + u\\]\nHypothesis\n\\[H_0: \\beta_1 = \\beta_2 \\rightarrow \\beta_1 - \\beta_2 = 0\\] \\[H_1: \\beta_1 &lt; \\beta_2 \\rightarrow \\beta_1 - \\beta_2 &lt; 0\\]\n\nfrause twoyear, clear\nreg lwage jc univ exper\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =     6,763\n-------------+----------------------------------   F(3, 6759)      =    644.53\n       Model |  357.752575         3  119.250858   Prob &gt; F        =    0.0000\n    Residual |  1250.54352     6,759  .185019014   R-squared       =    0.2224\n-------------+----------------------------------   Adj R-squared   =    0.2221\n       Total |  1608.29609     6,762  .237843255   Root MSE        =    .43014\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          jc |   .0666967   .0068288     9.77   0.000     .0533101    .0800833\n        univ |   .0768762   .0023087    33.30   0.000     .0723504    .0814021\n       exper |   .0049442   .0001575    31.40   0.000     .0046355    .0052529\n       _cons |   1.472326   .0210602    69.91   0.000     1.431041     1.51361\n------------------------------------------------------------------------------\n\n\n\ntest jc = univ\n\n\n ( 1)  jc - univ = 0\n\n       F(  1,  6759) =    2.15\n            Prob &gt; F =    0.1422\n\n\nManual transformation:\n\\[\\theta = \\beta_1 - \\beta_2 \\rightarrow \\beta_1 = \\theta + \\beta_2 \\]\n\\[log(wage)=\\beta_0 +(\\theta + \\beta_2) jc + \\beta_2 univ + \\beta_3 exper + u\\] \\[log(wage)=\\beta_0 +\\theta jc + \\beta_2 (univ+jc) + \\beta_3 exper + u\\]\n\ngen univjc = univ + jc\nreg lwage jc univjc exper\n\n\n      Source |       SS           df       MS      Number of obs   =     6,763\n-------------+----------------------------------   F(3, 6759)      =    644.53\n       Model |  357.752575         3  119.250858   Prob &gt; F        =    0.0000\n    Residual |  1250.54352     6,759  .185019014   R-squared       =    0.2224\n-------------+----------------------------------   Adj R-squared   =    0.2221\n       Total |  1608.29609     6,762  .237843255   Root MSE        =    .43014\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          jc |  -.0101795   .0069359    -1.47   0.142    -.0237761     .003417\n      univjc |   .0768762   .0023087    33.30   0.000     .0723504    .0814021\n       exper |   .0049442   .0001575    31.40   0.000     .0046355    .0052529\n       _cons |   1.472326   .0210602    69.91   0.000     1.431041     1.51361\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "mathref/math_1.html",
    "href": "mathref/math_1.html",
    "title": "Math Refresher: Basic Calculus",
    "section": "",
    "text": "This is a refresher on basic calculus. It is not meant to be a substitute for a full course on calculus, but rather a quick review of the basic concepts and techniques that will be used in this semester."
  },
  {
    "objectID": "mathref/math_1.html#introduction",
    "href": "mathref/math_1.html#introduction",
    "title": "Math Refresher: Basic Calculus",
    "section": "",
    "text": "This is a refresher on basic calculus. It is not meant to be a substitute for a full course on calculus, but rather a quick review of the basic concepts and techniques that will be used in this semester."
  },
  {
    "objectID": "mathref/math_1.html#limits",
    "href": "mathref/math_1.html#limits",
    "title": "Math Refresher: Basic Calculus",
    "section": "Limits",
    "text": "Limits\nThe limit of a function \\(f(x)\\) as \\(x\\) approaches \\(a\\) is the value that \\(f(x)\\) approaches as \\(x\\) gets closer and closer to \\(a\\). We write this as:\n\\[\\lim_{x \\to a} f(x) = L\\]\nIn this case, the limit of the function \\(f(x)\\) as \\(x\\) approaches \\(a\\) is \\(L\\). For example, consider the function \\(f(x) = x^2\\). The limit of \\(f(x)\\) as \\(x\\) approaches \\(2\\) is \\(4\\):"
  },
  {
    "objectID": "mathref/math_1.html#limits-to-derivatives",
    "href": "mathref/math_1.html#limits-to-derivatives",
    "title": "Math Refresher: Basic Calculus",
    "section": "Limits to Derivatives",
    "text": "Limits to Derivatives\nLimits can also be used to estimate derivatives. The derivative of a function \\(f(x)\\) is the slope of the function at a given point. The derivative of \\(f(x)\\) at \\(x = a\\) is written as \\(f'(a)\\). The derivative of \\(f(x)\\) is defined as:\n\\[f'(a) = \\lim_{h \\to 0} \\frac{f(a+h) - f(a)}{h}\\]\nIn other words, the deriviative is the slope of a function at a particular point \\(a\\). This can be proxied using derivatives, by choosing a very small value for \\(h\\).\nFor example, consider the function \\(f(x) = x^2\\). The derivative of \\(f(x)\\) at \\(x = a\\) is:\n\\[\\begin{aligned}\nf'(a) &= \\lim_{h \\to 0} \\frac{f(a+h) - f(a)}{h} \\\\\n&=\\lim_{h \\to 0} \\frac{(a+h)^2 - (a)^2}{h} \\\\\n&= \\lim_{h \\to 0} \\frac{a^2 + 2ah + h^2 - a^2}{h} \\\\\n&= \\lim_{h \\to 0} 2a + h= 2a\n\\end{aligned}\n\\]\nIf anything else fails, one can always rely on numerical differentiation."
  },
  {
    "objectID": "mathref/math_1.html#derivative-of-common-functions",
    "href": "mathref/math_1.html#derivative-of-common-functions",
    "title": "Math Refresher: Basic Calculus",
    "section": "Derivative of common functions",
    "text": "Derivative of common functions\nFor most common functions, the derivative can be calculated using the following rules:\n\nThe derivative of a constant is zero\nThe derivative of \\(x^n\\) is \\(nx^{n-1}\\)\nThe derivative of \\(ln(x)\\) is \\(\\frac{1}{x}\\)\nThe derivative of \\(e^x\\) is \\(e^x\\)\nThe derivative of \\(a^x\\) is \\(a^x \\ln a\\)\n\nThere are other rules for derivatives, but these are the ones that will be used most often."
  },
  {
    "objectID": "mathref/math_1.html#derivative-of-composite-functions",
    "href": "mathref/math_1.html#derivative-of-composite-functions",
    "title": "Math Refresher: Basic Calculus",
    "section": "Derivative of composite functions",
    "text": "Derivative of composite functions\nThe derivative of a composite function \\(f(g(x))\\) is given by the chain rule:\n\\[\\frac{d}{dx} f(g(x)) = f'(g(x)) g'(x)\\]\nFor example, consider the function \\(f(x) = \\ln(x^2)\\). The derivative of \\(f(x)\\) is:\n\\[\\begin{aligned}\n\\frac{d}{dx} \\ln(x^2) &= \\frac{1}{x^2} \\frac{d}{dx} x^2 \\\\\n&= \\frac{1}{x^2} 2x \\\\\n&= \\frac{2}{x}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "mathref/math_1.html#derivative-of-sums-and-products",
    "href": "mathref/math_1.html#derivative-of-sums-and-products",
    "title": "Math Refresher: Basic Calculus",
    "section": "Derivative of sums and products",
    "text": "Derivative of sums and products\nThe derivative of a sum of functions is the sum of the derivatives of the functions.\n\\[\\frac{d}{dx} (f(x) + g(x)) = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x)\\]\nThe derivative of a product of functions is given by the product rule:\n\\[\\frac{d}{dx} (f(x) g(x)) = f'(x) g(x) + f(x) g'(x)\\]\nThe derivative of a quotient of functions is given by the quotient rule:\n\\[\\frac{d}{dx} \\frac{f(x)}{g(x)} = \\frac{f'(x) g(x) - f(x) g'(x)}{g(x)^2}\\]\nWhich is a special case of th product rule."
  },
  {
    "objectID": "mathref/math_1.html#optimization-with-multiple-variables",
    "href": "mathref/math_1.html#optimization-with-multiple-variables",
    "title": "Math Refresher: Basic Calculus",
    "section": "Optimization with multiple variables",
    "text": "Optimization with multiple variables\nWhen considering multiple variables, we also need to rely on the first and second order conditions to find minimum and maximum values. Consider a function \\(f(x,y)\\). The first order conditions are:\n\\[\\begin{aligned}\n\\frac{\\partial}{\\partial x} f(x,y) &= 0 \\\\\n\\frac{\\partial}{\\partial y} f(x,y) &= 0\n\\end{aligned}\n\\]\nThis conditions now say that, in the direction of \\(x\\) and \\(y\\), the function \\(f(x,y)\\) is not changing anymore. Thus we have a potential maximum or minimum. Now, to identify a minimum, we need second order conditions to be:\n\\[\\begin{aligned}\nH=\\begin{bmatrix}\nf_{xx} & f_{xy} \\\\\nf_{xy} & f_{yy} \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\]\nIf \\(Det(H)&gt;0\\) and \\(f_{xx}&gt;0\\) then we have a minimum. If \\(Det(H)&gt;0\\) and \\(f_{xx}&lt;0\\) then we have a maximum. If \\(Det(H)&lt;0\\) then we have a saddle point. And if \\(Det(H)=0\\) then we have an inconclusive result."
  },
  {
    "objectID": "mathref/math_1.html#optimization-with-constraints",
    "href": "mathref/math_1.html#optimization-with-constraints",
    "title": "Math Refresher: Basic Calculus",
    "section": "Optimization with constraints",
    "text": "Optimization with constraints\nWhen optimizing a function with constraints, we can use the method of Lagrange multipliers. Consider a function \\(f(x,y)\\) subject to the constraint \\(g(x,y) = z\\). The Lagrangian is:\n\\[\\begin{aligned}\nL(x,y,\\lambda) = f(x,y) + \\lambda (z - g(x,y))\n\\end{aligned}\n\\]\nNotice that the Lagrangian is the function \\(f(x,y)\\) plus the constraint \\(g(x,y)\\) multiplied by a constant \\(\\lambda\\). The constant \\(\\lambda\\) is called the Lagrange multiplier. The constrain is written as the difference between the constant \\(z\\) and the function \\(g(x,y)\\). The Lagrangian is then optimized with respect to \\(x\\), \\(y\\), and \\(\\lambda\\). This are the equivalent of the first order conditions:\n\\[\\begin{aligned}\n\\frac{\\partial}{\\partial x} L(x,y,\\lambda) &= 0 \\\\\n\\frac{\\partial}{\\partial y} L(x,y,\\lambda) &= 0 \\\\\n\\frac{\\partial}{\\partial \\lambda} L(x,y,\\lambda) &= z - g(x,y)=0\n\\end{aligned}\n\\]\nThe last condition is the constraint, and it implies that the constraint must be satisfied. The second order conditions are the same as before."
  },
  {
    "objectID": "mathref/math_3.html",
    "href": "mathref/math_3.html",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "",
    "text": "A random variable is a variable whose value is determined by the outcome of a random experiment. For example, if we toss a coin, the outcome is random, but the possible values of \\(X\\) are 0 and 1. If we roll a die, the outcome is random with possible values 1, 2, 3, 4, 5, and 6.\nThere are two kinds of random variables:\n\nDiscrete random variables can only take on a finite number of values. For example, the number of heads in 10 coin tosses is a discrete random variable.\nContinuous random variables can take on any value in a range. For example, the height of a randomly selected person is a continuous random variable.\n\nIf \\(X\\) is discrete random variable, then \\(P(X=c)\\) is the probability that \\(X\\) takes on the value \\(c\\). It can be any value between 0 and 1.\nBy definition, the sum of all probabilities for all feasible values of \\(X\\) is 1. That is, \\(\\sum_{c} P(X=c)=1\\).\nIf \\(X\\) is continuous random variable, then \\(P(X=c)=0\\) for any value \\(c\\). The probability to observe a particular number is zero. Instead, when using continuous data, we focus on the probability of observing a value in a range. For example, \\(P(1.7 \\leq X \\leq 1.8)\\) is the probability that \\(X\\) is between 1.7 and 1.8, which can be any value between 0 and 1."
  },
  {
    "objectID": "mathref/math_3.html#random-variables",
    "href": "mathref/math_3.html#random-variables",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "",
    "text": "A random variable is a variable whose value is determined by the outcome of a random experiment. For example, if we toss a coin, the outcome is random, but the possible values of \\(X\\) are 0 and 1. If we roll a die, the outcome is random with possible values 1, 2, 3, 4, 5, and 6.\nThere are two kinds of random variables:\n\nDiscrete random variables can only take on a finite number of values. For example, the number of heads in 10 coin tosses is a discrete random variable.\nContinuous random variables can take on any value in a range. For example, the height of a randomly selected person is a continuous random variable.\n\nIf \\(X\\) is discrete random variable, then \\(P(X=c)\\) is the probability that \\(X\\) takes on the value \\(c\\). It can be any value between 0 and 1.\nBy definition, the sum of all probabilities for all feasible values of \\(X\\) is 1. That is, \\(\\sum_{c} P(X=c)=1\\).\nIf \\(X\\) is continuous random variable, then \\(P(X=c)=0\\) for any value \\(c\\). The probability to observe a particular number is zero. Instead, when using continuous data, we focus on the probability of observing a value in a range. For example, \\(P(1.7 \\leq X \\leq 1.8)\\) is the probability that \\(X\\) is between 1.7 and 1.8, which can be any value between 0 and 1."
  },
  {
    "objectID": "mathref/math_3.html#probability-distributions",
    "href": "mathref/math_3.html#probability-distributions",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nA probability distribution is a function that assigns probabilities to the values of a random variable. For discrete random variables, we can use a table to describe the probability distribution. For example, the probability distribution of the number of heads in 5 coin tosses is:\n\n\n\nNumber of heads\nProbability\n\n\n\n\n0\n0.03125\n\n\n1\n0.15625\n\n\n2\n0.3125\n\n\n3\n0.3125\n\n\n4\n0.15625\n\n\n5\n0.03125\n\n\n\nIn this case, the sum of all probabilities is 1.\nFor continuous random variables, we can use a function to describe the probability distribution. For example, we can say that the probability distribution of the height of a randomly selected person is:\n\\[f(x)\\]\nThis function has important properties:\n\n\\(f(x) \\geq 0\\) for all \\(x\\).\n\\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\).\n\\(P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) dx\\).\n\\(P(X \\leq a) + P(X &gt; a) = 1\\).\n\\(P(a \\leq X \\leq b) = P(X &lt; b) - P(X &lt; a)\\)."
  },
  {
    "objectID": "mathref/math_3.html#joint-probability-distributions",
    "href": "mathref/math_3.html#joint-probability-distributions",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Joint Probability Distributions",
    "text": "Joint Probability Distributions\nThe joint probability distribution of \\(X\\) and \\(Y\\) is a function that assigns probabilities to the values of \\(X\\) and \\(Y\\). For discrete random variables, we can use a table to describe the joint probability distribution. For continuous variables, it must be the case that:\n\\[\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x,y) dx dy = 1\\]"
  },
  {
    "objectID": "mathref/math_3.html#marginal-probability-distributions",
    "href": "mathref/math_3.html#marginal-probability-distributions",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Marginal Probability Distributions",
    "text": "Marginal Probability Distributions\nThe marginal probability distribution of \\(X\\) is the probability distribution of \\(X\\) ignoring the values of \\(Y\\). This can be expressed as:\n\\[P(x) = \\sum_{z=-\\infty}^{\\infty} P(x,y=z)\\]\nit still must be the case that\n\\[ \\sum_{w=-\\infty}^{\\infty}\\sum_{z=-\\infty}^{\\infty} P(x=w,y=z)=1\\]\nFor continuous random variables, we have the following \\[\\int_{-\\infty}^{\\infty} f(x) dx = 1\\]\nwhere \\(f(x)\\) is the marginal probability distribution of \\(X\\). What is left after we “integrate out” \\(Y\\) is the marginal probability distribution of \\(X\\).\n\\[f(x) = \\int_{-\\infty}^{\\infty} f(x,y) dy\\]"
  },
  {
    "objectID": "mathref/math_3.html#independence",
    "href": "mathref/math_3.html#independence",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Independence",
    "text": "Independence\nTwo random variables \\(X\\) and \\(Y\\) are independent if and only if:\n\\[P(x,y) = P(x)P(y) or f(x,y)=f(x)*f(y)\\]"
  },
  {
    "objectID": "mathref/math_3.html#conditional-probability",
    "href": "mathref/math_3.html#conditional-probability",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nThe conditional probability of \\(X\\) given \\(Y\\) is:\n\\[P(x|y) = \\frac{P(x,y)}{P(y)}\\]\nor, the conditional probabilty density function:\n\\[f(x|y) = \\frac{f(x,y)}{f(y)}\\]\nAnd if \\(X\\) and \\(Y\\) are independent, then:\n\\(P(x|y) = P(x)\\) or \\(f(x|y) = f(x)\\)."
  },
  {
    "objectID": "mathref/math_3.html#mean-and-variance",
    "href": "mathref/math_3.html#mean-and-variance",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Mean, and variance",
    "text": "Mean, and variance\nThe mean of a random variable \\(X\\) is:\n\\[E(X) = \\sum_{x} xP(x)\\] or \\[E(X) = \\int_{-\\infty}^{\\infty} xf(x) dx\\]\nWhich is a weighted sum of all possible values of \\(X\\), and where the weights are the probabilities (or densities) of each value. It can also be written or referred as:\n\\[E(X), \\mu_x , \\bar x\\]\nThis measure is also called the expected value of \\(X\\), and proviveds a measure of the “center” of the distribution of \\(X\\). It can be very sensitive to outliers.\nThe variance of a random variable \\(X\\) is:\n\\[Var(X) = E[(X-E(X))^2] \\]\n\\[Var(X) = \\sum_{x} (X-E(X))^2 P(x) \\] or\n\\[Var(X) = \\int_{x} (X-E(X))^2 f(x) dx \\]\nWhich is the expected value of the squared difference between \\(X\\) and its mean. It provides a measure of average the “spread” of the distribution of \\(X\\).\nIt could also be defined as follows:\n\\[\\sigma^2_x = Var(x) = E(X^2) - [E(X)]^2\\]\nThere are other measures that can be used to characterize a distribution, such as the median, the mode, the skewness, and the kurtosis. They are defined as follows:\n\nThe median is the value of \\(X\\) such that \\(P(X \\leq x) = 0.5\\).\nThe mode is the value of \\(X\\) that maximizes \\(P(X=x)\\).\nThe skewness is a measure of the asymmetry of the distribution of \\(X\\). It is defined as:\n\n\\[\\frac{E[(X-E(X))^3]}{[Var(X)]^{3/2}}\\]\n\nThe kurtosis is a measure of the “peakedness” of the distribution of \\(X\\). It is defined as:\n\n\\[\\frac{E[(X-E(X))^4]}{[Var(X)]^{2}}\\]\n\nThe quantiles of a distribution are values that divide the distribution into equal parts. For example, the 0.25 quantile is the value of \\(X\\) such that \\(P(X \\leq x) = 0.25\\).\n\nFor a normal distribution, the mean, median, and mode are all equal. The skewness is 0, and the kurtosis is 3."
  },
  {
    "objectID": "mathref/math_3.html#covariance-and-correlation",
    "href": "mathref/math_3.html#covariance-and-correlation",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Covariance and Correlation",
    "text": "Covariance and Correlation\nThe covariance of two random variables \\(X\\) and \\(Y\\) is:\n\\[Cov(X,Y) = E[(X-E(X))(Y-E(Y))]\\]\n\\[Cov(X,Y) = \\sum_{x}\\sum_{y} (x-E(X))(y-E(Y))P(x,y)\\]\n\\[Cov(X,Y) = \\int_{x}\\int_{y} (x-E(X))(y-E(Y))f(x,y)\\]\nThe covariance measures the linear association between \\(X\\) and \\(Y\\). If \\(X\\) and \\(Y\\) are independent, then \\(Cov(X,Y)=0\\). However, if \\(Cov(X,Y)=0\\), then \\(X\\) and \\(Y\\) are not necessarily independent. For example \\(y=(x-E(X))^2\\) and \\(x\\) are not independent, but \\(Cov(y,x)=0\\).\nThis measure is scale dependent. For example, if we measure \\(X\\) in meters, and \\(Y\\) in centimeters, then \\(Cov(X,Y)\\) will be 100 times larger than if we measure \\(X\\) in meters and \\(Y\\) in kilometers.\nAn alternative measure of association is the correlation coefficient, which is defined as:\n\\[Corr(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}\\] \\[\\rho_{X,Y} = \\frac{\\sigma_{X,Y}}{\\sigma_x \\sigma_y}\\]\nThis statistics is always between -1 and 1, regardless of the scale of \\(x\\) or \\(y\\)."
  },
  {
    "objectID": "mathref/math_3.html#propeties-of-mean-variance-and-covariance",
    "href": "mathref/math_3.html#propeties-of-mean-variance-and-covariance",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Propeties of Mean, Variance and Covariance",
    "text": "Propeties of Mean, Variance and Covariance\nConsider two random variables \\(X\\) and \\(Y\\), and let \\(a\\), \\(b\\), \\(c\\) and \\(d\\) be constants. Then:\n\n\\(Var(aX+b) = a^2Var(X)\\)\n\\(Cov(aX+b,cY+d) = acCov(X,Y)\\)\n\\(Var(aX+bY) = a^2Var(X) + b^2Var(Y) + 2abCov(X,Y)\\)\n\\(Cov(X,Y) = E(XY) - E(X)E(Y)\\)\n\\(Cov(X,X) = Var(X)\\)\n\nFor the mean:\n\n\\(E(aX+b) = aE(X)+b\\)\n\\(E(aX+bY) = aE(X) + bE(Y)\\)"
  },
  {
    "objectID": "mathref/math_3.html#some-useful-distributions",
    "href": "mathref/math_3.html#some-useful-distributions",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Some useful distributions",
    "text": "Some useful distributions\n\nDiscrete distributions\n\nBernoulli distribution: \\(X \\sim Bernoulli(p)\\), where \\(p=P(X=1)\\) and \\(1-p=P(X=0)\\). \\(E(X)=p\\) and variance \\(Var(X)=p(1-p)\\). Flip a coin with probability \\(p\\) of getting heads.\nBinomial distribution: \\(X \\sim Binomial(n,p)\\), where \\(p=P(X=1)\\) and \\(1-p=P(X=0)\\). \\(E(x)=np\\) and \\(Var(X)=np(1-p)\\). The binomial distribution is the distribution of the number of successes in \\(n\\) independent Bernoulli trials.\nPoisson distribution: \\(X \\sim Poisson(\\lambda)\\), where \\(\\lambda=E(X)=Var(x)\\). Typically used for counts. For example, the number of customers arriving at a store in a given hour.\n\n\n\nContinuous distributions\n\nUniform distribution: \\(X \\sim Uniform(a,b)\\), where \\(f(x)=\\frac{1}{b-a}\\) for \\(a \\leq x \\leq b\\), and \\(f(x)=0\\) otherwise. \\(E(X)=\\frac{a+b}{2}\\) and \\(Var(X)=\\frac{(b-a)^2}{12}\\). Time between bus arrivals.\nNormal distribution: \\(X \\sim Normal(\\mu,\\sigma^2)\\), where \\(f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\). \\(E(X)=\\mu\\) and \\(Var(X)=\\sigma^2\\). For example, the height of a randomly selected person.\nt-distribution: \\(X \\sim t(\\nu)\\), where \\(f(x)=\\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\sqrt{\\nu\\pi}\\Gamma(\\frac{\\nu}{2})}(1+\\frac{x^2}{\\nu})^{-\\frac{\\nu+1}{2}}\\). \\(E(X)=0\\) if \\(\\nu&gt;1\\), and \\(Var(X)=\\frac{\\nu}{\\nu-2}\\) if \\(\\nu&gt;2\\). For example, the distribution of the sample mean of a small sample from a normal distribution.\nAlternatively. \\(X \\sim t(\\nu)\\), where \\(X=\\frac{Z}{\\sqrt{V/\\nu}}\\), where \\(Z \\sim Normal(0,1)\\) and \\(V \\sim \\chi^2(\\nu)\\), and \\(Z\\) and \\(V\\) are independent.\nChi-squared distribution: \\(X \\sim \\chi^2(\\nu)\\), where \\(f(x)=\\frac{1}{2^{\\nu/2}\\Gamma(\\nu/2)}x^{\\nu/2-1}e^{-x/2}\\). \\(E(X)=\\nu\\) and \\(Var(X)=2\\nu\\).\nAlternatively, \\(X \\sim \\chi^2(\\nu)\\), where \\(X=Z_1^2+Z_2^2+...+Z_\\nu^2\\), where \\(Z_i \\sim Normal(0,1)\\), and \\(Z_1\\), \\(Z_2\\), …, \\(Z_\\nu\\) are independent.\nF-distribution:\n\\(X \\sim F(\\nu_1,\\nu_2)\\), where \\(f(x)=\\frac{\\Gamma(\\frac{\\nu_1+\\nu_2}{2})}{\\Gamma(\\frac{\\nu_1}{2})\\Gamma(\\frac{\\nu_2}{2})}(\\frac{\\nu_1}{\\nu_2})^{\\nu_1/2}x^{\\nu_1/2-1}(1+\\frac{\\nu_1}{\\nu_2}x)^{-(\\nu_1+\\nu_2)/2}\\). \\(E(X)=\\frac{\\nu_2}{\\nu_2-2}\\) if \\(\\nu_2&gt;2\\), and \\(Var(X)=\\frac{2\\nu_2^2(\\nu_1+\\nu_2-2)}{\\nu_1(\\nu_2-2)^2(\\nu_2-4)}\\) if \\(\\nu_2&gt;4\\).\nAlternatively, \\(X \\sim F(\\nu_1,\\nu_2)\\), where \\(X=\\frac{V_1/\\nu_1}{V_2/\\nu_2}\\), where \\(V_1 \\sim \\chi^2(\\nu_1)\\) and \\(V_2 \\sim \\chi^2(\\nu_2)\\), and \\(V_1\\) and \\(V_2\\) are independent."
  },
  {
    "objectID": "quarto/regress.html",
    "href": "quarto/regress.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "(1)\n(2)\n(3)\n(4)\n\n\n\n\nyears of education\n0.0885***\n0.0794***\n0.0554***\n0.0854***\n\n\n\n(0.00519)\n(0.00522)\n(0.00613)\n(0.00876)\n\n\nyears of work\n0.0153***\n0.00399*\n-0.00483*\n0.00874*\n\n\nexperience\n(0.00126)\n(0.00188)\n(0.00203)\n(0.00356)\n\n\nyears of job tenure\n\n0.00407*\n-0.000553\n0.00150\n\n\n\n\n(0.00196)\n(0.00212)\n(0.00368)\n\n\nage of respondent\n\n0.0114***\n0.0249***\n0.00576*\n\n\n\n\n(0.00174)\n(0.00224)\n(0.00275)\n\n\nConstant\n2.136***\n1.915***\n1.903***\n1.965***\n\n\n\n(0.0654)\n(0.0727)\n(0.0784)\n(0.126)\n\n\nObservations\n1434\n1434\n751\n683"
  },
  {
    "objectID": "quarto/table1.html",
    "href": "quarto/table1.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Male\nFemale\n\n\n\n\nN\n759 (46.1%)\n888 (53.9%)\n\n\nlog hourly wages\n3.440 (0.479)\n3.267 (0.570)\n\n\nyears of education\n11.800 (2.444)\n11.060 (2.260)\n\n\nyears of work experience\n14.077 (11.180)\n12.138 (8.327)\n\n\nyears of job tenure\n9.003 (9.061)\n6.605 (6.727)\n\n\nage of respondent\n38.516 (11.341)\n39.884 (10.727)\n\n\nMarital Status\n\n\n\n\nSingle\n297 (39.1%)\n268 (30.2%)\n\n\nMarried\n397 (52.3%)\n465 (52.4%)\n\n\nDivorced\n65 (8.6%)\n155 (17.5%)"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "The goal of the class is for you to become familiar and proficient with some essential tools that are used in most empirical analysis.\nWhile learning to implement all the methods we cover by hand is a great excercise to learn what they do, and how they work, it may not be a feasible practice in most real-world work, unless you decide to follow that path (Econometrics/applied Econometrics).\nFor this purpose, the main software we will use in this class (as evidence from all the code shared in the slides) its Stata. A self contained program that is yet flexible enough to add custom add on programs/commands.\nNevertheless, if you are new to Stata, there are quite few resources you may want to look into using this software"
  },
  {
    "objectID": "resources.html#stata",
    "href": "resources.html#stata",
    "title": "Resources",
    "section": "Stata",
    "text": "Stata\n\nStata Free-Webinars: https://www.stata.com/training/webinar/\nStata Video-Tutorials: https://www.stata.com/links/video-tutorials/\nGeneral Learning resources: https://www.stata.com/links/resources-for-learning-stata/\nExcellent Stata tutorial for beginners: https://grodri.github.io/stata/index\nOur own Tutorial! Stata Basics\n\nBut of course, Stata is not free. There are other resources you may want to explore, if you are interested in doing econometric analysis, but no longer have access to Stata. These are R, Python and Julia."
  },
  {
    "objectID": "resources.html#r-julia-python",
    "href": "resources.html#r-julia-python",
    "title": "Resources",
    "section": "R, Julia, Python",
    "text": "R, Julia, Python\nThese software are free, but usually require add-ons from different sources to estimate specialized models. They also have a steep, or rather steep-er (than Stata) learning curve. However, it is smart to learn other languages, at least to implement basic analysis. One resource you may find very convinient is the following:\n\nR, Python, Julia: http://www.upfie.net/\n\nThis site and its author(s) have put together a set of companion books to go along with the Textbook “Introductory Econometrics: A Modern Approach”. These books are rather inexpensive, providing some of the authors own insights, with full code in all three languages, that replicate the examples in the textbook.\n\nExample Codes: http://www.upfie.net/code.html\n\nThe authors also suggest other resources that could be of interest\n\nFurther Resources: http://www.upfie.net/links.html"
  },
  {
    "objectID": "resources.html#quarto",
    "href": "resources.html#quarto",
    "title": "Resources",
    "section": "Quarto",
    "text": "Quarto\nQuarto is not a programming language. Rather an interpreter that converts plain text to nicely formating documents, presentations, websites, etc. This site, for instance, was built using Quarto.\nBecause of this, I’m encouraging the use of Quarto, combined with nbstata/python, to produce answers to ALL homeworks or group works. So it will be easy to check and cross check your work with the code.\nTo use this, you need to have R-Studio here, or Visual Studio Code here with Quarto plug-in (if you use VSC) in your computers. You will also need python and nbstata.\nA good place to start learning how to use Quarto for dynamic documents its here (for R-studio) or here (for VCS).\nI also have a small example using Quarto with Stata here.\nTry it on, and let me know if you have any problems."
  },
  {
    "objectID": "rmethods/10_pooldata.html#pooling-data-together-cross-section-and-panel-data",
    "href": "rmethods/10_pooldata.html#pooling-data-together-cross-section-and-panel-data",
    "title": "Pool Cross-section and Panel Data",
    "section": "Pooling Data together: Cross-section and Panel Data",
    "text": "Pooling Data together: Cross-section and Panel Data\n\nUp to this point, we have cover the analysis of cross-section data.\n\nMany individuals at a single point in time.\n\nTowards the end of the semester, We will also cover the analysis of time series data.\n\nA single individual across time.\n\nToday, we will cover the analysis of panel data and repeated crossection: Many individuals across time.\nThis type of data, also known as longitudinal data, has advantages over crossection, as it provides more information that helps dealing with the unknown of \\(e\\).\nAnd its often the only way to answer certain questions."
  },
  {
    "objectID": "rmethods/10_pooldata.html#pooling-independent-crossections",
    "href": "rmethods/10_pooldata.html#pooling-independent-crossections",
    "title": "Pool Cross-section and Panel Data",
    "section": "Pooling independent crossections",
    "text": "Pooling independent crossections\n\nWe first consider the case of independent crossections.\n\nWe have access to surveys that may be collected regularly. (Household budget surveys)\nWe assume that individuals across this surveys are independent from each other (no panel structure).\n\nThis scenario is typically used for increasing sample-sizes and thus power of analysis (larger N smaller SE)\nOnly minor considerations are needed when analyzing this type of data.\n\nWe need to account for the fact Data comes from different years. This can be done by including year dummies.\nMay need to Standardize variables to make them comparable across years. (inflation adjustments, etc.)"
  },
  {
    "objectID": "rmethods/10_pooldata.html#example",
    "href": "rmethods/10_pooldata.html#example",
    "title": "Pool Cross-section and Panel Data",
    "section": "Example",
    "text": "Example\nLets use the data fertil1 to estimate the changes in fertility rates across time. This data comes from the General Social Survey.\n\n\nCode\nfrause fertil1, clear\nregress kids educ age agesq black east northcen west farm othrural town smcity i.year, robust  \n\n\n\n\n\n\nLinear regression                               Number of obs     =      1,129\n                                                F(17, 1111)       =      10.19\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.1295\n                                                Root MSE          =     1.5548\n\n------------------------------------------------------------------------------\n             |               Robust\n        kids | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |  -.1284268    .021146    -6.07   0.000    -.1699175   -.0869362\n         age |   .5321346   .1389371     3.83   0.000     .2595258    .8047433\n       agesq |   -.005804   .0015791    -3.68   0.000    -.0089024   -.0027056\n       black |   1.075658   .2013188     5.34   0.000     .6806496    1.470666\n        east |    .217324    .127466     1.70   0.088    -.0327773    .4674252\n    northcen |    .363114   .1167013     3.11   0.002     .1341342    .5920939\n        west |   .1976032   .1626813     1.21   0.225     -.121594    .5168003\n        farm |  -.0525575   .1460837    -0.36   0.719    -.3391886    .2340736\n    othrural |  -.1628537   .1808546    -0.90   0.368    -.5177087    .1920014\n        town |   .0843532   .1284759     0.66   0.512    -.1677295    .3364359\n      smcity |   .2118791   .1539645     1.38   0.169    -.0902149    .5139731\n             |\n        year |\n         74  |   .2681825   .1875121     1.43   0.153    -.0997353    .6361003\n         76  |  -.0973795   .1999339    -0.49   0.626    -.4896701    .2949112\n         78  |  -.0686665   .1977154    -0.35   0.728    -.4566042    .3192713\n         80  |  -.0713053   .1936553    -0.37   0.713    -.4512767    .3086661\n         82  |  -.5224842   .1879305    -2.78   0.006    -.8912228   -.1537456\n         84  |  -.5451661   .1859289    -2.93   0.003    -.9099776   -.1803547\n             |\n       _cons |  -7.742457   3.070656    -2.52   0.012     -13.7674   -1.717518\n------------------------------------------------------------------------------\n\n\n\nThis allow us to see how fertility rates have changed across time.\nOne could even interact the year dummies with other variables to see how the effect of other variables have changed across time.\n\n\n\nCode\nfrause cps78_85, clear\nregress lwage i.year##c.(educ i.female) exper expersq union, robust cformat(%5.4f)\n\n\n\nLinear regression                               Number of obs     =      1,084\n                                                F(8, 1075)        =     110.48\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.4262\n                                                Root MSE          =      .4127\n\n------------------------------------------------------------------------------\n             |               Robust\n       lwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     85.year |     0.1178     0.1239     0.95   0.342      -0.1253      0.3609\n        educ |     0.0747     0.0060    12.40   0.000       0.0629      0.0865\n    1.female |    -0.3167     0.0347    -9.12   0.000      -0.3848     -0.2486\n             |\n year#c.educ |\n         85  |     0.0185     0.0095     1.94   0.053      -0.0002      0.0371\n             |\n year#female |\n       85 1  |     0.0851     0.0518     1.64   0.101      -0.0165      0.1866\n             |\n       exper |     0.0296     0.0037     8.10   0.000       0.0224      0.0368\n     expersq |    -0.0004     0.0001    -5.11   0.000      -0.0006     -0.0002\n       union |     0.2021     0.0293     6.89   0.000       0.1446      0.2597\n       _cons |     0.4589     0.0855     5.37   0.000       0.2911      0.6267\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/10_pooldata.html#good-old-friend-chow-test",
    "href": "rmethods/10_pooldata.html#good-old-friend-chow-test",
    "title": "Pool Cross-section and Panel Data",
    "section": "Good old Friend: Chow test",
    "text": "Good old Friend: Chow test\n\nThe Chow test can be used to test whether the coefficients of a regression model are the same across two groups.\n\nwe have seen this test back when we were discussing dummy variables.\n\nWe can also use this test to check if coefficients of a regression model are the same across two time periods. (Has the wage structure changed across time?)\n\nThis is the case of interest here.\n\nNot much changes with before. Although it can be a bit more tedious to code."
  },
  {
    "objectID": "rmethods/10_pooldata.html#example-1",
    "href": "rmethods/10_pooldata.html#example-1",
    "title": "Pool Cross-section and Panel Data",
    "section": "Example",
    "text": "Example\n\n\nCode\nfrause cps78_85, clear\nregress lwage i.year##c.(educ i.female exper expersq i.union), robust\n\n\n\nLinear regression                               Number of obs     =      1,084\n                                                F(11, 1072)       =      82.83\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.4276\n                                                Root MSE          =     .41278\n\n------------------------------------------------------------------------------\n             |               Robust\n       lwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     85.year |   .1219978   .1521927     0.80   0.423    -.1766315    .4206271\n        educ |   .0768148   .0063312    12.13   0.000     .0643918    .0892378\n    1.female |  -.3155108   .0348402    -9.06   0.000    -.3838737    -.247148\n       exper |   .0249177   .0042985     5.80   0.000     .0164833    .0333522\n     expersq |  -.0002844   .0000918    -3.10   0.002    -.0004645   -.0001043\n     1.union |   .2039824   .0381315     5.35   0.000     .1291616    .2788033\n             |\n year#c.educ |\n         85  |    .013927   .0103252     1.35   0.178    -.0063329    .0341869\n             |\n year#female |\n       85 1  |   .0846136   .0524618     1.61   0.107    -.0183258     .187553\n             |\nyear#c.exper |\n         85  |   .0095289   .0073767     1.29   0.197    -.0049454    .0240033\n             |\n        year#|\n   c.expersq |\n         85  |  -.0002399   .0001592    -1.51   0.132    -.0005522    .0000724\n             |\n  year#union |\n       85 1  |  -.0018095   .0594387    -0.03   0.976    -.1184389      .11482\n             |\n       _cons |    .458257     .09386     4.88   0.000     .2740868    .6424271\n------------------------------------------------------------------------------\n\n\n\ntest 85.year#c.educ 85.year#1.female 85.year#c.exper   85.year#c.expersq 85.year#1.union\n\n\n ( 1)  85.year#c.educ = 0\n ( 2)  85.year#1.female = 0\n ( 3)  85.year#c.exper = 0\n ( 4)  85.year#c.expersq = 0\n ( 5)  85.year#1.union = 0\n\n       F(  5,  1072) =    1.65\n            Prob &gt; F =    0.1443"
  },
  {
    "objectID": "rmethods/10_pooldata.html#using-pool-crossection-for-causal-inference",
    "href": "rmethods/10_pooldata.html#using-pool-crossection-for-causal-inference",
    "title": "Pool Cross-section and Panel Data",
    "section": "Using Pool Crossection for Causal Inference",
    "text": "Using Pool Crossection for Causal Inference\n\nOne advantage of pooling crossection data is that it could to be used to estimate causal effects using a method known as Differences in Differences (DnD)\nConsider the following case:\n\nThere was a project regarding the construction of an incinerator in a city. You are asked to evaluate what the impact of this was on the prices of houses around the area.\nYou have access to data for two years: 1978 and 1981.\nIn 1978, there was no information about the project. In 1981, the project was announced, but it only began operations in 1985."
  },
  {
    "objectID": "rmethods/10_pooldata.html#section",
    "href": "rmethods/10_pooldata.html#section",
    "title": "Pool Cross-section and Panel Data",
    "section": "",
    "text": "we could start estimating the project using the simple model: \\[rprice = \\beta_0 + \\beta_1 nearinc + e\\]\n\nusing only 1981 data. But this would not be a good idea. Why?\n\n\nCode\nfrause kielmc, clear\nregress rprice nearinc if year == 1981, robust\n\n\n\nLinear regression                               Number of obs     =        142\n                                                F(1, 140)         =      24.35\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.1653\n                                                Root MSE          =      31238\n\n------------------------------------------------------------------------------\n             |               Robust\n      rprice | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     nearinc |  -30688.27   6219.265    -4.93   0.000     -42984.1   -18392.45\n       _cons |   101307.5   2951.195    34.33   0.000     95472.84    107142.2\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/10_pooldata.html#section-1",
    "href": "rmethods/10_pooldata.html#section-1",
    "title": "Pool Cross-section and Panel Data",
    "section": "",
    "text": "We could also estimate the model using only 1971 data. What would this be showing us?\n\n\n\nCode\nregress rprice nearinc if year == 1978, robust\n\n\n\nLinear regression                               Number of obs     =        179\n                                                F(1, 177)         =       9.87\n                                                Prob &gt; F          =     0.0020\n                                                R-squared         =     0.0817\n                                                Root MSE          =      29432\n\n------------------------------------------------------------------------------\n             |               Robust\n      rprice | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     nearinc |  -18824.37   5992.564    -3.14   0.002    -30650.44   -6998.302\n       _cons |   82517.23   1881.165    43.86   0.000     78804.83    86229.63\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/10_pooldata.html#section-2",
    "href": "rmethods/10_pooldata.html#section-2",
    "title": "Pool Cross-section and Panel Data",
    "section": "",
    "text": "So, using 1981 data we capture the Total price difference between houses near and far from the incinerator.\n\nThis captures both the announcement effect of the project, but also other factors (where would an incinerator be built?).\n\nUsing 1978 data we capture the price difference between houses near and far from the incinerator in the absence of the project.\n\nThis captures the effect of other factors that may be correlated with the incinerator project.\n\nUse both to see the impact!\n\n\\[Effect = -30688.27-(-18824.37)= -11863.9\\]\n\nThis is in essence a DnD model"
  },
  {
    "objectID": "rmethods/10_pooldata.html#difference-in-differences",
    "href": "rmethods/10_pooldata.html#difference-in-differences",
    "title": "Pool Cross-section and Panel Data",
    "section": "Difference in Differences",
    "text": "Difference in Differences\n\n\n\n\n\n\n\n\n\n\nControl\nTreatment\nTreat-Control\n\n\n\n\nPre-\n\\(\\bar y_{00}\\)\n\\(\\bar y_{10}\\)\n\\(\\bar y_{10}\\)-\\(\\bar y_{00}\\)\n\n\nPost-\n\\(\\bar y_{01}\\)\n\\(\\bar y_{11}\\)\n\\(\\bar y_{10}\\)-\\(\\bar y_{00}\\)\n\n\nPost-pre\n\\(\\bar y_{01}\\)-\\(\\bar y_{00}\\)\n\\(\\bar y_{11}\\)-\\(\\bar y_{10}\\)\nDD\n\n\n\n\nPost-Pre:\n\nTrend changes for the control\nTrend changes for the treated: A mix of the impact of the treatment and the trend change.\n\nTreat-Control:\n\nBaseline difference when looking at Pre-period\nTotal Price differentials when looking at Post-period: Mix of the impact of the treatment and the baseline difference.\n\nTake the Double Difference and you get the treatment effect."
  },
  {
    "objectID": "rmethods/10_pooldata.html#difference-in-differences-regression",
    "href": "rmethods/10_pooldata.html#difference-in-differences-regression",
    "title": "Pool Cross-section and Panel Data",
    "section": "Difference in Differences: Regression",
    "text": "Difference in Differences: Regression\n\nThis could also be achieved using a regression model:\n\n\\[ y = \\beta_0 + \\beta_1 post + \\beta_2 treat + \\beta_3 post*treat + e\\]\nWhere \\(\\beta_3\\) is the treatment effect. (only for 2x2 DD)\n\n\nCode\nregress rprice nearinc##y81, robust\n\n\n\nLinear regression                               Number of obs     =        321\n                                                F(3, 317)         =      17.75\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.1739\n                                                Root MSE          =      30243\n\n------------------------------------------------------------------------------\n             |               Robust\n      rprice | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   1.nearinc |  -18824.37    5996.47    -3.14   0.002    -30622.28   -7026.461\n       1.y81 |   18790.29   3498.376     5.37   0.000     11907.32    25673.26\n             |\n nearinc#y81 |\n        1 1  |   -11863.9   8635.585    -1.37   0.170    -28854.21    5126.401\n             |\n       _cons |   82517.23   1882.391    43.84   0.000     78813.67    86220.79\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/10_pooldata.html#difference-in-differences-regression-controls",
    "href": "rmethods/10_pooldata.html#difference-in-differences-regression-controls",
    "title": "Pool Cross-section and Panel Data",
    "section": "Difference in Differences: Regression + controls",
    "text": "Difference in Differences: Regression + controls\n\nOne advantage of DD is that it can control for those unobserved factors that may be correlated with outcome.\n\nWithout controls, however, estimates may not have enough precision.\n\nBut, we could add controls!\n\n\\[ y = \\beta_0 + X \\gamma + \\beta_1 post + \\beta_2 treat + \\beta_3 post*treat + e\\]\nBut its not as easy as it may seem! (just adding regressions is not a good approach)\nThis method requires other assumptions! (\\(\\gamma\\) is fixed), which may be very strong.\n\nNote: For DD to work, you need to assume the two groups follow the same path in the absence of the treatment. (Parallel trends assumption)\nOtherwise, you are just using trend differences!"
  },
  {
    "objectID": "rmethods/10_pooldata.html#diff-in-diff-in-diff",
    "href": "rmethods/10_pooldata.html#diff-in-diff-in-diff",
    "title": "Pool Cross-section and Panel Data",
    "section": "Diff in Diff in Diff",
    "text": "Diff in Diff in Diff\nAn Alternative approach is to use a triple difference model.\nSetup:\n\nYou still have two groups: Control and Treatment (which are easily identifiable)\nYou have two time periods: Pre and Post (which are also easily identifiable)\nYou have a different sample, where you can identify controls and treatment, as well as the pre- and post- periods. This sample was not treated!\n\nEstimation:\n\nEstimate the DD for the Original Sample, and the new untreated sample.\nObtaining the difference between these two estimates will give you the triple difference.\n\nExample: Smoking ban analysis based on age. (DD) But using both treated and untreated States (DDD)"
  },
  {
    "objectID": "rmethods/10_pooldata.html#general-framework-and-pseudo-panels",
    "href": "rmethods/10_pooldata.html#general-framework-and-pseudo-panels",
    "title": "Pool Cross-section and Panel Data",
    "section": "General Framework and Pseudo Panels",
    "text": "General Framework and Pseudo Panels\n\nOne general Structure for Policy analysis is the use of Pseudo Panels structure.\n\nPseudo panels are a way to use repeated crossection data, but controlling for some unobserved heterogeneity across specific groups. (the pseudo panels)\n\nFor Pseudo-panels, we need to identify a group that could be followed across time.\n\nThis cannot be a group of individuals (repeated crosection).\nBut we could use groups of states, cohorts (year of birth), etc.\n\nIn this case, the data would look like this: \\[y_{igt} = \\lambda_t + \\alpha_g + \\beta x_{gt} + z_{igt}\\gamma +  e_{igt}\\]\nWhere \\(g\\) is the group, \\(t\\) is the time, and \\(i\\) is the individual.\nAnd \\(\\beta\\) is the coefficient of interest. (impact of the Policy \\(x_{gt}\\)).\n\nThis may ony work if we assume \\(\\beta\\) is constant across time and groups."
  },
  {
    "objectID": "rmethods/1_introduction.html#what-is-econometrics",
    "href": "rmethods/1_introduction.html#what-is-econometrics",
    "title": "Introduction",
    "section": "What is econometrics?",
    "text": "What is econometrics?\nEconometrics is an amalgamation of Statistics and Economics, that typically analysis nonexperimental data.\n\nStatistics: Because we make use of numerous properties and mathematical properties to obtain derive Statitics related to our data\nEconomics: Because we aknowledge that we use data that comes from Agents interactions, and as such as subject to erros.\n\nWe use both tools to analyze data from the world around us.\n\nYour Economic intuition to make sense and explain relationships that you find, and mathematics/statistics to obtain estimates that are statistically sound."
  },
  {
    "objectID": "rmethods/1_introduction.html#when-is-it-useful",
    "href": "rmethods/1_introduction.html#when-is-it-useful",
    "title": "Introduction",
    "section": "When is it useful?",
    "text": "When is it useful?\nEconometrics is useful whenever we aim to:\n\nTest theories, Explore theoretical relationships, Verify Predictions\n\nBut also\n\nWe use Econometrics when we want to evaluate policies, or provide evidence for policy makers. Find that Causal effect.\n\nCaveat: We may not have the best data for this, but we can come up with cleaver designs to still do our job!"
  },
  {
    "objectID": "rmethods/1_introduction.html#where-does-econometric-fits-in-empirical-research",
    "href": "rmethods/1_introduction.html#where-does-econometric-fits-in-empirical-research",
    "title": "Introduction",
    "section": "Where does Econometric Fits in Empirical Research",
    "text": "Where does Econometric Fits in Empirical Research\nObvious answer\n\nYou will need Econometrics methods to analyze your data, however, (just as a reminder) you should be aware of “HOW” an Empirical Research should be made:\n\nS1. Research Question:\n\nA question that is Answerable (within bounderies of Time/Money/and availabilty)\nThat should help us understand a Topic Better\nThat is Specific Enough to be feasible, but General Enough to be of interest.\n\nKeep it simple"
  },
  {
    "objectID": "rmethods/1_introduction.html#where-does-econometric-fits-in-empirical-research-1",
    "href": "rmethods/1_introduction.html#where-does-econometric-fits-in-empirical-research-1",
    "title": "Introduction",
    "section": "Where does Econometric Fits in Empirical Research",
    "text": "Where does Econometric Fits in Empirical Research\nS2. Construct an Economic model\n\nTo understand what the problem before you start analyzing the research question. May not need a formal modeling (Heavy Math), but enough to have some understanding of the problem.\n\nS3. Decide on the Econometric model\n\nYou need to decide what data is needed and is available your model.\nHow you will estimate the model (assumptions on methods)\n\nS4. Estimate model, and Analyze Data\n\nEstimate the model, using Economtric tools and Methods fitted to the data and the research question.\nExplain results in light of your Economic Model, and theoretical predictions. The Economist in you!"
  },
  {
    "objectID": "rmethods/1_introduction.html#so-you-need-data",
    "href": "rmethods/1_introduction.html#so-you-need-data",
    "title": "Introduction",
    "section": "So you need Data",
    "text": "So you need Data"
  },
  {
    "objectID": "rmethods/1_introduction.html#need-for-data",
    "href": "rmethods/1_introduction.html#need-for-data",
    "title": "Introduction",
    "section": "Need for Data",
    "text": "Need for Data\nDifferent types of data may allow for using different econometric methodologies, and answer different types of questions.\n\nKeep in mind you will only have access to SAMPLES, never the Population\nThere will be instances that you come close to Population data, ie Census.\nBut Even Census data is not the Population (or Super Population we use in Econometrics)."
  },
  {
    "objectID": "rmethods/1_introduction.html#types-of-data",
    "href": "rmethods/1_introduction.html#types-of-data",
    "title": "Introduction",
    "section": "Types of Data:",
    "text": "Types of Data:\nCross-Section: Sample of the population collects data on Many individuals in a single point in time.\nTime Series Data: Data collected on a single individual across time.\nPanel Data: Data collected for Many individuals who are followed across time.\nRepeated Cross-Section: Pooled Cross-Section Data for different individuals collected at different points in time. Individuals are not followed across time."
  },
  {
    "objectID": "rmethods/1_introduction.html#visually",
    "href": "rmethods/1_introduction.html#visually",
    "title": "Introduction",
    "section": "Visually",
    "text": "Visually\n\nCrossectionTime SeriesPanel DataRepeated Crossection"
  },
  {
    "objectID": "rmethods/1_introduction.html#before-the-break",
    "href": "rmethods/1_introduction.html#before-the-break",
    "title": "Introduction",
    "section": "Before the Break:",
    "text": "Before the Break:\nCausality, Ceteris Paribus, and Counterfactuals\nThee important concepts for the Class\n\nCausality: This is what most applied research aims to identify. A causal effect is a change the variable interest experiences, only because a second variable changed, while all other factors remained FIXED.\n\nThis is different from associations or correlations.\n\nCeteris Paribus: In Econometric analysis, ceteris paribus implies that all factors, except the one analyzed, are assumed constant (There is no change), thus leading to causality\nCounterfactual: It is the consideration of what would have been if only a single factor changed in the analysis (for a given observation).\n\nWhat if didn’t apply to the MSC at Levy? If you got miss your plane to the US? etc."
  },
  {
    "objectID": "rmethods/1_introduction.html#thinking-about-counterfactuals-is-key",
    "href": "rmethods/1_introduction.html#thinking-about-counterfactuals-is-key",
    "title": "Introduction",
    "section": "Thinking about Counterfactuals is Key",
    "text": "Thinking about Counterfactuals is Key\nFor empirical work that aims to identify Causal Effects, it is important to understand the concept of counterfactual.\n\nIt will help you understand what is what you need to analyze,\nHow could those effects be identified in ideal scenarios (Experiments)\nWhat the limitation of those scenarios are\nAnd what alternatives are there to void those limitations"
  },
  {
    "objectID": "rmethods/1_introduction.html#example-causal-effect-of-fertilizer-on-crops",
    "href": "rmethods/1_introduction.html#example-causal-effect-of-fertilizer-on-crops",
    "title": "Introduction",
    "section": "Example: Causal effect of Fertilizer on Crops",
    "text": "Example: Causal effect of Fertilizer on Crops\nRQ: By how much will the production of soybeans increase if one increases the amount of fertilizer applied to the ground?\nCF: Same Piece of Land with and without Fertilizer (Impossible)\nEXP: Randomly Use Fertilizers Across different Plots of Land (Expensive but feasible)\nEA: Use Regressions to keep other all factors that can affect Land productivity fixed when Analyzing Expost Data (Inexpensive)"
  },
  {
    "objectID": "rmethods/1_introduction.html#example-causal-effect-of-smoking-on-babies-health",
    "href": "rmethods/1_introduction.html#example-causal-effect-of-smoking-on-babies-health",
    "title": "Introduction",
    "section": "Example: Causal Effect of Smoking on Babie’s health",
    "text": "Example: Causal Effect of Smoking on Babie’s health\nRQ: Does Smoking during Pregnancy decreases birthweight?\nCF: We consider the same woman. In one case she smokes through pregnancy, in the other she doesnt. Compare Babies Weight.\nEXP: Select a random sample of Pregnant Women and randomly select those who will be “forced” to smoke during pregnancy.\nEXP1: Select a Randome sample of PW with history of smoking. Randomly offer them a voucher and Counceling to quit smoking.\nEA: Consider women with similar characteristics, except for smoking, and compare their babies outcomes."
  },
  {
    "objectID": "rmethods/3_MLRM.html#why-stay-with-1-when-you-can-use-many-why-not",
    "href": "rmethods/3_MLRM.html#why-stay-with-1-when-you-can-use-many-why-not",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Why stay with 1 when you can use Many? … Why not?",
    "text": "Why stay with 1 when you can use Many? … Why not?\n\nThe SLRM we cover last week is a powerful tool to understand the mechanics behind regression analysis, however is too limited.\n\nUse one control? to fix everything ?!\n\nThe Natural alternative is to relax the assumption and Make things more flexible.\n\nIn other words…Allow for adding More controls\n\n\nThus, instead of:\n\\[y_i = \\beta_0 + \\beta_1 x_i + e_i\n\\]\nwe have to consider:\n\\[y_i = \\beta_0 + \\beta_1 x_{1i} +\\beta_2 x_{2i} + \\dots + \\beta_k x_{ki} + u_i\n\\]\nHow many can we add? and why does it help?"
  },
  {
    "objectID": "rmethods/3_MLRM.html#the-power-of-mlr-why-do-more-controls-help",
    "href": "rmethods/3_MLRM.html#the-power-of-mlr-why-do-more-controls-help",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "The power of MLR: Why do more controls help?",
    "text": "The power of MLR: Why do more controls help?\n\nOne more explicitly accounts for variables that before were hidden in \\(e_i\\).\nWe add \\(x_{2i},x_{3i},\\dots,x_{ki}\\) to the model model, and is no longer in \\(e_i\\)\nAllows for richer model specifications and nonlinearities:\nBefore: \\(y_i = \\beta_0 + \\beta_1 x_{1i} + e_i\\)\nNow : \\(y_i = \\beta_0 + \\beta_1 x_{1i} +\\beta_2 x^2_{1i} + \\beta_3 x^{1/2}_{1i} + \\beta_4 x^{-1}_{1i} + \\beta_5 x_{2i}+\\dots+e_i\\)\n\nThus, we can get closer to the unknown Population function, and explicitly handle some endogeneity problems (we control for it).\n\n\n\n\n\n\nWith great power…\n\n\nBeing able to add more controls is good, but:\n\nMay make things worse (bad controls)\nOr might not be feasible (small Sample)\nOr may be difficult to interpret (unless you know how to)"
  },
  {
    "objectID": "rmethods/3_MLRM.html#mostly-the-same",
    "href": "rmethods/3_MLRM.html#mostly-the-same",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Mostly the same",
    "text": "Mostly the same\n\nLinear in Parameters: \\(y = X\\beta + e\\) (And this is the pop function)\nRandom Sampling from the population of interest. (So errors \\(e_i\\) is independent from \\(e_j\\))\nNo Perfect Collinearity:\nThis is the alternative to \\(Var(x)&gt;0\\) (SLRM), and deserves more attention.\n\n\nWe want each variable in \\(X\\) to have some independent variation, from all other variables in the model.\n\nIn the SLRM, the independent variation idea was with respect to the constant.\n\nIf a variable was a linear combination of others, then \\(\\beta's\\) cannot be identified. You need to choose what to keep:\n\n\\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1+X_2) + e \\\\\n&=  \\beta_0 + (\\beta_1+\\beta_3) X_1 + (\\beta_2+\\beta_3) X_2 + e  \n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#section-1",
    "href": "rmethods/3_MLRM.html#section-1",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "Zero Conditional mean (Exogeneity): \\(E(e_i|X)=0\\)\nRequires that the errors and the explanatory variables are uncorrelated. This is “easier” to achieve, because we can now move variables form the error to the model.\nHowever, there could be things you can’t controls for (and remain lurking in your errors)\n\n\nI call this the most important assumption, because is the hardest to deal with\n\nIf A1-A4 Hold, then your estimates will be unbiased!\n\nHomoskedasticity Same as before. Errors dispersion does not change with respect to all \\(X's\\). \\[Var(e|X)=c\n\\]\n\nJust as with SLRM, this assumption will help with the estimation of Standard Errors."
  },
  {
    "objectID": "rmethods/3_MLRM.html#mlrm-estimation",
    "href": "rmethods/3_MLRM.html#mlrm-estimation",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "MLRM estimation",
    "text": "MLRM estimation\nAs before, not much has changed. We are still interested in finding \\(\\beta's\\) that Minimizes the (squared) error of the model when compared to the observed data:\n\\[\\hat \\beta = \\min_\\beta \\sum (y_i-X_i'\\beta)^2 = \\min_\\beta \\sum (y_i-\\beta_0-\\beta_1 x_{1i}-\\dots-\\beta_k x_{ki})^2\n\\]\nThe corresponding FOC generate \\(K+1\\) equations to identify \\(K+1\\) parameters:\n\\[\\begin{aligned}\n\\sum (y_i-X_i'\\beta) &= 0  \\\\\n\\sum x_{1i}(y_i-X_i'\\beta) &= 0 \\\\\n\\sum x_{2i}(y_i-X_i'\\beta) &= 0 \\\\ \\dots \\\\\\\n\\sum x_{ki}(y_i-X_i'\\beta) &= 0\n\\end{aligned} \\rightarrow X'(y-X\\beta) =0 \\rightarrow \\hat \\beta = (X'X)^{-1}X'y\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#mata-interlute-for-those-curious",
    "href": "rmethods/3_MLRM.html#mata-interlute-for-those-curious",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "mata Interlute (for those curious)",
    "text": "mata Interlute (for those curious)\n\n\nCode\nfrause gpa1, clear\ngen one =1 \nmata: y=st_data(.,\"colgpa\"); mata: x=st_data(.,\"hsgpa act one\")\nmata: xx=x'x ; ixx=invsym(xx) ; xy = x'y \nmata: b = ixx * xy ; b\n\n\n                 1\n    +---------------+\n  1 |  .4534558853  |\n  2 |  .0094260123  |\n  3 |  1.286327767  |\n    +---------------+"
  },
  {
    "objectID": "rmethods/3_MLRM.html#you-got-the-betas-how-do-you-interpret-them",
    "href": "rmethods/3_MLRM.html#you-got-the-betas-how-do-you-interpret-them",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "You got the \\(\\beta's\\), how do you interpret them?",
    "text": "You got the \\(\\beta's\\), how do you interpret them?\nInterpretation of MLRM is similar to the SLRM. For most cases, you simply look into the coefficients, and interpret effects in terms of Changes:\n\\[\\begin{aligned}\ny_i = \\hat\\beta_0 + \\hat\\beta_1 x_{1i}  + \\hat\\beta_2 x_{2i} + e_i \\\\\n\\Delta y_i =  \\hat\\beta_1 \\Delta  x_{1i}  + \\hat\\beta_2 \\Delta  x_{2i} + \\Delta e_i\n\\end{aligned}\n\\]\nUnder A1-A5 I can make use the above to make interpretations\n\n\\(\\hat \\beta_0\\) has no effect on “changes” of \\(y\\). Only its levels.\n\\(\\hat \\beta_1\\) indicates how much \\(\\Delta y_i\\) will be if \\(\\Delta x_{1i}\\) increases in 1 unit, if both \\(\\Delta x_{2i}\\) and \\(\\Delta e_i\\) remain constant (Ceteris Paribus)\n\n\\(\\Delta e_i=0\\) by assumption, and \\(\\Delta x_{2i}=0\\) becuse we are explicilty controlling for it (We impute this based on extrapolations)\nYou could also analyze the effect of \\(\\Delta x_{1i}\\) and \\(\\Delta x_{2i}\\) Simultaneously!"
  },
  {
    "objectID": "rmethods/3_MLRM.html#example",
    "href": "rmethods/3_MLRM.html#example",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Example",
    "text": "Example\n\nCode\nqui: frause wage1, clear\nqui: reg lwage educ exper tenure\nlocal b0:display %5.3f _b[_cons]\nlocal b1:display %5.3f _b[educ]\nlocal b2:display %5.3f _b[exper]\nlocal b3:display %5.3f _b[tenure]\ndisplay \"\\$log(wage) = `b0' + `b1' educ + `b2' exper + `b3' tenure$\"\n\n\\(log(wage) = 0.284 + 0.092 educ + 0.004 exper + 0.022 tenure\\)\n\n\\(\\beta_0\\) has no effect on changes, but level.\n\nIf someone has no education, experience or tenure, log(wages) will be 0.284. Why not wages? and Does it make sense to assume 0 education, experience and tenure?\n\n\\(\\beta_1\\): An additional year of education increases wages in 0.092log points or about 9.2%, if Experience and tenure do nor change (ceteris paribus).\n\nNotes:\n\nThink of Interpretations as counterfactual: \\(y_{post} - y_{pre}\\)\nAssumption: Other factors (unobserved \\(e\\)) remain fixed (is it always credible??)\nEffects can be combined. What if a person gains 1 year of education but losses 3 of tenure?"
  },
  {
    "objectID": "rmethods/3_MLRM.html#more-on-interpretation",
    "href": "rmethods/3_MLRM.html#more-on-interpretation",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "More on Interpretation",
    "text": "More on Interpretation\nUnder A1-A5, you can still interpret results as “counterfactual” at the individual level. However, its more common to do it based on Conditional means:\n\\[\\frac {\\Delta E(y|X)}{\\Delta X_k} \\simeq E(y|X_{-k},X_k+1)-E(y|X)\n\\]\nWhich mostly changes Language.\n\nThe expected effect of an increase in \\(X\\) in one unit."
  },
  {
    "objectID": "rmethods/3_MLRM.html#alternative-interpretation-partialling-out",
    "href": "rmethods/3_MLRM.html#alternative-interpretation-partialling-out",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Alternative Interpretation: Partialling out",
    "text": "Alternative Interpretation: Partialling out\n\nAn alternative way of interpreting (and understanding) MLRM is to think about partialling out interpretation.\nThis interpretation is based on the Frisch-Waugh-Lowell Theorem, which states that the following models should give you the SAME \\(\\beta's\\):\n\n\\[\\begin{aligned}\ny &= \\color{blue}{\\beta_1 } X_1 + \\beta_2 X_2 + e \\\\\n(I-P_{X^c_2}) y &= \\color{green}{\\beta_1} (I-P_{X^c_2}) X_1 + e \\\\\nP_{X^c_2} &= X^c_2 (X'^{c}_2  X^{c}_2) X'^{c}_2 : \\text{Projection Matrix}\n\\end{aligned}\n\\]\n\n\n\n\n\n\nPartialling out\n\n\n\\(\\beta_1\\) can be interpreted as the effect of \\(X_1\\) on \\(y\\), after all variation related to \\(X_2\\) has been “eliminated”.\nThus \\(\\beta_1\\) is the effect uniquely driven by \\(X_1\\)."
  },
  {
    "objectID": "rmethods/3_MLRM.html#example-1",
    "href": "rmethods/3_MLRM.html#example-1",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Example",
    "text": "Example\n\nqui {\n  frause oaxaca, clear\n  drop if lnwage==.\n  reg lnwage educ exper tenure\n  est sto m1\n  reg educ        exper tenure\n  predict r_educ , res\n  reg lnwage      exper tenure\n  predict r_lnwage , res\n  reg r_lnwage r_educ\n  est sto m2\n  reg lnwage educ\n  est sto m3\n}\nesttab m1 m2 m3, se  \n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                   lnwage        r_lnwage          lnwage   \n------------------------------------------------------------\neduc               0.0870***                       0.0800***\n                (0.00516)                       (0.00539)   \n\nexper              0.0113***                                \n                (0.00154)                                   \n\ntenure            0.00837***                                \n                (0.00188)                                   \n\nr_educ                             0.0870***                \n                                (0.00516)                   \n\n_cons               2.140***     8.93e-10           2.434***\n                 (0.0650)        (0.0124)        (0.0636)   \n------------------------------------------------------------\nN                    1434            1434            1434   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/3_MLRM.html#estimator-properties-unbiased",
    "href": "rmethods/3_MLRM.html#estimator-properties-unbiased",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Estimator Properties: Unbiased",
    "text": "Estimator Properties: Unbiased\nRecall, the estimator of \\(\\beta's\\) when you have multiple dependent variables:\n\\[\\begin{aligned}\n0  &: \\hat \\beta = (X'X)^{-1} X'y \\\\\nA1 \\text{ & }  A2 &: \\hat \\beta = (X'X)^{-1} X'(X\\beta + e) \\\\\n1  &: \\hat \\beta = (X'X)^{-1} X'X\\beta + (X'X)^{-1} X'e \\\\\nA3 &: det(X'X)\\neq 0 \\rightarrow (X'X)^{-1} \\text{ exists} \\\\\n2  &: \\hat \\beta = \\beta + (X'X)^{-1} X'e \\\\\nA4 &: E(e|X)=0 \\rightarrow E[(X'X)^{-1} X'e]=0 \\\\\n3  &: E(\\hat\\beta)= \\beta \\text{ unbiased}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#estimator-properties-variance-under-homoskedasticity",
    "href": "rmethods/3_MLRM.html#estimator-properties-variance-under-homoskedasticity",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Estimator Properties: Variance under Homoskedasticity",
    "text": "Estimator Properties: Variance under Homoskedasticity\nLets start with (2). \\(\\beta's\\) are random functions of the errors. Thus its variance will depend on \\(e\\).\n\\[\\begin{aligned}\n1 &: \\hat \\beta = \\beta + (X'X)^{-1} X'e \\\\\n2 &:\\hat \\beta - \\beta = (X'X)^{-1} X'e \\\\\n3 &: Var(\\hat \\beta - \\beta) = Var((X'X)^{-1} X'e) \\\\\n4 &: Var(\\hat \\beta - \\beta) = (X'X)^{-1} X' Var(e) X' (X'X)^{-1}  \\\\\n\\end{aligned}\n\\]\n\\(Var(e)\\) considers variance and covariance of each \\(e_i\\) and its combinations."
  },
  {
    "objectID": "rmethods/3_MLRM.html#section-2",
    "href": "rmethods/3_MLRM.html#section-2",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "By assumption A2, \\(cov(e_i,e_j)=0\\). And by assumption A5 \\(Var(e_i)=Var(e_j)\\).\n\\[\\begin{aligned}\nVar(\\hat \\beta - \\beta) &= (X'X)^{-1} X' \\sigma_e^2 I X' (X'X)^{-1} \\\\\nVar(\\hat \\beta - \\beta) &= \\sigma_e^2 (X'X)^{-1} \\\\\nVar(\\hat \\beta_j - \\beta_j) &= \\frac{\\sigma_e^2}{SST_j (1-R^2_j)}\n\\end{aligned}\n\\]\nBut we do not know \\(\\sigma^2_e\\). Thus, we also “estimate it”\n\\[\\hat \\sigma^2_e = \\frac{\\sum \\hat e^2}{N-K-1}\n\\]\nWhich is unbiased estimator for \\(\\sigma^2_e\\) if A1-A5 hold."
  },
  {
    "objectID": "rmethods/3_MLRM.html#section-3",
    "href": "rmethods/3_MLRM.html#section-3",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "\\[\\begin{aligned}\nVar(\\hat \\beta - \\beta) &= \\sigma_e^2 (X'X)^{-1} \\\\\nVar(\\hat \\beta_j - \\beta_j) &= \\frac{\\sigma_e^2}{SST_j (1-R^2_j)}  \\\\\\\n& = \\frac{\\sigma_e^2}{(N-1)Var(X_j) (1-R^2_j)} = \\frac{\\sigma_e^2}{(N-1)Var(X_j)}VIF_j\n\\end{aligned}\n\\]\nTo consider:\n\n\\(Var(\\beta)\\) increases with \\(\\sigma_e^2\\). More variation in the error, more variation of the coefficients.\n\\(Var(\\beta)\\) decreases with Sample size \\(N\\)\n\\(Var(\\beta)\\) also decreases with Variation in \\(X\\)\nHowever, it increases if there is less unique variation (Multicolinearity problem and VIF)"
  },
  {
    "objectID": "rmethods/3_MLRM.html#quick-note",
    "href": "rmethods/3_MLRM.html#quick-note",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Quick Note",
    "text": "Quick Note\n\n\\(R^2\\) are the same as SLRM: How much of variation is explained by the model.\n\nAlso \\(R^2 = corr(y,\\hat y)^2\\)\n\nThe fitted line goes over the “mean” of all variables\nMLRM Fits hyper-planes to the data\nRegression through the origin still a bad idea\nAlso, under A1-A5 OLS is the Best Linear Unbiased Estimator (BLUE)"
  },
  {
    "objectID": "rmethods/3_MLRM.html#ignoring-variables",
    "href": "rmethods/3_MLRM.html#ignoring-variables",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Ignoring Variables",
    "text": "Ignoring Variables\nIn the MLRM framework, its easier to see what happens when important variables are ignored.\n\\[\\text{True: } y = b_0 + b_1 x_1 + b_2 x_2 + e\n\\]\nBut instead you estimate the following :\n\\[\\text{Estimated: }y = g_0 + g_1 x_1 + v\n\\]\nUnless stronger assumptions are imposed, \\(g_1\\) will be a biased estimate of \\(b_1\\).\n\\[\\begin{aligned}\n\\hat g_1 &= \\frac{\\sum \\tilde x_1 \\tilde y}{\\sum \\tilde x_1^2}\n         = \\frac{\\sum \\tilde x_1 (b_1 \\tilde x_1 +\\tilde b_2 \\tilde x_2 + e) }{\\sum \\tilde x_1^2} \\\\\n         &= \\frac{b_1 \\sum \\tilde x_1^2}{\\sum \\tilde x_1^2}\n          + b_2 \\frac{\\sum \\tilde x_1\\tilde x_2}{\\sum \\tilde x_1^2}\n          +\\frac{\\sum \\tilde x_1 e}{\\sum \\tilde x_1^2} \\\\\n         &= b_1+b_2 \\delta_1 +\\frac{\\sum \\tilde x_1 e}{\\sum \\tilde x_1^2} \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#section-6",
    "href": "rmethods/3_MLRM.html#section-6",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "This implies that \\(g_1\\) is biased:\n\\[E(\\hat g_1) = b_1+b_2 \\delta_1\n\\]\nWhere \\(\\delta_1\\) is the coefficient in \\(x_2=\\delta_0+\\delta_1 x_1 + v\\).\nImplications:\n\nUnless\n\n\\(\\delta_1\\) is zero (\\(x_1\\) and \\(x_2\\) are linearly independent) or,\n\\(b_2\\) is zero (\\(x_2\\) was irrelevant)\n\nignoring \\(x_2\\) will generate biased (and inconsistent) estimates for \\(b_1\\).\n\nIn models with more controls, the direction of the biases will be harder to define, but similar rule’s of thumb can be used."
  },
  {
    "objectID": "rmethods/3_MLRM.html#adding-irrelevant-controls",
    "href": "rmethods/3_MLRM.html#adding-irrelevant-controls",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Adding irrelevant controls",
    "text": "Adding irrelevant controls\nAdding irrelevant controls will have no effect on bias and consistency.\nif your model is:\n\\[y=b_0+b_1 x_1 +e\n\\]\nbut you estimate:\n\\[y=g_0+g_1 x_1+g_2 x_2 +v\n\\]\nyour model is still unbiased:\n\\[\\begin{aligned}\ng &= (X'X)^{-1}X'(X \\beta^+ + e) \\\\\n    \\beta^+ &= [\\beta \\ ; 0] \\\\\ng &=  \\beta^+ + (X'X)^{-1}X'e \\rightarrow E(g) = \\beta^+\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#adding-bad-controls",
    "href": "rmethods/3_MLRM.html#adding-bad-controls",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Adding “bad” Controls",
    "text": "Adding “bad” Controls\nThe worst case, yet hard to see, is when you add “bad” Controls, also known as Colliers.\nFor example:\n\nSay you want to analyze the effect of education on wages, and you control for occupation. Will it create an unbiased estimate for education?\n\nNo. Your education affects your occupation choice. So some of the effect of education will be “absorbed” by occupation.\n\nSay you want to see the impact of health expenditure on health, and you control for “#visits to the doctor”\n\nThis may also affect your estimates, as expenditure may change how many times you Visits are highly related.\n\n\nIn general, you want to avoid using “channels” as Controls."
  },
  {
    "objectID": "rmethods/3_MLRM.html#what-about-standard-errors",
    "href": "rmethods/3_MLRM.html#what-about-standard-errors",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "What about Standard Errors",
    "text": "What about Standard Errors\n\nCase 1Case 2Case 3\n\n\nOmitting relevant variables that are correlated to \\(X's\\)\nWe wont talk about this. It violates A4, and creates endogeneity\n\n\nOmitting relevant variables that are uncorrelated to \\(X's\\)\n\nOmitted variables will be in the error \\(e\\). Thus variance of coefficients will be larger\n\n\\[\\begin{aligned}\nTrue: & y = b_0 + b_1 x_1 + b_2 x_2 + e  \\\\\nEstimated: & y = g_0 + g_1 x_1 + v   \\\\\n& Var(e)&lt;Var(v) \\rightarrow Var(b_1)&lt;Var(g_1)\n\\end{aligned}\n\\]\nThus Adding controls in Randomized experiements is still a good idea!\n\n\nAdding Irrelevant controls (related to X’s)\nCoefficients are unbiased, and \\(\\sigma^2_e\\) will also be unbiased.\nHowever, you may increase Multicolinearity in the model increasing \\(R_j^2\\) and \\(VIF_j\\).\nVariance of relevant coefficients will be larger.\n\\[\\begin{aligned}\nTrue: & y = b_0 + b_1 x_1  + e  \\\\\nEstimated: & y = g_0 + g_1 x_1 + g_2 x_2 + v   \\\\\n& Var(b_1)&lt;Var(g_1)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#prediction",
    "href": "rmethods/3_MLRM.html#prediction",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Prediction",
    "text": "Prediction\n\nYou can use MLRM to obtain predictions of outcomes.\nThey will be subject to the model specification.\nFor prediction you do not need to worry about “endogeneity” as much. Just on Predictive power (how ??)\n\n\nqui:frause oaxaca, clear\ngen wage = exp(lnwage)\nqui:reg wage educ female age agesq single married\npredict wage_hat\nlist wage wage_hat educ female age agesq single married in 1/5\n\n(213 missing values generated)\n(option xb assumed; fitted values)\n\n     +----------------------------------------------------------------------+\n     |     wage   wage_hat   educ   female   age   agesq   single   married |\n     |----------------------------------------------------------------------|\n  1. | 41.80602   25.61872      9        1    37    1369        1         0 |\n  2. | 36.63003   31.70813      9        0    62    3844        0         1 |\n  3. | 23.54788   30.71257   10.5        1    40    1600        0         1 |\n  4. | 29.76191    42.7976     12        0    55    3025        0         0 |\n  5. | 44.95504   35.76914     12        0    36    1296        0         1 |\n     +----------------------------------------------------------------------+"
  },
  {
    "objectID": "rmethods/3_MLRM.html#efficient-market",
    "href": "rmethods/3_MLRM.html#efficient-market",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Efficient Market",
    "text": "Efficient Market\n\nWe could use MLRM to test theories, like the Efficient Market Theory.\nFor housing, the Assessed price of a house should be all information needed to assess the price of the house. (other ammenities should not matter)\n\nfrause hprice1, clear\nqui:reg price assess bdrms llotsize lsqrft colonial\nmodel_display\nprice_hat = 206.645 + 1.007 assess + 11.404 bdrms + 1.363 llotsize - 38.335 lsqrft + 9.297 colonial\nN= 88 R2=0.831\nqui:reg lprice lassess bdrms llotsize lsqrft colonial\nmodel_display\nlprice_hat = 0.210 + 1.036 lassess + 0.025 bdrms + 0.008 llotsize - 0.092 lsqrft + 0.045 colonial\nN= 88 R2=0.777"
  },
  {
    "objectID": "rmethods/3_MLRM.html#testing-for-discrimination-cp",
    "href": "rmethods/3_MLRM.html#testing-for-discrimination-cp",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Testing for Discrimination (CP)",
    "text": "Testing for Discrimination (CP)\n\nWe could test for discrimination: Unexplained differences in outcomes once other factors are kept fixed.\nIt does require that groups are similar in terms of unobservables.\n\nqui: frause oaxaca, clear\nqui:reg lnwage female \nmodel_display\nlnwage_hat = 3.440 - 0.173 female\nN= 1434 R2=0.027\nqui:reg lnwage female educ age agesq single married exper tenure\nmodel_display\nlnwage_hat = 0.383 - 0.160 female + 0.064 educ + 0.113 age - 0.001 agesq - 0.072 single - 0.094 married - 0.000 exper + 0.007 tenure\nN= 1434 R2=0.345"
  },
  {
    "objectID": "rmethods/3_MLRM.html#treatment-evaluation",
    "href": "rmethods/3_MLRM.html#treatment-evaluation",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Treatment Evaluation",
    "text": "Treatment Evaluation\n\nUnder Random Assingment SRM was enough to estimate ATTs.\nBut if assigment was conditionally random, a better approach would be using MLRM\n\nfrause jtrain98, clear\nqui:reg earn98 train \nmodel_display\nearn98_hat = 10.610 - 2.050 train\nN= 1130 R2=0.016\nqui:reg earn98 train earn96 educ age married\nmodel_display\nearn98_hat = 4.667 + 2.411 train + 0.373 earn96 + 0.363 educ - 0.181 age + 2.482 married\nN= 1130 R2=0.405"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#introduction",
    "href": "rmethods/5_FXMRA.html#introduction",
    "title": "Multiple Regression Analysis",
    "section": "Introduction",
    "text": "Introduction\n\nMultiple Linear Regression models (MLRM), estimated via OLS, have very good properties, if all Assumptions (A1-A5,A6’) Hold.\nUp until now, we have discussed how to estimate them, and analyze them under “optimal” assumptions, in simplified cases.\nToday we will be adding other “minor” Features to MLR, and aim to better understand its features"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#scaling-and-shifting",
    "href": "rmethods/5_FXMRA.html#scaling-and-shifting",
    "title": "Multiple Regression Analysis",
    "section": "Scaling and shifting",
    "text": "Scaling and shifting\n\nSomething that we do not emphasize enough. Before analyzing your data, its important to analyze the nature of the data (summary stats, ranges, scales)\nWhen I talk about Scaling and shifting, I refer exclusibly to affine transormations of the following type:\n\n\\[x^* = a*x+c \\text{ or } x^* = a*(x+c1)+c2\n\\]\nThey either Shift, or change the scale of the data. Not the shape! (logs change shape)\n\nIf one applies affine transformations to the data, it will have NO effect on your model what-so-ever. (Same t’s same F’s, same \\(R^2\\))\nBut, your \\(\\beta's\\) will change. This could help understading and explaining the results."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#example",
    "href": "rmethods/5_FXMRA.html#example",
    "title": "Multiple Regression Analysis",
    "section": "Example:",
    "text": "Example:\n\n\nCode\nset linesize 255\nfrause bwght, clear\ngen bwkg = bwghtlbs*0.454\ngen bwgr = bwkg*1000\nregress bwght male white cigs lfaminc \nest sto m1\nregress bwghtlbs male white cigs lfaminc\nest sto m2\nregress bwkg male white cigs lfaminc\nest sto m3\nregress bwgr male white cigs lfaminc\nest sto m4\n\n\n\nBirthweight and Cig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOz\n\nlbs\n\nKgs\n\nGr\n\n\n\n\n\nmale\n3.123***\n\n0.195***\n\n0.089***\n\n88.605***\n\n\n\n\n(1.071)\n[2.916]\n(0.067)\n[2.916]\n(0.030)\n[2.916]\n(30.389)\n[2.916]\n\n\nwhite\n5.404***\n\n0.338***\n\n0.153***\n\n153.346***\n\n\n\n\n(1.392)\n[3.882]\n(0.087)\n[3.882]\n(0.039)\n[3.882]\n(39.497)\n[3.882]\n\n\ncigs\n-0.480***\n\n-0.030***\n\n-0.014***\n\n-13.628***\n\n\n\n\n(0.091)\n[-5.288]\n(0.006)\n[-5.288]\n(0.003)\n[-5.288]\n(2.577)\n[-5.288]\n\n\nlfaminc\n1.053*\n\n0.066*\n\n0.030*\n\n29.867*\n\n\n\n\n(0.632)\n[1.664]\n(0.040)\n[1.664]\n(0.018)\n[1.664]\n(17.946)\n[1.664]\n\n\n_cons\n110.603***\n\n6.913***\n\n3.138***\n\n3138.351***\n\n\n\n\n(2.071)\n[53.410]\n(0.129)\n[53.410]\n(0.059)\n[53.410]\n(58.760)\n[53.410]\n\n\nN\n1388\n\n1388\n\n1388\n\n1388\n\n\n\nR2\n0.046\n\n0.046\n\n0.046\n\n0.046"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#scaling-xs-and-ys",
    "href": "rmethods/5_FXMRA.html#scaling-xs-and-ys",
    "title": "Multiple Regression Analysis",
    "section": "Scaling X’s and Y’s",
    "text": "Scaling X’s and Y’s\n\nRe-scaling \\(y\\) will affect the all coefficients.\n\nReducing Scale, reduces scale of coefficients\n\nRe-scaling \\(x's\\) will only affect its coefficient and possible the constant.\n\nReducing (increasing) Scale will increase (reduce) Scale of coefficient\n\nIn both cases, Shifting the variable only affects the constant.\n\n\n\n\n\n\n\n\n\n\nRe-Scaling is an important tool/trick that can be used for interpreting more complex models."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#beta-or-standardized-coefficients",
    "href": "rmethods/5_FXMRA.html#beta-or-standardized-coefficients",
    "title": "Multiple Regression Analysis",
    "section": "Beta or Standardized Coefficients",
    "text": "Beta or Standardized Coefficients\n\nIn some fields (health), making inferences based on default scales can be difficult (the impact of 1microgram ?).\nTo avoid this type of problem researchers may opt to use Standardized or Beta coefficients.\n\nHow a \\(sd\\) change in \\(X's\\) affect the outcome (in \\(sd\\))\n\nGetting these coefficient is similar to applying the following transformation to all variables:\n\n\\[\\tilde w = \\frac{w-\\bar w}{\\sigma_w} \\rightarrow E(\\tilde w)=0 \\text{ and } Var(\\tilde w) = 1\n\\]\nreg y x1 x2 x3, beta\nest sto m1\nesttab m1, beta \n\nIt also helps you make comparison of the relative importance of each covariate explanatory power."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-single-dummies",
    "href": "rmethods/5_FXMRA.html#functional-forms-single-dummies",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Single Dummies",
    "text": "Functional Forms: Single Dummies\n\nDummies are variables that take only two values (preferably 0 and 1).\nThey are used to capture qualitative (binary) characteristics (ie Democrat, Union worker, etc)\nWhen used in regression analysis, they represent “shifts” in the Intercept: \\[y = b_0 + b_1 male + b_2 x_1 + b_3 x_2 + e\n\\]\n\nHere, \\(b_0\\) would be the “intercept” for “women” (base) while \\(b_0+b_1\\) would be the intercept for men.\n\nUnder A4, \\(b_1\\) is the expected outcome difference men have over women, everything else constant.\n\n\nUnless further restrictions are used, you can’t add Dummies for both categories in the model.\n\n* Stata Code\nreg y x1 x2 d    &lt;-- Possible if d = 0 or 1\nreg y x1 x2 i.d  &lt;-- Better"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-multiple-dummies",
    "href": "rmethods/5_FXMRA.html#functional-forms-multiple-dummies",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Multiple Dummies",
    "text": "Functional Forms: Multiple Dummies\n\nWe can use dummies to represent multiple (nonoverlapping) characteristics like Race, ranking or age group).\nOne needs a “base” or comparison group to analyze coefficients (or more).\nOrdered variables can be used as continuous, but using them as dummies requires creating dummies for each category.\n\n\\[\\begin{aligned}\ny &= b_0 + b_1 black + b_2 hispanic + b_3 other + b_4 x + e & || Base = White \\\\\ny &= b_0 + b_1 young + b_2 old + b_3 x + e & || Base = Adult\n\\end{aligned}\n\\]\n\nWhen using with ordered data, multiple dummies may create somewhat counterintuitive results\n\ntab race, gen(race_)  &lt;- creates dummies\nreg y i.race x1 x2 x3 &lt;- generally uses first group as base\nreg y ib2.race x1 x2 x3 &lt;- indicates a particular \"base\""
  },
  {
    "objectID": "rmethods/5_FXMRA.html#example-1",
    "href": "rmethods/5_FXMRA.html#example-1",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\n\nfrause beauty, clear\n** Union also a dummy. \n** looks as Continous\nqui:reg lwage exper union educ female looks\nest sto m1\ngen looks_good = looks&gt;=4 if !missing(looks)\nqui:reg lwage exper union educ female looks_good\nest sto m2\nqui:reg lwage exper union educ female i.looks\nest sto m3\nqui:reg lwage exper union educ female ib3.looks\nest sto m4\nesttab m1 m2 m3 m4, se star( * 0.1 ** 0.05 *** 0.01  ) nogaps nomtitle\ndisplay _n \"Exact Change Union : \" %5.3f (exp(_b[union])-1)*100 \"%\"\n\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n----------------------------------------------------------------------------\nexper              0.0137***       0.0134***       0.0135***       0.0135***\n                (0.00119)       (0.00120)       (0.00120)       (0.00120)   \nunion               0.201***        0.201***        0.196***        0.196***\n                 (0.0305)        (0.0307)        (0.0306)        (0.0306)   \neduc               0.0737***       0.0750***       0.0735***       0.0735***\n                (0.00528)       (0.00528)       (0.00528)       (0.00528)   \nfemale             -0.448***       -0.450***       -0.446***       -0.446***\n                 (0.0293)        (0.0294)        (0.0293)        (0.0293)   \nlooks              0.0555***                                                \n                 (0.0201)                                                   \nlooks_good                         0.0276                                   \n                                 (0.0299)                                   \n1.looks                                                 0          -0.266** \n                                                      (.)         (0.134)   \n2.looks                                             0.146          -0.121***\n                                                  (0.139)        (0.0439)   \n3.looks                                             0.266**             0   \n                                                  (0.134)             (.)   \n4.looks                                             0.264*       -0.00255   \n                                                  (0.136)        (0.0312)   \n5.looks                                             0.422**         0.156   \n                                                  (0.173)         (0.111)   \n_cons               0.408***        0.565***        0.338**         0.604***\n                 (0.0968)        (0.0774)         (0.149)        (0.0781)   \n----------------------------------------------------------------------------\nN                    1260            1260            1260            1260   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01\n\nExact Change Union : 21.598%"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-logarithms",
    "href": "rmethods/5_FXMRA.html#functional-forms-logarithms",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Logarithms",
    "text": "Functional Forms: Logarithms\n\nUsing Logarithms can help modeling some nonlinearities in the data.\nBecause it changes the “shape” of variables, it also changes the interpretation (Changes vs %Changes)\nBy reducing dispersion of dep. variable, CLM assumptions may hold.\n\nBut:\n\nCannot or should not be applied to all data types (ie Dummies, negatives, shares)\n(log-lin model): It is often better to use the exact percentage change rather than approximation: \\[\\begin{aligned}\nlog(y) &= b_0 + b_1 x_1 + b_2 x_2 + b_3 D + e \\\\\n\\frac{\\% \\Delta y}{\\Delta D} &= 100 (exp(b_3)-1)\\%\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-polynomials-x2-x3-etc",
    "href": "rmethods/5_FXMRA.html#functional-forms-polynomials-x2-x3-etc",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Polynomials (\\(x^2, x^3, etc\\))",
    "text": "Functional Forms: Polynomials (\\(x^2, x^3, etc\\))\n\nUp to this point, we have only considered linear models (\\(X's\\) enter asis or in logs). This almost always works! (Taylor expansion justification)\n\nSpecially if interested in Average Effects\n\nSome times, you may be interest in capturing some heterogeneity for \\(dy/dx\\). That can be done just adding “ANY” transformation of \\(X\\) in the model (\\(sin(x), 1/x, \\sqrt x\\), etc)\nFor practical, and theoretical purposes, however, we usually concentrate on quadratic terms.\n\nFor example: Increasing returns with decreasing marginal returns\nWe may be interested in “turning” points\n\nHowever, we now need to be careful about marginal effects!"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section",
    "href": "rmethods/5_FXMRA.html#section",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "\\[\\begin{aligned}\ny &=b_0+b_1 x_1 + b_2 x_1^2 + b_3 x_2 + e \\\\\n\\frac{dy}{dx_1} &= b_1+2b_2 x_1 =0 \\\\\nx_1^* &= - \\frac{b_1}{2b_2} x_1\n\\end{aligned}\n\\]\n\n\n\n\n\n\nTo consider\n\n\n\nMarginal effects are no longer constant. You need an \\(x_1\\) value to obtain them (mean? average?)\nWith Quadratic models, there is ALWAYS a turning point (but may not be relevant)\nMFX can be positive or negative for some value of \\(x_1\\) (but may not be relevant)\nUnless something else is done, coefficients may not make sense on their own.\n\n\n\n\n\nWhy not add further polynomials?\n\nEstimating them is easy (except for numerical precision), but adds complexity for interpretation. Nothing else."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#example-2",
    "href": "rmethods/5_FXMRA.html#example-2",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\n\nfrause hprice2, clear\ngen rooms2=rooms*rooms\nqui:reg lprice lnox dist rooms \nest sto m0\nqui:reg lprice lnox dist rooms rooms2\nest sto m1\nqui:reg lprice lnox dist c.rooms c.rooms#c.rooms\nest sto m2\nesttab m0 m1 m2, se varwidth(20) star(* 0.1 ** 0.05 *** 0.01) nogaps\n\n\n--------------------------------------------------------------------\n                              (1)             (2)             (3)   \n                           lprice          lprice          lprice   \n--------------------------------------------------------------------\nlnox                       -0.968***       -0.975***       -0.975***\n                          (0.110)         (0.106)         (0.106)   \ndist                      -0.0291***      -0.0223**       -0.0223** \n                         (0.0102)       (0.00995)       (0.00995)   \nrooms                       0.302***       -0.724***       -0.724***\n                         (0.0189)         (0.171)         (0.171)   \nrooms2                                     0.0794***                \n                                         (0.0131)                   \nc.rooms#c.rooms                                            0.0794***\n                                                         (0.0131)   \n_cons                       9.793***        13.05***        13.05***\n                          (0.271)         (0.599)         (0.599)   \n--------------------------------------------------------------------\nN                             506             506             506   \n--------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01\n\n\n\nNegative coefficient for \\(rooms\\), so is there a problem?\n\nFind “turnpoint” and summary Stats\n\n\nTurn point: 4.55\n\n\n\n    Variable |       Min        p1        p5       p10       p25       p50       p75       p90       p99       Max\n-------------+----------------------------------------------------------------------------------------------------\n       rooms |      3.56      4.52       5.3      5.59      5.88      6.21      6.62      7.15      8.34      8.78\n------------------------------------------------------------------------------------------------------------------\n\n\n\nDoes it make a difference how we estimate the model?\n\n\nqui:reg lprice lnox dist rooms rooms2\nmargins, dydx(rooms)\nqui:reg lprice lnox dist c.rooms c.rooms#c.rooms\nmargins, dydx(rooms) \n\n\nAverage marginal effects                                   Number of obs = 506\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  rooms\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       rooms |  -.7236433   .1706763    -4.24   0.000    -1.058973   -.3883139\n------------------------------------------------------------------------------\n\nAverage marginal effects                                   Number of obs = 506\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  rooms\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       rooms |   .2747106   .0188463    14.58   0.000     .2376831    .3117382\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-interactions-i-d1d2",
    "href": "rmethods/5_FXMRA.html#functional-forms-interactions-i-d1d2",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Interactions I (\\(d1*d2\\))",
    "text": "Functional Forms: Interactions I (\\(d1*d2\\))\n\nIt is possible to use multiple (unrelated) dummy variables.\nDummy interactions are feasible to allow for differential means across groups combined groups.\nYou still need a reference group that should be identified: \\[\\begin{aligned}\ny &= a_0 + a_1 female + a_2 union + a_3 female \\times union + e \\\\\ny &= b_0 + b_1 female \\times nonunion + b_2 male \\times union +b_3 female \\times union + e\n\\end{aligned}\n\\] Both models are equivalent. Also \\[\\begin{aligned}\n& E(y|male,nonunion)    && =a_0  &&= b_0   \\\\\n& E(y|female,nonunion) && = a_0 + a_1 && = b_0 + b_1 \\\\\n& E(y|male,union) && =a_0+a_2  &&= b_0+b_2 \\\\\n& E(y|female,union)  && = a_0 + a_1 + a_2 + a_3  &&= b_0 + b_3\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-1",
    "href": "rmethods/5_FXMRA.html#section-1",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "For this, you may need to use manual dummy creation, or use explicit interactions:\nreg y i.d1 i.d2 i.d1#i.d2\nreg y i.d1##i.d2\nreg y i.d1#i.d2\n\nYou set the interactions\nSimilar to one, but Stata does it for you\nCreates full set of interactions, as in 2nd model before\n\nOptions 1 and 3 will allow you using margins. For overall groups (all women, all unions) you need to decide how to get representative samples."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-interactions-ii-x1x2",
    "href": "rmethods/5_FXMRA.html#functional-forms-interactions-ii-x1x2",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Interactions II (\\(x1*x2\\))",
    "text": "Functional Forms: Interactions II (\\(x1*x2\\))\n\nYou may be interested in allowing for some interaction across continuous variables.\n\nie Interacted effect of household size and number of bedrooms\n\nAs with Polynomials, this allows for heterogeneity, thus effects are not constant.\n\n\\[\\begin{aligned}\ny &= a_0 + a_1 x_1 + a_2 x_2 + a_3 x_1 x_2 + e \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_1} &= a_1  + a_3 x_2 \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_2} &= a_2  + a_3 x_1\n\\end{aligned}\n\\]\n\nThus, coefficients, on their own, are difficult to interpret, unless \\(x_1\\) or \\(x_2\\) are zero"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-2",
    "href": "rmethods/5_FXMRA.html#section-2",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Shortcut: Affine transformation\n\nThere is a trick that could help easy and direct interpretation. re-scaling variables:\n\n\\[\\begin{aligned}\ny &= b_0 + b_1 x_1 + b_2 x_2 + b_3 (x_1-\\bar x_1)(x_2-\\bar x_2) + e \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_1} &= b_1  + b_3 (x_2-\\bar x_2) \\simeq b_1 \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_2} &= b_2  + b_3 (x_1-\\bar x_1) \\simeq b_2\n\\end{aligned}\n\\]\n\nAlso works with quadratic terms!\n\n\\[\\begin{aligned}\ny &= b_0 + b_1 x_1 + b_2 (x_1-\\bar x_1)^2 + b_3 x_2 + e \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_1} &= b_1  + 2 b_2 (x_1-\\bar x_1) \\simeq b_1 \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-interactions-iii-d1x1",
    "href": "rmethods/5_FXMRA.html#functional-forms-interactions-iii-d1x1",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Interactions III (\\(d1*x1\\))",
    "text": "Functional Forms: Interactions III (\\(d1*x1\\))\n\nDummy variables allows for shifts to the constant (intercept).\nInteracting with continuous variables allows for shifts in slopes!.\n\nThis can be useful to testing hypothesis: differences in returns to education by gender.\n\n\n\\[wage=b_0 + b_1 female + b_2 educ + b_3 educ \\times female + e\n\\]\n\n\\(b_1\\): Baseline wage differential between men and women.\n\\(b_2+b_3\\): Returns to education for women.\n\\(b_1 + b_3 \\overline{educ}\\): Average wage difference between men and women.\n\nStata:\nreg y x1 i.d c.x1#i.d"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-full-interactions-with-dummies",
    "href": "rmethods/5_FXMRA.html#functional-forms-full-interactions-with-dummies",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Full Interactions (with dummies)",
    "text": "Functional Forms: Full Interactions (with dummies)\n\nIt is possible to estimate models where all variables are interacted with a single dummy. This allows you to test the hypothesis if two groups have the same underlying parameters.\n\nDo men and women have the same wage structure?\n\nFull interactions is equivalent to estimating separate models:\n\n\\[\\begin{aligned}\nFT: & y = b_0 + b_1 x_1 + b_2 x_2 + g_0 d +g_1 x_1 d +g_2 x_2 d +e \\\\\nD0: & y = b_0 + b_1 x_1 + b_2 x_2  +e  && \\text{ if d=0 } \\\\\nD1: & y = (b_0+g_0) + (b_1+g_1) x_1 + (b_2+g_2) x_2 +e && \\text{ if d=1 } \\\\\n    & y = a_0 + a_1 x_1 + a_1 x_2 +e && \\text{ if d=1 } \\\\\nCS1: & H_0: g_0=g_1=g_2=0\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-3",
    "href": "rmethods/5_FXMRA.html#section-3",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Chow test\n\n\\(CS1\\) can be tested using F-stat for multiple hypothesis.\nBut, under homoskedasticty, one could also use what is known as the Chow test\n\n\\[\\begin{aligned}\nM1 &: y = b_0 + b_1 x_1 + b_2 x_2 + e \\\\\nM2 &: y = b_0 + b_1 x_1 + b_2 x_2 + b_3 d + e \\\\\nif \\ D=0 &: y = b_{00} + b_{01} x_1 + b_{02} x2 + e_0 \\\\\nif \\ D=1 &: y = b_{10} + b_{11} x_1 + b_{12} x2 + e_1\n\\end{aligned}\n\\]\nF-Stat (similar to before):\n\\[\\begin{aligned}\nF_{M1} = \\frac{(SSR_{M1}-SSR_0-SSR_1)/(k+1)}{(SSR_0+SSR_1)/(n - 2(k+1))} \\\\\nF_{M2} = \\frac{(SSR_{M2}-SSR_0-SSR_1)/k}{(SSR_0+SSR_1)/(n - 2(k+1))}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#example-3",
    "href": "rmethods/5_FXMRA.html#example-3",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\n\n frause gpa3, clear\ndrop if cumgpa==0\nreplace sat = sat /100\nqui:reg cumgpa sat hsperc tothrs\nest sto m1\nqui:reg cumgpa sat hsperc tothrs female\nest sto m2\nqui:reg cumgpa sat hsperc tothrs if female==0\nest sto m3\nqui:reg cumgpa sat hsperc tothrs if female==1\nest sto m4\nqui:reg cumgpa i.female##c.(sat hsperc tothrs)\nest sto m5\nesttab m1 m2 m3 m4 m5, mtitle( Simple With_fem Men Women Full_int) ///\nse star(* .1 ** 0.05 *** 0.01) nogaps noomitted \n\n(98 observations deleted)\nvariable sat was int now float\n(634 real changes made)\n\n--------------------------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)             (5)   \n                   Simple        With_fem             Men           Women        Full_int   \n--------------------------------------------------------------------------------------------\nsat                0.0933***       0.0938***       0.0679***        0.177***       0.0679***\n                 (0.0133)        (0.0130)        (0.0151)        (0.0244)        (0.0146)   \nhsperc           -0.00865***     -0.00730***     -0.00748***     -0.00869***     -0.00748***\n                (0.00105)       (0.00106)       (0.00119)       (0.00219)       (0.00116)   \ntothrs          -0.000599       -0.000586        -0.00155**       0.00141        -0.00155** \n               (0.000662)      (0.000647)      (0.000771)       (0.00111)      (0.000748)   \nfemale                              0.277***                                                \n                                 (0.0493)                                                   \n0.female                                                                                0   \n                                                                                      (.)   \n1.female                                                                           -0.855** \n                                                                                  (0.333)   \n1.female#c~t                                                                        0.109***\n                                                                                 (0.0310)   \n1.female#c~c                                                                     -0.00121   \n                                                                                (0.00271)   \n1.female#c~s                                                                      0.00296** \n                                                                                (0.00145)   \n_cons               1.900***        1.782***        2.070***        1.215***        2.070***\n                  (0.149)         (0.147)         (0.173)         (0.257)         (0.168)   \n--------------------------------------------------------------------------------------------\nN                     634             634             483             151             634   \n--------------------------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;.1, ** p&lt;0.05, *** p&lt;0.01\n\n\n\ntest 1.female#c.sat 1.female#c.hsperc 1.female#c.tothrs\ntest 1.female 1.female#c.sat 1.female#c.hsperc 1.female#c.tothrs\nmargins female, dydx(sat hsperc tothrs)\n\n\n ( 1)  1.female#c.sat = 0\n ( 2)  1.female#c.hsperc = 0\n ( 3)  1.female#c.tothrs = 0\n\n       F(  3,   626) =    6.26\n            Prob &gt; F =    0.0003\n\n ( 1)  1.female = 0\n ( 2)  1.female#c.sat = 0\n ( 3)  1.female#c.hsperc = 0\n ( 4)  1.female#c.tothrs = 0\n\n       F(  4,   626) =   12.75\n            Prob &gt; F =    0.0000\n\nAverage marginal effects                                   Number of obs = 634\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  sat hsperc tothrs\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nsat          |\n      female |\n          0  |   .0679302    .014607     4.65   0.000     .0392454    .0966149\n          1  |   .1772582   .0273126     6.49   0.000     .1236229    .2308936\n-------------+----------------------------------------------------------------\nhsperc       |\n      female |\n          0  |    -.00748   .0011573    -6.46   0.000    -.0097526   -.0052073\n          1  |  -.0086922   .0024524    -3.54   0.000    -.0135081   -.0038763\n-------------+----------------------------------------------------------------\ntothrs       |\n      female |\n          0  |  -.0015482   .0007477    -2.07   0.039    -.0030165   -.0000798\n          1  |    .001412   .0012472     1.13   0.258    -.0010371    .0038612\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#avg-partial-effects-vs-partial-effects-at-x_c",
    "href": "rmethods/5_FXMRA.html#avg-partial-effects-vs-partial-effects-at-x_c",
    "title": "Multiple Regression Analysis",
    "section": "Avg Partial effects vs Partial effects at \\(X_c\\)",
    "text": "Avg Partial effects vs Partial effects at \\(X_c\\)\n\nWhenever you have interactions, higher order polynomials (or any nonlinear transformation of \\(X\\)), marginal effects are no longer constant, and may depend on additional information:\n\n\\[y = b_0 + b_1 x_1 + b_2 x_1^2 + e \\rightarrow \\frac{dy}{dx} = b_1 + 2b_2 x_1\n\\]\n\nWhat to do in this cases?\n\nEstimate Average marginal effects: \\(AME = E\\left(\\frac{dy}{dx}\\right) = b_1 + 2b_2 \\overline{x}_1\\)\nEstimate Marginal effects at means: \\(MEM = \\frac{dy}{dx}\\Big|_{x=\\bar x} = b_1 + 2b_2 \\overline{x}_1\\)\nEstimate Marginal effects at relevant values\nReport ALL marginal effects"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-4",
    "href": "rmethods/5_FXMRA.html#section-4",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "In Stata you can do this only for interactions. For constructed variables you need f_able, or do it by hand.\n\nreg y c.x1##c.x1##c.x1\nmargins, dydx(x1) &lt;-- Default is Average Marginal Effects\nmargins, dydx(x1) atmeans &lt;-- Request marginal effects at means\nmargins, dydx(x1) at(x1=(1/5)) &lt;-- Request marginal effects at specific values of x1\n* and plot afterwards\nmarginsplot"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#goodness-of-fit-r2-vs-r2_adj",
    "href": "rmethods/5_FXMRA.html#goodness-of-fit-r2-vs-r2_adj",
    "title": "Multiple Regression Analysis",
    "section": "Goodness of Fit: \\(R^2\\) vs \\(R^2_{adj}\\)",
    "text": "Goodness of Fit: \\(R^2\\) vs \\(R^2_{adj}\\)\nWith Great power…\nIMPORTANT: Low \\(R^2\\) does not mean a bad model, nor high \\(R^2\\) mean a good one.\n\nIf \\(N\\) is constant, adding more variables to your model will increase the Goodness of fit \\(R^2\\) (even if marginally)\n\nThis may lead to the incorrect intuition of choosing models with the highest \\(R^2\\)\nThis is wrong because \\(R^2\\) only measures in-sample fitness.\n\nAlternative, the Adjusted \\(R^2\\) (\\(R_{adj}^2\\)), which penalizes using multiple controls\n\n\\[R^2_{adj} = 1-\\frac{SSR/(n-k-1)}{SST/(n-1)}=1-(1-R^2)\\frac{n-1}{n-k-1}\n\\]\n\nMore controls \\(k\\) will not always increase \\(R^2_{adj}\\)"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-5",
    "href": "rmethods/5_FXMRA.html#section-5",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "\\(R^2_{adj}\\) and Model Selection\n\n\\(R^2_{adj}\\) can be used to choose between nested models.\n\nIf adding variables improves \\(R_{adj}^2\\), then choose that model.\n\nBut it can also be used to choose between non-nested models:\n\n\\[\\begin{aligned}\nM1: & y = b_0 + b_1 x_1 + b_2 x_2 + e \\\\\nM2: & y = b_0 + b_1 x_1 + b_3 x_3 + e \\\\\nM3: & y = b_0 + b_1 ln(x_1) + b_2 ln(x_2) + e \\\\\nM4: & y = b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3 + e\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#log-models-and-prediction",
    "href": "rmethods/5_FXMRA.html#log-models-and-prediction",
    "title": "Multiple Regression Analysis",
    "section": "log models and Prediction",
    "text": "log models and Prediction\n\nTransforming the Depvariable with logs is quite useful for interpretation, and addressing overdispersion\nHowever, obtaining predictions from such models is not straight forward:\n\n\\[\\begin{aligned}\nln(y) &= a_0 + a_1 x_1 + a_2 x_2 + \\varepsilon \\\\\ny &= exp(a_0 + a_1 x_1 + a_2 x_2 + \\varepsilon ) \\\\\nE(y|x_1,x_2) &=E(e^{a_0 + a_1 x_1 + a_2 x_2}) \\times E(e^ \\varepsilon ) \\\\\n& E(e^ \\varepsilon )\\neq 1\n\\end{aligned}\n\\]\n\nTo make Predictions in a log model we need some approximation for \\(E(e^ \\varepsilon )\\)"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-6",
    "href": "rmethods/5_FXMRA.html#section-6",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "We have Options:\nLets call \\(E(e^ \\varepsilon ) = \\alpha_0\\)\nOption 1 : \\(\\alpha_0 = n^{-1} \\sum( \\exp {\\hat\\varepsilon})\\)\nOption 2 : Under Normality of \\(\\varepsilon\\), \\(\\alpha_0 = \\exp(\\hat \\sigma^2/2)\\)\nOption 3 : Call \\(\\hat m = \\exp(a_0 + a_1 x_1 + a_2 x_2)\\).\nRegress \\(y\\) on \\(\\hat m\\) without intercept. \\(\\alpha_0 = \\frac{\\hat m'y}{\\hat m'\\hat m}\\)\n\nYour \\(\\hat y\\) prediction can now be used to estimate a comparable \\(R^2\\)\n\n\\[R^2 = Corr(y,\\hat y)^2 \\text{ or } 1-\\frac{\\sum(y_i-\\alpha_0 \\hat m_i)^2}{\\sum(y-\\bar y)^2}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-7",
    "href": "rmethods/5_FXMRA.html#section-7",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Example\n\nfrause oaxaca, clear\ndrop if lnwage==.\ngen wage = exp(lnwage)\nqui:reg lnwage educ exper tenure female married divorced\npredict lnw_hat\npredict lnw_res, res\n** Case 1:\negen alpha_01 = mean( exp(lnw_res))\n** Case 2:\nqui:sum lnw_res\ngen alpha_02 = exp(r(Var)/2)\ngen elnw_hat = exp(lnw_hat)\nqui: reg wage elnw_hat, nocons\ngen alpha_03 = _b[elnw_hat]\ngen wage_1 = elnw_hat\ngen wage_2 = elnw_hat*alpha_01\ngen wage_3 = elnw_hat*alpha_02\ngen wage_4 = elnw_hat*alpha_03\nmata:  y = st_data(.,\"wage\"); my = mean(y)\nmata:  yh = st_data(.,\"wage_1 wage_2 wage_3 wage_4\")\nmata:\"R2_1 \"; 1 - sum((y:-yh[,1]):^2)/sum( (y:-my):^2 )\nmata:\"R2_2 \"; 1 - sum((y:-yh[,2]):^2)/sum( (y:-my):^2 )\nmata:\"R2_3 \"; 1 - sum((y:-yh[,3]):^2)/sum( (y:-my):^2 )\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n(option xb assumed; fitted values)\n  R2_1 \n  .1569552664\n  R2_2 \n  .1692562931\n  R2_3 \n  .1658805115"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#limited-dependent-variables",
    "href": "rmethods/5_FXMRA.html#limited-dependent-variables",
    "title": "Multiple Regression Analysis",
    "section": "Limited Dependent variables",
    "text": "Limited Dependent variables\n\nSo far, we have impliclity assumed your dep. variable is continuous and unbounded.\nHowever, OLS imposes no distributional assumptions (A6 is more convinience)\nThis means that LRM using OLS can be used for variables with limited distribution!\n\nlike Dummies or count variables"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#linear-probability-model---lpm",
    "href": "rmethods/5_FXMRA.html#linear-probability-model---lpm",
    "title": "Multiple Regression Analysis",
    "section": "Linear Probability Model - LPM",
    "text": "Linear Probability Model - LPM\n\nLPM can be used when the dep.variable is a dummy, and the goal is to explain the Likelihood of something to happen.\n\n\\[\\begin{aligned}\nD &= b_0 + b_1 x_1 + b_2 x_2 +b_3 x_3 + e \\\\\nE(D|Xs) &= P(D=1|Xs) \\\\\n        &= b_0 + b_1 x_1 + b_2 x_2 +b_3 x_3\n\\end{aligned}\n\\]\nNote:\n\nFor marginal effects, we no longer consider effects at the individual level.\nInstead we look into conditional means, and likelihood\n\n\nCode\nfrause mroz, clear\nqui: reg inlf  age educ exper kidsge6 kidslt6 nwifeinc \nmodel_display\n\nE(inlf|X) = 0.707 - 0.018 age + 0.040 educ + 0.023 exper + 0.013 kidsge6 - 0.272 kidslt6 - 0.003 nwifeinc"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-8",
    "href": "rmethods/5_FXMRA.html#section-8",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Problems with LPM\n\nLPM are easy to estimate and interpret but it has some problems:\n\nPredictions could fall below 0 or above 1 (what does it mean?)\nUnless more flexible functional forms are allowed, mfx are fixed.\nThe model is, by construction, Heteroskedastic:\n\n\n\\[Var(y|x)=p(x)*(1-p(x))\n\\]\nThus SE will be incorrect, affecting inference"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#modeling-count-data",
    "href": "rmethods/5_FXMRA.html#modeling-count-data",
    "title": "Multiple Regression Analysis",
    "section": "Modeling Count Data",
    "text": "Modeling Count Data\n\nYou could also use LRM (via OLS) to model count data.\n\nCount data is always possitive, but with discrete values\n\n\n\\[Children = b_0 + b_1 age + b_2 education + e\\]\n\nNothing changes for estimation, but its useful to change language:\n\n\nCode\nfrause fertil2, clear\nqui reg children age educ\nmodel_display\n\nE(children|X) = -1.997 + 0.175 age - 0.090 educ\n\n1 year of education decreases # of children in .09.\n1 year of education decreases Fertility .09 children per women.\nEvery 100 women, If they were 1 year more educated, we would expect to see 9 fewer children among them."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#prediction-policy-and-shifting",
    "href": "rmethods/5_FXMRA.html#prediction-policy-and-shifting",
    "title": "Multiple Regression Analysis",
    "section": "Prediction, Policy and Shifting",
    "text": "Prediction, Policy and Shifting\n\nAs mentioned before, intercepts, or constant in model regressions are usually meaningless.\n\nBecause \\(a_0 = E(y|X=0)\\) (does it make sense)\n\nConstant, however, can be useful if we apply some transformations to the data. \\[y = b_0 +  b_1 (x_1 - c_1) +  b_2 (x_2 - c_2) +  b_3 (x_3 - c_3) +e\n\\]\n\nIn this case \\(b_0\\) is the expected value of \\(y\\) when \\(x_1=c_1\\), \\(x_2=c_2\\) and \\(x_3=c_3\\). Thus, its now Useful!\n\nUsing this affine transformation, we can easily make predictions (and get SE) for any specific values of interest.\n\nGranted, you could also use “margins”\n\n\n\n\nCode\nfrause gpa2, clear\ngen sat0=sat-1200\ngen hsperc0=hsperc-30\ngen hsize0=hsize-5\ngen hsize20=hsize^2-25\nqui:reg colgpa sat hsperc c.hsize##c.hsize\nmargins, at(sat = 1200 hsperc = 30 hsize = 5)\nreg colgpa sat0 hsperc0 hsize0 hsize20\n\n\n\nAdjusted predictions                                     Number of obs = 4,137\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\nAt: sat    = 1200\n    hsperc =   30\n    hsize  =    5\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _cons |   2.700075   .0198778   135.83   0.000     2.661104    2.739047\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     4,137\n-------------+----------------------------------   F(4, 4132)      =    398.02\n       Model |  499.030503         4  124.757626   Prob &gt; F        =    0.0000\n    Residual |  1295.16517     4,132  .313447524   R-squared       =    0.2781\n-------------+----------------------------------   Adj R-squared   =    0.2774\n       Total |  1794.19567     4,136  .433799728   Root MSE        =    .55986\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        sat0 |   .0014925   .0000652    22.89   0.000     .0013646    .0016204\n     hsperc0 |  -.0138558    .000561   -24.70   0.000    -.0149557   -.0127559\n      hsize0 |  -.0608815   .0165012    -3.69   0.000    -.0932328   -.0285302\n     hsize20 |   .0054603   .0022698     2.41   0.016     .0010102    .0099104\n       _cons |   2.700075   .0198778   135.83   0.000     2.661104    2.739047\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-9",
    "href": "rmethods/5_FXMRA.html#section-9",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Policy Evaluation\n\nWhen modeling \\(y = b_0 + \\delta \\ trt + b_1 x_1 + b_2 x_2 + e\\) the treatment effect \\(\\delta\\) was estimated under homogeneity assumption (only intercept shift)\nThis assumption can be relaxed by estimating separate models or using interactions.\nEffects can be estimated manually (separate models), margins (for ATE’s) or using shifts!\n\nUsing Separate models: \\[\\begin{aligned}\ny &= b^0_0 +  b^0_1 x_1 + b^0_2 x_2 + e^0 \\text{ if trt=0} \\\\\ny &= b^1_0 +  b^1_1 x_1 + b^1_2 x_2 + e^1 \\text{ if trt=1} \\\\\n& ATE = E(\\hat y_1 - \\hat y_0 ) \\\\\n& ATT = E(\\hat y_1 - \\hat y_0 | trt=1) \\\\\n& ATU = E(\\hat y_1 - \\hat y_0 | trt=0)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-10",
    "href": "rmethods/5_FXMRA.html#section-10",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Or using Model Shits\n\\[\\begin{aligned}\ny  &= b_0 + \\delta_{ate} trt + b_1 x_1 + g_1 trt (x_1- E(x_1)) + e \\\\\ny  &= b_0 + \\delta_{att} trt + b_1 x_1 + g_1 trt (x_1- E(x_1|trt=1)) + e \\\\\ny  &= b_0 + \\delta_{atu} trt + b_1 x_1 + g_1 trt (x_1- E(x_1|trt=0)) + e \\\\\n\\end{aligned}\n\\]\n\n\nCode\nfrause jtrain98, clear\nforeach i in earn96 educ age married {\n  sum `i' if train==0, meanonly\n  gen atu_`i' = (`i' - r(mean))*train\n  sum `i' if train==1, meanonly\n  gen att_`i' = (`i' - r(mean))*train\n  sum `i' , meanonly\n  gen ate_`i' = (`i' - r(mean))*train\n}\nqui:reg earn98 train earn96 educ age married\nest sto m1\nqui:reg earn98 train earn96 educ age married ate*\nest sto m2\nqui:reg earn98 train earn96 educ age married atu*\nest sto m3\nqui:reg earn98 train earn96 educ age married att*\nest sto m4\n\nesttab m1 m2 m3 m4, keep(train) mtitle(Homogenous ATE ATU ATT) se\n\n\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n               Homogenous             ATE             ATU             ATT   \n----------------------------------------------------------------------------\ntrain               2.411***        3.106***        3.533***        2.250***\n                  (0.435)         (0.532)         (0.667)         (0.449)   \n----------------------------------------------------------------------------\nN                    1130            1130            1130            1130   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/7_spec.html#what-do-we-mean-with-model-miss-specification",
    "href": "rmethods/7_spec.html#what-do-we-mean-with-model-miss-specification",
    "title": "Multiple Regression Analysis",
    "section": "What do we mean with model miss-specification",
    "text": "What do we mean with model miss-specification\n\nThere are various kinds of model specification we will talk about.\n\nThere are important variables you did not include in your model: Endogeneity\nYou added all relevant variables…just not in the right way.\nYou added proxies for variables you had no access to (Question change)\nYou have all relevant data, but with errors.\nYou have some missing data"
  },
  {
    "objectID": "rmethods/7_spec.html#section",
    "href": "rmethods/7_spec.html#section",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Simple linear functions work in almost ALL cases. They can be thought as first order Taylor expansions: \\[\\begin{aligned}\ny &= f(x) + e \\\\\nf(x) &\\simeq f(x_0)\n+\\frac{\\partial f(x)}{\\partial x}|_{x=x_0}\n(x-x_0)+R+e \\\\\nf(x) &\\simeq \\color{red}{ f(x_0)}\n\\color{red}{-\\frac{\\partial f(x)}{\\partial x}|_{x=x_0} x_0}\n+\\frac{\\partial f(x)}{\\partial x}|_{x=x_0} x+R+e \\\\\ny &= \\color{red}{\\beta_0}+\\beta_1 x + R+ e\n\\end{aligned}\n\\]\n\nSo, for “reasonable” values of X, or when analyzing average marginal effects \\(R\\) should be small enough to be ignored.\n\nIn other words, for Overall effects Simple linear model works reasonably well! (most of the time)"
  },
  {
    "objectID": "rmethods/7_spec.html#section-1",
    "href": "rmethods/7_spec.html#section-1",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "If you are interested in individuals (or alike people), you may need flexiblity!\nIgnoring functional form misspecification imposes unwanted assumptions (homogeneity), that could create further problems.\n\nSpecially if data is skewed\n\nBut how flexible is flexible enough?\n\nWe will only consider quadratic terms and interactions,\nbut there is a large literature on making very flexible estimations (non-paramatric analysis)\n\n\n\n\nCode\nclear\nset seed 10\nset obs 1000\ngen p = (2*_n-1)/(2*_N) \ngen x = invchi2(5, p)/2\ngen y = 1 + x + (x-2.5)^2 + rnormal()  \nreg y x\ndisplay \"Quadratic\"\nqui:reg y c.x##c.x\nmargins, dydx(x)\ndisplay \"Cubic\"\nqui:reg y c.x##c.x##c.x\nmargins, dydx(x)\n\n\nNumber of observations (_N) was 0, now 1,000.\n\n      Source |       SS           df       MS      Number of obs   =     1,000\n-------------+----------------------------------   F(1, 998)       =   1287.16\n       Model |  22189.0552         1  22189.0552   Prob &gt; F        =    0.0000\n    Residual |  17204.2788       998  17.2387563   R-squared       =    0.5633\n-------------+----------------------------------   Adj R-squared   =    0.5628\n       Total |  39393.3339       999  39.4327667   Root MSE        =     4.152\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |   2.983973   .0831722    35.88   0.000      2.82076    3.147185\n       _cons |  -1.467351   .2458875    -5.97   0.000    -1.949866   -.9848348\n------------------------------------------------------------------------------\nQuadratic\n\nAverage marginal effects                                 Number of obs = 1,000\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  x\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |   1.033401   .0258934    39.91   0.000     .9825894    1.084213\n------------------------------------------------------------------------------\nCubic\n\nAverage marginal effects                                 Number of obs = 1,000\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  x\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |   1.041387   .0292892    35.56   0.000     .9839117    1.098863\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/7_spec.html#reset-ramsey-test",
    "href": "rmethods/7_spec.html#reset-ramsey-test",
    "title": "Multiple Regression Analysis",
    "section": "Reset Ramsey test",
    "text": "Reset Ramsey test\n\nIntuition: If the model is misspecified, perhaps we need to control for more non-linearities and interactions.\nNaive test: Add more controls (quadratics and interactions) (like White test, this will grow fast)\nReset - Ramsey test: Get predictions from original model, and add it as control\n\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\delta_1 \\hat y^2 + \\delta_2 \\hat y^3 +e\n\\]\n\\(H_0: \\delta_1 = \\delta_2 = 0\\): (everything is awesome)\n\\(H_1: H_0\\) is false: we need to fix the problem\n\nRRT does not tell you “How” to fix the problem.\n\nestat ovtest\n(bad name tho)"
  },
  {
    "objectID": "rmethods/7_spec.html#davidson-mackinnon-test",
    "href": "rmethods/7_spec.html#davidson-mackinnon-test",
    "title": "Multiple Regression Analysis",
    "section": "Davidson-MacKinnon test",
    "text": "Davidson-MacKinnon test\nTwo non-tested models:\n\\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + e \\\\\ny &= \\gamma_0 + \\gamma_1 log(x_1) + \\gamma_2 log(x_2) + e \\\\\n\\end{aligned}\n\\]\n\nWhich one is more appropriate? eq1? or eq2? This are non-nested models, so its difficult to say.\n\nYou could nest them:\n\n\n\\[y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 log(x_1) + \\theta_4 log(x_2) + e\n\\]\nand test \\(\\theta_1=\\theta_2=0\\) or \\(\\theta_3=\\theta_4=0\\)."
  },
  {
    "objectID": "rmethods/7_spec.html#section-2",
    "href": "rmethods/7_spec.html#section-2",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "or the “true” Davidson-MacKinnon test:\n\nFirst Obtain predictions from competing models: \\[\\begin{aligned}\n\\hat y &= \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 \\\\\n\\check y &= \\hat \\gamma_0 + \\hat\\gamma_1 log(x_1) + \\hat\\gamma_2 log(x_2) \\\\\n\\end{aligned}\n\\]\nThen add the predictions as added controls in the alternative model: \\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\theta_1 \\check y +e \\\\\ny &= \\gamma_0 + \\gamma_1 log(x_1) + \\gamma_2 log(x_2) + \\theta_1 \\hat y + e \\\\\n\\end{aligned}\n\\]\n\nUnfortunately, you may ended up with conflicting results."
  },
  {
    "objectID": "rmethods/7_spec.html#a-re-tell-of-omitted-variable-bias",
    "href": "rmethods/7_spec.html#a-re-tell-of-omitted-variable-bias",
    "title": "Multiple Regression Analysis",
    "section": "A re-tell of Omitted variable Bias",
    "text": "A re-tell of Omitted variable Bias\n\nWe know this. If a variable that SHOULD be in the model is not added, it will generate an OMV, unless it was uncorrelated to the model error.\n\nLesson: add important variables!\n\nWhat if those variables are not available? how do you solve the problem?\n\nIV (we will talk about that later) or\nProxy Variable (a bandaid)"
  },
  {
    "objectID": "rmethods/7_spec.html#proxies",
    "href": "rmethods/7_spec.html#proxies",
    "title": "Multiple Regression Analysis",
    "section": "Proxies",
    "text": "Proxies\nConsider: \\[log(wages) = \\beta_0 + \\color{blue}{\\beta_1} exper + \\color{blue}{\\beta_2} educ + \\beta_3 skill + e\n\\]\nWhere you are really interested in \\(\\beta_1 \\And \\beta_2\\).\n\nSince we dont have \\(skill\\), and omitting it will bias our coefficients, we can use a proxy \\(ASVAB\\).\n\n\\[log(wages) = \\beta_0 + \\color{blue}{\\beta_1} exper + \\color{blue}{\\beta_2} educ + \\gamma_3 ASVAB + e\n\\]\n\nand done?"
  },
  {
    "objectID": "rmethods/7_spec.html#section-3",
    "href": "rmethods/7_spec.html#section-3",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Using a Proxy will work only under the following condition:\n\nConditioning on the observed variable and proxy, the unobserved variable has to be uncorrelated to other variables in the model:\n\n\\[\\begin{aligned}\nE(x_3^*|x_1,x_2,x_3)&=\\alpha_0 + \\alpha_1 x_3 \\\\\nE(skill|exper,educ,ASVAB)&=\\alpha_0 + \\alpha_1 ASVAB\n\\end{aligned}\n\\]\nIf this happens, you can still estimate \\(\\beta_1 \\And \\beta_2\\), although the constant and slope of the proxy varible will be biased for the proxied variable.\n\\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x^*_3 + e \\ ; \\\n\\color{blue}{x^*_3 =  \\delta_0 + \\delta_1 x_3 + v} \\\\\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (\\delta_0 + \\delta_1 x_3 + v) + e \\\\\n&= \\color{brown}{\\beta_0 +\\beta_3\\delta_0} \\color{black}{+ \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 \\delta_1 x_3 +} \\color{green}{\\beta_3 v + e} \\\\\n&=\\color{brown}{\\alpha_0} + \\beta_1 x_1 + \\beta_2 x_2 + \\alpha_1 x_3 + \\color{green}{u} \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/7_spec.html#section-4",
    "href": "rmethods/7_spec.html#section-4",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "What about Lags (of dep variable)?\n\nIncreses Data requirements (panel? pseudo panel?)\nFurther assumptions are required (Past exogenous of present)\nBut allows controlling for underlying factors or historical factors\n\n\n\nCode\nfrause crime2, clear\nqui:reg crmrte unem llawexpc if year == 87\nest sto m1\nqui:reg crmrte unem llawexpc lcrmrt_1 if year == 87\nest sto m2\nqui:reg ccrmrte unem llawexpc if year==87  \nest sto m3\nesttab m1 m2 m3, se star(* .1 ** 0.05 *** 0.01) b(3) ///\nmtitle(crimert crimert change_crrt)\n\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                  crimert         crimert     change_crrt   \n------------------------------------------------------------\nunem               -3.659           0.346          -0.125   \n                  (3.471)         (2.127)         (2.152)   \n\nllawexpc           16.452         -20.059*        -10.377   \n                 (18.531)        (11.842)        (11.487)   \n\nlcrmrt_1                          127.111***                \n                                 (14.399)                   \n\n_cons              10.655        -337.106***       79.288   \n                (134.223)        (89.507)        (83.200)   \n------------------------------------------------------------\nN                      46              46              46   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;.1, ** p&lt;0.05, *** p&lt;0.01\n\n\nNote: Skip 9-2c and 9-3"
  },
  {
    "objectID": "rmethods/7_spec.html#why-is-x-not-the-real-x",
    "href": "rmethods/7_spec.html#why-is-x-not-the-real-x",
    "title": "Multiple Regression Analysis",
    "section": "Why is \\(X\\) not the real \\(X\\)?",
    "text": "Why is \\(X\\) not the real \\(X\\)?\n\nOften we treat data as if it they were perfect measures of the true data. But is that the case?\n\nAge: Do you report age in years, months, days, hours, minutes, etc\nWeight and Height: Even if measured, how accurate it can be? and do they make mistakes?\nIncome: Do people report income accurately? or they Lie? why?\n\nDepending on the type of error, magnitude, and if the affected variable is dep or indep, it may have diffrent consequences for OLS.\nFor now we will concentrate on a specific kind of measurement error: Classical measurement error\n\n\\[\\begin{aligned}\ny_{obs} &= y_{true} + \\varepsilon \\\\\nE(\\varepsilon) &=0; cov(\\varepsilon,y_{true})=0; cov(\\varepsilon,X's)=0\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/7_spec.html#error-in-y-dep-variable",
    "href": "rmethods/7_spec.html#error-in-y-dep-variable",
    "title": "Multiple Regression Analysis",
    "section": "Error in \\(y\\) (dep variable)",
    "text": "Error in \\(y\\) (dep variable)\n\nInstead of: \\(y^* = x\\beta + e\\)\nWe estimate \\(y^*+\\varepsilon = x\\beta + e \\rightarrow y^* = x\\beta + e-\\varepsilon\\)\nThis implies that \\(\\beta's\\) can still be unbiased when applying OLS.\nHowever variance will be larger than when using true data:\n\n\n\nCode\nqui: frause oaxaca, clear\nset seed 101\ngen lnwage2=lnwage + rnormal(2) \nqui:reg lnwage educ exper female\nest sto m1\nqui:reg lnwage2 educ exper female\nest sto m2\nesttab m1 m2, se\n\n\n(213 missing values generated)\n\n--------------------------------------------\n                      (1)             (2)   \n                   lnwage         lnwage2   \n--------------------------------------------\neduc               0.0858***       0.0902***\n                (0.00521)        (0.0120)   \n\nexper              0.0147***       0.0171***\n                (0.00126)       (0.00291)   \n\nfemale            -0.0949***      -0.0759   \n                 (0.0251)        (0.0580)   \n\n_cons               2.219***        4.132***\n                 (0.0687)         (0.159)   \n--------------------------------------------\nN                    1434            1434   \n--------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/7_spec.html#error-in-x-indep-variable",
    "href": "rmethods/7_spec.html#error-in-x-indep-variable",
    "title": "Multiple Regression Analysis",
    "section": "Error in \\(X\\) (indep variable)",
    "text": "Error in \\(X\\) (indep variable)\n\nInstead of: \\(y = \\beta_0 + \\beta_1 x^* + e\\)\nWe estimate \\(y = \\gamma_0 + \\gamma_1 (x^* + \\varepsilon) + v\\)\nBy adding an error \\(\\varepsilon\\) that has a zero relationship with \\(y\\), the “average” coefficient \\(\\gamma_1\\) will be between the true \\(\\beta_1\\) and 0. \\[\\begin{aligned}\n\\gamma_1 &=\\frac{\\sum (y-\\bar y)(x^* + \\varepsilon - \\bar x)}{\\sum (x^* + \\varepsilon - \\bar x)^2} =\\frac{\\sum (y-\\bar y)(x^* - \\bar x)+ \\sum (y-\\bar y) \\varepsilon}{\\sum (x^* - \\bar x)^2 + \\sum \\varepsilon^2} \\\\\n&= \\frac{\\sum (y-\\bar y)(x^* - \\bar x)}{\\sum (x^* - \\bar x)^2 + \\sum \\varepsilon^2} \\frac{\\sum (x^* - \\bar x)^2}{\\sum (x^* - \\bar x)^2} \\\\\n& =\\beta_1 \\frac{\\sigma^2_x}{\\sigma^2_x + \\sigma^2_\\varepsilon}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/7_spec.html#section-5",
    "href": "rmethods/7_spec.html#section-5",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Code\nfrause oaxaca, clear\nqui:sum educ\ngen educ_error = educ + rnormal()*r(sd)\nsum educ educ_error\nqui:reg lnwage educ\nest sto m1\nqui:reg lnwage educ_error\nest sto m2\nesttab m1 m2, se\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        educ |      1,647    11.40134    2.374952          5       17.5\n  educ_error |      1,647    11.36352    3.400767   .6707422   26.90462\n\n--------------------------------------------\n                      (1)             (2)   \n                   lnwage          lnwage   \n--------------------------------------------\neduc               0.0800***                \n                (0.00539)                   \n\neduc_error                         0.0399***\n                                (0.00395)   \n\n_cons               2.434***        2.898***\n                 (0.0636)        (0.0475)   \n--------------------------------------------\nN                    1434            1434   \n--------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/7_spec.html#missing-data-assume-sample-is-complete",
    "href": "rmethods/7_spec.html#missing-data-assume-sample-is-complete",
    "title": "Multiple Regression Analysis",
    "section": "Missing Data (Assume Sample is complete)",
    "text": "Missing Data (Assume Sample is complete)\n\nWhat is it? you dont have data! Your \\(N\\) falls.\n\nSome data for some observations are missing.\nWe may or may not know why they are missing\nand they maybe missing at random, or following unknown patterns.\n\nIf we are Missing data, and we do not know why, its a problem. We cant know if the sample represents the population, thus cannot be used for analysis."
  },
  {
    "objectID": "rmethods/7_spec.html#section-6",
    "href": "rmethods/7_spec.html#section-6",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "How to deal with it?\n\nif Missing completely at random (MCAR), analysis can be done as usual (no effects except smaller N)\nif Missing at random (MAR), the analysis can be done, often using standard methods:\n\nMissingness depends on observed factors (\\(X's\\)).\nIt is also known as exogenous sample selection.\nIntuitively, because all factors that determine selection are exogenous, you can identify who in the population is identified (Regression for men, women, high education, etc)\n\nIf Missing not at random (MNAR), you cant address the problem with standard analysis.\n\nSome methods such as Heckman selection or truncated regression, could be used. (advanced)\nOther wise, you can’t analyze the data (in a satisfactory manner)\nIntuitively, missingness is determined by unobserved factors, which also determines the outcome. (ie Analyze high wage population only)"
  },
  {
    "objectID": "rmethods/7_spec.html#outliers-and-influencers",
    "href": "rmethods/7_spec.html#outliers-and-influencers",
    "title": "Multiple Regression Analysis",
    "section": "Outliers and influencers",
    "text": "Outliers and influencers\n\nNot all data is made equal, and not all data has the same weight when estimating regressions.\nObservations with high Influence are those with outliers based on the conditional distribution (\\(y|x\\)).\n\nWhile outliers are not necessarily bad for analysis, it is important to understand how sensitive your results are to excluding some observations.\n\nObservations with high leverage are those with unusual characteristics.(\\(X's\\))\nCombination of both may have strong impacts on the regression analysis."
  },
  {
    "objectID": "rmethods/7_spec.html#section-7",
    "href": "rmethods/7_spec.html#section-7",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Leverage of an observation is determined by the following:\n\nDefine \\(H = X(X'X)^{-1}X'\\)\nLeverage \\(h_i = H[i,i]\\)\nHigh \\(h_i\\) denotes more influence in the model. (sensitive)\n\nInfluence is typically detected based on “studentized” residuals\n\n\\[r_i =  \\frac{\\hat e}{s_{-i}\\sqrt{1-h_i}}\n\\]"
  },
  {
    "objectID": "rmethods/7_spec.html#example",
    "href": "rmethods/7_spec.html#example",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\n\n\nCode\nqui:{\nfrause oaxaca, clear\ndrop if lnwage==.\nreg lnwage educ exper tenure female age\npredict lev, lev\nsum lev, meanonly\nreplace lev=lev/r(mean)\npredict rst, rstud\n}\nset scheme white2\ncolor_style tableau\nscatter lev rst"
  },
  {
    "objectID": "rmethods/7_spec.html#solutions",
    "href": "rmethods/7_spec.html#solutions",
    "title": "Multiple Regression Analysis",
    "section": "Solutions",
    "text": "Solutions\n\nThe problem with OLS is that it provides “too much weight” to outliers.\nThis is similar to the mean, which may not be very stable with extreme distributions.\n\nThere are at least two solutions to problems with outliers.\n\nRobust Regression (different from regression with robust Standard errors)\n\nThe idea is to penalize outliers, to reduce the impact on the estimated coefficients."
  },
  {
    "objectID": "rmethods/7_spec.html#section-8",
    "href": "rmethods/7_spec.html#section-8",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Quantile (median) Regression\n\nModifies the objective function to be minized:\n\n\n\\[\\beta's=\\min_\\beta \\sum |y-x\\beta|\n\\]\n\nInstead of using the squared of errors, it uses the absolute value.\n\nby doing this, coefficients are not sensitive to outliers! (as the median is better than the mean to capture typical values)\nDrawbacks: Its slower than OLS, and it can be difficult to interpret\n\n\nrreg &lt;- Robust Regression\nqreg &lt;- Quantile Regression"
  },
  {
    "objectID": "rmethods/9_ldvm.html#what-do-we-mean-limited",
    "href": "rmethods/9_ldvm.html#what-do-we-mean-limited",
    "title": "Limited Dependent Variable Models",
    "section": "What do we mean Limited??",
    "text": "What do we mean Limited??\n\n\nCode\nclear\nset obs 2000\ngen r1 = runiform()\ngen r2 = rchi2(5)/5 \ngen r3 = round(rchi2(3))*3\ngen r4 = rnormal()\nset scheme white2\ncolor_style tableau\nhistogram r1, name(m1, replace) \nhistogram r2, name(m2, replace)\nhistogram r3, name(m3, replace) width(1)  \nhistogram r4, name(m4, replace)\ngraph combine m1 m2 m3 m4\ngraph export images/fig9_1.png, width(1000) replace\n\n\n\nLimited Dependent variables"
  },
  {
    "objectID": "rmethods/9_ldvm.html#section",
    "href": "rmethods/9_ldvm.html#section",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "What do we mean Limited??\n\nWhen we think about “limited dependent variable” models, we refer to models when the distribution of the dep.variable is “limited”\n\nIn other words. The values it can take are restricted! (positive, or only integer), within a range, etc\n\nCan you still use LRM for them?\nWill anything change if you do?\nDo we care?"
  },
  {
    "objectID": "rmethods/9_ldvm.html#no-we-dont-but..",
    "href": "rmethods/9_ldvm.html#no-we-dont-but..",
    "title": "Limited Dependent Variable Models",
    "section": "No we dont, but..",
    "text": "No we dont, but..\n\nWe dont really care. In fact we have already use LRM on that fashion:\n\nLPM: Dep variable was a Dummy\nWages: Always positive\n# Children: Countable\n\nBut, there are couple of things one should consider.\n\nModels of this kind are usually heteroskedastic by construction. (robust? Weighted?)\nPredictions could made no sense.\nThere are better models we could use to analyze the data\n\n\nBetter under some assumptions\n\nHowever, this models cannot be estimated using OLS (there is no “close form solution”)\nWe may need to learn a new method: Maximum Likelihood"
  },
  {
    "objectID": "rmethods/9_ldvm.html#probits-and-logits",
    "href": "rmethods/9_ldvm.html#probits-and-logits",
    "title": "Limited Dependent Variable Models",
    "section": "Probits and Logits",
    "text": "Probits and Logits\n\nLPM are easy, fast, and good for most data analysis (exploration). But they have some limitations.\nMost limitations can be overcome with alternative models: Logit or Probit\nIn constrast with LPM (which aims to explain individual outcomes), Logit/probit aims to explain Conditional Probabilities:\n\n\\[p(y=1|x) = G(x\\beta)\\]\n\nwhere the function \\(G()\\) makes sure the predicted outcome is always between 0 and 1.\nCaveat: Because \\(G()\\) is nonlinear, this is a nonlinear model, and marginal effects are harder to estimate."
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-1",
    "href": "rmethods/9_ldvm.html#section-1",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "What to use for \\(G()\\)\n\nTwo leading options:\n\n\\[logit: G(x\\beta) = \\frac{\\exp{x\\beta}}{1+\\exp{x\\beta}}\\] \\[probit: G(x\\beta) = \\Phi(x\\beta)=\\int_{-\\infty}^{x\\beta}\\phi(z)dz\\]\n\nBut in practice Either will work. Then why the difference?"
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-2",
    "href": "rmethods/9_ldvm.html#section-2",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "Probits and Logits: Latent variables\n\nIt all comes down to the Latent variable!\nAssumption:\n\nEverybody has a latent score on every “binary” decision: The value to a decision \\(y^*\\) \\[y^* = x\\beta + e \\]\nIf \\(y^*\\) is above certain threshold (\\(y^*&gt;0\\)), you “do” something (\\(y=1\\)). If not you dont (\\(y=0\\)).\n\nThus the choice between logit and probit depends on the distribution of \\(e\\).\n\n\\(e\\) is normal, then probit\n\\(e\\) is logistic, then logit"
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-3",
    "href": "rmethods/9_ldvm.html#section-3",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "Some Math\nLatent Model:\n\\[ y^* = x\\beta + e \\]\nWe aim to measure the probablity of a positive latent.\n\\[\\begin{aligned}\nP(y^*&gt;0|x) & = P(x\\beta + e&gt;0|x) \\\\\n& = P( e&gt;- x\\beta|x) \\\\\n& = 1 - P( e &lt; - x\\beta|x) = 1-G( - x\\beta|x) \\\\\n& = G(x\\beta)\n\\end{aligned}\n\\]\nlast step valid only if \\(G()\\) is symetrical."
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-4",
    "href": "rmethods/9_ldvm.html#section-4",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "Marginal Effects?\n\nSame as before. The partial derivative!\n\n\\[\\begin{aligned}\np(y=1|x) &= G(\\beta_0 + \\beta_1 x_1 +\\beta_2 x_2 ) \\\\\n\\frac{\\partial p(y=1|x)}{\\partial x_1} = G'(x\\beta)\\beta_1=g(x\\beta)\\beta_1\n\\end{aligned}\n\\]\n\nBut if variables are dummies, we need to estimate true effect.\n\n\\[\\begin{aligned}\np(y=1|x) &= G(\\beta_0 + \\beta_1 x_1 +\\beta_2 D_2 ) \\\\\n\\frac{\\partial p(y=1|x)}{\\partial D_2} = G(\\beta_0 + \\beta_1 x_1 +\\beta_2 )-G(\\beta_0 + \\beta_1 x_1 )\n\\end{aligned}\n\\]\nand yes, you could also have interactions, polynomials, etc"
  },
  {
    "objectID": "rmethods/9_ldvm.html#mle-how-does-this-work",
    "href": "rmethods/9_ldvm.html#mle-how-does-this-work",
    "title": "Limited Dependent Variable Models",
    "section": "MLE: How does this work?",
    "text": "MLE: How does this work?\n\nMLE: Maximum Likelihood Estimator, is an alternative method to OLS that allows you to estimate parameters in nonlinear models.\nThe idea of the method is to “model” the conditional distribution of the data \\(F(y|x,\\theta)\\) or \\(f(y|x,\\theta)\\), assuming \\(X's\\) are given and modifying values of \\(\\theta\\) (distribution parameters).\n\\(LRM\\) could be estimated via MLE, but you will need More assumptions:\n\nThe error \\(e\\) is normal.\n\nThen “simply” find the parameters for the mean and variance that “maximizes” the probability that data Comes a given distribution.\nIn the case of Probit/logit, there is “only” one paramter we need to identify. The conditional probabilty \\(p(y=1|X)\\).\n\nExcept that we allow this to vary by \\(X\\)"
  },
  {
    "objectID": "rmethods/9_ldvm.html#likelihood-function-for-logitprobit",
    "href": "rmethods/9_ldvm.html#likelihood-function-for-logitprobit",
    "title": "Limited Dependent Variable Models",
    "section": "Likelihood function for Logit/probit",
    "text": "Likelihood function for Logit/probit\n\\[L_i = G(x\\beta)^{y=1}*(1-G(x\\beta))^{y=0}\n\\]\nUnder Independence:\n\\[L_D = L_1 \\times L_2 \\times \\dots L_N\n\\]\nThus we need to find the \\(\\beta's\\) that make \\(L_D\\) the largest.\nBut because we like sums over products:\n\\[LL_D = \\sum_{i=1}^N log(L_i)\n\\]"
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-5",
    "href": "rmethods/9_ldvm.html#section-5",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "Code\n  clear\n  set obs 25\n  gen r = runiform()&lt;.7\n  mata: \n    r = st_data(.,\"r\")\n    ll = J(99,2,0)\n    for(i=1;i&lt;=99;i++){\n      theta = i/100\n      // Log Properties\n      ll[i,]= theta,exp(sum(log(theta:^(r:==1) :* (1-theta):^(r:==0))))\n    }\n  end\n  qui getmata ll*=ll , force\n  ren ll1 theta\n  ren ll2 likelihood\n  *scatter likelihood theta \n\n\nNumber of observations (_N) was 0, now 25."
  },
  {
    "objectID": "rmethods/9_ldvm.html#testing",
    "href": "rmethods/9_ldvm.html#testing",
    "title": "Limited Dependent Variable Models",
    "section": "Testing?",
    "text": "Testing?\n\nYou can test two things:\n\nTest coefficients (\\(\\beta\\))\nTest marginal effects (\\(G'(x\\beta)\\beta\\))\n\nBoth test will most likely agree with each other, but some contradictions may arise. ### How?\nz-test and/or Wald test: Similar to t-test and Joint F-test we cover before. But, we now make the assumption of normality (not t-distribution)\nLog-Likelihood test. Similar to F-test for restricted and unrestricted model:\n\nEstimate both Restricted and unrestricted model. And obtain their Log Likelihoods (\\(\\mathcal{L}_ur\\)) and (\\(\\mathcal{L}_r\\)).\n\n\\[LR = 2 (\\mathcal{L}_ur-\\mathcal{L}_r) \\overset{a}\\sim \\chi^2_q\\]"
  },
  {
    "objectID": "rmethods/9_ldvm.html#stata---example",
    "href": "rmethods/9_ldvm.html#stata---example",
    "title": "Limited Dependent Variable Models",
    "section": "Stata - Example",
    "text": "Stata - Example\n\n\nCode\nfrause mroz, clear\n* LPM with Robust Standard errors\nqui:reg inlf nwifeinc educ exper expersq age kidslt6 kidsge6, robust\nest sto m1\nqui:logit inlf nwifeinc educ exper expersq age kidslt6 kidsge6, \nest sto m2a\nqui:margins, dydx(*) post\nest sto m2b\nprobit inlf nwifeinc educ exper expersq age kidslt6 kidsge6, \nest sto m3a\nqui:margins, dydx(*) post\nest sto m3b\n\n\n\nIteration 0:   log likelihood =  -514.8732  \nIteration 1:   log likelihood = -402.06651  \nIteration 2:   log likelihood = -401.30273  \nIteration 3:   log likelihood = -401.30219  \nIteration 4:   log likelihood = -401.30219  \n\nProbit regression                                       Number of obs =    753\n                                                        LR chi2(7)    = 227.14\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -401.30219                             Pseudo R2     = 0.2206\n\n------------------------------------------------------------------------------\n        inlf | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    nwifeinc |  -.0120237   .0048398    -2.48   0.013    -.0215096   -.0025378\n        educ |   .1309047   .0252542     5.18   0.000     .0814074     .180402\n       exper |   .1233476   .0187164     6.59   0.000     .0866641    .1600311\n     expersq |  -.0018871      .0006    -3.15   0.002     -.003063   -.0007111\n         age |  -.0528527   .0084772    -6.23   0.000    -.0694678   -.0362376\n     kidslt6 |  -.8683285   .1185223    -7.33   0.000    -1.100628    -.636029\n     kidsge6 |    .036005   .0434768     0.83   0.408     -.049208    .1212179\n       _cons |   .2700768    .508593     0.53   0.595    -.7267473    1.266901\n------------------------------------------------------------------------------\n\n\n\n\nCode\nset linesize 255\n*| classes: larger\ndisplay \"Prob Models\"\nesttab m1 m2a m2b m3a m3b, scalar(r2 ll) cell(b(fmt(%5.3f)) ///\nse(par([ ])) p( par(( )) ) )  gap  mtitle(LPM Logit Logit-mfx Probit Probit-mfx)\n\n\nProb Models\n\n-----------------------------------------------------------------------------\n                      (1)          (2)          (3)          (4)          (5)\n                      LPM        Logit    Logit-mfx       Probit   Probit-mfx\n                   b/se/p       b/se/p       b/se/p       b/se/p       b/se/p\n-----------------------------------------------------------------------------\nmain                                                                         \nnwifeinc           -0.003       -0.021       -0.004       -0.012       -0.004\n                  [0.002]      [0.008]      [0.001]      [0.005]      [0.001]\n                  (0.026)      (0.011)      (0.010)      (0.013)      (0.012)\n\neduc                0.038        0.221        0.039        0.131        0.039\n                  [0.007]      [0.043]      [0.007]      [0.025]      [0.007]\n                  (0.000)      (0.000)      (0.000)      (0.000)      (0.000)\n\nexper               0.039        0.206        0.037        0.123        0.037\n                  [0.006]      [0.032]      [0.005]      [0.019]      [0.005]\n                  (0.000)      (0.000)      (0.000)      (0.000)      (0.000)\n\nexpersq            -0.001       -0.003       -0.001       -0.002       -0.001\n                  [0.000]      [0.001]      [0.000]      [0.001]      [0.000]\n                  (0.002)      (0.002)      (0.001)      (0.002)      (0.001)\n\nage                -0.016       -0.088       -0.016       -0.053       -0.016\n                  [0.002]      [0.015]      [0.002]      [0.008]      [0.002]\n                  (0.000)      (0.000)      (0.000)      (0.000)      (0.000)\n\nkidslt6            -0.262       -1.443       -0.258       -0.868       -0.261\n                  [0.032]      [0.204]      [0.032]      [0.119]      [0.032]\n                  (0.000)      (0.000)      (0.000)      (0.000)      (0.000)\n\nkidsge6             0.013        0.060        0.011        0.036        0.011\n                  [0.014]      [0.075]      [0.013]      [0.043]      [0.013]\n                  (0.337)      (0.422)      (0.421)      (0.408)      (0.407)\n\n_cons               0.586        0.425                     0.270             \n                  [0.152]      [0.860]                   [0.509]             \n                  (0.000)      (0.621)                   (0.595)             \n-----------------------------------------------------------------------------\nN                     753          753          753          753          753\nr2                  0.264                                                    \nll                 -423.9       -401.8                    -401.3             \n-----------------------------------------------------------------------------\n\n\n\ndisplay \"LR test\"\nqui:probit inlf nwifeinc educ exper expersq age kidslt6 kidsge6 motheduc fatheduc, \nest sto unrestricted\nqui:probit inlf nwifeinc educ exper expersq age kidslt6 kidsge6 , \nest sto restricted\nlrtest unrestricted restricted\n\nLR test\n\nLikelihood-ratio test\nAssumption: restricted nested within unrestricted\n\n LR chi2(2) =   0.29\nProb &gt; chi2 = 0.8668"
  },
  {
    "objectID": "rmethods/9_ldvm.html#censored-and-truncated-data",
    "href": "rmethods/9_ldvm.html#censored-and-truncated-data",
    "title": "Limited Dependent Variable Models",
    "section": "Censored and Truncated Data",
    "text": "Censored and Truncated Data\n\nLogits and Probits, are not the only models that require MLE for estimation.\n\nAmong Discrete data models, you also have ologit/oprobit for ordered responses. mlogit/mprobit for unordered ones. Extends on logit/probit.\n\nThere are other interesting cases:\n\nWhen Data is censored.\nWhen Data is truncated."
  },
  {
    "objectID": "rmethods/9_ldvm.html#three-cases",
    "href": "rmethods/9_ldvm.html#three-cases",
    "title": "Limited Dependent Variable Models",
    "section": "Three Cases",
    "text": "Three Cases\n\nCase 1Case 2Case 3\n\n\n\n\\(y\\) is “conditionally-normal” and is Fully Observed.\nYou can estimate the model using OLS or ML\n\n\n\nCode\nqui:{\n  clear\n  set obs 999\n  gen p   = _n/(_N+1)\n  gen fob = invnormal(p)\n}\nqui:histogram fob\n\n\n\n\n\n\n\n\n\n\n\n\nData is observed for everyone, but is “censored” for some. tobit\n\nEither corner solution (how many hours you study) or Recoded: \\(y_{obs} = max(c,y^*)\\)\n\n\n\n\nCode\nqui: replace fob = -2 if fob&lt;-2\nqui:histogram fob, xlabel(-4 (2) 4)\n\n\n\n\n\n\n\n\n\n\n\n\nBelow (or above) some threshold, you do not have information on \\(y\\). truncreg \\[y_{obs} = y^* \\text{ if } y^*&gt;c\\]"
  },
  {
    "objectID": "rmethods/9_ldvm.html#estimation-censored-and-corner-solution",
    "href": "rmethods/9_ldvm.html#estimation-censored-and-corner-solution",
    "title": "Limited Dependent Variable Models",
    "section": "Estimation: Censored and Corner Solution",
    "text": "Estimation: Censored and Corner Solution\nIf data is censored or corner solution the estimation strategy is based on:\n\\[\\begin{aligned}\nL_i &= \\frac{1}{\\sigma} \\phi\\left( \\frac{y-x\\beta}{\\sigma} \\right) \\text{ if } y&gt;c \\\\\n    &= 1-\\Phi\\left(\\frac{x\\beta}{\\sigma} \\right) \\text{ if } y\\leq c \\\\\n\\end{aligned}\n\\]\nIf data is truncated, we need to “adjust” the distribution of what is observed\n\\[\\begin{aligned}\nL_i &= \\frac{1}{\\Phi\\left( x\\beta/\\sigma \\right)} \\frac{1}{\\sigma} \\phi\\left( \\frac{y-x\\beta}{\\sigma} \\right) \\text{ if } y&gt;c \\\\  \n\\end{aligned}\n\\]\nWe will put -truncated regression- on the side for now. But see here for an example."
  },
  {
    "objectID": "rmethods/9_ldvm.html#interpretation-it-depends",
    "href": "rmethods/9_ldvm.html#interpretation-it-depends",
    "title": "Limited Dependent Variable Models",
    "section": "Interpretation: It depends!",
    "text": "Interpretation: It depends!\n\nWhat are you interested in analyzing? and what type of data you have?\n\n\nLatent variable\\(P(y&gt;0|x)\\)\\(E(y|y&gt;0,x)\\)\\(E(y|x)\\)\n\n\n\nEasiest Case. Just need to consider the coefficients (as in LRM)\n\n\\[\n\\begin{aligned}\nE(y^*|x) &= x\\beta \\\\\n\\frac{\\partial E(y^*|x)}{\\partial x } &= \\beta_x\n\\end{aligned}\n\\]\n\nThe same applies if model was censored.\n\n\n\n\nIts an alternative approach to Probit models, where you are interest in analyzing why is data Not censored, or why is it above some threshold. (why people work)\nExtensive margin effect. \\[\n\\begin{aligned}\nP(y&gt;0|x) &= \\Phi\\left(\\frac{x\\beta}{\\sigma}\\right) \\\\\n\\frac{\\partial P(y&gt;0|x)}{\\partial x } &= \\frac{\\beta_x}{\\sigma} \\phi\\left(\\frac{x\\beta}{\\sigma}\\right)\n\\end{aligned}\n\\]\n\nNote: Coefficients \\(\\beta\\) need to be Standardized.\n\n\n\nIf corner solution, one may be interested in the effect of those with positive outcomes only.\nThis is the intensive margin effect. \\[\n\\begin{aligned}\nE(y|y&gt;0,x) &= x\\beta + \\sigma \\frac{\\phi(x\\beta / \\sigma )}{\\Phi(x\\beta / \\sigma )} \\\\\n\\frac{\\partial E(y|y&gt;0,x)}{\\partial x } &= \\beta_x\n\\left[ 1-\\frac{\\phi(x\\beta / \\sigma )}{\\Phi(x\\beta / \\sigma )} \\left( \\frac{x\\beta }{\\sigma }+ \\frac{\\phi(x\\beta / \\sigma )}{\\Phi(x\\beta / \\sigma )}\\right) \\right]\n\\end{aligned}\n\\]\n\n\n\n\nIn this case, one may be interested in estimating the expected effect on everyone.\nCombines both Intensive and extensive margin effects. Comparable to OLS.\n\n\\[\n\\begin{aligned}\nE(y|x) &= p(y&gt;0|x)*E(y|y&gt;0,x) + (1-p(y&gt;0|x))*0 \\\\\n\\frac{\\partial E(y|x)}{\\partial x } &= \\beta_x \\Phi(x\\beta)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/9_ldvm.html#example",
    "href": "rmethods/9_ldvm.html#example",
    "title": "Limited Dependent Variable Models",
    "section": "Example",
    "text": "Example\n\n\nCode\nfrause mroz, clear\nqui:tobit hours nwifeinc educ c.exper##c.exper   age kidslt6 kidsge6 , ll(0)\nqui:emargins, dydx(*) estore(m1)\nqui:emargins, dydx(*) predict(p(0,.)) estore(m2)\nqui:emargins, dydx(*) predict(e(0,.)) estore(m3)\nqui:emargins, dydx(*) predict(ystar(0,.)) estore(m4)\nesttab m1 m2 m3 m4, mtitle(Latent P(y&gt;0) E(y|y&gt;0) E(y) ) b(3) se\n\n\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                   Latent          P(y&gt;0)        E(y|y&gt;0)            E(y)   \n----------------------------------------------------------------------------\nnwifeinc           -8.814*         -0.002*         -3.969*         -5.189*  \n                  (4.459)         (0.001)         (2.008)         (2.621)   \n\neduc               80.645***        0.022***       36.312***       47.473***\n                 (21.583)         (0.006)         (9.703)        (12.621)   \n\nexper              91.929***        0.026***       37.593***       48.793***\n                  (7.997)         (0.002)         (2.966)         (3.587)   \n\nage               -54.405***       -0.015***      -24.497***      -32.026***\n                  (7.418)         (0.002)         (3.362)         (4.292)   \n\nkidslt6          -894.020***       -0.246***     -402.551***     -526.278***\n                (111.878)         (0.028)        (50.749)        (64.706)   \n\nkidsge6           -16.218          -0.004          -7.303          -9.547   \n                 (38.641)         (0.011)        (17.404)        (22.752)   \n----------------------------------------------------------------------------\nN                     753             753             753             753   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/9_ldvm.html#tobit-has-problems-too",
    "href": "rmethods/9_ldvm.html#tobit-has-problems-too",
    "title": "Limited Dependent Variable Models",
    "section": "Tobit has problems too",
    "text": "Tobit has problems too\n\nThat simple equation, too much aggregation\nHayek (in Fear the Boom and Bust)\n\n\nTobit, when addressing corner solutions, aims to explain two different actions (Engagement and intensity) with the same model. However, this may not be appropriate all the time.\n\nHW-Examples?\n\nWhen this happens, other models may be more appropritate like\n\ntwo part model: (literally model using two equations)\nHurdle Model (craggit or churdle)\n\nAlso…Normality…"
  },
  {
    "objectID": "rmethods/9_ldvm.html#censored-regression",
    "href": "rmethods/9_ldvm.html#censored-regression",
    "title": "Limited Dependent Variable Models",
    "section": "Censored Regression",
    "text": "Censored Regression\n\nApplies to the same cases as Tobit model. But, it usually refers to Censoring at other points of the distribution (upper censoring? mixed censoring?)\nFurthermore, applies to cases with different censoring thresholds!\n\nTypical Example, Unemployment duration\n\n\n\n\nCode\nqui:frause recid, clear\ngen lldur = ldurat             // Lower Limit\ngen uudur = ldurat if cens==0  // upper limit = . if censored.\nintreg lldur uudur workprg priors tserved felon alcohol drugs black married educ age\n\n\n(893 missing values generated)\n\nFitting constant-only model:\n\nIteration 0:   log likelihood = -2188.8689  \nIteration 1:   log likelihood = -1732.7406  \nIteration 2:   log likelihood = -1680.7927  \nIteration 3:   log likelihood =  -1680.427  \nIteration 4:   log likelihood =  -1680.427  \n\nFitting full model:\n\nIteration 0:   log likelihood = -2116.9831  \nIteration 1:   log likelihood = -1639.9495  \nIteration 2:   log likelihood =  -1597.634  \nIteration 3:   log likelihood = -1597.0592  \nIteration 4:   log likelihood =  -1597.059  \nIteration 5:   log likelihood =  -1597.059  \n\nInterval regression                                 Number of obs     =  1,445\n                                                           Uncensored =    552\n                                                        Left-censored =      0\n                                                       Right-censored =    893\n                                                       Interval-cens. =      0\n\n                                                    LR chi2(10)       = 166.74\nLog likelihood = -1597.059                          Prob &gt; chi2       = 0.0000\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     workprg |  -.0625715   .1200369    -0.52   0.602    -.2978396    .1726965\n      priors |  -.1372529   .0214587    -6.40   0.000    -.1793111   -.0951947\n     tserved |  -.0193305   .0029779    -6.49   0.000    -.0251672   -.0134939\n       felon |   .4439947   .1450865     3.06   0.002     .1596303     .728359\n     alcohol |  -.6349092   .1442166    -4.40   0.000    -.9175686   -.3522499\n       drugs |  -.2981602   .1327356    -2.25   0.025    -.5583171   -.0380033\n       black |  -.5427179   .1174428    -4.62   0.000    -.7729014   -.3125343\n     married |   .3406837   .1398431     2.44   0.015     .0665964    .6147711\n        educ |   .0229196   .0253974     0.90   0.367    -.0268584    .0726975\n         age |   .0039103   .0006062     6.45   0.000     .0027221    .0050984\n       _cons |   4.099386    .347535    11.80   0.000      3.41823    4.780542\n-------------+----------------------------------------------------------------\n    /lnsigma |   .5935864   .0344122    17.25   0.000     .5261398     .661033\n-------------+----------------------------------------------------------------\n       sigma |    1.81047   .0623022                      1.692387    1.936792\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/9_ldvm.html#truncated",
    "href": "rmethods/9_ldvm.html#truncated",
    "title": "Limited Dependent Variable Models",
    "section": "Truncated",
    "text": "Truncated\n\nIf Data is simply not there, as shown before, one needs to adjust Estimates.\nmarginal effects decisions are similar to Tobit\n\n\nfrause mroz, clear\nqui:truncreg hours nwifeinc educ c.exper##c.exper   age kidslt6 kidsge6 , ll(0)\nemargins, dydx(*) estore(m1b)\nemargins, dydx(*) predict(e(0,.)) estore(m2b)\nesttab m1 m1b m3 m2b, mtitle(Lat-Tobit Lat-Trunc E(y&gt;0)-Tobit E(y&gt;0)-Trunc ) b(3) se\n\n\nAverage marginal effects                                   Number of obs = 428\nModel VCE: OIM\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  nwifeinc educ exper age kidslt6 kidsge6\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    nwifeinc |   .1534399   5.164279     0.03   0.976    -9.968361    10.27524\n        educ |  -29.85254   22.83935    -1.31   0.191    -74.61684    14.91176\n       exper |   48.00824   8.578316     5.60   0.000     31.19504    64.82143\n         age |  -27.44381   8.293458    -3.31   0.001    -43.69869   -11.18893\n     kidslt6 |  -484.7109   153.7881    -3.15   0.002      -786.13   -183.2918\n     kidsge6 |  -102.6574   43.54347    -2.36   0.018    -188.0011   -17.31379\n------------------------------------------------------------------------------\n\nAverage marginal effects                                   Number of obs = 428\nModel VCE: OIM\n\nExpression: E(hours|hours&gt;0), predict(e(0,.))\ndy/dx wrt:  nwifeinc educ exper age kidslt6 kidsge6\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    nwifeinc |   .1094149   3.682546     0.03   0.976    -7.108243    7.327072\n        educ |  -21.28723   16.25065    -1.31   0.190    -53.13793    10.56346\n       exper |   32.66986   5.277772     6.19   0.000     22.32562    43.01411\n         age |  -19.56962   5.823226    -3.36   0.001    -30.98293   -8.156303\n     kidslt6 |  -345.6374   107.9599    -3.20   0.001    -557.2349   -134.0399\n     kidsge6 |  -73.20291   30.80594    -2.38   0.017    -133.5814   -12.82438\n------------------------------------------------------------------------------\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                Lat-Tobit       Lat-Trunc    E(y&gt;0)-Tobit    E(y&gt;0)-Trunc   \n----------------------------------------------------------------------------\nnwifeinc           -8.814*          0.153          -3.969*          0.109   \n                  (4.459)         (5.164)         (2.008)         (3.683)   \n\neduc               80.645***      -29.853          36.312***      -21.287   \n                 (21.583)        (22.839)         (9.703)        (16.251)   \n\nexper              91.929***       48.008***       37.593***       32.670***\n                  (7.997)         (8.578)         (2.966)         (5.278)   \n\nage               -54.405***      -27.444***      -24.497***      -19.570***\n                  (7.418)         (8.293)         (3.362)         (5.823)   \n\nkidslt6          -894.020***     -484.711**      -402.551***     -345.637** \n                (111.878)       (153.788)        (50.749)       (107.960)   \n\nkidsge6           -16.218        -102.657*         -7.303         -73.203*  \n                 (38.641)        (43.543)        (17.404)        (30.806)   \n----------------------------------------------------------------------------\nN                     753             428             753             428   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/9_ldvm.html#poisson",
    "href": "rmethods/9_ldvm.html#poisson",
    "title": "Limited Dependent Variable Models",
    "section": "Poisson",
    "text": "Poisson\n\nSome times, Data may be non-negative, and/or countable. OLS works well, but we could do better\nWith Count data, some data transformations (logs) are not possible, because of the zeroes.\nSo instead of assuming \\(y|x \\sim N(\\mu_x,\\sigma)\\), one could assume \\(y|x \\sim poisson(\\mu_x)\\)\n\n\\[P(y=k,\\mu_x) = \\frac{\\mu_x^k e ^{-\\mu_x}}{k!} \\text{ with } \\mu_x=\\exp(x\\beta)\\]\n\nFor a Poisson:\n\n\\(E(y|x) = \\exp{x\\beta}\\) and \\(Var(y|x) = \\exp{x\\beta}\\)\n\nAs hinted before, Count data is heteroskedastic. And Poisson assumes some structure to that."
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-6",
    "href": "rmethods/9_ldvm.html#section-6",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "Also convinient that Poisson models are very easy to interpret! (just like Log-lin models)\nAfter estimation:\n\n\\[\\frac{\\Delta \\% E(y|x)}{\\Delta x} \\simeq \\beta_x \\times 100 \\text{ or } (\\exp \\beta_x-1)\\times 100 \\]\n\nOther points.\n\nThe variance imposed in Poisson is very restrictive. This is a problem for Variance estimation!\nSolution: use Robust Standard Errors!\nLike LRM, poisson is robust to errors when modeling the conditional mean.\nPoisson is a very good alternative for continuous data too (if using Robust SE)\n\nWage models, trade models"
  },
  {
    "objectID": "rmethods/9_ldvm.html#example-1",
    "href": "rmethods/9_ldvm.html#example-1",
    "title": "Limited Dependent Variable Models",
    "section": "Example",
    "text": "Example\n\nfrause crime1, clear\nqui: reg narr86 pcnv avgsen tottime ptime86 qemp86 inc86 black hispan born60\nest sto m1\nqui:poisson narr86 pcnv avgsen tottime ptime86 qemp86 inc86 black hispan born60, robust\nest sto m2\nqui:emargins, dydx(*) estore(m3)\nesttab m1 m2 m3, se b(3) mtitle(LRM Poisson Poisson-mfx) ///\nkeep(pcnv ptime86  qemp86 inc86 black hispan) label varwidth(20) wrap\n\n\n--------------------------------------------------------------------\n                              (1)             (2)             (3)   \n                              LRM         Poisson     Poisson-mfx   \n--------------------------------------------------------------------\nmain                                                                \nproportion of prior        -0.132**        -0.402***       -0.162***\nconvictions               (0.040)         (0.101)         (0.040)   \n\nmos. in prison             -0.041***       -0.099***       -0.040***\nduring 1986               (0.009)         (0.022)         (0.009)   \n\n# quarters employed,       -0.051***       -0.038          -0.015   \n1986                      (0.014)         (0.034)         (0.014)   \n\nlegal income, 1986,        -0.001***       -0.008***       -0.003***\n$100s                     (0.000)         (0.001)         (0.001)   \n\n=1 if black                 0.327***        0.661***        0.267***\n                          (0.045)         (0.099)         (0.042)   \n\n=1 if Hispanic              0.194***        0.500***        0.202***\n                          (0.040)         (0.092)         (0.038)   \n--------------------------------------------------------------------\nObservations                 2725            2725            2725   \n--------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/9_ldvm.html#other-methods-of-interest",
    "href": "rmethods/9_ldvm.html#other-methods-of-interest",
    "title": "Limited Dependent Variable Models",
    "section": "Other Methods of interest",
    "text": "Other Methods of interest\n\nMLE opens the door to other methods that may be more approriate to analyze data\nThey may even be able to handle otherwise unsolvable data problems.\n\nologit, oprobit: Ordered qualitative variables\nmlogit, mprobit: Unordered Qualitative variables\nheckman: Endogenous Sample Selection\nfractional regression model: When the depvariable is an index\netc etc\n\nWorth knowing, but not for the exam!"
  },
  {
    "objectID": "rmethods/homework_2.html",
    "href": "rmethods/homework_2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Similar to HW1, propose a concise research project. Specify the dependent variable of interest, along with control variables.\nThe model specification must include both continuous and discrete variables in your model.\nDescribe your economic model and its corresponding econometric model, accompanied by a succinct description of your anticipated findings."
  },
  {
    "objectID": "rmethods/homework_2.html#note",
    "href": "rmethods/homework_2.html#note",
    "title": "Homework 2",
    "section": "Note",
    "text": "Note\nExplore all datasets available in frause and utilize the data from these datasets to determine the variables for analysis and the controls to incorporate into your homework.\nThe examples within the textbook can serve as valuable guidelines for your considerations here.\nYou have the freedom to explore other sources. If you do so, please include the data alongside your homework submission.\nExceptionally unique responses (distinguished by their thoroughness, detail, innovation, and presentation) may merit extra points."
  },
  {
    "objectID": "rmethods/index.html",
    "href": "rmethods/index.html",
    "title": "Research Methods: Econometrics I",
    "section": "",
    "text": "Class syllabus is here."
  },
  {
    "objectID": "rmethods/index.html#syllabus",
    "href": "rmethods/index.html#syllabus",
    "title": "Research Methods: Econometrics I",
    "section": "",
    "text": "Class syllabus is here."
  },
  {
    "objectID": "rmethods/index.html#zoom-link",
    "href": "rmethods/index.html#zoom-link",
    "title": "Research Methods: Econometrics I",
    "section": "Zoom Link",
    "text": "Zoom Link\nWhen there is need to attend a class online, and you have requested to attend online, use the following link:\nZoom class"
  },
  {
    "objectID": "rmethods/index.html#introduction",
    "href": "rmethods/index.html#introduction",
    "title": "Research Methods: Econometrics I",
    "section": "Introduction",
    "text": "Introduction\n\nChapter 1\nOnce Upon a time, why Econometrics?:"
  },
  {
    "objectID": "rmethods/index.html#part-i-basic-tools",
    "href": "rmethods/index.html#part-i-basic-tools",
    "title": "Research Methods: Econometrics I",
    "section": "Part I: Basic Tools",
    "text": "Part I: Basic Tools\n\n1. Chapter 2:\nThe Simple Regression Model\n\n\n2. Chapter 3:\nMultiple Regression Analysis: Estimation\n\n\n3. Chapter 4 - 5\nMRA: Inference and Asymptotics\n\n\nHomeWork 1 Github"
  },
  {
    "objectID": "rmethods/index.html#part-ii-addressing-problems-with-mra",
    "href": "rmethods/index.html#part-ii-addressing-problems-with-mra",
    "title": "Research Methods: Econometrics I",
    "section": "Part II: Addressing Problems with MRA",
    "text": "Part II: Addressing Problems with MRA\n\n4. Chapter 6 - 7\nMRA: Scaling, functional forms, Goodness of Fit, and Qualitative Information\n\n\n5. Chapter 8\nHeteroskedasticity\n\n\n6. Chapter 9\nFurther Problems: Functional form, Missing variables, measurement errors, missing data\n\n\nHomeWork 2 Github\n\n\nMIDTERM!\n\n7. Chapter 15\nInstrumental Variables and 2SLS\n\n\n8. Chapter 17\nLimited Dep Variables"
  },
  {
    "objectID": "rmethods/index.html#part-iii-panel-data-methods",
    "href": "rmethods/index.html#part-iii-panel-data-methods",
    "title": "Research Methods: Econometrics I",
    "section": "Part III: Panel Data Methods",
    "text": "Part III: Panel Data Methods\n\n9. Chapter 13\nPool Cross Section and Panel Data\n\n\n10. Chapter 14\nAdvanced Panel Data Methods\n\n\nHomeWork 3"
  },
  {
    "objectID": "rmethods/index.html#part-iv-time-series",
    "href": "rmethods/index.html#part-iv-time-series",
    "title": "Research Methods: Econometrics I",
    "section": "Part IV: Time Series",
    "text": "Part IV: Time Series\n\n11. Chapter 10 - 11\nBasics of Regression analysis with TSD\n\n\n12. Chapter 12 & 18\nAdvanced TSD Problems\n\n\nFinal TBD"
  },
  {
    "objectID": "rmethods/Syllabus.html",
    "href": "rmethods/Syllabus.html",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "",
    "text": "Instructor: Fernando Rios-Avila\nOffice: Room 307, Blithewood\nOffice Hours: Friday, 9:00 am to 10:00 am or by appointment\nPhone: 845-758-7719\nEmail: friosavi@levy.org\nTime and Location: Wednesday, 9:30 am to 12:50 pm"
  },
  {
    "objectID": "rmethods/Syllabus.html#course-description",
    "href": "rmethods/Syllabus.html#course-description",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Course Description",
    "text": "Course Description\nThe course aims to provide students with a foundation in applied econometrics that is required for the successful completion of the program. The emphasis of the course is on understanding the intuition behind model estimation, hypothesis testing, and economic interpretation of statistical results.\nWe begin by discussing the nature of econometrics and economic data. This is followed by a discussion of estimation and inference in univariate and multivariate regression models of cross-sectional data. We will review some of the consequences of heteroscedasticity, measurement errors, and endogeneity, among other issues of model specification and data measures. The second part of the course will cover advanced regression models such as limited dependent variables, panel data, and time series data.\nThe class is taught through a combination of lectures, discussion, homework, quizzes, and exams. Student involvement and participation in class are highly encouraged."
  },
  {
    "objectID": "rmethods/Syllabus.html#required-text",
    "href": "rmethods/Syllabus.html#required-text",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Required Text",
    "text": "Required Text\nIntroductory Econometrics: A Modern Approach\nby Jeffrey M. Wooldridge\n\nWhile any edition will do, exercises and exam material will be taken from the 7th edition.\n\nSuggested:\nUsing R for Introductory Econometrics (recommended)\nby Florian Heiss\n\nThis book introduces the free programming language and software package R with a focus on the implementation of standard tools and methods used in econometrics. It builds on the textbook “Introductory Econometrics: A Modern Approach” by Jeffrey M. Wooldridge. The book can be accessed online here.\nThe author also provides textbooks using the same structure, introducing Julia and python."
  },
  {
    "objectID": "rmethods/Syllabus.html#course-website",
    "href": "rmethods/Syllabus.html#course-website",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Course website",
    "text": "Course website\nWe will be using Github pages to provide all lectures and homework. It is your responsibility to access the site or communicate with me if any questions should arise.\nThis is where assignments, readings, and other information will be posted."
  },
  {
    "objectID": "rmethods/Syllabus.html#grading",
    "href": "rmethods/Syllabus.html#grading",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Grading",
    "text": "Grading\nGrading will be based on homework assignments, quizzes, and exams. Their grade distribution is as follows:\n\nHomework (55%): Three homework assignments will be given throughout the semester. Homework assignments are prepared for you to implement the methodologies covered in class, as well as encourage you to interpret the results. Each homework will consist of a small research project where you will be asked to answer a series of questions, as if you were writing a research paper.\nThe homework assignments will require using data and the statistical software Stata. You are free to use any of the data sets that come along with the textbook. They can be accessed using frause in Stata. It is encouraged that homework assignments are prepared and submitted in pairs.\nAll homework assignments will be posted on the course website. When submitting your homework, prepare a pdf, html, or doc file with your answers, and a do file with the code used to answer the questions. It is highly encouraged that you use markdown or quarto to prepare your homework, as it easily allows you to incorporate all necessary information to reproduce your results.\nMidterm and Final (40%): Two exams will be given. Each one is prepared to test you on concepts, interpretation, and intuition behind the econometric topics reviewed in class. The exams will be open book and open notes. You are allowed to use any printed material, including the textbook, your notes, and any other material you may find useful. You are not allowed to use any electronic devices, including computers, tablets, or phones, except for the use of a calculator.\nQuestions for the midterm and final will include three sections:\n\nMultiple-choice questions and concept questions.\nAnalytical section, equation solving, and derivations.\nEmpirical section, where you will be asked to interpret results from a regression analysis, as well as implement statistical tests.\n\nAnalytical and empirical sections will be taken from the problem sets and computational exercises in the textbook.\nQuizzes (5%): After each topic, there will be multiple-choice quizzes to test your knowledge of important concepts and ideas seen in class. There will also be open-ended questions or extra projects that will be provided during the semester. This includes 5 extra credit points.\nClass Participation: Class participation is highly encouraged. You are to participate in class discussions. You are also encouraged to ask questions and provide answers to questions asked in class. This counts for up to 5% of extra credit for your final grade."
  },
  {
    "objectID": "rmethods/Syllabus.html#attendance",
    "href": "rmethods/Syllabus.html#attendance",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Attendance:",
    "text": "Attendance:\nClass attendance, in-person or online, is highly recommended. Classes will not be recorded, but for exceptional cases, a link will be provided to attend the class online. Material for exams and homework will come from both class lectures as well as the book.\nThe only acceptable excuses for missing a test are medical reasons or family emergencies. If you have a legitimate excuse, a make-up exam will be issued soon after the date of the original exam. Any issues should be discussed with me before the actual exam takes place."
  },
  {
    "objectID": "rmethods/Syllabus.html#course-software",
    "href": "rmethods/Syllabus.html#course-software",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Course Software",
    "text": "Course Software\nThere are several statistical packages for analyzing data. In this course, we will be using the software Stata to cover all materials in class. Slides are self-replicable, thus you can copy and paste almost all code provided to replicate the results seen in class. The Institute will be providing you with licenses for Stata/BE for the length of the course.\nStata offers many free short webinars and video tutorials that may be useful if you never used Stata before, or even if you have some experience with it. Please see the resources page for more information.\nIf you decide to, you can use R, Julia, or Python to study and work on the course materials and homework. One of the recommended books has nice introductions and code that can help you get started with these software packages. The resources page has additional information on how to get started with these software packages.\nAs with many other skills, the best way to learn is to simply work with the packages, work on the book exercises, and ask any questions to me or your classmates when you find a problem you could not find a solution for."
  },
  {
    "objectID": "rmethods/Syllabus.html#additional-information",
    "href": "rmethods/Syllabus.html#additional-information",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Additional Information:",
    "text": "Additional Information:\nAll students are responsible for knowing Bard’s Policy on Academic Honesty as published in Bard College Student Handbook."
  },
  {
    "objectID": "rm_class.html",
    "href": "rm_class.html",
    "title": "Research Methods: Econometrics I",
    "section": "",
    "text": "Class syllabus is here."
  },
  {
    "objectID": "rm_class.html#syllabus",
    "href": "rm_class.html#syllabus",
    "title": "Research Methods: Econometrics I",
    "section": "",
    "text": "Class syllabus is here."
  },
  {
    "objectID": "rm_class.html#zoom-link",
    "href": "rm_class.html#zoom-link",
    "title": "Research Methods: Econometrics I",
    "section": "Zoom Link",
    "text": "Zoom Link\nWhen there is need to attend a class online, and you have requested to attend online, use the following link:\nZoom class"
  },
  {
    "objectID": "rm_class.html#introduction",
    "href": "rm_class.html#introduction",
    "title": "Research Methods: Econometrics I",
    "section": "Introduction",
    "text": "Introduction\n\nChapter 1\nOnce Upon a time, why Econometrics?:"
  },
  {
    "objectID": "rm_class.html#part-i-basic-tools",
    "href": "rm_class.html#part-i-basic-tools",
    "title": "Research Methods: Econometrics I",
    "section": "Part I: Basic Tools",
    "text": "Part I: Basic Tools\n\n1. Chapter 2:\nThe Simple Regression Model\n\n\n2. Chapter 3:\nMultiple Regression Analysis: Estimation\n\n\n3. Chapter 4 - 5\nMRA: Inference and Asymptotics\n\n\nHomeWork 1 Github"
  },
  {
    "objectID": "rm_class.html#part-ii-addressing-problems-with-mra",
    "href": "rm_class.html#part-ii-addressing-problems-with-mra",
    "title": "Research Methods: Econometrics I",
    "section": "Part II: Addressing Problems with MRA",
    "text": "Part II: Addressing Problems with MRA\n\n4. Chapter 6 - 7\nMRA: Scaling, functional forms, Goodness of Fit, and Qualitative Information\n\n\n5. Chapter 8\nHeteroskedasticity\n\n\n6. Chapter 9\nFurther Problems: Functional form, Missing variables, measurement errors, missing data\n\n\nHomeWork 2 Github\n\n\nMIDTERM!\n\n7. Chapter 15\nInstrumental Variables and 2SLS\n\n\n8. Chapter 17\nLimited Dep Variables"
  },
  {
    "objectID": "rm_class.html#part-iii-panel-data-methods",
    "href": "rm_class.html#part-iii-panel-data-methods",
    "title": "Research Methods: Econometrics I",
    "section": "Part III: Panel Data Methods",
    "text": "Part III: Panel Data Methods\n\n9. Chapter 13\nPool Cross Section and Panel Data\n\n\n10. Chapter 14\nAdvanced Panel Data Methods\n\n\nHomeWork 3"
  },
  {
    "objectID": "rm_class.html#part-iv-time-series",
    "href": "rm_class.html#part-iv-time-series",
    "title": "Research Methods: Econometrics I",
    "section": "Part IV: Time Series",
    "text": "Part IV: Time Series\n\n11. Chapter 10 - 11\nBasics of Regression analysis with TSD\n\n\n12. Chapter 12 & 18\nAdvanced TSD Problems\n\n\nFinal TBD"
  }
]