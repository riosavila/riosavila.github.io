[
  {
    "objectID": "rm-data/slides/week11.html#motivation",
    "href": "rm-data/slides/week11.html#motivation",
    "title": "Model Building for Prediction",
    "section": "Motivation",
    "text": "Motivation\n\nYou want to predict apartment rental prices using location, size, amenities, and other features. But with so many variables available,\n\nhow should you specify the candidate models?\nWhich variables should they include, in what functional forms, and with what interactions?\nHow can you make sure the candidates include truly effective predictive models?\n\nYou want to predict hourly sales for a new shop, based on data from a similar existing shop.\n\nHow should you define your y variable, and how should you select predictor variables for regression models to find the best fit?\nFinally, how can you evaluate the prediction in a way that informs decision-makers about the uncertainty of your prediction?"
  },
  {
    "objectID": "rm-data/slides/week11.html#what-is-old-whats-the-new",
    "href": "rm-data/slides/week11.html#what-is-old-whats-the-new",
    "title": "Model Building for Prediction",
    "section": "What is old? whats the new?",
    "text": "What is old? whats the new?\n\nWe have learned about the basics of prediction.\n\nWe need to worry about the out-of-sample prediction error. Not the in-sample error.\n\nWe know we can use cross-validation to select the best model.\nAnd we know we can construct the best model by hand, because we have domain knowledge.\n\nBut what if we have a lot of variables? What if we have a lot of data?"
  },
  {
    "objectID": "rm-data/slides/week11.html#as-k-grows-large-kxk-grows-larger",
    "href": "rm-data/slides/week11.html#as-k-grows-large-kxk-grows-larger",
    "title": "Model Building for Prediction",
    "section": "As \\(K\\) grows large \\(KxK\\) grows larger",
    "text": "As \\(K\\) grows large \\(KxK\\) grows larger\n\n1 variable, 1 model\n2 variables, 3 models\n3 variables, 7 models\n10 variables, 1023 models!\n20 variables, 1,048,575 models!"
  },
  {
    "objectID": "rm-data/slides/week11.html#prediction-process-key-steps",
    "href": "rm-data/slides/week11.html#prediction-process-key-steps",
    "title": "Model Building for Prediction",
    "section": "Prediction process: key steps",
    "text": "Prediction process: key steps\n\nStart with defining your “question” - what you want to predict.\nBased on the question, the target variable is defined (operationalize in the data).\n\nFollowed by defining the sample to be used (as the target)\nDetermine the target variable and functional form (label engineering)\n\nDefine the list of predictors (feature engineering)\n\nBuild the model(s)\nevaluate the model\nmake the prediction"
  },
  {
    "objectID": "rm-data/slides/week11.html#sample-design",
    "href": "rm-data/slides/week11.html#sample-design",
    "title": "Model Building for Prediction",
    "section": "Sample design",
    "text": "Sample design\n\nIn a prediction exercise, we are interested in predicting target for units that look those in the live data.\n\nSelect a sample that is representative of the live/target data.\n\nBut, there is a trade-off:\n\nKeep observations close to what we’ll have in live data,\nOr aim for a large sample size.\n\nSample design is eventually a compromise"
  },
  {
    "objectID": "rm-data/slides/week11.html#sample-design-filtering",
    "href": "rm-data/slides/week11.html#sample-design-filtering",
    "title": "Model Building for Prediction",
    "section": "Sample design: filtering",
    "text": "Sample design: filtering\n\nBefore settling on a model, we need to design the sample.\nFiltering our data to match the business/ policy question.\nIt may involve dropping observations based on key predictor values.\n\nWe would be looking for 3-4 star hotels, not all of them."
  },
  {
    "objectID": "rm-data/slides/week11.html#sample-design-spotting-errors",
    "href": "rm-data/slides/week11.html#sample-design-spotting-errors",
    "title": "Model Building for Prediction",
    "section": "Sample design: Spotting errors",
    "text": "Sample design: Spotting errors\n\nFor prediction exercise, we should spend more time on finding and deleting errors.\n\nWe have no chance predicting extreme values, and certainly not errors.\nThey provide no information, and they may distort the model.\n\nKeeping an extreme value that is likely to be an error, will have a high cost - the quadratic errors in the loss function will tilt the curve and our prediction will be off for most observations.\n\nOLS is very sensitive to outliers.\n\nStronger focus on dropping observations we think are errors.\nIf data is missing, You may either drop the observation or try to understand why it is missing, and use that in the prediction."
  },
  {
    "objectID": "rm-data/slides/week11.html#case-study-of-used-cars-sample-design",
    "href": "rm-data/slides/week11.html#case-study-of-used-cars-sample-design",
    "title": "Model Building for Prediction",
    "section": "Case study of used cars: Sample design",
    "text": "Case study of used cars: Sample design\n\nDropping hybrid cars, manual gear, truck\nDrop cars without a clean title (i.e., cars that had to be removed from registration due to a major accident)\nDrop when suspect cars with clearly erroneous data on miles run,\nDrop cars in a fair (=bad) condition, cars that are new\nData cleaning resulted in 281 observations ( I kept Fair and new)"
  },
  {
    "objectID": "rm-data/slides/week11.html#label-engineering---defining-target",
    "href": "rm-data/slides/week11.html#label-engineering---defining-target",
    "title": "Model Building for Prediction",
    "section": "Label engineering - defining target",
    "text": "Label engineering - defining target\n\nWe need to define what will our target variable be.\nIn some cases, this requires no action, the business question may define it:\n\nthe price of the hotel is one such case.\n\nOften it requires thinking and decision-making about definition.\n\nHow to define default, injury, purchase\n\nBinary vs continuous.\nLog vs level"
  },
  {
    "objectID": "rm-data/slides/week11.html#label-engineering---log-vs-level",
    "href": "rm-data/slides/week11.html#label-engineering---log-vs-level",
    "title": "Model Building for Prediction",
    "section": "Label engineering - log vs level",
    "text": "Label engineering - log vs level\n\nWhen price is the target variable, its relation to predictor variables is often closer to linear when expressed in log price.\nLog differences approximate relative, or percentage, differences, and relative price differences are often more stable.\nThe related technical advantage is that the distribution of log prices is often close to normal, which makes linear regressions give better approximation to average differences.\n\nAlso, Log() is not the only transformation that can be used.\n\nChoosing the right functional form is important but not always easy.\n\nEconometrics vs prediction mindset"
  },
  {
    "objectID": "rm-data/slides/week11.html#label-engineering---log-vs-level-1",
    "href": "rm-data/slides/week11.html#label-engineering---log-vs-level-1",
    "title": "Model Building for Prediction",
    "section": "Label engineering - log vs level",
    "text": "Label engineering - log vs level\n\nWhen the target variable is expressed in log terms, we want to predict the value of the target variable (\\(\\hat y\\)) not its \\(log(y)\\).\nSimply doing this \\(e^{\\log y}\\) is not the same as obtaining \\(\\hat y\\).\n\nTechnical details:\n\\(\\hat{y}_j = e^{ \\widehat{\\log y}_j + \\hat e_j}\\)\n\nBut, because \\(\\hat e_j\\) is not observed, we need to approximate it via “Some” method.\n\nIf we assume that the error term is normally distributed: \\[\\hat{y}_j = e^{\\widehat{\\ln y}_j} e^{\\hat{\\sigma}^2/2}\\]"
  },
  {
    "objectID": "rm-data/slides/week11.html#used-cars-case-study-label-engineering---log",
    "href": "rm-data/slides/week11.html#used-cars-case-study-label-engineering---log",
    "title": "Model Building for Prediction",
    "section": "Used cars case study: Label engineering - log?",
    "text": "Used cars case study: Label engineering - log?\n\nBusiness case is about price itself, continuous\nBut model can have level or log price as target\nLook at some patterns\nCompare model performance\nLog vs level model - some coefficients easier interpreted\nWhen we have two cars of same age and type; the one with 10% more miles in the odometer is predicted to be sold for 0.5% less.\nSE version is 1300 dollar more costly."
  },
  {
    "objectID": "rm-data/slides/week11.html#used-cars-case-study-label-engineering---log-1",
    "href": "rm-data/slides/week11.html#used-cars-case-study-label-engineering---log-1",
    "title": "Model Building for Prediction",
    "section": "Used cars case study: Label engineering - log?",
    "text": "Used cars case study: Label engineering - log?\n\n\n\nModel\nPoint prediction\n80% PI: upper bound\n80% PI: lower bound\n\n\n\n\nin logs\n8.63\n8.18\n9.08\n\n\nRecalculated to level\n5,932\n3,783\n9,301\n\n\nIn levels\n6,073\n4,317\n7,829\n\n\n\nAsymetric for Log-model, Symetric for Linear Model\nPick what works better"
  },
  {
    "objectID": "rm-data/slides/week11.html#feature-engineering",
    "href": "rm-data/slides/week11.html#feature-engineering",
    "title": "Model Building for Prediction",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nRequires the most effort\nFeature engineering - defining the list and functional form of variables we will consider as predictor.\nImportantly, we use both domain knowledge - information about the actual market, product or the society - and statistics to make decisions."
  },
  {
    "objectID": "rm-data/slides/week11.html#feature-engineering---checklist",
    "href": "rm-data/slides/week11.html#feature-engineering---checklist",
    "title": "Model Building for Prediction",
    "section": "Feature engineering - checklist",
    "text": "Feature engineering - checklist\n\nWhat to do with missing values\nDealing with ordered categorical values - continuous or set of binaries\nHow to use text to create variables (Identify key words)\nSelecting functional form\nThinking interactions"
  },
  {
    "objectID": "rm-data/slides/week11.html#what-to-do-with-missing-values",
    "href": "rm-data/slides/week11.html#what-to-do-with-missing-values",
    "title": "Model Building for Prediction",
    "section": "What to do with missing values",
    "text": "What to do with missing values\n\nMissing at random: Observations with missing variables are not systematically different from rest, may replace with sample mean and add binary flag.\nMissing systematically, by nonrandom selection: Must analyze reasons, may simply mean =0, look at the source of the data / questionnaire.\nIf very few missing and it is random, do not do anything.\nFew cases, you may want to impute or look for proxies."
  },
  {
    "objectID": "rm-data/slides/week11.html#what-to-do-with-different-type-of-variables",
    "href": "rm-data/slides/week11.html#what-to-do-with-different-type-of-variables",
    "title": "Model Building for Prediction",
    "section": "What to do with different type of variables",
    "text": "What to do with different type of variables\n\nBinary (e.g, yes/no; male/female; 1/2) – create a 0/1 binary variable\nString / factor – check values, and create a set of binaries.\nContinuous – nothing to do. Make sure it is stored as number. Perhaps Winsorize.\nText – Natural Language Processing. Mining the text to get useful info.\n\nCounting words, looking for key words, etc."
  },
  {
    "objectID": "rm-data/slides/week11.html#case-study-predicting-airbnb-apartment-prices",
    "href": "rm-data/slides/week11.html#case-study-predicting-airbnb-apartment-prices",
    "title": "Model Building for Prediction",
    "section": "Case study: Predicting Airbnb Apartment Prices",
    "text": "Case study: Predicting Airbnb Apartment Prices\n\nLondon,UK\nhttp://insideairbnb.com\n50K observations\n94 variables, including many binaries for location and amenities\nKey variables: size, type, location, amenities\nQuantitative target: - price (in USD)\nIn reality: GBP"
  },
  {
    "objectID": "rm-data/slides/week11.html#case-study-predicting-airbnb-apartment-prices-1",
    "href": "rm-data/slides/week11.html#case-study-predicting-airbnb-apartment-prices-1",
    "title": "Model Building for Prediction",
    "section": "Case study: Predicting Airbnb Apartment Prices",
    "text": "Case study: Predicting Airbnb Apartment Prices\n\nKey issue is to look at variables and think functional form\nGuests to accommodate goes up to 16, but most apartments accommodate 1 through 7. Keep as is. Add variables for type. No need for complicated models\nRegarding other predictors, we have several binary variables, which we kept as they were: type of bed, type of property (apartment, house, room), cancellation policy.\nLook at possible need for interactions by domain knowledge / visualization"
  },
  {
    "objectID": "rm-data/slides/week11.html#graphical-way-of-finding-relationships",
    "href": "rm-data/slides/week11.html#graphical-way-of-finding-relationships",
    "title": "Model Building for Prediction",
    "section": "Graphical way of finding relationships",
    "text": "Graphical way of finding relationships"
  },
  {
    "objectID": "rm-data/slides/week11.html#graphical-way-of-finding-interactions",
    "href": "rm-data/slides/week11.html#graphical-way-of-finding-interactions",
    "title": "Model Building for Prediction",
    "section": "Graphical way of finding interactions",
    "text": "Graphical way of finding interactions"
  },
  {
    "objectID": "rm-data/slides/week11.html#model-building",
    "href": "rm-data/slides/week11.html#model-building",
    "title": "Model Building for Prediction",
    "section": "Model building",
    "text": "Model building\nTwo methods to build models:\n\nby hand - mix domain knowledge and statistics\nby smart algorithms = machine learning"
  },
  {
    "objectID": "rm-data/slides/week11.html#model-building-and-selection-build-model-by-hand",
    "href": "rm-data/slides/week11.html#model-building-and-selection-build-model-by-hand",
    "title": "Model Building for Prediction",
    "section": "Model building and selection: Build model by hand",
    "text": "Model building and selection: Build model by hand\n\nUse domain knowledge drives picking key variables\nDrop garbage - drop variables those that are useless. May be because of poor coverage or quality, or they may be irrelevant.\nLook at a pairwise correlations. Multi-collinearity is an issue for smaller datasets\nPrefer variables that are easier to update - cheaper operation of a prediction model used in production\nMatters when you have relatively many variables compared to size of observations"
  },
  {
    "objectID": "rm-data/slides/week11.html#selecting-variables-in-regressions-by-lasso",
    "href": "rm-data/slides/week11.html#selecting-variables-in-regressions-by-lasso",
    "title": "Model Building for Prediction",
    "section": "Selecting Variables in Regressions by LASSO",
    "text": "Selecting Variables in Regressions by LASSO\n\nKey question: which features to enter into model, how to select?\n\nBy hand – domain knowledge. Advantage: interpretation, external validity\nDisadvantage: with many features it’s very hard. Esp. with many possible interactions!\n\nThere is room for an automatic selection process.\nSome are computationally very intensive (compare every option?)\nAdvantage: no need to use outside info\nDisadvantage: may be sensitive to overfitting, hard to interpret"
  },
  {
    "objectID": "rm-data/slides/week11.html#lasso-idea",
    "href": "rm-data/slides/week11.html#lasso-idea",
    "title": "Model Building for Prediction",
    "section": "LASSO idea",
    "text": "LASSO idea\n\nLASSO (the acronym of Least Absolute Shrinkage and Selection Operator) is a method to select variables to include in a linear regression to produce good predictions and avoid overfitting.\nLASSO is a shrinkage method: it shrinks coefficients towards zero to reduce variance\n\nCost is in bias - LASSO is not unbiased\nUnlike OLS\n\nLASSO is a feature selection method as well"
  },
  {
    "objectID": "rm-data/slides/week11.html#lasso-process",
    "href": "rm-data/slides/week11.html#lasso-process",
    "title": "Model Building for Prediction",
    "section": "LASSO process",
    "text": "LASSO process\n\nIt starts with a large set of potential predictor variables that, typically, include many interactions, polynomials for nonlinear patterns, etc.\nLASSO modifies the way regression coefficients are estimated by adding a penalty term for too many coefficients.\nThe way its penalty works makes LASSO assign zero coefficients to variables whose inclusion does not improve the fit of the regression much.\nAssigning zero coefficients to some variables means not including them in the regression."
  },
  {
    "objectID": "rm-data/slides/week11.html#side-note-lasso-vs-bic",
    "href": "rm-data/slides/week11.html#side-note-lasso-vs-bic",
    "title": "Model Building for Prediction",
    "section": "Side note: LASSO vs BIC",
    "text": "Side note: LASSO vs BIC\n\nThe purpose is similar to the adjusted in-sample measures of fit, such as the BIC.\nTo find a regression that balances fitting the data and the number of variables.\nBut its result is different:\n\ninstead of producing a better measure of fit to help find the best one\nit alters coefficients to produce a better regression directly."
  },
  {
    "objectID": "rm-data/slides/week11.html#lasso",
    "href": "rm-data/slides/week11.html#lasso",
    "title": "Model Building for Prediction",
    "section": "LASSO",
    "text": "LASSO\nConsider the linear regression with i=1…n observations and k variables, denoted 1…k:\n\\[y^E = \\beta_0 + \\sum_{j=1}^k \\beta_jx_j\\]\nCoefficients are estimated by OLS: which minimizes the sum of squared residuals:\n\\[\\min_\\beta \\sum_{i=1}^N (y_i - (\\beta_0 + \\sum_{j=1}^k \\beta_jx_{ij}))^2\\]\nLASSO modifies this minimization by a penalty term:\n\\[\\min_\\beta \\sum_{i=1}^N (y_i - (\\beta_0 + \\sum_{j=1}^k \\beta_jx_{ij}))^2 + \\lambda \\sum_{j=1}^k |\\beta_j|\\]"
  },
  {
    "objectID": "rm-data/slides/week11.html#lasso-how-it-works",
    "href": "rm-data/slides/week11.html#lasso-how-it-works",
    "title": "Model Building for Prediction",
    "section": "LASSO: how it works",
    "text": "LASSO: how it works\n\n\\(\\lambda\\) — tuning parameter.\nweight for penalty term vs OLS fit –&gt; Strength of the variable selection\nMain effect of this constraint is to force many coefficients to zero.\nBest way to keep the sum of the absolute value of the coefficients low while maximizing fit –&gt; zero coefficients on variables whose inclusion improves fit only a little.\nThis adjustment gets rid of the weakest predictors."
  },
  {
    "objectID": "rm-data/slides/week11.html#lasso-how-it-works-1",
    "href": "rm-data/slides/week11.html#lasso-how-it-works-1",
    "title": "Model Building for Prediction",
    "section": "LASSO: how it works",
    "text": "LASSO: how it works\n\nThe value of the tuning parameter \\(\\lambda\\) drives the strength of this selection.\nLarger \\(\\lambda\\) values lead to more aggressive selection and thus fewer variables left in the regression.\nBut how can one specify a \\(\\lambda\\) value that leads to the best prediction?\nWe don’t need, the algorithm does\nThe LASSO algorithm can numerically solve for coefficients and the \\(\\lambda\\) parameter at once.\nThis makes it fast.\nUnlike OLS, we have no closed form solutions."
  },
  {
    "objectID": "rm-data/slides/week11.html#other-shrinkage-methods",
    "href": "rm-data/slides/week11.html#other-shrinkage-methods",
    "title": "Model Building for Prediction",
    "section": "Other shrinkage methods",
    "text": "Other shrinkage methods\n\nSo LASSO is a shrinkage method: it shrinks coefficients towards zero to reduce variance\nThere are other ways, other functional forms\nRidge regression has a quadratic penalty:\n\n\\[\\min_\\beta \\sum_{i=1}^N (y_i - (\\beta_0 + \\sum_{j=1}^k \\beta_jx_{ij}))^2 + \\lambda \\sum_{j=1}^k \\beta_j^2\\]\n\nNo coefficient is shrunk to zero. But close…"
  },
  {
    "objectID": "rm-data/slides/week11.html#lasso-and-ridge",
    "href": "rm-data/slides/week11.html#lasso-and-ridge",
    "title": "Model Building for Prediction",
    "section": "Lasso and Ridge",
    "text": "Lasso and Ridge\n\nLasso, Ridge regressions called regularization\nLASSO is “L1”, Ridge is “L2”\nBoth may help reduce overfitting\nLASSO also acts as feature selection model\nElastic net helps find a parameter between \\(|\\beta_j|\\) and \\(\\beta_j^2\\) via cross validation"
  },
  {
    "objectID": "rm-data/slides/week11.html#airbnb-pricing-model-building",
    "href": "rm-data/slides/week11.html#airbnb-pricing-model-building",
    "title": "Model Building for Prediction",
    "section": "Airbnb Pricing Model building",
    "text": "Airbnb Pricing Model building\n\nProcess: build many models that differ in terms of features:\n\nWhich predictors are included\nFunctional form of predictors\n\nHere: specified eight linear regression models for predicting price.\nData has 4393 observations. This is our original data.\n80% is our work set (3515 observations), the rest we will use for diagnostics."
  },
  {
    "objectID": "rm-data/slides/week11.html#versions-of-the-airbnb-apartment-price-prediction-models",
    "href": "rm-data/slides/week11.html#versions-of-the-airbnb-apartment-price-prediction-models",
    "title": "Model Building for Prediction",
    "section": "Versions of the Airbnb apartment price prediction models",
    "text": "Versions of the Airbnb apartment price prediction models\n\n\n\n\n\n\n\n\n\nMod\nPredictor variables\nN var\nN coeff\n\n\n\n\nM1\nguests accommodated, linearly\n1\n2\n\n\nM2\n= M1 + N beds, N days review, type: property, room, bed type\n6\n8\n\n\nM3\n= M2 + bathroom, cancellation, review score, N reviews (3 cat)+ F(miss)\n11\n15\n\n\nM4\n= M3 + N guest squared, square+cubic for days since 1st review\n11\n17\n\n\nM5\n= M4 + room type + N reviews interacted with property type\n11\n22\n\n\nM6\n=M5 + air conditioning, pets allowed - interacted with property type\n13\n28\n\n\nM7\n=M6 + all other amenities\n70\n72\n\n\nM8\n=M7 + all other amenities interacted with property type + bed type\n70\n293"
  },
  {
    "objectID": "rm-data/slides/week11.html#comparing-model-fit-measures",
    "href": "rm-data/slides/week11.html#comparing-model-fit-measures",
    "title": "Model Building for Prediction",
    "section": "Comparing model fit measures",
    "text": "Comparing model fit measures\n\n\n\n\n\n\n\n\n\n\n\nModel\nN predictors\nR-squared\nBIC\nTraining RMSE\nTest RMSE\n\n\n\n\n(1)\n1\n0.40\n36042\n40.48\n40.16\n\n\n(2)\n7\n0.48\n35598\n37.73\n37.38\n\n\n(3)\n14\n0.51\n35478\n36.78\n36.51\n\n\n(4)\n16\n0.57\n24076\n31.95\n32.24\n\n\n(5)\n21\n0.57\n24096\n31.82\n32.18\n\n\n(6)\n27\n0.58\n24113\n31.60\n32.19\n\n\n(7)\n71\n0.61\n24281\n30.42\n31.77\n\n\n(8)\n293\n0.66\n25675\n28.04\n51.41"
  },
  {
    "objectID": "rm-data/slides/week11.html#training-and-test-set-rmse-for-eight-models",
    "href": "rm-data/slides/week11.html#training-and-test-set-rmse-for-eight-models",
    "title": "Model Building for Prediction",
    "section": "Training and test set RMSE for eight models",
    "text": "Training and test set RMSE for eight models\n\nTraining RMSE falls with complexity\nTest RMSE falls then rises\nWe pick Model M7 based on lowest CV RMSE."
  },
  {
    "objectID": "rm-data/slides/week11.html#the-lasso-model",
    "href": "rm-data/slides/week11.html#the-lasso-model",
    "title": "Model Building for Prediction",
    "section": "The LASSO model",
    "text": "The LASSO model\n\nStart with M8 and appr 300 candidate variables in the regression.\nWe ran the LASSO algorithm with 5-fold cross-validation for selecting the optimal value for λ.\nLASSO regression just marginally better but: LASSO is automatic, a great advantage.\nHere: domain knowledge helped create M7. In other cases, LASSO could be great."
  },
  {
    "objectID": "rm-data/slides/week11.html#evaluating-the-prediction-using-a-holdout-set",
    "href": "rm-data/slides/week11.html#evaluating-the-prediction-using-a-holdout-set",
    "title": "Model Building for Prediction",
    "section": "Evaluating the Prediction Using a Holdout Set",
    "text": "Evaluating the Prediction Using a Holdout Set\n\nModel selection: selecting the best model using cross-validation\nOnce we have picked the best model, we advised going back and using the entire original data for the final estimate and to make a prediction.\nWhat part of the data should we use to evaluate that final prediction?\nThe solution is a random split before we do the analysis.\nWork set: We do all of the work using one part of the data: model building, selecting the best model and then making the prediction itself.\nHoldout set: another part of the data for evaluating the prediction itself. Don’t touch till the end."
  },
  {
    "objectID": "rm-data/slides/week11.html#the-holdout-set",
    "href": "rm-data/slides/week11.html#the-holdout-set",
    "title": "Model Building for Prediction",
    "section": "The holdout set",
    "text": "The holdout set\n\nTo do diagnostics and give a good estimate of how the model may work in the live data\n\nAdditional twist to the process\nThe holdout set.\n\nHoldout set is set is not used in any way for modelling – taken out in the beginning\n\nThis avoids cross-contamination\n\nUsed to give best guess for performance in live data\nUsed to do diagnostics of our model"
  },
  {
    "objectID": "rm-data/slides/week11.html#post-prediction-diagnostics",
    "href": "rm-data/slides/week11.html#post-prediction-diagnostics",
    "title": "Model Building for Prediction",
    "section": "Post-prediction diagnostics",
    "text": "Post-prediction diagnostics\n\nPost-prediction diagnostics - understand better how our model works\nWe look at prediction interval to learn about what precision we may expect to see of the estimates.\nWe look at how the model work for different classes of observations\nsuch as young and old cars."
  },
  {
    "objectID": "rm-data/slides/week11.html#cross-validation-and-holdout-set-procedure",
    "href": "rm-data/slides/week11.html#cross-validation-and-holdout-set-procedure",
    "title": "Model Building for Prediction",
    "section": "Cross-validation and holdout set procedure",
    "text": "Cross-validation and holdout set procedure\n\nStarting with the original data, split it into a larger work set and a smaller holdout set.\nFurther split the work set into training sets and test sets for k-fold cross-validation.\nBuild models and select the best model using that training-test split.\nRe-estimate the best model using all observations in the work set.\nTake the estimated best model and apply it to the holdout set.\nEvaluate the prediction using the holdout set."
  },
  {
    "objectID": "rm-data/slides/week11.html#illustration-of-the-uses-of-the-original-data-and-the-live-data",
    "href": "rm-data/slides/week11.html#illustration-of-the-uses-of-the-original-data-and-the-live-data",
    "title": "Model Building for Prediction",
    "section": "Illustration of the uses of the original data and the live data",
    "text": "Illustration of the uses of the original data and the live data"
  },
  {
    "objectID": "rm-data/slides/week11.html#post-prediction-diagnostics-1",
    "href": "rm-data/slides/week11.html#post-prediction-diagnostics-1",
    "title": "Model Building for Prediction",
    "section": "Post-prediction diagnostics",
    "text": "Post-prediction diagnostics\n\nPost-prediction diagnostics - understand better how our model works\nWe look at prediction interval to learn about what precision we may expect to see of the estimates.\nWe look at how the model work for different classes of observations\nsuch as young and old cars."
  },
  {
    "objectID": "rm-data/slides/week11.html#data-work-and-holdout",
    "href": "rm-data/slides/week11.html#data-work-and-holdout",
    "title": "Model Building for Prediction",
    "section": "Data work and holdout",
    "text": "Data work and holdout\n\nData has 4393 observations. This is our original data.\nrandom 20% holdout set with 878 observations.\nThe remaining 80% is our work set (3515 observations).\nWork set will be used for cross-validation with several folds of training and test sets."
  },
  {
    "objectID": "rm-data/slides/week11.html#diagnostics",
    "href": "rm-data/slides/week11.html#diagnostics",
    "title": "Model Building for Prediction",
    "section": "Diagnostics",
    "text": "Diagnostics\n\nChose the OLS estimated M7.\nWhat can we say about model performance?\nAfter estimating the model on all observations in the work sample, we calculated its RMSE in the holdout sample. The RMSE for M7 is 41\nHigher than CV RMSE, could be other way around.\nLook at diagnostics on the holdout set."
  },
  {
    "objectID": "rm-data/slides/week11.html#diagnostics-prices",
    "href": "rm-data/slides/week11.html#diagnostics-prices",
    "title": "Model Building for Prediction",
    "section": "Diagnostics: prices",
    "text": "Diagnostics: prices\n\n\n\n\n\ny-y-hat plot\nhigher values not really caught."
  },
  {
    "objectID": "rm-data/slides/week11.html#diagnostics-variation-by-size",
    "href": "rm-data/slides/week11.html#diagnostics-variation-by-size",
    "title": "Model Building for Prediction",
    "section": "Diagnostics: variation by size",
    "text": "Diagnostics: variation by size\n\n\n\n\n\nThe model generates a very wide 80% PI for average apartment\nbar plot with PI bands\nwide intervals\nlinear and thus, hurts small numbers more"
  },
  {
    "objectID": "rm-data/slides/week11.html#prediction-with-big-data",
    "href": "rm-data/slides/week11.html#prediction-with-big-data",
    "title": "Model Building for Prediction",
    "section": "Prediction with Big Data",
    "text": "Prediction with Big Data\n\nThe principles of prediction are the same with Big Data as with moderate-sized data\nBig Data leads to smaller estimation error.\nThis reduction makes the total prediction error smaller\nThe magnitude of irreducible error, and problems with external validity, remain the same with Big Data"
  },
  {
    "objectID": "rm-data/slides/week11.html#prediction-with-big-data-1",
    "href": "rm-data/slides/week11.html#prediction-with-big-data-1",
    "title": "Model Building for Prediction",
    "section": "Prediction with Big Data",
    "text": "Prediction with Big Data\n\nAnother upside is that large number of rows sometimes comes with large number of variables\nRoom for more complex models\nConsideration: computing power (But there is AWS and Cloud Computing)\nWhen N is too large, we can take a random sample and select the best model with the help of usual cross-validation using that random sample"
  },
  {
    "objectID": "rm-data/slides/week11.html#summary",
    "href": "rm-data/slides/week11.html#summary",
    "title": "Model Building for Prediction",
    "section": "Summary",
    "text": "Summary\n\nOur aim was to build a prediction model for pricing apartments\nWe built a model, M7, with domain knowledge, and a horse race between models of various complexity\nPicked the winner by cross-validated RMSE\nThe model is useful for predication, but there is a great deal of uncertainty as suggested by diagnostics (on the holdout set)"
  },
  {
    "objectID": "rm-data/slides/week11.html#think-external-validity",
    "href": "rm-data/slides/week11.html#think-external-validity",
    "title": "Model Building for Prediction",
    "section": "Think external validity",
    "text": "Think external validity\n\nFuture dataset will look different\nThink about how much\nReally matters in prediction\nIf uncertain, pick simpler model"
  },
  {
    "objectID": "rm-data/slides/week11.html#main-takeaways",
    "href": "rm-data/slides/week11.html#main-takeaways",
    "title": "Model Building for Prediction",
    "section": "Main takeaways",
    "text": "Main takeaways\n\nWe can never evaluate all possible models to find the best one\nModel building is important to specify models that are likely among the best\nLASSO is an algorithm that can help in model building, by selecting the x variables and their functional forms\nExploratory data analysis and domain knowledge remain important alongside powerful algorithms, for assessing and improving the external validity of predictions"
  },
  {
    "objectID": "rm-data/slides/week11.html#stata-lasso",
    "href": "rm-data/slides/week11.html#stata-lasso",
    "title": "Model Building for Prediction",
    "section": "Stata: LASSO",
    "text": "Stata: LASSO\n\nLasso is one of the few machine learning algorithms that is available in Stata.\n\nStata also has a feature for elastic net and Ridge.\nfor now just focus on LASSO. help lasso\n\nThe syntax\n\n lasso model depvar [(alwaysvars)] othervars [if] [in] [weight] [, options]\n\nIt has various selection options (selection()) but we can use the default."
  },
  {
    "objectID": "rm-data/slides/week11.html#stata-lasso-1",
    "href": "rm-data/slides/week11.html#stata-lasso-1",
    "title": "Model Building for Prediction",
    "section": "Stata: LASSO",
    "text": "Stata: LASSO\n\nwebuse cattaneo2, clear\nlasso linear bweight c.mage##c.mage c.fage##c.fage c.mage#c.fage c.fedu##c.medu ///\n    i.(mmarried mhisp fhisp foreign alcohol msmoke fbaby prenatal1), nolog\nereturn display\n\n(Excerpt from Cattaneo (2010) Journal of Econometrics 155: 138–154)\n\nLasso linear model                          No. of obs        =      4,642\n                                            No. of covariates =         26\nSelection: Cross-validation                 No. of CV folds   =         10\n\n--------------------------------------------------------------------------\n         |                                No. of      Out-of-      CV mean\n         |                               nonzero       sample   prediction\n      ID |     Description      lambda     coef.    R-squared        error\n---------+----------------------------------------------------------------\n       1 |    first lambda    107.1305         0       0.0001     334929.8\n      37 |   lambda before    3.761556        10       0.0561     316156.2\n    * 38 | selected lambda     3.42739        11       0.0561     316154.9\n      39 |    lambda after     3.12291        11       0.0561     316156.8\n      65 |     last lambda    .2780062        19       0.0550     316532.1\n--------------------------------------------------------------------------\n* lambda selected by cross-validation.\n------------------------------------------------------------------------------\n     bweight | Coefficient\n-------------+----------------------------------------------------------------\n        mage |   .0026703\n             |\n      c.fedu#|\n      c.medu |   .3136748\n             |\n    mmarried |\nNot married  |  -140.4633\n     0.mhisp |  -34.00255\n   0.foreign |   63.71057\n   0.alcohol |   40.29426\n             |\n      msmoke |\n    0 daily  |   167.9734\n 6–10 daily  |  -36.54529\n  11+ daily  |   -78.1332\n             |\n       fbaby |\n         No  |   51.02337\n             |\n   prenatal1 |\n         No  |  -42.31267\n       _cons |   3137.903\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rm-data/slides/week11.html#stata-lasso-2",
    "href": "rm-data/slides/week11.html#stata-lasso-2",
    "title": "Model Building for Prediction",
    "section": "Stata: LASSO",
    "text": "Stata: LASSO\n\nqui: ssc install elasticregress\nwebuse cattaneo2, clear\nlassoregress bweight c.mage##c.mage c.fage##c.fage c.mage#c.fage c.fedu##c.medu ///\n    i.(mmarried mhisp fhisp foreign alcohol msmoke fbaby prenatal1), \nereturn display\n\n(Excerpt from Cattaneo (2010) Journal of Econometrics 155: 138–154)\n\nLASSO regression                       Number of observations     =      4,642\n                                       R-squared                  =     0.0597\n                                       alpha                      =     1.0000\n                                       lambda                     =     2.7733\n                                       Cross-validation MSE       =  3.165e+05\n                                       Number of folds            =         10\n                                       Number of lambda tested    =        100\n------------------------------------------------------------------------------\n     bweight | Coefficient\n-------------+----------------------------------------------------------------\n        mage |          0\n             |\n      c.mage#|\n      c.mage |          0\n             |\n        fage |          0\n             |\n      c.fage#|\n      c.fage |          0\n             |\n      c.mage#|\n      c.fage |          0\n             |\n        fedu |          0\n        medu |   .5584781\n             |\n      c.fedu#|\n      c.medu |   .3520153\n             |\n    mmarried |\nNot married  |          0  (empty)\n    Married  |   151.3538\n             |\n       mhisp |\n          0  |          0  (empty)\n          1  |   38.93988\n             |\n       fhisp |\n          0  |          0  (empty)\n          1  |          0\n             |\n     foreign |\n          0  |          0  (empty)\n          1  |  -72.23538\n             |\n     alcohol |\n          0  |          0  (empty)\n          1  |   -49.0195\n             |\n      msmoke |\n    0 daily  |          0  (empty)\n  1–5 daily  |  -144.4353\n 6–10 daily  |  -206.2044\n  11+ daily  |  -247.3819\n             |\n       fbaby |\n         No  |          0  (empty)\n        Yes  |  -50.38466\n             |\n   prenatal1 |\n         No  |          0  (empty)\n        Yes  |          0\n             |\n       _cons |     3256.7\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\n     bweight | Coefficient\n-------------+----------------------------------------------------------------\n        mage |          0\n             |\n      c.mage#|\n      c.mage |          0\n             |\n        fage |          0\n             |\n      c.fage#|\n      c.fage |          0\n             |\n      c.mage#|\n      c.fage |          0\n             |\n        fedu |          0\n        medu |   .5584781\n             |\n      c.fedu#|\n      c.medu |   .3520153\n             |\n    mmarried |\nNot married  |          0  (empty)\n    Married  |   151.3538\n             |\n       mhisp |\n          0  |          0  (empty)\n          1  |   38.93988\n             |\n       fhisp |\n          0  |          0  (empty)\n          1  |          0\n             |\n     foreign |\n          0  |          0  (empty)\n          1  |  -72.23538\n             |\n     alcohol |\n          0  |          0  (empty)\n          1  |   -49.0195\n             |\n      msmoke |\n    0 daily  |          0  (empty)\n  1–5 daily  |  -144.4353\n 6–10 daily  |  -206.2044\n  11+ daily  |  -247.3819\n             |\n       fbaby |\n         No  |          0  (empty)\n        Yes  |  -50.38466\n             |\n   prenatal1 |\n         No  |          0  (empty)\n        Yes  |          0\n             |\n       _cons |     3256.7\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rm-data/syllabus.html",
    "href": "rm-data/syllabus.html",
    "title": "Research Methods I",
    "section": "",
    "text": "Instructor: Fernando Rios-Avila\nEmail: friosavi@levy.org\nOffice Hours: Wednesdays 1:30pm to 4:00pm. Or by appointment. Other times can be arranged, but will be done remotely.\nCourse Website:\nClass Time: Wednesday, 9:30 am - 12:45 am"
  },
  {
    "objectID": "rm-data/syllabus.html#introduction-to-modern-research-tools",
    "href": "rm-data/syllabus.html#introduction-to-modern-research-tools",
    "title": "Research Methods I",
    "section": "1: Introduction to Modern Research Tools",
    "text": "1: Introduction to Modern Research Tools\n\nCourse overview and expectations.\nIntroduction to GitHub/Github-Desktop for version control and collaboration.\nGetting started with Quarto for reproducible research: RStudio and VSCode.\nOther Tools: Overleaf, Zotero\nData organization and management\nLab: Setting up GitHub, Quarto, VSCode, Zotero, and Overleaf"
  },
  {
    "objectID": "rm-data/syllabus.html#introduction-to-data-analysis",
    "href": "rm-data/syllabus.html#introduction-to-data-analysis",
    "title": "Research Methods I",
    "section": "2. Introduction to Data Analysis",
    "text": "2. Introduction to Data Analysis\n\nIntroduction to data analysis: The Process\nWhat is Data? What types of Data are there?\nData collection methods\nPreparing data for analysis\nTidy data principles\nData cleaning: Missing values, outliers, and errors\nReadings: Chapter 1 and 2"
  },
  {
    "objectID": "rm-data/syllabus.html#data-exploration",
    "href": "rm-data/syllabus.html#data-exploration",
    "title": "Research Methods I",
    "section": "3: Data Exploration",
    "text": "3: Data Exploration\n\nType of data vs type of analysis\nFrequencies, distributions, and summary statistics\nExploratory data analysis techniques and visualizations\nTheoretical distributions\nComparisons, correlations and conditional distributions\nLatent and observed variables\nReadings: Chapter 3 and 4"
  },
  {
    "objectID": "rm-data/syllabus.html#generalization-from-sample-to-population",
    "href": "rm-data/syllabus.html#generalization-from-sample-to-population",
    "title": "Research Methods I",
    "section": "4: Generalization: From Sample to Population",
    "text": "4: Generalization: From Sample to Population\n\nSampling and generalization\nRepetition and sampling variability\nConfidence intervals and standard errors: The Bootstrap method\nExternal validity\nHypothesis testing principles\nType I and Type II errors\nMultiple Hypothesis Testing and p-hacking\nReadings: Chapters 5 and 6"
  },
  {
    "objectID": "rm-data/syllabus.html#and-6-regression-analysis-i-simple-regression",
    "href": "rm-data/syllabus.html#and-6-regression-analysis-i-simple-regression",
    "title": "Research Methods I",
    "section": "5 and 6: Regression Analysis I: Simple Regression",
    "text": "5 and 6: Regression Analysis I: Simple Regression\n\nLinear and non-linear relationships\nLinear Regression: Estimation and interpretation\nCorrelations and coefficients: Searching for causality\nProperties and assumptions of the linear regression model\nTransformations and Semiparametric models\nExtreme values, Influential observations and measurement error\nGeneralizing Results: SE and CI\nTesting Hypotheses\nReadings: Chapters 7, 8, and 9"
  },
  {
    "objectID": "rm-data/syllabus.html#regression-analysis-ii-multiple-regression",
    "href": "rm-data/syllabus.html#regression-analysis-ii-multiple-regression",
    "title": "Research Methods I",
    "section": "7: Regression Analysis II: Multiple Regression",
    "text": "7: Regression Analysis II: Multiple Regression\n\nMultiple regression basics: Estimation and Inference\nProblems with multiple regression\nNon-linearities, interactions and qualitative variables\nReadings: Chapters 10"
  },
  {
    "objectID": "rm-data/syllabus.html#regression-analysis-iii-modeling-probabilities",
    "href": "rm-data/syllabus.html#regression-analysis-iii-modeling-probabilities",
    "title": "Research Methods I",
    "section": "8: Regression Analysis III: Modeling Probabilities",
    "text": "8: Regression Analysis III: Modeling Probabilities\n\nLinear Probability Model\nNon-linear models: Logit and Probit\nInterpretation and marginal effects\nGoodness of Fit and Predictive Power\nReadings: Chapter 11"
  },
  {
    "objectID": "rm-data/syllabus.html#time-series-analysis",
    "href": "rm-data/syllabus.html#time-series-analysis",
    "title": "Research Methods I",
    "section": "9: Time Series Analysis",
    "text": "9: Time Series Analysis\n\nIntroduction to time series data\nTrend and seasonality\nStationarity and autocorrelation\nSerial correlation\nReadings: Chapter 12"
  },
  {
    "objectID": "rm-data/syllabus.html#prediction",
    "href": "rm-data/syllabus.html#prediction",
    "title": "Research Methods I",
    "section": "10: Prediction",
    "text": "10: Prediction\n\nIntroduction to Prediction\nR2 vs AR2 and other measures of fit\nOverfitting and Cross-validation: Finding the right model\nExternal validity and generalization\nReadings: Chapter 13"
  },
  {
    "objectID": "rm-data/syllabus.html#model-building-for-prediction-lasso",
    "href": "rm-data/syllabus.html#model-building-for-prediction-lasso",
    "title": "Research Methods I",
    "section": "11: Model Building for Prediction: LASSO",
    "text": "11: Model Building for Prediction: LASSO\n\nThe Process of Prediction\nHow to choose \\(g(y)\\)\nWorking with \\(X's\\)\nIntroduction to LASSO: Prediction and Diagnosis\nReadings: Chapter 14"
  },
  {
    "objectID": "rm-data/syllabus.html#predicting-probabilities-and-classification",
    "href": "rm-data/syllabus.html#predicting-probabilities-and-classification",
    "title": "Research Methods I",
    "section": "12: Predicting Probabilities and Classification",
    "text": "12: Predicting Probabilities and Classification\n\nPredicting Probabilities and Classification\nClassification, confusion matrices, and ROC curves\nFinding the right threshold\nReadings: Chapter 17"
  },
  {
    "objectID": "rm-data/syllabus.html#forecasting-data",
    "href": "rm-data/syllabus.html#forecasting-data",
    "title": "Research Methods I",
    "section": "13: Forecasting Data",
    "text": "13: Forecasting Data\n\nIntroduction to Forecasting: Predicting the future\nTraining and Testing Data\nTrends, Seasonality, and Cycles\nForecasting with ARIMA\nVAR and External validity\nReadings: Chapter 18"
  },
  {
    "objectID": "rm-data/syllabus.html#term-paper-60-of-final-grade",
    "href": "rm-data/syllabus.html#term-paper-60-of-final-grade",
    "title": "Research Methods I",
    "section": "Term Paper (60% of final grade)",
    "text": "Term Paper (60% of final grade)\nThroughout the semester, students will work on a multi-part research project that applies the techniques learned in class to real-world data. This project will require students to propose a research question, collect and clean data, conduct exploratory and regression analyses, and present their findings.\n\nPart I: Research Proposal (5%, due Week 2)\n\nIntroduction (including research question and motivation)\nProposed data sources: Consider data from the textbook, Kaggle, or other sources\nExpected findings and relevance\nConclusion\nReferences (if any)\nSpecify which software (if other than Stata) you plan to use for your analysis\nCreate a GitHub repository for your project and submit the link with your proposal\n\n\n\nPart II: Data Collection and Cleaning (10%, due Week 4)\nWrite a report that includes the following:\n\nDescription of data sources, collection methods, and potential biases\nData overview and preprocessing steps\nDiscussion of issues encountered and solutions\nPreliminary analysis with descriptive statistics and visualizations\nInclude a data dictionary in your GitHub repository\n\n\n\nPeer Review Report (due Week 5)\n\nReview another student’s work and provide feedback on their data collection and cleaning process. Provide suggestions for improvement and identify any potential issues.\n\n\n\nInterim Progress Report (5%, due Week 7)\n\nBrief update on progress, challenges faced, and next steps\nAdditional visualizations or analyses\nIt should include a draft of literature review and methodology\n\nFor the literature review include summaries for 3-5 papers related to your research question.\nIt should also include a draft of the methodology section, including the model specification.\n\nAddress any feedback received from the peer review in Part II\n\n\n\nPart III: Data Analysis (15%, due Week 10)\nPresent a draft for data analysis that includes the following:\n\nModel specification discussion\nRegression results presentation\nInterpretation of results\nDiscuss Limitations and robustness checks\n\n\n\nPeer Review Report (due week 11)\n\nReview another student’s work and provide feedback on their data analysis. Provide suggestions for improvement and identify any potential issues.\n\n\n\nPart IV: Final Report (20%, due Week 13)\nComplete research paper should include:\n\nIntroduction\nLiterature Review\nData and Methodology\nRobustness Checks or Sensitivity/Sub-group Analysis\nConclusion\nReferences\nAppendices (if any)\n\n\n\nPresentaton (5%, Week 14)\n\n15-minute presentation of your research to the class\nSee here for an example for the kind of report expected at each stage, based on the first research proposal on the impact of remote work on urban housing prices.\n\n\n\nAdditional Requirements\n\nAll work should be submitted in Quarto format using GitHub.\nUnless Data used is confidential or too large, you should include all data in your GitHub repository.\nYour GitHub repository should include with all code, data (if possible), and the Quarto document for your report.\nIt should also include all papers used for the literature review.\nAt each stage, you should submit an email with the PDF and QMD files to the instructor, at or before the deadline. You should also push your work to GitHub to follow the progress of your project."
  },
  {
    "objectID": "rm-data/syllabus.html#resources",
    "href": "rm-data/syllabus.html#resources",
    "title": "Research Methods I",
    "section": "Resources",
    "text": "Resources\n\nTextbook: Békés, G., & Kézdi, G. (2021). Data Analysis for Business, Economics, and Policy. Cambridge University Press. There are additional resources available on the book’s website: Data Analysis\nSoftware: There are several statistical packages for analyzing data. In this course, we will be using the software Stata to cover all materials in class. Slides are self-replicable, thus you can copy and paste almost all code provided to replicate the results seen in class. The Institute will be providing you with licenses for Stata/BE for the length of the course.\n\nStata offers many free short webinars and video tutorials that may be useful if you never used Stata before, or even if you have some experience with it. Please see the resources page for more information.\nIf you decide to, you can also use R, Julia, or Python to study and work on the course materials and homework. The book for the class has a repository with all the code in Stata, R and Python. It could be of great advantage to you to learn other languages, as they are widely used in the industry and academia.\nAs with many other skills, the best way to learn is to simply work with the software, work on the book exercises, and ask any questions to me or your classmates when you find a problem you could not find a solution for.\nFor the additional software, please look into Quarto, GitHub, Zotero and VSCode."
  },
  {
    "objectID": "rm-data/slides/week09.html#motivation",
    "href": "rm-data/slides/week09.html#motivation",
    "title": "Time series data",
    "section": "Motivation",
    "text": "Motivation\n\nYou are considering investing in a company stock, and you want to know how risky that investment is. You have downloaded data on daily stock prices for many years. How should you define returns? How should you assess whether and to what extent returns on the company stock move together with market returns?\nHeating and cooling are important uses of electricity. How does weather conditions affect electricity consumption? We are going to use monthly data on temperature and residential electricity consumption in Arizona. How to formulate a model which captures these factors?"
  },
  {
    "objectID": "rm-data/slides/week09.html#what-is-special-in-the-analysis-of-time-series-data",
    "href": "rm-data/slides/week09.html#what-is-special-in-the-analysis-of-time-series-data",
    "title": "Time series data",
    "section": "What is special in the analysis of time series data?",
    "text": "What is special in the analysis of time series data?\n\nDifferent\n\nWe have time series data if we observe one unit across many time periods.\nThere is a special notation: \\(y_t\\), \\(t = 1, 2, \\ldots, T\\)\nTime series data presents additional opportunities as well as additional challenges to compare variables.\n\n\n\nWith other features\n\nData wrangling novelties: frequency and aggregation\nSpecial nature of time series: serial correlation\nCoefficient interpretation."
  },
  {
    "objectID": "rm-data/slides/week09.html#data-preparation",
    "href": "rm-data/slides/week09.html#data-preparation",
    "title": "Time series data",
    "section": "Data preparation",
    "text": "Data preparation\n\nWhat is Frequency? of time series? = time elapsed between two observations of a variable\nPractical problems with frequency:\n\nNot all data is captured at the same frequency\nThere may be regular/irregular gaps between them: e.g. weekends for stock-exchange\nTwo variables have different frequencies\nExtreme values (spikes) in your variable\n\nLast but not least: in Stata you need to Declare your data as time series data with tsset command"
  },
  {
    "objectID": "rm-data/slides/week09.html#consequences",
    "href": "rm-data/slides/week09.html#consequences",
    "title": "Time series data",
    "section": "Consequences",
    "text": "Consequences\n\nRegressions: to condition \\(y_t\\) on values of \\(x_t\\) the two variables need to be on the same frequency. Otherwise, we need to adjust one of them.\nHow to adjust? Aggregation!\n\nFlow variables: sum up the values within the interval. e.g. daily sales \\(\\rightarrow\\) weakly sales is the sum of daily sales.\nStock variables: take the end-period value. e.g. daily stock prices uses the closing price on a given day\nOther kinds of variables: usually take the average value"
  },
  {
    "objectID": "rm-data/slides/week09.html#what-is-not-special-in-time-series",
    "href": "rm-data/slides/week09.html#what-is-not-special-in-time-series",
    "title": "Time series data",
    "section": "What is not special in time series",
    "text": "What is not special in time series\n\nTime series regressions are special for several reasons…but many aspects remain the same\n\nGeneralization, confidence intervals: Same difficulties as in cross-section\nTime series regression uncover patterns rather than evidence of causality: There is no “sample of time”\nPractical data issues, missing observations, extreme values etc, remain\nCoefficient interpretation is based on conditional comparison"
  },
  {
    "objectID": "rm-data/slides/week09.html#what-is-special-in-time-series",
    "href": "rm-data/slides/week09.html#what-is-special-in-time-series",
    "title": "Time series data",
    "section": "What is special in time series",
    "text": "What is special in time series\n\nOrdering matters - key difference to cross section. Complications…Past before future\nTrend - variables tend to have trends! Increasing or decreasing values over time\nSeasonality - variables may show some cyclical component, such 4 seasons, months, - every e.g. December value is expected to be different.\nTime series values are often not independent - correlated in time\n\nFailure to account for this may lead to wrong (biased) conclusions"
  },
  {
    "objectID": "rm-data/slides/week09.html#ts-variables-may-have-trends-trends",
    "href": "rm-data/slides/week09.html#ts-variables-may-have-trends-trends",
    "title": "Time series data",
    "section": "TS Variables may have trends Trends",
    "text": "TS Variables may have trends Trends\n\nTime series data “tends” to have trends. how to detect them?.\nDefine change (or fist difference): \\(\\Delta y_t = y_t - y_{t-1}\\)\n\n\\[\n\\begin{align}\n\\text{Positive trend}: & E[\\Delta y_t] &gt; 0 \\\\\n\\text{Negative trend}: & E[\\Delta y_t] &lt; 0\n\\end{align}\n\\]\n\nOne may also consider two types of trends: linear and exponential\n\n\\[\n\\begin{align}\n\\text{Linear trend}: & E[\\Delta y_t] = \\text{constant} \\\\\n\\text{Exponential trend}: & E[\\Delta \\ln(y_t)] = \\text{constant}\n\\end{align}\n\\]\n\nExponential trend, Changes are proportional"
  },
  {
    "objectID": "rm-data/slides/week09.html#but-they-may-also-show-seasonality",
    "href": "rm-data/slides/week09.html#but-they-may-also-show-seasonality",
    "title": "Time series data",
    "section": "But they may also show Seasonality",
    "text": "But they may also show Seasonality\n\nThere is seasonal variation, or seasonality, if its expected value changes periodically.\n\nFollows the seasons of the year, days of the week, hours of the day.\n\nSeasonality may be linear, when the seasonal differences are constant; it may be exponential, if relative differences (that may be approximated by log differences) are constant.\nImportant real life phenomenon - many economic activities follow seasonal variation over the year, through the week or day.\n\nThus they need to be accounted for in the analysis."
  },
  {
    "objectID": "rm-data/slides/week09.html#what-is-special-in-time-series-stationarity",
    "href": "rm-data/slides/week09.html#what-is-special-in-time-series-stationarity",
    "title": "Time series data",
    "section": "What is special in time series: Stationarity",
    "text": "What is special in time series: Stationarity\n\nTo learn something, we need consistency in patterns If the patterns change constantly, we cannot learn anything from the data\n\n\n\nStationary time series have the same expected value and same distribution, at all times.\nStationarity is a feature of the time series itself.\nStationarity means stability (in expectations).\nIf series are stationary, we can learn from them!"
  },
  {
    "objectID": "rm-data/slides/week09.html#what-is-special-in-time-series-non-stationarity",
    "href": "rm-data/slides/week09.html#what-is-special-in-time-series-non-stationarity",
    "title": "Time series data",
    "section": "What is special in time series: Non-stationarity",
    "text": "What is special in time series: Non-stationarity\n\nIn absence of stationarity, we cannot learn anything from the data Or what we learn may be misleading\n\n\n\nNon-stationary time series are those that are not stable for some reason.\nSeries that violate stationarity because the expected value is different at different times:\n\nHas a trends\nHas seasonality\nHas some unstable patterns\n\n\n\n\n\nAlthough, some of this issues can be address by “controlling” for them in the regression."
  },
  {
    "objectID": "rm-data/slides/week09.html#a-special-case-random-walk",
    "href": "rm-data/slides/week09.html#a-special-case-random-walk",
    "title": "Time series data",
    "section": "A Special case: Random walk",
    "text": "A Special case: Random walk\nSome non-stationary variables are not easy to deal with:\n\n\nAnother example of non-stationary time series is the random walk.\nRandom walk when \\(y_t\\) follows a random walk if its value in \\(t\\) is the same as in \\((t - 1)\\) plus some random term: \\(y_t = y_{t-1} + e_t\\).\nTime series variables that follow random walk change in completely random ways.\nWhatever the previous change was the next one may be anything. Wherever it starts, a random walk variable may end up anywhere after a long time."
  },
  {
    "objectID": "rm-data/slides/week09.html#section",
    "href": "rm-data/slides/week09.html#section",
    "title": "Time series data",
    "section": "",
    "text": "Code\nqui: {\n  clear\nset scheme white2\ncolor_style bay\nqui:set obs 101\ngen r = runiform(0,2*_pi)\n\ngen y = 0 \ngen x = 0\nreplace y = y[_n-1] + sin(r) if _n&gt;1\nreplace x = x[_n-1] + cos(r) if _n&gt;1\ngen n = _n\n}\n\n*scatter  y x , connect(l) name(m1, replace) \n*line y x n, name(m2, replace)"
  },
  {
    "objectID": "rm-data/slides/week09.html#what-to-fear-of-random-walk",
    "href": "rm-data/slides/week09.html#what-to-fear-of-random-walk",
    "title": "Time series data",
    "section": "What to fear of Random walk",
    "text": "What to fear of Random walk\n\n\nRandom walks are impossible to predict\nafter a change, they don’t revert back to some value or trend line but continue their journey from that point.\nSpread rising from one interval to another\nWhy is this important?\n\n\n\n\nTo identify patterns, we need stationarity\nFor stationary series, we need stability of patterns\nAvoid series with random walk when running regressions\n\nUsing them would only cause problems of Spurious Regressions"
  },
  {
    "objectID": "rm-data/slides/week09.html#how-to-detect-it-unit-root-test",
    "href": "rm-data/slides/week09.html#how-to-detect-it-unit-root-test",
    "title": "Time series data",
    "section": "How to detect it: Unit root test",
    "text": "How to detect it: Unit root test\n\n\nWe can test if a series is a random walk.\nPhillips-Perron test is based on this model: \\[y_t = \\alpha + \\rho y_{t-1} + e_t\\]\nThis model represents a random walk if \\(\\rho = 1\\), which is also called a unit root\nRandom walk test = testing if the series has a unit root.\nThe Phillips-Perron test has hypothesis\n\n\\[H_0: \\rho = 1 \\text{ vs } H_A: \\rho &lt; 1\\]"
  },
  {
    "objectID": "rm-data/slides/week09.html#section-1",
    "href": "rm-data/slides/week09.html#section-1",
    "title": "Time series data",
    "section": "",
    "text": "This test does not follow the usual t-distribution. Has its own distribution.\nin Stata, you can use it with the pperron command. (see helpfile)\nWhen the p-value is large (e.g., larger than 0.05), we don’t reject the null, concluding that the time series variable follows a random walk\nMany versions of unit root test.\n\n(Augmented) Dicky Fuller test is another popular one.\n\n\n\\[\\Delta y_t = \\alpha + \\delta y_{t-1} + e_t\\]\n\n\\(H_0: \\delta = 0\\) vs \\(H_A: \\delta &lt; 0\\)\nTests usually agree, but not always. see dfuller"
  },
  {
    "objectID": "rm-data/slides/week09.html#time-series-summary",
    "href": "rm-data/slides/week09.html#time-series-summary",
    "title": "Time series data",
    "section": "Time Series: Summary",
    "text": "Time Series: Summary\n\nStationary series are those where the expected value, variance, and auto-correlation does not change.\nExamples of non–stationarity:\n\nTrend - Expected value is different in later time periods than in earlier time periods\nSeasonality - Expected value is different in periodically recurring time periods\nRandom walk – Variance keeps increasing over time\n\nThis is the one we need to avoid\n\n\nWhy care? Regression with time series data variables that are not stationary are likely to give misleading results (Spurious)."
  },
  {
    "objectID": "rm-data/slides/week09.html#what-to-do-with-ts-cook-book",
    "href": "rm-data/slides/week09.html#what-to-do-with-ts-cook-book",
    "title": "Time series data",
    "section": "What to do With TS: Cook-book",
    "text": "What to do With TS: Cook-book\n\n\nCheck if your variable is stationary\n\nVisualize\nDo a unit-root test\n\n\n\n\n\nIf there is a good reason to believe your variable trending (or is Random Walk)\n\nTake differences \\(\\Delta y_t\\), or add a trend variable\nTake percentage changes or log differences\n\nIn extremely rare cases, difference your variable twice\n\n\n\n\n\n\nIf your variable has a seasonality\n\nUse seasonality dummies in your regression\nMay consider to work with seasonal changes."
  },
  {
    "objectID": "rm-data/slides/week09.html#microsoft-and-sp-500-stock-prices---data",
    "href": "rm-data/slides/week09.html#microsoft-and-sp-500-stock-prices---data",
    "title": "Time series data",
    "section": "Microsoft and S&P 500 stock prices - data",
    "text": "Microsoft and S&P 500 stock prices - data\n\nDaily price of Microsoft stock and value of S&P 500 stock market index\nThe data covers 21 years starting with December 31 1997 and ending with December 31 2018.\nMany decisions to make…\nLook at data first"
  },
  {
    "objectID": "rm-data/slides/week09.html#case-study-stock-price-and-stock-market-index-value",
    "href": "rm-data/slides/week09.html#case-study-stock-price-and-stock-market-index-value",
    "title": "Time series data",
    "section": "Case study: Stock price and stock market index value",
    "text": "Case study: Stock price and stock market index value\n\nCode\nqui {\nuse \"data_slides/stock-prices-daily.dta\", clear\nren *, low\n}\nline p_sp500 date, name(m1, replace)\nline p_msft date, name(m2, replace)"
  },
  {
    "objectID": "rm-data/slides/week09.html#time-series-comparisons---sp-500-case-study",
    "href": "rm-data/slides/week09.html#time-series-comparisons---sp-500-case-study",
    "title": "Time series data",
    "section": "Time series comparisons - S&P 500 case study",
    "text": "Time series comparisons - S&P 500 case study\nKey decisions:\n\nDaily price = closing price\nGaps will be overlooked\n\nFriday-Monday gap ignored\nHolidays (Christmas, 4 of July (when would be a weekday)\n\nAll values kept, extreme values part of process\nIn finance, portfolio managers often focus on monthly returns, Hence we choose monthly returns to analyze.\nTake the last day of each month"
  },
  {
    "objectID": "rm-data/slides/week09.html#microsoft-and-sp-500-stock-prices---ts-plot",
    "href": "rm-data/slides/week09.html#microsoft-and-sp-500-stock-prices---ts-plot",
    "title": "Time series data",
    "section": "Microsoft and S&P 500 stock prices - ts plot",
    "text": "Microsoft and S&P 500 stock prices - ts plot\n\nCode\nqui {\nuse \"data_slides/stock-prices-daily.dta\", clear\nren *, low\nsort ym date\nbysort ym: gen flag = _n == _N\nkeep if flag==1\n}\nline p_sp500 ym, name(m1, replace)\nline p_msft ym, name(m2, replace)"
  },
  {
    "objectID": "rm-data/slides/week09.html#microsoft-and-sp-500-stock-prices---decisions-2",
    "href": "rm-data/slides/week09.html#microsoft-and-sp-500-stock-prices---decisions-2",
    "title": "Time series data",
    "section": "Microsoft and S&P 500 stock prices - decisions 2",
    "text": "Microsoft and S&P 500 stock prices - decisions 2\n\nMonthly time series plot - easier to read\nAdditional decision needed: it is obviously non-stationary (Phillips-Perron test: very high p-value)\nUse returns:\n\nReturns: percent change of the closing prices: \\(100\\% \\frac{y_t - y_{t-1}}{y_t}\\).\nmonthly returns - take the closing price for the last day of a month\nAlternative measure: first difference of log prices."
  },
  {
    "objectID": "rm-data/slides/week09.html#microsoft-and-sp-500---index-returns-pct",
    "href": "rm-data/slides/week09.html#microsoft-and-sp-500---index-returns-pct",
    "title": "Time series data",
    "section": "Microsoft and S&P 500 - index returns (pct)",
    "text": "Microsoft and S&P 500 - index returns (pct)\n\nCode\nqui: {\nuse \"data_slides/stock-prices-daily.dta\", clear\nren *, low\nsort ym date\nbysort ym: gen flag = _n == _N\nkeep if flag==1\ntsset ym\ngen ret_sp500 = 100 * (p_sp500 - p_sp500[_n-1]) / p_sp500[_n-1]\ngen ret_msft = 100 * (p_msft - p_msft[_n-1]) / p_msft[_n-1]\n}\n\nline ret_sp500 ym, name(m1, replace)\nline ret_msft ym, name(m2, replace)"
  },
  {
    "objectID": "rm-data/slides/week09.html#unit-root-test-microsoft-and-sp-500-returns",
    "href": "rm-data/slides/week09.html#unit-root-test-microsoft-and-sp-500-returns",
    "title": "Time series data",
    "section": "Unit root test: Microsoft and S&P 500 returns",
    "text": "Unit root test: Microsoft and S&P 500 returns\n\n\nCode\npperron ret_sp500\npperron ret_msft\n\n\n\nPhillips–Perron test for unit root       Number of obs   = 251\nVariable: ret_sp500                      Newey–West lags =   4\n\nH0: Random walk without drift, d = 0\n\n                                       Dickey–Fuller\n                   Test      -------- critical value ---------\n              statistic           1%           5%          10%\n--------------------------------------------------------------\n Z(rho)        -230.845      -20.301      -14.000      -11.200\n Z(t)           -14.365       -3.460       -2.880       -2.570\n--------------------------------------------------------------\nMacKinnon approximate p-value for Z(t) = 0.0000.\n\nPhillips–Perron test for unit root       Number of obs   = 251\nVariable: ret_msft                       Newey–West lags =   4\n\nH0: Random walk without drift, d = 0\n\n                                       Dickey–Fuller\n                   Test      -------- critical value ---------\n              statistic           1%           5%          10%\n--------------------------------------------------------------\n Z(rho)        -284.851      -20.301      -14.000      -11.200\n Z(t)           -19.346       -3.460       -2.880       -2.570\n--------------------------------------------------------------\nMacKinnon approximate p-value for Z(t) = 0.0000."
  },
  {
    "objectID": "rm-data/slides/week09.html#time-series-regressions-the-same",
    "href": "rm-data/slides/week09.html#time-series-regressions-the-same",
    "title": "Time series data",
    "section": "Time series regressions: The same",
    "text": "Time series regressions: The same\n\nRegression in time series data is defined and estimated the same way as in other data.\n\n\\[y^E_t = \\beta_0 + \\beta_1 x_{1t} + \\beta_2 x_{2t} + \\ldots\\]\n\nInterpretations similar to cross-section\n\n\\(\\beta_0\\): We expect \\(y\\) to be \\(\\beta_0\\) when all explanatory variables are zero.\n\\(\\beta_1\\): Comparing time periods with different \\(x_1\\) but the same in terms of all other explanatory variables, we expect \\(y\\) to be higher by \\(\\beta_1\\) when \\(x_1\\) is higher by one unit."
  },
  {
    "objectID": "rm-data/slides/week09.html#time-series-regression-but-different",
    "href": "rm-data/slides/week09.html#time-series-regression-but-different",
    "title": "Time series data",
    "section": "Time series regression: But different",
    "text": "Time series regression: But different\n\nWith time series data, we often estimate regressions in changes\nWe use the \\(\\Delta\\) notation for changes\n\n\\[\\Delta x_t = x_t - x_{t-1}\\]\n\nThe regression in changes is \\[\\Delta y^E_t = \\alpha + \\beta \\Delta x_t\\]\n\n\\(\\alpha\\): \\(y\\) is expected to change by \\(\\alpha\\) when \\(x\\) doesn’t change\n\\(\\beta\\): \\(y\\) is expected to change by \\(\\beta\\) more when \\(x\\) increases by one unit more"
  },
  {
    "objectID": "rm-data/slides/week09.html#time-series-regression-growth",
    "href": "rm-data/slides/week09.html#time-series-regression-growth",
    "title": "Time series data",
    "section": "Time series regression: Growth",
    "text": "Time series regression: Growth\n\nWe often have variables in relative or percentage changes,\n\n\\[\\%\\Delta  (y_t)^E = \\alpha + \\beta \\%\\Delta(x_t)\\]\n\nWe can approximate relative differences by log differences, which are here log change: first taking logs of the variables and then taking the first difference\n\n\\[\\Delta \\ln(y_t) = \\ln(y_t) - \\ln(y_{t1})\\]"
  },
  {
    "objectID": "rm-data/slides/week09.html#returns-on-a-company-stock-and-market-returns",
    "href": "rm-data/slides/week09.html#returns-on-a-company-stock-and-market-returns",
    "title": "Time series data",
    "section": "Returns on a company stock and market returns",
    "text": "Returns on a company stock and market returns\n\\[\\%\\Delta(\\text{MSFT}_t) = \\alpha + \\beta \\%\\Delta (\\text{SP500}_t)\\]\n\n\\(\\alpha = 0.54\\); \\(\\beta = 1.26\\)\nIntercept: returns on the Microsoft stock tend to be 0.54 percent when the S%P500 index doesn’t change.\nSlope: returns on the Microsoft stock tend to be 1.26% higher when the returns on the S&P500 index are 1% higher. The 95% CI is \\([1.06, 1.46]\\).\nR-squared: 0.36\nFirst difference of log prices. Estimate is 1.24\nDaily returns (percent), beta is 1.10"
  },
  {
    "objectID": "rm-data/slides/week09.html#issues-to-deal-with-before-regression-a-laundry-list",
    "href": "rm-data/slides/week09.html#issues-to-deal-with-before-regression-a-laundry-list",
    "title": "Time series data",
    "section": "Issues to deal with, before Regression: a laundry list",
    "text": "Issues to deal with, before Regression: a laundry list\n\nHandling trend(s) and random walk (RW)\n\nAdd trend variable or Difference the series\n\nTransforming the series, such as taking first differences or percent change\nHandling seasonality and special events\n\nInclude dummies\n\nReconsidering standard errors. Specially if series have Unit Roots\nDealing with serial correlation - taking time-to-build into account with lags"
  },
  {
    "objectID": "rm-data/slides/week09.html#trend-rw---spurious-regression",
    "href": "rm-data/slides/week09.html#trend-rw---spurious-regression",
    "title": "Time series data",
    "section": "Trend & RW - Spurious regression",
    "text": "Trend & RW - Spurious regression\n\nTrends, seasonality, and random walks can present serious threats to uncovering meaningful patterns in time series data.\n\nExample: time series regression in levels \\(y^E_t = \\alpha + \\beta x_t\\).\nIf both \\(y\\) and \\(x\\) have a positive trend, the slope coefficient \\(\\beta\\) will be positive whether the two variables are related or not.\nAssociations between variables only because of the effect of trends are said to be spurious correlation.\n\nThink of trend and seasonality are confounders (omitted variables)\n\ntrend: global tendencies e.g. economic activity, fashion\nseasonality: e.g. weather, holidays, human habits (sleep)\n\nAnother reason for spurious correlation is small sample size…"
  },
  {
    "objectID": "rm-data/slides/week09.html#time-series-regressions-trends-and-seasonality",
    "href": "rm-data/slides/week09.html#time-series-regressions-trends-and-seasonality",
    "title": "Time series data",
    "section": "Time series regressions: Trends and seasonality",
    "text": "Time series regressions: Trends and seasonality\n\nTrend as confounder example\nA regression of the price of college education in the U.S. on the GDP of Germany over the past few decades\nPositive slope coefficient even though that two may not be related in any fundamental way.\nBut US GDP is correlated with both\n\nExamples https://tylervigen.com/spurious-correlations"
  },
  {
    "objectID": "rm-data/slides/week09.html#time-series-regressions-trends-and-seasonality-1",
    "href": "rm-data/slides/week09.html#time-series-regressions-trends-and-seasonality-1",
    "title": "Time series data",
    "section": "Time series regressions: Trends and seasonality",
    "text": "Time series regressions: Trends and seasonality\n\nIn a regression, we shall deal with trends\n\nReplacing variables in the regression with their first differences\n\nVariables in differences – no trends – likely to to be stationary.\nCould be log difference for exponential trends\n\nCould also add trends to model\n\nIn a regression, we shall deal with seasonality\n\nIncluding binary season variables in regressions.\nLook at pattern, figure out if quarters, months, weeks, days of week, etc.\nOr work with year-on-year (event to event) changes instead of first differences."
  },
  {
    "objectID": "rm-data/slides/week09.html#trend-rw---solution-first-differences",
    "href": "rm-data/slides/week09.html#trend-rw---solution-first-differences",
    "title": "Time series data",
    "section": "Trend & RW - solution: first differences",
    "text": "Trend & RW - solution: first differences\n\\[\\Delta y^E_t = \\alpha + \\beta \\Delta x_t\\]\n\nCoefficients have the same interpretation as before, but relate changes in variables.\nBecause variables denote changes…\n\n\\(\\alpha\\) is the average change in \\(y\\) when \\(x\\) doesn’t change.\nThe slope coefficient on \\(\\Delta x_t\\) shows how much more \\(y\\) is expected to change when \\(x\\) changes by one more unit.\nThe slope shows how \\(y\\) is expected to change when \\(x\\) changes, in addition to \\(\\alpha\\)."
  },
  {
    "objectID": "rm-data/slides/week09.html#seasonality-in-time-series-regressions",
    "href": "rm-data/slides/week09.html#seasonality-in-time-series-regressions",
    "title": "Time series data",
    "section": "Seasonality in time series regressions",
    "text": "Seasonality in time series regressions\n\nCapturing seasonality is also crucial.\nHigher the frequency – the more important.\n\nPeople behave differently on different hours and days\nWeather varies over months\nHolidays, ect\n\nHave seasonal dummies if seasonality is stable.\n\n\\[y^E_t = \\alpha + \\beta x_t + \\delta_{Jan} + \\delta_{Feb} + \\cdots + \\delta_{Nov}\\]\n\nPattern may vary over time. If it does, solutions must capture exact pattern – (difficult, not covering here)"
  },
  {
    "objectID": "rm-data/slides/week09.html#another-problem-serial-correlation",
    "href": "rm-data/slides/week09.html#another-problem-serial-correlation",
    "title": "Time series data",
    "section": "Another Problem: Serial correlation",
    "text": "Another Problem: Serial correlation\n\nSerial correlation means correlation of a variable with its previous values\n\nIt usually refers to the correlation of the residuals with their previous values.\n\nOrder serial correlation coefficient is defined as \\[\\rho_k = \\text{Corr}[x_t, x_{t-k}]\\]\nIf independent variables are serially correlated, usually not a problem.\nIf dependent variable is serially correlated (the error), it is a problem.\nSerial correlation makes the usual standard error estimates wrong.\n\nEven classical heteroskedasticity robust SE is wrong - sometimes very wrong\n\nMore precisely it is serial correlation in residuals, but think about is as serial correlation in \\(y_t\\) is okay"
  },
  {
    "objectID": "rm-data/slides/week09.html#standard-errors-in-time-series-regressions",
    "href": "rm-data/slides/week09.html#standard-errors-in-time-series-regressions",
    "title": "Time series data",
    "section": "Standard errors in time series regressions",
    "text": "Standard errors in time series regressions\n\nIn most time series, there will be some serial correlation\nSol1: Use the Newey-West SE: in Stata newey command.\n\nThis incorporates structure of serial correlation of the regression residuals\nFine if heteroskedasticity as well\nNeed to specify lags, based on frequency and seasonality\n\nSol2: Have lagged dependent variable in the regression (one lag is usually enough) \\[y_t = \\alpha + \\beta x_t + \\gamma_1 y_{t-1} + \\gamma_2 y_{t-2} \\ldots\\]\n\nThis will change Coefficients and adjust SEs"
  },
  {
    "objectID": "rm-data/slides/week09.html#electricity-consumption-and-temperature",
    "href": "rm-data/slides/week09.html#electricity-consumption-and-temperature",
    "title": "Time series data",
    "section": "Electricity consumption and temperature",
    "text": "Electricity consumption and temperature\n\nWe have access to monthly weather and electricity data for Phoenix, Arizona: Overall 204 month\nAlso access to “cooling degree days” and “heating degree days” data\nThe cooling degree days measure Number of degrees that a day’s average temperature is above 65F (18C). This is add up over the month.\nSimilar for heating degree days but below 65F (18C)\nAccess to Electricity consumption data over the month\nAs expected, this values will be seasonal."
  },
  {
    "objectID": "rm-data/slides/week09.html#cs-modelling-decisions",
    "href": "rm-data/slides/week09.html#cs-modelling-decisions",
    "title": "Time series data",
    "section": "CS: Modelling decisions",
    "text": "CS: Modelling decisions\n\nThere is an exponential trend in electricity –&gt; use log difference\nFor easier interpretation, take first difference (FD) of cooling and heating days\nNatural question: How much does electricity consumption change when temperature changes?\nAdd monthly dummies, January (December to January) is reference\nUse Newey-West standard errors"
  },
  {
    "objectID": "rm-data/slides/week09.html#model-estimates",
    "href": "rm-data/slides/week09.html#model-estimates",
    "title": "Time series data",
    "section": "Model estimates",
    "text": "Model estimates\n\n\n\nVariables\n(1)\n(2)\n\n\n\n\n\\(\\Delta\\)CD\n0.031**\n0.017**\n\n\n\n(0.001)\n(0.002)\n\n\n\\(\\Delta\\)HD\n0.037**\n0.014**\n\n\n\n(0.003)\n(0.003)\n\n\nmonth = 2, February\n\n-0.274**\n\n\nmonth = 3, March\n\n-0.122**\n\n\n…\n\n\n\n\nConstant\n0.001\n0.092**\n\n\n\n(0.002)\n(0.013)\n\n\nObservations\n203\n203"
  },
  {
    "objectID": "rm-data/slides/week09.html#model-results",
    "href": "rm-data/slides/week09.html#model-results",
    "title": "Time series data",
    "section": "Model results",
    "text": "Model results\n\nSimple (1) model:\n\nIn months when cooling degrees increase by one degree and heating degrees do not change, electricity consumption increases by 3.1 percent, on average.\nWhen heating degrees increase by one degree and cooling degrees do not change, electricity consumption increases by 3.7 percent, on average.\n\nModel (2) with monthly dummies.\n\nThe reference month is January: constant (when cooling and heating degrees stay the same), electricity consumption increases by about 9% from December to January.\nThe other season coefficients compare to this change:\n\nFebruary: the January to February change is 28 percentage points lower than in the reference month, December to January."
  },
  {
    "objectID": "rm-data/slides/week09.html#electricity-consumption-and-temperature-different-se-estimates",
    "href": "rm-data/slides/week09.html#electricity-consumption-and-temperature-different-se-estimates",
    "title": "Time series data",
    "section": "Electricity consumption and temperature – different SE estimates",
    "text": "Electricity consumption and temperature – different SE estimates\n\n\n\nVariables\n(1)\n(2)\n(3)\n\n\n\n\n\nSimple SE\nNewey–West SE\nLagged dep.var\n\n\n\\(\\Delta\\)CD\n0.017**\n0.017**\n0.017**\n\n\n\n(0.002)\n(0.002)\n(0.002)\n\n\n\\(\\Delta\\)HD\n0.014**\n0.014**\n0.014**\n\n\n\n(0.002)\n(0.003)\n(0.002)\n\n\nLag of \\(\\Delta\\) ln Q\n\n\n-0.002\n\n\n\n\n\n(0.062)\n\n\nMonth dummies\nYES\nYES\nYES\n\n\nObservations\n203\n203\n202\n\n\nR-squared\n0.951\n0.951\n\n\n\nStandard errors in parentheses\n\n\n\n\n\n** p&lt;0.01, * p&lt;0.05"
  },
  {
    "objectID": "rm-data/slides/week09.html#electricity-consumption-and-temperature-different-se-estimates-1",
    "href": "rm-data/slides/week09.html#electricity-consumption-and-temperature-different-se-estimates-1",
    "title": "Time series data",
    "section": "Electricity consumption and temperature – different SE estimates",
    "text": "Electricity consumption and temperature – different SE estimates\n\nTo correct for serial correlation, compare simple SE model with two correctly specified models\n\nwith Newey-West SE\nwith Lagged dependent variable\n\nSE marginally different, and with lagged values, coefficients are also similar up to 3 digits\n\nSimilar, not the same\nSometimes substantial difference (if strong serial correlation)"
  },
  {
    "objectID": "rm-data/slides/week09.html#propagation-effect-changes-and-lags---fdl-model",
    "href": "rm-data/slides/week09.html#propagation-effect-changes-and-lags---fdl-model",
    "title": "Time series data",
    "section": "Propagation effect: changes and lags - FDL model",
    "text": "Propagation effect: changes and lags - FDL model\nIn time series, we can analyze how an impact builds up across several periods (time-to-build):\n\\[\\Delta y^E_t = \\alpha + \\beta_0 \\Delta x_t + \\beta_1 \\Delta x_{t-1} + \\beta_2 \\Delta x_{t-2}\\]\n\n\\(\\beta_0\\) = how many units more \\(y\\) is expected to change within the same time period when \\(x\\) changes by one more unit (No change before or after).\n\\(\\beta_1\\) = how much more \\(y\\) is expected to change in the next time period after \\(x\\) changed by one more unit.\nCumulative effect: \\(\\beta_\\text{cumul} = \\beta_0 + \\beta_1 + \\beta_2\\)\n\\(\\beta_k\\) are the short-term effects, \\(\\beta_\\text{cumul}\\) is the cumulative effect/Long Term Effect."
  },
  {
    "objectID": "rm-data/slides/week09.html#testing-the-cumulative-effect",
    "href": "rm-data/slides/week09.html#testing-the-cumulative-effect",
    "title": "Time series data",
    "section": "Testing the cumulative effect",
    "text": "Testing the cumulative effect\n\nTo get a SE on the cumulative effect, do a trick and transformation, and estimate a different model\n\n\\[\n\\begin{aligned}\n\\Delta y^E_t &= \\alpha + \\beta_0 \\Delta x_t + \\beta_1 \\Delta x_{t-1} + \\beta_2 \\Delta x_{t-2} \\\\\n\\Delta y^E_t &= \\alpha + (\\beta_\\text{cumul} - \\beta_1 - \\beta_2) \\Delta x_t + \\beta_1 \\Delta x_{t-1} + \\beta_2 \\Delta x_{t-2} \\\\\n\\Delta y^E_t &= \\alpha + \\beta_\\text{cumul} \\Delta x_t + \\beta_1 (\\Delta x_{t-1}- \\Delta x_t) + \\beta_2 (\\Delta x_{t-2} - \\Delta x_t)\\\\\n\\Delta y^E_t &= \\alpha + \\beta_\\text{cumul} \\Delta x_{t} + \\delta_0 \\Delta(\\Delta x_t) + \\delta_1 \\Delta(\\Delta x_{t-1})\n\\end{aligned}\\]\n\n\\(\\beta_\\text{cumul}\\) is exactly the same as \\(\\beta_0 + \\beta_1 + \\beta_2\\)\nUsually estimate both. Separate and cumulative effect\nOften need a few lags"
  },
  {
    "objectID": "rm-data/slides/week09.html#propagation-effect-changes-and-lags---fdl-model-1",
    "href": "rm-data/slides/week09.html#propagation-effect-changes-and-lags---fdl-model-1",
    "title": "Time series data",
    "section": "Propagation effect: changes and lags - FDL model",
    "text": "Propagation effect: changes and lags - FDL model\nSee Case Study in Chapter 12"
  },
  {
    "objectID": "rm-data/slides/week09.html#summary-of-the-process",
    "href": "rm-data/slides/week09.html#summary-of-the-process",
    "title": "Time series data",
    "section": "Summary of the process",
    "text": "Summary of the process\n\nDecide on frequency; deal with gaps if necessary.\nPlot the series. Identify features and issues.\nHandle trends by transforming variables (Often: first difference d.).\nSpecify regression that handles seasonality, usually by including season dummies.\nInclude or don’t include lags of the right-hand-side variable(s).\nHandle serial correlation. newey or lagged dependent variable.\nInterpret coefficients in a way that pays attention to potential trend and seasonality.\nTime series econometrics very complicated beyond this.\nBut: These steps often good enough."
  },
  {
    "objectID": "rm-data/slides/week09.html#main-takeaways",
    "href": "rm-data/slides/week09.html#main-takeaways",
    "title": "Time series data",
    "section": "Main takeaways",
    "text": "Main takeaways\n\nRegressions with time series data allow for additional opportunities, but they pose additional challenges, too\nRegressions with time series data help uncover associations from changes and associations across time\nTrend, seasonality, and random walk-like non-stationarity are additional challenges\nDo not regress variables that have trend or seasonality; without dealing with them they produce spurious results"
  },
  {
    "objectID": "rm-data/slides/week07.html#motivation",
    "href": "rm-data/slides/week07.html#motivation",
    "title": "Multiple regression analysis",
    "section": "Motivation",
    "text": "Motivation\n\nWe are interested in finding evidence for or against labor market discrimination of women. Compare wages for men and women who share similarities in wage relevant factors such as experience and education.\nFind a good deal on a hotel to spend a night in a European city- analyzed the pattern of hotel price and distance and many other features to find hotels that are underpriced not only for their location but also those other features."
  },
  {
    "objectID": "rm-data/slides/week07.html#topics-to-cover",
    "href": "rm-data/slides/week07.html#topics-to-cover",
    "title": "Multiple regression analysis",
    "section": "Topics to cover",
    "text": "Topics to cover\n\nMultiple regression mechanics\nEstimation and interpreting coefficients\nNon-linear terms, interactions\nVariable selection, small sample problems\nMultiple regression and causality\nMultiple regression and prediction"
  },
  {
    "objectID": "rm-data/slides/week07.html#multivariate-regression",
    "href": "rm-data/slides/week07.html#multivariate-regression",
    "title": "Multiple regression analysis",
    "section": "Multivariate Regression",
    "text": "Multivariate Regression\n\nWhenever you start modeling an outcome \\(y\\), there will always be two factors that will determine that outcome:\n\nFactors that you can control (e.g., education, experience, etc.)\nFactors that you cannot control (e.g., errors)\n\n\nMultiple regression analysis uncovers average \\(y\\) as a function of more than one \\(x\\) variable: \\(y^E = f(x_1, x_2, ...)\\).\nIt can lead to better predictions \\(\\hat{y}\\) by considering more explanatory variables.\nIt may improve the interpretation of slope coefficients by comparing observations that are similar in terms of other \\(x's\\) variables."
  },
  {
    "objectID": "rm-data/slides/week07.html#multivariate-regression-1",
    "href": "rm-data/slides/week07.html#multivariate-regression-1",
    "title": "Multiple regression analysis",
    "section": "Multivariate Regression",
    "text": "Multivariate Regression\n\nMultiple linear regression specifies a linear function of the explanatory variables for the average \\(y\\): \\[y^E = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k\\]\nBut, now that we now we can have more than one \\(x\\) variable, what happens if we don’t?"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-ommited-variable-bias",
    "href": "rm-data/slides/week07.html#mr-ommited-variable-bias",
    "title": "Multiple regression analysis",
    "section": "MR: Ommited Variable Bias",
    "text": "MR: Ommited Variable Bias\nLets say we have to models: \\[\\begin{aligned}\ny &= \\alpha_0 + \\alpha_1 x_1 + \\varepsilon_1  \\\\\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon_2\n\\end{aligned}\n\\]\n\nHow do \\(\\alpha_1\\) and \\(\\beta_1\\) compare?\nLets Start by regressing \\(x_2\\) on \\(x_1\\): \\(x_2 = \\delta_0 + \\delta_1 x_1 + u\\)\nAnd plug this back into the second equation:\n\n\\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 (\\delta_0 + \\delta_1 x_1 + u) + \\varepsilon_2 \\\\\ny &= (\\beta_0 + \\beta_2\\delta_0) + (\\beta_1 + \\beta_2 \\delta_1) x  + (\\beta_2 u + \\varepsilon_2)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-ommited-variable-bias-1",
    "href": "rm-data/slides/week07.html#mr-ommited-variable-bias-1",
    "title": "Multiple regression analysis",
    "section": "MR: Ommited Variable Bias",
    "text": "MR: Ommited Variable Bias\n\nSo it turns out that:\n\n\\[\\alpha_1 = \\beta_1 + \\beta_2 \\delta_1 \\rightarrow \\beta_1-\\alpha_1 = -\\beta_2 \\delta_1\n\\]\n\nBy “ignoring” \\(x_2\\) in the first regression, we are actually estimating a biased coefficient for \\(x_1\\).\n\nAssuming the second model is the “true” model\n\nThis is what is known as the Omitted variable bias OBV\n\nNote: You could also have a bias because you are including a variable that should not be there. This is known as bad control."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-ommited-variable-bias-2",
    "href": "rm-data/slides/week07.html#mr-ommited-variable-bias-2",
    "title": "Multiple regression analysis",
    "section": "MR: Ommited Variable Bias",
    "text": "MR: Ommited Variable Bias\n\nOBV is a common problem in empirical research because we can never include all the variables that determine \\(y\\).\nHowever, mechanically, there are two cases where OBV is not a problem:\n\nWhen \\(x_1\\) and \\(x_2\\) are uncorrelated (\\(\\delta_1 = 0\\))\nWhen \\(y\\) and \\(x_2\\) are uncorrelated (\\(\\beta_2 = 0\\))"
  },
  {
    "objectID": "rm-data/slides/week07.html#simple-example",
    "href": "rm-data/slides/week07.html#simple-example",
    "title": "Multiple regression analysis",
    "section": "Simple example",
    "text": "Simple example\n\nTS regression: Regress month-to-month change in log quantity sold of Beer (\\(y\\)) on month-to-month change in log price (\\(x_1\\)).\n\n\\(\\beta = -0.5\\): sales tend to decrease by 0.5% when our price increases by 1%.\n\nRobustness: \\(x_2\\): change in ln average price charged by our competitors\n\nNew Results: \\(\\hat{\\beta}_1 = -3\\) and \\(\\hat{\\beta}_2 = 3\\)\n\nThere is a OBV (Model 1 is flatter than Model 2)\nPossibly the result of two things:\n\na positive association between the two price changes (\\(\\delta_1\\)) and\na positive association between competitor price and our own sales (\\(\\beta_2\\))."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-some-language",
    "href": "rm-data/slides/week07.html#mr-some-language",
    "title": "Multiple regression analysis",
    "section": "MR: Some language",
    "text": "MR: Some language\n\nSetup: Multiple regression with two explanatory variables (\\(x_1\\) and \\(x_2\\)),\nTechnicallity:: We measure differences in expected \\(y\\) across observations that differ in \\(x_1\\) but are similar in terms of \\(x_2\\).\nInterpretation: Difference in \\(y\\) by \\(x_1\\), conditional on \\(x_2\\). OR controlling for \\(x_2\\).\n\nWe condition on \\(x_2\\), or control for \\(x_2\\), when we include \\(x_2\\) in a multiple regression that focuses on average differences in \\(y\\) by \\(x_1\\).\n\nWhat we care is \\(x_1\\)’s effect on \\(y\\), but we control for \\(x_2\\) to get a better estimate of this effect.\nConfounding: \\(x_2\\) is a confounder if \\(x_2\\) is correlated with \\(x_1\\) and \\(y\\).\n\nThus, we have a problem if we omit \\(x_2\\) from the regression."
  },
  {
    "objectID": "rm-data/slides/week07.html#stata-multiple-regression",
    "href": "rm-data/slides/week07.html#stata-multiple-regression",
    "title": "Multiple regression analysis",
    "section": "Stata: Multiple regression",
    "text": "Stata: Multiple regression\nregress y x1 [x2 x3 ... ], robust\nestimates store m1\nesttab m1, star(* 0.10 ** 0.05 *** 0.01) label"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-standard-errors",
    "href": "rm-data/slides/week07.html#mr-standard-errors",
    "title": "Multiple regression analysis",
    "section": "MR: Standard Errors",
    "text": "MR: Standard Errors\n\\[\\text{SE}(\\hat{\\beta}_1) = \\frac{\\text{Std}[e]}{\\sqrt{n}\\text{Std}(x_1)\\color{blue}{\\sqrt{1 - R^2_1}}}\\]\n\nSame:\n\nthe SE is small if better the fit, large samples, or large the Std of \\(x_1\\).\n\nNew: \\(\\sqrt{1 - R^2_1}\\) term in the denominator.\n\nthe R-squared of the regression of \\(x_1\\) on \\(x_2\\)\n\nThe higher is \\(R^2_1\\), the larger the SE of \\(\\hat{\\beta}_1\\).\nNote: in practice, use robust SE"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-collinearity",
    "href": "rm-data/slides/week07.html#mr-collinearity",
    "title": "Multiple regression analysis",
    "section": "MR: Collinearity",
    "text": "MR: Collinearity\n\nPerfectly collinearity is when \\(x_i\\) is a linear function of \\(x_{-i}\\).\n\nConsequence: cannot calculate coefficients.\nOne will be dropped by software (but you should know which one).\n\nStrong but imperfect correlation between explanatory is sometimes called multicollinearity.\n\nConsequence: We can get the slope coefficients and their standard errors,\nBut, the standard errors may be large."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-collinearity-and-se",
    "href": "rm-data/slides/week07.html#mr-collinearity-and-se",
    "title": "Multiple regression analysis",
    "section": "MR: Collinearity and SE",
    "text": "MR: Collinearity and SE\n\nStrong multicollinearity is a problem because it increases the standard errors of the coefficients.\n\nIt is typically a problem when the sample size is small.\n\nNumerically, it could make the coefficient estimates unstable. (rare)\nMore often, you may need to either drop one of the variables, or\nCombine them into a single variable. (index)\n\nHow to know how strong is the multicollinearity problem ??\n\nEstimate \\(R^2\\) of the regression of \\(x_i\\) on all other \\(x\\) variables. For all cases!\n\nor use the estat vif command in Stata. (only with OLS)"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-collinearity-1",
    "href": "rm-data/slides/week07.html#mr-collinearity-1",
    "title": "Multiple regression analysis",
    "section": "MR: Collinearity",
    "text": "MR: Collinearity\n\nqui:frause oaxaca, clear\nqui:regress lnwage female educ exper tenure c.age c.age#c.age, robust\nestat vif\n\n\n\n\n\n    Variable |       VIF       1/VIF  \n-------------+----------------------\n      female |      1.11    0.900413\n        educ |      1.15    0.873284\n       exper |      2.48    0.403924\n      tenure |      1.82    0.549891\n         age |     54.62    0.018310\n c.age#c.age |     53.08    0.018839\n-------------+----------------------\n    Mean VIF |     19.04"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-testing-single-hypotheses",
    "href": "rm-data/slides/week07.html#mr-testing-single-hypotheses",
    "title": "Multiple regression analysis",
    "section": "MR: Testing Single hypotheses",
    "text": "MR: Testing Single hypotheses\n\nSame as before, but now we have more than one \\(x\\) variable to test.\n\n\\(H_0: \\beta_1 = 0\\)\n\nYou may want to be careful with multiple testing.\n\ntesting each coefficient separately with the same \\(\\alpha\\) level\n\nThere is also testing single hypotheses on combinations of coefficients.\n\n$H_0: _1-2*_2=0 $\n\nAs before, you just need to know the point estimate and the standard error to calculate the t-statistic."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-testing-joint-hypotheses",
    "href": "rm-data/slides/week07.html#mr-testing-joint-hypotheses",
    "title": "Multiple regression analysis",
    "section": "MR: Testing Joint hypotheses",
    "text": "MR: Testing Joint hypotheses\n\nTesting joint hypotheses: null hypotheses that contain statements about more than one regression coefficient: \\(H_0: \\beta_1 = \\beta_2 = 0\\) vs \\(H_1: H_0\\) is false\nThis kind of test is used to evaluate a subset of the coefficients (such as all geographical variables) are all zero.\nBut for doing this you need a new test statistic: the F-test.\n\nDifference with the t-test:\n\nIn contrast with the t-test, the F-test follows an F-distribution.\nThis distribution is not symmetric! And you need to know the degrees of freedom.\n\nHow many restrictions are you imposing? and how many coefficients did you estimate?\n\nAlso, all test are on-sided"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-testing-joint-hypotheses-1",
    "href": "rm-data/slides/week07.html#mr-testing-joint-hypotheses-1",
    "title": "Multiple regression analysis",
    "section": "MR: Testing Joint hypotheses",
    "text": "MR: Testing Joint hypotheses\n\nF-test"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-testing-hypotheses-in-stata",
    "href": "rm-data/slides/week07.html#mr-testing-hypotheses-in-stata",
    "title": "Multiple regression analysis",
    "section": "MR: Testing hypotheses in Stata",
    "text": "MR: Testing hypotheses in Stata\n\nRegressionJoint testSingle test Combined\n\n\n\nqui:webuse dui, clear\nregress  citations  fines i.taxes i.csize i.college, robust nohead\n** regress, coefleg to know \"names\" of variables\n\n------------------------------------------------------------------------------\n             |               Robust\n   citations | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       fines |  -7.690437   .3843873   -20.01   0.000    -8.445672   -6.935201\n             |\n       taxes |\n        Tax  |  -4.493918   .5819239    -7.72   0.000    -5.637269   -3.350566\n             |\n       csize |\n     Medium  |   5.492308    .531599    10.33   0.000     4.447834    6.536782\n      Large  |   11.23563   .5709191    19.68   0.000      10.1139    12.35736\n             |\n     college |\n    College  |   5.828441    .588277     9.91   0.000     4.672607    6.984274\n       _cons |   94.21955   3.948926    23.86   0.000     86.46079    101.9783\n------------------------------------------------------------------------------\n\n\n\n\n\ntest 1.taxes 1.college // &lt;- automatically test the joint hypothesis\n\n\n ( 1)  1.taxes = 0\n ( 2)  1.college = 0\n\n       F(  2,   494) =   66.74\n            Prob &gt; F =    0.0000\n\n\n\n\n\n** \"H0: 2*B_Taxes = B_fines\"\nlincom 2*1.taxes-fines\n\n\n ( 1)  - fines + 2*1.taxes = 0\n\n------------------------------------------------------------------------------\n   citations | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |  -1.297398   1.098122    -1.18   0.238    -3.454964    .8601678\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-non-linear-patterns",
    "href": "rm-data/slides/week07.html#mr-non-linear-patterns",
    "title": "Multiple regression analysis",
    "section": "MR: Non-linear patterns",
    "text": "MR: Non-linear patterns\n\nSurprise! you can use the same tools as with single regression\n\nUses splines, polynomials, other non-linear functions of \\(x\\) variables.\n\nNon-linear function of various \\(x_i\\) variables may be combined.\nAs show before, using non-linear functions will increase multicollinearity, but worry not about that type of collinearity.\nBe more careful with the interpretation of the coefficients."
  },
  {
    "objectID": "rm-data/slides/week07.html#cs-understanding-the-gender-difference-in-earnings",
    "href": "rm-data/slides/week07.html#cs-understanding-the-gender-difference-in-earnings",
    "title": "Multiple regression analysis",
    "section": "CS: Understanding the gender difference in earnings",
    "text": "CS: Understanding the gender difference in earnings\n\nIn the USA (2014), women tend to earn about 20% less than men\nAim 1: Find patterns to better understand the gender gap. Our focus is the interaction with age.\nAim 2: Think about if there is a causal link from being female to getting paid less."
  },
  {
    "objectID": "rm-data/slides/week07.html#cs-the-data",
    "href": "rm-data/slides/week07.html#cs-the-data",
    "title": "Multiple regression analysis",
    "section": "CS: The data",
    "text": "CS: The data\n\n2014 census data\nAge between 15 to 65\nExclude self-employed (earnings is difficult to measure)\nInclude those who reported 20 hours more as their usual weekly time worked\nEmployees with a graduate degree (higher than 4-year college)\nUse log hourly earnings (\\(\\ln w\\)) as dependent variable\nUse gender and add age as explanatory variables"
  },
  {
    "objectID": "rm-data/slides/week07.html#cs-the-model",
    "href": "rm-data/slides/week07.html#cs-the-model",
    "title": "Multiple regression analysis",
    "section": "CS: The model",
    "text": "CS: The model\nWe are quite familiar with the relation between earnings and gender: \\[\\ln w_E = \\alpha + \\beta\\text{female}, \\beta &lt; 0\\] Let’s include age as well: \\[\\ln w_E = \\beta_0 + \\beta_1\\text{female} + \\beta_2\\text{age}\\]\nWhat happens if we do not include age?"
  },
  {
    "objectID": "rm-data/slides/week07.html#cs-the-regression",
    "href": "rm-data/slides/week07.html#cs-the-regression",
    "title": "Multiple regression analysis",
    "section": "CS: The Regression",
    "text": "CS: The Regression\n\n\n\nVariables\nln wage\nln wage\nage\n\n\n\n\nfemale\n-0.195**\n-0.185**\n-1.484**\n\n\n\n(0.008)\n(0.008)\n(0.159)\n\n\nage\n\n0.007**\n\n\n\n\n\n(0.000)\n\n\n\nConstant\n3.514**\n3.198**\n44.630**\n\n\n\n(0.006)\n(0.018)\n(0.116)\n\n\nObservations\n18,241\n18,241\n18,241\n\n\nR-squared\n0.028\n0.046\n0.005\n\n\n\n\nNote: Robust standard errors in parentheses\n*** p&lt;0.01, ** p&lt;0.05, * p&lt;0.1\nSource: cps-earnings dataset. 2014 CPS Morg.\n\nWhat if Age is not linear? (has a non-linear effect on earnings)"
  },
  {
    "objectID": "rm-data/slides/week07.html#cs-adjustment-and-robustness",
    "href": "rm-data/slides/week07.html#cs-adjustment-and-robustness",
    "title": "Multiple regression analysis",
    "section": "CS: Adjustment and Robustness",
    "text": "CS: Adjustment and Robustness\n\n\n\nVariable\nModel 1\nModel 2\nModel 3\nModel 4\n\n\n\n\nfemale\n-0.195**\n-0.185**\n-0.183**\n-0.183**\n\n\n\n(0.008)\n(0.008)\n(0.008)\n(0.008)\n\n\nage\n\n0.007**\n0.063**\n0.572**\n\n\n\n\n(0.000)\n(0.003)\n(0.116)\n\n\nage2\n\n\n-0.001**\n-0.017**\n\n\n\n\n\n(0.000)\n(0.004)\n\n\nage3\n\n\n\n0.000**\n\n\n\n\n\n\n(0.000)\n\n\nObservations\n18,241\n18,241\n18,241\n18,241\n\n\nR-squared\n0.028\n0.046\n0.060\n0.062\n\n\n\n\nNote: Robust standard errors in parentheses, *** p&lt;0.01, ** p&lt;0.05, * p&lt;0.1\nSource: cps-earnings dataset. 2014 CPS Morg."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-qualitative-variables",
    "href": "rm-data/slides/week07.html#mr-qualitative-variables",
    "title": "Multiple regression analysis",
    "section": "MR: Qualitative variables",
    "text": "MR: Qualitative variables\n\nMR can also handle using qualitative variables as explanatory variables.\nTwo ways to include qualitative variables:\n\nCreate a dummy for each category.\nLet the software create the binary variables for you.\n\nYou can only include \\(k-1\\) dummies (dummy variable trap)\n\nLeft out category is the reference category (Base).\n\nStata Corner\n\ni. in front of a variable tells Stata to treat it as a categorical variable. (makes dummies on the background): reg lnwage i.educ\nBut, the categorical variable cannot be negative."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-qualitative-variables-1",
    "href": "rm-data/slides/week07.html#mr-qualitative-variables-1",
    "title": "Multiple regression analysis",
    "section": "MR: Qualitative variables",
    "text": "MR: Qualitative variables\nSay \\(X\\) is a qualitative variable with \\(3\\) categories: low, medium, and high. \\[y^E = \\beta_0 + \\beta_1 D^{med} + \\beta_2 D^{high}\\]\n\nlow is the reference category. Other values compared to this.\n\\(\\beta_0\\) shows average \\(y\\) in the reference category. (medium and high \\(=0\\))\n\\(\\beta_1\\): Average difference between \\(D_{medium}\\) and \\(D_low\\)\n\\(\\beta_2\\): Average difference between \\(D_{high}\\) and \\(D_low\\)"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-how-to-pick-a-reference-category",
    "href": "rm-data/slides/week07.html#mr-how-to-pick-a-reference-category",
    "title": "Multiple regression analysis",
    "section": "MR: How to pick a reference category?",
    "text": "MR: How to pick a reference category?\n\nChoose the category to which we want to compare the rest.\n\nHome country, the capital city, the lowest or highest value group.\n\nOr, chose a category with a large number of observations.\n\nImportant when inference is important, and SE are needed.\n\nFor prediction, it does not matter."
  },
  {
    "objectID": "rm-data/slides/week07.html#cs-gender-difference-in-earnings-and-education",
    "href": "rm-data/slides/week07.html#cs-gender-difference-in-earnings-and-education",
    "title": "Multiple regression analysis",
    "section": "CS: Gender difference in earnings and education",
    "text": "CS: Gender difference in earnings and education\n\n\n\n\n\n\n\n\n\nVariables\nln wage\nln wage\nln wage\n\n\n\n\nfemale\n-0.195**\n-0.182**\n-0.182**\n\n\n\n(0.008)\n(0.009)\n(0.009)\n\n\ned_Profess\n\n0.134**\n-0.002\n\n\n\n\n(0.015)\n(0.018)\n\n\ned_PhD\n\n0.136**\n\n\n\n\n\n(0.013)\n\n\n\ned_MA\n\n\n-0.136**\n\n\n\n\n\n(0.013)\n\n\nConstant\n3.514**\n3.473**\n3.609**\n\n\n\n(0.006)\n(0.007)\n(0.013)\n\n\nObservations\n18,241\n18,241\n18,241\n\n\nR-squared\n0.028\n0.038\n0.038"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-interactions",
    "href": "rm-data/slides/week07.html#mr-interactions",
    "title": "Multiple regression analysis",
    "section": "MR: Interactions",
    "text": "MR: Interactions\n\nOften data is made up of important groups: male and female workers or countries in different continents.\nand, Some of the patterns we are after may vary across these groups.\nThe strength of a relation may also be altered by a special variable.\n\nIn medicine, a moderator variable can reduce or amplify the effect of a drug on people.\nIn business, financial strength can affect how firms may weather a recession.\n\nMessage: different patterns for subsets of observations."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-interactions-and-parallel-lines",
    "href": "rm-data/slides/week07.html#mr-interactions-and-parallel-lines",
    "title": "Multiple regression analysis",
    "section": "MR: Interactions and parallel lines",
    "text": "MR: Interactions and parallel lines\n\nOption 1: Simply the Dummy to the model\n\n\\(y^E = \\beta_0 + \\beta_1 x_1 + \\beta_2 D\\)\n\nThis assumes \\(\\beta_1\\) is the same for both groups, only the intercepts are different.\nOption 2: Different slopes\n\n\\(y^E = \\beta_0 + \\beta_1 x_1 + \\beta_2 D + \\beta_3 x_1 \\times D\\)\n\n\nThis now assumes slopes are different for both groups.\nOption 3: Separate regressions\n\nBut Option 2 is better for testing if slopes are different."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-interaction-with-two-continuous-variable",
    "href": "rm-data/slides/week07.html#mr-interaction-with-two-continuous-variable",
    "title": "Multiple regression analysis",
    "section": "MR: Interaction with two continuous variable",
    "text": "MR: Interaction with two continuous variable\n\nInteractions can also be used with continuous variables, \\(x_1\\) and \\(x_2\\): \\[y_E = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2\\]\nExample:\n\n\\(y\\) is change in revenue \\(x_1\\) is change in global demand, \\(x_2\\) is firm’s financial health\nThe interaction can capture that drop in demand can cause financial problems in firms, but less so for firms with better balance sheet.\n\nPerhaps biggest challenge is to interpret a model with interactions."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-interaction-with-two-continuous-variable-1",
    "href": "rm-data/slides/week07.html#mr-interaction-with-two-continuous-variable-1",
    "title": "Multiple regression analysis",
    "section": "MR: Interaction with two continuous variable",
    "text": "MR: Interaction with two continuous variable\n\nThe interaction term \\(x_1 x_2\\) captures how this two variables affect each others effect on \\(y\\).\n\nTypically we assume this is zero.\n\nThe coefficient \\(\\beta_3\\) captures the magnitude of the interaction effect.\nHowever, if you are interested in the relationship betwee \\(x_1\\) or \\(x_2\\) with \\(y\\), you need extra care.\n\n\\(x_1\\) on \\(y\\) is \\(\\frac{dy}{dx_1}=\\beta_1 + \\beta_3 x_2\\).\n\\(x_2\\) on \\(y\\) is \\(\\frac{dy}{dx_2}=\\beta_2 + \\beta_3 x_1\\).\n\nSo you need to “fix” \\(x_2\\) to see the effect of \\(x_1\\) on \\(y\\). (typically at the mean)"
  },
  {
    "objectID": "rm-data/slides/week07.html#cs-interaction-between-gender-and-age",
    "href": "rm-data/slides/week07.html#cs-interaction-between-gender-and-age",
    "title": "Multiple regression analysis",
    "section": "CS: Interaction between gender and age",
    "text": "CS: Interaction between gender and age\n\n\n\nSeparate, Earning for men rises faster with age\nWith interaction, Same result!.\nFemale dummy is close to zero. Does this mean no gender gap?\n\nNo, Cumulative effect: \\(-0.036-0.003*age\\)\n\n\n\n\n\n\nVariables\nln wage (Women)\nln wage (Men)\nln wage (All)\n\n\n\n\nfemale\n\n\n-0.036\n\n\n\n\n\n(0.035)\n\n\nage\n0.006**\n0.009**\n0.009**\n\n\n\n(0.001)\n(0.001)\n(0.001)\n\n\nfemale × age\n\n\n-0.003**\n\n\n\n\n\n(0.001)\n\n\nConstant\n3.081**\n3.117**\n3.117**\n\n\n\n(0.023)\n(0.026)\n(0.026)\n\n\nObservations\n9,685\n8,556\n18,241\n\n\nR-squared\n0.011\n0.028\n0.047"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-stata-corner",
    "href": "rm-data/slides/week07.html#mr-stata-corner",
    "title": "Multiple regression analysis",
    "section": "MR: Stata corner",
    "text": "MR: Stata corner\n\nIn Stata you can use i. to create dummies for all categories of a variable.\nYou can also use # to create interactions between variables.\n\nUnless specified, Stata assume variables are categorical. You can use c. for continuous variables.\n\nYou can also use ## to create interactions plus the main effects.\n\nregress y i.x1##c.x2\nis equivalent to\nregress y i.x1 c.x2 i.x1#c.x2\nwhere i.x1 will create all dummies for x1\n\nIf dummies and interactions are created this way you can use margins to calculate effects of main variables.\n\nmargins, dydx(x1 x2)"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-causal-analysis",
    "href": "rm-data/slides/week07.html#mr-causal-analysis",
    "title": "Multiple regression analysis",
    "section": "MR: Causal analysis",
    "text": "MR: Causal analysis\n\nOne main reason to estimate multiple regressions is to get closer to a causal interpretation.\nBy conditioning on other observable variables, we can get closer to comparing similar objects – “apples to apples” – even in observational data.\nBut getting closer is not the same as getting there.\nIn principle, one could try conditioning on every potential confounder: variables that would affect \\(y\\) and the causal variable \\(x_1\\) at the same time.\nCeteris paribus = conditioning on every such relevant variable. (everything else constant)."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-causal-analysis-1",
    "href": "rm-data/slides/week07.html#mr-causal-analysis-1",
    "title": "Multiple regression analysis",
    "section": "MR: Causal analysis",
    "text": "MR: Causal analysis\n\nIn randomized experiments, we can use causal language: treated and untreated units similar - by random grouping.\nIn observational data, comparisons don’t uncover causal relations.\n\nCautious with language. Avoid use of “effect”, “increase”. But could use “associated with”, “linked to”.\nRegression, even with multiple \\(x\\) is just comparison. Conditional mean."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-causal-analysis-dont-overdo-it",
    "href": "rm-data/slides/week07.html#mr-causal-analysis-dont-overdo-it",
    "title": "Multiple regression analysis",
    "section": "MR: Causal analysis Don’t overdo it",
    "text": "MR: Causal analysis Don’t overdo it\n\nNot all variables should be included as control variables even if correlated both with the causal variable and the dependent variable.\nBad conditioning variables are variables that are correlated both with the causal variable and the dependent variable but are actually part of the causal mechanism.\n\nThis is the reason to exclude them.\n\nExample, should you control for visit to the doctor when estimating the effect of health spending on health?\n\nNo, because visiting the doctor is part of the causal mechanism."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-causal-analysis-2",
    "href": "rm-data/slides/week07.html#mr-causal-analysis-2",
    "title": "Multiple regression analysis",
    "section": "MR: Causal analysis",
    "text": "MR: Causal analysis\n\nA multiple regression on observational data is rarely capable of uncovering a causal relationship.\n\nCannot capture all potential confounder. (Not ceteris paribus)\nPotential Bad conditioning variables (bad controls)\nWe can never really know.\n\nMultiple regression can get us closer to uncovering a causal relationship\n\nCompare units that are the same in many respects - controls"
  },
  {
    "objectID": "rm-data/slides/week07.html#cs-understanding-the-gender-difference-in-earnings-1",
    "href": "rm-data/slides/week07.html#cs-understanding-the-gender-difference-in-earnings-1",
    "title": "Multiple regression analysis",
    "section": "CS: Understanding the gender difference in earnings",
    "text": "CS: Understanding the gender difference in earnings\n\n\n\n\n\n\n\n\n\n\nVariables\nln wage (Model 1)\nln wage (Model 2)\nln wage (Model 3)\nln wage (Model 4)\n\n\n\n\nfemale\n-0.224**\n-0.212**\n-0.151**\n-0.141**\n\n\n\n(0.012)\n(0.012)\n(0.012)\n(0.012)\n\n\nAge and education\n\nYES\nYES\nYES\n\n\nFamily circumstances\n\n\nYES\nYES\n\n\nDemographic background\n\n\n\nYES\n\n\nJob characteristics\n\n\n\nYES\n\n\nUnion member\n\n\n\nYES\n\n\nAge in polynomial\n\n\n\nYES\n\n\nHours in polynomial\n\n\n\nYES\n\n\nObservations\n9,816\n9,816\n9,816\n9,816\n\n\nR-squared\n0.036\n0.043\n0.182\n0.195\n\n\n\nMore and more confounders added"
  },
  {
    "objectID": "rm-data/slides/week07.html#regression-table-detour",
    "href": "rm-data/slides/week07.html#regression-table-detour",
    "title": "Multiple regression analysis",
    "section": "Regression table detour",
    "text": "Regression table detour\n\nRegression table with many \\(x\\) vars is hard to present\nIn presentation, suppress unimportant coefficients\nIn paper, you may present more, but mostly if you want to discuss them or for sanity check\nSanity check: do control variable coefficient make sense by and large?\nCheck \\(N\\) of observations: if the same sample, should be exactly the same.\n\\(R^2\\) is enough, no need for other stuff (unless other methods are used)"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-prediction-and-benchmarking",
    "href": "rm-data/slides/week07.html#mr-prediction-and-benchmarking",
    "title": "Multiple regression analysis",
    "section": "MR: Prediction and benchmarking",
    "text": "MR: Prediction and benchmarking\n\nSecond reason to estimate a multiple regression is to make a prediction\n\nfind the best guess for the dependent variable \\(y_j\\) for a particular observation \\(j\\) \\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\cdots\\]\n\nA \\(\\hat{y} \\ vs \\ y\\) Scatter plot is a good way to visualize the fit of a prediction.\n\nAs well as identifying over or underpredictions.\n\nWe want the regression to produce as good a fit as possible.\n\nA common danger: Overfitting the data: finding patterns in the data that are not true in the population\n\nWe will discuss more about prediction in the next chapters."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-variable-selection",
    "href": "rm-data/slides/week07.html#mr-variable-selection",
    "title": "Multiple regression analysis",
    "section": "MR: Variable selection",
    "text": "MR: Variable selection\n\nHow should one decide which variables to include? and how?\nDepends on the purpose: prediction or causality.\n\nGeneral Advice:\n\nLot of judgment calls: theory, data, and context.\nNon-linear fit, use a non-parametric first and if non-linear, pick a model that is close - quadratic, piecewise spline.\nIf two or many variables strongly correlated, pick one of them.\nKeep it as simple as possible: Parsimony is a virtue."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-variable-selection-for-causal-questions",
    "href": "rm-data/slides/week07.html#mr-variable-selection-for-causal-questions",
    "title": "Multiple regression analysis",
    "section": "MR: Variable selection for causal questions",
    "text": "MR: Variable selection for causal questions\n\nCausal question: \\(x\\) impact on \\(y\\). Having \\(z\\) variables to condition on, to get closer to causality.\nOur aim is to focus on the coefficient on one variable. What matters are the estimated value of the coefficient and its confidence interval. Not prediction.\nKeep \\(z\\) – keep variables that help comparing units\nDrop \\(z\\) if they not matter, or if they are part of the causal mechanism. (affected by \\(x\\))\n\nFunctional form for \\(z\\) matters only for crucial confounders. (linear is fine)\n\nPresent the model you judge is best, and then report a few other solutions – robustness."
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-variable-selection-process",
    "href": "rm-data/slides/week07.html#mr-variable-selection-process",
    "title": "Multiple regression analysis",
    "section": "MR: Variable selection – process",
    "text": "MR: Variable selection – process\n\nSelect control variables you want to include\nSelect functional form one by one\nFocus on key variables by domain knowledge (theory), add the rest linearly\nKey issue is sample size\n\nFor 20-40 obs, about 1-2 variables.\nFor 50-100 obs, about 2-4 variables\nFew hundred obs, 5-10 variables could work\nFew thousand obs, few dozen variables, including industry/country/profession etc dummmmies, interactions.\n10-100K obs - many variables, polynomials, interactions"
  },
  {
    "objectID": "rm-data/slides/week07.html#mr-variable-selection-for-prediction",
    "href": "rm-data/slides/week07.html#mr-variable-selection-for-prediction",
    "title": "Multiple regression analysis",
    "section": "MR: Variable selection for prediction",
    "text": "MR: Variable selection for prediction\n\nIf Prediction is the goal, keep whatever works\nBalance is needed to ensure it works beyond the data at hand\nOverfitting: building a model that captures some patterns that may fit the data in hand, but does not generalize well.\nFocus on functional form, interactions\nValue simplicity. Easier to explain, more robust.\nFormal way:\n\nBIC and AIC. Similar to R-squared but takes into account number of variables. The smaller, the better\n\nYou may use Adj R2, (although not perfect) to compare models."
  },
  {
    "objectID": "rm-data/slides/week07.html#summary-take-away",
    "href": "rm-data/slides/week07.html#summary-take-away",
    "title": "Multiple regression analysis",
    "section": "Summary take-away",
    "text": "Summary take-away\n\nMultiple regression are linear models with several \\(x\\) variables.\nMay include binary variables and interactions\nMultiple regression can take us closer to a causal interpretation and help make better predictions."
  },
  {
    "objectID": "rm-data/slides/week04.html#generalization",
    "href": "rm-data/slides/week04.html#generalization",
    "title": "Generalizing from Data",
    "section": "Generalization",
    "text": "Generalization\n\nSometimes we analyze a dataset with the goal of learning about patterns in that dataset alone.\n\nIn such cases there is no need to generalize our findings to other datasets.\nExample: We search for a good deal among offers of hotels, all we care about are the observations in our dataset.\n\nOften we analyze a dataset in order to learn about patterns that may be true in other situations.\n\nWe are interested in finding the relationship between our dataset and the situation we care about.\nExample: Will the treatment we are studying work in other settings?"
  },
  {
    "objectID": "rm-data/slides/week04.html#generalization-1",
    "href": "rm-data/slides/week04.html#generalization-1",
    "title": "Generalizing from Data",
    "section": "Generalization",
    "text": "Generalization\n\nGoal: Generalize the results from a single dataset to other situations.\nThe act of generalization is called inference: we infer something from our data about a more general sitatuation.\n\nFor this we want to test hypothesis based on our estimates (evidence)\n\nTwo Things to consider\n\nStatistical inference: the process of using data (in hand) to infer the properties of a population. Identify general pattern.\nExternal validity: the extent to which our data represents the general pattern we care about in other settings."
  },
  {
    "objectID": "rm-data/slides/week04.html#statistical-inference",
    "href": "rm-data/slides/week04.html#statistical-inference",
    "title": "Generalizing from Data",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nThere are several statistical methods to make inference.\nThe general pattern (A model) is an abstract thing that may or may not exist.\nIf we can assume that the general pattern exists, the tools of statistical inference can be very helpful.\n\nIf we find a positive relationship between two variables in our data, we can use statistical inference to say if the same relationship is likely in the population."
  },
  {
    "objectID": "rm-data/slides/week04.html#general-patterns-1-population-and-representative-sample",
    "href": "rm-data/slides/week04.html#general-patterns-1-population-and-representative-sample",
    "title": "Generalizing from Data",
    "section": "General patterns 1: Population and representative sample",
    "text": "General patterns 1: Population and representative sample\n\nThe cleanest example of representative data is a representative sample of a well-defined population.\nA sample is representative of a population if the distribution of all variables is very similar in the sample and the population. \\[f(y,x,z,...)_{sample} \\approx f(y,x,z,...)_{population}\\]\nRandom sampling is the best way to achieve a representative sample."
  },
  {
    "objectID": "rm-data/slides/week04.html#general-patterns-2-no-population-but-general-pattern",
    "href": "rm-data/slides/week04.html#general-patterns-2-no-population-but-general-pattern",
    "title": "Generalizing from Data",
    "section": "General patterns 2: No population but general pattern",
    "text": "General patterns 2: No population but general pattern\n\n“Representation” is less straightforward in other setups.\nThere isn’t a “population” from which a random sample was drawn on purpose.\n\nUsing the past to uncover a pattern of the future. (Time series)\nUse analogy to generalize patterns on Products A into Products B. (requires external Validity)\n\nInstead, we should think of our data as one that represents a general pattern (a model).\n\n\\(X\\beta\\) exists, and each year is a random realization.\n\\(X\\beta\\) exists, and each product is a random version.\n\nYou can think of a “general pattern” as the “true” model dictating the data."
  },
  {
    "objectID": "rm-data/slides/week04.html#external-validity",
    "href": "rm-data/slides/week04.html#external-validity",
    "title": "Generalizing from Data",
    "section": "External validity",
    "text": "External validity\n\nHow likely is that what we learn is relevant other situations we care about?\nAre our findings unique to our data? or can they happen “out there”?\n\nWith external validity, our data can tell what to expect.\nNo external validity: whatever we learn from our data, may turn out to be not relevant at all.\n\nThis has been a problem with RCTs in economics: the results are not always generalizable to other settings."
  },
  {
    "objectID": "rm-data/slides/week04.html#the-process-of-inference",
    "href": "rm-data/slides/week04.html#the-process-of-inference",
    "title": "Generalizing from Data",
    "section": "The process of inference",
    "text": "The process of inference\nThe process of inference:\n\nConsider a statistic we may care about, such as the mean.\nCompute its estimated value from a dataset.\nInfer the value in the population, that our data represents.\n\nIt is good practice to divide the inference problem into two:\n\nUse statistical inference to learn about the population the data represents.\nAssess external validity: Assess how the data in hand represents the population we care about."
  },
  {
    "objectID": "rm-data/slides/week04.html#stock-market-returns-inference",
    "href": "rm-data/slides/week04.html#stock-market-returns-inference",
    "title": "Generalizing from Data",
    "section": "Stock market returns: Inference",
    "text": "Stock market returns: Inference\n\nTask: Assess the likelihood of experiencing a loss of 5% on an investment portfolio from one day to the next\nData: day-to-day returns on the S&P 500, from 25 August 2006 to 26 August 2016: 2,519 days.\nFinding: 0.5% of the days in the dataset have a loss of 5% or more.\nInference problem:\n\nHow can we generalize this finding? What can we infer from this 0.5% chance for the next calendar year?"
  },
  {
    "objectID": "rm-data/slides/week04.html#repeated-samples-1",
    "href": "rm-data/slides/week04.html#repeated-samples-1",
    "title": "Generalizing from Data",
    "section": "Repeated samples",
    "text": "Repeated samples\n\nNormally, There is one sample. But, theoretical framework assumes you could obtain many (repeated) samples. (Frequentist approach)\nThe goal of statistical inference is learning the value of a statistic in the population represented by our data.\n\nBut, each repeated samples, would give a different value of the statistic.\n\nBecause of the different values, the statistic obtained with repeated samples will have a distribution\n\nThis is the sampling distribution.\n\nThe standard deviation of the sampling distribution is what is called the standard error of the statistic (typical error across random samples)."
  },
  {
    "objectID": "rm-data/slides/week04.html#repeated-samples-properties",
    "href": "rm-data/slides/week04.html#repeated-samples-properties",
    "title": "Generalizing from Data",
    "section": "Repeated samples properties",
    "text": "Repeated samples properties\nThe sampling distribution has three important properties:\n\nUnbiasedness: The average of the values in repeated samples is equal to its true value (=the value in the entire population / general pattern).\nAsymptotic normality: The sampling distribution is approximately normal. With large sample size, it is very very close.\nRoot-n convergence: The standard error (the standard deviation of the sampling distribution) is smaller the larger the samples, with a proportionality factor of the square root of the sample size."
  },
  {
    "objectID": "rm-data/slides/week04.html#repeated-samples-2",
    "href": "rm-data/slides/week04.html#repeated-samples-2",
    "title": "Generalizing from Data",
    "section": "Repeated samples",
    "text": "Repeated samples\n\nEasier concept:\n\nWhen data is sample from a well-defined population - many other samples could have turned out instead of what we have.\nExample: Mexican firms - random sample - population of firms.\n\nHarder concept:\n\nSome times there is no clear definition of population. (but there is a model).\nData of returns on an investment portfolio is as a particular realization of the history of returns that could have turned out differently.\n\nMultiverse: many possible histories of returns, we see only one."
  },
  {
    "objectID": "rm-data/slides/week04.html#case-study",
    "href": "rm-data/slides/week04.html#case-study",
    "title": "Generalizing from Data",
    "section": "Case study",
    "text": "Case study\nStock market returns: A simulation\n\nWe can not rerun history many many times…\nSo we will run a Simulation exercise - to better understand how repeated samples work.\nSuppose the 11-year dataset is the population - the fraction of days with 5%+ losses is 0.5% in the entire 11 years’ data. That’s the true value.\nWe assume we have only 500 days of daily returns in our dataset.\nTask: estimate the true value of the fraction in the 11-year period from the data we have using a simulation exercise."
  },
  {
    "objectID": "rm-data/slides/week04.html#stock-market-returns-a-simulation-1",
    "href": "rm-data/slides/week04.html#stock-market-returns-a-simulation-1",
    "title": "Generalizing from Data",
    "section": "Stock market returns: A simulation",
    "text": "Stock market returns: A simulation\n\nuse data_slides/sp500.dta, clear\ngen return = (value - value[_n-1])/value[_n-1]\ngen lost5 = return &lt; -0.05\nset seed 1\n\n** Simulation\ngen mn_lost5=.\nforvalues i = 1/1000 {\n    preserve\n      qui:sample 500, count\n      sum lost5 , meanonly\n    restore\n    qui:replace mn_lost5 = r(mean) in `i'\n}"
  },
  {
    "objectID": "rm-data/slides/week04.html#stock-market-returns-a-simulation-2",
    "href": "rm-data/slides/week04.html#stock-market-returns-a-simulation-2",
    "title": "Generalizing from Data",
    "section": "Stock market returns: A simulation",
    "text": "Stock market returns: A simulation\n\n\n(start=0, width=.002)"
  },
  {
    "objectID": "rm-data/slides/week04.html#the-standard-error-and-the-confidence-interval",
    "href": "rm-data/slides/week04.html#the-standard-error-and-the-confidence-interval",
    "title": "Generalizing from Data",
    "section": "The standard error and the confidence interval",
    "text": "The standard error and the confidence interval\n\nConfidence interval (CI) is a measure of statistical inference that allows some margin of error.\nThe CI defines a range where we can expect the true value in the population, with a probability.\nProbability tells how likely it is that the true value is in that range, if we were to draw many repeated samples."
  },
  {
    "objectID": "rm-data/slides/week04.html#if-ex-and-sex-are-known",
    "href": "rm-data/slides/week04.html#if-ex-and-sex-are-known",
    "title": "Generalizing from Data",
    "section": "If E(X) and SE(X) are known",
    "text": "If E(X) and SE(X) are known\n\nIf we know the true value of a statistic and its standard error, then we can calculate the CI.\n\nThe CI is centered around the true value of the statistic.\nThis CI is the range of values that we can expect the sample statistic to fall in, with a certain probability.\nfor example, 95% CI: the sample statistic will fall within the CI in 95% of the repeated samples.\nBut in 5% of the cases, the sample statistic will fall outside the CI."
  },
  {
    "objectID": "rm-data/slides/week04.html#if-ex-and-sex-are-not-known",
    "href": "rm-data/slides/week04.html#if-ex-and-sex-are-not-known",
    "title": "Generalizing from Data",
    "section": "If E(X) and SE(X) are not known",
    "text": "If E(X) and SE(X) are not known\n\nWhen we say “95% CI”, we mean that if we were to draw many repeated samples, the true value would fall within the CI in 95% of the cases.\nHowever, it also means that in 5% of the cases, the true value would fall outside the CI.\n\nThis means, some times (5% of the cases) we will be wrong."
  },
  {
    "objectID": "rm-data/slides/week04.html#confidence-interval",
    "href": "rm-data/slides/week04.html#confidence-interval",
    "title": "Generalizing from Data",
    "section": "Confidence interval",
    "text": "Confidence interval\n\nCI is almost always symmetric around the estimated value of the statistic in our dataset. (if we assume normality of the sampling distribution)\nHow to calculate the CI?\n\nGet estimated value.\nDefine probability, confidence level (Say 95%).\nCalculate CI with the use of SE. \\[95\\% CI= \\hat\\mu \\pm 1.96SE\\]\n\nUnder Normality, 90% CI is the ±1.645SE interval, the 99 % CI is the ±2.576SE.\nBut we commonly use the rule of 2: ±2SE."
  },
  {
    "objectID": "rm-data/slides/week04.html#calculating-the-standard-error",
    "href": "rm-data/slides/week04.html#calculating-the-standard-error",
    "title": "Generalizing from Data",
    "section": "Calculating the standard error",
    "text": "Calculating the standard error\n\nEstimating the sample mean \\(\\bar{x}\\) is easy. But how do we estimate the standard error?\n\nIn reality, we don’t get to observe the sampling distribution. Instead, we observe a single dataset.\nThat dataset is one of many potential samples that could have been drawn from the population.\n\nGood news: We can get a very good idea of how the sampling distribution would look like - good estimate of the standard error - even from a single sample.\nGetting SE – Option 1: Use a formula. \\(\\leftarrow\\) Theoretical approach.\nGetting SE – Option 2: Simulate \\(\\leftarrow\\), The bootstrap method."
  },
  {
    "objectID": "rm-data/slides/week04.html#calculating-the-standard-error-1",
    "href": "rm-data/slides/week04.html#calculating-the-standard-error-1",
    "title": "Generalizing from Data",
    "section": "Calculating the standard error",
    "text": "Calculating the standard error\nConsider the statistic of the sample mean.\n\nAssume the values of \\(x\\) are independent across observations in the dataset.\n\\(\\bar{x}\\) is the estimate of the true mean value of \\(x\\) in the population.\nAssume sampling distribution is approximately normal, with the true value as its mean.\n\nThe standard error formula for the estimated \\(\\bar{x}\\) is \\[SE (\\bar{x}) = \\frac{1}{\\sqrt{n}} Std[x]\\]\nwhere \\(Std[x]\\) is the standard deviation of the variable \\(x\\) in the data and \\(n\\) is the number of observations in the data."
  },
  {
    "objectID": "rm-data/slides/week04.html#the-standard-error-formula",
    "href": "rm-data/slides/week04.html#the-standard-error-formula",
    "title": "Generalizing from Data",
    "section": "The standard error formula",
    "text": "The standard error formula\n\nThe standard error is larger…\n\nthe larger the standard deviation of the variable.\nthe smaller the sample\n\nThe larger the standard error, the wider the confidence interval, and the less precise the estimate (wider CI)."
  },
  {
    "objectID": "rm-data/slides/week04.html#external-validity-2",
    "href": "rm-data/slides/week04.html#external-validity-2",
    "title": "Generalizing from Data",
    "section": "External validity",
    "text": "External validity\n\nIn statistical inference the CI represents the uncertainty about the true value of the statistic in the population that our data represents.\nBut What is the population, we care about? How close is our data to this?\nExternal validity: Can we generalize the pattern we found in our data to other situations?\nHigh external validity: if our data is close to the population.\nExternal validity is as important as statistical inference, but it is not a statistical question."
  },
  {
    "objectID": "rm-data/slides/week04.html#external-validity-3",
    "href": "rm-data/slides/week04.html#external-validity-3",
    "title": "Generalizing from Data",
    "section": "External validity",
    "text": "External validity\n\nThe three most important challenges to external validity are:\n\nTime: we have data on the past, but we care about the future.\nSpace: our data is on one country, but interested how a pattern would hold elsewhere in the world.\nSub-groups: our data is on 25-30 year old people. Would a pattern hold on younger / older people?"
  },
  {
    "objectID": "rm-data/slides/week04.html#external-validity-portafolio-example",
    "href": "rm-data/slides/week04.html#external-validity-portafolio-example",
    "title": "Generalizing from Data",
    "section": "External validity: Portafolio Example",
    "text": "External validity: Portafolio Example\n\nDaily 5%+ loss probability with a 95% CI [0.2, 0.8] in our sample. This captures uncertainty.\nExternal Validity: Would this data be representative of the events of one year in the future?\n\nProbably not, because the future is uncertain.\nOur data: 2006-2016 dataset includes the financial crisis and great recession of 2008-2009. uncertain if the future will have similar events.\n\nHence, the real CI is likely to be substantially wider."
  },
  {
    "objectID": "rm-data/slides/week04.html#external-validity-managers-example",
    "href": "rm-data/slides/week04.html#external-validity-managers-example",
    "title": "Generalizing from Data",
    "section": "External validity: Managers Example",
    "text": "External validity: Managers Example\n\nManager and firm size evidence in Mexico.\nHow to think about external validity?\n\nWould the same patterns hold in other countries? Develped countries? Emerging markets?\nWould the same patterns hold in other sectors? Other industries?\n\nOnly Mexico? only firms of a certain size?"
  },
  {
    "objectID": "rm-data/slides/week04.html#the-bootstrap-1",
    "href": "rm-data/slides/week04.html#the-bootstrap-1",
    "title": "Generalizing from Data",
    "section": "The bootstrap",
    "text": "The bootstrap\n\nBootstrap is a method to create synthetic samples that are similar but different.\nAn method that is very useful in general.\n\nThe method you use, when you don’t know…\n\nIt is essential for many advanced statistics applications such as machine learning.\nThe bootstrap is a method to estimate uncertainty in a statistic, that uses the data itself.\n\n\nto lift oneself by one’s bootstraps"
  },
  {
    "objectID": "rm-data/slides/week04.html#the-bootstrap-2",
    "href": "rm-data/slides/week04.html#the-bootstrap-2",
    "title": "Generalizing from Data",
    "section": "The bootstrap",
    "text": "The bootstrap\n\nThe bootstrap method takes the original dataset and draws many repeated samples (with replacement) of the size of that dataset.\nSay you have a dataset of 10 observations, named 1, 2, 3, …, 10.\n\nBootstrap sample 1: 2, 5, 5, 7, 8, 9, 9, 9, 10, 10.\nBootstrap sample 2: 1, 1, 2, 3, 4, 5, 6, 7, 8 , 9.\nAnd so on, repeated many times.\n\nEach new sample is called a bootstrap sample."
  },
  {
    "objectID": "rm-data/slides/week04.html#the-bootstrap-3",
    "href": "rm-data/slides/week04.html#the-bootstrap-3",
    "title": "Generalizing from Data",
    "section": "The bootstrap",
    "text": "The bootstrap\n\n\n\na Bsample is (almost) always the same size as the original dataset.\nSome Data is repeated, some is left out.\nTypically, we require between 500-10,000 bootstrap samples. (Stata’s default is 50)\nComputationally intensive, but feasible"
  },
  {
    "objectID": "rm-data/slides/week04.html#the-bootstrap-method-how-does-it-work",
    "href": "rm-data/slides/week04.html#the-bootstrap-method-how-does-it-work",
    "title": "Generalizing from Data",
    "section": "The bootstrap method: How does it work?",
    "text": "The bootstrap method: How does it work?\n\nFor each BSample, you estimate the statistic of interest. (e.g. mean)\nThe distribution of the statistic across these repeated bootstrap samples is a good approximation to the sampling distribution.\nIn this case, the bootstrap Standard Error is the standard deviation of the statistic across the bootstrap samples.\nAlso, the 95% CI is the 2.5th and 97.5th percentiles of the distribution of the statistic across the bootstrap samples. Or you can use the estimated SE."
  },
  {
    "objectID": "rm-data/slides/week04.html#stock-market-returns-the-bootstrap-standard-error",
    "href": "rm-data/slides/week04.html#stock-market-returns-the-bootstrap-standard-error",
    "title": "Generalizing from Data",
    "section": "Stock market returns: The Bootstrap standard error",
    "text": "Stock market returns: The Bootstrap standard error\nStata Corner\nBootstraping in Stata can be easy. Most commands have a built-in bootstrap option. Otherwise, we can program it!\n\ndisplay \"Bootstrap\"\nbootstrap mean=r(mean), nowarn reps(1000) seed(1) dots(100): sum lost5, meanonly\nest sto m1\ndisplay \"Formula\"\nmean lost5\nest sto m2"
  },
  {
    "objectID": "rm-data/slides/week04.html#stock-market-returns-the-bootstrap-standard-error-1",
    "href": "rm-data/slides/week04.html#stock-market-returns-the-bootstrap-standard-error-1",
    "title": "Generalizing from Data",
    "section": "Stock market returns: The Bootstrap standard error",
    "text": "Stock market returns: The Bootstrap standard error\nStata Corner\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean\n\n\n\n\n\nFormula\n0.0052\n[0.0023,0.0080]\n\n\n\n\nBootstrap\n\n\n0.0052\n[0.0024,0.0080]\n\n\nN\n2519\n\n2519"
  },
  {
    "objectID": "rm-data/slides/week04.html#generalization---summary",
    "href": "rm-data/slides/week04.html#generalization---summary",
    "title": "Generalizing from Data",
    "section": "Generalization - Summary",
    "text": "Generalization - Summary\n\nGeneralization is a key task - finding beyond the actual dataset.\nThis process is made up of discussing statistical inference and external validity.\nStatistical inference generalizes from our dataset to the population using a variety of statistical tools.\nExternal validity is the concept of discussing beyond the population for a general pattern we care about; an important but typically somewhat speculative process."
  },
  {
    "objectID": "rm-data/slides/week04.html#motivation",
    "href": "rm-data/slides/week04.html#motivation",
    "title": "Generalizing from Data",
    "section": "Motivation",
    "text": "Motivation\n\nThe internet allowed the emergence of specialized online retailers while brick-and-mortar shops also sell goods on the main street. How to measure price inflation in the age of these options?\nTo help answer this, we can collect and compare online and offline prices of the same products and test if they are the same."
  },
  {
    "objectID": "rm-data/slides/week04.html#the-logic-of-hypothesis-testing",
    "href": "rm-data/slides/week04.html#the-logic-of-hypothesis-testing",
    "title": "Generalizing from Data",
    "section": "The logic of hypothesis testing",
    "text": "The logic of hypothesis testing\n\nA hypothesis is a statement about the population parameter, of which we are not sure if true or not.\nHypothesis testing = analyze our data to make a decision on the hypothesis\nReject the hypothesis if there is enough evidence against it.\nDon’t reject it if there isn’t enough evidence against it.\n\nBut NEVER accept it as true.\n\nImportant asymmetry here: rejecting a hypothesis is a more conclusive decision than not rejecting it."
  },
  {
    "objectID": "rm-data/slides/week04.html#inference",
    "href": "rm-data/slides/week04.html#inference",
    "title": "Generalizing from Data",
    "section": "Inference",
    "text": "Inference\n\nTesting a hypothesis: making inference with a focus on a specific statement.\n\nHypothesis: It is cheaper to buy online than offline.\n\nCan answer questions about the population represented by our data.\nBut, It is an inference: have to assess external validity."
  },
  {
    "objectID": "rm-data/slides/week04.html#the-setup",
    "href": "rm-data/slides/week04.html#the-setup",
    "title": "Generalizing from Data",
    "section": "The setup",
    "text": "The setup\n\nDefine the the statistic we want to test, \\(s\\) (e.g. mean).\nWe are interested in the true value of \\(s\\), \\(s_{true}\\).\n\nThis implies the true value in the population.\n\nThe value of the statistic in our data is its estimated value, denoted by a hat on top \\(\\hat{s}\\)."
  },
  {
    "objectID": "rm-data/slides/week04.html#hypothesis-testing-h0-vs-ha",
    "href": "rm-data/slides/week04.html#hypothesis-testing-h0-vs-ha",
    "title": "Generalizing from Data",
    "section": "Hypothesis testing: H0 vs HA",
    "text": "Hypothesis testing: H0 vs HA\n\nNeed to formally state the question as two competing hypotheses of which only one can be true:\n\na null hypothesis \\(H_0\\) and an alternative hypothesis \\(H_a\\).\n\nThey are formulated in terms of the unknown true value of the statistic. (we now the sample value)\nTogether they cover all possibilities.\n\n\n\\(H_0\\): Online and offline prices are the same. \\(H_a\\): Online and offline prices are different."
  },
  {
    "objectID": "rm-data/slides/week04.html#the-null-is-protected",
    "href": "rm-data/slides/week04.html#the-null-is-protected",
    "title": "Generalizing from Data",
    "section": "The Null is protected",
    "text": "The Null is protected\n\nInnocent (H0) until proven guilty (Ha)\n\n\nTesting a hypothesis \\(H_0\\)= see if there is enough evidence in our data to reject the null.\nThe null is protected: We start assuming the Null is true\n\nIf we have strong evidence against it, we reject it\nIf not, we don’t reject it."
  },
  {
    "objectID": "rm-data/slides/week04.html#types-of-testing-h_0-vs-h_a",
    "href": "rm-data/slides/week04.html#types-of-testing-h_0-vs-h_a",
    "title": "Generalizing from Data",
    "section": "Types of testing: \\(H_0\\) vs \\(H_a\\)",
    "text": "Types of testing: \\(H_0\\) vs \\(H_a\\)\n\nThere are two types of Hypothesis:\n\nTwo-sided alternative: We are interested if the true value of the statistic is different from the hypothesized value.\n\n\n\\[H_0: \\theta = 42 \\ \\ vs  \\ \\ H_A: \\theta \\neq 42\\]\n\nOne-sided alternative: We are interested if the true value of the statistic is greater or smaller than the hypothesized value. \\[H_0: \\theta \\leq 42 \\ \\ vs  \\ \\ H_A: \\theta &gt; 42\\]"
  },
  {
    "objectID": "rm-data/slides/week04.html#the-logic-of-hypothesis-testing-1",
    "href": "rm-data/slides/week04.html#the-logic-of-hypothesis-testing-1",
    "title": "Generalizing from Data",
    "section": "The logic of hypothesis testing",
    "text": "The logic of hypothesis testing\n\n\\(H_A\\) is (often) what I want to prove\n\\(H_0\\) is what I wanna reject so that we can prove \\(H_A\\)\n\\(H_0\\) is not rejected\n\nnot enough evidence or\ntrue (ie \\(H_A\\) is false)\n\nI can never say \\(H_0\\) is true."
  },
  {
    "objectID": "rm-data/slides/week04.html#case-study---online-vs-offline-prices",
    "href": "rm-data/slides/week04.html#case-study---online-vs-offline-prices",
    "title": "Generalizing from Data",
    "section": "Case Study - online vs offline prices",
    "text": "Case Study - online vs offline prices\n\nQuestion: Do the online and offline prices of the same products differ?\nThis data includes 10 to 50 products in each retail store included in the survey (the largest retailers in the U.S. that sell their products both online and offline).\nThe products were selected by the data collectors in offline stores, and they were matched to the same products the same stores sold online.\nThe statistic of interest is the difference in average prices."
  },
  {
    "objectID": "rm-data/slides/week04.html#section",
    "href": "rm-data/slides/week04.html#section",
    "title": "Generalizing from Data",
    "section": "",
    "text": "Each product \\(i\\) has an off-line and on-line price.\nThe statistic with \\(n\\) observations (products) in the data, is: \\[s = \\bar{p}_\\text{diff} = \\frac{1}{n} \\sum_{i=1}^n (p_{i,\\text{online}} - p_{i,\\text{offline}})\\]\nThe average of the price differences is equal to the difference of the average prices \\[\\frac{1}{n} \\sum_{i=1}^n (p_{i,\\text{online}} - p_{i,\\text{offline}}) = \\frac{1}{n} \\sum_{i=1}^n p_{i,\\text{online}} - \\frac{1}{n} \\sum_{i=1}^n p_{i,\\text{offline}}\\]"
  },
  {
    "objectID": "rm-data/slides/week04.html#section-1",
    "href": "rm-data/slides/week04.html#section-1",
    "title": "Generalizing from Data",
    "section": "",
    "text": "Descriptive statistics of the difference:\n\nThe mean difference is USD -0.05: online prices are, on average, 5 cents lower in this dataset.\nSpread around this average: Std: USD 10\nExtreme values matter: Range: -380 — USD +415.\nOf the 6439 products, 64% have the same online and offline price, for 87%, the difference within ±1 dollars."
  },
  {
    "objectID": "rm-data/slides/week04.html#case-study---external-validity",
    "href": "rm-data/slides/week04.html#case-study---external-validity",
    "title": "Generalizing from Data",
    "section": "Case Study - External validity:",
    "text": "Case Study - External validity:\n\nThe products in the data may not represent all products sold at these stores.\n\nBias? Were the products selected randomly?\n\nStrictly: The findings refer to products sond online-offline by large retail stores. And those selected by the people collecing the data.\nMore broadly: price differences among all products in the U.S. sold both online and offline by the same retailers.\n\nMay not be representative of smaller retailers"
  },
  {
    "objectID": "rm-data/slides/week04.html#t-test",
    "href": "rm-data/slides/week04.html#t-test",
    "title": "Generalizing from Data",
    "section": "T-test",
    "text": "T-test\n\nThe t-test is the testing procedure based on the t-statistic\nWe compare the estimated value of the statistic \\(\\hat{s}\\) to zero. (\\(H_0\\))\nEvidence to reject the null is based on difference between \\(\\hat{s}\\) and zero.\n\nReject the null if difference large (its un unlikely to be zero).\nNot reject the null if the difference is small ( not enough evidence against it)."
  },
  {
    "objectID": "rm-data/slides/week04.html#t-test-1",
    "href": "rm-data/slides/week04.html#t-test-1",
    "title": "Generalizing from Data",
    "section": "T-test",
    "text": "T-test\n\nThe test statistic is a statistic that measures the (standardized) distance of the estimated value from what the true value would be if \\(H_0\\) was true.\nUses estimated value of \\(s\\) (\\(\\hat{s}\\)) and the standard error of estimate (SE (\\(\\hat{s}\\))).\nConsider \\(H_0: s_\\text{true} = 0, H_A: s_\\text{true} \\neq 0\\). The t-statistic for this hypotheses is: \\[t = \\frac{\\hat{s}}{\\text{SE}(\\hat{s})}\\]"
  },
  {
    "objectID": "rm-data/slides/week04.html#t-test-2",
    "href": "rm-data/slides/week04.html#t-test-2",
    "title": "Generalizing from Data",
    "section": "T-test",
    "text": "T-test\nWhen \\(\\hat{s}\\) is the average of a variable \\(x\\), the t-statistic is simply \\[t = \\frac{\\bar{x}}{\\text{SE}(\\bar{x})}\\]\nWhen \\(\\hat{s}\\) is the average of a variable \\(x\\) minus a number, the t-statistic is \\[t = \\frac{\\bar{x} - \\text{number}}{\\text{SE}(\\bar{x})}\\]\nWhen \\(\\hat{s}\\) is the difference between two averages, say, \\(\\bar{x}_A\\) and \\(\\bar{x}_B\\), the t-statistic is \\[t = \\frac{\\bar{x}_A - \\bar{x}_B}{\\text{SE}(\\bar{x}_A - \\bar{x}_B)}\\]"
  },
  {
    "objectID": "rm-data/slides/week04.html#t-test-3",
    "href": "rm-data/slides/week04.html#t-test-3",
    "title": "Generalizing from Data",
    "section": "T-test",
    "text": "T-test\n\nWhile we can use SE to calculate the t-statistic, SE may be more difficult to calculate in some situations.\n\nDifferent samples, different SE, etc\n\nSome times you may want to use Bootstrap to calculate SE.\nStata Corner: ttest command in Stata calculates the t-statistic for a difference in means."
  },
  {
    "objectID": "rm-data/slides/week04.html#making-a-decision",
    "href": "rm-data/slides/week04.html#making-a-decision",
    "title": "Generalizing from Data",
    "section": "Making a decision",
    "text": "Making a decision\n\nOnce you obtain your t-statistic (or other relevant statistic), you need to make a decision regarding the null hypothesis.\nIn hypothesis testing the decision is based on a clear rule specified in advance. A critical value.\n\nThis makes the decision straightforward + transparent\nHelps avoid personal bias:put more weight on the evidence that supports our prejudices."
  },
  {
    "objectID": "rm-data/slides/week04.html#making-a-decision-decision-rulecritical-value",
    "href": "rm-data/slides/week04.html#making-a-decision-decision-rulecritical-value",
    "title": "Generalizing from Data",
    "section": "Making a decision: decision rule/Critical value",
    "text": "Making a decision: decision rule/Critical value\n\nThe Critical value is a threshold that determines if the test statistic is large enough to reject the null.\n\nRecall, we start assuming the null is true.\nThen we need to test if our evidence (estimates) is different enough from the null to reject it.\nThe critical value is what determines how different is different enough.\n\nNull rejected if the test statistic is larger than the critical value"
  },
  {
    "objectID": "rm-data/slides/week04.html#making-a-decision-possible-outcomes",
    "href": "rm-data/slides/week04.html#making-a-decision-possible-outcomes",
    "title": "Generalizing from Data",
    "section": "Making a decision: Possible outcomes",
    "text": "Making a decision: Possible outcomes\n\n\n\nSome times we are right:\n\nReject the null when it is false,\nor do not reject the null when it is true.\n\nBut, We can be wrong:\n\nReject the null even though it is true\nor do not reject the null even though is false.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\n\nDo not reject \\(H_0\\)\nCorrect\nFalse negative (Type II)\n\n\nReject \\(H_0\\)\nFalse positive (TYPE I)\nCorrect"
  },
  {
    "objectID": "rm-data/slides/week04.html#making-a-decision-error-of-type-i-and-ii",
    "href": "rm-data/slides/week04.html#making-a-decision-error-of-type-i-and-ii",
    "title": "Generalizing from Data",
    "section": "Making a decision: Error of type I and II",
    "text": "Making a decision: Error of type I and II\n\nBoth types of errors are wrong but\nDuring Testing the null is protected: we only reject it if there is enough evidence against it.\nThe background assumption\n\nwrongly rejecting the null (a false positive) is a bigger mistake than wrongly accepting it (a false negative).\n\nDecision rule (critical value) is chosen in a way that makes false positives rare."
  },
  {
    "objectID": "rm-data/slides/week04.html#making-a-decision-critical-values",
    "href": "rm-data/slides/week04.html#making-a-decision-critical-values",
    "title": "Generalizing from Data",
    "section": "Making a decision: Critical values",
    "text": "Making a decision: Critical values\n\nA commonly applied critical value for a t-statistic is ±2 (or 1.96), a 95% confidence level, or a 5% level of significance (alpha).\nOther critical values can be set: 10% (1.65), 1% (2.58), etc.\nThat choice of 5% means that we tolerate a 5% chance for being wrong when rejecting the null (1/20)."
  },
  {
    "objectID": "rm-data/slides/week04.html#making-a-decision-in-a-picture",
    "href": "rm-data/slides/week04.html#making-a-decision-in-a-picture",
    "title": "Generalizing from Data",
    "section": "Making a decision: In a picture",
    "text": "Making a decision: In a picture"
  },
  {
    "objectID": "rm-data/slides/week04.html#false-negative-fn-and-false-positive-fp",
    "href": "rm-data/slides/week04.html#false-negative-fn-and-false-positive-fp",
    "title": "Generalizing from Data",
    "section": "False negative (FN) and False positive (FP)",
    "text": "False negative (FN) and False positive (FP)\n\n\n\nFixing the chance of FP affects the chance of FN at the same time.\nA FN arises when the t-statistic is within the critical values and we don’t reject the null even though the null is not true.\nThis can happen if Sample is small or The difference between true value and null is small"
  },
  {
    "objectID": "rm-data/slides/week04.html#size-and-power-of-the-test",
    "href": "rm-data/slides/week04.html#size-and-power-of-the-test",
    "title": "Generalizing from Data",
    "section": "Size and power of the test",
    "text": "Size and power of the test\nUnder the null:\n\nSize of the test: the probability of committing a false positive.\nLevel of significance: The maximum probability of false positives we tolerate.\n\nUnder the alternative:\n\nPower of the test: the probability of avoiding a false negative\nHighpower is more likely if:\n\nThe sample size is large\nThe null is far from the true value\nThe standard error is small"
  },
  {
    "objectID": "rm-data/slides/week04.html#recap",
    "href": "rm-data/slides/week04.html#recap",
    "title": "Generalizing from Data",
    "section": "Recap",
    "text": "Recap\n\nIn hypothesis testing we make decisions by a rule\n\nA false positive: decision to reject the null when it is true.\nA false negative: decision not to reject the nullwhen it is false.\n\nThe level of significance is the maximum probability of a false positive that we tolerate (\\(\\alpha\\)=5%).\nThe power of the test is the probability of avoiding a false negative.\n\nIn statistical testing we fix the level of significance of the test to be small (5%, 1%) and hope for high power (based on design).\n\nTests with more observations have more power in general."
  },
  {
    "objectID": "rm-data/slides/week04.html#the-p-value-1",
    "href": "rm-data/slides/week04.html#the-p-value-1",
    "title": "Generalizing from Data",
    "section": "The p-value",
    "text": "The p-value\n\nThe p-values are an alternative approach to do hypothesis testing.\n\nBefore we choose a critical value for a given “significance level” (5%, 1%, etc).\nThis approach suggests using the model significance.\n\nThe smallest significance level at which we can reject \\(H_0\\) in the data\nor largest probability of a false positive that we can tolerate.\n\n\nCalculatiion Will depend on the test statistic and sampling distribution.\nRemember, you can never be certain! (P is never zero)"
  },
  {
    "objectID": "rm-data/slides/week04.html#what-p-value-to-pick",
    "href": "rm-data/slides/week04.html#what-p-value-to-pick",
    "title": "Generalizing from Data",
    "section": "What p-value to pick?",
    "text": "What p-value to pick?\n\np-value is about a trade-off. Large (10-15%) or small (1%) depends on scenarios\nGuilty beyond reasonable doubt? (life or death scenario)\n\nPick a conservative value, like 1% or lower\n\nProof of concept? (a new idea, a new product)\n\nIt’s great if it works at 5%, but even 10-15% means it’s much more likely to be true"
  },
  {
    "objectID": "rm-data/slides/week04.html#case-study---comparing-online-and-offline-prices-testing-hypotheses",
    "href": "rm-data/slides/week04.html#case-study---comparing-online-and-offline-prices-testing-hypotheses",
    "title": "Generalizing from Data",
    "section": "Case Study - Comparing online and offline prices: Testing hypotheses",
    "text": "Case Study - Comparing online and offline prices: Testing hypotheses\n\nLet’s fix the level of significance at 5%.\n\nThe value of the statistic in the dataset is -0.054. Its standard error is 0.124.\nThe t-statistic is 0.44. This is well within ±2.\n\nDon’t reject the null hypothesis of zero difference.\nThe p-value of the test is 0.66.\nSo we don’t reject the null\nWe have not “proven” that online and offline prices are the same, but we have not found evidence that they are different."
  },
  {
    "objectID": "rm-data/slides/week04.html#multiple-testing-motivation",
    "href": "rm-data/slides/week04.html#multiple-testing-motivation",
    "title": "Generalizing from Data",
    "section": "Multiple testing: motivation",
    "text": "Multiple testing: motivation\n\nMedical dataset: data on 400 patients\nA particular heart disease binary variable and 100 feature of life style (sport, eating, health background, socio-economic factors)\nLook for a pattern – is the heart disease equally likely for poor vs rich, take vitamins vs not, etc.\nYou test one-by-one\nYou find that for half a dozen factors, there is a difference\nis there any problem with this procedure?"
  },
  {
    "objectID": "rm-data/slides/week04.html#multiple-testing",
    "href": "rm-data/slides/week04.html#multiple-testing",
    "title": "Generalizing from Data",
    "section": "Multiple testing",
    "text": "Multiple testing\n\nThe pre-set level of significance / p-value are defined for a single test\nbut, In many cases, you will consider doing many many tests.\n\nDifferent measures (mean, median, range, etc)\nDifferent products, retailers, countries\nDifferent measures of management quality\n\nFor multiple tests, you cannot use the same approach as for a single one.\nYou need to be even more conservative in rejecting the null."
  },
  {
    "objectID": "rm-data/slides/week04.html#multiple-testing-example",
    "href": "rm-data/slides/week04.html#multiple-testing-example",
    "title": "Generalizing from Data",
    "section": "Multiple testing: Example",
    "text": "Multiple testing: Example\n\nConsider 100 tests. The Nulls are true for all tests.\nSet \\(\\alpha\\)=5% for each test.\nIn the data, even if the null is true, you will reject 5% of the time. (false positives)\nHowever, if you do “use the evidence” from all tests, it would seem that the null is false in 99.4% of the cases. (by chance)\n\nThis is p-hacking. Choosing what works!"
  },
  {
    "objectID": "rm-data/slides/week04.html#multiple-testing-example-1",
    "href": "rm-data/slides/week04.html#multiple-testing-example-1",
    "title": "Generalizing from Data",
    "section": "Multiple testing: Example",
    "text": "Multiple testing: Example\n\nThere are various ways to deal with probabilities of false positives when testing multiple hypotheses.\n\nOften complicated.\n\nPossible Solution: If you have a few dozens of cases, just use a strict criteria (such as 0.1-0.5% instead than 1-5%) for rejecting null hypotheses.\nA very strict such adjustment is the Bonferroni correction that suggests dividing the single hypothesis value by the number of hypotheses.\nOther methods exists, but are similar in spirit.\nRisk: by being more conservative, you are more likely to obtain false negatives."
  },
  {
    "objectID": "rm-data/slides/week04.html#summary",
    "href": "rm-data/slides/week04.html#summary",
    "title": "Generalizing from Data",
    "section": "Summary",
    "text": "Summary\nTesting in statistics means making a decision about the value of a statistic in the general pattern represented by the data.\n\nHypothesis starts with explicitly stating \\(H_0\\) and \\(H_A\\).\nA statistical test rejects \\(H_0\\) if there is enough evidence against it; otherwise it does not reject it.\nTesting multiple hypotheses at the same time is a tricky business; it pays to be very conservative with rejecting the null."
  },
  {
    "objectID": "rm-data/slides/week02.html#motivation",
    "href": "rm-data/slides/week02.html#motivation",
    "title": "Origins of Data, and Data Preparation",
    "section": "Motivation",
    "text": "Motivation\n\nSuppose, you want to understand the extent and patterns of differences in online and offline prices. How would you go about collecting data?\n\nA super project, the Billion Prices Project at MIT did a variety of data collection approaches such as crowd-sourcing platforms, mobile phone apps and web scraping methods.\n\nInterested in understanding more about management practices?\n\nThe World Management Survey is a major effort by academics to survey practices around the world - asking the same questions in many countries the same way."
  },
  {
    "objectID": "rm-data/slides/week02.html#what-is-data-0s-and-1s",
    "href": "rm-data/slides/week02.html#what-is-data-0s-and-1s",
    "title": "Origins of Data, and Data Preparation",
    "section": "What is data ? 0s and 1s…",
    "text": "What is data ? 0s and 1s…\n\nData is a collection of numbers, characters, images, or other formats.\nThey provide information about something. (Prices? Management practices? Hotel characteristics? etc.)\nOf course, depending on how the data was collected, and what structure it has, it can be more or less useful for answering a particular question."
  },
  {
    "objectID": "rm-data/slides/week02.html#what-is-data-1",
    "href": "rm-data/slides/week02.html#what-is-data-1",
    "title": "Origins of Data, and Data Preparation",
    "section": "What is data ?",
    "text": "What is data ?\nAs Economist, we are more familiar with a specific data structure:\n\nData is most straightforward to analyze if it is in a data table form (2D Matrix form):\n\nA single file with rows and columns.\nEach row is an observation, and each column is a variable.\n\nHow do you find it in the real world?1\n\nStorage: Comma separated values .csv (.txt) is simplest, but other formats are possible\nStata (.dta), SPSS (.sav), R (.rda), Python (.pkl), etc.\n\nA Dataset is a collection of data tables that may be related to each other."
  },
  {
    "objectID": "rm-data/slides/week02.html#data-structures",
    "href": "rm-data/slides/week02.html#data-structures",
    "title": "Origins of Data, and Data Preparation",
    "section": "Data structures",
    "text": "Data structures\nAside from “format”, data can be structured in different ways:\n\nCross-sectional (xsec) data have information on many units observed at the same time\nTime series (tseries) data have information on a single unit observed many times\nMulti-dimensional (panel?) data have multiple dimensions (the observations)"
  },
  {
    "objectID": "rm-data/slides/week02.html#data-structures-panel-xt-data",
    "href": "rm-data/slides/week02.html#data-structures-panel-xt-data",
    "title": "Origins of Data, and Data Preparation",
    "section": "Data structures: Panel (xt) data",
    "text": "Data structures: Panel (xt) data\nMulti-dimensional: Panel data is of particular interest in economics:\n\nA common type of panel data has many units, each observed multiple times\n\ncountries observed repeatedly for several years\n\nIn xt data tables observations are identified by two ID variables: one for the cross-sectional units, one for time\nxt data is balanced if all cross-sectional units are observed at the very same time periods\nIt is called unbalanced if some cross-sectional units are observed more times than others"
  },
  {
    "objectID": "rm-data/slides/week02.html#data-quality-is-key",
    "href": "rm-data/slides/week02.html#data-quality-is-key",
    "title": "Origins of Data, and Data Preparation",
    "section": "Data quality is key",
    "text": "Data quality is key\n\nKeyword: Quality, Quality, Quality\nData quality is key for any analysis\n\nGarbage-in-garbage-out: If data is useless, then answers of our analysis are bound to be useless…\n… no matter how fancy a method we apply to it.\n\nData quality is generally a subjective notion: Different standards for different purposes\nFirst you have to specify what is your (research) question:\n\nWhat do you want to explore or understand?\n\nIf you have a clear (pseudo) answer, then you can decide on your data quality\n\n\n\nHowever, there are some objective measures to decide if you have your question"
  },
  {
    "objectID": "rm-data/slides/week02.html#dimensions-of-data-quality",
    "href": "rm-data/slides/week02.html#dimensions-of-data-quality",
    "title": "Origins of Data, and Data Preparation",
    "section": "Dimensions of data quality",
    "text": "Dimensions of data quality\n\nContent - what is the variable really capturing?\nValidity - how close the actual content of the variable to the intended content\nReliability - if we were to measure the same variable multiple times for the same observation it should give the same result\nComparability of measurement - how similarly the same variable is measured across different observations\nCoverage - what proportion of the population are represented in the data\nUnbiased selection - if coverage is incomplete, is it representative of the population?\n\n\nHow was the data collected??\nDoes the variable capture what it is supposed to capture? are labels correct? is the variable measured correctly? (is =1 a true or a false?)"
  },
  {
    "objectID": "rm-data/slides/week02.html#you-should-allways-know-your-data",
    "href": "rm-data/slides/week02.html#you-should-allways-know-your-data",
    "title": "Origins of Data, and Data Preparation",
    "section": "YOU should allways know your data",
    "text": "YOU should allways know your data\n\nHow data was born? How was it collected, and processed?\nAll details of measurement that may be relevant for their analysis\n\nWhen in doubt, ask the data source. Manuals, codebooks, etc.\n\nBecause of this, you may want to have a:\n\nREADME.txt that describes where dataset comes from\nVARIABLES.xls that provides basic information on your variables (cookbook)"
  },
  {
    "objectID": "rm-data/slides/week02.html#secondary-data-general-characteristics",
    "href": "rm-data/slides/week02.html#secondary-data-general-characteristics",
    "title": "Origins of Data, and Data Preparation",
    "section": "Secondary data: General characteristics",
    "text": "Secondary data: General characteristics\n\nType: Data, or information, collected by someone else, for different purposes\n\nTax records collects do not contain demographics and education.\nA Survey that collects Demographics, but not income data.\n\nData quality consequences\n\nMay not contain variables that we need\nValidity may be high or low\nPotential selection bias if low covarege\n\nFrequent advantages\n\nInexpensive?\nOften many observations\n\n\n\n\nSecondary data is data that was collected by someone else for a different purpose\nWe cant control how the data was collected, but we can control how we use it. You need to assume that the data is not perfect. And assume consequences of that."
  },
  {
    "objectID": "rm-data/slides/week02.html#how-to-collect-the-data",
    "href": "rm-data/slides/week02.html#how-to-collect-the-data",
    "title": "Origins of Data, and Data Preparation",
    "section": "How to Collect the Data",
    "text": "How to Collect the Data\n\nBy hand:\n\nMany data sources are available online: World Bank and IMF data, etc. (usually easy to download)\nFor the US, www.ipums.org has a lot of standardized data (CPS, ATUS, ACS, etc.)\n\nAutomated API:\n\nMany agencies also offer API (Application Programming Interface) to directly load data into a statistical software\nAPI widely used in many context. see here for a list from Berkeley"
  },
  {
    "objectID": "rm-data/slides/week02.html#how-to-collect-the-data-1",
    "href": "rm-data/slides/week02.html#how-to-collect-the-data-1",
    "title": "Origins of Data, and Data Preparation",
    "section": "How to Collect the Data",
    "text": "How to Collect the Data\n\nData from online platform (web scraping):\n\nhtml code includes data, that can be collected and analyzed\nR (rvest) and Python (beautiful soup) can be used for that purpose\nStata does not have good tools for web scraping\n\nNeed extensive cleaning\nCan be repeated (automated) if data is updated frequently\n\nMindful of the terms of service of the website\n\nData collection limited to what is on a site\n\n\n\nWeb scraping is a powerful tool that needs a know-how, and requires extensive cleaning\nWeb scraping is limited to what is on a site\nAlways be mindful of the terms of service of the website. Not all websites allow web scraping"
  },
  {
    "objectID": "rm-data/slides/week02.html#administrative-data-general-characteristics",
    "href": "rm-data/slides/week02.html#administrative-data-general-characteristics",
    "title": "Origins of Data, and Data Preparation",
    "section": "Administrative Data: General Characteristics",
    "text": "Administrative Data: General Characteristics\n\nBusiness transactions, Government records, taxes, social security\nMany advantages\n\nOften great coverage (Census), few missing values, high quality content (tax records)\nMany well defined and documented variables\n\nSome disadvantages\n\nDefined for a different purpose, not your research question\nOften not detailed/specific enough\nBiggest problem is very limited access: Need to apply for access"
  },
  {
    "objectID": "rm-data/slides/week02.html#survey-general-characteristics",
    "href": "rm-data/slides/week02.html#survey-general-characteristics",
    "title": "Origins of Data, and Data Preparation",
    "section": "Survey: General characteristics",
    "text": "Survey: General characteristics\n\nSurveys collect data by asking people (respondents) and recording their answers\nAnswers should be short(!) and easily(!) transformed into variables\nMajor advantage: you can ask what you want to know\nHow?\n\nself-administered surveys and interviews\nWeb, telephone, in person, mix - computer aided interview\n\nWhat could go wrong? (and assume House MD is wrong)\n\n\nChoice of data collection approach matters a great deal. Can be done efficiently, with good aids. And depending on design, can be cheap"
  },
  {
    "objectID": "rm-data/slides/week02.html#sampling",
    "href": "rm-data/slides/week02.html#sampling",
    "title": "Origins of Data, and Data Preparation",
    "section": "Sampling",
    "text": "Sampling\n\nPerhaps one can collect data on all observations we want (the population)\nbut, more often we don’t because it’s impractical or prohibitively expensive\nSampling is when we purposefully collect data on a subset/sample (\\(&lt;100%\\) coverage) of the population\nSampling is the process that selects that subset (How do we select the sample?)"
  },
  {
    "objectID": "rm-data/slides/week02.html#what-we-want-representative-samples",
    "href": "rm-data/slides/week02.html#what-we-want-representative-samples",
    "title": "Origins of Data, and Data Preparation",
    "section": "What We Want: Representative samples",
    "text": "What We Want: Representative samples\n\nA sample is good if it represents the population\n\nall important variables have very similar distributions in the sample and the population\nall patterns in the sample are very similar to the patterns in the population\n\nExamples\n\nThe age distribution of a sample of employees is the same as the age distribution of all employees\nThe income distribution in the CPS is the same as the income distribution in the US\n\nBut how can we tell?"
  },
  {
    "objectID": "rm-data/slides/week02.html#how-can-we-tell-if-a-sample-is-representative",
    "href": "rm-data/slides/week02.html#how-can-we-tell-if-a-sample-is-representative",
    "title": "Origins of Data, and Data Preparation",
    "section": "How can we tell if a sample is representative",
    "text": "How can we tell if a sample is representative\n\nNever for sure\n\nIf you knew the population, you wouldn’t need the sample\nWe know the patterns in the sample but not in the population\n\nBut, we could do Benchmarking\n\nWe may know a few distributions or patterns in the population\nThose should be similar in the sample\nExample: Using the Census to check the age distribution in the CPS\n\nOr, knowing the process of sampling\n\nRandom sampling is known to lead to representative samples with high likelihood"
  },
  {
    "objectID": "rm-data/slides/week02.html#sampling-random-sampling",
    "href": "rm-data/slides/week02.html#sampling-random-sampling",
    "title": "Origins of Data, and Data Preparation",
    "section": "Sampling: Random sampling",
    "text": "Sampling: Random sampling\n\nRandom sampling is a selection rule independent of any important variable\n\nits the most likely to produce representative samples\nAny other methods may lead to biased selection\n\nExamples\n\nGood: people with odd-numbered birth dates (a 50% sample)\nGood: the first half of a list of firms sorted by a random number generated by the computer\nBad: the first half of a list of people by alphabetical order\nBad: firms that were established in the most recent years"
  },
  {
    "objectID": "rm-data/slides/week02.html#random-sampling-is-best",
    "href": "rm-data/slides/week02.html#random-sampling-is-best",
    "title": "Origins of Data, and Data Preparation",
    "section": "Random sampling is best",
    "text": "Random sampling is best\n\nProvided sample is large enough (N \\(\\rightarrow\\) infinity)\nIn small samples (dozens) anything is possible\nIn a representative sample, size (N) matters, not coverage\n\nCPS ~ 70,000 households (0.02% of US); ATUS ~ 9,000 ppl (0.003% of US)\n\nLarger samples better (more power/precise estimates) but …"
  },
  {
    "objectID": "rm-data/slides/week02.html#sampling-clusterstratified-sampling",
    "href": "rm-data/slides/week02.html#sampling-clusterstratified-sampling",
    "title": "Origins of Data, and Data Preparation",
    "section": "Sampling: Cluster/Stratified sampling",
    "text": "Sampling: Cluster/Stratified sampling\n\nSome times, however, random sampling is prohibitively expensive.\nFurthermore, sometimes we want to oversample some groups (rare groups), to make sure we have enough observations\nBoth approaches may help to reduce costs of collection."
  },
  {
    "objectID": "rm-data/slides/week02.html#section",
    "href": "rm-data/slides/week02.html#section",
    "title": "Origins of Data, and Data Preparation",
    "section": "",
    "text": "Random samplingCluster samplingStratified sampling"
  },
  {
    "objectID": "rm-data/slides/week02.html#what-is-different-with-big-data",
    "href": "rm-data/slides/week02.html#what-is-different-with-big-data",
    "title": "Origins of Data, and Data Preparation",
    "section": "What is different with Big Data?",
    "text": "What is different with Big Data?\n\nmassive datasets that are (billions?) Not necessarily representative\noften automatically and continuously collected and stored (Transaction data, tweets, etc.)\n\nNot necessarily for analytic purposes\n\nComplex\n\ntext (video, music/noise), network, multidimensional, maps"
  },
  {
    "objectID": "rm-data/slides/week02.html#different-yet-the-same",
    "href": "rm-data/slides/week02.html#different-yet-the-same",
    "title": "Origins of Data, and Data Preparation",
    "section": "Different yet the same",
    "text": "Different yet the same\nDifferent:\n\nA particular source of uncertainty of the results of an analysis is greatly reduced\nRare or more nuanced patterns can be uncovered\nPractical challenges (storage, processing, etc.)\nSome challenges may be solved by working with a random subsample\n\nSame:\n\nNeed to represent entire population if incomplete coverage\nExample: Big Data with 75% coverage with a selection bias leads to biased results\nNon-big data from same population with 1% random sample leads to good results"
  },
  {
    "objectID": "rm-data/slides/week02.html#sample-selection-bias",
    "href": "rm-data/slides/week02.html#sample-selection-bias",
    "title": "Origins of Data, and Data Preparation",
    "section": "Sample selection bias",
    "text": "Sample selection bias\n\nThe sample you collect is different from the population\nThis difference is crucial in the story\nExample: Predicting presidential election\n\n1936: Literary Digest. FD Roosevelt vs Landon. 10m people asked. 2m replied. Biggest poll ever. Landon was predicted win 57%\n\n1948 Chicago Tribune. Dewey predicted beat Truman. Used phone registry"
  },
  {
    "objectID": "rm-data/slides/week02.html#legal-and-ethical-aspects",
    "href": "rm-data/slides/week02.html#legal-and-ethical-aspects",
    "title": "Origins of Data, and Data Preparation",
    "section": "Legal and ethical aspects",
    "text": "Legal and ethical aspects\nDuring Data collection, be aware of ethical and legal constraints, Special care with sensitive information\nMore with web scraping…\nAlways communicate with the source owner(s) and or with legal professional if you are planning to use seemingly sensitive data (names, addresses, etc.)"
  },
  {
    "objectID": "rm-data/slides/week02.html#ai-and-data-collection-wrangling",
    "href": "rm-data/slides/week02.html#ai-and-data-collection-wrangling",
    "title": "Origins of Data, and Data Preparation",
    "section": "AI and data collection, wrangling",
    "text": "AI and data collection, wrangling\n\nData collection and management often behind walls\nAI can help write code to web-scrape, etc. (Python is quite good at it)\nAI is great to give a first impression of your dataset, incl. quality, data structure\nAI is helpful to discuss sampling ideas\nAI needs context to do good, and will not have proper domain knowledge\nAI needs supervision\n\nLesson: AI is a tool, not a replacement"
  },
  {
    "objectID": "rm-data/slides/week02.html#main-takeaway",
    "href": "rm-data/slides/week02.html#main-takeaway",
    "title": "Origins of Data, and Data Preparation",
    "section": "Main takeaway",
    "text": "Main takeaway\n\nKnow your data\n\nHow it was born,\nWhat its main advantages are\nWhat its main disadvantages are\n\nData quality determines the results of your analysis\nData quality is determined by how the data was born, and how you are planning to use it"
  },
  {
    "objectID": "rm-data/slides/week02.html#motivation-1",
    "href": "rm-data/slides/week02.html#motivation-1",
    "title": "Origins of Data, and Data Preparation",
    "section": "Motivation",
    "text": "Motivation\n\nDoes immunization of infants against measles save lives in poor countries? Use data on immunization rates in various countries in various years from the World Bank. How should you store, organize and use the data to have all relevant information in an accessible format that lends itself to meaningful analysis?\nYou want to know, who has been the best manager in the top English football league. Have downloaded data on football games and on managers. To answer your question you need to combine this data. How should you do that? And are there issues with the data that you need to address?"
  },
  {
    "objectID": "rm-data/slides/week02.html#variable-types-qualitative-vs-quantitative",
    "href": "rm-data/slides/week02.html#variable-types-qualitative-vs-quantitative",
    "title": "Origins of Data, and Data Preparation",
    "section": "Variable types: Qualitative vs quantitative",
    "text": "Variable types: Qualitative vs quantitative\n\nData can be born (collected, generated) in different form, and our variables may capture the quality or the quantity of a phenomenon.\nQuantitative variables are born as numbers. Typically take many values. (age, height, steps…)\nQualitative variables, also called categorical variables, take on a few values, with each value having a specific interpretation (belonging a category). (Race, Gender, Brand, etc)"
  },
  {
    "objectID": "rm-data/slides/week02.html#variable-types-dummies-or-binary",
    "href": "rm-data/slides/week02.html#variable-types-dummies-or-binary",
    "title": "Origins of Data, and Data Preparation",
    "section": "Variable types: Dummies or binary",
    "text": "Variable types: Dummies or binary\n\nA special case is a binary variable, which can take on two values\n…yes/no answer to whether the observation belongs to some group. Best to represent these as 0 or 1 variables: 0 for no, 1 for yes.\n\nExample: is_female, is_head_of_household, is_pregnant, is_employed\n\nFlag - binary showing existence of some issue (such as missing value for another variable, presence in another dataset)\n\nExample: missing_age, missing_income, in_sample\n\nNote Some times Surveys do NOT use 0-1 for binary variables. Be careful.\n\nssc install fre\nfre categorical_variable\n* This program will tabulate the data and show you the labels if any"
  },
  {
    "objectID": "rm-data/slides/week02.html#variable-types---formal-definition",
    "href": "rm-data/slides/week02.html#variable-types---formal-definition",
    "title": "Origins of Data, and Data Preparation",
    "section": "Variable types - formal definition",
    "text": "Variable types - formal definition\n\nNominal qualitative variables take on values that cannot be unambiguously ordered: Color, brands, race\nOrdinal, or ordered variables take on values that are unambiguously ordered. Grade, satisfaction, income brackets\nInterval/continuous variables are ordered variables, with a comparable “change”: Age, Degree Celsius, Price in dollars."
  },
  {
    "objectID": "rm-data/slides/week02.html#data-wrangling",
    "href": "rm-data/slides/week02.html#data-wrangling",
    "title": "Origins of Data, and Data Preparation",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nData wrangling is the process of transforming raw data to a set of data tables that can be used for a variety of downstream purposes such as data analysis.\n\n\n\n\nUnderstanding and storing\n\nstart from raw data\nunderstand the structure and content\ncreate tidy data tables\nunderstand links between tables\n\n\n\n\nData cleaning\n\nunderstand features, variable types\nfilter duplicates\nlook for and manage missing observations\nunderstand limitations\n\n\n\n\nThis is a crucial because out there, Data is Messy"
  },
  {
    "objectID": "rm-data/slides/week02.html#the-tidy-data-approach",
    "href": "rm-data/slides/week02.html#the-tidy-data-approach",
    "title": "Origins of Data, and Data Preparation",
    "section": "The tidy data approach",
    "text": "The tidy data approach\nA useful concept of organizing and cleaning data is called the tidy data approach:\n\nEach observation forms a row.\nEach variable forms a column.\nEach type of observational unit forms a table. (One for Families, One for Members)\nEach observation has a unique identifier (ID) (Family ID and Person ID)\nCan be merged with other tables if needed.\n\nAdvantages:\n\nTidy tables are easy to work with, and make finding errors easy.\nEasy to understand and extend: New observations adds rows; new variables adds columns."
  },
  {
    "objectID": "rm-data/slides/week02.html#simple-tidy-data-table",
    "href": "rm-data/slides/week02.html#simple-tidy-data-table",
    "title": "Origins of Data, and Data Preparation",
    "section": "Simple tidy data table",
    "text": "Simple tidy data table\n\n\n\nhotel_id\nprice\ndistance\n\n\n\n\n21897\n81\n1.7\n\n\n21901\n85\n1.4\n\n\n21902\n83\n1.7\n\n\n\nSource: hotels-vienna data. Vienna, 2017 November week-end.\nEach Row a new observation, Each Column a new Variable"
  },
  {
    "objectID": "rm-data/slides/week02.html#tidy-data-table-of-multi-dimensional-data",
    "href": "rm-data/slides/week02.html#tidy-data-table-of-multi-dimensional-data",
    "title": "Origins of Data, and Data Preparation",
    "section": "Tidy data table of multi-dimensional data",
    "text": "Tidy data table of multi-dimensional data\n\nThe tidy approach - store xt data so that One row is one it observation (Cross-section unit i observed at time t). (Long format)\n\nThe next row then may be the same cross-sectional unit observed in the next time period.\n\nYou may want to use similar criteria with multi-dimensional data (ijt data)\n\nAlternative wide format : one row refers to one cross-sectional unit, and different time periods are represented in different columns. Not the best way to keep the data"
  },
  {
    "objectID": "rm-data/slides/week02.html#displaying-immunization-rates-across-countries---wide",
    "href": "rm-data/slides/week02.html#displaying-immunization-rates-across-countries---wide",
    "title": "Origins of Data, and Data Preparation",
    "section": "Displaying immunization rates across countries - WIDE",
    "text": "Displaying immunization rates across countries - WIDE\n\n\n\n\n\n\n\n\n\n\n\n\nCountry\nimm2015\nimm2016\nimm2017\ngdppc2015\ngdppc2016\ngdppc2017\n\n\n\n\nIndia\n87\n88\n88\n5743\n6145\n6516\n\n\nPakistan\n75\n75\n76\n4459\n4608\n4771\n\n\n\nSource: world-bank-vaccination data\nWide format of country-year panel data, each row is one country, different years are different variables.\nimm: rate of immunization against measles among 12–13-month-old infants.\ngdppc: GDP per capital, PPP, constant 2011 USD."
  },
  {
    "objectID": "rm-data/slides/week02.html#displaying-immunization-rates-across-countries---long",
    "href": "rm-data/slides/week02.html#displaying-immunization-rates-across-countries---long",
    "title": "Origins of Data, and Data Preparation",
    "section": "Displaying immunization rates across countries - LONG",
    "text": "Displaying immunization rates across countries - LONG\n\n\n\nCountry\nYear\nimm\ngdppc\n\n\n\n\nIndia\n2015\n87\n5743\n\n\nIndia\n2016\n88\n6145\n\n\nIndia\n2017\n88\n6516\n\n\nPakistan\n2015\n75\n4459\n\n\nPakistan\n2016\n75\n4608\n\n\nPakistan\n2017\n76\n4771\n\n\n\nNote: Tidy (long) format of country-year panel data, each row is one country in one year.\nimm: rate of immunization against measles among 12–13-month-old infants.\ngdppc: GDP per capital, PPP, constant 2011 USD. Source: world-bank-vaccination data."
  },
  {
    "objectID": "rm-data/slides/week02.html#stata-programming-corner",
    "href": "rm-data/slides/week02.html#stata-programming-corner",
    "title": "Origins of Data, and Data Preparation",
    "section": "Stata Programming corner",
    "text": "Stata Programming corner\n\nTransforming your data from Wide to Long format (or viceversa) can be done using reshape\n\n* From wide to long\nren *, low // &lt;- Make sure your variables are all lower case\nreshape long imm gdppc, /// &lt;- Make variable Long, and indicate what variables to \"make\" long\n    i(country) j(year) string // &lt;- also the dimension that was previously \"wide\" Year\n\n* From long to wide\nreshape wide imm gdppc, /// &lt;- Make variable Long, and indicate what variables to \"make\" long\n    i(country) j(year)  // &lt;- also the dimension that was previously \"wide\" Year"
  },
  {
    "objectID": "rm-data/slides/week02.html#stata-programming-corner-1",
    "href": "rm-data/slides/week02.html#stata-programming-corner-1",
    "title": "Origins of Data, and Data Preparation",
    "section": "Stata Programming corner",
    "text": "Stata Programming corner\n\nSome times, you may need to reshape only 1 variable, and keep the rest as they are.\n\nAdding head of household education to all family members.\n\nYou can do it two ways:\n\nCreate a smaller dataset and merge\nID head, and gen the new variable\nbysort hid: egen head_educ = max(educ*(is_head==1))"
  },
  {
    "objectID": "rm-data/slides/week02.html#a-complex-dataset-relational-database",
    "href": "rm-data/slides/week02.html#a-complex-dataset-relational-database",
    "title": "Origins of Data, and Data Preparation",
    "section": "A complex Dataset: Relational database",
    "text": "A complex Dataset: Relational database\n\nSome datasets cannot be stored in a single table.\n\nok, they could, but would be very inefficient.\n\nData like this are typically stored in a relational database.\n\nSmaller tidy datasets can be stored in a single table (single unit of observation),\nand that can be linked to other tables with a unique identifiers (ID or Keys).\n\nThis structure forces you to better understand your data.\nAfter understanding the data, you can merge/join/link/match tables as needed."
  },
  {
    "objectID": "rm-data/slides/week02.html#identifying-successful-futbol-managers",
    "href": "rm-data/slides/week02.html#identifying-successful-futbol-managers",
    "title": "Origins of Data, and Data Preparation",
    "section": "Identifying successful Futbol managers",
    "text": "Identifying successful Futbol managers\n\nReview the example, Specially if interested in Futbol\nIn short, Data can have different structures (all tidy)\nSome structures are more useful than others.\nUnderstanding those structures will allow you to work with the data"
  },
  {
    "objectID": "rm-data/slides/week02.html#american-time-use-survey",
    "href": "rm-data/slides/week02.html#american-time-use-survey",
    "title": "Origins of Data, and Data Preparation",
    "section": "American Time Use Survey",
    "text": "American Time Use Survey\n\nThe ATUS is a good example of a relational dataset\nIf downloaded RAW (census) you need to navigate through many files:\n\nATUS-ACT: contains all time activities, plus other info, for the individuals interviewed in the ATUS. Keys: tucaseid and tuactivity_n\nATUS-CPS: Data for all Family members, from CPS. Keys tucaseid tulineno\nATUS-RESP: Some aggregated TimeUse, and additional respondand information. Key tucaseid\nATUS-ROST: Basic demographics for all household members. Keys tucaseid tulineno\nATUS-SUM: Aggregated Time use data, by different types. Keys tucaseid"
  },
  {
    "objectID": "rm-data/slides/week02.html#american-time-use-survey-1",
    "href": "rm-data/slides/week02.html#american-time-use-survey-1",
    "title": "Origins of Data, and Data Preparation",
    "section": "American Time Use Survey",
    "text": "American Time Use Survey\n\nDepending on your goals, you may want to combine information from various datasets\n\nAggregate some data, combine others, transform.\n\nUnderstanding the data structure may also help you see how to best “merge the data”"
  },
  {
    "objectID": "rm-data/slides/week02.html#stata-corner-types-of-merging",
    "href": "rm-data/slides/week02.html#stata-corner-types-of-merging",
    "title": "Origins of Data, and Data Preparation",
    "section": "Stata-Corner: Types of Merging",
    "text": "Stata-Corner: Types of Merging\nThere are 4 3 types of merging, depending of the master or using dataset\n\n1:1 merging: Both master and using datasets are uniquely by the same variables. use atus-rost merge 1:1 tucaseid tulineno using atus-cps\n1:m merging: Each observation in the master file will be merge with many units in the using dataset. Master has unique ID. use atus-resp merge 1:m tucaseid using atus-act\nm:1 merging: Many observations in the master will be merge with one unit in the using. Using has a unique ID\nuse atus-act merge m:1 tucaseid using atus-resp\nm:m merging: its wrong…dont do it. Perhaps think joinby instead"
  },
  {
    "objectID": "rm-data/slides/week02.html#stata-corner",
    "href": "rm-data/slides/week02.html#stata-corner",
    "title": "Origins of Data, and Data Preparation",
    "section": "Stata-Corner",
    "text": "Stata-Corner\n\n\n\n\n\n\nImportant\n\n\n\nEvery time you do a merge, Stata will create a variable called _merge that will tell you what happened to the merge.\nIf this variable exist in your datasets (master or using) you will get an error.\nso make sure to drop (or rename) it before merging (after you have checked it)"
  },
  {
    "objectID": "rm-data/slides/week02.html#complex-data---tidy-data-summary",
    "href": "rm-data/slides/week02.html#complex-data---tidy-data-summary",
    "title": "Origins of Data, and Data Preparation",
    "section": "Complex data - tidy data: summary",
    "text": "Complex data - tidy data: summary\n\nCreating a tidy data is important so data tables are easy to understand, combine and extend in the future.\nIf relational data, IDs are essential (allow to link tables)\nOften raw data will not come in a tidy format, and you will need to work understanding the structure, relationships and find the individual ingredients.\nFor analysis work, may need to combine tidy data tables\nBut probably only need a fraction of all variables."
  },
  {
    "objectID": "rm-data/slides/week02.html#data-cleaning-1",
    "href": "rm-data/slides/week02.html#data-cleaning-1",
    "title": "Origins of Data, and Data Preparation",
    "section": "Data cleaning",
    "text": "Data cleaning\nWith most data, in addition to understand it, you need to “clean”, before using it (Very Important)\n\nEntity resolution:\n\nDealing with duplicates: Why are they there? What to do?\nAmbiguous identification: is it John or Jonh or Jon ?\nnon-entity rows: uh? what is this?\n\nMissing values\n\nwhy is it missing? is it missing at random? by design? endogenous?"
  },
  {
    "objectID": "rm-data/slides/week02.html#dealing-with-duplicates",
    "href": "rm-data/slides/week02.html#dealing-with-duplicates",
    "title": "Origins of Data, and Data Preparation",
    "section": "Dealing with duplicates",
    "text": "Dealing with duplicates\n\n** Duplicates**: Observations appearing more than once in the data.\n\nMay be the result of human error, or the features of data source (e.g., data scraped from classified ads. Some posts appear more than once).\n\nOften, easy spot\n\nduplicates report in Stata, or bysort ID: gen dup = _n\n\nbut one needs to investigate. Makes sense / an error? something else?\nDecision: what to keep. Sometimes no clear-cut way, but usually no big deal."
  },
  {
    "objectID": "rm-data/slides/week02.html#entity-identification-and-resolution",
    "href": "rm-data/slides/week02.html#entity-identification-and-resolution",
    "title": "Origins of Data, and Data Preparation",
    "section": "Entity identification and resolution",
    "text": "Entity identification and resolution\n\nYou need to have unique IDs\n\nyou can use isid to check if a variable(s) is a unique identifier\nIf not, check why not. Perhaps wrong ID?\n\nPossible cases:\n\nSame identifier, different entities\nDifferent identifiers, same entity (??)\n\nEntity resolution: process of identifying, merging and eliminating duplicate entities."
  },
  {
    "objectID": "rm-data/slides/week02.html#entity-resolution-example",
    "href": "rm-data/slides/week02.html#entity-resolution-example",
    "title": "Origins of Data, and Data Preparation",
    "section": "Entity resolution example",
    "text": "Entity resolution example\n\n\n\nTeam ID\nUnified name\nOriginal name\n\n\n\n\n19\nMan City\nManchester City\n\n\n19\nMan City\nMan City\n\n\n19\nMan City\nMan. City\n\n\n19\nMan City\nManchester City F.C.\n\n\n20\nMan United\nManchester United\n\n\n20\nMan United\nManchester United F.C.\n\n\n20\nMan United\nManchester United Football Club\n\n\n20\nMan United\nMan United"
  },
  {
    "objectID": "rm-data/slides/week02.html#getting-rid-of-non-entity-observations",
    "href": "rm-data/slides/week02.html#getting-rid-of-non-entity-observations",
    "title": "Origins of Data, and Data Preparation",
    "section": "Getting rid of non-entity observations",
    "text": "Getting rid of non-entity observations\n\nSometimes, data may contain rows that do not belong to an entity we want\n\nFor example, region-level data may contain country-level aggregates\n\nFind them and drop them (unless you have a good reason to keep them)\nCommon Case: a data table from the World Bank on countries often includes observations for larger regions (continents, low income countries, etc)"
  },
  {
    "objectID": "rm-data/slides/week02.html#missing-values",
    "href": "rm-data/slides/week02.html#missing-values",
    "title": "Origins of Data, and Data Preparation",
    "section": "Missing values",
    "text": "Missing values\n\nA frequent and important issue: missing values.\nMissing values mean that the value of a variable is available for some, but not all, observations.\nDifferent languages may encode missing values differently.\n\nIn Stata dot “.”, an empty space “” are missing for numbers and strings.\nBut, “.” is considered larger than any number, so be careful when coding.\n\nSurveys, may also have their own rules for coding missing\n\nbinary 0 for no, 1 for yes, 9 for missing\npercent 0-100, 9999 for missing\nnumeric, range is 1-100000, 9999999999 for missing"
  },
  {
    "objectID": "rm-data/slides/week02.html#missing-values-what-to-do",
    "href": "rm-data/slides/week02.html#missing-values-what-to-do",
    "title": "Origins of Data, and Data Preparation",
    "section": "Missing values: What to do?",
    "text": "Missing values: What to do?\nDepends on: Scope: How much missing? and Reason: Why missing?\n\nLook at content of data. This could be related to data quality (especially coverage)\nMissing by design may not be a problem. (Ever been pregnant? missing for men)\nMissing values should be counted, because they mean fewer observations with valid information. (compounding effect)\nBig problem: potential selection bias.\n\nIs data missing at random?\nIs the data still representative?"
  },
  {
    "objectID": "rm-data/slides/week02.html#missing-values-understanding-the-selection-process",
    "href": "rm-data/slides/week02.html#missing-values-understanding-the-selection-process",
    "title": "Origins of Data, and Data Preparation",
    "section": "Missing values: Understanding the selection process",
    "text": "Missing values: Understanding the selection process\n\nRandom: When missing data really means no information, it may be the result of errors in the data collection process. Rare.\nIn some other cases, missing just means “zero” or “no”. In these instances, we should simply recode (replace) the missing values as “zero” or as “no”. (how many children? missing means zero)\nOften, values are missing for a reason.\n\nSome survey respondents may not know the answer to a question or refuse to answer it,\nThey are likely to be different from those who provide valid answers."
  },
  {
    "objectID": "rm-data/slides/week02.html#missing-values-what-can-we-do",
    "href": "rm-data/slides/week02.html#missing-values-what-can-we-do",
    "title": "Origins of Data, and Data Preparation",
    "section": "Missing values: what can we do?",
    "text": "Missing values: what can we do?\nTwo basic options:\n\nRestrict the analysis to observations with non-missing values for all variables used in the analysis.\n\nDefault option in many statistical packages.\n\nImputation - Fill in some value for the missing values, such as the mean or median.\n\nThere are more advanced and better options, but should be used with caution.\nNot all imputation methods are created equal.\n\nBe conservative, impute if absolutely necessary, and document it."
  },
  {
    "objectID": "rm-data/slides/week02.html#missing-values-some-practical-advice",
    "href": "rm-data/slides/week02.html#missing-values-some-practical-advice",
    "title": "Origins of Data, and Data Preparation",
    "section": "Missing values: Some practical advice",
    "text": "Missing values: Some practical advice\n\nFor binary variables: zero if yes/no.\nFor qualitative nominal variables, missing as a new value: white, blue, red and missing.\nFor ordinal variables, missing could be recoded to a neutral variable.\nFor quantitative variables -&gt; mean or median? (try not to! There are good imputation methods)\nif impute, create a flag and use it analysis. At the very least for sensitivity analysis.\n(Bad) Imputation will have consequences, be conservative."
  },
  {
    "objectID": "rm-data/slides/week02.html#more-on-data-cleaning",
    "href": "rm-data/slides/week02.html#more-on-data-cleaning",
    "title": "Origins of Data, and Data Preparation",
    "section": "More on Data Cleaning",
    "text": "More on Data Cleaning\n\nConsider the data quality, and the data collection process.\n\nAre there outliers? data entry errors? bunching?\n\nUnderstand the data generating process.\n\nAre there missing values? why?\n\nThis is an iterative process, and you may need to go back and forth between data cleaning and data analysis.\nMore on this with EDA and Data Visualization"
  },
  {
    "objectID": "rm-data/slides/week02.html#data-wrangling-cooking-recipe",
    "href": "rm-data/slides/week02.html#data-wrangling-cooking-recipe",
    "title": "Origins of Data, and Data Preparation",
    "section": "Data wrangling: Cooking recipe",
    "text": "Data wrangling: Cooking recipe\n\nWrite a code: it can be repeated, commented, cleaned, and improved later\nUnderstand the structure of the dataset, recognize links. Draw a schema\nStart by looking into the data to spot issues. (summarize, tabulate, edit, browse)\n\ndo they have meaningful ranges? Correct them or set them as missing\n\nStore data in tidy datasets. one row is one observation, one column a variable\nHave a description of variables (Labels, make sure you know what they are)\nIdentify missing values and store them in an appropriate format. Make edits if needed.\nDocument every step of data cleaning &lt;- Very Important and goes to the code"
  },
  {
    "objectID": "rm-data/slides/week02.html#ai-and-data-wrangling-upside",
    "href": "rm-data/slides/week02.html#ai-and-data-wrangling-upside",
    "title": "Origins of Data, and Data Preparation",
    "section": "AI and data wrangling: Upside",
    "text": "AI and data wrangling: Upside\nIf given the right instructions, and information, AI can help you with data wrangling:\n\nunderstands your data structure\ncombines datasets\nsummarizes the data\nunderstands your variables\nfinds potential problems\n\nBut, it is not perfect. You need to understand the data and the process. Review and control."
  },
  {
    "objectID": "rm-data/slides/week01/template.html",
    "href": "rm-data/slides/week01/template.html",
    "title": "My Economic Analysis",
    "section": "",
    "text": "This is a simple template for a report."
  },
  {
    "objectID": "rm-data/slides/week01/template.html#method-1",
    "href": "rm-data/slides/week01/template.html#method-1",
    "title": "My Economic Analysis",
    "section": "Method 1",
    "text": "Method 1"
  },
  {
    "objectID": "rm-data/research-proposals/proposal9.html",
    "href": "rm-data/research-proposals/proposal9.html",
    "title": "To Boldly Go: Resource Allocation and Post-Scarcity Economics in Star Trek",
    "section": "",
    "text": "Introduction\nThe Star Trek universe presents a vision of a future where scarcity has been largely eliminated through advanced technology. This research proposal aims to analyze the economic system depicted in Star Trek, focusing on resource allocation mechanisms, the nature of work, and the implications of technologies like replicators for economic theory.\n\n\nBackground and Research Question\nStar Trek depicts a future where the Federation has moved beyond money-based economies, thanks to technologies like matter replicators that can produce most goods on demand (Saadia 2016). This post-scarcity scenario challenges many fundamental assumptions of economics, which is traditionally defined as the study of resource allocation under conditions of scarcity (Margolis 1998).\nHowever, even in this abundant future, some forms of scarcity persist, such as scarce antimatter for starship fuel, and limited spots in Starfleet Academy. This mix of abundance and residual scarcity provides a unique lens through which to examine economic principles (Krauss 2007).\nMoreover, the show depicts various alien civilizations with different economic systems, allowing for comparative analysis. The Ferengi, for instance, represent an extreme form of capitalism, contrasting sharply with the Federation’s post-scarcity economy.\nMain Research Question: How does the post-scarcity economy depicted in Star Trek challenge and inform traditional economic theories of resource allocation and value?\nSecondary Research Questions:\n\nWhat mechanisms replace market-based resource allocation in Star Trek’s moneyless economy?\nHow does the nature of work and human motivation change in a post-scarcity environment?\nWhat insights does Star Trek offer about the transition from a scarcity-based to a post-scarcity economy?\n\n\n\nPotential Data Sources\n\nTV Series and Films: Scripts and scenes from various Star Trek series and movies\nStar Trek Technical Manuals: Detailed information about technologies in the Star Trek universe\nFan Wikis: Comprehensive databases of Star Trek lore\nEconomic Literature: Theories on post-scarcity economics and resource allocation\nFuturist Predictions: Technological forecasts related to automation and resource abundance\n\n\n\nPotential Approach\nWe will employ a mixed-methods approach, combining qualitative content analysis with economic modeling. First, we will conduct a systematic review of Star Trek series and films, coding for depictions of economic activities, resource allocation decisions, and technologies with economic implications.\nUsing this data, we will construct a model of the Federation’s economy, estimating key parameters such as production possibilities, resource constraints, and mechanisms for allocating scarce resources (e.g., starship assignments). We will then compare this model with various real-world economic systems and theoretical post-scarcity models.\nTo analyze the nature of work and motivation, we will conduct a comparative analysis between the depiction of work in Star Trek and real-world theories of work motivation and job satisfaction. We will also develop a theoretical model of skill development and career progression in a post-scarcity environment.\nFor the transition to post-scarcity, we will use the limited information provided in Star Trek about Earth’s history to construct a speculative model of economic transition, comparing it with real-world technological transitions and economic development theories.\n\n\nExpected Findings\nWe anticipate finding that Star Trek’s post-scarcity economy challenges many fundamental economic assumptions, potentially offering insights into alternative resource allocation mechanisms beyond market-based systems. We expect to see that while replicator technology eliminates scarcity for most goods, the allocation of truly scarce resources (like starships) involves complex decision-making processes that may inform real-world resource allocation theories.\nRegarding work and motivation, we expect to find that Star Trek depicts a shift from extrinsic to intrinsic motivation, potentially offering insights into how work might be restructured in highly automated future economies.\nWe also anticipate that the study of various alien economies in Star Trek will provide a rich comparative framework for understanding different economic systems and their cultural underpinnings.\n\n\nConclusion\nThis research will provide a novel perspective on economic theory by examining a fictional post-scarcity society. While based on a speculative future, the findings may offer valuable insights into potential long-term economic developments, particularly as we move towards increasing automation and resource efficiency. Moreover, this study could demonstrate the potential of using science fiction scenarios as thought experiments for economic theory and policy planning.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/startrek-economics\nThis repository will contain all data collection scripts, economic models, visualization code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nKrauss, Lawrence M. 2007. The Physics of Star Trek. Basic Books.\n\n\nMargolis, Howard. 1998. “Star Trek: Where No Economy Has Gone Before.” Reason 30 (5): 58.\n\n\nSaadia, Manu. 2016. Trekonomics: The Economics of Star Trek. Pipertext."
  },
  {
    "objectID": "rm-data/research-proposals/proposal7.html",
    "href": "rm-data/research-proposals/proposal7.html",
    "title": "The Iron Bank Always Collects: A Study of Debt and Financial Institutions in Game of Thrones",
    "section": "",
    "text": "Introduction\nGeorge R.R. Martin’s “A Song of Ice and Fire” series and its television adaptation “Game of Thrones” present a rich, complex world with its own economic systems. This research proposal aims to analyze the role of debt and financial institutions in the series, focusing on the Iron Bank of Braavos, to draw parallels with real-world economic history and financial systems.\n\n\nBackground and Research Question\nThe Iron Bank of Braavos plays a crucial role in the political economy of the Game of Thrones world, financing wars and influencing the rise and fall of regimes (McCaffrey 2018). Its operations bear similarities to historical institutions like the Medici Bank and modern central banks (Graeber 2011). The series depicts various forms of debt, from personal loans to sovereign debt, and their consequences, providing a fictional lens through which to examine real-world financial dynamics.\nThe economic aspects of Game of Thrones have been subject to some academic scrutiny (Hudson 2017), but a comprehensive analysis of its financial institutions and their parallels to real-world economic history is lacking. This research aims to fill this gap, using the fictional world as a case study to explore broader economic principles.\nMain Research Question: How do the depictions of debt and financial institutions in Game of Thrones reflect real-world historical and contemporary economic phenomena?\nSecondary Research Questions:\n\nWhat parallels can be drawn between the Iron Bank of Braavos and historical financial institutions?\nHow does sovereign debt in Game of Thrones compare to real-world sovereign debt crises?\nWhat insights does the series offer about the relationship between financial power and political authority?\n\n\n\nPotential Data Sources\n\nBook Series: “A Song of Ice and Fire” by George R.R. Martin\nTV Series: Scripts and scenes from “Game of Thrones”\nFan Wikis: Detailed information about the economic aspects of the Game of Thrones world\nHistorical Economic Data: Information on medieval and early modern financial systems\nContemporary Economic Data: Modern sovereign debt and banking system data\n\n\n\nPotential Approach\nWe will employ a mixed-methods approach, combining qualitative textual analysis with quantitative historical comparisons. First, we will conduct a systematic review of all mentions and depictions of the Iron Bank and debt in both the book series and TV show, coding for types of transactions, terms of loans, and consequences of default.\nUsing this data, we will construct a model of the Game of Thrones financial system, estimating key parameters such as interest rates, default risks, and the Bank’s impact on political stability. We will then compare this model with historical data on institutions like the Medici Bank and the Bank of England.\nTo analyze sovereign debt, we will use comparative case studies, contrasting the Crown’s debt to the Iron Bank with real-world sovereign debt crises. We will also employ network analysis to map the relationships between financial and political power in the series.\n\n\nExpected Findings\nWe anticipate finding significant parallels between the Iron Bank and historical financial institutions, particularly in terms of their political influence and role in state formation. We expect to see that sovereign debt in Game of Thrones mirrors many aspects of real-world sovereign debt, including the consequences of default and the relationship between debt and political legitimacy.\nWe also anticipate finding that the series offers insights into the interplay between financial and political power, potentially highlighting how control over credit can be as important as military might in shaping political outcomes.\n\n\nConclusion\nThis research will provide a novel perspective on financial history and institutions by examining them through the lens of a popular fantasy series. While based on a fictional setting, the findings may offer insights into real-world financial dynamics, particularly regarding the role of debt in political economy. Moreover, this study could demonstrate the potential of using popular media as a tool for economic education and analysis.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/ironbank-economics\nThis repository will contain all data collection scripts, economic models, visualization code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nGraeber, David. 2011. Debt: The First 5000 Years. Melville House.\n\n\nHudson, John. 2017. “Winter Is Coming: The Medieval World of Game of Thrones.” History Today 67 (7).\n\n\nMcCaffrey, Matthew. 2018. “The Economics of Game of Thrones.” The Independent Review 22 (4): 593–605."
  },
  {
    "objectID": "rm-data/research-proposals/proposal5.html",
    "href": "rm-data/research-proposals/proposal5.html",
    "title": "The Impact of Financial Literacy Programs on Household Savings and Investment Behavior",
    "section": "",
    "text": "Introduction\nFinancial literacy is increasingly recognized as a critical life skill in today’s complex economic environment. This research proposal aims to evaluate the effectiveness of financial literacy programs in improving household savings and investment behavior, with a focus on long-term financial well-being.\n\n\nBackground and Research Question\nFinancial literacy has been linked to better financial decision-making, increased savings, and improved economic outcomes (Lusardi and Mitchell 2014). However, financial literacy levels remain low in many countries, even among developed economies (Klapper, Lusardi, and Van Oudheusden 2015). In response, many governments and organizations have implemented financial literacy programs, but their effectiveness remains debated (Fernandes, Lynch Jr, and Netemeyer 2014).\nPrevious studies have examined the short-term effects of financial education, but less is known about its long-term impact on savings and investment behavior. Moreover, the heterogeneity in program effectiveness across different demographic groups and the mechanisms through which financial literacy affects behavior are not well understood (Kaiser and Menkhoff 2017).\nMain Research Question: How do financial literacy programs impact household savings and investment behavior in the long term?\nSecondary Research Questions:\n\nHow does the effectiveness of financial literacy programs vary across different demographic groups?\nWhat are the mechanisms through which financial literacy affects savings and investment behavior?\nHow do different types of financial literacy programs (e.g., school-based, workplace-based, community-based) compare in terms of effectiveness?\n\n\n\nPotential Data Sources\n\nFinancial Literacy Program Data: Information from government agencies or NGOs implementing these programs\nHousehold Finance Data: National household finance surveys or panel studies\nDemographic Data: National statistical offices\nFinancial Market Participation Data: National securities depositories or financial regulators\nBehavioral Data: Custom surveys or experiments conducted as part of the study\n\n\n\nPotential Approach\nWe will use a combination of quasi-experimental methods and randomized controlled trials (RCTs) to evaluate the impact of financial literacy programs. For existing programs, we will employ a difference-in-differences approach, comparing changes in financial behavior between program participants and non-participants over time.\nFor new programs, we will conduct RCTs, randomly assigning individuals or households to treatment (financial literacy program) and control groups. We will collect data on financial knowledge, attitudes, and behaviors before the program, immediately after, and at several points in the future to assess long-term effects.\nTo understand mechanisms, we will use mediation analysis, examining how changes in financial knowledge and attitudes mediate the effect of the program on financial behaviors. We will also use heterogeneity analysis to examine how program effects vary across different demographic groups.\n\n\nExpected Findings\nWe anticipate finding positive effects of financial literacy programs on savings rates and investment diversification, with stronger effects for more intensive and longer-duration programs. We expect to see heterogeneity in program effectiveness, with potentially larger impacts for individuals with lower initial financial literacy levels.\nWe also anticipate that the effects of financial literacy programs will be mediated by changes in financial attitudes and self-efficacy, in addition to increases in financial knowledge. We expect to find that different types of programs (school-based, workplace-based, community-based) may be more effective for different demographic groups.\n\n\nConclusion\nThis research will provide valuable insights into the effectiveness of financial literacy programs and their long-term impact on household financial behavior. The findings will have important implications for policymakers designing financial education initiatives, for educators developing financial literacy curricula, and for individuals seeking to improve their financial well-being.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/financial-literacy-impact\nThis repository will contain all data processing scripts, econometric models, experimental designs, visualization code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nFernandes, Daniel, John G Lynch Jr, and Richard G Netemeyer. 2014. “Financial Literacy, Financial Education, and Downstream Financial Behaviors.” Management Science 60 (8): 1861–83.\n\n\nKaiser, Tim, and Lukas Menkhoff. 2017. “Does Financial Education Impact Financial Literacy and Financial Behavior, and If so, When?” The World Bank Economic Review 31 (3): 611–30.\n\n\nKlapper, Leora, Annamaria Lusardi, and Peter Van Oudheusden. 2015. “Financial Literacy Around the World: Insights from the Standard & Poor’s Ratings Services Global Financial Literacy Survey.” World Bank.\n\n\nLusardi, Annamaria, and Olivia S Mitchell. 2014. “The Economic Importance of Financial Literacy: Theory and Evidence.” Journal of Economic Literature 52 (1): 5–44."
  },
  {
    "objectID": "rm-data/research-proposals/proposal3.html",
    "href": "rm-data/research-proposals/proposal3.html",
    "title": "The Impact of Climate Change on Agricultural Productivity: A Global Analysis",
    "section": "",
    "text": "Introduction\nClimate change poses significant challenges to global food security through its impact on agricultural productivity. This research proposal aims to quantify the effects of climate change on agricultural yields across different regions and crops, providing insights for adaptation strategies and policy interventions.\n\n\nBackground and Research Question\nAgriculture is highly sensitive to climate conditions, and changes in temperature and precipitation patterns can significantly affect crop yields (Lobell, Schlenker, and Costa-Roberts 2011). While some regions may benefit from warmer temperatures or increased CO2 levels, others face threats from extreme weather events, water scarcity, and shifting growing seasons (Rosenzweig et al. 2014).\nPrevious studies have examined the impact of climate change on agriculture at local or regional levels, but a comprehensive global analysis is needed to understand the full scope of the challenge. Moreover, the heterogeneous effects across different crops and regions need to be quantified to inform targeted adaptation strategies (Challinor et al. 2014).\nMain Research Question: How does climate change impact agricultural productivity across different regions and crops globally?\nSecondary Research Questions:\n\nWhich regions and crops are most vulnerable to climate change impacts?\nHow do adaptation measures mitigate the negative effects of climate change on agricultural productivity?\n\n\n\nPotential Data Sources\n\nAgricultural Data: FAO’s FAOSTAT database for crop yields and production\nClimate Data: World Bank’s Climate Change Knowledge Portal\nSoil Data: FAO’s Harmonized World Soil Database\nSocioeconomic Data: World Bank’s World Development Indicators\nAdaptation Measures: UNFCCC’s database on climate change adaptation measures\n\n\n\nPotential Approach\nWe will use a panel data approach, combining time-series and cross-sectional data on crop yields, climate variables, and other relevant factors across countries and regions. The main econometric model will be a fixed-effects regression, allowing us to control for time-invariant country-specific factors.\nTo address potential non-linear relationships between climate variables and crop yields, we will use flexible functional forms, such as polynomial terms or semi-parametric methods. We will also interact climate variables with indicators for adaptation measures to assess their effectiveness in mitigating climate impacts.\nTo account for spatial correlation in agricultural productivity and climate patterns, we will employ spatial econometric techniques. This will allow us to capture spillover effects and improve the precision of our estimates.\n\n\nExpected Findings\nWe anticipate finding significant negative impacts of climate change on global agricultural productivity, with substantial heterogeneity across regions and crops. We expect that regions with already warm climates and limited adaptive capacity will be most vulnerable. We also anticipate that certain adaptation measures, such as drought-resistant crop varieties or improved irrigation systems, will show effectiveness in mitigating negative impacts.\n\n\nConclusion\nThis research will provide a comprehensive global assessment of climate change impacts on agriculture, informing policy decisions on climate adaptation and food security. The findings will be crucial for identifying vulnerable regions and crops, prioritizing adaptation efforts, and developing targeted strategies to ensure global food security in the face of climate change.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/climate-change-agriculture\nThis repository will contain all data processing scripts, econometric models, visualization code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nChallinor, Andrew J, James Watson, David B Lobell, S Mark Howden, Daniel R Smith, and Netra Chhetri. 2014. “A Meta-Analysis of Crop Yield Under Climate Change and Adaptation.” Nature Climate Change 4 (4): 287–91.\n\n\nLobell, David B, Wolfram Schlenker, and Justin Costa-Roberts. 2011. “Climate Trends and Global Crop Production Since 1980.” Science 333 (6042): 616–20.\n\n\nRosenzweig, Cynthia, Joshua Elliott, Delphine Deryng, Alex C Ruane, Christoph Müller, Almut Arneth, Kenneth J Boote, et al. 2014. “Assessing Agricultural Risks of Climate Change in the 21st Century in a Global Gridded Crop Model Intercomparison.” Proceedings of the National Academy of Sciences 111 (9): 3268–73."
  },
  {
    "objectID": "rm-data/research-proposals/proposal10.html",
    "href": "rm-data/research-proposals/proposal10.html",
    "title": "For the Horde: Resource Competition and Virtual Economies in World of Warcraft",
    "section": "",
    "text": "Introduction\nWorld of Warcraft (WoW), one of the most popular massively multiplayer online role-playing games (MMORPGs), has created a complex virtual economy that mirrors many aspects of real-world economic systems. This research proposal aims to analyze the economic dynamics within WoW, focusing on resource competition, market structures, and the intersection between virtual and real-world economies.\n\n\nBackground and Research Question\nWorld of Warcraft’s virtual economy involves millions of players engaging in production, trade, and consumption of virtual goods (Castronova 2005). The game features scarce resources, a player-driven auction house, and even experiences inflation and market crashes (Dibbell 2006). This virtual economy provides a unique laboratory for studying economic behavior and testing economic theories (Castronova 2008).\nMoreover, the existence of “gold farming” - the practice of playing the game to earn virtual currency which is then sold for real money - creates interesting intersections between the virtual and real economies (Heeks 2009). This phenomenon raises questions about the nature of value and the boundaries between virtual and real economic activity.\nThe game also presents interesting scenarios of resource competition, both between players (e.g., competition for rare spawns) and between factions (Horde vs. Alliance), which can be analyzed through the lens of game theory and resource economics.\nMain Research Question: How do the economic dynamics in World of Warcraft reflect and differ from real-world economic systems, and what insights can they provide for economic theory and policy?\nSecondary Research Questions:\n\nHow does resource competition in WoW compare to real-world resource competition, and what strategies emerge?\nWhat factors influence inflation and market stability in the WoW economy?\nHow does the intersection of virtual and real economies in WoW challenge traditional notions of economic value and activity?\n\n\n\nPotential Data Sources\n\nIn-game Economic Data: Auction house prices, resource spawn rates, etc. (potentially through API access or data scraping)\nPlayer Surveys: Custom surveys on economic behavior and decision-making in WoW\nWoW Forums and Wikis: Player discussions and documentation of economic strategies\nAcademic Literature: Existing studies on virtual economies and WoW\nReal-world Economic Data: For comparison with WoW economic trends\n\n\n\nPotential Approach\nWe will employ a mixed-methods approach, combining quantitative analysis of in-game economic data with qualitative analysis of player behavior and strategies. First, we will collect and analyze time-series data on prices, trade volumes, and resource availability in the WoW economy, using econometric techniques to identify trends and patterns.\nTo study resource competition, we will use game theory models to analyze player strategies around contested resources. We will also conduct surveys and interviews with players to understand their economic decision-making processes and strategies.\nFor the intersection of virtual and real economies, we will analyze the market for “gold farming” services, examining factors that influence exchange rates between virtual and real currencies. We will also explore the legal and ethical implications of this intersection.\nTo compare WoW’s economy with real-world economies, we will use comparative analysis, looking at factors such as inflation rates, market concentration, and responses to economic shocks.\n\n\nExpected Findings\nWe anticipate finding that the WoW economy exhibits many features of real-world economies, including market cycles, inflation, and emergent economic strategies. We expect to see that resource competition in WoW leads to complex strategies that may offer insights into real-world resource economics.\nRegarding the virtual-real economy intersection, we expect to find that the value of virtual currencies is influenced by both in-game factors and real-world economic conditions, challenging traditional notions of economic value.\nWe also anticipate finding that the controlled environment of WoW allows for clearer observation of certain economic phenomena, potentially offering insights that could inform real-world economic policy and theory.\n\n\nConclusion\nThis research will provide a novel perspective on economic dynamics by examining them in a virtual world. While based on a game environment, the findings may offer valuable insights into real-world economic phenomena, particularly in areas such as resource competition, market behavior, and the increasing digitalization of economic activity. Moreover, this study could demonstrate the potential of using virtual worlds as laboratories for economic research and policy experimentation.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/warcraft-economics\nThis repository will contain all data collection scripts, economic models, visualization code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nCastronova, Edward. 2005. Synthetic Worlds: The Business and Culture of Online Games. University of Chicago Press.\n\n\n———. 2008. “A Test of the Law of Demand in a Virtual World: Exploring the Petri Dish Approach to Social Science.” CESifo Working Paper Series.\n\n\nDibbell, Julian. 2006. Play Money: Or, How i Quit My Day Job and Made Millions Trading Virtual Loot. Basic Books.\n\n\nHeeks, Richard. 2009. “Understanding \"Gold Farming\" and Real-Money Trading as the Intersection of Real and Virtual Economies.” Journal of Virtual Worlds Research 2 (4)."
  },
  {
    "objectID": "rm-data/quizes/Quiz-grade.html",
    "href": "rm-data/quizes/Quiz-grade.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Lazaroes 4 4 Joe 4 5 Emi 4 5 Chenning 4 5 Brendon 4 5 Shane 4 5 Kailin 3 0\nq4 q5\nLazaroes q5=1 q4=0\nJoe q5=5 q4=5 Emi q5=0 q4=5 Chenning q5=5 q4=5 Brendon q5=5 q4=5 Shane q5=5 q4=4 Kailin q5=4 q4=4"
  },
  {
    "objectID": "rm-data/playlist.html",
    "href": "rm-data/playlist.html",
    "title": "Data Analysis in Economics",
    "section": "",
    "text": "Econometrics Masterclass\n            \n                Based on \"Data Analysis for Economic and Policy\"\n                \n                    Episodes created using NotebookLM by Google\n                \n            \n        \n\n        \n            \n                \n            \n\n            \n                \n                \n                \n                \n                \n                    \n                \n                \n                    0:00\n                    0:00"
  },
  {
    "objectID": "rm-data/index.html",
    "href": "rm-data/index.html",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "",
    "text": "The detailed class syllabus is available here.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#syllabus",
    "href": "rm-data/index.html#syllabus",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "",
    "text": "The detailed class syllabus is available here.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#instructor-information",
    "href": "rm-data/index.html#instructor-information",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Instructor Information",
    "text": "Instructor Information\n\nInstructor: Fernando Rios-Avila\nEmail: friosavi@levy.org\nOffice Hours: Wednesdays 1:30pm to 4:00pm, or by appointment. Other times can be arranged remotely.\nClass Time: Wednesday, 9:30 am - 12:45 pm",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#course-description",
    "href": "rm-data/index.html#course-description",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Course Description",
    "text": "Course Description\nThis course focuses on providing students with the tools and skills necessary to conduct data analysis for economics and policy research. Students will be exposed to the entire process of data analysis, from formulating questions and collecting data to cleaning, exploring, analyzing, and presenting results. The course covers exploratory data analysis, regression analysis, and introduces topics on prediction with machine learning. Students will gain hands-on experience using Stata, with Quarto for reproducible reporting, and GitHub for version control and collaboration.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#course-objectives",
    "href": "rm-data/index.html#course-objectives",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Course Objectives",
    "text": "Course Objectives\nBy the end of this course, students will be able to:\n\nApply advanced data analysis techniques to economic and policy questions.\nUse modern tools such as GitHub and Quarto for research collaboration and reproducibility.\nFormulate research questions and design appropriate data collection methods.\nClean, organize, and explore data using various techniques and visualizations.\nApply regression analysis techniques to analyze relationships between variables.\nUse machine learning methods for prediction and classification tasks.\nImplement data analysis techniques using Stata.\nEffectively communicate research findings through written reports and oral presentations.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#required-textbook",
    "href": "rm-data/index.html#required-textbook",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Required Textbook",
    "text": "Required Textbook\nBékés, G., & Kézdi, G. (2021). Data Analysis for Business, Economics, and Policy. Cambridge University Press.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#software-requirements",
    "href": "rm-data/index.html#software-requirements",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Software Requirements",
    "text": "Software Requirements\n\nStata: A student license will be provided.\nQuarto: Free and open-source software for reproducible research.\nVSCode: Free and open-source code editor.\nGitHub/GitHub-Desktop: Free platform for version control and collaboration.\nZotero: Free reference manager.\n\n\n\n\n\n\n\nImportant\n\n\n\nAll homework assignments are required to be submitted in Quarto format, using GitHub repositories to submit the assignments.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#audio-podcasts",
    "href": "rm-data/index.html#audio-podcasts",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Audio Podcasts",
    "text": "Audio Podcasts\nIf you are interested in listening to an audio-podcast like summary of the chapters, you can find them here.\nThe audiofiles were created using NotebookLM by Google. The audiofiles are generated using the text from the book. The audiofiles are not perfect, but they can be useful to listen to the content of the book while you are doing other activities.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#course-outline",
    "href": "rm-data/index.html#course-outline",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Course Outline",
    "text": "Course Outline\n\nPart I: Introduction to Modern Research Tools\n\nWeek 1: Course Overview and Tools Setup\n\nIntroduction to GitHub and Quarto\nData Organization and Management\nSlides\nHomework 1\n\n\n\n\nPart II: Data Analysis and Exploration\n\nWeek 2: Introduction to Data Analysis\n\nData Collection and Preparation\nTidy Data Principles\nReading: Békés & Kézdi (2021), Chapters 1-2\nSlides\nHomework 2\n\n\n\nWeek 3: Data Exploration\n\nExploratory Data Analysis Techniques\nData Cleaning and Tidy Data Principles\nReading: Békés & Kézdi (2021), Chapters 3-4\nSlides\nHomework 3\n\n\n\n\nPart III: Generalization and Regression Analysis\n\nWeek 4: Generalization: From Sample to Population\n\nSampling and Hypothesis Testing\nConfidence Intervals and Errors\nReading: Békés & Kézdi (2021), Chapters 5-6\nSlides\nHomework 4\n\n\n\nWeeks 5-6: Regression Analysis I: Simple Regression\n\nDate: October 9 and 16\nLinear Regression and Causality\nModel Assumptions and Transformations\nReading: Békés & Kézdi (2021), Chapters 7-9\nSlides\nHomework 5\nHomework 6\nAudio Summary v1. Chapter 7\n\n\n\nWeek 7: Regression Analysis II: Multiple Regression\n\nDate: October 18 (Make-up class)\nEstimation and Inference\nInteractions and Non-linearities\nReading: Békés & Kézdi (2021), Chapter 10\nSlides\nHomework 7\n\n\n\nWeek 8: Regression Analysis III: Modeling Probabilities\n\nDate: October 23\nLogit and Probit Models\nInterpretation and Predictive Power\nReading: Békés & Kézdi (2021), Chapter 11\nSlides\nHomework 8\n\n\n\n\nPart IV: Advanced Topics\n\nWeek 9: Time Series Analysis\n\nDate: October 30\nTrend, Seasonality, and Stationarity\nReading: Békés & Kézdi (2021), Chapter 12\nSlides\nHomework 9\n\n\n\nWeek 10: Prediction\n\nDate: November 6\nModel Fit and Cross-validation\nReading: Békés & Kézdi (2021), Chapter 13\nSlides\nHomework 10\n\n\n\nWeek 11: Model Building for Prediction: LASSO\n\nDate: November 13\nLASSO for Prediction and Diagnosis\nReading: Békés & Kézdi (2021), Chapter 14\nSlides\nHomework 11\n\n\n\nWeek 12: Predicting Probabilities and Classification\n\nDate: November 20\nClassification Techniques and ROC Curves\nReading: Békés & Kézdi (2021), Chapter 17\nSlides\nHomework 12\n\n\n\nWeek 13: Forecasting Data\n\nDate: November 27 (TBD)\nARIMA and Forecasting Techniques\nReading: Békés & Kézdi (2021), Chapter 18\nSlides\nHomework 13",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#grading-policy",
    "href": "rm-data/index.html#grading-policy",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Grading Policy",
    "text": "Grading Policy\n\nWeekly Quizzes: 10%\nWeekly Problem Sets: 30%\nTerm Paper: 60%\n\n\nTerm Paper Schedule (60% of final grade)\n\nPart I: Research Proposal (5%, due Week 2). See here for few examples of research proposals.\nPart II: Data Collection and Cleaning (10%, due Week 4)\nInterim Progress Report (5%, due Week 7): October 18\nPeer Review Report: (Week 8): October 23\nPart III: Data Analysis (15%, due Week 10): November 6\nPeer Review Report: (Week 11): November 13\nPart IV: (15%) Final Report: December 2\nPresentation (5%) due Last Date of Class: December 4 or 11\n\nComplete research paper should include:\n\nIntroduction\nLiterature Review\nData and Methodology\nRobustness Checks or Sensitivity/Sub-group Analysis\nConclusion\nReferences\nAppendices (if any)\nPresentation (5%)\n\n15-minute presentation of your research to the class\n\nSee here for an example for the kind of report expected at each stage, based on the first research proposal on the impact of remote work on urban housing prices.\n\n\n\nAdditional Requirements\n\nYou should submit a PDF of your report by the deadline, along with the Github repository link\nYour GitHub repository should include all code, data, and the Quarto document for your report\nAt each stage, you should submit your work to GitHub to follow the progress of your project",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#resources",
    "href": "rm-data/index.html#resources",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Resources",
    "text": "Resources\n\nTextbook: Békés, G., & Kézdi, G. (2021). Data Analysis for Business, Economics, and Policy. Cambridge University Press. Additional resources are available on the book’s website: Data Analysis",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#course-policies",
    "href": "rm-data/index.html#course-policies",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance: Attendance is highly recommended. Classes will not be recorded, and except for exceptional cases, there will be no online classes.\nLate Assignments: Late assignments will not be accepted unless prior arrangements have been made with the instructor.\nAcademic Integrity: All work submitted must be your own. Plagiarism will not be tolerated and will result in a failing grade for the assignment or course.\nAI Usage: The use of AI in the class is allowed. However, you must disclose any AI tools used in your assignments. AI is a tool you can use to generate ideas, edit your text, provide help with coding, etc. However, it is completely unacceptable to use AI to generate the entire assignment. You will have to be able to explain and defend your work in class.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/hw-material/report6.html",
    "href": "rm-data/hw-material/report6.html",
    "title": "Economic Dynamics in Eric Ugland’s ‘The Good Guys’ Series",
    "section": "",
    "text": "This report examines the economic systems and dynamics present in Eric Ugland’s popular LitRPG series, “The Good Guys.” We will explore how the author integrates economic principles into the game-like world, analyzing their impact on character development and plot progression. The report will include a mathematical model of the in-game economy, a visual representation of economic trends, and a comparative analysis of different economic zones within the series."
  },
  {
    "objectID": "rm-data/hw-material/report6.html#the-mathematical-foundation-of-the-in-game-economy",
    "href": "rm-data/hw-material/report6.html#the-mathematical-foundation-of-the-in-game-economy",
    "title": "Economic Dynamics in Eric Ugland’s ‘The Good Guys’ Series",
    "section": "The Mathematical Foundation of the In-Game Economy",
    "text": "The Mathematical Foundation of the In-Game Economy\nIn “The Good Guys,” the in-game economy is governed by a complex system of resource generation, currency valuation, and market dynamics. We can model the basic economic growth within the game using a modified version of the Solow-Swan model, as shown in Equation 1:\n\\[\nY(t) = K(t)^\\alpha (A(t)L(t))^{1-\\alpha} e^{\\beta Q(t)}\n\\tag{1}\\]\nWhere:\n\nY(t) is the total production in the game world at time t\nK(t) is the capital stock\nA(t) is the level of technology\nL(t) is the labor force (players and NPCs)\nQ(t) represents the impact of quests and missions\n\\(\\alpha\\) and \\(\\beta\\) are constants representing the elasticity of output with respect to capital and quests, respectively\n\nThis equation demonstrates how the unique elements of the game world, such as quests (Q), interact with traditional economic factors to drive growth and development."
  },
  {
    "objectID": "rm-data/hw-material/report6.html#economic-trends-across-game-zones",
    "href": "rm-data/hw-material/report6.html#economic-trends-across-game-zones",
    "title": "Economic Dynamics in Eric Ugland’s ‘The Good Guys’ Series",
    "section": "Economic Trends Across Game Zones",
    "text": "Economic Trends Across Game Zones\nTo visualize the economic disparities between different zones in the game world, we’ve compiled data on average player wealth across five major regions. Figure 1 illustrates these differences:\n\n\n\n\n\n\n\n\nFigure 1: Average Player Wealth Across Game Zones\n\n\n\n\n\nAs evident from Figure 1, there is a significant increase in average player wealth as they progress through the game zones. This economic progression serves as a motivator for players to advance in the game, mirroring real-world economic incentives1."
  },
  {
    "objectID": "rm-data/hw-material/report6.html#comparative-analysis-of-economic-systems",
    "href": "rm-data/hw-material/report6.html#comparative-analysis-of-economic-systems",
    "title": "Economic Dynamics in Eric Ugland’s ‘The Good Guys’ Series",
    "section": "Comparative Analysis of Economic Systems",
    "text": "Comparative Analysis of Economic Systems\nThe diverse economic systems present in “The Good Guys” series offer a rich ground for analysis. Table 1 provides a comparison of these systems:\n\n\n\nTable 1: Comparison of Economic Systems in “The Good Guys”\n\n\n\n\n\n\n\n\n\n\n\nEconomic System\nPrimary Currency\nMain Economic Activities\nPlayer Impact\n\n\n\n\nStarting Town\nCopper Coins\nBasic trade, simple quests\nLow\n\n\nMerchant City\nGold Coins\nComplex trade, investments\nHigh\n\n\nDragon’s Lair\nDragon Scales\nRare item trade, high-risk ventures\nVery High\n\n\n\n\n\n\nAs shown in Table 1, the economic systems become more complex and impactful as players progress, offering increasing opportunities for wealth accumulation and economic strategy."
  },
  {
    "objectID": "rm-data/hw-material/report6.html#footnotes",
    "href": "rm-data/hw-material/report6.html#footnotes",
    "title": "Economic Dynamics in Eric Ugland’s ‘The Good Guys’ Series",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis economic progression system is a common feature in many RPGs and LitRPG novels, serving as a form of “gamified capitalism” that keeps players engaged and motivated.↩︎"
  },
  {
    "objectID": "rm-data/hw-material/report4.html",
    "href": "rm-data/hw-material/report4.html",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "",
    "text": "This report explores the emerging genre of LitRPG (Literary Role-Playing Game), a unique fusion of literature and gaming elements. We will examine the key characteristics of LitRPG, its growing popularity, and its impact on both the literary and gaming worlds. The report includes an analysis of typical LitRPG progression systems, reader engagement statistics, and future trends in the genre."
  },
  {
    "objectID": "rm-data/hw-material/report4.html#defining-litrpg",
    "href": "rm-data/hw-material/report4.html#defining-litrpg",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "Defining LitRPG",
    "text": "Defining LitRPG\nLitRPG is a literary genre that incorporates elements of role-playing games into the narrative structure. According to Johnson (2021), the genre typically features characters progressing through a game-like world, complete with statistics, levels, and skill trees. This unique blend of storytelling and gaming mechanics has led to a surge in popularity among readers who enjoy both literature and video games."
  },
  {
    "objectID": "rm-data/hw-material/report4.html#character-progression-in-litrpg",
    "href": "rm-data/hw-material/report4.html#character-progression-in-litrpg",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "Character Progression in LitRPG",
    "text": "Character Progression in LitRPG\nOne of the key features of LitRPG is the quantifiable progression of characters. This is often represented by a character’s stats or attributes, which can be expressed mathematically. A common formula for calculating a character’s overall power level in many LitRPG novels is:\n\\[\nPower Level = \\sqrt{(Strength + Agility) \\times Intelligence} \\times Level\n\\tag{1}\\]\nThis equation (Equation 1) demonstrates how various attributes contribute to a character’s overall capabilities, providing readers with a tangible sense of growth and achievement."
  },
  {
    "objectID": "rm-data/hw-material/report4.html#reader-engagement-in-litrpg",
    "href": "rm-data/hw-material/report4.html#reader-engagement-in-litrpg",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "Reader Engagement in LitRPG",
    "text": "Reader Engagement in LitRPG\nThe unique structure of LitRPG novels has led to high levels of reader engagement. To illustrate this, we conducted a survey of 1000 LitRPG readers, asking them to rate their engagement levels compared to traditional fantasy novels.\n\n\n\n\n\n\n\n\nFigure 1: Reader Engagement: LitRPG vs Traditional Fantasy\n\n\n\n\n\nAs shown in Figure 1, LitRPG novels tend to generate higher levels of reader engagement compared to traditional fantasy novels. This increased engagement can be attributed to the interactive elements and clear progression systems inherent in LitRPG stories."
  },
  {
    "objectID": "rm-data/hw-material/report4.html#popular-litrpg-subgenres",
    "href": "rm-data/hw-material/report4.html#popular-litrpg-subgenres",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "Popular LitRPG Subgenres",
    "text": "Popular LitRPG Subgenres\nThe LitRPG genre has spawned several popular subgenres, each with its own unique characteristics and fan base. The table below outlines some of the most prominent subgenres:\n\n\n\nTable 1: Popular LitRPG Subgenres\n\n\n\n\n\nSubgenre\nDescription\nPopularity Rating\n\n\n\n\nDungeon Core\nProtagonist is a dungeon\n8/10\n\n\nVR LitRPG\nSet in virtual reality\n9/10\n\n\nApocalypse LitRPG\nReal world becomes game-like\n7/10\n\n\nCultivation LitRPG\nBased on Eastern cultivation novels\n8/10\n\n\n\n\n\n\nAs seen in Table 1, VR LitRPG is currently the most popular subgenre, likely due to its relatability and connection to current technological trends1."
  },
  {
    "objectID": "rm-data/hw-material/report4.html#footnotes",
    "href": "rm-data/hw-material/report4.html#footnotes",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe popularity of VR LitRPG may also be influenced by the growing interest in virtual reality technology in the real world.↩︎"
  },
  {
    "objectID": "rm-data/hw-material/report2.html",
    "href": "rm-data/hw-material/report2.html",
    "title": "The Strategic Depth of StarCraft and Its Esports Legacy",
    "section": "",
    "text": "StarCraft, a real-time strategy game developed by Blizzard Entertainment, has left an indelible mark on the esports landscape. This report explores the strategic complexity of StarCraft and its enduring impact on competitive gaming. We will examine the game’s mechanics, its influence on player decision-making, and its role in shaping the modern esports industry."
  },
  {
    "objectID": "rm-data/hw-material/report2.html#player-decision-making-and-apm",
    "href": "rm-data/hw-material/report2.html#player-decision-making-and-apm",
    "title": "The Strategic Depth of StarCraft and Its Esports Legacy",
    "section": "Player Decision-Making and APM",
    "text": "Player Decision-Making and APM\nOne of the key metrics in competitive StarCraft is Actions Per Minute (APM). This measure reflects a player’s ability to execute complex strategies rapidly. Kim et al. (2016) found a strong correlation between APM and player skill level. The following figure illustrates this relationship:\n\n\n\n\n\n\n\n\nFigure 1: Correlation between APM and Player Skill Rating\n\n\n\n\n\nAs shown in Figure 1, there is a clear positive correlation between a player’s skill rating and their APM. This relationship highlights the importance of both strategic thinking and mechanical skill in competitive StarCraft."
  },
  {
    "objectID": "rm-data/hw-material/report2.html#starcraft-tournaments-and-prize-pools",
    "href": "rm-data/hw-material/report2.html#starcraft-tournaments-and-prize-pools",
    "title": "The Strategic Depth of StarCraft and Its Esports Legacy",
    "section": "StarCraft Tournaments and Prize Pools",
    "text": "StarCraft Tournaments and Prize Pools\nThe popularity of StarCraft as an esport is evident in the number and scale of tournaments organized worldwide. The following table shows the top StarCraft tournaments by prize pool:\n\n\n\nTable 1: Top StarCraft Tournaments by Prize Pool\n\n\n\n\n\nTournament Name\nYear\nPrize Pool (USD)\nWinner\n\n\n\n\nWCG 2005\n2005\n$75,000\nLi “Sky” Xiaofeng\n\n\nBlizzCon 2007\n2007\n$100,000\nYoan “ToD” Merlo\n\n\nWCG 2009\n2009\n$200,000\nJae Ho “Moon” Jang\n\n\n\n\n\n\nAs seen in Table 1, the prize pools for StarCraft tournaments have grown significantly over time, reflecting the game’s increasing popularity and the growth of the esports industry as a whole."
  },
  {
    "objectID": "rm-data/hw-material/report2.html#footnotes",
    "href": "rm-data/hw-material/report2.html#footnotes",
    "title": "The Strategic Depth of StarCraft and Its Esports Legacy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGame-tree complexity is a measure used in game theory to quantify the number of possible game states in a given game. It provides a way to compare the complexity of different games mathematically.↩︎"
  },
  {
    "objectID": "rm-data/hw-material/report1-example.html",
    "href": "rm-data/hw-material/report1-example.html",
    "title": "Example Replication Report",
    "section": "",
    "text": "asdas This template replicates the structure and content of the report “The Impact of Resource Management in StarCraft: A Strategic Analysis” by Kathryn Janeway.\nIt considers all elements of the original report, but does not include the image for replication.\n\n---\ntitle: \"The Impact of Resource Management in StarCraft: A Strategic Analysis\"\nformat: \n    pdf:\n        documentclass: article \n        number-sections: true\n        margin-top: 1in\n        margin-bottom: 1in\n        margin-left: 1in\n        margin-right: 1in\n        linestretch:  1.5\n        fontsize: 12pt\n    html: default\nexecute: \n  echo: false\n  warning: false     \nauthor: \"Kathryn Janeway\"        \nbibliography: references.bib\n---\n\n# Introduction\n\nThis report examines the crucial role of resource management in the popular real-time strategy game StarCraft. We will explore how effective resource allocation influences gameplay dynamics and strategic decision-making. The analysis will include a mathematical model of resource gathering, a visualization of unit production rates, and a comparison of resource types across different races.\n\n# Resource Dynamics in StarCraft\n\n## Mathematical Model of Resource Gathering\n\nIn StarCraft, the rate of resource accumulation can be modeled using a simple differential equation. If we denote the amount of resources as $R$ and time as $t$, we can express the rate of change of resources as:\n\n$$\n\\frac{dR}{dt} = \\alpha N - \\beta P\n$${#eq-resource}\n\nWhere $\\alpha$ is the gathering rate per worker, $N$ is the number of workers, $\\beta$ is the consumption rate, and $P$ is the production rate of units or structures. This model, as shown in @eq-resource, forms the basis of the game's economic system [@choi2015].\n\n## Unit Production Rates\n\nTo illustrate the impact of resource management on unit production, we've created a visualization of unit production rates for different races in StarCraft.\n\n:::{#fig-production fig-pos=\"H\" }\n\n![](figure_production.png)\n\nUnit Production Rates by Race in StarCraft\n\n:::\n\nAs shown in @fig-production, the Zerg race has the highest unit production rate, reflecting their swarm-based strategy. This aligns with the game's design philosophy, where each race has unique strengths and weaknesses[^1].\n\n## Resource Types Comparison\n\nStarCraft features two primary resource types: minerals and vespene gas. Their availability and usage vary across races:\n\n:::{#tbl-resources tbl-pos=\"H\"}\n\n| Race    | Mineral Usage | Gas Usage | Resource Dependency |\n|---------|---------------|-----------|---------------------|\n| Terran  | High          | Medium    | Balanced            |\n| Protoss | Medium        | High      | Gas-heavy           |\n| Zerg    | High          | Low       | Mineral-heavy       |\n\nResource Usage by Race \n\n:::\n\n@tbl-resources illustrates how different races prioritize resources, influencing their strategic options and tech progression paths.\n\n# Strategic Implications\n\nThe resource management system in StarCraft creates a complex strategic landscape. Players must balance immediate needs with long-term goals, deciding whether to invest in economy, technology, or military strength. According to @kim2010, successful players often demonstrate superior resource management skills, which translate into strategic advantages on the battlefield.\n\n# Conclusion\n\nResource management is a cornerstone of StarCraft gameplay, deeply influencing strategic decisions and overall game outcomes. The mathematical model, production rate analysis, and resource usage comparison presented in this report highlight the intricate balance between gathering, allocation, and consumption of resources. Understanding these dynamics is crucial for players aiming to master the game and for game designers seeking to create balanced and engaging strategy games.\n\n[^1]: This design approach contributes to StarCraft's enduring popularity in esports and casual gaming circles.\n\n# References\n\n::: {#refs}\n:::"
  },
  {
    "objectID": "rm-data/homework.html",
    "href": "rm-data/homework.html",
    "title": "HomeWorks",
    "section": "",
    "text": "This document provides the homework for the each week of the course."
  },
  {
    "objectID": "rm-data/homework.html#hw01",
    "href": "rm-data/homework.html#hw01",
    "title": "HomeWorks",
    "section": "Homework 1",
    "text": "Homework 1\nChoose one of the following topics, and create a PDF version of it using Quarto. For this, create a new repository in your GitHub account, this repository should be named homework_1, and contain the QMD file that you will use to create the PDF.\nFor this homework use the following resources\n\nTemplate: template html\n\nJust copy the Heading of this file in your QMD file\n\nBibliography: reference.bib\n\nAdd this in the same folder as your QMD file\n\nAll figures, if any, can be saved as PNG or JPG files from the linked pages.\nTables, if any, may have to be replicated using markdown tables.\n\nSubmit the link to your repository, the PDF and QMD files via email.\n\nTopics\n\nThe Impact of Resource Management in StarCraft: A Strategic Analysis My replication\n\nhtml\npdf\nqmd\n\nThe Strategic Depth of StarCraft and Its Esports Legacy html pdf\nThe Mathematics of Dungeons and Dragons: A Statistical Adventure html pdf\nThe Rise of LitRPG: Blending Literature and Gaming html pdf\nThe Impact of ‘The Good Guys’ on Modern Fantasy Literature html pdf\nEconomic Dynamics in Eric Ugland’s ‘The Good Guys’ Series html pdf\nThe Impact of House Allegiances on Power Dynamics in Westeros html pdf"
  },
  {
    "objectID": "rm-data/homework.html#hw02",
    "href": "rm-data/homework.html#hw02",
    "title": "HomeWorks",
    "section": "Homework 2",
    "text": "Homework 2\n\nDownload country–year panel data on three variables (“indicators”) of your choice from the World Bank website https://data.worldbank.org/, or using their API program (wbdata).\nOnce you have the data, clean it up and save a tidy version of it in a Stata file.\nIndicate which countries are ranked highest and lowest in each of the three indicators in the year 2000.\nWrite a short report on what you did to obtain the data, how many countries and years you ended up with in the data (after cleannig), and what difficulties you encountered, if any.\n\nIt is not necessary to download the data directly into Stata. You can download the data in CSV format, excel, and then Copy-paste or import it into Stata."
  },
  {
    "objectID": "rm-data/homework.html#hw03",
    "href": "rm-data/homework.html#hw03",
    "title": "HomeWorks",
    "section": "Homework 3",
    "text": "Homework 3\n\nChoose the same 2016/7 season from the football dataset as in data exercise (Book) and produce a different table showing the extent of home team advantage. Compare the results and discuss what you find.\n\nor\n\nUsing the wms-management-survey dataset, pick a country different from Mexico, reproduce all figures and tables of the Book case study, and compare your results to what was found for Mexico."
  },
  {
    "objectID": "rm-data/homework.html#hw04",
    "href": "rm-data/homework.html#hw04",
    "title": "HomeWorks",
    "section": "Homework 4",
    "text": "Homework 4\nNothing here"
  },
  {
    "objectID": "rm-data/homework.html#hw05",
    "href": "rm-data/homework.html#hw05",
    "title": "HomeWorks",
    "section": "Homework 5",
    "text": "Homework 5\nNothing here"
  },
  {
    "objectID": "rm-data/homework.html#hw06",
    "href": "rm-data/homework.html#hw06",
    "title": "HomeWorks",
    "section": "Homework 6",
    "text": "Homework 6\nq1: Use the wms-management-survey dataset and pick a country. (Not two students should pick the same country). Estimate a linear regression with the management quality score (X) and employment (Y). Interpret the slope coefficient, create its 95% CI, and interpret that, too. Explore potential nonlinearities in the patterns of association by lpoly. Estimate a regression that can capture those nonlinearities, and carry out a test to see if you can reject that the linear approximation was good enough for the population of firms represented by the data."
  },
  {
    "objectID": "rm-data/homework.html#hw07",
    "href": "rm-data/homework.html#hw07",
    "title": "HomeWorks",
    "section": "Homework 7",
    "text": "Homework 7"
  },
  {
    "objectID": "rm-data/homework.html#hw08",
    "href": "rm-data/homework.html#hw08",
    "title": "HomeWorks",
    "section": "Homework 8",
    "text": "Homework 8"
  },
  {
    "objectID": "rm-data/homework.html#hw09",
    "href": "rm-data/homework.html#hw09",
    "title": "HomeWorks",
    "section": "Homework 9",
    "text": "Homework 9\nUse the stocks-prices dataset that can be found here. This dataset contains the closing prices for the SP500, Apple, Disney, GameStop, Meta and Nvidia at the end of the month. Using this data, choose one of the companies and reproduce the analysis of the Returns on a company stock and market returns case study (for example SP500 and Apple).\nThe full case study can be found here. You need to produce the same tables and figures as in the case study, with brief explanations of what you find. No need to use the Daily data, just the Monthly data."
  },
  {
    "objectID": "rm-data/homework.html#hw10",
    "href": "rm-data/homework.html#hw10",
    "title": "HomeWorks",
    "section": "Homework 10",
    "text": "Homework 10\nUse the data on house prices here. This dataset contains the prices of houses in a city, along with many other house charcteristics. You are obtaining 80% of the data to estimate the model. Build a model that you think its best to predict the price of a house. You can use any of the variables in the dataset, and you can create new variables if you think they are useful.\nDescribe at least 3 different specifications you tried, and which one you choose and why. The one with the best fit on the test data (I have access to) can skip next homework."
  },
  {
    "objectID": "rm-data/homework.html#hw11",
    "href": "rm-data/homework.html#hw11",
    "title": "HomeWorks",
    "section": "Homework 11",
    "text": "Homework 11"
  },
  {
    "objectID": "rm-data/homework.html#hw12",
    "href": "rm-data/homework.html#hw12",
    "title": "HomeWorks",
    "section": "Homework 12",
    "text": "Homework 12"
  },
  {
    "objectID": "rm-data/homework.html#hw13",
    "href": "rm-data/homework.html#hw13",
    "title": "HomeWorks",
    "section": "Homework 13",
    "text": "Homework 13"
  },
  {
    "objectID": "rm-data/example_proposals.html",
    "href": "rm-data/example_proposals.html",
    "title": "Example of Research Proposals",
    "section": "",
    "text": "This page contains few examples of research proposals. The goal is to give you an idea of what expect your research proposal to look like for the term paper.\n\nThe Impact of Remote Work on Urban Housing Prices link\nThe Effect of Social Media Sentiment on Stock Market Volatility link\nThe Impact of Climate Change on Agricultural Productivity: A Global Analysis link\nThe Economic Impact of Artificial Intelligence Adoption in Small and Medium Enterprises link\nThe Impact of Financial Literacy Programs on Household Savings and Investment Behavior link\nThe Economics of Springfield: A Case Study of Resource Allocation in The Simpson link\nThe Iron Bank Always Collects: A Study of Debt and Financial Institutions in Game of Thrones link\nThe Economics of the Matrix: Scarcity, Choice, and Human Capital in a Simulated Reality link\nTo Boldly Go: Resource Allocation and Post-Scarcity Economics in Star Trek link\nFor the Horde: Resource Competition and Virtual Economies in World of Warcraft link"
  },
  {
    "objectID": "rm-data/hw-material/pdf-template.html",
    "href": "rm-data/hw-material/pdf-template.html",
    "title": "Template for PDF",
    "section": "",
    "text": "Use this as the YAML template for your PDF files.\nMake sure you change “Your Title” to the title of your document and “author” to the name of the author.\n---\ntitle: \"Your Title\"\nformat: \n    pdf:\n        documentclass: article \n        number-sections: true\n        margin-top: 1in\n        margin-bottom: 1in\n        margin-left: 1in\n        margin-right: 1in\n        linestretch:  1.5\n        fontsize: 11pt\n    html: default \nauthor: \"author\"        \nbibliography: references.bib\n---"
  },
  {
    "objectID": "rm-data/hw-material/report1.html",
    "href": "rm-data/hw-material/report1.html",
    "title": "The Impact of Resource Management in StarCraft: A Strategic Analysis",
    "section": "",
    "text": "This report examines the crucial role of resource management in the popular real-time strategy game StarCraft. We will explore how effective resource allocation influences gameplay dynamics and strategic decision-making. The analysis will include a mathematical model of resource gathering, a visualization of unit production rates, and a comparison of resource types across different races."
  },
  {
    "objectID": "rm-data/hw-material/report1.html#mathematical-model-of-resource-gathering",
    "href": "rm-data/hw-material/report1.html#mathematical-model-of-resource-gathering",
    "title": "The Impact of Resource Management in StarCraft: A Strategic Analysis",
    "section": "Mathematical Model of Resource Gathering",
    "text": "Mathematical Model of Resource Gathering\nIn StarCraft, the rate of resource accumulation can be modeled using a simple differential equation. If we denote the amount of resources as \\(R\\) and time as \\(t\\), we can express the rate of change of resources as:\n\\[\n\\frac{dR}{dt} = \\alpha N - \\beta P\n\\tag{1}\\]\nWhere \\(\\alpha\\) is the gathering rate per worker, \\(N\\) is the number of workers, \\(\\beta\\) is the consumption rate, and \\(P\\) is the production rate of units or structures. This model, as shown in Equation 1, forms the basis of the game’s economic system (Choi and Kim 2015)."
  },
  {
    "objectID": "rm-data/hw-material/report1.html#unit-production-rates",
    "href": "rm-data/hw-material/report1.html#unit-production-rates",
    "title": "The Impact of Resource Management in StarCraft: A Strategic Analysis",
    "section": "Unit Production Rates",
    "text": "Unit Production Rates\nTo illustrate the impact of resource management on unit production, we’ve created a visualization of unit production rates for different races in StarCraft.\n\n\n\n\n\n\n\n\nFigure 1: Unit Production Rates by Race in StarCraft\n\n\n\n\n\nAs shown in Figure 1, the Zerg race has the highest unit production rate, reflecting their swarm-based strategy. This aligns with the game’s design philosophy, where each race has unique strengths and weaknesses1."
  },
  {
    "objectID": "rm-data/hw-material/report1.html#resource-types-comparison",
    "href": "rm-data/hw-material/report1.html#resource-types-comparison",
    "title": "The Impact of Resource Management in StarCraft: A Strategic Analysis",
    "section": "Resource Types Comparison",
    "text": "Resource Types Comparison\nStarCraft features two primary resource types: minerals and vespene gas. Their availability and usage vary across races:\n\n\n\nTable 1: Resource Usage by Race\n\n\n\n\n\nRace\nMineral Usage\nGas Usage\nResource Dependency\n\n\n\n\nTerran\nHigh\nMedium\nBalanced\n\n\nProtoss\nMedium\nHigh\nGas-heavy\n\n\nZerg\nHigh\nLow\nMineral-heavy\n\n\n\n\n\n\nTable 1 illustrates how different races prioritize resources, influencing their strategic options and tech progression paths."
  },
  {
    "objectID": "rm-data/hw-material/report1.html#footnotes",
    "href": "rm-data/hw-material/report1.html#footnotes",
    "title": "The Impact of Resource Management in StarCraft: A Strategic Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis design approach contributes to StarCraft’s enduring popularity in esports and casual gaming circles.↩︎"
  },
  {
    "objectID": "rm-data/hw-material/report3.html",
    "href": "rm-data/hw-material/report3.html",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "",
    "text": "Dungeons and Dragons (D&D) is a popular tabletop role-playing game that has captivated players for decades. This report explores the mathematical underpinnings of D&D, focusing on the probability distributions of dice rolls and their impact on gameplay. We will examine the statistical nature of character abilities, combat outcomes, and skill checks, providing insights into the game’s mechanics through equations, data visualization, and tabular analysis."
  },
  {
    "objectID": "rm-data/hw-material/report3.html#the-probability-of-adventure",
    "href": "rm-data/hw-material/report3.html#the-probability-of-adventure",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "The Probability of Adventure",
    "text": "The Probability of Adventure\nAt the heart of D&D lies the rolling of dice, particularly the iconic twenty-sided die (d20). The probability of rolling any number on a d20 is uniform, but the outcomes of these rolls can be modified by character abilities and situational modifiers. The probability of success for any given action can be expressed as:\n\\[\nP(success) = \\frac{21 - (DC - modifier)}{20}\n\\tag{1}\\]\nWhere DC is the Difficulty Class of the task, and the modifier is the character’s relevant skill or ability modifier. This equation (Equation 1) forms the foundation of many D&D mechanics1."
  },
  {
    "objectID": "rm-data/hw-material/report3.html#visualizing-character-ability-scores",
    "href": "rm-data/hw-material/report3.html#visualizing-character-ability-scores",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "Visualizing Character Ability Scores",
    "text": "Visualizing Character Ability Scores\nCharacter creation in D&D often involves rolling dice to determine ability scores. The most common method is rolling 4d6 and dropping the lowest die. Let’s visualize the distribution of these rolls:\n\n\n\n\n\n\n\n\nFigure 1: Distribution of D&D Ability Scores (4d6 drop lowest)\n\n\n\n\n\nFigure Figure 1 illustrates the distribution of ability scores using the 4d6 drop lowest method. This bell-shaped curve demonstrates why most characters have average abilities, with exceptional scores being rare."
  },
  {
    "objectID": "rm-data/hw-material/report3.html#combat-outcomes",
    "href": "rm-data/hw-material/report3.html#combat-outcomes",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "Combat Outcomes",
    "text": "Combat Outcomes\nCombat in D&D involves a complex interplay of attack rolls, damage calculations, and defensive abilities. Table Table 1 summarizes the average damage output for different weapon types:\n\n\n\nTable 1: Average Damage Output by Weapon Type\n\n\n\n\n\nWeapon Type\nAverage Damage\nCritical Hit Chance\n\n\n\n\nDagger\n2.5\n5%\n\n\nLongsword\n4.5\n5%\n\n\nGreataxe\n6.5\n5%\n\n\n\n\n\n\nAs shown in Table 1, weapon choice significantly impacts potential damage output, with larger weapons generally dealing more damage at the cost of other factors like weight and required strength."
  },
  {
    "objectID": "rm-data/hw-material/report3.html#the-role-of-randomness",
    "href": "rm-data/hw-material/report3.html#the-role-of-randomness",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "The Role of Randomness",
    "text": "The Role of Randomness\nWhile skill and strategy play crucial roles in D&D, the element of chance introduced by dice rolls adds excitement and unpredictability to the game. According to Tormey (2019), this balance between player agency and random chance is what makes D&D both challenging and engaging. The interplay between player decisions and dice rolls creates a unique narrative experience in each game session."
  },
  {
    "objectID": "rm-data/hw-material/report3.html#footnotes",
    "href": "rm-data/hw-material/report3.html#footnotes",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis simplified equation assumes a linear probability distribution and does not account for critical successes or failures, which are typically represented by rolling a natural 20 or 1, respectively.↩︎"
  },
  {
    "objectID": "rm-data/hw-material/report5.html",
    "href": "rm-data/hw-material/report5.html",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "",
    "text": "This report examines the influence of Eric Ugland’s “The Good Guys” series on contemporary fantasy literature. We will explore the unique elements of Ugland’s work, its reception among readers, and its impact on the genre as a whole. The analysis will include quantitative data on book sales, a comparison with other works in the genre, and insights from literary critics."
  },
  {
    "objectID": "rm-data/hw-material/report5.html#uglands-narrative-formula",
    "href": "rm-data/hw-material/report5.html#uglands-narrative-formula",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "Ugland’s Narrative Formula",
    "text": "Ugland’s Narrative Formula\nOne of the key factors contributing to the success of “The Good Guys” series is Ugland’s innovative approach to character progression. This can be represented by the following equation:\n\\[\nP = (E \\times S) + (L \\times C)\n\\tag{1}\\]\nWhere P represents character progression, E is experience gained, S is skill level, L is luck factor, and C is character choices. This formula Equation 1 encapsulates Ugland’s balance between traditional RPG elements and character-driven storytelling."
  },
  {
    "objectID": "rm-data/hw-material/report5.html#sales-performance",
    "href": "rm-data/hw-material/report5.html#sales-performance",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "Sales Performance",
    "text": "Sales Performance\nThe series’ popularity can be visualized through its sales performance over time:\n\n\n\n\n\n\n\n\nFigure 1: Monthly sales of ‘The Good Guys’ series over two years\n\n\n\n\n\nAs shown in Figure 1, the series has experienced steady growth in sales, with periodic spikes coinciding with new book releases."
  },
  {
    "objectID": "rm-data/hw-material/report5.html#genre-comparison",
    "href": "rm-data/hw-material/report5.html#genre-comparison",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "Genre Comparison",
    "text": "Genre Comparison\nTo contextualize the success of “The Good Guys,” we can compare its key metrics with other popular fantasy series:\n\n\n\nTable 1: Comparison of popular LitRPG series\n\n\n\n\n\nSeries\nAvg. Rating\nBooks Published\nTotal Sales (millions)\n\n\n\n\nThe Good Guys\n4.6\n11\n2.5\n\n\nCradle\n4.7\n11\n3.0\n\n\nThe Land\n4.5\n8\n2.0\n\n\n\n\n\n\nThe data in Table 1 demonstrates that “The Good Guys” holds its own against other well-established series in the genre."
  },
  {
    "objectID": "rm-data/hw-material/report5.html#critical-reception",
    "href": "rm-data/hw-material/report5.html#critical-reception",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "Critical Reception",
    "text": "Critical Reception\nLiterary critics have praised Ugland’s work for its fresh take on the LitRPG genre. According to Johnson (2022), “Ugland’s ‘The Good Guys’ series represents a significant evolution in LitRPG storytelling, blending traditional fantasy elements with modern gaming concepts in a uniquely engaging way.”1\nThe series has also been noted for its contribution to the broader fantasy genre. Smith (2023) argues that “The Good Guys” has “pushed the boundaries of what readers expect from fantasy literature, potentially influencing the direction of the genre for years to come.”"
  },
  {
    "objectID": "rm-data/hw-material/report5.html#footnotes",
    "href": "rm-data/hw-material/report5.html#footnotes",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis praise is particularly noteworthy given the often-skeptical reception of LitRPG works by mainstream literary critics.↩︎"
  },
  {
    "objectID": "rm-data/hw-material/report7.html",
    "href": "rm-data/hw-material/report7.html",
    "title": "The Impact of House Allegiances on Power Dynamics in Westeros",
    "section": "",
    "text": "This report examines the intricate power dynamics in Westeros, as depicted in George R.R. Martin’s “A Song of Ice and Fire” series and its television adaptation, “Game of Thrones.” We will explore how house allegiances shape the political landscape, analyze key alliances, and discuss their implications for the struggle for the Iron Throne."
  },
  {
    "objectID": "rm-data/hw-material/report7.html#house-allegiances-and-power-distribution",
    "href": "rm-data/hw-material/report7.html#house-allegiances-and-power-distribution",
    "title": "The Impact of House Allegiances on Power Dynamics in Westeros",
    "section": "House Allegiances and Power Distribution",
    "text": "House Allegiances and Power Distribution\nThe distribution of power in Westeros can be modeled using a modified version of the Lanchester equations, which originally described the relative strengths of military forces. In our context, we adapt this to represent the power dynamics between major houses:\n\\[\n\\frac{dR}{dt} = -\\alpha L, \\quad \\frac{dL}{dt} = -\\beta R\n\\tag{1}\\]\nWhere \\(R\\) and \\(L\\) represent the strength of rival houses (e.g., Stark and Lannister), and \\(\\alpha\\) and \\(\\beta\\) are coefficients representing the effectiveness of each house’s strategy and resources."
  },
  {
    "objectID": "rm-data/hw-material/report7.html#visualization-of-house-alliances",
    "href": "rm-data/hw-material/report7.html#visualization-of-house-alliances",
    "title": "The Impact of House Allegiances on Power Dynamics in Westeros",
    "section": "Visualization of House Alliances",
    "text": "Visualization of House Alliances\nTo better understand the complex web of alliances in Westeros, we’ve created a network graph representing the relationships between major houses throughout the series.\n\n\n\n\n\n\n\n\nFigure 1: Network of Major House Alliances in Westeros\n\n\n\n\n\nFigure Figure 1 illustrates the complex network of alliances between major houses in Westeros. The connections between houses play a crucial role in determining the balance of power, as discussed in Martin (2011)."
  },
  {
    "objectID": "rm-data/hw-material/report7.html#key-factors-influencing-house-power",
    "href": "rm-data/hw-material/report7.html#key-factors-influencing-house-power",
    "title": "The Impact of House Allegiances on Power Dynamics in Westeros",
    "section": "Key Factors Influencing House Power",
    "text": "Key Factors Influencing House Power\nSeveral factors contribute to a house’s overall power and influence in Westeros. Table Table 1 summarizes these key elements:\n\n\n\nTable 1: Key Factors Influencing House Power in Westeros\n\n\n\n\n\nFactor\nDescription\nImpact\n\n\n\n\nMilitary Strength\nSize and training of armies\nHigh\n\n\nEconomic Resources\nWealth and control over trade\nHigh\n\n\nPolitical Alliances\nRelationships with other houses\nMedium\n\n\nDragons\nPossession of dragons (Targaryen-specific)\nVery High\n\n\n\n\n\n\nAs seen in Table Table 1, military strength and economic resources are crucial for maintaining power. However, the reintroduction of dragons by House Targaryen significantly alters the balance, as noted in Equation Equation 11."
  },
  {
    "objectID": "rm-data/hw-material/report7.html#footnotes",
    "href": "rm-data/hw-material/report7.html#footnotes",
    "title": "The Impact of House Allegiances on Power Dynamics in Westeros",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe presence of dragons could be represented by an additional term in the Lanchester equations, significantly increasing the α coefficient for the house possessing them.↩︎"
  },
  {
    "objectID": "rm-data/Peer_review/Cheming/peer_review.html",
    "href": "rm-data/Peer_review/Cheming/peer_review.html",
    "title": "Feedback on Predicting the Next-Quarter Corporate Profit with Vector Autoregression",
    "section": "",
    "text": "Positive Feedback\nThe theoretical background and framework is really strong.\nYour research question is very clearly laid out.\nYour interpretation of the VAR results is the most fleshed out. This is a strong part of your analysis.\nIt’s great that you explained the limitations of the regression model in your conclusion.\n\n\nQuestions and Suggestions\nI suggest avoiding the use of first-person narrative in a report like this.\nI couldn’t find an explanation for the RHS variables, do you explain what those are in the report?\nThe use of math is impressive, but I think you could clarify your explanation of why the VAR formula is relevant to your specific research.\nI think something needs to be fixed where you’re referencing your regression table. See the following sentence: The result of the regression is in “../results/reg_table_1.txt”, and we only present the significant part related to corporate profit here.\nI think all your figures could use a little bit more explanantion and interpretations.\nDid you use splines to create your last graph?\nTypo: heterskedasticity - You misspelled heteroskedasticity."
  },
  {
    "objectID": "rm-data/research-proposals/proposal1.html",
    "href": "rm-data/research-proposals/proposal1.html",
    "title": "The Impact of Remote Work on Urban Housing Prices",
    "section": "",
    "text": "Introduction\nThe COVID-19 pandemic has dramatically altered work arrangements worldwide, with a significant shift towards remote work. This change has potential far-reaching implications for urban housing markets. This research proposal aims to investigate the impact of increased remote work adoption on housing prices in major urban centers.\n\n\nBackground and Research Question\nThe traditional model of urban development has been centered around the concept of central business districts, where job concentration drives housing demand in surrounding areas. This model has been a cornerstone of urban economics for decades (Alonso 1964). However, the rapid adoption of remote work, accelerated by the COVID-19 pandemic, challenges this traditional understanding (Dingel and Neiman 2020).\nRemote work allows employees to live further from their workplace, potentially reducing the premium placed on centrally located housing. Early evidence suggests that this shift is already impacting housing markets, with some urban centers experiencing slower price growth or even price declines, while suburban and rural areas see increased demand (Ramani and Bloom 2021). However, the long-term implications of this trend remain unclear.\nMain Research Question: How has the increase in remote work adoption influenced housing prices in major urban centers?\nSecondary Research Questions:\n\nIs there a correlation between the percentage of remote workers in a city and changes in housing prices?\nHow does the impact of remote work on housing prices vary across different urban areas?\n\n\n\nPotential Data Sources\n\nRemote Work Adoption: U.S. Census Bureau’s American Community Survey (ACS)\nHousing Prices: Zillow Home Value Index (ZHVI)\nUrban Characteristics: U.S. Census Bureau’s City and Town Population Totals\nEmployment Data: Bureau of Labor Statistics\n\n\n\nPotential Approach\nThis study will employ a difference-in-differences (DiD) approach to analyze the impact of remote work adoption on housing prices. We will use the COVID-19 pandemic as an exogenous shock that dramatically increased remote work adoption. The treatment group will consist of cities with high levels of remote work adoption, while the control group will include cities with lower levels of remote work adoption.\nWe will control for various urban characteristics, such as population size, pre-pandemic economic conditions, and industry composition. To address potential endogeneity concerns, we will use an instrumental variable approach, using pre-pandemic internet connectivity as an instrument for remote work adoption.\n\n\nExpected Findings\nWe anticipate finding a negative relationship between remote work adoption and housing price growth in urban centers. We expect this effect to be more pronounced in cities with a higher concentration of jobs suitable for remote work. However, we also anticipate heterogeneity in the results, with some cities potentially showing resilience in housing prices despite increased remote work.\n\n\nConclusion\nThis research will contribute to our understanding of how changing work patterns influence urban housing markets. The findings will have implications for urban planning, housing policy, and corporate real estate strategies in the post-pandemic era.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/remote-work-housing-prices\nThis repository will contain all data processing scripts, analysis code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nAlonso, William. 1964. Location and Land Use: Toward a General Theory of Land Rent. Harvard University Press.\n\n\nDingel, Jonathan I, and Brent Neiman. 2020. “How Many Jobs Can Be Done at Home?” Journal of Public Economics 189: 104235.\n\n\nRamani, Arjun, and Nicholas Bloom. 2021. “Work from Home and the Office Real Estate Apocalypse.” National Bureau of Economic Research."
  },
  {
    "objectID": "rm-data/research-proposals/proposal2.html",
    "href": "rm-data/research-proposals/proposal2.html",
    "title": "The Effect of Social Media Sentiment on Stock Market Volatility",
    "section": "",
    "text": "Introduction\nIn the age of digital communication, social media platforms have become significant sources of information and sentiment expression. This research proposal aims to investigate the relationship between social media sentiment and stock market volatility, focusing on how sentiment expressed on platforms like Twitter can influence or predict market movements.\n\n\nBackground and Research Question\nThe Efficient Market Hypothesis (EMH) suggests that stock prices reflect all available information (Fama 1970). However, behavioral finance theories argue that investor sentiment can lead to deviations from fundamental values (Baker and Wurgler 2007). With the rise of social media, these platforms have become a new source of real-time information and sentiment that could potentially influence market behavior.\nPrevious studies have shown that social media sentiment can predict stock returns and trading volume (Bollen, Mao, and Zeng 2011). However, the relationship between social media sentiment and market volatility is less explored. Volatility is a crucial measure of market risk and plays a significant role in asset pricing and risk management (Poon and Granger 2003).\nMain Research Question: To what extent does social media sentiment predict or influence stock market volatility?\nSecondary Research Questions: 1. How does the predictive power of social media sentiment vary across different industries or company sizes? 2. Is there a lag between changes in social media sentiment and changes in market volatility?\n\n\nPotential Data Sources\n\nSocial Media Data: Twitter API for tweets related to specific stocks or market indices\nStock Market Data: Yahoo Finance API for historical stock prices and volatility indices\nCompany Information: Compustat database for company characteristics\nNews Sentiment: RavenPack database for comparison with traditional media sentiment\n\n\n\nPotential Approach\nWe will use natural language processing (NLP) techniques to analyze the sentiment of tweets related to specific stocks or market indices. We will then construct a daily sentiment index for each stock or index in our sample.\nTo analyze the relationship between social media sentiment and market volatility, we will use a Vector Autoregression (VAR) model. This will allow us to capture the dynamic relationships between sentiment and volatility while controlling for other factors that might influence volatility, such as trading volume and macroeconomic news.\nWe will also conduct Granger causality tests to determine if social media sentiment has predictive power for future volatility. To address potential endogeneity concerns, we will use an instrumental variable approach, using exogenous events that affect sentiment but are unlikely to directly affect volatility.\n\n\nExpected Findings\nWe anticipate finding a significant relationship between social media sentiment and stock market volatility. We expect that negative sentiment will be associated with increased volatility, while positive sentiment may have a stabilizing effect. We also anticipate that the strength of this relationship may vary across different industries, with tech stocks potentially showing a stronger link to social media sentiment.\n\n\nConclusion\nThis research will contribute to our understanding of how information dissemination through social media influences financial markets. The findings will have implications for risk management strategies, regulatory policies regarding social media and market manipulation, and the development of sentiment-based trading strategies.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/social-media-market-volatility\nThis repository will contain all data collection scripts, sentiment analysis code, econometric models, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nBaker, Malcolm, and Jeffrey Wurgler. 2007. “Investor Sentiment in the Stock Market.” Journal of Economic Perspectives 21 (2): 129–52.\n\n\nBollen, Johan, Huina Mao, and Xiaojun Zeng. 2011. “Twitter Mood Predicts the Stock Market.” Journal of Computational Science 2 (1): 1–8.\n\n\nFama, Eugene F. 1970. “Efficient Capital Markets: A Review of Theory and Empirical Work.” The Journal of Finance 25 (2): 383–417.\n\n\nPoon, Ser-Huang, and Clive WJ Granger. 2003. “Forecasting Volatility in Financial Markets: A Review.” Journal of Economic Literature 41 (2): 478–539."
  },
  {
    "objectID": "rm-data/research-proposals/proposal4.html",
    "href": "rm-data/research-proposals/proposal4.html",
    "title": "The Economic Impact of Artificial Intelligence Adoption in Small and Medium Enterprises",
    "section": "",
    "text": "Introduction\nArtificial Intelligence (AI) is rapidly transforming various sectors of the economy. While much attention has been given to AI adoption in large corporations, its impact on Small and Medium Enterprises (SMEs) remains understudied. This research proposal aims to investigate the economic effects of AI adoption on SMEs, focusing on productivity, profitability, and employment dynamics.\n\n\nBackground and Research Question\nAI technologies have the potential to significantly enhance business processes, decision-making, and customer interactions (Brynjolfsson, Rock, and Syverson 2017). However, the adoption and impact of AI may differ between large corporations and SMEs due to differences in resources, expertise, and scale (Ghobakhloo 2020).\nSMEs play a crucial role in most economies, often accounting for the majority of businesses and employment (Ayyagari, Demirguc-Kunt, and Maksimovic 2011). Understanding how AI adoption affects these firms is essential for policymakers and business leaders. Previous studies have examined the impact of digital technologies on SMEs (Neirotti, Raguseo, and Paolucci 2018), but the specific effects of AI adoption remain unclear.\nMain Research Question: What is the economic impact of AI adoption on Small and Medium Enterprises?\nSecondary Research Questions:\n\nHow does AI adoption affect productivity and profitability in SMEs?\nWhat are the employment effects of AI adoption in SMEs?\nHow does the impact of AI adoption vary across different industries and firm sizes within the SME category?\n\n\n\nPotential Data Sources\n\nAI Adoption Data: Surveys conducted by national statistical offices or industry associations\nFirm-level Financial Data: Orbis database by Bureau van Dijk\nEmployment Data: National labor force surveys or social security records\nIndustry Classification: Standard Industrial Classification (SIC) codes\nFirm Characteristics: National business registries\n\n\n\nPotential Approach\nWe will use a difference-in-differences (DiD) approach to estimate the causal effect of AI adoption on various economic outcomes. We will compare the performance of SMEs that adopt AI (treatment group) with similar firms that do not adopt AI (control group) before and after the adoption.\nTo address potential selection bias, we will use propensity score matching to ensure that the treatment and control groups are comparable in terms of observable characteristics. We will also employ an instrumental variable approach, using the geographical variation in broadband internet availability as an instrument for AI adoption.\nTo examine heterogeneous effects, we will interact the AI adoption variable with industry and firm size indicators. We will also conduct a series of robustness checks, including placebo tests and alternative definitions of AI adoption.\n\n\nExpected Findings\nWe anticipate finding positive effects of AI adoption on productivity and profitability in SMEs, although these effects may vary across industries and firm sizes. We expect to see larger positive effects in knowledge-intensive industries and for firms at the upper end of the SME size spectrum.\nRegarding employment, we anticipate a more nuanced picture. While AI adoption may lead to some job displacement in certain roles, it may also create new job opportunities and increase demand for skilled workers. Overall, we expect to find a skill-biased change in employment structure rather than a significant net change in total employment.\n\n\nConclusion\nThis research will provide valuable insights into the economic impact of AI adoption on SMEs, a crucial but often overlooked segment of the economy. The findings will have important implications for policymakers designing support programs for SMEs, for business leaders making technology investment decisions, and for workers and educators preparing for the future of work in an AI-driven economy.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/ai-impact-sme\nThis repository will contain all data processing scripts, econometric models, visualization code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nAyyagari, Meghana, Asli Demirguc-Kunt, and Vojislav Maksimovic. 2011. “Small Vs. Young Firms Across the World: Contribution to Employment, Job Creation, and Growth.” World Bank Policy Research Working Paper, no. 5631.\n\n\nBrynjolfsson, Erik, Daniel Rock, and Chad Syverson. 2017. “Artificial Intelligence and the Modern Productivity Paradox: A Clash of Expectations and Statistics.” National Bureau of Economic Research.\n\n\nGhobakhloo, Morteza. 2020. “Industry 4.0, Digitization, and Opportunities for Sustainability.” Journal of Cleaner Production 252: 119869.\n\n\nNeirotti, Paolo, Elisabetta Raguseo, and Emilio Paolucci. 2018. “Digital Capabilities and the Performance of Small and Medium-Sized Enterprises: The Mediating Role of Productivity.” Information & Management 55 (7): 871–85."
  },
  {
    "objectID": "rm-data/research-proposals/proposal6.html",
    "href": "rm-data/research-proposals/proposal6.html",
    "title": "The Economics of Springfield: A Case Study of Resource Allocation in The Simpsons",
    "section": "",
    "text": "Introduction\nThe long-running animated series “The Simpsons” provides a unique lens through which to examine economic principles in a fictional setting. This research proposal aims to analyze the economic structure and resource allocation mechanisms in the town of Springfield, using it as a case study to explore real-world economic theories and phenomena.\n\n\nBackground and Research Question\n“The Simpsons” presents a microcosm of American society, complete with its own economic system (Alberti 2004). The town of Springfield features various industries, public services, and economic actors that interact in ways that often mirror real-world economic dynamics. From the nuclear power plant’s monopoly to Moe’s Tavern’s role in the informal economy, Springfield offers numerous examples of economic principles in action (Scanlan and Feinberg 2013).\nThe show’s longevity and consistency in depicting Springfield’s economy provide a unique opportunity to study how a fictional economy responds to various shocks and policy interventions over time. This can offer insights into the applicability and limitations of economic theories in a controlled, albeit fictional, environment.\nMain Research Question: How does the fictional economy of Springfield in “The Simpsons” reflect real-world economic principles and theories?\nSecondary Research Questions:\n\nHow does the monopolistic structure of the Springfield Nuclear Power Plant affect resource allocation in the town?\nWhat role does corruption (as exemplified by Mayor Quimby) play in Springfield’s economic development?\nHow do external shocks (e.g., the various disasters that befall Springfield) impact its economic resilience?\n\n\n\nPotential Data Sources\n\nEpisode Scripts: Transcripts of all “The Simpsons” episodes\nFan Wikis: Detailed information about Springfield’s businesses and economic events\nAcademic Literature: Existing analyses of economics in “The Simpsons”\nReal-world Economic Data: For comparison with Springfield’s fictional economy\n\n\n\nPotential Approach\nWe will employ a mixed-methods approach, combining qualitative content analysis with quantitative modeling. First, we will conduct a systematic review of all episodes, coding for economic events, policies, and outcomes. This will allow us to create a comprehensive database of Springfield’s economic history.\nUsing this database, we will construct economic models of Springfield, estimating key parameters such as the town’s GDP, unemployment rate, and income distribution. We will then use these models to simulate the effects of various economic policies and shocks depicted in the show.\nTo analyze the role of institutions, we will use a comparative institutional analysis, contrasting Springfield’s institutions with real-world examples. We will also employ network analysis to map the economic relationships between characters and businesses in Springfield.\n\n\nExpected Findings\nWe anticipate finding that Springfield’s economy exhibits many real-world economic phenomena, including market failures, externalities, and public choice problems. We expect to see significant impacts of the nuclear plant’s monopoly on resource allocation and evidence of the “resource curse” related to Springfield’s over-reliance on this industry.\nWe also anticipate finding that corruption plays a significant role in Springfield’s economic outcomes, potentially leading to inefficient resource allocation. However, we expect to observe a high degree of economic resilience, with Springfield quickly recovering from numerous disasters, possibly highlighting the role of social capital in economic recovery.\n\n\nConclusion\nThis research will provide a novel perspective on economic principles by examining them through the lens of a popular cultural artifact. While based on a fictional setting, the findings may offer insights into real-world economic dynamics, particularly in small, isolated economies. Moreover, this study could demonstrate the potential of using popular media as a tool for economic education and analysis.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/springfield-economics\nThis repository will contain all data collection scripts, economic models, visualization code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nAlberti, John. 2004. Leaving Springfield: The Simpsons and the Possibility of Oppositional Culture. Wayne State University Press.\n\n\nScanlan, Stephen J, and Seth L Feinberg. 2013. “The Simpsons and the Economy.” The Simpsons in the Classroom: Embiggening the Learning Experience with the Wisdom of Springfield, 116–30."
  },
  {
    "objectID": "rm-data/research-proposals/proposal8.html",
    "href": "rm-data/research-proposals/proposal8.html",
    "title": "The Economics of the Matrix: Scarcity, Choice, and Human Capital in a Simulated Reality",
    "section": "",
    "text": "Introduction\nThe Matrix trilogy presents a unique scenario where most of humanity exists within a simulated reality. This research proposal aims to analyze the economic implications of such a system, focusing on concepts of scarcity, choice, and human capital in both the simulated world and the real world of the films.\n\n\nBackground and Research Question\nThe Matrix depicts a world where machines use human bodies as an energy source, while human minds exist in a simulated reality (Irwin 2002). This scenario raises intriguing questions about the nature of economic activity in a world where physical scarcity is largely artificial, but cognitive scarcity remains real (Chalmers 2005).\nThe films also present two contrasting economic systems: the seemingly abundant but illusory economy of the Matrix, and the scarce, survival-focused economy of Zion. This dichotomy offers a unique opportunity to explore how different resource constraints shape economic behavior and institutions (Yeffeth 2003).\nMoreover, the concept of human capital takes on new dimensions in the Matrix, where individuals can instantly learn new skills by downloading them directly to their minds. This aspect of the films allows us to explore questions about the nature of skill acquisition, the value of education, and the role of human capital in economic growth.\nMain Research Question: How do the economic systems depicted in the Matrix trilogy reflect and challenge traditional economic concepts of scarcity, choice, and human capital?\nSecondary Research Questions:\n\nHow does the artificial scarcity in the Matrix compare to real-world economic scarcity?\nWhat are the economic implications of instant skill acquisition as depicted in the films?\nHow does the economy of Zion reflect real-world economies operating under extreme resource constraints?\n\n\n\nPotential Data Sources\n\nFilm Scripts: Detailed scripts of the Matrix trilogy\nSupplementary Material: The Animatrix and related comics\nFan Wikis: Detailed information about the Matrix universe\nEconomic Literature: Theories on scarcity, choice, and human capital\nTechnological Forecasting: Predictions about future AI and VR technologies\n\n\n\nPotential Approach\nWe will employ a mixed-methods approach, combining qualitative analysis of the films with economic modeling. First, we will conduct a systematic review of the trilogy, coding for economic concepts, resource allocation mechanisms, and depictions of skill acquisition.\nUsing this data, we will construct economic models of both the Matrix and Zion, estimating key parameters such as production possibilities, resource constraints, and returns to human capital. We will then compare these models with real-world economic data and theories.\nTo analyze the implications of instant skill acquisition, we will develop a theoretical model of human capital accumulation based on the Matrix’s depiction, comparing it with standard models of education and on-the-job training.\nWe will also use comparative analysis to contrast the economic systems of the Matrix and Zion with various real-world economic systems, from abundance-oriented tech economies to resource-constrained developing economies.\n\n\nExpected Findings\nWe anticipate finding that the Matrix’s economy challenges traditional concepts of scarcity, potentially offering insights into post-scarcity economic theories. We expect to see that while physical scarcity is artificially imposed in the Matrix, cognitive scarcity (e.g., limitations on attention and decision-making) remains a key economic factor.\nRegarding human capital, we expect to find that the Matrix’s depiction of instant skill acquisition has profound implications for theories of human capital and economic growth, potentially highlighting the importance of knowledge distribution over knowledge creation in driving economic progress.\nFor Zion’s economy, we anticipate finding parallels with real-world economies operating under extreme resource constraints, potentially offering insights into economic resilience and adaptation.\n\n\nConclusion\nThis research will provide a novel perspective on fundamental economic concepts by examining them through the lens of a popular science fiction narrative. While based on a fictional setting, the findings may offer insights into real-world economic phenomena, particularly as we move towards increasingly digitalized and AI-driven economies. Moreover, this study could demonstrate the potential of using science fiction scenarios as thought experiments for economic theory and policy.\n\n\nGitHub Repository\nThe data analysis and code for this project will be available in the following GitHub repository:\nhttps://github.com/yourusername/matrix-economics\nThis repository will contain all data collection scripts, economic models, visualization code, and the final paper in Quarto format.\n\n\n\n\n\n\n\n\nReferences\n\nChalmers, David J. 2005. “The Matrix as Metaphysics.” Science Fiction and Philosophy: From Time Travel to Superintelligence, 36–53.\n\n\nIrwin, William. 2002. The Matrix and Philosophy: Welcome to the Desert of the Real. Open Court.\n\n\nYeffeth, Glenn. 2003. Taking the Red Pill: Science, Philosophy and the Religion in the Matrix. BenBella Books."
  },
  {
    "objectID": "rm-data/slides/CaseStudy_SLR.html",
    "href": "rm-data/slides/CaseStudy_SLR.html",
    "title": "Gender Gap in Earnings: A Case Study",
    "section": "",
    "text": "The gender gap in earnings is a critical issue in labor economics, reflecting potential inequalities in the labor market. This case study investigates whether there are systematic wage differences between male and female workers, focusing on data from the U.S. Current Population Survey (CPS) for 2014. The study aims to understand the factors contributing to the gender gap and examines specific occupations to provide detailed insights into these disparities.\n\n\n\n\n\nThe CPS is a monthly survey conducted by the U.S. Census Bureau and the Bureau of Labor Statistics. It provides comprehensive data on labor force characteristics, including employment, earnings, and demographic information. The survey uses a rotating panel design, where households are interviewed for four consecutive months, not interviewed for the next eight months, and then interviewed again for four months. This design helps in capturing both short-term and long-term labor market trends.\n\n\n\nTo focus on the working-age population, the sample includes individuals aged 16-65 who are employed and have reported earnings. Self-employed individuals are excluded to maintain consistency in earnings data, as self-employment income can vary significantly and may not be directly comparable to wage and salary earnings. After applying these restrictions, the dataset for 2014 consists of 149,316 observations.\n\n\n\nWeekly earnings data are collected before tax deductions. High earnings are top-coded at $2,884.6 to account for inflation and to prevent the influence of outliers on the analysis. This top-coding represents the top 2.5% of earnings in 2014. To control for differences in hours worked, weekly earnings are divided by ‘usual’ weekly hours, as reported in the survey. This adjustment is crucial because women may work fewer hours on average than men, affecting their total earnings.\n\n\n\n\nThe descriptive statistics provide an overview of the earnings distribution by gender. The table below summarizes the key percentiles of earnings for male and female workers:\n\n\n\nGender\nMean\np25\np50\np75\np90\np95\n\n\n\n\nMale\n24\n13\n19\n30\n45\n55\n\n\nFemale\n20\n11\n16\n24\n36\n45\n\n\n\nThese statistics reveal a 17% average difference in per-hour earnings between men and women, highlighting a significant gender gap.\n\n\n\n\n\nThe analysis focuses on computer science occupations, which are often associated with high earnings and significant gender disparities. The sample size for this occupation is 4,740. The regression model used is:\n\\[\n\\ln(w)_E = \\alpha + \\beta \\times G_{female}\n\\]\nWhere \\(G_{female}\\) is a binary variable indicating if the individual is female. The regression estimate \\(\\hat{\\beta} = -0.1475\\) suggests that female employees in computer science earn 14.7% less on average than their male counterparts. The 95% confidence interval for this estimate is [-18.2%, -11.2%], which does not include zero, allowing us to rule out equal average earnings with 95% confidence. This finding is statistically significant, with a standard error of 0.0177.\n\n\n\nFor market research analysts and marketing specialists, the sample size is 281, with females comprising 61% of the sample. The regression estimate \\(\\hat{\\beta} = -0.113\\) indicates that female analysts earn 11.3% less on average. However, the 95% confidence interval [-23%, +1%] includes zero, indicating that we cannot rule out equal average earnings with 95% confidence. The p-value of 0.068 suggests that the result is not statistically significant at the 5% level, although it is at the 10% level.\n\n\n\n\n\n\nThe difference in confidence intervals between the two occupations can be attributed to:\n\nTrue Difference: The gender gap is higher in computer science occupations, possibly due to industry-specific factors such as negotiation practices, discrimination, or differences in experience and education levels.\nStatistical Error: The smaller sample size for market analysts may lead to more variability in estimates. Smaller samples tend to have larger standard errors, resulting in wider confidence intervals. This variability can obscure true differences in earnings.\n\n\n\n\nTo formally test whether average earnings are the same by gender, we examine if the coefficient on the gender variable is zero. The t-statistic for market analysts is 1.8, which falls within the critical values for a 5% significance level (±2), indicating that we cannot reject the null hypothesis of equal average earnings. The p-value of 0.07 further supports this conclusion, as it is greater than the 0.05 threshold for statistical significance.\n\n\n\n\nThis case study highlights the complexities of analyzing the gender gap in earnings. While significant disparities exist in computer science occupations, the evidence is less clear for market research analysts due to sample size limitations. The findings underscore the need for further data collection and analysis to draw more definitive conclusions about the gender gap across different sectors. Understanding these disparities is crucial for developing policies aimed at achieving gender equality in the workplace."
  },
  {
    "objectID": "rm-data/slides/CaseStudy_SLR.html#introduction",
    "href": "rm-data/slides/CaseStudy_SLR.html#introduction",
    "title": "Gender Gap in Earnings: A Case Study",
    "section": "",
    "text": "The gender gap in earnings is a critical issue in labor economics, reflecting potential inequalities in the labor market. This case study investigates whether there are systematic wage differences between male and female workers, focusing on data from the U.S. Current Population Survey (CPS) for 2014. The study aims to understand the factors contributing to the gender gap and examines specific occupations to provide detailed insights into these disparities."
  },
  {
    "objectID": "rm-data/slides/CaseStudy_SLR.html#data-collection-and-methodology",
    "href": "rm-data/slides/CaseStudy_SLR.html#data-collection-and-methodology",
    "title": "Gender Gap in Earnings: A Case Study",
    "section": "",
    "text": "The CPS is a monthly survey conducted by the U.S. Census Bureau and the Bureau of Labor Statistics. It provides comprehensive data on labor force characteristics, including employment, earnings, and demographic information. The survey uses a rotating panel design, where households are interviewed for four consecutive months, not interviewed for the next eight months, and then interviewed again for four months. This design helps in capturing both short-term and long-term labor market trends.\n\n\n\nTo focus on the working-age population, the sample includes individuals aged 16-65 who are employed and have reported earnings. Self-employed individuals are excluded to maintain consistency in earnings data, as self-employment income can vary significantly and may not be directly comparable to wage and salary earnings. After applying these restrictions, the dataset for 2014 consists of 149,316 observations.\n\n\n\nWeekly earnings data are collected before tax deductions. High earnings are top-coded at $2,884.6 to account for inflation and to prevent the influence of outliers on the analysis. This top-coding represents the top 2.5% of earnings in 2014. To control for differences in hours worked, weekly earnings are divided by ‘usual’ weekly hours, as reported in the survey. This adjustment is crucial because women may work fewer hours on average than men, affecting their total earnings."
  },
  {
    "objectID": "rm-data/slides/CaseStudy_SLR.html#descriptive-statistics",
    "href": "rm-data/slides/CaseStudy_SLR.html#descriptive-statistics",
    "title": "Gender Gap in Earnings: A Case Study",
    "section": "",
    "text": "The descriptive statistics provide an overview of the earnings distribution by gender. The table below summarizes the key percentiles of earnings for male and female workers:\n\n\n\nGender\nMean\np25\np50\np75\np90\np95\n\n\n\n\nMale\n24\n13\n19\n30\n45\n55\n\n\nFemale\n20\n11\n16\n24\n36\n45\n\n\n\nThese statistics reveal a 17% average difference in per-hour earnings between men and women, highlighting a significant gender gap."
  },
  {
    "objectID": "rm-data/slides/CaseStudy_SLR.html#analysis-of-gender-gap-in-specific-occupations",
    "href": "rm-data/slides/CaseStudy_SLR.html#analysis-of-gender-gap-in-specific-occupations",
    "title": "Gender Gap in Earnings: A Case Study",
    "section": "",
    "text": "The analysis focuses on computer science occupations, which are often associated with high earnings and significant gender disparities. The sample size for this occupation is 4,740. The regression model used is:\n\\[\n\\ln(w)_E = \\alpha + \\beta \\times G_{female}\n\\]\nWhere \\(G_{female}\\) is a binary variable indicating if the individual is female. The regression estimate \\(\\hat{\\beta} = -0.1475\\) suggests that female employees in computer science earn 14.7% less on average than their male counterparts. The 95% confidence interval for this estimate is [-18.2%, -11.2%], which does not include zero, allowing us to rule out equal average earnings with 95% confidence. This finding is statistically significant, with a standard error of 0.0177.\n\n\n\nFor market research analysts and marketing specialists, the sample size is 281, with females comprising 61% of the sample. The regression estimate \\(\\hat{\\beta} = -0.113\\) indicates that female analysts earn 11.3% less on average. However, the 95% confidence interval [-23%, +1%] includes zero, indicating that we cannot rule out equal average earnings with 95% confidence. The p-value of 0.068 suggests that the result is not statistically significant at the 5% level, although it is at the 10% level."
  },
  {
    "objectID": "rm-data/slides/CaseStudy_SLR.html#discussion",
    "href": "rm-data/slides/CaseStudy_SLR.html#discussion",
    "title": "Gender Gap in Earnings: A Case Study",
    "section": "",
    "text": "The difference in confidence intervals between the two occupations can be attributed to:\n\nTrue Difference: The gender gap is higher in computer science occupations, possibly due to industry-specific factors such as negotiation practices, discrimination, or differences in experience and education levels.\nStatistical Error: The smaller sample size for market analysts may lead to more variability in estimates. Smaller samples tend to have larger standard errors, resulting in wider confidence intervals. This variability can obscure true differences in earnings.\n\n\n\n\nTo formally test whether average earnings are the same by gender, we examine if the coefficient on the gender variable is zero. The t-statistic for market analysts is 1.8, which falls within the critical values for a 5% significance level (±2), indicating that we cannot reject the null hypothesis of equal average earnings. The p-value of 0.07 further supports this conclusion, as it is greater than the 0.05 threshold for statistical significance."
  },
  {
    "objectID": "rm-data/slides/CaseStudy_SLR.html#conclusion",
    "href": "rm-data/slides/CaseStudy_SLR.html#conclusion",
    "title": "Gender Gap in Earnings: A Case Study",
    "section": "",
    "text": "This case study highlights the complexities of analyzing the gender gap in earnings. While significant disparities exist in computer science occupations, the evidence is less clear for market research analysts due to sample size limitations. The findings underscore the need for further data collection and analysis to draw more definitive conclusions about the gender gap across different sectors. Understanding these disparities is crucial for developing policies aimed at achieving gender equality in the workplace."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#course-overview",
    "href": "rm-data/slides/week01/week01.html#course-overview",
    "title": "Introduction to Reproducible Research",
    "section": "Course Overview",
    "text": "Course Overview\nDuration: 3 hours (180 minutes)\n\nIntroduction to Reproducible Research\n\nZotero for Bibliography Management\nVersion Control with Git, GitHub, and GitHub-Desktop\n\nVisual Studio Code (VSC) for Coding\n\nQuarto and VSC for Documents\n\nOther Essential Tools and Practices\n\nPractical Exercise\nQ&A and Resources for Further Learning\n\n\nWelcome to our course on Reproducible Research in Economics. Over the next 3 hours, we’ll cover a range of tools and practices that will help you make your research more transparent, collaborative, and reproducible."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#why-reproducibility-matters-in-economics",
    "href": "rm-data/slides/week01/week01.html#why-reproducibility-matters-in-economics",
    "title": "Introduction to Reproducible Research",
    "section": "Why Reproducibility Matters in Economics",
    "text": "Why Reproducibility Matters in Economics\n\nEnhances credibility and transparency of research\n\nEnables easier verification, replocation and extension of studies\n\nFacilitates collaboration and knowledge sharing\nAligns with growing expectations from journals and funding bodies\n\nMany journals now require reproducibility as part of the submission process\n\n\n\nReproducibility is crucial in economics as it ensures that other researchers can replicate and build upon your work. This transparency enhances the credibility of your findings and contributes to the cumulative nature of scientific knowledge."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#tools-well-cover",
    "href": "rm-data/slides/week01/week01.html#tools-well-cover",
    "title": "Introduction to Reproducible Research",
    "section": "Tools We’ll Cover",
    "text": "Tools We’ll Cover\n\nGit & GitHub for version control\nZotero for bibliography management\nVisual Studio Code as our integrated development environment\nQuarto for dynamic document creation\nSupporting tools for project organization and data management\n\n\nThese tools form a powerful ecosystem that will streamline your research workflow and make reproducibility a natural part of your process."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#setting-up-your-environment",
    "href": "rm-data/slides/week01/week01.html#setting-up-your-environment",
    "title": "Introduction to Reproducible Research",
    "section": "Setting Up Your Environment",
    "text": "Setting Up Your Environment\n\nCreate a GitHub account https://github.com/\nDownload and install GitHub Desktop https://desktop.github.com/download\nInstall Zotero and the Zotero Connector for your browser https://www.zotero.org/download\nInstall Visual Studio Code https://code.visualstudio.com/Download\nInstall Quarto https://quarto.org/\n(Optional) Install R, Python, or Stata as needed\n\n\nWe’ll guide you through the installation process for each of these tools. Don’t worry if you encounter any issues - we’re here to help!"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#why-do-we-care-about-version-control",
    "href": "rm-data/slides/week01/week01.html#why-do-we-care-about-version-control",
    "title": "Introduction to Reproducible Research",
    "section": "Why do we care about Version Control?",
    "text": "Why do we care about Version Control?\n\nLife without version control\n\nDo you keep every variant of every program you ever wrote on a project?\nIf so, how do you keep track of them? What if there is a bug?\nIf only keep latests, how do you know what you tried?\n\nDo you have the ThesisV1, ThesisV123, ThesisFinal, ThesisFinalFinal, ThesisFinalFinalFinal problem?"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#what-is-git",
    "href": "rm-data/slides/week01/week01.html#what-is-git",
    "title": "Introduction to Reproducible Research",
    "section": "What is Git?",
    "text": "What is Git?\n\nGit is a program that does version control (tracking changes in files)\nIt is the most popular version control program in software development and data science\nIt is “easy” to set up and get started\nThere are many programs that add intuitive interfaces on top of Git\nGit integrates seamlessly with online collaboration tools like GitHub and GitLab"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#why-use-git",
    "href": "rm-data/slides/week01/week01.html#why-use-git",
    "title": "Introduction to Reproducible Research",
    "section": "Why use Git?",
    "text": "Why use Git?\n\nGit is the current standard for version control.\nIt has been used in the software industry for long, and it is now being adopted in other fields.\nIt can help you keep track of changes in your code and documents.\n\nIt does not save every version of every file, but it saves the changes you made.\nWhich means, you can go back to a previous version of your code or document.\nIt also helps you keep track of who made what changes and when (Comments)\n\nCaveat: hard to keep track of changes in binary files (e.g., .docx, .pdf, .xlsx)"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#how-to-use-git",
    "href": "rm-data/slides/week01/week01.html#how-to-use-git",
    "title": "Introduction to Reproducible Research",
    "section": "How to use Git?",
    "text": "How to use Git?\n\nYou can use Git from the command line! (Most powerful, but steep learning curve)\nOr you can use a GUI (Graphical User Interface) like GitHub Desktop, or Rstudio (which has Git integrated for Projects)\nIf you need a remote repository, you can use GitHub, GitLab, or Bitbucket (among others)\n\nGitHub is the most popular, and we will be using it."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#basic-git-concepts",
    "href": "rm-data/slides/week01/week01.html#basic-git-concepts",
    "title": "Introduction to Reproducible Research",
    "section": "Basic Git Concepts",
    "text": "Basic Git Concepts\n\nRepository: A project’s folder containing all files and version history. It can be local (your PC) or remote (GitHub).\nCommit: A snapshot of your project at a specific point in time (like a save point in a game)\n\nCommit message: A brief description of the changes made. All changes are local.\n\nPush: Sending your commits to a remote repository (like GitHub)\nPull: Getting changes from a remote repository (from GitHub to your PC)\n\n\n\n\nIf you want to use GitHub as a Dropbox-like service, you need to commit and push your changes."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#additional-git-concepts",
    "href": "rm-data/slides/week01/week01.html#additional-git-concepts",
    "title": "Introduction to Reproducible Research",
    "section": "Additional Git Concepts",
    "text": "Additional Git Concepts\n\nBranch: An independent line of development\nMerge: Combining changes from different branches\n\nSo far in our work we have been working on the main branch. That should be the case for most of your work.\n\n\n\nThese concepts form the foundation of version control with Git. Understanding them will help you manage your project’s history effectively."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#github-desktop-getting-started",
    "href": "rm-data/slides/week01/week01.html#github-desktop-getting-started",
    "title": "Introduction to Reproducible Research",
    "section": "GitHub Desktop: Getting Started",
    "text": "GitHub Desktop: Getting Started\n\nGitHub Desktop provides a user-friendly interface for Git operations, that works well with GitHub.\n\nIt is a good way to get started with Git, create repositories, and manage your projects.\n\nDownload and install GitHub Desktop\nLog in with your GitHub account\n\nCreate a new repository or clone an existing one\n\nEvery one can have access to public repositories (transparent) but only you and collaborators can access private repositories.\n\nMake changes, commit, and push to GitHub\n\n\nGitHub Desktop provides a user-friendly interface for Git operations, making it easier for beginners to get started with version control."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#what-should-a-the-git-repository-contain",
    "href": "rm-data/slides/week01/week01.html#what-should-a-the-git-repository-contain",
    "title": "Introduction to Reproducible Research",
    "section": "What should a the Git repository contain?",
    "text": "What should a the Git repository contain?\n\nThe Core of the project (The code, the data, the documents):\n\nCode (.do, .R, .m, .jl)\n.txt files, .md. files, .qmd files.\nLaTeX .tex, .bib, etc\n\nYou may also want to include (small) Raw Data (.csv)\n\nor a link to the data (other ways to access it)\n\nNormally, you wouldnt want binary files, but the following are exceptions:\n\nPDF files\nWord, Excel, PowerPoint files\nPerhaps images, if they are part of the project"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#section",
    "href": "rm-data/slides/week01/week01.html#section",
    "title": "Introduction to Reproducible Research",
    "section": "",
    "text": "You could include full datasets, but consider:\n\nGitHub doesn’t allow files larger than 100 MB, or projects with total size larger than 1 GB.\nLook into Git LFS (Large File Storage) for large files."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#what-could-you-ignore",
    "href": "rm-data/slides/week01/week01.html#what-could-you-ignore",
    "title": "Introduction to Reproducible Research",
    "section": "What could you ignore?",
    "text": "What could you ignore?\nConsider adding a .gitignore file to your repository to exclude:\n\nTemporary files (.bak, .swp, .log, .aux)\nJunk files (.RData, .pyc)\nor simply any format or file you do not want to include. (keep the repository clean)"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#project-workflow",
    "href": "rm-data/slides/week01/week01.html#project-workflow",
    "title": "Introduction to Reproducible Research",
    "section": "Project Workflow",
    "text": "Project Workflow\n\n\nStart a new project: Create a new repository on GitHub or Github Desktop\n\nClone it to your local machine (if you started on GitHub)\n\n\nCreate your project structure, add files, and start working.\nWhen you have made changes, and are ready to “save” them, commit them (Local Snapshot).\n\nAdd a message that describes the changes you made (make them meaningful).\n\nWhen you are ready to share your changes with the world, push them to GitHub (Remote repository).\nIf working with others, you may want to pull changes from GitHub to your local machine."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#section-1",
    "href": "rm-data/slides/week01/week01.html#section-1",
    "title": "Introduction to Reproducible Research",
    "section": "",
    "text": "Commit often, push when you are ready to share.\n\nSmall mistakes are easier to fix.\n\nA bit of a challenge if working with multiple computers, but it is doable.\nAnd always check, everything works before pushing."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#introduction-to-zotero",
    "href": "rm-data/slides/week01/week01.html#introduction-to-zotero",
    "title": "Introduction to Reproducible Research",
    "section": "Introduction to Zotero",
    "text": "Introduction to Zotero\n\nSomething that is not often taught in school is how to manage your bibliography.\n\nYou work on your paper, forget where you got a quote, and then you have to go back and find it!\nWorse, you have to format your bibliography, and you have to do it manually.\nthen you change text, and drop a citation from text and refereces don’t match.\n\nThis is where Zotero comes in.\n\nIt is a free, open-source reference management software.\nIt helps you collect, organize, cite, and share your research sources.\n\nMore importantly, it will create bib files for all your references.\n\nIt integrates with word processors and browsers.\n\n\n\nZotero is a powerful tool that simplifies the process of managing your research sources and creating bibliographies."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#key-zotero-features",
    "href": "rm-data/slides/week01/week01.html#key-zotero-features",
    "title": "Introduction to Reproducible Research",
    "section": "Key Zotero Features",
    "text": "Key Zotero Features\n\nBrowser connector for easy capture of online sources\nPDF annotation and management\nCitation style editor for customized formatting\n\nMany of Standard styles are already included. So you dont have to worry about formatting.\n\nSynchronization across devices (cloud storage)\n\n\nThese features make Zotero an essential tool for managing your research literature and ensuring accurate, consistent citations in your work."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#integrating-zotero-with-your-workflow",
    "href": "rm-data/slides/week01/week01.html#integrating-zotero-with-your-workflow",
    "title": "Introduction to Reproducible Research",
    "section": "Integrating Zotero with Your Workflow",
    "text": "Integrating Zotero with Your Workflow\n\nUse Zotero Connector to save sources while browsing\nOrganize sources into collections for different projects\nUse tags and notes for efficient source management\nGenerate in-text citations and bibliographies in your documents\n\n\nIntegrating Zotero into your research workflow can significantly streamline your literature management and citation process."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#why-not-word-or-google-docs",
    "href": "rm-data/slides/week01/week01.html#why-not-word-or-google-docs",
    "title": "Introduction to Reproducible Research",
    "section": "Why not Word or Google Docs?",
    "text": "Why not Word or Google Docs?\n\nGit can keep track of changes in your code, and documents, but you need a good editor to write them.\nMSWord, Google Docs, etc are good for writing, binary files…Git cannot read them.\nThus, you need to write your code in plain text files (more later).\n\nFor example, most software code is written in plain text files.\nThis is also true for LaTeX, html and Markdown (more later).\n\nA good alternative for this is Visual Studio Code (VSC)."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#what-is-visual-studio-code",
    "href": "rm-data/slides/week01/week01.html#what-is-visual-studio-code",
    "title": "Introduction to Reproducible Research",
    "section": "What is Visual Studio Code?",
    "text": "What is Visual Studio Code?\n\nFree, open-source code editor by Microsoft that is constantly updated.\nSupports multiple programming languages (R, Python, julia, Stata (with plugins))\nRich ecosystem of extensions (for Git, Quarto, Stata, Latex, etc)\nIntegrated terminal (will become important later)\nUsually keeps your last session open, so you can pick up where you left off.\n\n\nVisual Studio Code has become one of the most popular code editors due to its flexibility, performance, and extensive feature set."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#setting-up-vsc",
    "href": "rm-data/slides/week01/week01.html#setting-up-vsc",
    "title": "Introduction to Reproducible Research",
    "section": "Setting Up VSC",
    "text": "Setting Up VSC\n\nDownload and install Visual Studio Code\nInstall language extensions for R, Python, or Stata This will give you syntax highlighting, code completion, and other language-specific features Even allow for running code from the editor\nInstall helpful extensions like GitLens and Copilot (Free AI for coding)\n\n\nWe’ll walk through the setup process and highlight some of the most useful extensions for economic research."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#what-can-vsc-do-for-you",
    "href": "rm-data/slides/week01/week01.html#what-can-vsc-do-for-you",
    "title": "Introduction to Reproducible Research",
    "section": "What can VSC do for you?",
    "text": "What can VSC do for you?\n\nAs said before. Great editor for plain text files Code (R, Julia, Stata, Python, etc). And documents (Markdown, LaTeX, etc).\nIt has a robust markdown support.\nIt has a built-in terminal, so you can run your code from the editor."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#introduction-to-quarto",
    "href": "rm-data/slides/week01/week01.html#introduction-to-quarto",
    "title": "Introduction to Reproducible Research",
    "section": "Introduction to Quarto",
    "text": "Introduction to Quarto\n\nNext-generation tool for literate programming\nSupports multiple languages (R, Python, Julia, Observable JS, Stata with plugins)\nIt can be used to create documents (PDF’s, Word, others), websites (like Mine), books (see examples), and presentations (like this one!)\nExtends and improves upon R Markdown"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#what-is-quarto",
    "href": "rm-data/slides/week01/week01.html#what-is-quarto",
    "title": "Introduction to Reproducible Research",
    "section": "What is Quarto?",
    "text": "What is Quarto?\n\nIs an open-source scientific and technical publishing system\nYou can use plain text markdown in your favorite editor.\nCreate dynamic content with Python, R, Julia, and Observable.\nPublish reproducible, production quality articles, presentations, dashboards, websites, blogs, and books in HTML, PDF, MS Word, ePub, and more.\nWrite using Pandoc markdown would allow you to including equations (LaTeX), citations (Zotero), crossrefs, figures panels, callouts, advanced layout, and more."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#how-quarto-works",
    "href": "rm-data/slides/week01/week01.html#how-quarto-works",
    "title": "Introduction to Reproducible Research",
    "section": "How Quarto Works?",
    "text": "How Quarto Works?\nWhen you render a .qmd file with Quarto, the executable code blocks are processed by Jupyter, and the resulting combination of code, markdown, and output is converted to plain markdown. Then, this markdown is processed by Pandoc, which creates the finished format."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#creating-a-quarto-document",
    "href": "rm-data/slides/week01/week01.html#creating-a-quarto-document",
    "title": "Introduction to Reproducible Research",
    "section": "Creating a Quarto Document",
    "text": "Creating a Quarto Document\n\nIn VSC, create a new file: Quarto document.\n\nSave it in the folder where you have your data and code.\n\nAdd or modify the YAML header for document metadata. This tells Quarto how to render the document, who wrote it, other relevant information.\n\nDocument Body: Markdown for text formatting (plain text), with minimal formatting.\nYou can include images, links, tables, citations, etc. But also code chunks for running and displaying analysis. In some cases (R), you can also add In-line code for dynamic text"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#another-flavor-of-quarto-visual-editor",
    "href": "rm-data/slides/week01/week01.html#another-flavor-of-quarto-visual-editor",
    "title": "Introduction to Reproducible Research",
    "section": "Another Flavor of Quarto: Visual Editor",
    "text": "Another Flavor of Quarto: Visual Editor\n\nQuarto also has a visual editor option. This is more user-friendly for first timers, and has a more WYSIWYG feel.\nIt does allow for more advanced features including Citations!\nSince Visual Editor is simpler to use, lets cover the basics of using plain text Quarto in VSC."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#first-the-yaml-header",
    "href": "rm-data/slides/week01/week01.html#first-the-yaml-header",
    "title": "Introduction to Reproducible Research",
    "section": "First, the YAML Header",
    "text": "First, the YAML Header\n\nYAML HeaderYAML Header HTMLYAML Header PDF\n\n\nYAML is not necessary for Quarto, but it is useful for metadata and formatting. It typically would include:\n\nTitle\nAuthor\nFormat (HTML, PDF, Word, etc)\nOther metadata (date, keywords, etc)\n\n\n\n---\ntitle: \"My Economic Analysis\"\nauthor: \"Your Name\"\nformat: html\ndate: last-modified\n---\n\n\n---\ntitle: \"My Economic Analysis\"\nauthor: \n  - name: Author One\n  - name: Author Two\nformat: \n  pdf: \n    number-sections: true\n    documentclass: article\ndate: today\n---"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#yaml-header-options",
    "href": "rm-data/slides/week01/week01.html#yaml-header-options",
    "title": "Introduction to Reproducible Research",
    "section": "YAML Header Options",
    "text": "YAML Header Options\nFor a template, you can use the following template.qmd\nFor more options see YAML PDF Options\n\nThis example shows the basic structure of a Quarto document. The YAML header sets document properties."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#then-the-body-text-formatting",
    "href": "rm-data/slides/week01/week01.html#then-the-body-text-formatting",
    "title": "Introduction to Reproducible Research",
    "section": "Then the Body: Text Formatting",
    "text": "Then the Body: Text Formatting\nQuarto uses Pandoc markdown for text formatting. This is a simple, plain text format that is easy to write and read in any text editor."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#then-the-body-text-formatting-1",
    "href": "rm-data/slides/week01/week01.html#then-the-body-text-formatting-1",
    "title": "Introduction to Reproducible Research",
    "section": "Then the Body: Text Formatting",
    "text": "Then the Body: Text Formatting\n\nHeadingsH RenderedFormatF RenderedListL rendered\n\n\n# Heading 1\n\n## Heading 2\n\n### Heading 3\n\n\n\nHeading 1\n\n\n\nHeading 2\nHeading 3\n\n\n\n\n*italics*, **bold**, ***bold italics***\n\n~~strikeout~~, `code`\n\nsuperscript^2^, subscript~2~\n\n\n\nitalics, bold, bold italics\nstrikeout, code\nsuperscript2, subscript2\n\n\n\nLists:\n\n- Item 1\n- Item 2\n  - Subitem 1\n  - Subitem 2\n\nLists:\n\n1. item 4\n    1. item 5 \n    1. item 5  \n2. ds\n\n\n\n\nLists:\n\nItem 1\nItem 2\n\nSubitem 1\nSubitem 2\n\n\n\nLists:\n\nitem 4\n\nitem 5\nitem 5\n\n\nds"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#heading-2",
    "href": "rm-data/slides/week01/week01.html#heading-2",
    "title": "Introduction to Reproducible Research",
    "section": "Heading 2",
    "text": "Heading 2\nHeading 3"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#authoring-in-quarto",
    "href": "rm-data/slides/week01/week01.html#authoring-in-quarto",
    "title": "Introduction to Reproducible Research",
    "section": "Authoring in Quarto",
    "text": "Authoring in Quarto\n\nQuarto supports various types of content for the creation of academic writting.\nThis include: equations, tables, images, footnotes, references and cross-references.\nWe will see how to include these in your document."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#equations",
    "href": "rm-data/slides/week01/week01.html#equations",
    "title": "Introduction to Reproducible Research",
    "section": "Equations",
    "text": "Equations\nQuarto supports LaTeX equations. You can include them in your document using the standard LaTeX syntax, for inline equations: $...$, and for display equations: $$...$$.\nFor example, the theory of relativity can be expressed as (\\(E=mc^2\\))$E=mc^2$. But also as:\n$$E = \\frac{{m \\cdot c^2}}{{\\sqrt{1 - \\frac{{v^2}}{{c^2}}}}}\n$$\n\\[E = \\frac{{m \\cdot c^2}}{{\\sqrt{1 - \\frac{{v^2}}{{c^2}}}}}\n\\]"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#equations-cross-references",
    "href": "rm-data/slides/week01/week01.html#equations-cross-references",
    "title": "Introduction to Reproducible Research",
    "section": "Equations cross-references",
    "text": "Equations cross-references\nDisplay equations can be cross-referenced. For that you need to add a label to the equation, and then reference it in the text.\nFor example:\n$$E = mc^2\n$${#eq-emc2}\n\\[E = mc^2\n\\qquad(1)\\]\nYou can reference this equation as @eq-emc2 which renders Equation 1.\nMore complex equations can be created using LaTeX syntax."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#tables-and-figures",
    "href": "rm-data/slides/week01/week01.html#tables-and-figures",
    "title": "Introduction to Reproducible Research",
    "section": "Tables and Figures",
    "text": "Tables and Figures\nFor Quarto, you can keep track of Tables and Figures. However, you can use “anything” as a figure or table.\nThe simplest approach is the following:\n\n\n:::{#tbl-mytable}\n  content...\n:::\n\n:::{#fig-myfig}\n  content...\n:::\n\nQuarto will not Care what is the content. A “Table” could be containing a figure, and a “Figure” could be a table.\nAnd you can reference them as @tbl-mytable and @fig-myfig. Quarto will take care of the numbering."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#tables-how",
    "href": "rm-data/slides/week01/week01.html#tables-how",
    "title": "Introduction to Reproducible Research",
    "section": "Tables: how?",
    "text": "Tables: how?\nYou can add tables 4 different ways:\n\nA file with markdown code: Markdown tables will render in any format.\nA file with LaTeX code: LaTeX tables will only render in PDF.\nA file with HTML code: HTML tables will render in any format.\n\nif you are using R, kable and gt will render tables in HTML and PDF."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#tables-markdown",
    "href": "rm-data/slides/week01/week01.html#tables-markdown",
    "title": "Introduction to Reproducible Research",
    "section": "Tables: Markdown",
    "text": "Tables: Markdown\n\n\n:::{#tbl-mytable}\n\n{{&lt; include table_example.txt &gt;}}\n\nThis are notes for the table\n\nAnd this the Title\n:::\n\nCrossreference:\n\n@tbl-mytable shows nothing.\n\n\n\n\nTable 1: And this the Title\n\n\n\n\n\nColumn 1\nColumn 2\nColumn 3\n\n\n\n\nData\nData\nData\n\n\nData\nData\nData\n\n\n\nThis are notes for the table\n\n\n\nCrossreference:\nTable 1 shows nothing.\n\nThe table is included in table_markdown.txt and is rendered in the document."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#tables-latex",
    "href": "rm-data/slides/week01/week01.html#tables-latex",
    "title": "Introduction to Reproducible Research",
    "section": "Tables: LaTeX",
    "text": "Tables: LaTeX\nThis wont render in HTML, but will render in PDF.\n\n\n:::{#tbl-mytable}\n\n{{&lt; include table_latex.txt &gt;}}\n\nThis are notes for the table\n\nAnd this the Title\n:::\n\n:::{#tbl-mytable}\n\n\\begin{tabular}{|c|c|c|}\n    \\hline\n    Column 1 & Column 2 & Column 3 \\\\\n    \\hline\n    Row 1 & Cell 1 & Cell 2 \\\\\n    \\hline\n    Row 2 & Cell 3 & Cell 4 \\\\\n    \\hline\n\\end{tabular}\n\nThis are notes for the table\n\nAnd this the Title\n:::"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#tables-html",
    "href": "rm-data/slides/week01/week01.html#tables-html",
    "title": "Introduction to Reproducible Research",
    "section": "Tables: HTML",
    "text": "Tables: HTML\nThere are two ways to include HTML tables in Quarto:\n\n\n:::{#tbl-mytable}\n\n{{&lt; include table_html.txt &gt;}}\n\nThis are notes for the table\n\nAnd this the Title\n:::\n\n:::{#tbl-mytable}\n\n```{=html}\n{{&lt; include table_html.txt &gt;}}\n```\n\nThis are notes for the table\n\nAnd this the Title\n:::"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#tables-figure",
    "href": "rm-data/slides/week01/week01.html#tables-figure",
    "title": "Introduction to Reproducible Research",
    "section": "Tables: Figure",
    "text": "Tables: Figure\nIf you are in a pinch, you can use a figure (from a well formated Excel file) as a table.\n\n\n:::{#tbl-mytable2}\n\n![](transition-git.jpg){width=50% fig-align=\"center\"}\n\nThis are notes for the table\n\nAnd this the Title\n:::\n\n\n\n\nTable 2: And this the Title\n\n\n\n\n\n\n\nThis are notes for the table"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#tables-comment",
    "href": "rm-data/slides/week01/week01.html#tables-comment",
    "title": "Introduction to Reproducible Research",
    "section": "Tables: comment",
    "text": "Tables: comment\nWhen producing tables for PDF, you may notice that tables do not always render where expected.\nLaTeX is trying to optimize the layout of the document.\nTo force a table to render where you want, you can use the tbl-pos=\"H\" attribute.\n:::{#tbl-mytable tbl-pos=\"H\"}\n\n{{&lt; include table_latex.txt &gt;}}\n\nThis are notes for the table\n\nAnd this the Title\n:::"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#figures",
    "href": "rm-data/slides/week01/week01.html#figures",
    "title": "Introduction to Reproducible Research",
    "section": "Figures",
    "text": "Figures\nAs with tables, you can include figures in your document using the following syntax:\n\n:::{#fig-myfig1}\n\n![](transition-git.jpg){width=50% fig-align=\"center\"}\n\nThis are notes for the Figure\n\nAnd this the title\n:::\nYou can use .png, .jpg, .svg, .pdf, and other image formats, with few restrictions.\nWorks across all formats.\n\n\n\n\n\n\n\n\n\nThis are notes for the Figure\n\n\nFigure 1: And this the title\n\n\n\n::::"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#footnotes",
    "href": "rm-data/slides/week01/week01.html#footnotes",
    "title": "Introduction to Reproducible Research",
    "section": "Footnotes",
    "text": "Footnotes\nFootnotes should not be used excessively, but they can be useful for additional information or references.\nTo add a footnote, use the following syntax:\nThis is a footnote[^1]. But this too ^[This is another footnote].\n\n[^1]: This is the text of the footnote.\nThis is a footnote1. But this too 2.\nThis is the text of the footnote.This is another footnote"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#citations-source-code",
    "href": "rm-data/slides/week01/week01.html#citations-source-code",
    "title": "Introduction to Reproducible Research",
    "section": "Citations: Source code",
    "text": "Citations: Source code\nQuarto supports citations using the @key syntax. It you cite a reference, it will be included in the bibliography.\nFirst, you need to include a .bib file with your references in the YAML header.\nbibliography: references.bib\nAccording to @smith2021, this citation works. However not everyone likes it [@doe2021]. However [@smith2021; @doe2021] works too.\nAccording to Smith (2021), this citation works. However not everyone likes it (Doe 2021). However (Smith 2021; Doe 2021) works too.\n\n\nDoe, Jane. 2021. “Advancements in Machine Learning Algorithms.” International Journal of Artificial Intelligence 5 (3): 50–65. https://doi.org/10.5678/ijai.2021.67890.\n\n\nSmith, John. 2021. “A Comprehensive Study on Data Analysis Techniques.” Journal of Data Science 10 (2): 100–120. https://doi.org/10.1234/jds.2021.12345."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#citations-visual-editor",
    "href": "rm-data/slides/week01/week01.html#citations-visual-editor",
    "title": "Introduction to Reproducible Research",
    "section": "Citations: Visual Editor",
    "text": "Citations: Visual Editor\nIf you are using Visual Editor (cntrl+shift+F4), you can add citations by using cntrl+shift+F8.\nThis will open a search box where you can search for the reference you want to cite. And then add it to the text.\nQuarto will add the reference.bib file to the YAML header, and will add the information to the bibliography."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#last-but-not-least-rendering",
    "href": "rm-data/slides/week01/week01.html#last-but-not-least-rendering",
    "title": "Introduction to Reproducible Research",
    "section": "Last but not least: Rendering",
    "text": "Last but not least: Rendering\nTo render your document you have two options:\n\nPress the Botton on the upper right corner of the VSC window. This creates a preview of the document, and you can see how it looks. This also renders one format at a time.\nUse the terminal. You can use the terminal to render the document in multiple formats at once.\n\nJust type: quarto render path_name/file.qmd and it will render the document in the formats specified in the YAML header.\n\nThere are other options, you can explore at your leisure."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#project-organization",
    "href": "rm-data/slides/week01/week01.html#project-organization",
    "title": "Introduction to Reproducible Research",
    "section": "Project Organization",
    "text": "Project Organization\nThis is one more thing we never really learn, but it is important! Declutter your projects!\nGood Organization will make it easy to find things, and to share your work with others.\n\nOne Project, One Folder\nUse consistent folder structures\nSeparate data, code, and output\nUse relative paths for reproducibility\nCreate a README file to guide others\n\n\nGood project organization is crucial for reproducibility and collaboration. It helps others (and your future self) understand and navigate your project easily."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#suggested-folder-structure",
    "href": "rm-data/slides/week01/week01.html#suggested-folder-structure",
    "title": "Introduction to Reproducible Research",
    "section": "Suggested Folder Structure",
    "text": "Suggested Folder Structure\n\nData and CodeResults and Documents\n\n\nroot-project/\n├── data/\n│   ├── raw/       &lt;- Depending on size: full raw data or instructions to get it.\n│   ├── processed/ &lt;- Stores your cleaned data\n│   ├── final/     &lt;- Opt: Stores the final Data for analysis\n│   └── doc/       &lt;- Opt: Manuals, dictionaries, codebooks, etc.\n├── code/          &lt;- Will contain all code files: Try to keep \"Small\" files (specific tasks).\n│   ├── 01-setup.do    &lt;- Setup file (installing packages, setting up directories, etc.)\n│   ├── 02-cleaning.do &lt;- Cleaning file (data cleaning and preprocessing)\n|   ├── 03-analysis.do &lt;- Analysis file (main analysis script)\n|   ├── 04-visuals.do  &lt;- Opt: Visualizations file and (creating plots and figures)\n\n\n├── results/\n│   ├── figures/ &lt;- All the figures you have created.\n│   ├── tables/  &lt;- All the tables you have created.\n│   └── other/   &lt;- .ster, .log, etc\n├── reports/\n│   ├── proposal/      &lt;- OPT: Include the proposal and any feedback you have received.\n│   ├── papers/        &lt;- May include the Raw tex (Qmd, Tex, bib) and the PDF\n│   └── presentations/ &lt;- If you have any presentations, include them here.\n└── README.md &lt;- A readme file with instructions on how to understand the project."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#package-management",
    "href": "rm-data/slides/week01/week01.html#package-management",
    "title": "Introduction to Reproducible Research",
    "section": "Package Management",
    "text": "Package Management\n\nDepending on the Software, it may be easier, or harder to manage packages.\n\nStata: ssc, net\nR: renv, packrat, checkpoint\nPython: pip, conda\n\nIdeally document versions, or “save” all the packages you are using.\n\nIn Stata, most required tools are available out of the box.\n\nYou may want to use ssc or net to install additional packages.\nBut some may require going to the authors website.\n\n\n\nPackage management ensures that your code runs with the same package versions across different environments, enhancing reproducibility."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#data-storage-and-sharing",
    "href": "rm-data/slides/week01/week01.html#data-storage-and-sharing",
    "title": "Introduction to Reproducible Research",
    "section": "Data Storage and Sharing",
    "text": "Data Storage and Sharing\n\nFor Small Projects, you can use GitHub to store your data.\nFor larger ones, use repositories like Zenodo or OSF for data sharing\nProvide metadata and documentation for your datasets. Specially when you create it.\nConsider data anonymization when necessary, or synthetic data if not possible to share.\nUse proper licensing for your data and code\n\n\nProper data management and sharing practices are essential for reproducibility and can increase the impact of your research."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#ai-and-generative-assistants",
    "href": "rm-data/slides/week01/week01.html#ai-and-generative-assistants",
    "title": "Introduction to Reproducible Research",
    "section": "AI and Generative Assistants",
    "text": "AI and Generative Assistants\n\nAI and GPT can assist in coding, writing, and data analysis. But be aware of their limitations.\nGitHub Copilot can help with code completion and suggestions, as well as text completition.\n\nIt uses your “work-environment” to suggest code, or comments.\n\nBut, like other tools, it is not perfect. It may suffer from allucinations, or code that does not work.\n\nIt is a tool, not a replacement for your work.\n\nUse it to speed up your work, but always check the results.\n\nExample: Recently, we use AI to go over many files of data summaries, and it was able to find the data we were looking for. Required some cleaning, but it was faster than doing it manually.\n\n\n\nAI coding assistants can significantly boost productivity, but it’s important to understand their capabilities and limitations, especially in a research context."
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#example",
    "href": "rm-data/slides/week01/week01.html#example",
    "title": "Introduction to Reproducible Research",
    "section": "Example",
    "text": "Example\n\nWe will go over the repository for a mock-up project, created exclusively for this course.\nThe repository contains all the elements we have discussed: data, code, documents, and more.\nThis is a simple example, but it illustrates how you can structure your own projects for reproducibility and transparency.\nAssume the project to be a -mock- academic paper. Its not meant to be a real paper.\n\nRepository: https://github.com/friosavila/rm-example"
  },
  {
    "objectID": "rm-data/slides/week01/week01.html#thank-you",
    "href": "rm-data/slides/week01/week01.html#thank-you",
    "title": "Introduction to Reproducible Research",
    "section": "Thank You!",
    "text": "Thank You!\nQuestions? Feedback?\n\nThank you for participating in this course on reproducible research in economics. We hope you found it valuable and are excited to apply these tools and practices in your own work."
  },
  {
    "objectID": "rm-data/slides/week03.html#motivation",
    "href": "rm-data/slides/week03.html#motivation",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Motivation",
    "text": "Motivation\n\nYou want to understand the market conditions for hotels in Vienna, using prices.\n\nHow should you start the analysis itself?\nHow to describe the data and present the key features?\nHow to explore the data and check whether it is clean enough for (further) analysis?"
  },
  {
    "objectID": "rm-data/slides/week03.html#exploratory-data-analysis-eda---describing-variables",
    "href": "rm-data/slides/week03.html#exploratory-data-analysis-eda---describing-variables",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Exploratory data analysis (EDA) - describing variables",
    "text": "Exploratory data analysis (EDA) - describing variables\n5 reasons to do EDA:\n\nTo check data cleaning (part of iterative process)\nTo guide subsequent analysis (for further analysis)\nTo give context of the results of subsequent analysis (for interpretation)\nTo ask additional questions (for specifying the (research) question)\nOffer simple, but possibly important answers to questions.\n\nAll and all, EDA should help you identify some of the key features of the data and how they relate to each other."
  },
  {
    "objectID": "rm-data/slides/week03.html#key-tasks-describe-variables",
    "href": "rm-data/slides/week03.html#key-tasks-describe-variables",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Key tasks: describe variables",
    "text": "Key tasks: describe variables\n\n\nLook at key variables:\n\nwhat values they can take and\nhow often they take each of those values.\nare there extreme values\n\n\nDescribe what you see:\n\nDescriptive statistics: key features summarized\nto understand variables you work with\nto make comparisons"
  },
  {
    "objectID": "rm-data/slides/week03.html#frequency-values",
    "href": "rm-data/slides/week03.html#frequency-values",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Frequency values",
    "text": "Frequency values\n\nThe frequency or more precisely, absolute frequency or count, of a value of a variable is simply the number of observations with that particular value.\nThe relative frequency is the frequency expressed in relative, or percentage, terms: the proportion of observations with that particular value among all observations.\n\nIf missing values exist, Relative frequency could be relative to total observations or to non-missing observations.\n\nCan also use Probabilities: the relative likelihood of a value of a variable.\nHow to? tabulate [variable], or better yet fre [variable] (SSC install)"
  },
  {
    "objectID": "rm-data/slides/week03.html#the-distribution",
    "href": "rm-data/slides/week03.html#the-distribution",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The distribution",
    "text": "The distribution\nA key part of EDA is to look at (empirical) distribution of most important variables.\n\nAll variables have a distribution.\nThe distribution determines the frequency of each value in the data.\nMay be expressed in terms of absolute frequencies (number of observations) or relative frequencies (percent of observations).\nThe distribution of a variable completely describes the variable as it occurs in the data."
  },
  {
    "objectID": "rm-data/slides/week03.html#histograms",
    "href": "rm-data/slides/week03.html#histograms",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Histograms",
    "text": "Histograms\nHistogram reveals important properties of a distribution:\n\nAs with tabulation, Histograms show the empirical distribution of a variable.\nThey may allow you to see:\n\nNumber and location of modes: these are the peaks in the distribution (compared to neighbors).\n\nShape of the distribution:\n\nCenter, tails, if its symmetric or not, Long [left or right tails], and extreme values.\n\nExtreme values: values that are very different from the rest. Extreme values are at the far end of the tails of histograms. (may even be signal of errors or missing)"
  },
  {
    "objectID": "rm-data/slides/week03.html#extreme-values",
    "href": "rm-data/slides/week03.html#extreme-values",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Extreme values",
    "text": "Extreme values\n\nExtreme values are substantially larger or smaller values for one or a handful of observations. Big departures from distribution.\nNeed conscious decision.\n\nIs this an error? (drop or replace)\nIs this not an error, code for missing? (replace)\nIs this not an error but not part of what we want to talk about? (drop?)\nIs this an integral feature of the data? (keep)"
  },
  {
    "objectID": "rm-data/slides/week03.html#how-to",
    "href": "rm-data/slides/week03.html#how-to",
    "title": "Exploratory Analysis and Comparisons",
    "section": "How to?",
    "text": "How to?\nHistograms in Stata are created with the histogram command.\nhistogram [variable] [if in] [fweight], [bin(#) width(#) discrete] ///\n          [density] [frequency] [fraction] \n\nYou can only create the histogram of one variable at a time. (unless combined)\nand you can determine how “fine” or “coarse” the histogram is. (A bit of art)"
  },
  {
    "objectID": "rm-data/slides/week03.html#hotel-stars-histograms",
    "href": "rm-data/slides/week03.html#hotel-stars-histograms",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Hotel Stars histograms",
    "text": "Hotel Stars histograms\n\n\n\n\nCode\nqui:histogram stars, d frequency ///\n    scale(1.5) addlabels xlabel(1(.5)5)\n\n\n\n\n\nAbsolute frequency\n\n\n\n\n\n\n\nCode\nqui:histogram stars, d percent ///\n    scale(1.5) addlabels xlabel(1(.5)5)\n\n\n\n\n\nRelative frequency"
  },
  {
    "objectID": "rm-data/slides/week03.html#hotel-price-histograms",
    "href": "rm-data/slides/week03.html#hotel-price-histograms",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Hotel price histograms",
    "text": "Hotel price histograms\n\n\n\n\nCode\nqui:histogram price, d  ///\n    scale(1.5) width(1) \n\n\n\n\n\nPrice Distribution 1$ bin\n\n\n\n\n\n\n\nCode\nqui:histogram price,  ///\n    scale(1.5) width(30)\n\n\n\n\n\nPrice Distribution 30$ bin"
  },
  {
    "objectID": "rm-data/slides/week03.html#alternative-kdensity",
    "href": "rm-data/slides/week03.html#alternative-kdensity",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Alternative Kdensity",
    "text": "Alternative Kdensity\n\nPerhaps one weakness of Histograms are the implicit binning. The density “jumps” from bin to bin.\nAn alternative would be use smaller bins, requesting jumps to be smoother.\nThis is done with Kernel Density Estimation (KDE) plots. kdensity in Stata.\nTwo limitations:\n\nNot useful with discrete or limited variables\nAlso requires the use of bandwiths"
  },
  {
    "objectID": "rm-data/slides/week03.html#kdensity-for-price",
    "href": "rm-data/slides/week03.html#kdensity-for-price",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Kdensity for price",
    "text": "Kdensity for price\n\nExmp1Exmp2Exmp3\n\n\n\n\nCode\n *| kdensity price,   ///\n    scale(1.5)  note(\"\") bw(10)\n\n\n\n\n\n\nCode\n kdensity price,   ///\n    scale(1.5)  note(\"\") bw(1)\n\n\n\n\n\nBandwidth of 1\n\n\n\n\n\n\n\n\nCode\n kdensity price,   ///\n    scale(1.5)  note(\"\") bw(30)\n\n\n\n\n\nBandwidth of 30"
  },
  {
    "objectID": "rm-data/slides/week03.html#eda-and-cleaning---vienna-hotels",
    "href": "rm-data/slides/week03.html#eda-and-cleaning---vienna-hotels",
    "title": "Exploratory Analysis and Comparisons",
    "section": "EDA and cleaning - Vienna hotels",
    "text": "EDA and cleaning - Vienna hotels\n\nStart with full data N=428\nTabulate key qualitative variables\nAccommodation type - could be apartment, etc. Focus on hotels. N=264\nStars - focus on 3, 3.5, 4 stars. &lt;3 not well covered, &gt;4 vary a lot. N=218\nLook at quantitative variables, focus on extreme values.\nStart with price. p=1012 likely error drop. keep others N= 217\nDistance: some hotels are far away. define cutoff. drop beyond 8km N=214\nCheck why hotels could be far away. Find variable city_actual. Tabulate. Realise few hotels are not in Vienna. Drop them. N=207\nthe final cut: Hotels, 3 to 4 stars, below 1000 euros, less than 8km from center, in Vienna actual N=207."
  },
  {
    "objectID": "rm-data/slides/week03.html#what-are-summary-statistics",
    "href": "rm-data/slides/week03.html#what-are-summary-statistics",
    "title": "Exploratory Analysis and Comparisons",
    "section": "What are summary statistics?",
    "text": "What are summary statistics?\n\nSummary statistics are numbers that summarize the distribution of a variable.\n\nThey provide numbers for the central tendency, spread, and shape.\n\nSummary statistics are used to describe the data and to make comparisons between different datasets."
  },
  {
    "objectID": "rm-data/slides/week03.html#summary-statistics-central-tendency",
    "href": "rm-data/slides/week03.html#summary-statistics-central-tendency",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Summary statistics: Central Tendency",
    "text": "Summary statistics: Central Tendency\n\nThe most used statistic is the mean:\n\n\\[\\bar{x} = \\frac{\\sum x_i}{n}\\]\nwhere \\(x_i\\) is the value of variable \\(x\\) for observation \\(i\\) in the dataset that has \\(n\\) observations in total. Two key features:\n\nAdd a constant, the mean changes by the same constant.\nMultiply by a constant, the mean changes by the same constant."
  },
  {
    "objectID": "rm-data/slides/week03.html#the-expected-value",
    "href": "rm-data/slides/week03.html#the-expected-value",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The Expected value",
    "text": "The Expected value\n\nThe expected value is the value that one can expect for a randomly chosen observation. It relates to the distribution of the population, not the sample\nThe notation for the expected value is \\(E[x]\\).\nFor a quantitative variable, the expected value is the mean\nFor a qualitative variable, means are not defined, but you can consider proportions."
  },
  {
    "objectID": "rm-data/slides/week03.html#the-median-and-other-quantiles",
    "href": "rm-data/slides/week03.html#the-median-and-other-quantiles",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The median and other quantiles",
    "text": "The median and other quantiles\n\nThe median is another statistic of central tendency. It indicates the middle value of the distribution. Its a special case of quantiles.\n\nIts main advantage with the mean is that it is less sensitive to extreme values.\n\nquantiles: a quantile is the value that divides the observations in the dataset to two parts in specific proportions.\n\\[Q_\\tau(Y) \\rightarrow \\frac{1}{N}\\sum I(y&lt;Q_\\tau) = \\tau \\]\nThe median and 25th and 75th percentiles are the most common quantiles used in EDA."
  },
  {
    "objectID": "rm-data/slides/week03.html#the-mode",
    "href": "rm-data/slides/week03.html#the-mode",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The mode",
    "text": "The mode\n\nYet another measure of central tendency.\nThe mode is the value with the highest frequency in the data (the most common).\nIf distributions have multimodal, you may be able to obtain multiple modes.\nMultiple modes are apart from each other, each standing out in its “neighborhood”, but they may have different frequencies."
  },
  {
    "objectID": "rm-data/slides/week03.html#summary",
    "href": "rm-data/slides/week03.html#summary",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Summary",
    "text": "Summary\n\nThe mean, median and mode are different statistics for the central value of the distribution\nThey try to provide you the most representative value of the distribution.\n\nThe mode is the most frequent value\nThe median is the middle value\nThe mean is the value that one can expect for a randomly chosen observation.\n\n\ntabstat vars, stats(mean p50 )\nsummarize vars, detail"
  },
  {
    "objectID": "rm-data/slides/week03.html#spread-of-distributions",
    "href": "rm-data/slides/week03.html#spread-of-distributions",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Spread of distributions",
    "text": "Spread of distributions\n\nSpread of distributions is often used in analysis.\n\nIt tells you how concentrated or dispersed the values of a variable are.\n\nThe statistics that measure the spread of distributions are the range, inter-quantile ranges, the standard deviation and the variance."
  },
  {
    "objectID": "rm-data/slides/week03.html#ranges",
    "href": "rm-data/slides/week03.html#ranges",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Ranges",
    "text": "Ranges\nThere are three common measures of ranges:\n\nThe range is the difference between the highest value (the maximum) and the lowest value (the minimum) of a variable.\nThe inter-quantile ranges is the difference between two quantiles- the third quartile (the 75th percentile) and the first quartile (the 25th percentile). Can be used as an alternative to Standard deviation.\nThe 90-10 percentile range gives the difference between the 90th percentile and the 10th percentile."
  },
  {
    "objectID": "rm-data/slides/week03.html#standard-deviation",
    "href": "rm-data/slides/week03.html#standard-deviation",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Standard deviation",
    "text": "Standard deviation\nThe most widely used measure of spread is the standard deviation, and Its square is the variance.\n\\[\n\\begin{aligned}\nVar[x] &= \\frac{\\sum (x_i - \\bar{x})^2}{n}=S^2_x \\\\\nStd[x] &= \\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n}}=S_x\n\\end{aligned}\n\\]\n\nThe variance is less intuitive measure (Squared), but easier to work with (mean)\nThe SD captures typical (not Mean) differences from the mean.\nFor the same mean, higher SD means more volatility."
  },
  {
    "objectID": "rm-data/slides/week03.html#coefficient-of-variation",
    "href": "rm-data/slides/week03.html#coefficient-of-variation",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Coefficient of variation",
    "text": "Coefficient of variation\nUnit Free alternative, Coefficient of variation:\n\\[CV = \\frac{Std[x]}{\\bar{x}}\n\\]\n\nThe coefficient of variation is the standard deviation divided by the mean. It reads, how much variation is there in the data relative to the mean."
  },
  {
    "objectID": "rm-data/slides/week03.html#other-uses-for-sd-standardized-values",
    "href": "rm-data/slides/week03.html#other-uses-for-sd-standardized-values",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Other uses for SD: Standardized values",
    "text": "Other uses for SD: Standardized values\nThe SD is often used to re-calculate differences between values in order to express them as typical distance.\n\\[x_{standardized} = \\frac{(x - \\bar{x})}{Std[x]}\n\\]\n\nThe standardized value has a mean of zero and a standard deviation of one.\nRepresents the difference from the mean in units of standard deviation.\n\nFor example: a standardized value of one shows a value is one standard deviation larger than the mean; a standardized value of negative one shows a value is one standard deviation smaller than the mean"
  },
  {
    "objectID": "rm-data/slides/week03.html#distribution-shape-skewness",
    "href": "rm-data/slides/week03.html#distribution-shape-skewness",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Distribution Shape: Skewness",
    "text": "Distribution Shape: Skewness\n\nA distribution is skewed if it isn’t symmetric.\n\nIt may be skewed in two ways, having a long left tail or having a long right tail.\nExample: hotel price distributions having a long right tail\n\nSkewness and the presence of extreme values are related.\nWhen extreme values are important for the analysis, skewness of distributions is important, too."
  },
  {
    "objectID": "rm-data/slides/week03.html#skewness-measures",
    "href": "rm-data/slides/week03.html#skewness-measures",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Skewness measures",
    "text": "Skewness measures\nThere are two common measures of skewness:\n\\[\nSk^1 = \\frac{(\\bar{x} - median(x))}{Std[x]} \\text{ and } Sk^2 = \\frac{\\sum(x_i-\\bar x)^3}{Std[x]^3}\n\\]\n\nWhen the distribution is symmetric its mean = median.\nSkewed to the right \\(\\bar x &gt; Q_{50}(x)\\).\nWhen a distribution is skewed with a long left tail the mean is smaller than the median\nTo make this measure comparable, better to standardize the measure\n\\(SK^2\\) is another Skewness measure. if Possitive, Skewed to the right, if negative, to the left."
  },
  {
    "objectID": "rm-data/slides/week03.html#stata-corner-how-to",
    "href": "rm-data/slides/week03.html#stata-corner-how-to",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Stata Corner: How to?",
    "text": "Stata Corner: How to?\nTwo basic options to get summary statistics in Stata:\n\nsummarize command: provides basic statistics for all variables in the dataset. Include detail option for more statistics.\ntabstat command: provides more flexibility. You can choose which statistics to show and for which variables.\nuse estpost to store the results and create well formatted tables.\nSee Stata Summary Statistics for examples on how to use these commands."
  },
  {
    "objectID": "rm-data/slides/week03.html#visualizing-summary-statistics",
    "href": "rm-data/slides/week03.html#visualizing-summary-statistics",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Visualizing summary statistics",
    "text": "Visualizing summary statistics\n\nAs mentioned before Histograms are a good way to visualize the distribution of a variable.\nHowever, if you would like to also visualize the summary statistics, you can use box plots\nThe box plot is a visual representation of many quantiles and extreme values."
  },
  {
    "objectID": "rm-data/slides/week03.html#box-plot",
    "href": "rm-data/slides/week03.html#box-plot",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Box Plot",
    "text": "Box Plot\n\nFull SampleBy Stars\n\n\n\n\nCode\nuse data_slides/hotels-vienna.dta, clear\nqui:drop if price&gt;800\ngraph box price, scale(1.4)  ///\n  ytitle(\"Price in dollars (log Scale)\") \n\n\n\n\n\nBox Plot: Viena prices\n\n\n\n\n\n\n\n\nCode\nuse data_slides/hotels-vienna.dta, clear\nqui:drop if price&gt;800\n graph box price if stars&gt;1, scale(1.4) ///\n  over(stars)  xsize(10) ysize(4) ///\n  ytitle(\"Price in dollars (log Scale)\")  \n\n\n\n\n\nBox Plot: Viena prices"
  },
  {
    "objectID": "rm-data/slides/week03.html#theoretical-distributions",
    "href": "rm-data/slides/week03.html#theoretical-distributions",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Theoretical distributions",
    "text": "Theoretical distributions\n\nTheoretical distributions are distributions of variables with idealized properties.\nTheoretical distributions are fully captured by few parameters: these are statistics determine the whole distributions\nFor example, the normal distribution is fully captured by two parameters: the mean and the standard deviation.\nThey may not accomodate empirical data"
  },
  {
    "objectID": "rm-data/slides/week03.html#theoretical-distributions-1",
    "href": "rm-data/slides/week03.html#theoretical-distributions-1",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Theoretical distributions",
    "text": "Theoretical distributions\nTheoretical distributions can be helpful:\n\nHave well-known properties!\nIn real life, many variables surprisingly close to theoretical distributions.\nWill be useful when generalizing from data"
  },
  {
    "objectID": "rm-data/slides/week03.html#the-normal-distribution",
    "href": "rm-data/slides/week03.html#the-normal-distribution",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The Normal distribution",
    "text": "The Normal distribution\n\n\n\nHistogram is bell-shaped\nOutcome (event), can take any value\nDistribution is captured by \\(\\mu\\) the mean and \\(\\sigma\\) the SD\nSymmetric = median, mean (and mode) are the same."
  },
  {
    "objectID": "rm-data/slides/week03.html#the-log-normal-distribution",
    "href": "rm-data/slides/week03.html#the-log-normal-distribution",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The log-normal distribution",
    "text": "The log-normal distribution\n\n\n\nAsymmetrically distributed with long right tails.\n\nDerived from a normally distributed variable (x), transform it: (\\(x^* = e^x\\)). The result is a distributed log-normal.\n\nAlways non-negative\nExample distributions of income, or firm size.\n\n\n\n\nNumber of observations (_N) was 0, now 10,000.\n(bin=40, start=.79124224, width=1.5880267)"
  },
  {
    "objectID": "rm-data/slides/week03.html#the-normality-of-reality",
    "href": "rm-data/slides/week03.html#the-normality-of-reality",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The Normality of Reality",
    "text": "The Normality of Reality\n\nQuite suprisingly, many variables tend to follow normal distributions.\n\nEspecially when adding them up.\n\nMay not be a good approximation when\n\nThere are reasons for non-symmetry (e.g. income)\nIf extreme values are “common”\n\nVariables are well approximated by the log-normal if they are the result of many things multiplied."
  },
  {
    "objectID": "rm-data/slides/week03.html#data-visualization-essentials",
    "href": "rm-data/slides/week03.html#data-visualization-essentials",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Data Visualization Essentials",
    "text": "Data Visualization Essentials\n\n\n\nPurposeful Decision-Making\n\nAvoid default settings\nDefine purpose, focus, and audience\nChoose appropriate graph type\n\nKey Considerations\n\nData type (qualitative, quantitative, time series)\nFormatting (colors, fonts, sizes)\nEssential elements: title, axis labels, legend\n\n\n\n\nOne Graph, One Message\n\nTailor complexity to audience (general vs. specialist)\nBe explicit about purpose and target audience"
  },
  {
    "objectID": "rm-data/slides/week03.html#data-visualization-process",
    "href": "rm-data/slides/week03.html#data-visualization-process",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Data Visualization Process",
    "text": "Data Visualization Process\n\n\n\nPlanning\n\nDetermine content and audience\nSelect graph type and elements\nSeek help when needed (AI, online resources)\n\nExecution\n\nInclude supporting elements for understanding\nEnsure readability (use scale() function)\n\n\n\n\nEssential Components\n\nTitle (if not in the document)\nAxis titles and labels (what’s being measured)\nLegend (group explanations)\n\nFinal Check\n\nVerify all elements support the main message\nConfirm graph is clear and accessible to the audience\n\n\n\nSee DataViz for a guide of how to create graphs in Stata."
  },
  {
    "objectID": "rm-data/slides/week03.html#ai-and-data-explorationviz",
    "href": "rm-data/slides/week03.html#ai-and-data-explorationviz",
    "title": "Exploratory Analysis and Comparisons",
    "section": "AI and data exploration/Viz",
    "text": "AI and data exploration/Viz\n\nAI is very good at describing the data, if you give it the tools (data)\nPretty good with python, but less proficient with Stata for complex graphs.\nStill good to have someone to ask without judgement."
  },
  {
    "objectID": "rm-data/slides/week03.html#summary-steps-of-eda",
    "href": "rm-data/slides/week03.html#summary-steps-of-eda",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Summary steps of EDA",
    "text": "Summary steps of EDA\n\nFirst focus on the most important variables. Go back to look at others if subsequent analysis suggests to.\nFor qualitative variables, list relative frequencies.\nFor quantitative variables, look at histograms. May decide for transformation, learn about key aspects of data.\nCheck for extreme values. Decide what to do with them.\nLook at summary statistics. It may prompt actions, such as focusing on some part of the dataset.\nDo further exploration if necessary (time series data, comparisons across groups of observations, correlations, etc.)"
  },
  {
    "objectID": "rm-data/slides/week03.html#motivation-1",
    "href": "rm-data/slides/week03.html#motivation-1",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Motivation",
    "text": "Motivation\n\nAre larger companies better managed?\n\nAnswering this question may help in benchmarking management practices in a specific company, assessing the value of a company, or estimating the potential benefits of a merger between two companies.\nTo answer this question you downloaded data from the World Management Survey.\n\nHow should you use the data to measure firm size and the quality of management?\nHow should you assess whether larger companies are better managed?"
  },
  {
    "objectID": "rm-data/slides/week03.html#the-y-and-the-x",
    "href": "rm-data/slides/week03.html#the-y-and-the-x",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The \\(y\\) and the \\(x\\)",
    "text": "The \\(y\\) and the \\(x\\)\n\nMuch of data analysis is built on comparing values of a \\(y\\) variable against one, or more, \\(x\\) variables.\nOur job is to uncover the patterns of association:\n\nHow observations with particular values of one variable (\\(x\\)) tend have particular values of the other variable (\\(y\\)).\n\nThe role of \\(y\\) is different from the role of \\(x\\).\n\nWe are interested in \\(y\\)\n\\(X's\\) are factors that you will use to analyze \\(y\\)."
  },
  {
    "objectID": "rm-data/slides/week03.html#the-y-and-the-x-1",
    "href": "rm-data/slides/week03.html#the-y-and-the-x-1",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The \\(y\\) and the \\(x\\)",
    "text": "The \\(y\\) and the \\(x\\)\n\nThis asymmetry comes from the goal of our analysis.\nGoal 1: predicting the value of a \\(y\\) variable with the help of other variables - many \\(x\\) variables, such as \\(x_1\\), \\(x_2\\),…\n\nThis is more useful when we do not know \\(y\\) but know \\(x\\).\n\nGoal 2: learn about the effect of a causal variable \\(x\\) on an outcome variable \\(y\\).\nAssuming everything else remains constant, What the value of \\(y\\) would be if we could change \\(x\\)"
  },
  {
    "objectID": "rm-data/slides/week03.html#comparison-and-conditioning",
    "href": "rm-data/slides/week03.html#comparison-and-conditioning",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Comparison and conditioning",
    "text": "Comparison and conditioning\n\nSimilar ideas: Comparison \\(\\rightarrow\\) conditioning\nWe compare \\(y\\), by values of \\(x\\) \\(\\rightarrow\\) we condition y on x.\n\n\\(x\\) (by the values of which we make comparisons) \\(\\rightarrow\\) conditioning variable.\n\\(y\\) \\(\\rightarrow\\) outcome variable.\n\nCompare salaries of workers (\\(y\\)) with low and high level of education (\\(x\\))\n\nsalary is the outcome\neducation is the conditioning variable."
  },
  {
    "objectID": "rm-data/slides/week03.html#comparisons-and-conditional-distributions",
    "href": "rm-data/slides/week03.html#comparisons-and-conditional-distributions",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Comparisons and conditional distributions",
    "text": "Comparisons and conditional distributions\n\n\n\nThe conditional distribution of a variable is the distribution of the outcome variable given the conditioning variable: \\(f(y|X=x)\\)\nStraightforward if \\(x\\) is qualitative (simple if binary)\nWith quantitative variables, this definition is less intuitive.\n\n\n\n\nCode\nqui: use \"data_slides/hotels-vienna-london\", clear\ndrop if price &gt; 1000\nset scheme white2\ncolor_style tableau\ntwo (kdensity price) ///\n(kdensity price if city==\"Vienna\") ///\n(kdensity price if city==\"London\"), ///\nlegend(order(1 \"All\" 2 \"Vienna\" 3 \"London\")) ///\nxtitle(\"Hotel Prices\") xsize(9) ysize(6)\n\n\n(395 observations deleted)"
  },
  {
    "objectID": "rm-data/slides/week03.html#conditional-statistic",
    "href": "rm-data/slides/week03.html#conditional-statistic",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Conditional statistic",
    "text": "Conditional statistic\nIf there is a Conditional distribution, there is a conditional statistic.\n\nConditional Stat is the Stat of a variable for each value of the conditioning variable.\n\nThe conditional expectation of variable y for different values of variable \\(x\\) is \\(E[y|x]\\)\n\nThis is a function: for a value of \\(x\\), the conditional expectation is the expected value of \\(y\\) for observations that have that \\(x\\) value\nIt gives different values for different values of \\(x\\)."
  },
  {
    "objectID": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size",
    "href": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Case Study - Management quality and firm size",
    "text": "Case Study - Management quality and firm size\n\nQuestion: Are larger Firms Better managed?\nData: World Management Survey\nAnswering this questions may help inform policy decisions.\nHow to measure firm size and quality of management?"
  },
  {
    "objectID": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size-1",
    "href": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size-1",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Case Study - Management quality and firm size",
    "text": "Case Study - Management quality and firm size\n\nInterviews by CEO/senior managers, based on that a score is given. Average across different domains.\n\ntracking and reviewing performance or\ntime horizon and breadth of targets, etc\n\nNormalized - standardized score\nFirm size: Consider three bins: small (100–199), medium (200–999), large (1000+)"
  },
  {
    "objectID": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size-2",
    "href": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size-2",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Case Study - Management quality and firm size",
    "text": "Case Study - Management quality and firm size\n\n\n\n\n\n\n\nmean\np50\nsd\n\n\n\n\nSmall\n2.68\n2.78\n0.51\n\n\nMedium\n2.94\n3.00\n0.62\n\n\nLarge\n3.19\n3.08\n0.55\n\n\nTotal\n2.94\n2.94\n0.60\n\n\nObservations\n300\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmall\nMedium\nLarge\nTotal\n\n\n\n\n1\n19.44\n8.33\n6.94\n10.67\n\n\n2\n37.50\n28.85\n26.39\n30.33\n\n\n3\n31.94\n35.90\n30.56\n33.67\n\n\n4\n11.11\n21.79\n27.78\n20.67\n\n\n5\n0.00\n5.13\n8.33\n4.67\n\n\nTotal\n100.00\n100.00\n100.00\n100.00\n\n\nN\n300"
  },
  {
    "objectID": "rm-data/slides/week03.html#other-options",
    "href": "rm-data/slides/week03.html#other-options",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Other options",
    "text": "Other options\n\nSince \\(x\\) is qualitative, and there are “enough” observations in each category, its also posible to plot the conditional distribution of \\(y\\) for each value of \\(x\\).\n\ntwo (histogram management ), by(firm_size) \ntwo (kdensity management ), by(firm_size) \ngraph box management, over(firm_size) intensity(30)"
  },
  {
    "objectID": "rm-data/slides/week03.html#conditional-and-joint-distributions",
    "href": "rm-data/slides/week03.html#conditional-and-joint-distributions",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Conditional and joint distributions",
    "text": "Conditional and joint distributions\n\nThe previous Design assume \\(x\\) to be discrete (Made Discrete). But what if not? Too many values!.\n\nNeed to think about joint distributions\n\nThe joint distribution of two variables shows the probabilities (frequencies) of each value combination of the two variables.\nA scatter plot is a two-dimensional graph with the values of each of the two variables measured on its two axes.\nWorks better when dataset is relatively small.\nFor larger samples, we can bin values, and use “bin scatter”\nBin scatter shows conditional means for bins we created"
  },
  {
    "objectID": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size-3",
    "href": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size-3",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Case Study - Management quality and firm size",
    "text": "Case Study - Management quality and firm size\n\n\n\n\nCode\nscatter management emp_firm, xtitle(\"Firm size\") ytitle(\"Management score\") ///\n  legend(off) scale(1.5) \n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsort emp_firm\nqui:drop2 emp_firm_bin emp_mean_bin\nxtile emp_firm_bin = _n, n(20)\nbysort emp_firm_bin: egen emp_mean_bin=mean(emp_firm)\nbysort emp_firm_bin:egen mean_mng=mean(management)\n\nscatter mean_mng emp_mean_bin, xtitle(\"Firm size\") ytitle(\"Management score\") ///\n  scale(1.5) legend(off) ylabel(1/5) ///\n  note(\"Using 20 bins\")\n\n\nvariable emp_firm_bin not found\nvariable emp_mean_bin not found\n\n\n\n\n\n\n\n\n\n\n\nSome association shown. Scatter not easy to read, bin-scatter shows positive (weak) association. Notice Scale of y-axis. Flat line for large firms"
  },
  {
    "objectID": "rm-data/slides/week03.html#other-options-1",
    "href": "rm-data/slides/week03.html#other-options-1",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Other Options",
    "text": "Other Options\n\nModel Based ScatterplotScale Change\n\n\n\n\nCode\ntwo (scatter management emp_firm) ///\n    (lfitci management emp_firm, fcolor(%30)), ///\n    xtitle(\"Firm size\") ytitle(\"Management score\") ///\n    legend(off) scale(1.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nscatter management emp_firm, ///\n  xtitle(\"Firm size\") ytitle(\"Management score\") ///\n  xscale(log) scale(1.5) xlabel(100 250 500 1000 2000 3000 4000 5000)"
  },
  {
    "objectID": "rm-data/slides/week03.html#dependence-and-independence",
    "href": "rm-data/slides/week03.html#dependence-and-independence",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Dependence and independence",
    "text": "Dependence and independence\n\nDependence of two variables \\(y\\) and \\(x\\) means that the conditional distributions of \\(y|x\\) changes with \\(x\\)\n\nThis is what we showed earlier\n\nIndependence means the opposite: the distribution of \\(y|x\\) is the same, regardless of the value of \\(x\\).\nDependence, may take many forms.\n\n\\(y\\) may be more or less spread out for different \\(x\\) values.\nthe mean of \\(y\\) is different for different \\(x\\) values.\n\n\n\\[E[y|X=x_1] \\neq E[y|X=x_2]\\]"
  },
  {
    "objectID": "rm-data/slides/week03.html#mean-dependence",
    "href": "rm-data/slides/week03.html#mean-dependence",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Mean dependence",
    "text": "Mean dependence\n\nMean-dependence: conditional expectation \\(E[y|x]\\) varies with the value of \\(x\\).\nTwo variables are positively mean-dependent if the average of two variables increase together.\nCovariance and Correlation Coefficient are measures of mean linear dependence.\n\nThey measure the same thing, but the correlation coefficient is a standardized version of the covariance."
  },
  {
    "objectID": "rm-data/slides/week03.html#covariance",
    "href": "rm-data/slides/week03.html#covariance",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Covariance",
    "text": "Covariance\nThe formula for the covariance between two variables \\(x\\) and \\(y\\) with n observations is:\n\\[Cov[x, y] = \\frac{1}{n}\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})\n\\]\n\nThe Covariance is the average of the product of the deviations of the two variables from their respective means.\nPositive covariance: positive deviations of \\(x\\) go with positive deviations of \\(y\\).\nNegative covariance: positive deviations of \\(x\\) go with negative deviations of \\(y\\)."
  },
  {
    "objectID": "rm-data/slides/week03.html#the-correlation-coefficient",
    "href": "rm-data/slides/week03.html#the-correlation-coefficient",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The correlation coefficient",
    "text": "The correlation coefficient\n\\[Corr[x, y] = \\rho_{xy}= \\frac{Cov[x, y]}{Std[x]Std[y]}\\]\n\\[-1 \\leq Corr[x, y] \\leq 1\\]\n\nThe correlation coefficient is the standardized version of the covariance.\nIt is bound to be between negative one and positive one."
  },
  {
    "objectID": "rm-data/slides/week03.html#dependence-mean-dependence-correlation",
    "href": "rm-data/slides/week03.html#dependence-mean-dependence-correlation",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Dependence, mean-dependence, correlation",
    "text": "Dependence, mean-dependence, correlation\n\n\n\n\n\n\nNote\n\n\nIf two variables are independent, they are also mean-independent, Thus \\(E[y|x] = E[y]\\) of any value of x.\n\n\n\n\n\nIs this true the other way around?\n\nNo, it is not.\n\nSpecial cases:\n\n\\(\\rho = 0\\) but mean dependence (Sqrt of x)\n\\(\\rho = 0\\) and mean independence but different spread of \\(y\\) (Heteroskedasticity)"
  },
  {
    "objectID": "rm-data/slides/week03.html#case-study---management-quality-firm-size-industry",
    "href": "rm-data/slides/week03.html#case-study---management-quality-firm-size-industry",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Case Study - Management quality, firm size, Industry",
    "text": "Case Study - Management quality, firm size, Industry\n\nMeasures of management quality and their correlation with size by industry\n\n\nIndustry\nCorrelation\nObservations\n\n\n\n\nAuto\n0.50\n26\n\n\nChemicals\n0.05\n69\n\n\nElectronics\n0.33\n24\n\n\nFood, drinks, tobacco\n0.05\n34\n\n\nMaterials, metals\n0.32\n50\n\n\nTextile, apparel\n0.29\n43\n\n\nWood, furniture, paper\n0.28\n29\n\n\nOther\n0.44\n25\n\n\nAll\n0.30\n300"
  },
  {
    "objectID": "rm-data/slides/week03.html#measuring-a-latent-concept",
    "href": "rm-data/slides/week03.html#measuring-a-latent-concept",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Measuring a latent concept",
    "text": "Measuring a latent concept\n\nOften a concept is hard, even impossible, to measure…directly\nWe often call them Latent variables: A variable that is not observed nor can be measured.\nExamples:\n\nQuality of management at a firm - it is a concept that may be measured with a collection of variables, not a single one of them\nIQ - measured by a series of quiz-like questions.\nEmployment satisfaction - measured by a series of questions about the job\n\nHow do you combine multiple observed variables"
  },
  {
    "objectID": "rm-data/slides/week03.html#condensing-information",
    "href": "rm-data/slides/week03.html#condensing-information",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Condensing information",
    "text": "Condensing information\n\nAlternatives:\n\nUse one observed variable only: perhaps the one that is the best measure\nUse all variables individually\nSummarize them into a single variable\n\nUse a weighted average of all variables\nPrincipal component analysis (PCA)\nLatent variable analysis, ETC"
  },
  {
    "objectID": "rm-data/slides/week03.html#using-a-single-variable-or-a-few",
    "href": "rm-data/slides/week03.html#using-a-single-variable-or-a-few",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Using a single variable (or a few)",
    "text": "Using a single variable (or a few)\n\nUsing one measured variable and exclude the rest has the advantage of easy interpretation.\n\nThe others could be used for robustness checks\n\nIt has the disadvantage of discarding potentially useful information.\nBut, can be often a sensible start"
  },
  {
    "objectID": "rm-data/slides/week03.html#using-an-weighted-average",
    "href": "rm-data/slides/week03.html#using-an-weighted-average",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Using an [Weighted] Average",
    "text": "Using an [Weighted] Average\n\nTaking the average of all measured variables makes use of all information.\n\n\\[\\bar{z_i} = \\frac{1}{k}\\sum_{j=1}^k z_i^j \\text{ or }\n\\bar{z_i} = \\frac{\\sum_{j=1}^k w_j \\times z_i^j}{\\sum_{j=1}^k w_j}\n\\]\n\nAll should be measured in the same Scale. Simple and a natural interpretation\nYou can also use weights to give more importance to some variables than others.\nOr can use sub-groups indices to create a composite index."
  },
  {
    "objectID": "rm-data/slides/week03.html#using-an-weighted-average-1",
    "href": "rm-data/slides/week03.html#using-an-weighted-average-1",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Using an [Weighted] Average",
    "text": "Using an [Weighted] Average\n\nIMPORTANT: All variables should be measured in the same scale.\n\nOtherwise, the average would be meaningless.\n\nThus, need bring it to common scale.\n\nstandardization: Z-score \\[\\tilde z_i^j = \\frac{z_i^j - \\bar{z}}{s_{z}}\\]\n0-1 scale: Min-Max scaling \\[\\tilde z_i^j = = \\frac{z_i^j - \\min(z^j)}{\\max(z^j) - \\min(z^j)}\\]\n\n\n\nYou may also want to consider using same units (dollars) or use transformations (logs)"
  },
  {
    "objectID": "rm-data/slides/week03.html#let-the-decide",
    "href": "rm-data/slides/week03.html#let-the-decide",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Let the 🖥️ decide",
    "text": "Let the 🖥️ decide\n\nSome times you may need to use other methods to combine variables. Machine learning methods!\n\n\nPrincipal component analysis (PCA) is a method used for Data Reduction. Get weights to combine variables.\nThe weights are constructed based on how correlated variables are. (high correlation, high weight)\nA Bit of black box method. But commonly used in practice: Wealth index, etc.\n\npca var1 var2 var3 .. , components(#)\npredict pca\n\nCan give odd results"
  },
  {
    "objectID": "rm-data/slides/week03.html#what-to-use",
    "href": "rm-data/slides/week03.html#what-to-use",
    "title": "Exploratory Analysis and Comparisons",
    "section": "What to use?",
    "text": "What to use?\n\nZ-scores, and averages, are simple, easy to understand, - Transparent\nTypically marginally different to PCA (Try both)\nBut, Need to pay attention\n\nLook at correlation signs, you may check it first (PCA is better here) (EDA. Do signs make sense?)\nSensitive to extreme values (But can be fixed)"
  },
  {
    "objectID": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size-4",
    "href": "rm-data/slides/week03.html#case-study---management-quality-and-firm-size-4",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Case Study - Management quality and firm size",
    "text": "Case Study - Management quality and firm size\n\nThe latent concept here is the overall quality of management, but we have 18 variables that measure different aspects of management.\nEach were measured on a scale of 1 (worst practice) to 5 (best practice).\nLets use Simple Average and PCA to create a composite index."
  },
  {
    "objectID": "rm-data/slides/week03.html#stata-corner",
    "href": "rm-data/slides/week03.html#stata-corner",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Stata Corner",
    "text": "Stata Corner\n\nuse \"data_slides\\wb-mx-management.dta\", clear\n** Simple mean\negen mng_mean = rowmean(perf* talent* lean*)\n** PCA\npca perf* talent* lean*, components(1)\npredict mng_pca\nlabel var mng_mean \"Management Score (Mean)\"\nlabel var mng_pca \"Management Score (PCA)\""
  },
  {
    "objectID": "rm-data/slides/week03.html#scatter-pca-vs-mean",
    "href": "rm-data/slides/week03.html#scatter-pca-vs-mean",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Scatter PCA vs Mean",
    "text": "Scatter PCA vs Mean\n\nA Correlation analysis could also be useful to compare the two measures."
  },
  {
    "objectID": "rm-data/slides/week03.html#comparison-and-variation-in-x",
    "href": "rm-data/slides/week03.html#comparison-and-variation-in-x",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Comparison and variation in \\(x\\)",
    "text": "Comparison and variation in \\(x\\)\n\nVariation in the conditioning variable is necessary to make comparisons.\nExample: to uncover the effect of price changes on sales you need many observations with different price values.\nGeneralization: The more variation is there in the conditioning variable the better are the chances for comparison.\n\nBecause the more likely you can capture “reality”"
  },
  {
    "objectID": "rm-data/slides/week03.html#source-of-variation",
    "href": "rm-data/slides/week03.html#source-of-variation",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Source of variation",
    "text": "Source of variation\n\nNot all variation is the same. you must ask:\n\nWhy is there variation in the conditioning variable?\n\nDepending on the source of variation, the interpretation of the comparison may be different.\n\nGood variation: You can make causal statements\nBad variation: At best you can make correlation statements"
  },
  {
    "objectID": "rm-data/slides/week03.html#the-good-experimental-data",
    "href": "rm-data/slides/week03.html#the-good-experimental-data",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The Good: Experimental data",
    "text": "The Good: Experimental data\n\nSay you have an intervention or treatment.\n\nSome people get the treatment, others do not.\n\nIn experimental data, there is controlled variation: a rule deciding treatment\nThis is the best source of variation for causal analysis.\n\nDifferences in the outcome variable will be due to the treatment variable only.\n\nExample: drug trial\n\nMedical experiment: some patients receive the drug while others receive a placebo (treatment/control)\nOutcome is recovery from the illness or not"
  },
  {
    "objectID": "rm-data/slides/week03.html#the-bad-observational-data",
    "href": "rm-data/slides/week03.html#the-bad-observational-data",
    "title": "Exploratory Analysis and Comparisons",
    "section": "The Bad? Observational data",
    "text": "The Bad? Observational data\n\nMost data used in business, economics and policy analysis are observational.\nIn observational data, no variable is fully controlled.\nThey are the results of decisions, choices, interactions, expectations, etc. (sources of variation)\n\nSome of this could be random (good) but not all\n\nYou can still make comparisons, but you must be careful.\n\nAny difference in the outcome variable could be for other reasons.\nDoes smoking cause cancer? Or are people who smoke more likely to have other habits that cause cancer?"
  },
  {
    "objectID": "rm-data/slides/week03.html#source-of-variation-and-causal-analysis",
    "href": "rm-data/slides/week03.html#source-of-variation-and-causal-analysis",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Source of variation and causal analysis",
    "text": "Source of variation and causal analysis\n\nExperimental data: - Easy - if conditioning variable is experimentally controlled -\n\nMade sure that differences in the outcome variable are due to that variable only\n\nObservational data: - Hard - many other things may be different when the value of the conditioning variable differs\n\nYou must be careful in making causal statements\n\n\nHowever, There are -advanced- methods that can help identify causal relationships in observational data. (Advanced Econometrics)"
  },
  {
    "objectID": "rm-data/slides/week03.html#ai-and-patterns",
    "href": "rm-data/slides/week03.html#ai-and-patterns",
    "title": "Exploratory Analysis and Comparisons",
    "section": "AI and patterns",
    "text": "AI and patterns\n\nGenAI is great to give you a first review of patterns – similar to a few lines of code, or panda profiler in Python\njudgment of correlation (weak, strong) is often wrong.\nyou need to know what pattern to pursue\nCan ask to explain different metrics of dependence"
  },
  {
    "objectID": "rm-data/slides/week03.html#summary-1",
    "href": "rm-data/slides/week03.html#summary-1",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Summary",
    "text": "Summary\n\nBe explicit about what \\(y\\) and \\(x\\) are in your data and how they are related to the question of your analysis.\nFor qualitative variables, correlation can be shown by summarizing conditional probabilities (frequencies).\nFor quantitative variables, scatterplots offer a visual insight to the pattern of the relationship.\nThe correlation coefficient captures a simple measure of mean dependence."
  },
  {
    "objectID": "rm-data/slides/week03.html#functional-form-ln-transformation",
    "href": "rm-data/slides/week03.html#functional-form-ln-transformation",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Functional form: ln transformation",
    "text": "Functional form: ln transformation\n\nOften, quasi-nonlinear patterns can be approximated with \\(y\\) or \\(x\\) transformed by taking logs.\nWhen transformed by taking the natural logarithm, differences in variable values we approximate relative/percentage differences.\n\n\\[ln(x + \\Delta x) - ln(x) \\approx \\frac{\\Delta x}{x}\\]"
  },
  {
    "objectID": "rm-data/slides/week03.html#logarithmic-transformation---interpretation",
    "href": "rm-data/slides/week03.html#logarithmic-transformation---interpretation",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Logarithmic transformation - interpretation",
    "text": "Logarithmic transformation - interpretation\n\n\\(ln(x)\\): the natural logarithm of x\n\nOften refered as \\(log(x)\\) or \\(ln(x)\\) but they are often the same\n\nThis transformation “compresses” the distribution of x\nbut:\n\n\\(x\\) needs to be a positive number\n\\(ln(0)\\) or \\(ln(-|x|)\\) are not defined in \\(\\mathbb{R}\\).\n\nAdvantage: Log transformation allows for comparison in relative terms – percentages\n\n\\[\\begin{aligned}\nln(a) - ln(b) &\\approx \\frac{a-b}{0.5(a+b)} \\\\\nln(1.01)-ln(1) &= 0.0099 \\approx 0.01 \\\\\nln(1.1)-ln(1) &= 0.095 \\approx 0.1\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rm-data/slides/week03.html#logarithmic-functions-of-y-andor-x",
    "href": "rm-data/slides/week03.html#logarithmic-functions-of-y-andor-x",
    "title": "Exploratory Analysis and Comparisons",
    "section": "Logarithmic Functions of y and/or x",
    "text": "Logarithmic Functions of y and/or x\n\nThis transformation works well if \\(\\Delta x\\) is small (\\(&lt;0.3\\))\nFor larger differences, relative differences need to be calculated by hand\nA difference of 0.1 log units corresponds to a 10% difference\nFor larger differences,\n\nif log difference is +1.0, it corresponds to a +170% difference\nif log difference is -1.0, it corresponds to a -63% difference\n\nThis transformation will be used often in economics."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression",
    "href": "rm-data/slides/week05-06.html#regression",
    "title": "Regression Analysis I",
    "section": "Regression",
    "text": "Regression\n\nRegression is the most widely used method of comparison in data analysis.\nDoing so uncovers the pattern of association between \\(y\\) and \\(x\\). Here, it is important what you use as \\(y\\) or \\(x\\).\nSimple regression uncovers mean-dependence between two variables.\n\nIt amounts to comparing average values of one variable, called the dependent variable (\\(y\\)), for observations that are different in the other variable, the explanatory variable (\\(x\\)).\n\nMultiple regression analysis involves more variables -&gt; for later."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression---uses",
    "href": "rm-data/slides/week05-06.html#regression---uses",
    "title": "Regression Analysis I",
    "section": "Regression - uses",
    "text": "Regression - uses\n\nDiscovering patterns of association between variables is often a good starting point even if our question is more ambitious.\nCausal analysis: uncovering the effect of one variable on another variable. Concerned with one parameter.\nPredictive analysis: what to expect of a \\(y\\) variable (long-run polls, hotel prices) for various values of another \\(x\\) variable (immediate polls, distance to the city center)."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression---names-and-notation",
    "href": "rm-data/slides/week05-06.html#regression---names-and-notation",
    "title": "Regression Analysis I",
    "section": "Regression - names and notation",
    "text": "Regression - names and notation\n\nRegression analysis is a method that uncovers the average value of a variable \\(y\\) for different values of another variable \\(x\\).\n\n\\[\\mathbb{E}[y|x]=y^E = f(x)\\]\n\ndependent variable or left-hand-side variable, or simply the \\(y\\) variable,\nexplanatory variable, right-hand-side variable, or simply the \\(x\\) variable\n“regress y on x,” or “run a regression of y on x” = do simple regression analysis with \\(y\\) as the dependent variable and \\(x\\) as the explanatory variable."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression---type-of-patterns",
    "href": "rm-data/slides/week05-06.html#regression---type-of-patterns",
    "title": "Regression Analysis I",
    "section": "Regression - type of patterns",
    "text": "Regression - type of patterns\nRegression may find:\n\nLinear patterns: positive (negative) association - average \\(y\\) tends to be higher (lower) at higher values of \\(x\\).\nNon-linear patterns: association may be even non-monotonic - \\(y\\) tends to be higher for higher values of \\(x\\) in a certain range of the \\(x\\) variable and lower for higher values of \\(x\\) in another range of the \\(x\\) variable\nNo association or relationship (A flat line)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#non-parametric-and-parametric-regression",
    "href": "rm-data/slides/week05-06.html#non-parametric-and-parametric-regression",
    "title": "Regression Analysis I",
    "section": "Non-parametric and parametric regression",
    "text": "Non-parametric and parametric regression\n\nNon-parametric regressions describe the \\(\\mathbb{E}[y] = f(x)\\) pattern without imposing a specific functional form on \\(f\\).\nData driven and flexible, no theory\nCan capture any pattern\nParametric regressions impose a functional form on \\(f\\). Parametric examples include:\n\nlinear functions: \\(f(x) = a + bx\\);\nexponential functions: \\(f(x) = a x^b\\);\nquadratic functions: \\(f(x) = a + bx + cx^2\\),\nor any functions which have parameters of a, b, c, etc.\n\nRestrictive, but they produce readily interpretable numbers."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#non-parametric-regression-average-by-each-value",
    "href": "rm-data/slides/week05-06.html#non-parametric-regression-average-by-each-value",
    "title": "Regression Analysis I",
    "section": "Non-parametric regression: average by each value",
    "text": "Non-parametric regression: average by each value\n\nNon-parametric regressions come (also) in various forms.\nMost intuitive non-parametric regression for \\(\\mathbb{E}[y|x] = f(x)\\) shows average \\(y\\) for each and every value of \\(x\\).\nWorks well when \\(x\\) has few values and there are many observations in the data,\nThere is no functional form imposed on \\(f\\) here."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#non-parametric-regression-categorical-variable",
    "href": "rm-data/slides/week05-06.html#non-parametric-regression-categorical-variable",
    "title": "Regression Analysis I",
    "section": "Non-parametric regression: Categorical variable",
    "text": "Non-parametric regression: Categorical variable\n\nSometimes, there are no straightforward functional form on \\(f\\) (linear not meaningful).\n\nCategorical variables\nOrdered variables.\n\nFor example, Hotels: average price of hotels with the same numbers of stars and compare these averages = non-parametric regression analysis."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#non-parametric-regression-bins",
    "href": "rm-data/slides/week05-06.html#non-parametric-regression-bins",
    "title": "Regression Analysis I",
    "section": "Non-parametric regression: bins",
    "text": "Non-parametric regression: bins\n\nWith many \\(x\\) values therea are two ways to do non-parametric regression analysis: bins and smoothing.\nBins - based on grouped values of \\(x\\) (Discretization of \\(x\\))\n\nBins are disjoint categories (no overlap) that span the entire range of \\(x\\) (no gaps).\n\nMany ways to create bins - equal size, equal number of observations per bin, or bins defined by analyst.\n\nsee binscatter or make your own"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#non-parametric-regression-lpoly",
    "href": "rm-data/slides/week05-06.html#non-parametric-regression-lpoly",
    "title": "Regression Analysis I",
    "section": "Non-parametric regression: lpoly",
    "text": "Non-parametric regression: lpoly\n\nProduce “smooth” graph - both continuous and has no kink at any point.\nalso called smoothed conditional means plots = non-parametric regression shows conditional means, smoothed to get a better image.\nLowess = most widely used non-parametric regression methods that produce a smooth graph.\nlocally weighted scatterplot smoothing (sometimes abbreviated as “loess”).\nA smooth curve fit around a bin scatter.\nwider bandwidth results in a smoother graph but may miss important details of the pattern.\nnarrower bandwidth produces a more rugged-looking graph\nIn Stata one of the commands for this its lpoly but also lowess."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#non-parametric-regression-lowess-loess",
    "href": "rm-data/slides/week05-06.html#non-parametric-regression-lowess-loess",
    "title": "Regression Analysis I",
    "section": "Non-parametric regression: lowess (loess)",
    "text": "Non-parametric regression: lowess (loess)\n\nSmooth non-parametric regression methods, including lowess, do not produce numbers that would summarize the \\(\\mathbb{E}[y]|x = f(x)\\) pattern.\nProvide a value \\(\\mathbb{E}[y]\\) for each of the particular \\(x\\) values that occur in the data, as well as for all \\(x\\) values in-between.\nGraph – we interpret these graphs in qualitative, not quantitative ways.\nThey can show interesting shapes in the pattern, such as non-monotonic parts, steeper and flatter parts, etc.\nGreat way to find relationship patterns"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels",
    "href": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels",
    "title": "Regression Analysis I",
    "section": "Case Study: Finding a good deal among hotels",
    "text": "Case Study: Finding a good deal among hotels\n\n\nCode\nset scheme white2\ncolor_style tableau\nuse data_slides/hotels-vienna.dta, clear\nqui:drop if distance&gt;6\ntwo (lpolyci price distance, bw(.3) fcolor(%20)) ///\n(lpolyci price distance, bw(.6) fcolor(%20)) ///\n(lpolyci price distance, bw(.15) fcolor(%20)), ///\nlegend(order(2 \"bw(.3)\" 4 \"bw(.6)\" 6 \"bw(.15)\")) ///\nytitle(\"Price\") xtitle(\"Distance from CityCenter\")"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#linear-regression-1",
    "href": "rm-data/slides/week05-06.html#linear-regression-1",
    "title": "Regression Analysis I",
    "section": "Linear regression",
    "text": "Linear regression\nLinear regression is the most widely used method in data analysis.\n\nImposes linearity assumption of the function \\(f\\) in \\(\\mathbb{E}[y|x] = f(x)\\). (Linearity of Coefficients)\nLinear functions have two parameters, also called coefficients: the intercept and the slope. \\(\\mathbb{E}[y|x] = \\alpha + \\beta x\\)\nCan be any function, including any nonlinear function, of the original variables themselves\nThis line is the best-fitting line one can draw through the scatterplot.\nIt is the best fit in the sense that it is the line that is closest to all points of the scatterplot."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#linear-regression---assumption-vs-approximation",
    "href": "rm-data/slides/week05-06.html#linear-regression---assumption-vs-approximation",
    "title": "Regression Analysis I",
    "section": "Linear regression - assumption vs approximation",
    "text": "Linear regression - assumption vs approximation\n\nAssumption: The regression function is linear in its coefficients.\nApproximation: Whatever the form of the \\(\\mathbb{E}[y|x] = f(x)\\) the \\(\\mathbb{E}[y|x] = \\alpha + \\beta x\\) regression fits a line through it.\n\nThis may or may not be a good approximation."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#linear-regression-coefficients",
    "href": "rm-data/slides/week05-06.html#linear-regression-coefficients",
    "title": "Regression Analysis I",
    "section": "Linear regression coefficients",
    "text": "Linear regression coefficients\n\\[\\mathbb{E}[y|x] = \\alpha + \\beta x\\]\nTwo coefficients:\n\nintercept: \\(\\alpha =\\) average value of \\(y\\) when \\(x\\) is zero:\n\n\\(\\mathbb{E}[y|x=0] = \\alpha + \\beta \\times 0 = \\alpha\\).\n\nslope: \\(\\beta =\\) expected difference in \\(y\\) corresponding to a one unit difference in x.\n\n\\(\\mathbb{E}[y|x=x_0+1] - \\mathbb{E}[y|x_0] = (\\alpha + \\beta \\times (x_0 + 1)) - (\\alpha + \\beta \\times x_0) = \\beta\\)."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression---slope-coefficient-interpretation",
    "href": "rm-data/slides/week05-06.html#regression---slope-coefficient-interpretation",
    "title": "Regression Analysis I",
    "section": "Regression - slope coefficient interpretation",
    "text": "Regression - slope coefficient interpretation\nSeveral good ways to interpret the slope coefficient\n\n\\(y\\) is \\(\\beta\\) higher, on average, for observations with a one-unit higher value of \\(x\\).\nComparing two observations that differ in \\(x\\) by one unit, we expect \\(y\\) to be \\(\\beta\\) higher for the observation with one unit higher \\(x\\).\n\nAvoid using\n\n“decrease/increase” – not right, unless time series or causal relationship only\n“effect” – not right, unless causal relationship"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression-binary-x",
    "href": "rm-data/slides/week05-06.html#regression-binary-x",
    "title": "Regression Analysis I",
    "section": "Regression: binary \\(x\\)",
    "text": "Regression: binary \\(x\\)\nSimplest case:\n\n\\(x\\) is a binary variable, zero or one.\n\\(\\alpha\\) is the average value of \\(y\\) when \\(x\\) is zero (\\(\\mathbb{E}[y|x=0] = \\alpha\\)).\n\\(\\beta\\) is the difference in average \\(y\\) between observations with \\(x=1\\) and observations with \\(x=0\\)\n\n\\(\\mathbb{E}[y|x=1] - \\mathbb{E}[y|x=0]= \\beta\\).\n\nGraphically, the regression line of linear regression goes through two points: average \\(y\\) when \\(x\\) is zero (\\(\\alpha\\)) and average \\(y\\) when \\(x\\) is one (\\(\\alpha + \\beta\\))."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression-coefficient-formula",
    "href": "rm-data/slides/week05-06.html#regression-coefficient-formula",
    "title": "Regression Analysis I",
    "section": "Regression coefficient formula",
    "text": "Regression coefficient formula\nNotation\n\nPopulation coefficients are \\(\\alpha\\) and \\(\\beta\\).\nSample estimates - \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)\nThe slope coefficient formula is \\[\\hat{\\beta} = \\frac{\\text{Cov}[x, y]}{\\text{Var}[x]} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n\\]\nSlope coefficient formula is normalized version of the covariance between \\(x\\) and \\(y\\)."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression-coefficient-formula-1",
    "href": "rm-data/slides/week05-06.html#regression-coefficient-formula-1",
    "title": "Regression Analysis I",
    "section": "Regression coefficient formula",
    "text": "Regression coefficient formula\n\nThe intercept – average \\(y\\) minus average \\(x\\) multiplied by the estimated slope \\(\\hat{\\beta}\\). \\[\\hat{\\alpha} = \\bar{y} - \\hat{\\beta} \\bar{x}\n\\]\nThe formula of the intercept reveals that the regression line always goes through the point of average \\(x\\) and average y."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#ordinary-least-squares-ols",
    "href": "rm-data/slides/week05-06.html#ordinary-least-squares-ols",
    "title": "Regression Analysis I",
    "section": "Ordinary Least Squares (OLS)",
    "text": "Ordinary Least Squares (OLS)\n\n\n\nThe formulas Provided for the slope and intercept can be found using OLS (an estimation method).\nOLS gives the best-fitting linear regression line.\nIt gives the best line by minimizing the squared of the model errors.\n\n\n\n\nCode\nclear \nqui:set obs 20\nqui:gen x = rnormal()+1\nqui:gen y = 1+x+rnormal()\nqui:gen yh=1+x\ntwo (scatter y x, msize(3) mcolor(gs3%50)) ///\n   (line yh x, color(navy)) (pcarrow yh x y x, color(gs9)), ///\nlegend(off)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression-coefficient-formula-2",
    "href": "rm-data/slides/week05-06.html#regression-coefficient-formula-2",
    "title": "Regression Analysis I",
    "section": "Regression coefficient formula",
    "text": "Regression coefficient formula\n\nOrdinary Least Squares – OLS is method used to find the best fit minimizing the Squared “residuals”.\n\\[\\min_{\\alpha,\\beta} \\sum_{i=1}^n (y_i - \\alpha - \\beta x_i)^2\\]\n\nFor this minimization problem, we can use calculus to give \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\), the values for \\(\\alpha\\) and \\(\\beta\\) that give the minimum. Please check out U7.1."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#recap",
    "href": "rm-data/slides/week05-06.html#recap",
    "title": "Regression Analysis I",
    "section": "Recap",
    "text": "Recap\n\nSimple regression analysis amounts to comparing average values of a dependent variable (\\(y\\)) for observations that are different in the explanatory variable (\\(x\\)).\nSimple regression in any way or form: comparing conditional means."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels-1",
    "href": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels-1",
    "title": "Regression Analysis I",
    "section": "Case Study: Finding a good deal among hotels",
    "text": "Case Study: Finding a good deal among hotels\n\n\nCode\nuse data_slides/hotels-vienna.dta, clear\nqui:drop if distance&gt;6\nqui: keep if inrange(stars,3,4)\nqui: drop if price&gt;300\ntwo (lpolyci price distance, bw(.3) fcolor(%20)) ///\n(lfitci price distance, fcolor(%20)) ///\n(scatter price distance, color(%20)), ///\nlegend(order(2 \"bw(.3)\" 4 \"Linear\" )) ///\nytitle(\"Price\") xtitle(\"Distance from CityCenter\") ///\nscale(1.4) note(Alpha = 131.9 Beta = -12)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#predicted-values",
    "href": "rm-data/slides/week05-06.html#predicted-values",
    "title": "Regression Analysis I",
    "section": "Predicted values",
    "text": "Predicted values\n\nThe predicted value of the dependent variable is the best guess for its average value, given \\(x\\), using our model.\nIn a linear regression they are given by: \\(\\hat{y} = \\hat{\\alpha} + \\hat{\\beta}x\\)\nWhat about non-parametric regressions\n\nIt depends on how the Model was estimated."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#residuals",
    "href": "rm-data/slides/week05-06.html#residuals",
    "title": "Regression Analysis I",
    "section": "Residuals",
    "text": "Residuals\n\nThe residual is the difference between the actual value and predicted value of an observation: \\(e_i = y_i - \\hat{y}_i\\), where \\(\\hat{y}_i = \\hat{\\alpha} + \\hat{\\beta}x_i\\)\nThe residual is meaningful only for actual observation, cannot be “predicted” out of sample.\nBut, The residual may be important on its own right.\nMay help in identifying Outliers: Cases where \\(y\\) is much higher or much lower than “it should be” (based on the regression). (Good deals or places to avoid)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels-2",
    "href": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels-2",
    "title": "Regression Analysis I",
    "section": "Case Study: Finding a good deal among hotels",
    "text": "Case Study: Finding a good deal among hotels\n\n\nCode\nqui:drop2 pr_hat res\nqui:reg price distance\nqui:predict pr_hat\nqui:predict res, res\nqui:sort res\nlist hotel_id price distance pr_hat res star in 1/5\n\n\nvariable pr_hat not found\nvariable res not found\n\n     +------------------------------------------------------------+\n     | hotel_id   price   distance     pr_hat         res   stars |\n     |------------------------------------------------------------|\n  1. |    22080      54        1.1   118.6571   -64.65714       3 |\n  2. |    22122      59         .8   122.2709    -63.2709       3 |\n  3. |    21912      60        1.1   118.6571   -58.65714       4 |\n  4. |    22073      59        1.2   117.4525   -58.45255       3 |\n  5. |    22127      58        1.4   115.0434   -57.04337     3.5 |\n     +------------------------------------------------------------+\n\n\nNot the best model (functional form , other characteristics), but a good start!"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression-modelling-r2",
    "href": "rm-data/slides/week05-06.html#regression-modelling-r2",
    "title": "Regression Analysis I",
    "section": "Regression modelling: \\(R^2\\)",
    "text": "Regression modelling: \\(R^2\\)\n\nFitness of a regression captures how predicted values compare to the actual values.\nR-squared (R^2) represents how much of the variation in \\(y\\) is captured by the regression, and how much is left for residual variation \\[R^2 = \\frac{\\text{Var}[\\hat{y}]}{\\text{Var}[y]} = 1 - \\frac{\\text{Var}[e]}{\\text{Var}[y]}\n\\]\nThis follows: \\[\\text{Var}[y] = \\text{Var}[\\hat{y}] + \\text{Var}[e]\\]"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#model-fit---r2",
    "href": "rm-data/slides/week05-06.html#model-fit---r2",
    "title": "Regression Analysis I",
    "section": "Model fit - R^2",
    "text": "Model fit - R^2\n\nR-squared (or R^2) can also be identified for non-parametric regressions. \\[R^2_1 = \\frac{\\text{Var}[\\hat{y}]}{\\text{Var}[y]} \\text{ or } R^2_2= 1 - \\frac{\\text{Var}[e]}{\\text{Var}[y]}\n\\]\n\nThey may not be the same!\n\nYou could also estimate it using the “Squared correlation” between \\(y\\) and \\(\\hat y\\).\nThe value of R-squared is always between zero and one.\nR-squared is zero, if the predicted values are just the average of the observed outcome \\(\\hat{y}_i = \\bar{y}_i\\), \\(\\forall i\\)."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#model-fit---how-to-use-r2",
    "href": "rm-data/slides/week05-06.html#model-fit---how-to-use-r2",
    "title": "Regression Analysis I",
    "section": "Model fit - how to use \\(R^2\\)",
    "text": "Model fit - how to use \\(R^2\\)\n\nR-squared may help in choosing between different versions of regression for the same data.\n\nChoose between regressions with different functional forms\nPredictions are likely to be better with high \\(R^2\\)\nMore on this later\n\nR-squared matters less when the goal is to characterize the association between \\(y\\) and \\(x\\)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression-and-causation",
    "href": "rm-data/slides/week05-06.html#regression-and-causation",
    "title": "Regression Analysis I",
    "section": "Regression and causation",
    "text": "Regression and causation\n\nUp to this point, try to be very careful to use neutral language, not talk about causation\nThink back to sources of variation in \\(x\\)\nWhen we have observational data, and we pick \\(x\\) and \\(y\\) and decide how to run the regression\nRegression is a method of comparison: it compares observations that are different in variable \\(x\\) and shows corresponding average differences in variable \\(y\\). Not necessarily Causal Relations."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression-and-causation---possible-relations",
    "href": "rm-data/slides/week05-06.html#regression-and-causation---possible-relations",
    "title": "Regression Analysis I",
    "section": "Regression and causation - possible relations",
    "text": "Regression and causation - possible relations\n\nSlope of the \\(\\mathbb{E}[y|x] = \\alpha + \\beta x\\) regression is not zero in our data\nSeveral reasons, not mutually exclusive:\n\n\\(x\\) causes \\(y\\): Yay!\n\\(y\\) causes \\(x\\). Noo!\nA third variable causes both \\(x\\) and \\(y\\) (or many such variables do) Double NoO!\n\nIn reality if we have observational data, there is a mix of these relations."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#regression-and-causation-1",
    "href": "rm-data/slides/week05-06.html#regression-and-causation-1",
    "title": "Regression Analysis I",
    "section": "Regression and causation",
    "text": "Regression and causation\n\nYes: “correlation (regression) does not imply causation”\n\nBetter: we should not infer cause and effect from comparisons in observational data.\n\nSuggested approach is two steps\n\nFirst interpret precisely the object (correlation of slope coefficient)\nConclude and discuss causal claims if any"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels-3",
    "href": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels-3",
    "title": "Regression Analysis I",
    "section": "Case Study: Finding a good deal among hotels",
    "text": "Case Study: Finding a good deal among hotels\n\nFit and causation\nThe R-squared of the regression is 0.10 = 10%.\nThere is a lot left unexplained.\n\nStill, good for cross-sectional regression with a single explanatory variable.\nIn any case it is the fit of the best-fitting line."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels-4",
    "href": "rm-data/slides/week05-06.html#case-study-finding-a-good-deal-among-hotels-4",
    "title": "Regression Analysis I",
    "section": "Case Study: Finding a good deal among hotels",
    "text": "Case Study: Finding a good deal among hotels\n\nSlope is -12\nDoes that mean that a longer distance causes hotels to be cheaper?"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#summary-take-away",
    "href": "rm-data/slides/week05-06.html#summary-take-away",
    "title": "Regression Analysis I",
    "section": "Summary take-away",
    "text": "Summary take-away\n\nRegression – method to compare average \\(y\\) across observations with different values of \\(x\\).\nNon-parametric regressions (bin scatter, lowess, lpoly): use them to visualize complicated patterns of association between \\(y\\) and \\(x\\), No Number to interpret.\nLinear regression – linear approximation of the average pattern of association \\(y\\) and \\(x\\)\nWhen \\(\\beta\\) is not zero, one of three things (+ any combination) may be true:\n\n\\(x\\) causes y\n\\(y\\) causes x\nA third variable causes both \\(x\\) and y"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#motivation",
    "href": "rm-data/slides/week05-06.html#motivation",
    "title": "Regression Analysis I",
    "section": "Motivation",
    "text": "Motivation\n\nInterested in the pattern of association between life expectancy in a country and how rich that country is.\n\nUncovering that pattern is interesting for many reasons: discovery and learning from data.\n\nIdentify countries where people live longer than what we would expect based on their income, or countries where people live shorter lives.\nAnalyzing regression residuals.\nGetting a good approximation of the \\(y_E = f(x)\\) function is important."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#functional-form-1",
    "href": "rm-data/slides/week05-06.html#functional-form-1",
    "title": "Regression Analysis I",
    "section": "Functional form",
    "text": "Functional form\n\nSo far, we have only considered linear regression. (aside from non-parametric regressions)\nRelationships between \\(y\\) and \\(x\\) are often complicated!\nWhen and why care about the shape of a regression?\n\nWhen we need to talk about the non-average person.\n\nHow can we capture function form better?\n\nWe can transform variables in a simple linear regression."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#functional-form---linear-approximation",
    "href": "rm-data/slides/week05-06.html#functional-form---linear-approximation",
    "title": "Regression Analysis I",
    "section": "Functional form - linear approximation",
    "text": "Functional form - linear approximation\n\nLinear regression is a linear approximation to a regression of unknown shape:\nBut, we may want to modify the regression to better characterize nonlinear patterns\n\nprediction or analyze residuals - better fit\nwe want to go beyond the average pattern of association (different \\(x\\)s)\nall we care about is the average pattern of association, but the linear approximation is bad\n\nNot care\n\nif all we care about is the average pattern of association,\nif linear regression is good approximation to the average pattern"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#functional-form---types",
    "href": "rm-data/slides/week05-06.html#functional-form---types",
    "title": "Regression Analysis I",
    "section": "Functional form - types",
    "text": "Functional form - types\nNon linearities can be captured in many ways: - Natural log transformation: \\(\\ln(x)\\) when interested in relative differences - Piecewise linear splines: For flexibility in the pattern of association - Polynomials - quadratic form: Flexible yet simple"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#log-transformation",
    "href": "rm-data/slides/week05-06.html#log-transformation",
    "title": "Regression Analysis I",
    "section": "log transformation",
    "text": "log transformation\n\nSome times, some patterns are better approximated when \\(y\\) or \\(x\\) are measured as relative differences\n\nParticularly relevant if there is no natural base for comparison.\n\nTaking the natural logarithm of a variable is often a good solution in such cases, because they approximate relative differences."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#logarithmic-transformation---interpretation",
    "href": "rm-data/slides/week05-06.html#logarithmic-transformation---interpretation",
    "title": "Regression Analysis I",
    "section": "Logarithmic transformation - interpretation",
    "text": "Logarithmic transformation - interpretation\n\n\\(\\ln(x)\\) or \\(\\log(x)\\) is the natural logarithm of \\(x\\)\n\nYou can only use it if \\(x\\) is always a positive number\n\\(\\ln(0)\\) or \\(\\ln(\\text{negative number })\\) are not \\(Real\\)\n\nUsing this transformation, you can compare relative differences: \\[\\ln(x + \\Delta x) - \\ln(x) \\approx \\frac{\\Delta x}{x}\\]\nas long as \\(\\Delta x\\) is small.\n\n\\(\\ln(1.01)-\\ln(1) = 0.0099 \\approx 0.01\\)\n\\(\\ln(1.1)-\\ln(1) = 0.095 \\approx 0.1\\)\nbut…\\(\\ln(1.4)-\\ln(1) = 0.336\\) much less than 0.4"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#when-to-take-logs",
    "href": "rm-data/slides/week05-06.html#when-to-take-logs",
    "title": "Regression Analysis I",
    "section": "When to take logs?",
    "text": "When to take logs?\n\nWhen comparison makes mores sense in relative terms\n\nPercentage differences, relative differences, growth rates\n\nMost important examples\n\nPrices\nSales, turnover, GDP\nPopulation, employment\nCapital stock, inventories"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#interpreting-parameters-of-regressions-with-log-variables",
    "href": "rm-data/slides/week05-06.html#interpreting-parameters-of-regressions-with-log-variables",
    "title": "Regression Analysis I",
    "section": "Interpreting parameters of regressions with log variables",
    "text": "Interpreting parameters of regressions with log variables\n\nlog-lin modellin-log modellog-log modelKeep in mind\n\n\n\\(\\ln(y^E) = \\alpha + \\beta x_i\\)\n\nlog \\(y\\), level \\(x\\)\n\\(\\alpha\\) is average \\(\\ln(y)\\) when \\(x\\) is zero. (Often meaningless.)\n\\(\\beta\\): \\(y\\) is \\(\\beta * 100\\) percent higher, on average for observations with one unit higher \\(x\\).\n\n\n\n\\(y^E = \\alpha + \\beta\\ln(x_i)\\)\n\nlevel \\(y\\), log \\(x\\)\n\\(\\alpha\\) is : average \\(y\\) when \\(\\ln(x)\\) is zero (and thus \\(x\\) is one), not very meaningful.\n\\(\\beta\\): \\(y\\) is \\(\\beta/100\\) units higher, on average, for observations with one percent higher \\(x\\).\n\n\n\n\\(\\ln(y^E) = \\alpha + \\beta\\ln(x_i)\\)\n\nlog \\(y\\), log \\(x\\)\n\\(\\alpha\\): is average \\(\\ln(y)\\) when \\(\\ln(x)\\) is zero. (Often meaningless.)\n\\(\\beta\\): \\(y\\) is \\(\\beta\\) percent higher on average for observations with one percent higher \\(x\\).\n\nElasticity!\n\n\n\n\n\nPrecise interpretation is key\nThe interpretation of the slope (and the intercept) coefficient(s) differs in each case!\nOften verbal comparison is made about a 10% difference in \\(x\\) if using level-log or log-log regression."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#to-take-log-or-not-to-take-log",
    "href": "rm-data/slides/week05-06.html#to-take-log-or-not-to-take-log",
    "title": "Regression Analysis I",
    "section": "To Take log or Not to Take log",
    "text": "To Take log or Not to Take log\nDecide for substantive reason:\n\nTake logs if variable is likely affected in multiplicative ways\nDon’t take logs if variable is likely affected in additive ways\n\nDecide for statistical reason:\n\nLinear regression is better at approximating average differences if distribution of dependent variable is closer to normal.\nTake logs if skewed distribution with long right tail\nMost often the substantive and statistical arguments are aligned"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#to-take-log-or-not-to-take-log-1",
    "href": "rm-data/slides/week05-06.html#to-take-log-or-not-to-take-log-1",
    "title": "Regression Analysis I",
    "section": "To Take log or Not to Take log",
    "text": "To Take log or Not to Take log\n\nLog needs variable to be positive: Never negative, never zero\nSometimes you may be able to combine Logs with Dummies (if zero or negative values are present)\nSometimes adding a constant seems to do the trick\n\n\\(\\ln(x+1)\\) if \\(x\\) is positive or zero\nBut Not a good solution. May need to consider other transformations"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#hotel-price-distance-regression-and-functional-form",
    "href": "rm-data/slides/week05-06.html#hotel-price-distance-regression-and-functional-form",
    "title": "Regression Analysis I",
    "section": "Hotel price-distance regression and functional form",
    "text": "Hotel price-distance regression and functional form\nComparing different models\n\nCode\nqui {\n  set linesize 255\n  capture gen log_price = log(price)\n  capture gen log_distance = log(distance)\n  regress price distance\n  est sto m1\n  regress log_price distance\n  est sto m2\n  regress price log_distance\n  est sto m3\n  regress log_price log_distance\n  est sto m4\n}\nesttab m1 m2 m3 m4, se md nostar nonumber note(\"\")\n\n\n\n\n\n\n\n\n\n\n\n\nprice\nlog_price\nprice\nlog_price\n\n\n\n\ndistance\n-12.05\n-0.104\n\n\n\n\n\n(2.001)\n(0.0161)\n\n\n\n\nlog_distance\n\n\n-21.28\n-0.176\n\n\n\n\n\n(2.251)\n(0.0183)\n\n\n_cons\n131.9\n4.829\n114.8\n4.682\n\n\n\n(3.740)\n(0.0301)\n(2.087)\n(0.0169)\n\n\nN\n321\n321\n320\n320\n\n\n\nAs Excercise, plot the different models."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#which-model-shall-we-choose---substantive-reasons",
    "href": "rm-data/slides/week05-06.html#which-model-shall-we-choose---substantive-reasons",
    "title": "Regression Analysis I",
    "section": "Which model shall we choose? - Substantive reasons",
    "text": "Which model shall we choose? - Substantive reasons\n\nIt depends on the goal of the analysis!\nPrices\n\nWe are after a good deal on a single night – absolute price differences are meaningful.\nPercentage differences in price may remain valid if inflation and seasonal fluctuations affect prices proportionately.\nOr we are after relative differences - we do not mind about the magnitude that we are paying, we only need the best deal.\n\nDistance\n\nDistance could make more sense in miles than in relative terms – given our purpose is to find a relatively cheap hotel."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#which-model-shall-we-choose---statistical-reasoning",
    "href": "rm-data/slides/week05-06.html#which-model-shall-we-choose---statistical-reasoning",
    "title": "Regression Analysis I",
    "section": "Which model shall we choose? - Statistical reasoning",
    "text": "Which model shall we choose? - Statistical reasoning\n\nVisual inspection\n\nWhich model captures patterns better?\n\nCompare fit measure (\\(R^2\\))\n\nBut be careful. If \\(y\\) is in logs, \\(R^2\\) is not directly comparable to \\(R^2\\) when \\(y\\) is in levels.\nits like comparing apples and oranges\n\nFinal verdict:\n\nYour call…."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#other-transformations-splines",
    "href": "rm-data/slides/week05-06.html#other-transformations-splines",
    "title": "Regression Analysis I",
    "section": "Other transformations: splines",
    "text": "Other transformations: splines\n\nWarning Splines are another way to estimate non-parametric models. Just a bit more parametric.\nA regression with a piecewise linear spline (of \\(x\\)) results in connected line segments for the mean dependent variable,\n\neach line segment corresponding to a specific interval of the explanatory variable.\n\nThe points of connection are called knots,\nThe places of the knots (the boundaries of the intervals of the explanatory variable) need to be specified by the analyst.\nPlot-twist: The segments need not to be linear!"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#other-transformations-splines-1",
    "href": "rm-data/slides/week05-06.html#other-transformations-splines-1",
    "title": "Regression Analysis I",
    "section": "Other transformations: splines",
    "text": "Other transformations: splines\n\nAdvantage: We can interpret parameters!\nThe formula:\n\n\\[y^E = \\alpha_1 + \\beta_1 x[\\text{if } $x$ &lt; k_1] + (\\alpha_2 + \\beta_2 x)[\\text{if } k_1 \\leq $x$ \\leq k_2] + \\dots + (\\alpha_m + \\beta_m x)[\\text{if } $x$ \\geq k_{m-1}]\\]\nBut we we usually assume that \\(\\alpha_2, \\alpha_3, \\dots, \\alpha_m = 0\\) and only allow \\(\\beta's\\) to change"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#other-transformations-splines-2",
    "href": "rm-data/slides/week05-06.html#other-transformations-splines-2",
    "title": "Regression Analysis I",
    "section": "Other transformations: splines",
    "text": "Other transformations: splines\nInterpretation of the most important parameters\n\n\\(\\alpha\\): average \\(y\\) when \\(x\\) is zero.\n\\(\\beta_1\\): How much higher \\(y\\) is, on average, for observations with one unit higher \\(x\\) value, if \\(x \\in (-\\infty , k_1)\\).\n\\(\\beta_2\\): How much higher \\(y\\) is, on average, for observations with one unit higher \\(x\\) value, if \\(x \\in (k_2, k_2)\\).\nEtc. This is the “slope” of the line segment.\nYou can also use marginal slopes."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#section",
    "href": "rm-data/slides/week05-06.html#section",
    "title": "Regression Analysis I",
    "section": "",
    "text": "Splines - Example\n\nHow to use itResults\n\n\n\nYou need to create all necessary variables, given your knots.\nSay we choose knots 1 and 2 for the price analysis\n\n\n*              v Knot  v knot2\nmkspline dist1 1 dist2 2 dist3= distance  \n\n\n\n\n\nCode\nregress price dist1 dist2 dist3\n\n\n\n      Source |       SS           df       MS      Number of obs   =       321\n-------------+----------------------------------   F(3, 317)       =     41.08\n       Model |  162850.558         3  54283.5192   Prob &gt; F        =    0.0000\n    Residual |  418846.994       317  1321.28389   R-squared       =    0.2800\n-------------+----------------------------------   Adj R-squared   =    0.2731\n       Total |  581697.551       320  1817.80485   Root MSE        =    36.349\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       dist1 |  -75.91485   8.747817    -8.68   0.000    -93.12596   -58.70373\n       dist2 |  -.1596926   7.344118    -0.02   0.983    -14.60907    14.28968\n       dist3 |    2.54916   3.843377     0.66   0.508     -5.01259    10.11091\n       _cons |   174.5939   6.098417    28.63   0.000     162.5954    186.5924\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#other-transformations-splines-3",
    "href": "rm-data/slides/week05-06.html#other-transformations-splines-3",
    "title": "Regression Analysis I",
    "section": "Other transformations: splines",
    "text": "Other transformations: splines\n\nSplines can handles any kind of nonlinearity\nOffers a lot of flexibility,\nBut requires decisions from the analyst\n\nHow many knots?\nWhere to locate them\nDecision based on scatterplot, theory / business knowledge\nMachine learning\n\nThey can also be more complicated: quadratic, cubic or B-splines. Smooth and flexible."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#polynomials",
    "href": "rm-data/slides/week05-06.html#polynomials",
    "title": "Regression Analysis I",
    "section": "Polynomials",
    "text": "Polynomials\nThis is a simpler way to capture non-linearities\n\nQuadratic function of the explanatory variable, allowing for a smooth change in the slope\n\nTechnically: quadratic function is not a linear function (a parabola, not a line), but the model is still linear in its coefficients.\n\nHandles nonlinearities similar to a parabola.\nLess flexible, but easier interpretation!\n\nJust need basic calculus, or “logic”"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#the-quadratic-form",
    "href": "rm-data/slides/week05-06.html#the-quadratic-form",
    "title": "Regression Analysis I",
    "section": "The quadratic form",
    "text": "The quadratic form\n\\[y^E = \\alpha + \\beta_1 x + \\beta_2 x^2\\]\n\n\\(\\beta_1\\) has no interpretation (unless x=0),\n\\(\\beta_2\\neq 0\\) if the functional form is U-shaped (\\(\\beta_2 &gt; 0\\)) or inverted U-shaped (\\(\\beta_2 &lt; 0\\)).\n\nBut you may not see it in the data\n\nThe slope: \\(\\beta_1 + 2\\beta_2 x\\) is different at different values of \\(x\\).\nYou can use slope for comparing the effect of \\(x\\) on \\(y\\) for small changes in \\(x\\).\nFor large changes, need to calculate manually."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#life-expectancy-and-income",
    "href": "rm-data/slides/week05-06.html#life-expectancy-and-income",
    "title": "Regression Analysis I",
    "section": "Life expectancy and income",
    "text": "Life expectancy and income\n\nIs there a relationship between How long people live in a country and how rich that country is?\nTo identify countries where people live longer than what we would expect based on their income, or countries where people live shorter lives.\nAnalyzing regression residuals – getting a good approximation of the \\(y_E = f(x)\\) function is important."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#life-expectancy-and-income-1",
    "href": "rm-data/slides/week05-06.html#life-expectancy-and-income-1",
    "title": "Regression Analysis I",
    "section": "Life expectancy and income",
    "text": "Life expectancy and income\n\nuse data_slides/wb-lifeexpectancy.dta, clear\nkeep if year == 2017\nsum gdppc lifeexp\n\n(4,847 observations deleted)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       gdppc |        182    19.22786    20.38674   .6707771   113.2622\n     lifeexp |        182    72.30765    7.648017     52.214   84.68049"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#life-expectancy-and-gdp",
    "href": "rm-data/slides/week05-06.html#life-expectancy-and-gdp",
    "title": "Regression Analysis I",
    "section": "Life expectancy and GDP",
    "text": "Life expectancy and GDP\n\nLife Exp vs GDPpcLife Exp vs Log GDPpcRegressionInteresting Findings\n\n\n\n\nCode\nscatter lifeexp gdppc, scale(1.4) ///\nytitle(Life Expectancy) xtitle(GDP per capita) ///\nxlabel(0(25)100)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nscatter lifeexp gdppc, scale(1.4) ///\nytitle(Life Expectancy) xtitle(GDP per capita) ///\nxscale(log) xlabel(1 2 5 10 25 50 100)\n\n\n\n\n\n\n\n\n\n\n\n\ngen log_gdp = log(gdppc)\nregress lifeexp log_gdp\npredict resid, res\npredict life_hat\nsort resid\n\n\n      Source |       SS           df       MS      Number of obs   =       182\n-------------+----------------------------------   F(1, 180)       =    382.77\n       Model |  7200.86382         1  7200.86382   Prob &gt; F        =    0.0000\n    Residual |  3386.21735       180  18.8123186   R-squared       =    0.6802\n-------------+----------------------------------   Adj R-squared   =    0.6784\n       Total |  10587.0812       181  58.4921612   Root MSE        =    4.3373\n\n------------------------------------------------------------------------------\n     lifeexp | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     log_gdp |   5.333648   .2726172    19.56   0.000     4.795712    5.871585\n       _cons |   59.65933   .7220205    82.63   0.000     58.23462    61.08404\n------------------------------------------------------------------------------\n(option xb assumed; fitted values)\n\n\n\n\n\nlist countryname lifeexp gdppc life_hat if inrange(_n,1,5) | inrange(_n,178,182) \n\n\n     +---------------------------------------------------+\n     |       countryname   lifeexp      gdppc   life_hat |\n     |---------------------------------------------------|\n  1. | Equatorial Guinea    57.939   22.29894   76.21785 |\n  2. |           Nigeria    53.875   5.351441   68.60581 |\n  3. |          Eswatini    58.268   9.567586   71.70473 |\n  4. |     Cote d'Ivoire    54.102   3.564596   66.43867 |\n  5. |           Lesotho    54.568   2.845889   65.23766 |\n     |---------------------------------------------------|\n178. |           Lebanon    79.758   11.64702    72.7537 |\n179. |           Vietnam    76.454   6.233485   69.41956 |\n180. |           Vanuatu    72.334    2.82708   65.20229 |\n181. |         Nicaragua    75.653   5.169298   68.42111 |\n182. |   Solomon Islands    71.006   2.126353   63.68308 |\n     +---------------------------------------------------+"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#life-expectancy-and-income-can-we-do-better",
    "href": "rm-data/slides/week05-06.html#life-expectancy-and-income-can-we-do-better",
    "title": "Regression Analysis I",
    "section": "Life expectancy and income: Can we do better?",
    "text": "Life expectancy and income: Can we do better?\n\nProbably…Linear regression seems a good fit, but you can try other functional forms.\n\nQuadradic, Splines, etc\n\nKeep \\(y\\) is the same for easy comparison\n\nExplore the residuals, see if you can do better."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#which-functional-form-to-choose---guidelines",
    "href": "rm-data/slides/week05-06.html#which-functional-form-to-choose---guidelines",
    "title": "Regression Analysis I",
    "section": "Which functional form to choose? - guidelines",
    "text": "Which functional form to choose? - guidelines\nStart with deciding whether you care about nonlinear patterns or not.\n\nLinear approximation OK if focus is on an average association.\nTransform variables for a better interpretation of the results (e.g. log), and it often makes linear regression better approximate the average association.\nAccommodate a nonlinear pattern if our focus is\n\non prediction,\nanalysis of residuals,\nabout how an association varies beyond its average.\n\nKeep in mind - simpler the better!"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#data-is-messy",
    "href": "rm-data/slides/week05-06.html#data-is-messy",
    "title": "Regression Analysis I",
    "section": "Data Is Messy",
    "text": "Data Is Messy\n\nClean and neat data exist only in dreams, and textbooks\nData may be messy in many ways\n\nStructure, storage type differs from what we want\nNeeds cleaning (see Chapters 1,2)\nSome observations are influential\nVariables measured with error\nSome observations may represent more individuals (Weights?)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#extreme-values",
    "href": "rm-data/slides/week05-06.html#extreme-values",
    "title": "Regression Analysis I",
    "section": "Extreme values",
    "text": "Extreme values\n\nSome observations may contain Extreme values, compared to the rest of the data\nExtreme values examples\n\nBanking sector employment share in countries. Luxembourg: 10%\nHotel price of 1 US dollars, 10,000 US dollars\nProduction of a small firm: 1,000,000,000 units"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#influential-observations",
    "href": "rm-data/slides/week05-06.html#influential-observations",
    "title": "Regression Analysis I",
    "section": "Influential observations",
    "text": "Influential observations\n\nInfluential observations\n\nTheir inclusion or exclusion influences the regression line (sensitivity)\nInfluential observations are often extreme values (on \\(x\\) or \\(y\\))\nBut not all extreme values are influential observations\n\nInfluential observations example\n\nVery large tech companies in a regression of size and average wage\nA single mixed-race worker in a regression"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#what-to-do-with-them",
    "href": "rm-data/slides/week05-06.html#what-to-do-with-them",
    "title": "Regression Analysis I",
    "section": "What to do with them?",
    "text": "What to do with them?\n\nDepends on why they are extreme\nIf by mistake: may want to drop them (EUR1000+) (or Impute them)\nIf by nature: don’t want to drop them (Part of the distribution)\nGrey zone: patterns work differently for them for substantive reasons\nGeneral rule: avoid dropping observations based on value of \\(y\\) variable\n\nDropping extreme observations by \\(x\\) variable may be OK\nBut those are valuable as they represent informative and large variation"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#measurement-errors-in-variables",
    "href": "rm-data/slides/week05-06.html#measurement-errors-in-variables",
    "title": "Regression Analysis I",
    "section": "Measurement Errors In Variables",
    "text": "Measurement Errors In Variables\n\nGoal: measuring the association between variables\n\nFurthermore, we are interested in the estimated value (not just the sign)\n\nBut, observed variables have measurement error\n\nMistake, hard-to-measure data, created variables\n\nOften cannot do anything about it!\nSo, what the consequence is of such errors??\nDoes the answer depend on the type of measurement error?"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#classical-measurement-error",
    "href": "rm-data/slides/week05-06.html#classical-measurement-error",
    "title": "Regression Analysis I",
    "section": "Classical Measurement Error",
    "text": "Classical Measurement Error\n\\[w = w^* + \\varepsilon\\]\n\nis zero on average (so it does not affect the average of the measured variable) and\nis independent of all other relevant variables, including the error-free variable.\n\n\nRecording errors: E.g., due to mistakes in entering data\nReporting errors: in surveys or administrative data If they are random around the true quantities"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#cme-in-the-dependent-variable-y",
    "href": "rm-data/slides/week05-06.html#cme-in-the-dependent-variable-y",
    "title": "Regression Analysis I",
    "section": "CME in the dependent variable (\\(y\\))",
    "text": "CME in the dependent variable (\\(y\\))\nConsider \\(y = y^* + e\\), where \\(y\\) is the measured variable, \\(y^*\\) is the error-free variable, and \\(e\\) is the measurement error (noise).\nThe slope coefficient of \\(y\\) and \\(y^*\\) on \\(x\\) are:\n\\[\\beta^* = \\frac{\\text{Cov}[y^*, x]}{\\text{Var}[x]} \\text{ and } \\beta = \\frac{\\text{Cov}[y^* + e, x]}{\\text{Var}[x]}\n\\]\n\\[\\beta = \\frac{\\text{Cov}[y^*, x]}{\\text{Var}[x]} + \\left(\\frac{\\text{Cov}[e, x]}{\\text{Var}[x]}\\approx  0\\right) \\approx \\beta^*  \n\\]\n\nConsequence: classical measurement error in \\(y\\) is not expected to affect the regression coefficients."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#cme-in-the-explanatory-variable-x",
    "href": "rm-data/slides/week05-06.html#cme-in-the-explanatory-variable-x",
    "title": "Regression Analysis I",
    "section": "CME in the explanatory variable (\\(x\\))",
    "text": "CME in the explanatory variable (\\(x\\))\nConsider \\(x = x^* + e\\), where \\(x\\) is the measured variable, \\(x^*\\) is the error-free variable, and \\(e\\) is the measurement error (noise).\nThe slope coefficient are: \\[\\beta^* = \\frac{\\text{Cov}[y, x^*]}{\\text{Var}[x^*]} \\text{ and } \\beta = \\frac{\\text{Cov}[y, x^*+e]}{\\text{Var}[x^*+e]}\n\\]\n\\[\\beta = \\frac{\\text{Cov}[y, x^*]+(\\text{Cov}[y, e]\\approx 0)}{\\text{Var}[x^*]+\\text{Var}[e]} = \\beta^* \\frac{\\text{Var}[x^*]}{\\text{Var}[x^*]+\\text{Var}[e]}\n\\]\n\nConsequence: CME in \\(x\\) will affect the regression coefficients. (Towards zero) Attenuation Bias\n\\(\\alpha\\) is also affected"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#classical-measurement-error-in-the-explanatory-variable-x",
    "href": "rm-data/slides/week05-06.html#classical-measurement-error-in-the-explanatory-variable-x",
    "title": "Regression Analysis I",
    "section": "Classical measurement error in the explanatory variable (\\(x\\))",
    "text": "Classical measurement error in the explanatory variable (\\(x\\))\n\nNoise to signal ratio is: \\(\\frac{\\text{Var}[e]}{\\text{Var}[x^*]}\\)\n\nHow much noise is there compared to the true variation \\(x*\\)\n\nWhen the noise-to-signal ratio is low, we may safely ignore the problem.\n\nthis happens often when\n\nwhen we are confident that recording errors are at not important\nwhen our data has an aggregate variable estimated from very large samples.\n\n\nWhen the noise-to-signal ratio is substantial\n\nwe may be better of assessing its consequences."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#extra-non-classical-measurement-error",
    "href": "rm-data/slides/week05-06.html#extra-non-classical-measurement-error",
    "title": "Regression Analysis I",
    "section": "Extra: non-classical measurement error",
    "text": "Extra: non-classical measurement error\n\nIn real-life data measurement error in variables may or may not be classical\n\nVery often, it isn’t!\n\nVariables measured with error may be less dispersed (non-zero mean)\n\nExample: Self reported Income\n\nMeasurement error may be related to variables of interest\n\nExample: Self-reported weight and height\n\nThis often means that modelling needs to be redesigned"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#classical-measurement-error-summary",
    "href": "rm-data/slides/week05-06.html#classical-measurement-error-summary",
    "title": "Regression Analysis I",
    "section": "Classical measurement error summary",
    "text": "Classical measurement error summary\n\nCME in the dependent (\\(y\\)) variable is not expected to affect the regression coefficients.\nCME in the explanatory (\\(x\\)) variable will affect the regression coefficients.\n\nThe estimated beta will be closer to zero than it would be without measurement error.\n\nAlmost all variables are measured with error. Need to think about consequences."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#hotel-ratings-and-measurement-error",
    "href": "rm-data/slides/week05-06.html#hotel-ratings-and-measurement-error",
    "title": "Regression Analysis I",
    "section": "Hotel ratings and measurement error",
    "text": "Hotel ratings and measurement error\n\nReview the case of Ratings and Prices of hotels\n\nAverage customer ratings are noisy and bad proxies for the true quality of a hotel.\nThe true quality of a hotel is unobserved.\n\nRegressions for hotels with few ratings are likely to produce attenuated slope coefficients.\nAnd that is what you can find!\n\nI would argue the same with Amazon reviews\n\nBut what to do?\n\nperhaps Robustness checks. Restrict regressions to cases with different levels of Measurement error."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#using-weights-in-regressions",
    "href": "rm-data/slides/week05-06.html#using-weights-in-regressions",
    "title": "Regression Analysis I",
    "section": "Using weights in regressions",
    "text": "Using weights in regressions\n\nDifferent observations may have different weights (importance or size)\n\nto denote different size of larger units in the data\npopulation of countries\n\nUse weights of size IF want to uncover the patterns of association for the individuals\n\nwho make up the larger units (e.g., people in countries),\n\nAlso, use weights when you want your data to be representative of the population\n\nwhen you want to generalize the results to the population"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#life-expectancy-and-gdp-per-capita---weights",
    "href": "rm-data/slides/week05-06.html#life-expectancy-and-gdp-per-capita---weights",
    "title": "Regression Analysis I",
    "section": "Life expectancy and GDP per capita - weights",
    "text": "Life expectancy and GDP per capita - weights\n\n\nCode\nqui: reg lifeexp log_gdp [w=population ]\npredict life_hatw\nqui: reg lifeexp log_gdp \npredict life_hatnw\n*p*redict life_hat\ntwo (scatter lifeexp gdppc [w=population ],  color(%50)) ///\n(scatter lifeexp gdppc [w=population ] if inlist(countryname,\"China\",\"India\",\"United States\"),  color(%50) mlabel(countryname)) ///\n(line life_hatw life_hatnw gdppc,sort  ), scale(1.4) legend(off) ///\nytitle(Life Expectancy) xtitle(GDP per capita) ///\nxscale(log) xlabel(1 2 5 10 25 50 100)\n\n\n(option xb assumed; fitted values)\n(option xb assumed; fitted values)\n(analytic weights assumed)\n(analytic weights assumed)\n(analytic weights assumed)\n(analytic weights assumed)\n(analytic weights assumed)\n(analytic weights assumed)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#summary-take-away-1",
    "href": "rm-data/slides/week05-06.html#summary-take-away-1",
    "title": "Regression Analysis I",
    "section": "Summary take-away",
    "text": "Summary take-away\n\nNonlinear functional forms may or may not be important for regression analysis.\nThey are usually important for prediction.\nless important for causal analysis.\nWhen important, we have multiple options: Logs, splines, polynomials\nInfluential observations and other extreme values are usually best analyzed with the rest of the data\nDiscard them only if you have a good reason."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#generalizing-reminder",
    "href": "rm-data/slides/week05-06.html#generalizing-reminder",
    "title": "Regression Analysis I",
    "section": "Generalizing: reminder",
    "text": "Generalizing: reminder\n\nWe have uncovered some pattern in our data. We are interested in generalize the results.\nQuestion: Is the pattern we see in our data\n\nTrue in general?\nor is it just a special case (unique to the sample)?\n\nInference - the act of generalizing results\n\nFrom a particular dataset to other situations.\n\nFrom a sample to population = statistical inference\nBeyond (other dates, countries, people, firms) = external validity"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#generalizing-linear-regression",
    "href": "rm-data/slides/week05-06.html#generalizing-linear-regression",
    "title": "Regression Analysis I",
    "section": "Generalizing Linear Regression",
    "text": "Generalizing Linear Regression\n\nWe estimated the linear model\n\n\\(\\hat{\\beta}\\) is the average difference in \\(y\\) in the dataset between observations that are different in terms of \\(x\\) by one unit.\n\\(\\hat{y}_i\\) best guess for the expected value (average) of the dependent variable for observation \\(i\\) with value \\(x_i\\)\n\nSometimes all we care about patterns in the data we have.\nBut often we are interested in patterns beyond the Dataset.\nTo what extent they can be generalized"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#statistical-inference-confidence-interval",
    "href": "rm-data/slides/week05-06.html#statistical-inference-confidence-interval",
    "title": "Regression Analysis I",
    "section": "Statistical Inference: Confidence Interval",
    "text": "Statistical Inference: Confidence Interval\n\nThe 95% CI of the slope coefficient is similar to estimating a 95% CI of any other statistic. \\[CI(\\hat{\\beta})_{95\\%} = [\\hat{\\beta} - 1.96SE(\\hat{\\beta}), \\hat{\\beta} + 1.96SE(\\hat{\\beta})]\n\\]\nThe standard error (SE) of the slope coefficient\n\nis conceptually the same as the SE of any statistic.\nmeasures the spread of the values of the statistic across hypothetical repeated samples drawn from the same population our data represents"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#standard-error-of-the-slope",
    "href": "rm-data/slides/week05-06.html#standard-error-of-the-slope",
    "title": "Regression Analysis I",
    "section": "Standard Error of the Slope",
    "text": "Standard Error of the Slope\n\n\n\\[SE(\\hat{\\beta}) = \\frac{Std[e]}{\\sqrt n Std[x]}\\]\n\nWhere:\n\nResidual: \\(e = y - \\hat{\\alpha} - \\hat{\\beta}x\\)\n\\(Std[e]\\), the standard deviation (SD) of the regression residual,\n\\(Std[x]\\), the SD of the explanatory variable,\n\\(\\sqrt{n}\\) , Often we use \\(\\sqrt{n - 2}\\).\n\n\n\n\nA smaller standard error translates into narrower CI and more precise estimates.\nWe get more precision if\n\nsmaller the standard deviation of the residual (better fit)\nlarger the standard deviation of the explanatory variable – more variation in \\(x\\) is good.\nmore data.\n\nThis formula is correct assuming homoskedasticity"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#heteroskedasticity-robust-se",
    "href": "rm-data/slides/week05-06.html#heteroskedasticity-robust-se",
    "title": "Regression Analysis I",
    "section": "Heteroskedasticity Robust SE",
    "text": "Heteroskedasticity Robust SE\n\nSimple SE formula is not correct in general.\nHomoskedasticity assumption, the goodness of fit of the regression line is the same across the entire range of the \\(x\\) variable.\n\nResiduals are spread evenly around the regression line.\n\nIn general this is not true\nHeteroskedasticity: the fit may differ at different values of \\(x\\) so that the spread of actual \\(y\\) around the regression is different for different values of \\(x\\)\nSo…what to do?\n\nNeed to adjust the SE formula!"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#heteroskedasticity-you-have-options",
    "href": "rm-data/slides/week05-06.html#heteroskedasticity-you-have-options",
    "title": "Regression Analysis I",
    "section": "Heteroskedasticity: You have options",
    "text": "Heteroskedasticity: You have options\n\nThere are many ways to correct for heteroskedasticity\n\nGeneralized least squares (GLS)\nWeighted least squares (WLS)\nFeasible generalized least squares (FGLS)\nHuber-White robust standard errors\n\nTraditionally, you also want to test if you have a Heteroskedasticity problem\n\nWhite test, Breusch-Pagan test\n\n\nBut for now lets assume you have a heteroskedasticity problem"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#heteroskedasticity-robust-se-1",
    "href": "rm-data/slides/week05-06.html#heteroskedasticity-robust-se-1",
    "title": "Regression Analysis I",
    "section": "Heteroskedasticity Robust SE",
    "text": "Heteroskedasticity Robust SE\n\nWhite-Huber Robust SE is correct with and without heteroskedasticity.\nSame properties as before: smaller when \\(Std[e]\\) is small, \\(Std[x]\\) is large and \\(n\\) is large\nMathematically, Huber-White SE “corrects” the simple SE using the residuals from the regression. \\[Var_r(\\hat{\\beta}) = \\frac{\\sum (x_i-\\bar x)^2 \\hat e_i^2}{\\left(\\sum (x_i-\\bar x)^2\\right)^2}\\]\n\nNote: there are many heteroskedastic-robust formula: ‘HC0’, ‘HC1’, ‘HC2’, ‘HC3’. Stata uses HC1 when you ask for robust SE: regress y x, robust"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#anythings-else",
    "href": "rm-data/slides/week05-06.html#anythings-else",
    "title": "Regression Analysis I",
    "section": "Anythings else?",
    "text": "Anythings else?\n\nNope\n\nCoefficient and \\(R^2\\) remain the same\n\nJust make sure you are using robust SE.\nSE may be similar, but most likely larger than the simple SE"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#testing-if-true-beta-is-zero",
    "href": "rm-data/slides/week05-06.html#testing-if-true-beta-is-zero",
    "title": "Regression Analysis I",
    "section": "Testing if (true) beta is zero",
    "text": "Testing if (true) beta is zero\n\nTesting hypotheses: decide if a statement about a general pattern is true.\nThe question: are the Dependent variable and the explanatory variable related at all?\nThe null and the alternative: \\[H_0: \\beta_{true} = 0, H_A: \\beta_{true} \\neq 0\\]\nThe t-statistic is: \\[t = \\frac{\\hat{\\beta} - 0}{SE(\\hat{\\beta})}\\]\nOften \\(t = 2\\) (1.96) is the critical value, which corresponds to 95% CI. or 5% significance level (\\(\\alpha\\)) \\((t = 2.6 \\rightarrow 99\\%)\\)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#testing-if-true-beta-is-zero-1",
    "href": "rm-data/slides/week05-06.html#testing-if-true-beta-is-zero-1",
    "title": "Regression Analysis I",
    "section": "Testing if (true) beta is zero",
    "text": "Testing if (true) beta is zero\nPractical guidance, Same as before!:\n\nChoose a critical value.\n\np-value, the probability of a false positive in our dataset\nBalancing act: false positive (FP) and negative (FN)\n\nHigher critical value\n\nFP: less likely (less likely rejection of the null).\nFN: more likely (high risk of not rejecting a null even though it’s false)\n\nTypical critical values: 5% (1.96)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#ohh-that-p5-cutoff",
    "href": "rm-data/slides/week05-06.html#ohh-that-p5-cutoff",
    "title": "Regression Analysis I",
    "section": "Ohh, that \\(p=5\\%\\) cutoff",
    "text": "Ohh, that \\(p=5\\%\\) cutoff\n\nWhen testing, you start with a critical value first\nOften the standard to publish a result is to have a p value below 5%.\n\nArbitrary, but…there is lots of discussion about it.\n\nIf you find a result that cannot be told apart from 0 at 1% (max 5%), you should say that explicitly.\n\nSometimes that is what you want to say. A non-significant result is also a result."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#dealing-with-5-10",
    "href": "rm-data/slides/week05-06.html#dealing-with-5-10",
    "title": "Regression Analysis I",
    "section": "Dealing with 5-10%",
    "text": "Dealing with 5-10%\n\nSometimes regression result may be significant at 10%.\nWhat not to do? Avoid language like…\n\n“a barely detectable statistically significant difference” (\\(p=0.073\\))\n“a margin at the edge of significance” (\\(p=0.0608\\))\n\nSometimes you work on a proposal: Proof of concept.\n\nTo be lenient is okay. You may need more power!\nSay the point estimate and note the 95% confidence interval.\n\nSometimes you are looking for a proof. Beyond reasonable doubt.\n\nHere you wanna be below 1% (or less)\nBe honest…present the p-value, and the CI."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#p-hacking",
    "href": "rm-data/slides/week05-06.html#p-hacking",
    "title": "Regression Analysis I",
    "section": "p-Hacking",
    "text": "p-Hacking\n\nJust as before. Be honest. Do not fixate on the 5% level.\n\nSuggestion:\n\nPresent your most conservative result first\n\nExample: if uncertain, keep extreme values in.\n\nShow robustness checks: many additional regressions with different decisions"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#chance-events-and-size-of-data",
    "href": "rm-data/slides/week05-06.html#chance-events-and-size-of-data",
    "title": "Regression Analysis I",
    "section": "Chance Events And Size of Data",
    "text": "Chance Events And Size of Data\n\nSome times you just need more power.\nFinding patterns by chance may go away with more observations\nSpecificities to a single dataset may be less important if more sources\nMore observations help only if\n\nErrors and idiosyncrasies affect some observations but not all\nAdditional observations are from appropriate source\nIf worried about specificities of Vienna more observations from Vienna would not help"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#prediction-uncertainty",
    "href": "rm-data/slides/week05-06.html#prediction-uncertainty",
    "title": "Regression Analysis I",
    "section": "Prediction uncertainty",
    "text": "Prediction uncertainty\n\nGoal: predicting the value of \\(y\\) for observations outside the dataset, when only the value of \\(x\\) is known.\nWe predict \\(y\\) based on coefficient estimates, which are relevant in the general pattern/population. With linear regression you have a simple model: \\[y_i = \\hat{\\alpha} + \\hat{\\beta}x_i + \\epsilon_i\n\\]\nThe estimated statistic here is a predicted value for a particular observation \\(\\hat{y}_j\\). For an observation \\(j\\) with known value \\(x_j\\) this is \\[\\hat{y}_j = \\hat{\\alpha} + \\hat{\\beta}x_j\\]"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#prediction-uncertainty-1",
    "href": "rm-data/slides/week05-06.html#prediction-uncertainty-1",
    "title": "Regression Analysis I",
    "section": "Prediction uncertainty",
    "text": "Prediction uncertainty\n\nYou can produce two kinds of intervals:\n\nConfidence interval for the predicted value/regression line\n\nUncertainty comes from \\(\\hat{\\alpha}, \\hat{\\beta}\\)\n\nPrediction interval, uncertainty comes from \\(\\hat{\\alpha}, \\hat{\\beta}\\) and \\(\\epsilon_i\\)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#confidence-interval-of-the-regression-line-i.",
    "href": "rm-data/slides/week05-06.html#confidence-interval-of-the-regression-line-i.",
    "title": "Regression Analysis I",
    "section": "Confidence interval of the regression line I.",
    "text": "Confidence interval of the regression line I.\n\nThe predicted value \\(\\hat{y}_j\\) is based on \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\) only.\n\nThus, the CI of the predicted value combines the CI for \\(\\hat{\\alpha}\\) and the CI for \\(\\hat{\\beta}\\).\n\nWhat to expect if we know the value of \\(x_j\\) and \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\)?.\n\\[95\\%CI(\\hat{y}_j) = \\hat{y_j} \\pm 1.96 SE(\\hat{y}_j)\\]\nThe standard error of the predicted value is \\[SE(\\hat{y}_j) = \\frac{Std[e]}{\\sqrt{\\frac{1}{n}+ \\frac{(x_j - \\bar{x})^2}{nVar[x]}}}\n\\]\nUse robust SE formula in practice, but a simple formula is instructive"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#prediction-interval",
    "href": "rm-data/slides/week05-06.html#prediction-interval",
    "title": "Regression Analysis I",
    "section": "Prediction interval",
    "text": "Prediction interval\n\nPrediction interval answers:\n\nWhere to expect the particular \\(y_j\\) value if we know the corresponding \\(x_j\\) value and the estimates of the regression coefficients?\n\nThe CI of the predicted value is about \\(\\hat{y}_j\\): Where would the average value of the dependent variable be if we know \\(x_j\\).\nThe PI (prediction interval) is about \\(y_j\\) itself not its average value, what range of values we expect for \\(y_j\\) if we know \\(x_j\\).\nSo PI starts with CI. But adds additional uncertainty (\\(Std[\\epsilon_i]\\)) that actual \\(y_j\\) will be around its conditional."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#prediction-interval-1",
    "href": "rm-data/slides/week05-06.html#prediction-interval-1",
    "title": "Regression Analysis I",
    "section": "Prediction interval",
    "text": "Prediction interval\n\nThe formula for the 95% prediction interval is\n\n\\(95\\%PI(\\hat{y}_j) = \\hat{y} \\pm 1.96SPE(\\hat{y}_j)\\)\n\\(SPE(\\hat{y}_j) = Std[e]\\sqrt{\\textbf{1} + \\frac{1}{n} + \\frac{(x_j - \\bar{x})^2}{nVar[x]}}\\)\n\nSPE – Standard Prediction Error (SE of prediction)\nSummarizes the additional uncertainty: the actual \\(y_j\\) value is expected to be spread around its average value.\n\nThis is best estimated by the standard deviation of the residual \\(e\\).\n\nIn the formula, all elements get very small if \\(n\\) gets large, except for the new element."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#prediction-interval-2",
    "href": "rm-data/slides/week05-06.html#prediction-interval-2",
    "title": "Regression Analysis I",
    "section": "Prediction interval",
    "text": "Prediction interval\n\n\nCode\nqui:ssc install frause,\nqui:frause oaxaca, clear\nqui: drop if runiform()&lt;.8 \nqui: sort age\nqui: reg lnwage age\nqui: predict lnwage_hat\nqui: predict se_ci, stdp\nqui: predict se_pi, stdf\nqui: gen ci_low = lnwage_hat - 1.96*se_ci\nqui: gen ci_up = lnwage_hat + 1.96*se_ci\nqui: gen pi_low = lnwage_hat - 1.96*se_pi\nqui: gen pi_up = lnwage_hat + 1.96*se_pi\ntwoway  (rarea pi_low pi_up age, color(gs5%25) ) ///\n(rarea ci_low ci_up age, color(gs5%25) ) ///\n       (scatter lnwage age, color(navy)) (line lnwage_hat age, color(navy)) ///\n       , legend(off) ytitle(Log Wages) xtitle(Age)"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#external-validity-1",
    "href": "rm-data/slides/week05-06.html#external-validity-1",
    "title": "Regression Analysis I",
    "section": "External validity",
    "text": "External validity\n\nStatistical inference helps us generalize to the population or general pattern\nIs this true beyond the data (other dates, countries, people, firms)?\n\nwe can’t assess it using our data.\n\nWe’ll never really know. Only think, investigate, make assumption, and hope…"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#however",
    "href": "rm-data/slides/week05-06.html#however",
    "title": "Regression Analysis I",
    "section": "However…",
    "text": "However…\n\nAnalyzing other data can help!\nFocus on \\(\\beta\\), the slope coefficient on \\(x\\).\nThe three common dimensions of generalization are: time, space, and other groups.\nTo learn about external validity, we always need additional data, on say, other countries or time periods.\nWe can then repeat regression and see if slope is similar!\nMeta-analysis: combining results from different studies to learn about external validity."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#stability-of-hotel-prices---idea",
    "href": "rm-data/slides/week05-06.html#stability-of-hotel-prices---idea",
    "title": "Regression Analysis I",
    "section": "Stability of hotel prices - idea",
    "text": "Stability of hotel prices - idea\n\nHere we ask different questions: whether we can infer something about the price–distance pattern for situations outside the data:\n\nIs the slope coefficient close to what we have in Vienna, November, weekday:\n\nOther dates (we will do this)\nOther cities\nOther type of accommodation: apartments\n\nCompare them to our benchmark model result"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#benchmark-model",
    "href": "rm-data/slides/week05-06.html#benchmark-model",
    "title": "Regression Analysis I",
    "section": "Benchmark model",
    "text": "Benchmark model\n\nThe benchmark model is a spline with a knot at 2 miles.\nDependent variable: log price\nData is restricted to 2017, November weekday in Vienna, 3-4 star hotels, within 8 miles."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#comparing-dates",
    "href": "rm-data/slides/week05-06.html#comparing-dates",
    "title": "Regression Analysis I",
    "section": "Comparing dates",
    "text": "Comparing dates\n\nResultsDiscussion\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-NOV-weekday\n2017-NOV-weekend\n2017-DEC-holiday\n2018-JUNE-weekend\n\n\n\n\ndist_0_2\n-0.31\n-0.44\n-0.36\n-0.31\n\n\n\n(0.038)\n(0.052)\n(0.041)\n(0.037)\n\n\ndist_2_7\n0.02\n0.00\n0.07\n0.04\n\n\n\n(0.033)\n(0.036)\n(0.050)\n(0.039)\n\n\nConstant\n5.02\n5.51\n5.13\n5.16\n\n\n\n(0.042)\n(0.067)\n(0.048)\n(0.050)\n\n\nObservations\n207\n125\n189\n181\n\n\nR.squared\n0.314\n0.430\n0.382\n0.306\n\n\n\nNote: Robust standard errors in parentheses *** p&lt;0.01, ** p&lt;0.05, * p&lt;0.1\n\n\n\nNovember weekday and the June weekend: \\(\\hat{\\beta}_1 = -0.31\\)\nEstimate is similar for December (-0.36 log units)\nDifferent for the November weekend: they are 0.44 log units or 55% (exp(0.44) - 1) cheaper during the November weekend.\n\nThe corresponding 95% confidence intervals overlap somewhat: they are [-0.39,-0.23] and [-0.54,-0.34].\nThus we cannot say for sure that the price–distance patterns are different during the weekday and weekend in November."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#stability-of-hotel-prices---takeaway",
    "href": "rm-data/slides/week05-06.html#stability-of-hotel-prices---takeaway",
    "title": "Regression Analysis I",
    "section": "Stability of hotel prices - takeaway",
    "text": "Stability of hotel prices - takeaway\n\nFairly stable overtime but uncertainty is larger\nEvidence of some external validity in Vienna\nExternal validity – if model applied beyond data, there is additional uncertainty!"
  },
  {
    "objectID": "rm-data/slides/week05-06.html#take-away",
    "href": "rm-data/slides/week05-06.html#take-away",
    "title": "Regression Analysis I",
    "section": "Take-away",
    "text": "Take-away\n\nRegression Fundamentals: Regression analysis is essential for identifying relationships between variables. It’s used for both causal and predictive analysis.\nCoefficient Interpretation: In linear regression, the intercept and slope have specific meanings, indicating the expected values and changes in the dependent variable relative to the explanatory variable.\nNon-linear Relationships: Non-linear patterns can be captured using non-parametric methods and transformations like logs and splines. Depends on the goal of the analysis.\nData Challenges: Real-world data issues like measurement errors and extreme values can affect regression results. Analysts must carefully manage these challenges.\nGeneralization and Inference: Statistical inference helps generalize findings beyond the sample, but external validity requires additional data and context."
  },
  {
    "objectID": "rm-data/slides/week05-06.html#reading-homework",
    "href": "rm-data/slides/week05-06.html#reading-homework",
    "title": "Regression Analysis I",
    "section": "Reading-Homework",
    "text": "Reading-Homework\nRead Case Study as an example of a simple linear regression analysis."
  },
  {
    "objectID": "rm-data/slides/week08.html#what-we-will-see-today",
    "href": "rm-data/slides/week08.html#what-we-will-see-today",
    "title": "Modeling Probabilities",
    "section": "What we will see today",
    "text": "What we will see today\n\nLinear Probability Model - LPM\nLogit & probit\nGoodness of fit\nDiagnostics\nSummary"
  },
  {
    "objectID": "rm-data/slides/week08.html#motivation",
    "href": "rm-data/slides/week08.html#motivation",
    "title": "Modeling Probabilities",
    "section": "Motivation",
    "text": "Motivation\n\nWhat are the health benefits of not smoking? Considering the 50+ population, we can investigate if differences in smoking habits are correlated with differences in health status.\n\ngood health vs bad health"
  },
  {
    "objectID": "rm-data/slides/week08.html#binary-events",
    "href": "rm-data/slides/week08.html#binary-events",
    "title": "Modeling Probabilities",
    "section": "Binary events",
    "text": "Binary events\n\nSome outcomes are things that either happen or don’t happen, which can be captured by binary variables\n\ne.g. a person is healthy or not, a person is employed or not, a person is a smoker or not. We dont see a person that is half healthy, half employed, or half a smoker.\n\nHow can we model these events?\n\nWe have seen this before. Instead of modeling the value itself, we model the probability of the event happening.\n\n\n\\[E[y] = P[y = 1]\\]\n\nIn fact, the average of a 0–1 event is the probability of that event happening. Which can also be estimated as conditional probabilities:\n\n\\[E[y|x_1, x_2, ...] = P[y = 1|x_1, x_2, ...]\\]\n\nGood news, we can use the same tools we have been using to model these probabilities."
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-linear-probability-model",
    "href": "rm-data/slides/week08.html#lpm-linear-probability-model",
    "title": "Modeling Probabilities",
    "section": "LPM: Linear probability model",
    "text": "LPM: Linear probability model\n\nLinear Probability Model (LPM) is a linear regression with a binary dependent variable\n\nIt has the goal of modeling the probability of an event happening\n\nA linear regressions with binary dependent variables shows:\n\ndifferences in expected \\(y\\) by \\(x\\), represent diferences in probability of \\(y = 1\\) by \\(x\\).\n\nIntroduce notation for probability: \\(y^P = P[y = 1|x_1, x_2, . . .]\\)\nLinear probability model (LPM) regression is \\(y^P = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\)"
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-interpretation",
    "href": "rm-data/slides/week08.html#lpm-interpretation",
    "title": "Modeling Probabilities",
    "section": "LPM: Interpretation",
    "text": "LPM: Interpretation\n\\[y^P = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\]\n\nSo far nothing changes in terms of modelling or estimation. However, the interpretation of the coefficients changes.\n\n\\(y^P\\) denotes the probability that the dependent variable is one, conditional on the right-hand-side variables of the model.\n\\(\\beta_0\\) shows the predicted probability of \\(y\\) if all \\(x\\) are zero.\n\\(\\beta_1\\) shows the difference in the probability that \\(y = 1\\) for observations that are different in \\(x_1\\) but are the same in terms of \\(x_2\\). (ceteris paribus)"
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-modelling",
    "href": "rm-data/slides/week08.html#lpm-modelling",
    "title": "Modeling Probabilities",
    "section": "LPM: Modelling",
    "text": "LPM: Modelling\n\nLinear probability model (LPM) can be estimated using OLS. (just like linear regression)\nWe can use all transformations in \\(x\\), that we used before:\n\nLog, Polinomials, Splines, dummies, interactions, etc. They all work.\n\nAll formulae and interpretations for standard errors, confidence intervals, hypotheses and p-values of tests are the same.\nIMPORTANT Heteroskedasticity robust error are essential in this case!\n\nBy construction LPMs are heteroskedastic!\nAnd ignoring this fact will lead to biased standard errors and confidence intervals."
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-prediction",
    "href": "rm-data/slides/week08.html#lpm-prediction",
    "title": "Modeling Probabilities",
    "section": "LPM: Prediction",
    "text": "LPM: Prediction\n\nPredicted values - \\(\\hat{y}_P\\) - may be problematic. Although they are calculated the same way, they need to be interpreted as probabilities.\n\n\\[\\hat{y}^P = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2\\]\n\nPredicted values need to be between 0 and 1 because they are probabilities\nBut in LPM, predictions may be below 0 and above 1. No formal bounds in the model.\n\nmore likely than certain, less likely than impossible"
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-prediction-1",
    "href": "rm-data/slides/week08.html#lpm-prediction-1",
    "title": "Modeling Probabilities",
    "section": "LPM: Prediction",
    "text": "LPM: Prediction\n\nWhen to get worried?:\n\nWith continuous variables that can take any value (GDP, Population, sales, etc), this could be a serious issue (extrapolation)\n\nWe need to check if predictions are within the 0-1 range at least “in-sample”. But, this is not a guarantee that it will be the case “out-of-sample”.\n\nWith binary variables, no problem (‘saturated models’) (interpolation)\n\nNot problem because “simple” means will always be between 0 and 1.\n\n\nSo, a problem if goal is prediction!\nNot a big issue for inference → uncover patterns of association.\n\nBut it may give biased estimates…(in theory)"
  },
  {
    "objectID": "rm-data/slides/week08.html#cs-does-smoking-pose-a-health-risk",
    "href": "rm-data/slides/week08.html#cs-does-smoking-pose-a-health-risk",
    "title": "Modeling Probabilities",
    "section": "CS: Does smoking pose a health risk?",
    "text": "CS: Does smoking pose a health risk?\n\nThis is on of the few datasets from the book that is not directly available from their website. If interested, you need to go over the repository, and follow the instructions to access the data.\n\nThus, we will use a different dataset to illustrate the concepts."
  },
  {
    "objectID": "rm-data/slides/week08.html#cs-does-smoking-during-pregnancy-affect-birth-weight",
    "href": "rm-data/slides/week08.html#cs-does-smoking-during-pregnancy-affect-birth-weight",
    "title": "Modeling Probabilities",
    "section": "CS: Does smoking during pregnancy affect birth weight?",
    "text": "CS: Does smoking during pregnancy affect birth weight?\n\nThe question is whether, and by how much, smoking during pregnancy affects the likelihood that a baby is born with low birth weight.\nWe will use “lbw” dataset from Stata’s example datasets.\nThe dataset contains information on 189 observations of mothers and their newborns.\n\nlow is a binary variable indicating whether the baby was born with low birth weight (&lt;2500gr &lt;5.5lbs).\nsmoke is a binary variable indicating whether the mother smoked during pregnancy."
  },
  {
    "objectID": "rm-data/slides/week08.html#data",
    "href": "rm-data/slides/week08.html#data",
    "title": "Modeling Probabilities",
    "section": "Data",
    "text": "Data\n\n\\(low = 1\\) if baby was born with low birth weight\n\\(low = 0\\) if baby was born with normal weight -Some demographic information on all individual\nWe exclude women &lt;15 years old and &gt;40 years old\n\nAlso exclude women with Weight &gt; 200lbs (before pregnancy)"
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-in-stata",
    "href": "rm-data/slides/week08.html#lpm-in-stata",
    "title": "Modeling Probabilities",
    "section": "LPM: in Stata",
    "text": "LPM: in Stata\n\nStart with a simple univariate model: \\(P[low|smoke] = \\alpha + \\beta[smoke]\\)\n\n\nwebuse lbw, clear\ndrop if age &lt; 15 | age &gt; 40 | lwt &gt; 200\nreg low smoke, robust\n\n\n\n\n(Hosmer & Lemeshow data)\n(10 observations deleted)\n\nLinear regression                               Number of obs     =        179\n                                                F(1, 177)         =       4.70\n                                                Prob &gt; F          =     0.0316\n                                                R-squared         =     0.0272\n                                                Root MSE          =     .46208\n\n------------------------------------------------------------------------------\n             |               Robust\n         low | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       smoke |    .157405   .0726412     2.17   0.032     .0140507    .3007593\n       _cons |   .2568807   .0420845     6.10   0.000     .1738289    .3399326\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-interpretation-1",
    "href": "rm-data/slides/week08.html#lpm-interpretation-1",
    "title": "Modeling Probabilities",
    "section": "LPM: Interpretation",
    "text": "LPM: Interpretation\nInterpretation:\n\nThe coefficient on smoker shows the difference in the probability of a baby.\nBabies are 15.7 percentage points more likely to be born with low birth weight if the mother smoked during pregnancy.\n\nAre you comparing Apples to apples?\nLets add additional controls to capture other factors"
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-with-many-regressors-i",
    "href": "rm-data/slides/week08.html#lpm-with-many-regressors-i",
    "title": "Modeling Probabilities",
    "section": "LPM: with many regressors I",
    "text": "LPM: with many regressors I\n\nMultiple regression – closer to causality\nCompare women who are very similar in many respects but are different in smoking habits\nSmokers / non-smokers – different in many other behaviors and conditions:\n\npersonal traits (age, race)\nbehavior pre-pregnancy (Pre-pregnancy weight)\nMedical history (History of Hypertension)\nbackground for pregnancy (Number of prenatal visits, Previous premature labor)"
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-with-many-regressors-ii",
    "href": "rm-data/slides/week08.html#lpm-with-many-regressors-ii",
    "title": "Modeling Probabilities",
    "section": "LPM with many regressors II",
    "text": "LPM with many regressors II\n\nMay also consider functional form selection or interactions\nTrial and error, or theory-based\nUseful to check bivariate relationships (scatter plots, Lpoly, correlations)\n\nFor now, assume linear relationships"
  },
  {
    "objectID": "rm-data/slides/week08.html#lpm-with-many-regressors-iii",
    "href": "rm-data/slides/week08.html#lpm-with-many-regressors-iii",
    "title": "Modeling Probabilities",
    "section": "LPM with many regressors III",
    "text": "LPM with many regressors III\n\nCode\nqui {\ngen any_premature = ptl &gt;0\nren ftv no_of_visits_1tr\nren ht hist_hyper\nren lwt wgt_bef_preg\nreg low smoke age i.race any_premature hist_hyper  no_of_visits_1tr wgt_bef_preg, robust nohead\n}\nest store lpm_results\nesttab lpm_results,   se  wide nonumber ///\ncollabel(b se) md drop(1.race) nomtitle b(3) nonotes\n\n\n\n\n\nb\nse\n\n\n\n\nsmoke\n0.151*\n(0.075)\n\n\nage\n-0.005\n(0.007)\n\n\n2.race\n0.210\n(0.112)\n\n\n3.race\n0.126\n(0.079)\n\n\nany_premature\n0.275**\n(0.097)\n\n\nhist_hyper\n0.396**\n(0.143)\n\n\nno_of_visits_1tr\n-0.005\n(0.034)\n\n\nwgt_bef_preg\n-0.002\n(0.002)\n\n\n_cons\n0.464\n(0.252)\n\n\nN\n179\n\n\n\n\nRobust standard errors in parentheses * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "rm-data/slides/week08.html#detour-regression-tables",
    "href": "rm-data/slides/week08.html#detour-regression-tables",
    "title": "Modeling Probabilities",
    "section": "Detour: Regression Tables",
    "text": "Detour: Regression Tables\n\nIf need to show many explanatory variables\nDo not show table 12*2 rows, people will not see it.\n\nAvoid copy pasting from your document! Those tables are unwieldy.\n\nEither only show selected variables (smoke + 2-3 others)\nOr may need to create two columns. (a bit more work)\n\nIn my case, Wide format did the trick.\n\nMake site you have title, N of observations, footnote on SE, stars.\nSE, stars: many different notations. Check carefully.\n\nesttab default is \\(p^{***}= p&lt;0.001\\), \\(0.01\\) and \\(0.05\\)\nIn papers there is \\(p^{***}=p&lt;0.01\\), \\(0.05\\) and \\(0.1\\)."
  },
  {
    "objectID": "rm-data/slides/week08.html#does-smoking-pose-a-health-risk-for-the-baby",
    "href": "rm-data/slides/week08.html#does-smoking-pose-a-health-risk-for-the-baby",
    "title": "Modeling Probabilities",
    "section": "Does smoking pose a health risk for the baby?",
    "text": "Does smoking pose a health risk for the baby?\n\nCoefficient on smoking during pregnancy is -.151.\n\nWomen who smoked during pregnancy are 15.1 percentage points more likely to have a baby with low birth weight.\n\nThe 95% confidence interval is relatively wide \\([0.002, 0.300]\\), but it does not contain zero\nAge, Race?, Nr of Visits and Pre-pregnancy weight do not seem to be factors\nHypertension and previous premature labor are significant factors, increasing the probability of low birth weight by 40pp and 27.5pp, respectively."
  },
  {
    "objectID": "rm-data/slides/week08.html#lpms-predicted-probabilities",
    "href": "rm-data/slides/week08.html#lpms-predicted-probabilities",
    "title": "Modeling Probabilities",
    "section": "LPM’s predicted probabilities",
    "text": "LPM’s predicted probabilities\n\n\nCode\nqui: reg low smoke age i.race any_premature hist_hyper  no_of_visits_1tr wgt_bef_preg, robust nohead\nqui: predict low_hat\nqui:histogram low_hat\nsum low_hat\n\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     low_hat |        179    .3184358    .1878437  -.0346098   .9483117"
  },
  {
    "objectID": "rm-data/slides/week08.html#analysis-of-lpms-predicted-probabilities",
    "href": "rm-data/slides/week08.html#analysis-of-lpms-predicted-probabilities",
    "title": "Modeling Probabilities",
    "section": "Analysis of LPM’s predicted probabilities",
    "text": "Analysis of LPM’s predicted probabilities\n\nWhat to doWhat we find\n\n\n\nDrill down in distribution:\n\nLooking at the composition of people: top vs bottom part of probability distribution\nLook at average values of covariates for top and bottom X% of predicted probabilities!\n\n\n\n\n\n\nCode\nsort low_hat\nset linesize 255\n \nqui: gen flag = 1 if _n&lt;=5\nqui: replace  flag = 2 if _n&gt;=_N-4\nlist low_hat smoke age race any_premature hist_hyper  no_of_visits_1tr wgt_bef_preg   if flag==1\nlist low_hat smoke age race any_premature hist_hyper  no_of_visits_1tr wgt_bef_preg   if flag==2\n\n\n\n     +---------------------------------------------------------------------------------+\n     |   low_hat       smoke   age    race   any_pr~e   hist_h~r   no_of_~r   wgt_be~g |\n     |---------------------------------------------------------------------------------|\n  1. | -.0346098   Nonsmoker    32   White          0          0          2        186 |\n  2. | -.0280454   Nonsmoker    36   White          0          0          0        175 |\n  3. |  .0019138   Nonsmoker    32   White          0          0          0        170 |\n  4. |  .0158307   Nonsmoker    23   White          0          0          0        190 |\n  5. |  .0284496   Nonsmoker    28   White          0          0          0        167 |\n     +---------------------------------------------------------------------------------+\n\n     +--------------------------------------------------------------------------------+\n     |  low_hat       smoke   age    race   any_pr~e   hist_h~r   no_of_~r   wgt_be~g |\n     |--------------------------------------------------------------------------------|\n175. | .7197446      Smoker    34   Black          0          1          0        187 |\n176. | .7369648   Nonsmoker    17   Black          0          1          0        142 |\n177. | .8160616      Smoker    18   Black          1          0          0        110 |\n178. | .8545191   Nonsmoker    26   Other          1          1          1        154 |\n179. | .9483117   Nonsmoker    25   Other          1          1          0        105 |\n     +--------------------------------------------------------------------------------+"
  },
  {
    "objectID": "rm-data/slides/week08.html#probability-models-logit-and-probit",
    "href": "rm-data/slides/week08.html#probability-models-logit-and-probit",
    "title": "Modeling Probabilities",
    "section": "Probability models: logit and probit",
    "text": "Probability models: logit and probit\n\nPrediction: predicted probability need to be between 0 and 1\n\nThus, for prediction, we must use non-linear models\nActually, its a quasi-linear model.\n\nThe model, itself, is linear in the parameters\nbut need to relate this to the probability of the \\(y = 1\\) event, using a nonlinear function that maps the linear index into a 0-1 range: ‘Link function’\n\n\\[\\begin{aligned}\nXB &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... \\\\\ny^P &= F(XB) \\rightarrow y^P \\in (0,1)\n\\end{aligned}\n\\]\n\nTwo options: Logit and probit – different link function"
  },
  {
    "objectID": "rm-data/slides/week08.html#link-functions-i.",
    "href": "rm-data/slides/week08.html#link-functions-i.",
    "title": "Modeling Probabilities",
    "section": "Link functions I.",
    "text": "Link functions I.\nCall \\(XB = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...\\)\n\nThe Logit:\n\\(y^P = \\Lambda(XB) = \\frac{\\exp(XB)}{1 + \\exp(XB)}\\)\nThe probit:\n\\(y^P = \\Phi(XB) \\rightarrow \\Phi(z) = \\int_{-\\infty}^z \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz\\)\n\nwhere \\(\\Lambda()\\) is called logistic function, and \\(\\Phi()\\) is the cumulative distribution function (CDF) of the standard normal distribution."
  },
  {
    "objectID": "rm-data/slides/week08.html#link-functions-ii.",
    "href": "rm-data/slides/week08.html#link-functions-ii.",
    "title": "Modeling Probabilities",
    "section": "Link functions II.",
    "text": "Link functions II.\n\n\n\nBoth link functions are S-shaped curves bounded between 0 and 1.\nThere is but a small difference between the two.\nbut estimated coefficients will be different.\n\n\n\n\nCode\nqui {\nclear\nrange p 0 1 202\ndrop if p==0 | p==1 \ngen x = invnormal(p)\ngen y = (x+rnormal())&gt;0\nreg  y x\npredict y_1\nlogit y x\npredict y_2\nprobit y x\npredict y_3\ndrop if abs(x)&gt;2\ntwo (line y_1 x ) (line y_2 x) (line y_3 x), ///\nlegend(order(1 \"LPM\" 2 \"Logit\" 3 \"Probit\") pos(3) ring(0) col(1)) \n}"
  },
  {
    "objectID": "rm-data/slides/week08.html#logit-and-probit-interpretation",
    "href": "rm-data/slides/week08.html#logit-and-probit-interpretation",
    "title": "Modeling Probabilities",
    "section": "Logit and probit interpretation",
    "text": "Logit and probit interpretation\n\nBoth the probit and the logit transform the \\(\\beta_0 + \\beta_1 x_1 + ...\\) linear combination using a link function that shows an S-shaped curve.\nThe slope of this curve keeps changing as we change whatever is inside, but it’s steepest when \\(y^P = 0.5\\) (inflection point)\nThe difference in \\(y^P\\) corresponds to changes in probabilities, between any two values of \\(x\\).\nTo find how much is related to a particular \\(x\\), You need to take the partial derivatives.\nImportant consequence: no direct interpretation of the raw coefficient values!\n\nThus, always know if you are interpreting the raw coefficients or the marginal differences."
  },
  {
    "objectID": "rm-data/slides/week08.html#marginal-differences-marginal-effects",
    "href": "rm-data/slides/week08.html#marginal-differences-marginal-effects",
    "title": "Modeling Probabilities",
    "section": "Marginal differences (marginal effects)",
    "text": "Marginal differences (marginal effects)\n\nNOTE As before, the word “effect” should be used with caution. In the book, they use “marginal differences” instead. as a more neutral term.\n\n\nLink functions makes associates \\(\\Delta x\\) into \\(\\Delta y_P\\). we do not interpret raw coefficients! (except for direction)\nInstead, transform them into ‘marginal differences’ for interpretation purposes\nThey are also called ‘marginal effects’ or ‘average marginal effects (AME)’ or ‘average partial effects’.\nAverage marginal difference has the same interpretation as the coefficient of linear probability models, but with caveats."
  },
  {
    "objectID": "rm-data/slides/week08.html#marginal-differences-discrete-x",
    "href": "rm-data/slides/week08.html#marginal-differences-discrete-x",
    "title": "Modeling Probabilities",
    "section": "Marginal differences: Discrete \\(x\\)",
    "text": "Marginal differences: Discrete \\(x\\)\n\nif \\(x\\) is a categorical (0-1), the marginal difference is the difference in the predicted probability of \\(y = 1\\), that corresponds to a change from \\(x = 0\\) to \\(x = 1\\).\n\n\\[\\Delta y^P = y^P(x = 1) - y^P(x = 0)\\]\n\nThen we simply “average” this difference across all observations."
  },
  {
    "objectID": "rm-data/slides/week08.html#marginal-differences-continous-x",
    "href": "rm-data/slides/week08.html#marginal-differences-continous-x",
    "title": "Modeling Probabilities",
    "section": "Marginal differences: continous \\(x\\)",
    "text": "Marginal differences: continous \\(x\\)\n\nIf \\(x\\) is continuous, the marginal difference is calculated as the derivative (for a small change in \\(x\\)).\n\n\\[\\frac{\\partial y^P}{\\partial x_1} = \\beta_1 \\cdot f(XB)\\]\n\nWhich is then averaged across all observations to report a single number.\nIn practice, we interpret this as the change in the probability of \\(y = 1\\) for a one-unit change in \\(x_1\\)."
  },
  {
    "objectID": "rm-data/slides/week08.html#how-to-estimate-this-models",
    "href": "rm-data/slides/week08.html#how-to-estimate-this-models",
    "title": "Modeling Probabilities",
    "section": "How to estimate this models?",
    "text": "How to estimate this models?\nMaximum likelihood estimation!\n\nWhen estimating a logit or probit model, we use ‘maximum likelihood’ estimation.\n\nSee 11.U2 for details.\n\nIdea for maximum likelihood is another way to get coefficient estimates.\n\n1st You specify a (conditional) distribution, that you will use during the estimation.\n\nThis is logistic for logit and normal for probit model.\n\n2nd You maximize this function w.r.t. your \\(\\beta\\) parameters → gives the maximum likelihood for this model.\n\nDifferent from OLS: No closed form solution → need to use search algorithms.\n\nThus… more computationally intensive."
  },
  {
    "objectID": "rm-data/slides/week08.html#predictions-for-lmp-logit-and-probit-i.",
    "href": "rm-data/slides/week08.html#predictions-for-lmp-logit-and-probit-i.",
    "title": "Modeling Probabilities",
    "section": "Predictions for LMP, Logit and Probit I.",
    "text": "Predictions for LMP, Logit and Probit I.\n\n\nCode\nqui {\n  webuse lbw, clear\n  drop if age &lt; 15 | age &gt; 40 | lwt &gt; 200\n  gen any_premature = ptl &gt;0\n  ren ftv no_of_visits_1tr\n  ren ht hist_hyper\n  ren lwt wgt_bef_preg\n  gen black = 2.race\n  gen other = 3.race\n  reg low smoke age i.black i.other i.any_premature hist_hyper , robust nohead\n  predict low_hat_ols\n  est sto lpm_results\n  logit low smoke age black other any_premature hist_hyper , robust nohead\n  predict low_hat_logit\n   probit low smoke age black other any_premature hist_hyper , robust nohead\n  predict low_hat_probit\n   two (scatter  low_hat_logit low_hat_probit low_hat_ols) ///\n      (line low_hat_ols low_hat_ols, sort), ///\n      legend(order(1 \"Logit\" 2 \"Probit\" 3 \"LPM\") pos(3) ring(0) col(1))\n}"
  },
  {
    "objectID": "rm-data/slides/week08.html#coefficient-results-for-logit-and-probit",
    "href": "rm-data/slides/week08.html#coefficient-results-for-logit-and-probit",
    "title": "Modeling Probabilities",
    "section": "Coefficient results for logit and probit",
    "text": "Coefficient results for logit and probit\n\nCode\nqui {\n  logit low smoke age i.black i.other i.any_premature hist_hyper , robust nohead\n  est sto logit_results\n  margins, dydx(*) post\n  est sto logit_mfx\n  probit low smoke age i.black i.other i.any_premature hist_hyper , robust nohead  \n  est sto probit_results\n  margins, dydx(*) post\n  est sto probit_mfx\n}\nesttab lpm_results logit_results  logit_mfx probit_results probit_mfx, se  nonumber drop(0.*) ///\nmtitle(LPM Logit Logit_MFX Probit Probit_MFX) collabel(none) md ///\nstar(* 0.10 ** 0.05 *** 0.01) nonotes \n\n\n\n\n\n\n\n\n\n\n\n\n\nLPM\nLogit\nLogit_MFX\nProbit\nProbit_MFX\n\n\n\n\nmain\n\n\n\n\n\n\n\nsmoke\n0.166**\n0.925**\n0.169**\n0.549**\n0.169**\n\n\n\n(0.0739)\n(0.397)\n(0.0695)\n(0.235)\n(0.0697)\n\n\nage\n-0.00703\n-0.0447\n-0.00818\n-0.0271\n-0.00835\n\n\n\n(0.00669)\n(0.0377)\n(0.00688)\n(0.0221)\n(0.00679)\n\n\n1.black\n0.192*\n1.012*\n0.202*\n0.607*\n0.202*\n\n\n\n(0.111)\n(0.526)\n(0.109)\n(0.314)\n(0.108)\n\n\n1.other\n0.150*\n0.884**\n0.164**\n0.524**\n0.163**\n\n\n\n(0.0770)\n(0.435)\n(0.0787)\n(0.254)\n(0.0779)\n\n\n1.any_premature\n0.284***\n1.330***\n0.279***\n0.810***\n0.280***\n\n\n\n(0.0972)\n(0.441)\n(0.0944)\n(0.270)\n(0.0956)\n\n\nhist_hyper\n0.370***\n1.720**\n0.315***\n1.063**\n0.327***\n\n\n\n(0.137)\n(0.674)\n(0.117)\n(0.415)\n(0.122)\n\n\n_cons\n0.270\n-0.971\n\n-0.576\n\n\n\n\n(0.183)\n(0.983)\n\n(0.577)\n\n\n\nN\n179\n179\n179\n179\n179\n\n\n\nStandard errors in parentheses * p &lt; 0.1, ** p &lt; 0.04, *** p &lt; 0.01"
  },
  {
    "objectID": "rm-data/slides/week08.html#does-smoking-pose-a-health-risk-logit-and-probit",
    "href": "rm-data/slides/week08.html#does-smoking-pose-a-health-risk-logit-and-probit",
    "title": "Modeling Probabilities",
    "section": "Does smoking pose a health risk?– logit and probit",
    "text": "Does smoking pose a health risk?– logit and probit\n\nLPM – interpret the coefficients as usual.\nLogit, probit - Interpret the marginal differences. Basically the same.\n\nMarginal differences are essentially the same across the logit and the probit.\nEssentially the same as the corresponding LPM coefficients.\n\nHappens often:\n\nOften LPM is good enough for interpretation.\nCheck if logit/probit very different.\n\nif so, Investigate functional forms if yes."
  },
  {
    "objectID": "rm-data/slides/week08.html#goodness-of-fit",
    "href": "rm-data/slides/week08.html#goodness-of-fit",
    "title": "Modeling Probabilities",
    "section": "Goodness of fit",
    "text": "Goodness of fit\n\nThere is no generally accepted goodness of fit measure\n\nThis is because we do not observe probabilities only 1 and 0, so we cannot FIT those probabilities.\n\nThere are, however, other options to evaluate the quality of the model.\n\nR-squared\nBrier score\nPseudo R-squared\nlog-loss"
  },
  {
    "objectID": "rm-data/slides/week08.html#goodness-of-fit-r-squared",
    "href": "rm-data/slides/week08.html#goodness-of-fit-r-squared",
    "title": "Modeling Probabilities",
    "section": "Goodness of fit: R-squared",
    "text": "Goodness of fit: R-squared\n\\[R^2 = 1 - \\frac{\\sum_{i=1}^n (\\hat{y}_P^i - y_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\\]\n\nR-squared is not useful for binary outcomes\n\nIt can be calculated, but it lacks the interpretation we had for linear models, because we are fitting the probabilities, not the outcomes."
  },
  {
    "objectID": "rm-data/slides/week08.html#brier-score",
    "href": "rm-data/slides/week08.html#brier-score",
    "title": "Modeling Probabilities",
    "section": "Brier score",
    "text": "Brier score\n\\[\\text{Brier} = \\frac{1}{n}\\sum_{i=1}^n (\\hat{y}_P^i - y_i)^2\\]\n\nThe Brier score is the average distance (mean squared difference) between predicted probabilities and the actual value of \\(y\\).\nSmaller the Brier score, the better.\nWhen comparing two predictions, the one with the smaller Brier score is the better prediction because it produces less (squared) error on average.\nRelated to a main concept in prediction: mean squared error (MSE)"
  },
  {
    "objectID": "rm-data/slides/week08.html#pseudo-r2",
    "href": "rm-data/slides/week08.html#pseudo-r2",
    "title": "Modeling Probabilities",
    "section": "Pseudo R2",
    "text": "Pseudo R2\n\\[pR^2 = 1 - \\frac{\\text{Log-likelihood}_{\\text{model}}}{\\text{Log-likelihood}_{\\text{intercept only}}}\\]\n\nIt is similar to the R-squared, as it measures the goodness of fit, tailored to nonlinear models and binary outcomes.\n\nMost widely used: McFadden’s R-squared (Stata uses this)\n\nComputes the ratio of log-likelihood of the model vs intercept only.\nCan be computed for the logit and the probit but not for the linear probability model. (unless you re-define the Log-likelihood)"
  },
  {
    "objectID": "rm-data/slides/week08.html#log-loss",
    "href": "rm-data/slides/week08.html#log-loss",
    "title": "Modeling Probabilities",
    "section": "Log-loss",
    "text": "Log-loss\n\\[\\text{Log-loss} = \\frac{1}{n}\\sum_{i=1}^n \\left[y_i \\log(\\hat{y}_P^i) + (1-y_i) \\log(1-\\hat{y}_P^i) \\right]\\]\n\nThe log-loss is a measured derived from the log-likelihood function. It measures how much observed data dissagrees with the predicted probabilities.\nThe smaller (close to zero) the log-loss, the better the model."
  },
  {
    "objectID": "rm-data/slides/week08.html#practical-use",
    "href": "rm-data/slides/week08.html#practical-use",
    "title": "Modeling Probabilities",
    "section": "Practical use",
    "text": "Practical use\n\nThere are several measured of model fit, but they often give the same ranking of models.\nDo not use R-squared. Even for LPM, it has no interpretation.\nIf using probit vs logit: pseudo R-squared may be used to rank logit and probit models.\nUse, especially for prediction: Brier score is a metric that can be computed for all models and is used in prediction."
  },
  {
    "objectID": "rm-data/slides/week08.html#bias-of-the-predictions",
    "href": "rm-data/slides/week08.html#bias-of-the-predictions",
    "title": "Modeling Probabilities",
    "section": "Bias of the predictions",
    "text": "Bias of the predictions\n\nPost-prediction: we may be interested to study some features of our model\nOne specific goal: evaluating the bias of the prediction.\n\nProbability predictions are unbiased if they are right on average = the average of predicted probabilities is equal to the actual probability of the outcome.\nIf the prediction is unbiased, the bias is zero.\n\nUnless the model is really bad, unconditional bias is not a big issue.\n\nOnly Probit will be biased."
  },
  {
    "objectID": "rm-data/slides/week08.html#calibration",
    "href": "rm-data/slides/week08.html#calibration",
    "title": "Modeling Probabilities",
    "section": "Calibration",
    "text": "Calibration\n\nUnbiasedness refers to the whole distribution of probability predictions.\nA finer and stricter concept is calibration\nA prediction is well calibrated if the actual probability of the outcome is equal to the predicted probability for each and every value of the predicted probability. \\[E(y|y^P) = y^P\\]\n‘Calibration curve’ is used to show this.\nA model may be unbiased (right on average) but not well calibrated\n\nunderestimate high probability events and overestimate low probability ones"
  },
  {
    "objectID": "rm-data/slides/week08.html#calibration-curve",
    "href": "rm-data/slides/week08.html#calibration-curve",
    "title": "Modeling Probabilities",
    "section": "Calibration curve",
    "text": "Calibration curve\n\nHorizontal axis shows the values of all predicted probabilities (\\(\\hat{y}_P\\)).\nVertical axis shows the fraction of \\(y = 1\\) observations for all observations with the corresponding predicted probability.\nThe closer the curve is to the 45-degree line, the better the calibration.\n\nIn practice:\n\nCreate bins for predicted probabilities and make comparisons of the actual event’s probability.\nOr use non-parametric methods to estimate the calibration curve."
  },
  {
    "objectID": "rm-data/slides/week08.html#calibration-curve-1",
    "href": "rm-data/slides/week08.html#calibration-curve-1",
    "title": "Modeling Probabilities",
    "section": "Calibration curve",
    "text": "Calibration curve\n\n\nCode\ntwo (lpoly low low_hat_ols) (lpoly low low_hat_logit) ///\n(lpoly low low_hat_probit) (function y = x, range(0 1) lcolor(black%50)), ///\nlegend(order(1 \"LPM\" 2 \"Logit\" 3 \"Probit\") ) scale(1.5) ///\nxtitle(\"Predicted probability\") ytitle(\"Avg outcome\")"
  },
  {
    "objectID": "rm-data/slides/week08.html#probability-models-summary",
    "href": "rm-data/slides/week08.html#probability-models-summary",
    "title": "Modeling Probabilities",
    "section": "Probability models summary",
    "text": "Probability models summary\n\nFind patterns when \\(y\\) is binary can be done using model probability with regressions\nLinear probability model is mostly good enough, easy inference.\n\nto-go for data exploration, quick analysis, and diagnostics\nbut, predicted values could be below 0, above 1\n\nLogit (and probit) - better when aim is prediction, predicted values strictly between 0-1\nMost often, LPM, logit, probit - similar inference\nUse marginal (average) differences (with logit/probit)\nNo trivial goodness of fit. Brier score or pseudo-R-Squared.\nCalibration is important for prediction."
  },
  {
    "objectID": "rm-data/slides/week10.html#motivation",
    "href": "rm-data/slides/week10.html#motivation",
    "title": "Prediction Setup",
    "section": "Motivation",
    "text": "Motivation\n\nImagine you want to sell your car soon and need to predict its price. You have data on similar used cars, and several regression models could help estimate its value now and in a year. How do you choose the best model?\nOr, take an ice cream shop—using past sales and temperature data, you want to predict sales for the coming days. What factors should you consider to ensure your prediction is as accurate as possible?"
  },
  {
    "objectID": "rm-data/slides/week10.html#prediction-basics-the-process",
    "href": "rm-data/slides/week10.html#prediction-basics-the-process",
    "title": "Prediction Setup",
    "section": "Prediction Basics: The Process",
    "text": "Prediction Basics: The Process\n\nThe idea of behind prediction is that we want to assign a value of \\(y\\)(\\(\\hat y\\)) (target or outcome) for observations we do not have the value.\nWhat we have, in our original data or working sample, is a set of observations with \\(y\\) and \\(x\\) values. We use them to “learn” the patterns of association between \\(y\\) and \\(x\\).\nBut for the target observations, we only have \\(x\\) values."
  },
  {
    "objectID": "rm-data/slides/week10.html#prediction-basics-the-process-1",
    "href": "rm-data/slides/week10.html#prediction-basics-the-process-1",
    "title": "Prediction Setup",
    "section": "Prediction Basics: The Process",
    "text": "Prediction Basics: The Process\n\nThe process of prediction involves:\n\nDetermine what is the target variable \\(y\\) and the features \\(x\\).\nUsing the original data to estimate a model that describes the patterns of association between \\(y\\) and \\(x\\).\nUsing the estimated model to predict the value of \\(y\\) for the target observations.\n\nEvaluation of the prediction (how actual data compares to predicted data)\nRinse and repeat: Choose a different model, evaluate, choose the best model.\n\nBest model for the Live data not the original data."
  },
  {
    "objectID": "rm-data/slides/week10.html#cs-price-cars",
    "href": "rm-data/slides/week10.html#cs-price-cars",
    "title": "Prediction Setup",
    "section": "CS: Price cars",
    "text": "CS: Price cars\n\nYou want to sell your car through online advertising\nTarget is continuous (in dollars)\nFeatures are continuous or categorical\nThe business question:\n\nWhat price should you put into the ad?"
  },
  {
    "objectID": "rm-data/slides/week10.html#cs-price-apartments",
    "href": "rm-data/slides/week10.html#cs-price-apartments",
    "title": "Prediction Setup",
    "section": "CS: Price apartments",
    "text": "CS: Price apartments\n\nYou are planning to run an AirBnB business\nTarget is continuous (in dollars)\nFeatures are varied from text to binary\nThe business question:\n\nHow should you price apartments/houses?"
  },
  {
    "objectID": "rm-data/slides/week10.html#cs-predict-companys-exit-from-business",
    "href": "rm-data/slides/week10.html#cs-predict-companys-exit-from-business",
    "title": "Prediction Setup",
    "section": "CS: Predict company’s exit from business",
    "text": "CS: Predict company’s exit from business\n\nYou have a consulting company\nPredict which firms will go out of business (exit) from a pool of partners\nTarget is binary: exit / stay\nFeatures of financial and management info\nBusiness decision:\n\nWhich firms to give loan to?"
  },
  {
    "objectID": "rm-data/slides/week10.html#predictive-analysis-what-is-new",
    "href": "rm-data/slides/week10.html#predictive-analysis-what-is-new",
    "title": "Prediction Setup",
    "section": "Predictive Analysis: what is new?",
    "text": "Predictive Analysis: what is new?\n\nMost of econometrics focused on finding relationships between \\(X\\) and \\(Y\\)\n\nWhat is the relationship like (+/-, linear, etc.)\nIs it a robust relationship – true in the population /general pattern? (causal?)\n\nNow, we use \\(x_1, x_2, \\dots\\) to predict \\(y\\): \\(\\hat{y}_j = \\hat{f}(x_j)\\), How is this different?\n\n\n\nWe care less about\n\nIndividual coefficient values, multicollinearity\n\nWe still care about the stability of our results.\nBut, should we care about causality?\n\nNot so much, we care more about making the best prediction."
  },
  {
    "objectID": "rm-data/slides/week10.html#what-are-we-predicting",
    "href": "rm-data/slides/week10.html#what-are-we-predicting",
    "title": "Prediction Setup",
    "section": "What are we predicting?",
    "text": "What are we predicting?\n\n\\(Y\\) is quantitative (e.g price)\n\nQuantitative prediction. Expected price of a car given its characteristics.\nits a “Regression” problem\nWe could obtain a point prediction or an interval prediction (As before)\n\n\\(Y\\) is binary (e.g. Default or nor)\n\n\\(Y\\) takes values in a finite set of (unordered) classes (survived/died, sold/not sold, car model)\nWe may want to types of predictions: Probability prediction (\\(\\hat P(y|x=1)\\)) or classification (\\(\\hat y = 1 \\text{ or }0| x\\))\n\nTime series prediction (Forecasting).\n\nMake predictions about the future based on historical and current data.\n\\(\\hat y_t = f(x_{t-1}, x_{t-2}, \\dots)\\)"
  },
  {
    "objectID": "rm-data/slides/week10.html#what-is-different",
    "href": "rm-data/slides/week10.html#what-is-different",
    "title": "Prediction Setup",
    "section": "What is Different?",
    "text": "What is Different?\n\nIn principle, the process is the same as before, but we have a diffent goal.\n\nWe need to pay more attention to other aspects of the process.\n\nLabel Engeneering: What transformation is suitable for the target variable?\nFeature engineering (variable selection): What variables and functional forms should be included\nModel estimation and prediction, based on variable selection\n\nDecisions regarding model complexity and Specification (Know-How or Machine Learning)\n\nModel evaluation considering complexity and loss\nKey idea: Focus on systematically combine estimation and model selection"
  },
  {
    "objectID": "rm-data/slides/week10.html#the-tool-supervised-machine-learning",
    "href": "rm-data/slides/week10.html#the-tool-supervised-machine-learning",
    "title": "Prediction Setup",
    "section": "The Tool: Supervised Machine Learning",
    "text": "The Tool: Supervised Machine Learning\n\nSupervised Machine Learning is a set of tools that help us predict the value of a target variable \\(Y\\) based on a set of features \\(X\\).\nWe will use one of the oldest and most commonly used method:\n\n\n\nLinear Regression"
  },
  {
    "objectID": "rm-data/slides/week10.html#the-prediction-error",
    "href": "rm-data/slides/week10.html#the-prediction-error",
    "title": "Prediction Setup",
    "section": "The Prediction Error",
    "text": "The Prediction Error\n\nWe have seen this.\nThe Regression model can produce a Predicted value \\(\\hat{y}_j\\) for target observation \\(j\\)\nbut, actual value \\(y_j\\) is not known (that is why we are predicting)\nThus, there will be a prediction error\n\n\\[e_j = \\hat{y}_j - y_j\n\\]\n\nError = actual value - predicted value"
  },
  {
    "objectID": "rm-data/slides/week10.html#the-prediction-error-1",
    "href": "rm-data/slides/week10.html#the-prediction-error-1",
    "title": "Prediction Setup",
    "section": "The Prediction Error",
    "text": "The Prediction Error\n\nThe ideal prediction error, is zero: our predicted value is right on target.\n\nRare…\n\nThe prediction error carries information: direction and size.\nDirection of miss:\n\nPositive if we overpredict\nNegative if we underpredict\nDegree of wrongness depends on the decision problem. (price is right?)\n\nSize:\n\nmagnitude of the error may depend on the nature of the problem and the loss function."
  },
  {
    "objectID": "rm-data/slides/week10.html#decomposing-the-prediction-error",
    "href": "rm-data/slides/week10.html#decomposing-the-prediction-error",
    "title": "Prediction Setup",
    "section": "Decomposing the prediction error",
    "text": "Decomposing the prediction error\nAssume the best model for \\(Y\\) is \\(Y= f(X,Z)+\\epsilon\\), but we estimate \\(Y^E = g(X,Z)\\), and obtain \\(\\hat g(X,Z)\\)\n\nThe prediction error can be decomposed into three parts:\n\nestimation error: Difference between \\(g(X)\\) and \\(\\hat g(X)\\)\nmodel error: Difference between \\(f(X)\\) and \\(g(X)\\).\ngenuine error: error that cant be eliminated even if have the best possible model. \\(\\epsilon\\)"
  },
  {
    "objectID": "rm-data/slides/week10.html#interval-prediction-for-quantitative-target-variables",
    "href": "rm-data/slides/week10.html#interval-prediction-for-quantitative-target-variables",
    "title": "Prediction Setup",
    "section": "Interval prediction for quantitative target variables",
    "text": "Interval prediction for quantitative target variables\n\nOne advantage of regressions - it’s easy quantify uncertainty of prediction\n\nThis can be used to obtain Interval predictions\n\nInterval predictions quantify 2-out-of-3 sources of prediction uncertainty: estimation error and genuine (or irreducible) error.\nThey do not include the third source, model uncertainty! (Bayesian methods can help with this)\nThe 95% prediction interval (PI) tells where to expect the actual value for the target observation.\nThe PI for linear regression requires homoskedasticity. (but could be relaxed)"
  },
  {
    "objectID": "rm-data/slides/week10.html#loss-functions",
    "href": "rm-data/slides/week10.html#loss-functions",
    "title": "Prediction Setup",
    "section": "Loss Functions",
    "text": "Loss Functions\n\nWe use a Loss function to quantify the cost of prediction error\n\nIt attaches a value to the prediction error, specifying how bad it is\nThus, Loss function determines best predictor\n\nIdeally, it is derived from decision problem,\n\nHow much more costly is to overpredict than underpredict?\n\nIn practice, highly crafted loss functions are rare (Machine learning, Neural Networks, Etc), so we use common ones\nLoss functions could be used to both estimate, but also to evaluate/compare models\nPlot Twist: Loss function for OLS is the L2 Square loss function"
  },
  {
    "objectID": "rm-data/slides/week10.html#loss-functions-1",
    "href": "rm-data/slides/week10.html#loss-functions-1",
    "title": "Prediction Setup",
    "section": "Loss Functions",
    "text": "Loss Functions\n\nThe most important Loss functions have the following characteristics:\n\nSymmetry: losses due to errors in opposing direction are similar\n\nAsymmetric loss: overprediction is more costly than underprediction\n\nConvexity: Twice as large errors generate more than twice as large losses. (We penalize large errors more than small ones)\n\nLinear loss: Errors are penalized proportionally to their size"
  },
  {
    "objectID": "rm-data/slides/week10.html#loss-functions-of-various-shapes",
    "href": "rm-data/slides/week10.html#loss-functions-of-various-shapes",
    "title": "Prediction Setup",
    "section": "Loss Functions of Various Shapes",
    "text": "Loss Functions of Various Shapes\n\n\nCode\nqui {\nclear\nset scheme white2\ncolor_style tableau\nset obs 301\nrange r -5 5 \ngen ll = r^2\ngen ll2 = 2*abs(r)\ngen ll3 = 1.5*r^2*(r&gt;0)+0.5*r^2*(r&lt;0)\ndrop if ll&gt;30 | ll2&gt;30 | ll3&gt;30\nline ll ll2 ll3 r, lw(1 1 1) ///\n    legend(order(1 \"Symetric-Convex\" 2 \"Symetric-Linear\" 3 \"Asymetric-Convex\") )\n}"
  },
  {
    "objectID": "rm-data/slides/week10.html#examples-1-used-cars",
    "href": "rm-data/slides/week10.html#examples-1-used-cars",
    "title": "Prediction Setup",
    "section": "Examples 1 – used cars",
    "text": "Examples 1 – used cars\n\nThe loss function for predicting the value of our used car depends on how we value money and how we value how much time it takes to sell our car (value of your car).\nA too low prediction may lead to selling our car cheap but fast;\nA too high prediction may make us wait a long time and, possibly, revising the sales price downwards before selling our car.\nWhat kind of loss function would make sense?"
  },
  {
    "objectID": "rm-data/slides/week10.html#examples-2---creditors",
    "href": "rm-data/slides/week10.html#examples-2---creditors",
    "title": "Prediction Setup",
    "section": "Examples 2 - creditors",
    "text": "Examples 2 - creditors\n\nCreditors decide whether to issue a loan only to potential debtors that are predicted to pay it back with high likelihood.\nTwo kinds of errors are possible:\n\ndebtors that would pay back their loan don’t get a loan\ndebtors that would not pay back their loan get one nevertheless.\n\nThe costs of the first error are due to missed business opportunity; the costs of the second error are due to direct loss of money.\nThese losses may be quantified in relatively straightforward ways.\nWhat kind of loss function would make sense?"
  },
  {
    "objectID": "rm-data/slides/week10.html#common-loss-functions",
    "href": "rm-data/slides/week10.html#common-loss-functions",
    "title": "Prediction Setup",
    "section": "Common loss functions",
    "text": "Common loss functions\n\n\n\\[SQR: L(e_j) = e^2_j = (\\hat{y}_j - y_j)^2\n\\]\n\nThe most widely used loss function\nSymmetric: Losses due to errors in opposing direction are same\nConvex: Twice as large errors generate more than twice (4x) as large losses\n\n\n\\[ABS: L(e_j) = |e_j| = Abs(\\hat{y}_j - y_j)\n\\]\n\nUsed for Median regression (Quantile regression)\nSymmetric: Losses due to errors in opposing direction are same\nLinear: Twice as large errors generate twice as large losses\nQuantile Regressions use Asymetric loss functions"
  },
  {
    "objectID": "rm-data/slides/week10.html#mean-squared-error-mse",
    "href": "rm-data/slides/week10.html#mean-squared-error-mse",
    "title": "Prediction Setup",
    "section": "Mean Squared Error: MSE",
    "text": "Mean Squared Error: MSE\n\nThe most common way to quantify and aggregate the loss function is using the Mean Squared Error (MSE)\nSquared loss \\(\\rightarrow\\) Mean Squared Error (MSE)\n\n\\[\\begin{align*}\n\\text{MSE} &= \\frac{1}{K} \\sum_{k=1}^K (\\hat{y}_k - y_k)^2 \\\\\n\\text{RMSE} &= \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{K} \\sum_{k=1}^K (\\hat{y}_k - y_k)^2}\n\\end{align*}\n\\]\n\nUsing this function typically implies we are interested in the Mean as the best predictor"
  },
  {
    "objectID": "rm-data/slides/week10.html#mse-decomposition-bias-and-variance",
    "href": "rm-data/slides/week10.html#mse-decomposition-bias-and-variance",
    "title": "Prediction Setup",
    "section": "MSE decomposition : Bias and Variance",
    "text": "MSE decomposition : Bias and Variance\n\nThe MSE can be decomposed into two parts: Bias and Variance \\[\\begin{align*}\nMSE &= \\frac{1}{J}\\sum_{j=1}^J (\\hat{y}_j - y_j)^2 \\\\\n&= \\left(\\frac{1}{J}\\sum_{j=1}^J (\\hat{y}_j - y_j)\\right)^2 + \\frac{1}{J}\\sum_{j=1}^J (\\hat y_j - \\bar{\\hat y})^2 \\\\\n&= \\text{Bias}^2 + \\text{PredictionVariance}\n\\end{align*}\n\\]\nThe bias of a prediction is the average of its prediction error.\n\nHow far off is the average prediction from the actual value?\n\nThe variance of a prediction describes shows how it varies around its average."
  },
  {
    "objectID": "rm-data/slides/week10.html#mse-decomposition-bias-and-variance-1",
    "href": "rm-data/slides/week10.html#mse-decomposition-bias-and-variance-1",
    "title": "Prediction Setup",
    "section": "MSE decomposition : Bias and Variance",
    "text": "MSE decomposition : Bias and Variance\n\\[\\begin{align*}\n\\text{MSE} &= \\frac{1}{K} \\sum_{k=1}^K (\\hat{y}_k - y_k)^2 \\\\\n         &= \\left(\\frac{1}{K} \\sum_{k=1}^K (\\hat{y}_k - \\bar{y})\\right)^2 + \\frac{1}{K} \\sum_{k=1}^K (y_k - \\bar{y})^2 \\\\\n         &= \\text{Bias}^2 + \\text{PredictionVariance}\n\\end{align*}\\]\n\nOLS is unbiased. Some other methods will allow for some bias in return for lower variance."
  },
  {
    "objectID": "rm-data/slides/week10.html#model-selection",
    "href": "rm-data/slides/week10.html#model-selection",
    "title": "Prediction Setup",
    "section": "Model selection",
    "text": "Model selection\n\nModel selection is finding the best fit while avoiding overfitting, and aiming for high external validity.\nTo do this, we aim to choose a model that is flexible enough to capture the patterns in the data but not too flexible to capture noise.\n\nBias-Variance tradeoff\nBalancing the complexity of the model\n\nConsider two models. They could be different for two reasons:\n\ndifferent functional forms (spline vs. quadratic)\ndifferent number of variables (simple vs. complex)"
  },
  {
    "objectID": "rm-data/slides/week10.html#how-to-choose-a-model",
    "href": "rm-data/slides/week10.html#how-to-choose-a-model",
    "title": "Prediction Setup",
    "section": "How to choose a model?",
    "text": "How to choose a model?\n\nTypically, we would say that the best model is the one that has the highest \\(R^2\\) or the lowest MSE.\n\nThis may be true for the original data, but not for the target observations.\nAlso, \\(R^2\\) and MSE always increase when we add more variables.\n\nSo, what we need is a way to check how well the models predict the target observations. (unobserved cases)\nWe want to avoid overfitting at all costs."
  },
  {
    "objectID": "rm-data/slides/week10.html#example",
    "href": "rm-data/slides/week10.html#example",
    "title": "Prediction Setup",
    "section": "Example",
    "text": "Example\n\nAssume the true model is \\(y = 1 + x -.5 x^2 + \\epsilon\\), with \\(\\epsilon ~ N(0,1.5)\\)\nwe create 100 observations from the above process but use only 30 for modeling. We try running regressions with ever more complex models\nMake predictions, and see how well we did.\n\n\n\n\n\n\\(R^2\\)\nMSE\nOoS MSE\n\n\n\n\n^1\n.3224669\n1.934762\n2.949084\n\n\n^2\n.6255072\n1.069401\n2.93478\n\n\n^3\n.6344791\n1.043781\n2.978827\n\n\n^4\n.713217\n.8189369\n4.138006\n\n\n^5\n.7259535\n.7825666\n3.557651\n\n\n\nSee how \\(R^2\\) increases monotonously, improving “in-sample fit”. But, the OoS MSE worsens after a certain point."
  },
  {
    "objectID": "rm-data/slides/week10.html#example-2",
    "href": "rm-data/slides/week10.html#example-2",
    "title": "Prediction Setup",
    "section": "Example 2",
    "text": "Example 2"
  },
  {
    "objectID": "rm-data/slides/week10.html#underfit-vs-overfit",
    "href": "rm-data/slides/week10.html#underfit-vs-overfit",
    "title": "Prediction Setup",
    "section": "Underfit vs overfit",
    "text": "Underfit vs overfit\n\nA model that fits the data worse in the “working/original” data compared to the “live/target” data is said to “underfit” the model.\n\nSimple: we should build a better model.\n\nIf the model fits the data working better than target data, then it over-fits it. Needs to be corrected.\n\nOverfitting\n\nOverfitting is a key aspect of external validity\n\nFinding a model that fits the data better than alternative models, but makes worse actual prediction.\n\nOverfitting is a common problem in prediction analysis"
  },
  {
    "objectID": "rm-data/slides/week10.html#reason-for-overfitting",
    "href": "rm-data/slides/week10.html#reason-for-overfitting",
    "title": "Prediction Setup",
    "section": "Reason for overfitting",
    "text": "Reason for overfitting\nThe typical reason for overfitting is fitting a model that is too complex on the dataset. - Complexity: number of estimated coefficients - Often: fitting a model with too many predictor variables. - Including too many variables from the dataset that do not really add to the predictive power of the regression. - Problems of multicollinearity, too many interactions, etc. - Too detailed nonlinear patterns - as piecewise linear splines with many knots - polynomials of high degree."
  },
  {
    "objectID": "rm-data/slides/week10.html#finding-the-best-model-by-best-fit-and-penalty",
    "href": "rm-data/slides/week10.html#finding-the-best-model-by-best-fit-and-penalty",
    "title": "Prediction Setup",
    "section": "Finding the best model by best fit and penalty",
    "text": "Finding the best model by best fit and penalty\n\nAs shown earlier, traditional measures of fit, such as \\(R^2\\) and MSE, are not good for finding the best model. They always increase with the number of variables.\nWe were only able to conclude model fitness because we had the true model (and the Out-of-Sample data)\n\nThis is not the case in practice.\n\nSo, how do we find the best model?\n\nIndirectly: Penalize the number of variables\nDirectly: Use a training-test sample"
  },
  {
    "objectID": "rm-data/slides/week10.html#indirect-evaluation-criteria",
    "href": "rm-data/slides/week10.html#indirect-evaluation-criteria",
    "title": "Prediction Setup",
    "section": "Indirect evaluation criteria",
    "text": "Indirect evaluation criteria\n\nMain methods: AIC, BIC and adjusted \\(R^2\\)\n\nAdvantage: easy to compute\nDisadvantage: assumptions. They may not penalize enough.\n\nAdjusted \\(R^2\\) – just add a penalty for having many RHS vars\n\ncorrects with \\((n - 1)/(n - k - 1)\\)\n\nAIC = \\(-2 \\times \\ln(\\text{likelihood}) + 2 \\times k\\)\nBIC = \\(-2 \\times \\ln(\\text{likelihood}) + \\ln(N) \\times k\\)\n\nBoth are based on information loss theory\nBIC puts heavier penalty on models with many RHS variables, than AIC."
  },
  {
    "objectID": "rm-data/slides/week10.html#example-redone",
    "href": "rm-data/slides/week10.html#example-redone",
    "title": "Prediction Setup",
    "section": "Example, redone",
    "text": "Example, redone\n\n\nCode\n* This is the full code for previous and current example\nclear\nset seed 10\nset obs 100\ngen x = runiform(-2 ,2)\ngen y = 1 + x - 0.5 * x^2 + 1.5* rnormal()\n \ncapture program drop fit_stat1\nprogram fit_stat1, rclass\n    tempvar yhat smp\n    gen `smp' = e(sample)\n    predict `yhat'\n    replace `yhat'=(y-`yhat')^2\n    \n    matrix result=e(r2)\n    sum `yhat' if `smp', meanonly\n    matrix result=result, r(mean)\n    sum `yhat' if !`smp', meanonly\n    matrix result=result, r(mean)\n    return matrix result = result\nend\n\nreg y c.x if _n&lt;=30\nfit_stat1\nmatrix rr2=r(result)\nreg y c.x##c.x if _n&lt;=30\nfit_stat1\nmatrix rr2=rr2\\r(result)\nreg y c.x##c.x##c.x if _n&lt;=30\nfit_stat1\nmatrix rr2=rr2\\r(result)\nreg y c.x##c.x##c.x##c.x if _n&lt;=30\nfit_stat1\nmatrix rr2=rr2\\r(result)\nreg y c.x##c.x##c.x##c.x##c.x if _n&lt;=30\nfit_stat1\nmatrix rr2=rr2\\r(result)\n\ncapture program drop fit_stat2\nprogram fit_stat2, rclass\n    tempvar yhat smp\n    gen `smp' = e(sample)\n    predict `yhat'\n    replace `yhat'=(y-`yhat')^2    \n    matrix result=e(r2), e(r2_a)\n    sum `yhat' if `smp', meanonly\n    matrix result=result, r(mean)\n    sum `yhat' if !`smp', meanonly\n    matrix result=result, r(mean)\n    qui: estat ic\n    matrix result=result, r(S)[1,5..6]\n    return matrix result = result\nend\n\nreg y c.x if _n&lt;=30\nfit_stat2\nmatrix rr2=r(result)\nreg y c.x##c.x if _n&lt;=30\nfit_stat2\nmatrix rr2=rr2\\r(result)\nreg y c.x##c.x##c.x if _n&lt;=30\nfit_stat2\nmatrix rr2=rr2\\r(result)\nreg y c.x##c.x##c.x##c.x if _n&lt;=30\nfit_stat2\nmatrix rr2=rr2\\r(result)\nreg y c.x##c.x##c.x##c.x##c.x if _n&lt;=30\nfit_stat2\nmatrix rr2=rr2\\r(result)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\)\n\\(aR^2\\)\nMSE\nOoS MSE\nAIC\nBIC\n\n\n\n\n^1\n0.3225\n0.2983\n1.9348\n2.9491\n108.9358\n111.7382\n\n\n^2\n0.6255\n0.5978\n1.0694\n2.9348\n93.1493\n97.3529\n\n\n^3\n0.6345\n0.5923\n1.0438\n2.9788\n94.4218\n100.0266\n\n\n^4\n0.7132\n0.6673\n0.8189\n4.1380\n89.1439\n96.1499\n\n\n^5\n0.7260\n0.6689\n0.7826\n3.5577\n89.7810\n98.1882"
  },
  {
    "objectID": "rm-data/slides/week10.html#finding-the-best-model-by-training-and-test-samples",
    "href": "rm-data/slides/week10.html#finding-the-best-model-by-training-and-test-samples",
    "title": "Prediction Setup",
    "section": "Finding the best model by training and test samples",
    "text": "Finding the best model by training and test samples\n\nSimilar to bootstrapping (brute force approach for SE), its also possible to use a “bute-force” approach to find the best model.\nThis would require “imitating” the process of out-of-sample prediction.\n\nCut the dataset into training and test sample (80-20 ?)\nChoose Some evaluation criterion (loss function)\nEstimate the model on the training sample\nPredict and evaluate the model on the test sample\n\nProblem: 80% (or less), may be too small training. And 20% (one shoot) could be different from the rest of the data."
  },
  {
    "objectID": "rm-data/slides/week10.html#lets-make-things-better-k-fold-cross-validation",
    "href": "rm-data/slides/week10.html#lets-make-things-better-k-fold-cross-validation",
    "title": "Prediction Setup",
    "section": "Lets make things Better: K-fold cross-validation",
    "text": "Lets make things Better: K-fold cross-validation\n\nIf one is not enough, why not use more?\nSplit sample into \\(k=5\\) groups (equal size)\nNow, assume that each “fold” is the test sample, and the rest is the training sample.\n\nDo the excercise k times, and every observation will be in the test sample once.\n\nAdd up the MSEs, or get the average MSE.\nStill has a random component, but less so than a single split."
  },
  {
    "objectID": "rm-data/slides/week10.html#k-fold-cross-validation",
    "href": "rm-data/slides/week10.html#k-fold-cross-validation",
    "title": "Prediction Setup",
    "section": "K-fold cross-validation",
    "text": "K-fold cross-validation\n\nCode\n* This is the full code for previous and current example\nclear\nset seed 10\nqui: set obs 30\ngen x = runiform(-2 ,2)\ngen y = 1 + x - 0.5 * x^2 + 1.5* rnormal()\n\n** Create 5 folds\ngen fold = mod(_n,5)+1\n\ncapture program drop fit_stat3\nprogram fit_stat3, rclass\n    tempvar yhat aux\n    qui:gen `yhat' = .\n    qui:forvalues i = 1/5 {\n        reg `0' if fold!=`i'\n        predict `aux'\n        replace `yhat' = `aux' if fold==`i'\n        drop `aux'\n    }\n    qui: replace `yhat'=(y-`yhat')^2    \n    sum `yhat', meanonly\n    return scalar mse = r(mean)\nend\n\nfit_stat3 y c.x \ndisplay \"^1, mse: \" %5.3f `r(mse)' _n\nfit_stat3 y c.x##c.x \ndisplay \"^2, mse: \" %5.3f `r(mse)' _n\nfit_stat3 y c.x##c.x##c.x \ndisplay \"^3, mse: \" %5.3f `r(mse)' _n\nfit_stat3 y c.x##c.x##c.x##c.x \ndisplay \"^4, mse: \" %5.3f `r(mse)' _n\nfit_stat3 y c.x##c.x##c.x##c.x##c.x \ndisplay \"^5, mse: \" %5.3f `r(mse)' _n\n\n^1, mse: 4.851\n^2, mse: 3.196\n^3, mse: 2.805\n^4, mse: 3.085\n^5, mse: 3.160"
  },
  {
    "objectID": "rm-data/slides/week10.html#when-k-rightarrow-n.-loocv",
    "href": "rm-data/slides/week10.html#when-k-rightarrow-n.-loocv",
    "title": "Prediction Setup",
    "section": "When K \\(\\rightarrow\\) N. LooCV",
    "text": "When K \\(\\rightarrow\\) N. LooCV\n\nK-fold cross-validation has two weaknesses:\n\nRandomness: different splits may lead to different results\nSmall sample size for training may be too small\n\nAn alternative to address both is increasing K … until K=N\n\nThis is the Leave-one-out cross-validation (LOOCV)\nTrain on N-1 observations, predict on the left-out (N=1) observation\n\nThis can be computationally expensive, unless you are estimating Linear regression models.\n\nLOOCV its really fast for OLS"
  },
  {
    "objectID": "rm-data/slides/week10.html#bic-vs-test-rmse",
    "href": "rm-data/slides/week10.html#bic-vs-test-rmse",
    "title": "Prediction Setup",
    "section": "BIC vs test RMSE",
    "text": "BIC vs test RMSE\n\nIn practice, BIC is the best indirect criterion – closest to test sample.\nThe advantage of BIC is that it needs no sample splitting which may be a problem in small samples.\nThe advantage of test MSE is that it makes no assumption.\nBIC is a good first run, quick, is often not very wrong.\nTest MSE is the best, but may be computationally expensive."
  },
  {
    "objectID": "rm-data/slides/week10.html#external-validity-and-stable-patterns",
    "href": "rm-data/slides/week10.html#external-validity-and-stable-patterns",
    "title": "Prediction Setup",
    "section": "External validity and stable patterns",
    "text": "External validity and stable patterns\n\nBIC, Training-test, k-fold cross-validation… All very nice\n\nIn the end, they all use the information in the data.\n\nHow would things look for the target observation(s)? unknown!!\nThe issue of stationarity – how our data is related to other datasets we may use our model\nIn the end we can’t know but need to think about it.\n\nPlus, if there is no external validity, your model fit in an outside data source is likely to be worse…"
  },
  {
    "objectID": "rm-data/slides/week10.html#external-validity-and-stable-patterns-1",
    "href": "rm-data/slides/week10.html#external-validity-and-stable-patterns-1",
    "title": "Prediction Setup",
    "section": "External validity and stable patterns",
    "text": "External validity and stable patterns\n\nMost predictions will be on future data\nHigh external validity requires that the environment is stationary.\nStationarity means that the way variables are distributed remains the same over time.\n\nEnsures that the relationship between predictors and the target variable is the same in the data and the forecasted future.\n\nIf the relationship breaks down whatever we establish in our data won’t be true in the future, leading to wrong forecasts."
  },
  {
    "objectID": "rm-data/slides/week10.html#external-validity-and-stable-patterns-2",
    "href": "rm-data/slides/week10.html#external-validity-and-stable-patterns-2",
    "title": "Prediction Setup",
    "section": "External validity and stable patterns",
    "text": "External validity and stable patterns\n\nExternal validity and stable patterns - Very broad concept\nIt’s about representativeness of actual data \\(\\rightarrow\\) to live data\n\nRemember hotels? (other dates, other cities).\n\nDomain knowledge can help. Inner knowledge of the process can help.\nStudy if patterns were stable in the past / other locations were stable can help."
  },
  {
    "objectID": "rm-data/slides/week10.html#main-takeaways",
    "href": "rm-data/slides/week10.html#main-takeaways",
    "title": "Prediction Setup",
    "section": "Main takeaways",
    "text": "Main takeaways\n\nPrediction uses the original data with \\(y\\) and \\(x\\) to predict the value of \\(y\\) for observations in the live data, in which \\(x\\) is observed but \\(y\\) is not\nPrediction uses a model that describes the patterns of association between \\(y\\) and \\(x\\) in the original data\nCross-validation can help find the best model in the population, or general pattern, represented by the original data\nStability of the patterns of association is needed for a prediction with high external validity"
  },
  {
    "objectID": "rm-data/slides/week10.html#cs-predicting-the-price-of-used-cars",
    "href": "rm-data/slides/week10.html#cs-predicting-the-price-of-used-cars",
    "title": "Prediction Setup",
    "section": "CS: Predicting the price of used cars",
    "text": "CS: Predicting the price of used cars\n\nSUse data from here for replication.\nDrop car with price&gt;20k or those price = 1$.\nWhile distribution is skewed, Log(price) is not normal either. We will keep it as is.\nTwo areas, we keep Chicago only.\ndrop if odometer&gt;100K miles\nVariables: price, age, odometer, type, condition, cylinders, dealer"
  },
  {
    "objectID": "rm-data/slides/week10.html#cs-competing-models",
    "href": "rm-data/slides/week10.html#cs-competing-models",
    "title": "Prediction Setup",
    "section": "CS: Competing models:",
    "text": "CS: Competing models:\n\nModel 1 age, age squared\nModel 2 age, age squared, odometer, odometer squared\nModel 3 age, age squared, odometer, odometer squared, LE, condition , dealer\nModel 4 age, age squared, odometer, odometer squared, LE, condition , dealer, SE, XLE, cylinder\nModel 5 same as Model 4 but with all variables interacted with age\nModel 6 same as Model 4 but with all variables interacted with condition\n\n\n\nCode\n* setup\nuse data_slides/used-cars.dta, clear\nglobal model1 age c.age#c.age\nglobal model2 $model1 lnodometer c.lnodometer#c.lnodometer\nglobal model3 $model2 LE i.condition_recode i.dealer\nglobal model4 $model3 SE XLE cylinder\nglobal model5 c.($model4)##c.age\nglobal model6 c.($model4)##i.condition_recode"
  },
  {
    "objectID": "rm-data/slides/week10.html#cs-test-data",
    "href": "rm-data/slides/week10.html#cs-test-data",
    "title": "Prediction Setup",
    "section": "CS: Test Data",
    "text": "CS: Test Data\n\nAssume that 20% of the data is “target” data.\nLets run all models, using different evaluation criteria. And see which one is the best.\nSee below for the program..Rather long\n\n\n\nCode\ngen rnd = runiform()\nsort rnd\ngen test = _n/_N&lt;=.2\n\ncapture program drop fit_stat4\nprogram fit_stat4, rclass\n    reg `0' if test==0\n    tempvar yxhat\n    predict `yxhat' if test==1\n    replace `yxhat' = (price-`yxhat')^2  if test==1\n    matrix result = e(r2), e(r2_a)\n    estat ic\n    matrix result = result, r(S)[1,5..6]\n    ** K-fold cross-validation\n    capture drop fold\n    gen fold = runiform() if test==0\n    sort fold \n    drop fold\n    gen fold = mod(_n,5)+1 if test==0\n    \n    *** Kfold1\n    tempvar yhat aux\n    qui:gen `yhat' = .\n    qui:forvalues i = 1/5 {\n        reg `0' if fold!=`i' & test==0\n        predict `aux'\n        replace `yhat' = `aux' if fold==`i' & test==0\n        drop `aux'\n    }\n    qui: replace `yhat'=(price-`yhat')^2     \n    sum `yhat' if test==0, meanonly\n    matrix result = result, sqrt(r(mean))\n    *** Kfold2\n    capture drop fold\n    gen fold = runiform() if test==0\n    sort fold \n    drop fold\n    gen fold = mod(_n,5)+1 if test==0\n    \n    drop `yhat'  \n    qui:gen `yhat' = .\n    qui:forvalues i = 1/5 {\n        reg `0' if fold!=`i' & test==0\n        predict `aux'\n        replace `yhat' = `aux' if fold==`i' & test==0\n        drop `aux'\n    }\n    qui: replace `yhat'=(price-`yhat')^2    \n    sum `yhat' if test==0, meanonly\n    matrix result = result, sqrt(r(mean))\n    ** test on live data\n    sum `yxhat' if test==1, meanonly\n    matrix result = result, sqrt(r(mean))\n    matrix colname result = r2 r2_a AIC BIC Kfold1 Kfold2 test\n    ** \n    return matrix result = result\nend"
  },
  {
    "objectID": "rm-data/slides/week10.html#cs-results",
    "href": "rm-data/slides/week10.html#cs-results",
    "title": "Prediction Setup",
    "section": "CS: Results",
    "text": "CS: Results\n\nCode\nqui: fit_stat4 price $model1\nmatrix fresults = r(result)\nqui: fit_stat4 price $model2\nmatrix fresults = fresults\\r(result)\nqui: fit_stat4 price $model3\nmatrix fresults = fresults\\r(result)\nqui: fit_stat4 price $model4\nmatrix fresults = fresults\\r(result)\nqui: fit_stat4 price $model5\nmatrix fresults = fresults\\r(result)\nqui: fit_stat4 price $model6\nmatrix fresults = fresults\\r(result)\nmatrix rowname fresults = Model1 Model2 Model3 Model4 Model5 Model6\nset linesize 250\nesttab matrix(fresults, fmt(%8.3f)) , md  nomtitle\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr2\nr2_a\nAIC\nBIC\nKfold1\nKfold2\ntest\n\n\n\n\nModel1\n0.711\n0.708\n4823.338\n4834.032\n2472.245\n2470.654\n1697.617\n\n\nModel2\n0.776\n0.772\n4760.551\n4778.374\n2185.247\n2204.960\n1345.265\n\n\nModel3\n0.779\n0.771\n4766.944\n4802.590\n2270.672\n2307.392\n1346.395\n\n\nModel4\n0.785\n0.775\n4765.275\n4811.613\n2388.426\n2259.348\n1381.180\n\n\nModel5\n0.797\n0.778\n4772.246\n4857.794\n2605.005\n2425.626\n1481.571\n\n\nModel6\n0.851\n0.824\n4724.611\n4867.191\n2555.466\n2670.216\n1523.569"
  },
  {
    "objectID": "rm-data/slides/week12.html#motivation",
    "href": "rm-data/slides/week12.html#motivation",
    "title": "Probability and Classification",
    "section": "Motivation",
    "text": "Motivation\n\nSay you work for a consultancy agency helping a bank. You are asked to predict which firms will default on their loans.\n\nWhat would be better: a model that predicts probabilities or a model that classifies firms into default or not default?\nHow do you use to decide which firms should get a loan.\n\nCompanies need to assess the likelihood of their suppliers or clients staying in business, as it impacts their own operations.\n\nHow to use historical data on company exits, along with key features, to predict the probability of a company’s exit."
  },
  {
    "objectID": "rm-data/slides/week12.html#previously-on-da",
    "href": "rm-data/slides/week12.html#previously-on-da",
    "title": "Probability and Classification",
    "section": "Previously on DA",
    "text": "Previously on DA\n\nIn the previous weeks, we have covered the basics of prediction when the target is quantitative.\n\nAlmost stright forward: predict the value of the target. Consider many specifications and pick the best one.\n\nWe also cover the basic of probability modeling\n\nLPM, Logit and probit models: When your dependent variable is Binary.\n\nHowever, we have not fully cover how to use these models for prediction."
  },
  {
    "objectID": "rm-data/slides/week12.html#prediction-with-qualitative-target",
    "href": "rm-data/slides/week12.html#prediction-with-qualitative-target",
    "title": "Probability and Classification",
    "section": "Prediction with qualitative target",
    "text": "Prediction with qualitative target\n\nConsider cases where \\(Y\\) is qualitative\n\nWhether a debtor defaults (will default) on their loan\nEmail is spam or not\nGame result is win / lose (no draw).\n\nFor all this cases, the target (dep variable) is binary.\nThe question is: Given this, What is the best way to predict the target?\n\nPredict the probability of “success” (default, spam, win)\nor make a classification (default, spam, win) based on a probability."
  },
  {
    "objectID": "rm-data/slides/week12.html#the-process",
    "href": "rm-data/slides/week12.html#the-process",
    "title": "Probability and Classification",
    "section": "The process",
    "text": "The process\n\nPredict probability: We have done this.\n\nPredicted probability between 0 and 1 (logit, probit or LPM in extreme cases)\nFor each observation we predicted a probability. Often that is it.\n\n\nif logit: \\[\\Pr[y_i = 1|x_i] = \\Lambda \\times (\\beta_0 + \\beta_1x_i) = \\frac{\\exp (\\beta_0 + \\beta_1x_i)}{1 + \\exp (\\beta_0 + \\beta_1x_i)}\\]\n\nThats it! You can probably go couple of steps further and use various specifications, as well as LASSO (for logit) to pick the best model.\nBest model can still be picked based on RMSE, brier score or Calibration."
  },
  {
    "objectID": "rm-data/slides/week12.html#refresher-probability-models",
    "href": "rm-data/slides/week12.html#refresher-probability-models",
    "title": "Probability and Classification",
    "section": "Refresher: Probability Models",
    "text": "Refresher: Probability Models\n\nLPM - not this time: Predicted value MUST be between 0 and 1\nLogit or probit (or other non-linear probability models)\nNonlinear probability models \\[\\Pr[y_i = 1|x_i] = \\Lambda(\\beta_0 + \\beta_1x_i) = \\frac{\\exp (\\beta_0 + \\beta_1x_i)}{1 + \\exp (\\beta_0 + \\beta_1x_i)}\\]\n\nPredicted probability between 0 and 1\nStarts with a linear combination of the explanatory variables\nMultiplies them with coefficients, just like linear regression\nAnd then transforms that into something that is always between 0 and 1, the predicted probability."
  },
  {
    "objectID": "rm-data/slides/week12.html#whats-new-with-binary-target",
    "href": "rm-data/slides/week12.html#whats-new-with-binary-target",
    "title": "Probability and Classification",
    "section": "What’s New with Binary target?",
    "text": "What’s New with Binary target?\n\nThe predicted Probability is not a value.\nDesire to classify\n\nassign 0 or 1\nbased on a probability that comes from a model\nBut how?\n\nWe also need new measures of fit\n\nSome based on probabilities\nOthers based on classification"
  },
  {
    "objectID": "rm-data/slides/week12.html#whats-not-new-with-binary-target",
    "href": "rm-data/slides/week12.html#whats-not-new-with-binary-target",
    "title": "Probability and Classification",
    "section": "What’s NOT new with Binary target?",
    "text": "What’s NOT new with Binary target?\n\nNeed best fit\n\nWith highest external validity\n\nUsual worries: overfit\n\nCross-validation helps avoid worst overfit\n\nModels similar to those used earlier\n\nRegression-like models (probability models)\nTree-based models (CART, Random Forest) &lt;- We will not cover this"
  },
  {
    "objectID": "rm-data/slides/week12.html#probability-prediction-and-process",
    "href": "rm-data/slides/week12.html#probability-prediction-and-process",
    "title": "Probability and Classification",
    "section": "Probability prediction and process",
    "text": "Probability prediction and process\n\nWe build models to predict probability when:\n\naim is to predict probabilities – (Duh!)\naim is to classify (predict 0 or 1) – (we need probabilities first)\n\nBuild models\n\nseveral Logit models by domain knowledge\nLASSO - Logit LASSO\n\nPick the best model via cross-validation using RMSE / Brier score\n\nOr other LOSS function, if you have one"
  },
  {
    "objectID": "rm-data/slides/week12.html#classification-process",
    "href": "rm-data/slides/week12.html#classification-process",
    "title": "Probability and Classification",
    "section": "Classification process",
    "text": "Classification process\n\nAfter you predict your conditional probability, you can make classifications based on some threshold or Rule.\n\nFor example if \\(\\Pr[y = 1] &gt; 0.5\\) then predict 1, otherwise predict 0\nBut we can choose any threshold. but how? (say top 10%?)\n\nWe need to consider that we can make errors\n\nFalse negative\nFalse positive\n\nThus we need to consider a threshold that minimizes the expected errors"
  },
  {
    "objectID": "rm-data/slides/week12.html#classification-table-confusion-matrix",
    "href": "rm-data/slides/week12.html#classification-table-confusion-matrix",
    "title": "Probability and Classification",
    "section": "Classification Table: Confusion Matrix",
    "text": "Classification Table: Confusion Matrix\n\n\n\n\n\n\n\n\n\n\n\\(y_j = 0\\)\n\\(y_j = 1\\)\nTotal\n\n\n\n\n\\(\\hat{y}_j = 0\\)\nTN\nFN\nTN + FN\n\n\nPredicted negative\n(true negative)\n(false negative)\n(all classified negative)\n\n\n\\(\\hat{y}_j = 1\\)\nFP\nTP\nFP + TP\n\n\nPredicted positive\n(false positive)\n(true positive)\n(all classified positive)\n\n\nTotal\nTN + FP\nFN + TP\nTN + FN + FP + TP\n\n\n(all actual negative)\n(all actual positive)\n(N, all observations)"
  },
  {
    "objectID": "rm-data/slides/week12.html#classification-table-making-errors",
    "href": "rm-data/slides/week12.html#classification-table-making-errors",
    "title": "Probability and Classification",
    "section": "Classification Table: making errors",
    "text": "Classification Table: making errors\n\n\n\n\n\n\n\n\n\n\n\\(y_j = 0\\)\n\\(y_j = 1\\)\nTotal\n\n\n\n\n\\(\\hat{y}_j = 0\\)\nPredict firm stay\nPredict firm stay\nTN + FN\n\n\nPredicted negative\n(Firm did stay )\n(Firm exited )\n(all classified stay )\n\n\n\\(\\hat{y}_j = 1\\)\nPredict firm exit\nPredict firm exit\nFP + TP\n\n\nPredicted positive\n(Firm stayed )\n(Firm did exit)\n(all classified exit)\n\n\nTotal\nTN + FP\nFN + TP\nTN + FN + FP + TP\n\n\n(all actual stay )\n(all actual exit)\n(N, all observations)"
  },
  {
    "objectID": "rm-data/slides/week12.html#measures-of-classification",
    "href": "rm-data/slides/week12.html#measures-of-classification",
    "title": "Probability and Classification",
    "section": "Measures of classification",
    "text": "Measures of classification\nThere are several measures of classification, each with a different focus.\n\nAccuracy \\(=(TP+TN)/N\\)\n\nThe proportion of rightly guessed observations\nHit rate\n\nSensitivity \\(=TP / (TP+FN)\\)\n\nThe proportion of true positives among all actual positives\nProbability of predicted \\(y\\) is 1 conditional on \\(y = 1\\)\n\nSpecificity \\(= TN/(TN+FP)\\)\n\nThe proportion of true negatives among all actual negatives\nProbability predicted \\(y\\) is 0 conditional on \\(y = 0\\)"
  },
  {
    "objectID": "rm-data/slides/week12.html#measures-of-classification-1",
    "href": "rm-data/slides/week12.html#measures-of-classification-1",
    "title": "Probability and Classification",
    "section": "Measures of classification",
    "text": "Measures of classification\n\nThe key point is that there is a trade-off between making false positive and false negative errors.\nThis is the essential insight in classification\nThis can be expressed with specificity and sensitivity."
  },
  {
    "objectID": "rm-data/slides/week12.html#roc-curve",
    "href": "rm-data/slides/week12.html#roc-curve",
    "title": "Probability and Classification",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nThe ROC curve is a popular graphic for simultaneously displaying specificity and sensitivity for all possible thresholds.\nROC: Receiver operating characteristic curve\n\nName from engineering\n\nFor each threshold, we can compute confusion table \\(\\rightarrow\\) calculate sensitivity and specificity\nThen, we can plot sensitivity vs 1-specificity for all thresholds\n\nHorizontal axis: False positive rate (one minus specificity) = the proportion of FP among actual negatives\nVertical axis: is true positive rate (sensitivity) = proportion of TP among actual positives"
  },
  {
    "objectID": "rm-data/slides/week12.html#roc-curve-intuition",
    "href": "rm-data/slides/week12.html#roc-curve-intuition",
    "title": "Probability and Classification",
    "section": "ROC Curve Intuition",
    "text": "ROC Curve Intuition\n\nConsider this:\n\nIf the threshold is 0, we predict all observations as 1. The sensitivity is 1, but the specificity is 0.\nIf the threshold is 1, we predict all observations as 0. The sensitivity is 0, but the specificity is 1.\nThe “ideal” threshold is somewhere in between.\n\nROC curve shows how true positives and false positives increases relative to each other."
  },
  {
    "objectID": "rm-data/slides/week12.html#roc-curve-intuition-1",
    "href": "rm-data/slides/week12.html#roc-curve-intuition-1",
    "title": "Probability and Classification",
    "section": "ROC Curve Intuition",
    "text": "ROC Curve Intuition\n\nRandom CovariateNof ChildrenFull Model"
  },
  {
    "objectID": "rm-data/slides/week12.html#area-under-roc-curve",
    "href": "rm-data/slides/week12.html#area-under-roc-curve",
    "title": "Probability and Classification",
    "section": "Area Under ROC Curve",
    "text": "Area Under ROC Curve\n\nROC curve: the closer it is to the top left column, the better the (insample) prediction.\nArea under ROC (AUC) curve summarizes quality of probabilistic prediction\n\nFor all possible threshold choices\nArea \\(=\\) 0.5 if random classification\nArea \\(&gt;\\) 0.5 if curve mostly over 45 degree line\n\nAUC is a good statistic to compare models\n\nDefined from a non-threshold dependent model (ROC)\nThe larger the better\nRanges between 0 and 1."
  },
  {
    "objectID": "rm-data/slides/week12.html#stata-corner",
    "href": "rm-data/slides/week12.html#stata-corner",
    "title": "Probability and Classification",
    "section": "Stata Corner",
    "text": "Stata Corner\n\nLogit estimation: logit y x1 x2 x3\nPredict probabilities: predict yhat, pr\nClassification:\n\ngen yhat_class = (yhat &gt; 0.5)\nestat classification\n\nROC curve: lroc"
  },
  {
    "objectID": "rm-data/slides/week12.html#model-selection-nr.1-probability-models",
    "href": "rm-data/slides/week12.html#model-selection-nr.1-probability-models",
    "title": "Probability and Classification",
    "section": "Model selection Nr.1: Probability models",
    "text": "Model selection Nr.1: Probability models\n\nModel selection when we have no loss function, based on probability models only\n\nPredict probabilities (No actual classification)\nUse predicted probability to calculate RMSE\nPick by smallest RMSE\n\nOr\n\nDraw up ROC curve and get AUC, Pick the model with the largest AUC\nMore frequently used in practice\nLess sensitive to class imbalance\n\nIn practice, AUC is more frequently used"
  },
  {
    "objectID": "rm-data/slides/week12.html#how-we-make-classification-from-predicted-probability",
    "href": "rm-data/slides/week12.html#how-we-make-classification-from-predicted-probability",
    "title": "Probability and Classification",
    "section": "How we make classification from predicted probability?",
    "text": "How we make classification from predicted probability?\n\nWe set a threshold!\nThe process of classification\n\nIf probability of event is higher than this threshold\\(\\rightarrow\\) assign (predict) class 1; and 0 otherwise.\n\nWho sets the threshold?\n\nUsually approximated by 0.5\nor by the frequency of the event in the data"
  },
  {
    "objectID": "rm-data/slides/week12.html#classification-select-the-threshold-with-loss-function",
    "href": "rm-data/slides/week12.html#classification-select-the-threshold-with-loss-function",
    "title": "Probability and Classification",
    "section": "Classification: select the threshold with loss function",
    "text": "Classification: select the threshold with loss function\n\nFind optimal threshold with loss function.\n\nA loss function is a dollar value assigned to false positive and false negative.\nMost often the costs of FP and FN are very different.\n\n\nConsider loss function\n\\[E[loss] = \\Pr[FN] \\times loss(FN) + \\Pr[FP] \\times loss(FP)\\]\n\nIn ideal case, the minimization of this suggests that optimal threshold is:\n\n\\[\\text{Threshold} = \\frac{loss(FP)}{loss(FN) + loss(FP)}\\]\n\nOr we can try finding the threshold that minimizes the expected loss using Cross-validation. (Software issue)"
  },
  {
    "objectID": "rm-data/slides/week12.html#when-to-use-formula",
    "href": "rm-data/slides/week12.html#when-to-use-formula",
    "title": "Probability and Classification",
    "section": "When to use Formula",
    "text": "When to use Formula\n\nFormula\n\nWhen dataset is “large”\nWhen our model has a “good” fit \\(\\text{Threshold}_{\\min E (loss)} = \\frac{loss(FP)}{loss(FN) + loss(FP)}\\)\n\nIn practice\n\nPro: easy to use, often close enough\nCon: not the best cutoff, especially for smaller data, and poorer model"
  },
  {
    "objectID": "rm-data/slides/week12.html#class-imbalance",
    "href": "rm-data/slides/week12.html#class-imbalance",
    "title": "Probability and Classification",
    "section": "Class imbalance",
    "text": "Class imbalance\n\nA potential issue for some dataset - relative frequency of the classes.\nClass imbalance = the event we care about is very rare or very frequent (\\(\\Pr(y = 1)\\) or \\(\\Pr(y = 0)\\) is very small)\n\nFraud, Sport injury\n\nWhat is rare?\n\nSomething like 1%, 0.1%. (10% should be okay.)\nDepends on size: in larger dataset we can identify rare patterns better.\n\nConsequence: Hard to find those rare events.\n\nYou may be able to identify some patterns by chance."
  },
  {
    "objectID": "rm-data/slides/week12.html#class-imbalance-the-consequences",
    "href": "rm-data/slides/week12.html#class-imbalance-the-consequences",
    "title": "Probability and Classification",
    "section": "Class imbalance: the consequences",
    "text": "Class imbalance: the consequences\n\nMethods we use not good at handling it.\nBoth for the models to predict probabilities, and for the measures of fit used for model selection.\nThe functional form assumptions behind the logit model tend to matter more, the closer the probabilities are to zero or one.\nCross-validation can be less effective at avoiding overfitting with very rare or very frequent events if the dataset is not very big. (Many samples will not even have the event.)\nUsual measures of fit can be less good at differentiating models.\nConsequence: Model fitting and selection setup not ideal"
  },
  {
    "objectID": "rm-data/slides/week12.html#class-imbalance-what-to-do",
    "href": "rm-data/slides/week12.html#class-imbalance-what-to-do",
    "title": "Probability and Classification",
    "section": "Class imbalance: what to do",
    "text": "Class imbalance: what to do\n\nWhat to do? Two key insights.\n\nKnow when it’s happening, and be ready for poor performance.\nMay need an action: rebalance sample to help build better models\n\nDownsampling – randomly drop observations from frequent class to balance out more\n\nBefore: 100,000 observations 1% event rate (99,000 \\(y = 1\\), 1,000 \\(y = 0\\))\nAfter 10,000 observations 10% event rate (9,000 \\(y = 1\\), 1,000 \\(y = 0\\))\n\nOver-sampling of rare events\ntry Smart algorithms: Synthetic Minority Over-Sampling Technique (SMOTE)"
  },
  {
    "objectID": "rm-data/slides/week12.html#case-study",
    "href": "rm-data/slides/week12.html#case-study",
    "title": "Probability and Classification",
    "section": "Case study",
    "text": "Case study"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-case-study-background",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-case-study-background",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Case study: background",
    "text": "Firm exit case study: Case study: background\n\nBanks and business partners are often interested in the stability of their customers.\nPredicting which firms will be around to do business with is an important part of many prediction projects.\nWorking with financial and non-financial information, your task may be to predict which firms are more likely to default than others.\nGoal: Predict corporate default - exit from the market.\n\nWe have to figure out and decide on target, features, etc."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-bisnode-firms-dataset",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-bisnode-firms-dataset",
    "title": "Probability and Classification",
    "section": "Firm exit case study: bisnode-firms dataset",
    "text": "Firm exit case study: bisnode-firms dataset\n\nFirm data\nMany different type of variables\n\nFinancial, Management, Ownership, Status (HQ)\n\nDataset is a panel data\nRows are identified by company id (comp-id) and year.\nWe’ll focus on a cross-section of 2012."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-label-target-engineering",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-label-target-engineering",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Label (target) engineering",
    "text": "Firm exit case study: Label (target) engineering\n\nDefining our target. There is no “exit” - we have to define it!\nOption: If a firm is operational in year \\(t\\), but is not in business in \\(t + 2\\) -&gt; Exit.\nThis definition is broad\n\nDefaults / forced exit\nOrderly closure\nAcquisitions"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-sample-design",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-sample-design",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Sample design",
    "text": "Firm exit case study: Sample design\n\nLook at a cross section\n\nYear=2012\nstatus_alive=1\n\nKeep if established in 2012\nWe do not care about all firms. Not very small and very large\n\nBelow 10 million euros\nAbove 1000 euros\n\nHardest call: keep when important variables are not missing\n\nBalance sheet like liquid assets\nOwnership like foreign\nIndustry classification\n\nEnd with 19K observation, 20% default rate"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-features---overview",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-features---overview",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Features - overview",
    "text": "Firm exit case study: Features - overview\n\nKey predictors\n\nsize: sales, sales growth\nmanagement: foreign, female, young, number of managers\nregion, industry, firm age\nother financial variables from the balance sheet and P&L.\n\nFor financial variables, we use ratios (to sales or size of balance sheet).\nHere it will turn out be important to look at functional form carefully, especially regarding financial variables.\nMix domain knowledge and statistics.\nPlenty of analyst calls."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-feature-engineering",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-feature-engineering",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Feature engineering",
    "text": "Firm exit case study: Feature engineering\n\nGrowth rates\n\n1 year growth rate of sales. Log difference.\nCould use longer time period, but Lose observations\n\n\nOwnership, management info\n\nKeep if well covered, impute some, but drop if key vars missing\nSometimes simplify (unless big data): ceo_young = ceo_age_mod &lt;40  & ceo_age_mod &gt;15\n\nIndustry categories - need simplify\nForeign ownership - above a threshold\nNumerical variables from balance sheet: Check functional form - logs, polynomials"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-feature-engineering-tools",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-feature-engineering-tools",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Feature engineering tools",
    "text": "Firm exit case study: Feature engineering tools\n\nCheck coverage (missing values)\nDecide on imputation vs drop\nCategorical (factor) variables\nNumerical variables\n\nCheck functional form - logs, polynomials\nLook at relationships in scatterplot, loess and decide"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-feature-engineering-1",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-feature-engineering-1",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Feature engineering",
    "text": "Firm exit case study: Feature engineering\n\nMay need to make cleaning steps.\nCreate binary variables (flags) when implementing changes to values.\nWhen financial values are negative: replace with zero and add a flag to capture imputation.\n\nZeros will not work with logs.\n\nAnnual growth in sales (difference in log sales) vs default\n\nTry editing variables by Winsorizing and adding flags for extreme values.\nSome ODD shapes due to extreme values."
  },
  {
    "objectID": "rm-data/slides/week12.html#the-weird-shape",
    "href": "rm-data/slides/week12.html#the-weird-shape",
    "title": "Probability and Classification",
    "section": "The Weird Shape",
    "text": "The Weird Shape"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-winsorizing",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-winsorizing",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Winsorizing",
    "text": "Firm exit case study: Winsorizing\n\nWhen edge of a distribution is weird…\nWinsorizing is a process to keep observations with extreme values in sample\n\nfor each variable, we\n\nidentify a threshold value, and replace values outside that threshold with the threshold value itself\nand add a flag variable.\n\n\nTwo ways to do it:\n\nan automatic approach, where the lowest and highest 1 percent or 5 percent is replaced and flagged.\nPick thresholds by domain knowledge as well as by looking at lowess. Preferred."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-firm-sales-growth",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-firm-sales-growth",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Firm sales growth",
    "text": "Firm exit case study: Firm sales growth\n\n\nThe winsorized value simply equals original value in a range and flat below/after."
  },
  {
    "objectID": "rm-data/slides/week12.html#case-study-firm-exit-model-features-1",
    "href": "rm-data/slides/week12.html#case-study-firm-exit-model-features-1",
    "title": "Probability and Classification",
    "section": "Case study: firm exit: Model features 1",
    "text": "Case study: firm exit: Model features 1\n\nFirm: Age of firm, squared age, a dummy if newly established, industry categories, location regions for its headquarters, and dummy if located in a big city.\nFinancial 1: Winsorized financial variables: fixed, liquid (incl current), intangible assets, current liabilities, inventories, equity shares, subscribed capital, sales revenues, income before tax, extra income, material, personal and extra expenditure.\nFinancial 2: Flags (extreme, low, high, zero - when applicable) and polynomials: Quadratic terms are created for profit and loss, extra profit and loss, income before tax, and share equity.\nGrowth: Sales growth is captured by a winsorized growth variable, its quadratic term and flags for extreme low and high values."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-model-features-2",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-model-features-2",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Model features 2",
    "text": "Firm exit case study: Model features 2\n\nHR: For the CEO: female dummy, winsorized age and flags, flag for missing information, foreign management dummy; and labor cost, and flag for missing labor cost information.\nData Quality: Variables related to the data quality of the financial information flag for a problem, and the length of the year that the balance sheet covers.\nInteractions: Interactions with sales growth, firm size, and industry."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-models",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-models",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Models",
    "text": "Firm exit case study: Models\nModels (number of predictors)\n\nLogit M1: handpicked few variables (\\(p = 11\\))\nLogit M2: handpicked few variables + Firm (\\(p = 18\\))\nLogit M3: Firm, Financial 1, Growth (\\(p = 35\\))\nLogit M4: M3 + Financial 2 + HR + Data Quality (\\(p = 79\\))\nLogit M5: M4 + interactions (\\(p = 153\\))\nLogit LASSO: M5 + LASSO (\\(p = 142\\))\nNumber of coefficients = N of predictors +1 (constant)"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-data",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-data",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Data",
    "text": "Firm exit case study: Data\n\n\\(N = 19,036\\)\n\\(N = 15,229\\) in work set (80%)\nCross validation 5x training + test sets\n\nUsed for cross-validation\n\n\\(N = 3,807\\) in holdout set (20%)\n\nUsed only for diagnostics of selected model."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-comparing-model-fit",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-comparing-model-fit",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Comparing model fit",
    "text": "Firm exit case study: Comparing model fit\n\n\n\n\nVariables\nCoefficients\nCV RMSE\n\n\n\n\nLogit M1\n4\n12\n0.374\n\n\nLogit M2\n9\n19\n0.366\n\n\nLogit M3\n22\n36\n0.364\n\n\nLogit M4\n30\n80\n0.362\n\n\nLogit M5\n30\n154\n0.363\n\n\nLogit LASSO\n30\n143\n0.362\n\n\n\n\n5-fold cross-validated on work set, average RMSE\nWill use Logit M4 model as benchmark"
  },
  {
    "objectID": "rm-data/slides/week12.html#classification",
    "href": "rm-data/slides/week12.html#classification",
    "title": "Probability and Classification",
    "section": "Classification",
    "text": "Classification\n\nPicked a model on RMSE/Brier score\nFor classification, we will need a threshold"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-roc-curve",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-roc-curve",
    "title": "Probability and Classification",
    "section": "Firm exit case study: ROC curve",
    "text": "Firm exit case study: ROC curve\n\n\nROC curve shows trade-off for various values of the threshold\nGo through values of the ROC curve for selected threshold values, between 0.05 and 0.75, by steps of 0.05"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-auc",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-auc",
    "title": "Probability and Classification",
    "section": "Firm exit case study: AUC",
    "text": "Firm exit case study: AUC\n\n\n\nModel\nRMSE\nAUC\n\n\n\n\nLogit M1\n0.374\n0.738\n\n\nLogit M2\n0.366\n0.771\n\n\nLogit M3\n0.364\n0.777\n\n\nLogit M4\n0.362\n0.782\n\n\nLogit M5\n0.363\n0.777\n\n\nLogit LASSO\n0.362\n0.768\n\n\n\n\nCan calculate the AUC for all our models\nModel selection by RMSE or AUC\nHere: same (could be different if close)"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-comparing-two-thresholds",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-comparing-two-thresholds",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Comparing two thresholds",
    "text": "Firm exit case study: Comparing two thresholds\n\nTake the Logit M4 model, predict probabilities and use that to classify on the holdout set\nTwo thresholds: 50% and 20%\nPredict exit if probability &gt; threshold"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-comparing-two-thresholds-1",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-comparing-two-thresholds-1",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Comparing two thresholds",
    "text": "Firm exit case study: Comparing two thresholds\n\n\n\n\n\n\n\n\n\n\n\n\n\nThreshold: 0.5\n\n\nThreshold: 0.2\n\n\n\n\n\n\n\nActual stay\nActual exit\nTotal\nActual stay\nActual exit\nTotal\n\n\nPredicted stay\n75%\n15%\n90%\n57%\n7%\n64%\n\n\nPredicted exit\n4%\n6%\n10%\n22%\n14%\n36%\n\n\nTotal\n79%\n21%\n100%\n79%\n21%\n100%"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-threshold-choice-consequences",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-threshold-choice-consequences",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Threshold choice consequences",
    "text": "Firm exit case study: Threshold choice consequences\n\nHaving a higher threshold leads to\n\nfewer predicted exits:\n\n10% when the threshold is 50% (36% for threshold 20%).\n\nfewer false positives (4% versus 22%)\nmore false negatives (15% versus 7%).\n\nThe 50% threshold leads to a higher accuracy rate than the 20% threshold\n\n50% threshold: 75% + 6% = 81%\n20% threshold: 57% + 14% = 71%\neven though the 20% threshold is very close to the actual proportion of exiting firms."
  },
  {
    "objectID": "rm-data/slides/week12.html#summary",
    "href": "rm-data/slides/week12.html#summary",
    "title": "Probability and Classification",
    "section": "Summary",
    "text": "Summary\nFirst option: no loss fn\n\nOn the work set, do 5 fold CV and loop over models\nDo Probability predictions\nCalculate average RMSE on test for each fold\nDraw ROC Curve and calculate AUC for each fold\nPick best model based on avg RMSE\nTake best model and estimate RMSE on holdout\\(\\rightarrow\\)best guess for live data performance\nOutput: probability ranking - most likely to least likely.\nShow ROC curve and confusion table with logit on holdout 4 at \\(t = 0.5\\) and \\(t = 0.2\\) - to illustrate trade-off."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-the-loss-function",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-the-loss-function",
    "title": "Probability and Classification",
    "section": "Firm exit case study: The loss function",
    "text": "Firm exit case study: The loss function\n\nLoss function = FN, FP\nWhat matters is FN/FP\nFN=10\n\nIf the model predicts staying in business and the firm exits the market (a false negative), the bank loses all 10 thousand euros.\n\nFP=1\n\nIf predict exit and the bank denies the loan but the firm stays in business in fact (a false positive), the bank loses the profit opportunity of 1 thousand euros.\n\nWith correct decisions, there is no loss."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-finding-the-threshold",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-finding-the-threshold",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Finding the threshold",
    "text": "Firm exit case study: Finding the threshold\n\nFind threshold by formula or algo\nFormula: the optimal classification threshold is \\(1/11 = 0.091\\)\nAlgo: search thru possible cutoffs"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-finding-the-threshold-1",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-finding-the-threshold-1",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Finding the threshold",
    "text": "Firm exit case study: Finding the threshold\n\nConsider all thresholds \\(T = 0.01, 0.02—1\\)\nCalculate the expected loss for all thresholds\nPick when loss function has the minimum\nDone in CV, this is fold Nr.5."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study",
    "href": "rm-data/slides/week12.html#firm-exit-case-study",
    "title": "Probability and Classification",
    "section": "Firm exit case study",
    "text": "Firm exit case study\n\nModel selection process\n\nPredict probabilities\nUse predicted probabilities and loss function to pick optimal threshold\nUse that threshold to calculate expected loss\nPick model with smallest expected loss (in 5-fold CV)\n\nWe run the threshold selection algorithm on the work set, with 5-fold cross-validation.\nBest is model Logit M4\nthe optimal classification threshold by algo is 0.082. Close to formula (0.091)\nThe average expected loss of 0.64."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-studysummary-of-process-with-loss-function",
    "href": "rm-data/slides/week12.html#firm-exit-case-studysummary-of-process-with-loss-function",
    "title": "Probability and Classification",
    "section": "Firm exit case study:Summary of process with loss function",
    "text": "Firm exit case study:Summary of process with loss function\n\nOn the work set, do 5 fold CV and loop over models\nDo Probability predictions\nCalculate average RMSE on each test folds\nDraw ROC Curve and find optimal threshold with loss function (1,10)\n\nshow: threshold search - loss plots and ROC curve for fold 5\n\nSummarize: for each model: average of optimal thresholds, threshold for fold 5, average expected loss, expected loss for fold Nr.5.\nPick best model based on average expected loss\nTake best model, re-estimate it on work set + find optimal threshold and estimate expected loss on holdout set\n\nHere is the continued content in Quarto format:"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-studysummary-of-process-with-loss-function-1",
    "href": "rm-data/slides/week12.html#firm-exit-case-studysummary-of-process-with-loss-function-1",
    "title": "Probability and Classification",
    "section": "Firm exit case study:Summary of process with loss function",
    "text": "Firm exit case study:Summary of process with loss function\n\nConfusion table on holdout with optimal threshold\\(\\rightarrow\\)what to expect in live data."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-cart",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-cart",
    "title": "Probability and Classification",
    "section": "Firm exit case study: CART",
    "text": "Firm exit case study: CART\n\nCART\n\na small tree we built for illustration purposes\nwith only three variables:\n\nfirm size (sales),\nbinary variable for having a foreign management\nBinary if the firm is new.\n\nTerminal nodes with share of exit predictions"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-random-forest",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-random-forest",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Random Forest",
    "text": "Firm exit case study: Random Forest\n\nThe model outperforms the logit models, with a cross validated RMSE of 0.358 and AUC of 0.808.\nWe used predicted probabilities to find the optimal thresholds, and used this to make the classification.\nThe expected loss: 0.587\n\nsmaller than for the best logit (0.642)\n\nFor the random forest we re-estimate the model on work set, and do prediction on holdout set.\nHoldout RMSE RF is 0.358 (vs 0.366 best logit)\nHoldout AUC is 0.808 vs 0.784 for best logit."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-random-forest-1",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-random-forest-1",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Random Forest",
    "text": "Firm exit case study: Random Forest\n\nNote that finding the optimal threshold is rather important.\nIf used a 0.5 threshold, the expected loss jumped to \\(-1.540\\) vs \\(-0.587\\) for the best threshold model.\nThis is 2.6 times the loss from the optimal threshold.\nThe default option in random forest (and many ML models) for classification is majority voting\nMajority voting is threshold=50%"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-random-forest-2",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-random-forest-2",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Random Forest",
    "text": "Firm exit case study: Random Forest\n\nNote that finding the optimal threshold is rather important.\nUsed a 0.5 threshold, the expected loss jumped to \\(-1.540\\) vs \\(-0.587\\) for the best threshold model.\nThis is 2.6 times the loss from the optimal threshold.\nThe default option in random forest (and many ML models) for classification is majority voting\nMajority voting is threshold=50% - NO!!!!!\nDon’t use it!!!!!!\nUnless loss function: FN=FP"
  },
  {
    "objectID": "rm-data/slides/week12.html#repetition-for-sake-of-argument",
    "href": "rm-data/slides/week12.html#repetition-for-sake-of-argument",
    "title": "Probability and Classification",
    "section": "Repetition for sake of argument",
    "text": "Repetition for sake of argument\nIf you don’t have a loss function, you can’t classify."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-random-forest-3",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-random-forest-3",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Random Forest",
    "text": "Firm exit case study: Random Forest\n\nNo loss function\n\nPredict probabilities\n\nLoss function\n\nPredict probabilities\nTake these probabilities and classify by threshold selected\n\nAlternative: use threshold and change the classification rule\n\nCan be done in caret/ranger"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-comparing-two-thresholds-2",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-comparing-two-thresholds-2",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Comparing two thresholds",
    "text": "Firm exit case study: Comparing two thresholds\n\nPredict exit if probability&gt;10.9%\nExpected loss: \\((1.33 \\times 10 + 45.4 \\times 1)/100 = 0.587\\)\n\n\n\n\n\nactual stay\nactual exit\n\n\n\n\npredicted stay\n33.6%\n1.3%\n\n\npredicted exit\n45.4%\n19.7%"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-summary-of-process-with-rf",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-summary-of-process-with-rf",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Summary of process with RF",
    "text": "Firm exit case study: Summary of process with RF\n\nRun probability forest on work set with 5-CV\nGet average (ie over the folds) RMSE and AUC\nNow use loss function (1,10) and search for best thresholds and expected loss over folds\nShow ROC, loss on fold 5\nOptimal Threshold, average expected loss is calculated\nTake model to holdout and estimate RMSE, AUC and expected loss\\(\\rightarrow\\)what you expect in live data\n+1 Show expected loss with classification RF and default majority voting to compare"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-summary-of-model-for-model-selection",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-summary-of-model-for-model-selection",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Summary of model for model selection",
    "text": "Firm exit case study: Summary of model for model selection\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nPreds\nCoeffs\nRMSE\nAUC\nthreshold\nexp. loss\n\n\n\n\nLogit M1\n11\n12\n0.374\n0.736\n0.089\n0.722\n\n\nLogit M4\n36\n79\n0.362\n0.784\n0.082\n0.619\n\n\nLogit LASSO\n36\n143\n0.362\n0.768\n0.106\n0.642\n\n\nRF probability\n36\nn.a.\n0.354\n0.808\n0.098\n0.587\n\n\n\n\nRMSE, AZC, Threshold, Loss: all 5-fold CV results (averages)."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-business-application",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-business-application",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Business application",
    "text": "Firm exit case study: Business application\n\nConsider this setup\n\nFor each firm we review, we get 1000 euros in revenues,\nLoss function: loans to bad companies = \\(-10,000\\) euros,\nmissed loans to good ones = \\(-1,000\\) euros."
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-business-application-1",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-business-application-1",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Business application",
    "text": "Firm exit case study: Business application\n\nSimplest model 1 classifies with expected loss 0.722 euro per firm, the Random Forest model has 0.587 euro.\nBuilding a better model yields 135 euros higher profit per firm \\((0.722 - 0.587) \\times 1000 = 135\\)\nIf we do 1000 deals, it is 135,000 euros in profit.\nIf a regulator asks for an interpretable model, we shall compare with the logit M4 model and have 103,000 euros in expected profit.\nWhy does it matter?"
  },
  {
    "objectID": "rm-data/slides/week12.html#firm-exit-case-study-business-application-2",
    "href": "rm-data/slides/week12.html#firm-exit-case-study-business-application-2",
    "title": "Probability and Classification",
    "section": "Firm exit case study: Business application",
    "text": "Firm exit case study: Business application\n\nRandom Forest gets us 135K profit, best logit is 103K compared to some simple model.\nWe can take this and compare to development costs\nProfit for good analysis."
  },
  {
    "objectID": "rm-data/slides/week12.html#summary-1",
    "href": "rm-data/slides/week12.html#summary-1",
    "title": "Probability and Classification",
    "section": "Summary",
    "text": "Summary\n\nDecide whether the goal is predicting probabilities or classification.\nThe outcome of prediction with a binary target variable is always the predicted probabilities as a function of predictors.\nWhen our goal is probability prediction, we should find the best model that predicts probabilities by cross-validation + RMSE/AUC.\nWhen our goal is classification, we should find the best model that has the smallest expected loss.\n\nWith formula for threshold or search algorithm\nFinding the optimal classification threshold needs a loss function."
  },
  {
    "objectID": "rm-data/slides/week12.html#summary-2",
    "href": "rm-data/slides/week12.html#summary-2",
    "title": "Probability and Classification",
    "section": "Summary",
    "text": "Summary\n\nWithout a loss function, no classification.\nIf you don’t have one, make it up.\nDon’t rely on default 0.5."
  }
]