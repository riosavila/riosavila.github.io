[
  {
    "objectID": "rm-data/syllabus.html",
    "href": "rm-data/syllabus.html",
    "title": "Research Methods I",
    "section": "",
    "text": "Instructor: Fernando Rios-Avila\nEmail: friosavi@levy.org\nOffice Hours: Wednesdays 1:30pm to 4:00pm. Or by appointment. Other times can be arranged, but will be done remotely.\nCourse Website:\nClass Time: Wednesday, 9:30 am - 12:45 am"
  },
  {
    "objectID": "rm-data/syllabus.html#introduction-to-modern-research-tools",
    "href": "rm-data/syllabus.html#introduction-to-modern-research-tools",
    "title": "Research Methods I",
    "section": "1: Introduction to Modern Research Tools",
    "text": "1: Introduction to Modern Research Tools\n\nCourse overview and expectations.\nIntroduction to GitHub/Github-Desktop for version control and collaboration.\nGetting started with Quarto for reproducible research: RStudio and VSCode.\nOther Tools: Overleaf, Zotero\nData organization and management\nLab: Setting up GitHub, Quarto, VSCode, Zotero, and Overleaf"
  },
  {
    "objectID": "rm-data/syllabus.html#introduction-to-data-analysis",
    "href": "rm-data/syllabus.html#introduction-to-data-analysis",
    "title": "Research Methods I",
    "section": "2. Introduction to Data Analysis",
    "text": "2. Introduction to Data Analysis\n\nIntroduction to data analysis: The Process\nWhat is Data? What types of Data are there?\nData collection methods\nPreparing data for analysis\nTidy data principles\nData cleaning: Missing values, outliers, and errors\nLab: Introduction to Stata, and Quarto\nReadings: Chapter 1 and 2"
  },
  {
    "objectID": "rm-data/syllabus.html#data-exploration",
    "href": "rm-data/syllabus.html#data-exploration",
    "title": "Research Methods I",
    "section": "3: Data Exploration",
    "text": "3: Data Exploration\n\nType of data vs type of analysis\nFrequencies, distributions, and summary statistics\nExploratory data analysis techniques and visualizations\nTheoretical distributions\nComparisons, correlations and conditional distributions\nLatent and observed variables\nReadings: Chapter 3 and 4"
  },
  {
    "objectID": "rm-data/syllabus.html#generalization-from-sample-to-population",
    "href": "rm-data/syllabus.html#generalization-from-sample-to-population",
    "title": "Research Methods I",
    "section": "4: Generalization: From Sample to Population",
    "text": "4: Generalization: From Sample to Population\n\nSampling and generalization\nRepetition and sampling variability\nConfidence intervals and standard errors: The Bootstrap method\nExternal validity\nHypothesis testing principles\nType I and Type II errors\nMultiple Hypothesis Testing and p-hacking\nLab: Conducting hypothesis tests in Stata\nReadings: Chapters 5 and 6"
  },
  {
    "objectID": "rm-data/syllabus.html#and-6-regression-analysis-i-simple-regression",
    "href": "rm-data/syllabus.html#and-6-regression-analysis-i-simple-regression",
    "title": "Research Methods I",
    "section": "5 and 6: Regression Analysis I: Simple Regression",
    "text": "5 and 6: Regression Analysis I: Simple Regression\n\nLinear and non-linear relationships\nLinear Regression: Estimation and interpretation\nCorrelations and coefficients: Searching for causality\nProperties and assumptions of the linear regression model\nTransformations and Semiparametric models\nExtreme values, Influential observations and measurement error\nGeneralizing Results: SE and CI\nTesting Hypotheses\nPresenting Results\nReadings: Chapters 7, 8, and 9\nLab: Simple regression analysis in Stata: Interpretation and presentation of results"
  },
  {
    "objectID": "rm-data/syllabus.html#regression-analysis-ii-multiple-regression",
    "href": "rm-data/syllabus.html#regression-analysis-ii-multiple-regression",
    "title": "Research Methods I",
    "section": "7: Regression Analysis II: Multiple Regression",
    "text": "7: Regression Analysis II: Multiple Regression\n\nMultiple regression basics: Estimation and Inference\nProblems with multiple regression\nNon-linearities, interactions and qualitative variables\nReadings: Chapters 10"
  },
  {
    "objectID": "rm-data/syllabus.html#regression-analysis-iii-modeling-probabilities",
    "href": "rm-data/syllabus.html#regression-analysis-iii-modeling-probabilities",
    "title": "Research Methods I",
    "section": "8: Regression Analysis III: Modeling Probabilities",
    "text": "8: Regression Analysis III: Modeling Probabilities\n\nLinear Probability Model\nNon-linear models: Logit and Probit\nInterpretation and marginal effects\nGoodness of Fit and Predictive Power\nReadings: Chapter 11\nLab: Multiple regression analysis and Probability models"
  },
  {
    "objectID": "rm-data/syllabus.html#time-series-analysis",
    "href": "rm-data/syllabus.html#time-series-analysis",
    "title": "Research Methods I",
    "section": "9: Time Series Analysis",
    "text": "9: Time Series Analysis\n\nIntroduction to time series data\nTrend and seasonality\nStationarity and autocorrelation\nSerial correlation\nReadings: Chapter 12\nLab: Using time series data"
  },
  {
    "objectID": "rm-data/syllabus.html#prediction",
    "href": "rm-data/syllabus.html#prediction",
    "title": "Research Methods I",
    "section": "10: Prediction",
    "text": "10: Prediction\n\nIntroduction to Prediction\nR2 vs AR2 and other measures of fit\nOverfitting and Cross-validation: Finding the right model\nExternal validity and generalization\nReadings: Chapter 13"
  },
  {
    "objectID": "rm-data/syllabus.html#model-building-for-prediction-lasso",
    "href": "rm-data/syllabus.html#model-building-for-prediction-lasso",
    "title": "Research Methods I",
    "section": "11: Model Building for Prediction: LASSO",
    "text": "11: Model Building for Prediction: LASSO\n\nThe Process of Prediction\nHow to choose \\(g(y)\\)\nWorking with \\(X's\\)\nIntroduction to LASSO: Prediction and Diagnosis\nReadings: Chapter 14\nLab: Prediction and LASSO in Stata"
  },
  {
    "objectID": "rm-data/syllabus.html#predicting-probabilities-and-classification",
    "href": "rm-data/syllabus.html#predicting-probabilities-and-classification",
    "title": "Research Methods I",
    "section": "12: Predicting Probabilities and Classification",
    "text": "12: Predicting Probabilities and Classification\n\nPredicting Probabilities and Classification\nClassification, confusion matrices, and ROC curves\nFinding the right threshold\nReadings: Chapter 17"
  },
  {
    "objectID": "rm-data/syllabus.html#forecasting-data",
    "href": "rm-data/syllabus.html#forecasting-data",
    "title": "Research Methods I",
    "section": "13: Forecasting Data",
    "text": "13: Forecasting Data\n\nIntroduction to Forecasting: Predicting the future\nTraining and Testing Data\nTrends, Seasonality, and Cycles\nForecasting with ARIMA\nVAR and External validity\nReadings: Chapter 18"
  },
  {
    "objectID": "rm-data/syllabus.html#term-paper-60-of-final-grade",
    "href": "rm-data/syllabus.html#term-paper-60-of-final-grade",
    "title": "Research Methods I",
    "section": "Term Paper (60% of final grade)",
    "text": "Term Paper (60% of final grade)\nThroughout the semester, students will work on a multi-part research project that applies the techniques learned in class to real-world data. This project will require students to propose a research question, collect and clean data, conduct exploratory and regression analyses, and present their findings.\n\nPart I: Research Proposal (5%, due Week 2)\n\nIntroduction (including research question and motivation)\nProposed data sources: Consider data from the textbook, Kaggle, or other sources\nExpected findings and relevance\nConclusion\nReferences (if any)\nSpecify which software (Stata or R) you plan to use for your analysis\nCreate a GitHub repository for your project and submit the link with your proposal\n\n\n\nPart II: Data Collection and Cleaning (10%, due Week 4)\nWrite a report that includes the following:\n\nDescription of data sources, collection methods, and potential biases\nData overview and preprocessing steps\nDiscussion of issues encountered and solutions\nPreliminary analysis with descriptive statistics and visualizations\nInclude a data dictionary in your GitHub repository\n\nPeer Review: All work will be peer-reviewed by another student in the class. Feedback should be constructive and provide suggestions for improvement.\n\n\nInterim Progress Report (5%, due Week 7)\n\nBrief update on progress, challenges faced, and next steps\nPreliminary visualizations or analyses\nAddress any feedback received from the peer review in Part II\n\n\n\nPart III: Data Analysis (15%, due Week 10)\n\nModel specification discussion\nRegression results presentation\nInterpretation of results\nDiscuss Limitations and and robustness checks\nAddress any feedback received from the peer review in Part II\n\nPeer Review: All work will be peer-reviewed by another student in the class. Feedback should be constructive and provide suggestions for improvement.\n\n\nPart IV: Final Report and Presentation (25%, due Week 14)\n\nComplete research paper including all previous sections plus:\n\nLiterature Review\nRobustness Checks or Sensitivity/Sub-group Analysis\nConclusion\nReferences\nAppendices (if any)\n\n15-minute presentation of your research to the class\n\n\n\nAdditional Requirements\n\nYou should submit a PDF of your report by the deadline\nYour GitHub repository should include with all code, data, and the Quarto document for your report\nAt each stage, you should submit your work to GitHub to follow the progress of your project"
  },
  {
    "objectID": "rm-data/syllabus.html#resources",
    "href": "rm-data/syllabus.html#resources",
    "title": "Research Methods I",
    "section": "Resources",
    "text": "Resources\n\nTextbook: Békés, G., & Kézdi, G. (2021). Data Analysis for Business, Economics, and Policy. Cambridge University Press. There are additional resources available on the book’s website: Data Analysis\nSoftware: There are several statistical packages for analyzing data. In this course, we will be using the software Stata to cover all materials in class. Slides are self-replicable, thus you can copy and paste almost all code provided to replicate the results seen in class. The Institute will be providing you with licenses for Stata/BE for the length of the course.\n\nStata offers many free short webinars and video tutorials that may be useful if you never used Stata before, or even if you have some experience with it. Please see the resources page for more information.\nIf you decide to, you can also use R, Julia, or Python to study and work on the course materials and homework. The book for the class has a repository with all the code in Stata, R and Python. It could be of great advantage to you to learn other languages, as they are widely used in the industry and academia.\nAs with many other skills, the best way to learn is to simply work with the software, work on the book exercises, and ask any questions to me or your classmates when you find a problem you could not find a solution for.\nFor the additional software, please look into Quarto, GitHub, Zotero and VSCode."
  },
  {
    "objectID": "rm-data/index.html",
    "href": "rm-data/index.html",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "",
    "text": "The detailed class syllabus is available here.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#syllabus",
    "href": "rm-data/index.html#syllabus",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "",
    "text": "The detailed class syllabus is available here.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#instructor-information",
    "href": "rm-data/index.html#instructor-information",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Instructor Information",
    "text": "Instructor Information\n\nInstructor: Fernando Rios-Avila\nEmail: friosavi@levy.org\nOffice Hours: Wednesdays 1:30pm to 4:00pm, or by appointment. Other times can be arranged remotely.\nClass Time: Wednesday, 9:30 am - 12:45 pm",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#course-description",
    "href": "rm-data/index.html#course-description",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Course Description",
    "text": "Course Description\nThis course focuses on providing students with the tools and skills necessary to conduct data analysis for economics and policy research. Students will be exposed to the entire process of data analysis, from formulating questions and collecting data to cleaning, exploring, analyzing, and presenting results. The course covers exploratory data analysis, regression analysis, and introduces topics on prediction with machine learning. Students will gain hands-on experience using Stata, with Quarto for reproducible reporting, and GitHub for version control and collaboration.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#course-objectives",
    "href": "rm-data/index.html#course-objectives",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Course Objectives",
    "text": "Course Objectives\nBy the end of this course, students will be able to:\n\nApply advanced data analysis techniques to economic and policy questions.\nUse modern tools such as GitHub and Quarto for research collaboration and reproducibility.\nFormulate research questions and design appropriate data collection methods.\nClean, organize, and explore data using various techniques and visualizations.\nApply regression analysis techniques to analyze relationships between variables.\nUse machine learning methods for prediction and classification tasks.\nImplement data analysis techniques using Stata.\nEffectively communicate research findings through written reports and oral presentations.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#required-textbook",
    "href": "rm-data/index.html#required-textbook",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Required Textbook",
    "text": "Required Textbook\nBékés, G., & Kézdi, G. (2021). Data Analysis for Business, Economics, and Policy. Cambridge University Press.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#software-requirements",
    "href": "rm-data/index.html#software-requirements",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Software Requirements",
    "text": "Software Requirements\n\nStata: A student license will be provided.\nQuarto: Free and open-source software for reproducible research.\nVSCode: Free and open-source code editor.\nGitHub/GitHub-Desktop: Free platform for version control and collaboration.\nZotero: Free reference manager.\n\n\n\n\n\n\n\nImportant\n\n\n\nAll homework assignments are required to be submitted in Quarto format, using GitHub repositories to submit the assignments.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#course-outline",
    "href": "rm-data/index.html#course-outline",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Course Outline",
    "text": "Course Outline\n\nPart I: Introduction to Modern Research Tools\n\nWeek 1: Course Overview and Tools Setup\n\nIntroduction to GitHub and Quarto\nData Organization and Management\nSlides\nHomework 1\n\n\n\n\nPart II: Data Analysis and Exploration\n\nWeek 2: Introduction to Data Analysis\n\nData Collection and Preparation\nTidy Data Principles\nReading: Békés & Kézdi (2021), Chapters 1-2\nSlides\nHomework 2\n\n\n\nWeek 3: Data Exploration\n\nExploratory Data Analysis Techniques\nData Cleaning and Tidy Data Principles\nReading: Békés & Kézdi (2021), Chapters 3-4\nSlides\nHomework 3\n\n\n\n\nPart III: Generalization and Regression Analysis\n\nWeek 4: Generalization: From Sample to Population\n\nSampling and Hypothesis Testing\nConfidence Intervals and Errors\nReading: Békés & Kézdi (2021), Chapters 5-6\nSlides\nHomework 4\n\n\n\nWeeks 5-6: Regression Analysis I: Simple Regression\n\nLinear Regression and Causality\nModel Assumptions and Transformations\nReading: Békés & Kézdi (2021), Chapters 7-9\nSlides\nHomework 5\nHomework 6\n\n\n\nWeek 7: Regression Analysis II: Multiple Regression\n\nEstimation and Inference\nInteractions and Non-linearities\nReading: Békés & Kézdi (2021), Chapter 10\nSlides\nHomework 7\n\n\n\nWeek 8: Regression Analysis III: Modeling Probabilities\n\nLogit and Probit Models\nInterpretation and Predictive Power\nReading: Békés & Kézdi (2021), Chapter 11\nSlides\nHomework 8\n\n\n\n\nPart IV: Advanced Topics\n\nWeek 9: Time Series Analysis\n\nTrend, Seasonality, and Stationarity\nReading: Békés & Kézdi (2021), Chapter 12\nSlides\nHomework 9\n\n\n\nWeek 10: Prediction\n\nModel Fit and Cross-validation\nReading: Békés & Kézdi (2021), Chapter 13\nSlides\nHomework 10\n\n\n\nWeek 11: Model Building for Prediction: LASSO\n\nLASSO for Prediction and Diagnosis\nReading: Békés & Kézdi (2021), Chapter 14\nSlides\nHomework 11\n\n\n\nWeek 12: Predicting Probabilities and Classification\n\nClassification Techniques and ROC Curves\nReading: Békés & Kézdi (2021), Chapter 17\nSlides\nHomework 12\nWeek 13: Forecasting Data\nARIMA and Forecasting Techniques\nReading: Békés & Kézdi (2021), Chapter 18\nSlides\nHomework 13",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#grading-policy",
    "href": "rm-data/index.html#grading-policy",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Grading Policy",
    "text": "Grading Policy\n\nWeekly Quizzes: 10%\nWeekly Problem Sets: 30%\nTerm Paper: 60%\nReferee Reports: 5% (Extra Credit)\n\n\nTerm Paper Schedule (60% of final grade)\n\nPart I: Research Proposal (5%, due Week 2)\nPart II: Data Collection and Cleaning (10%, due Week 4)\nInterim Progress Report (5%, due Week 7)\nPart III: Data Analysis (15%, due Week 10)\nPeer Review Report: (Extra 5% Week 11):\nPart IV: Final Report and Presentation (25%, due Week 14)\n\nComplete research paper including all previous sections plus:\n\nLiterature Review\nRobustness Checks or Sensitivity/Sub-group Analysis\nConclusion\nReferences\nAppendices (if any)\n15-minute presentation of your research to the class\n\n\n\nAdditional Requirements\n\nYou should submit a PDF of your report by the deadline, along with the Github repository link\nYour GitHub repository should include all code, data, and the Quarto document for your report\nAt each stage, you should submit your work to GitHub to follow the progress of your project",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#resources",
    "href": "rm-data/index.html#resources",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Resources",
    "text": "Resources\n\nTextbook: Békés, G., & Kézdi, G. (2021). Data Analysis for Business, Economics, and Policy. Cambridge University Press. Additional resources are available on the book’s website: Data Analysis",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "rm-data/index.html#course-policies",
    "href": "rm-data/index.html#course-policies",
    "title": "Research Methods I: Data Analysis for Economics and Policy",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance: Attendance is highly recommended. Classes will not be recorded, and except for exceptional cases, there will be no online classes.\nLate Assignments: Late assignments will not be accepted unless prior arrangements have been made with the instructor.\nAcademic Integrity: All work submitted must be your own. Plagiarism will not be tolerated and will result in a failing grade for the assignment or course.\nAI Usage: The use of AI in the class is allowed. However, you must disclose any AI tools used in your assignments. AI is a tool you can use to generate ideas, edit your text, provide help with coding, etc. However, it is completely unacceptable to use AI to generate the entire assignment. You will have to be able to explain and defend your work in class.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods I: Data Analysis for Economics and Policy"
    ]
  },
  {
    "objectID": "Stata_Basics.html",
    "href": "Stata_Basics.html",
    "title": "Stata-Basics",
    "section": "",
    "text": "You can use command window to type and excute commands directly into Stata.\nGreat for interactive exploration and analysis…\nBut highly recommended that Final analysis is “always” done in a “do-file”\n\n\n\n\n. display \"Hola\"\nHola\n\n. display \"2+2=\" 2+2\n2+2=4\n\n. display \"The probability that z &gt;1.95 is \" %5.3f 1-normal(1.95)\nThe probability that z &gt;1.95 is 0.026\n\n. \n\n\n\n\n\n\n99% of Stata commands come with Extensive help.\nIf you do not know how to use a command, or about a test just “ask for help”\n\nhelp help\n[R] help -- Display help in Stata\n-------------------------------------------------------------------------------\nStata's help system\n    There are several kinds of help available to the Stata user. For more\n    information, see Advice on getting help.  The information below is\n    technical details about Stata's help command.\n-------------------------------------------------------------------------------\nSyntax\n        help [command_or_topic_name] [, nonew name(viewername)\n              marker(markername)]\nMenu\n    Help &gt; Stata command...\nDescription\n    The help command displays help information about the specified command or\n    topic.  help launches a new Viewer to display help for the specified\n    command or topic or displays help on the console in Stata for\n    Unix(console).  If help is not followed by a command or a topic name,\n    Stata displays advice for using the help system and documentation.\nFor estimation commands, and specialized tests, help even provides links to the manuals.\nThe manuals have extensive detailed information on methods, formulas, references, and examples.\n\n\nOf course there is 1% that is “documented/undocumented” or truly undocumented.\nMost Community-contributed commands also have helpfiles, but are not always fully documented.\nYou could also ask for helps on “topics”: help sample selection\n\n\n\n\n\nStata, for all practical purposes, is self-contained.\n\nYou do not need outside sources to analyze your data, estimate models, create tables, etc.\n\nHowever, many users provide add-ons that may help to make your work “easier”\n\nMain Stata repository : Boston College Statistical Software Components (SSC) archive\n\n\n** For using Wooldridge Book Datasets\nssc install frause, replace \n** For Easiy tables\nnet install estout, replace from(https://raw.githubusercontent.com/benjann/estout/master/)\n** My own installer for extra utilities\nnet install fra, replace from(https://friosavila.github.io/stpackages) \nfra install fra_tools, replace\n\nIf at any point there is code that produces an error, and there is no help, let me know.\n\n\n\n\n\nStata Files have format dta.\nLoading Stata-data into Stata is very easy.\n\nDouble-click (opens a new Stata)\nDrag and Drop into your Stata instance\nLoad it from menu File&gt;open\nor using a do-file or command window\n\nOther Formats required extra work.\n\nUse other software to “translate” it into Stata\nMenu: File&gt;import&gt; many choices\n\n\n\n\n\n\n\n. ** Most Stata example files\n. ** Syntax:   sysuse  [filename], [clear]\n. sysuse dir\n  abortion.dta    citytemp.dta    nlsw88.dta      titanic.dta\n  auto.dta        citytemp4.dta   nlswide1.dta    tsline1.dta\n  auto16.dta      educ99gdp.dta   oaxaca.dta      tsline2.dta\n  auto2.dta       gapminder.dta   pop2000.dta     uslifeexp.dta\n  autornd.dta     gnp96.dta       ri.dta          uslifeexp2.dta\n  avocado.dta     lifeexp.dta     sandstone.dta   voter.dta\n  bplong.dta      mortgages.dta   scorecard.dta   xtline1.dta\n  bpwide.dta      mroz.dta        snow.dta        yule.dta\n  cancer.dta      network1.dta    sp500.dta\n  castle.dta      network1a.dta   surface.dta\n  census.dta      nhefs.dta       texas.dta\n\n. \n\n\n\n\n\n** Web data from Stata\nwebuse \"data-file-address\", clear\n** From other sites\nwebuse set [webaddress]\nwebuse data-file-address, clear \nwebuse set \n** from frause and Wooldrige\nfrause , dir\nfrause wage1, clear\n** from anyadress\nuse \"filename-adress\", clear\nuse \"https://friosavila.github.io/playingwithstata/data2/wage1.dta\", clear\n\n\n\n\n\nsysusewebusefrause\n\n\n\n\n\n. sysuse auto, clear\n(1978 automobile data)\n\n. des\n\nContains data from C:\\Program Files\\Stata17/ado\\base/a/auto.dta\n Observations:            74                  1978 automobile data\n    Variables:            12                  13 Apr 2020 17:45\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nmake            str18   %-18s                 Make and model\nprice           int     %8.0gc                Price\nmpg             int     %8.0g                 Mileage (mpg)\nrep78           int     %8.0g                 Repair record 1978\nheadroom        float   %6.1f                 Headroom (in.)\ntrunk           int     %8.0g                 Trunk space (cu. ft.)\nweight          int     %8.0gc                Weight (lbs.)\nlength          int     %8.0g                 Length (in.)\nturn            int     %8.0g                 Turn circle (ft.)\ndisplacement    int     %8.0g                 Displacement (cu. in.)\ngear_ratio      float   %6.2f                 Gear ratio\nforeign         byte    %8.0g      origin     Car origin\n-------------------------------------------------------------------------------\nSorted by: foreign\n\n. list in 1/3\n\n     +------------------------------------------------------------------------+\n  1. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Concord | 4,099 |  22 |     3 |      2.5 |    11 |  2,930 |    186 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       40     |          121     |         3.58     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n     +------------------------------------------------------------------------+\n  2. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Pacer   | 4,749 |  17 |     3 |      3.0 |    11 |  3,350 |    173 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       40     |          258     |         2.53     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n     +------------------------------------------------------------------------+\n  3. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Spirit  | 3,799 |  22 |     . |      3.0 |    12 |  2,640 |    168 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       35     |          121     |         3.08     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n. \n\n\n\n\n\n\n\n. webuse smoking, clear\n(Smoking and mortality data)\n\n. des\n\nContains data from https://www.stata-press.com/data/r17/smoking.dta\n Observations:        17,260                  Smoking and mortality data\n    Variables:            16                  27 Dec 2020 15:21\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nserno           int     %10.0g                Serial number\ncigs            byte    %10.0g                Daily cigarette consumption\nsysbp           int     %10.0g                Systolic blood pressure (mm Hg)\ndiasbp          float   %10.0g                Diastolic blood pressure (mm Hg)\nmap             float   %9.0g                 Mean arterial pressure (mm Hg)\nage             byte    %10.0g                Age (years)\nht              double  %10.0g                Height (cm)\nwt              double  %10.0g                Weight (kg)\nchol            double  %10.0g                Cholesterol (mmol/l)\ngradd1          byte    %8.0g                 Job grade 1\ngradd2          byte    %8.0g                 Job grade 2\ngradd3          byte    %8.0g                 Job grade 3\nall10           byte    %8.0g                 Ten year mortality\npyar            double  %10.0g                Years of follow-up\nchd             byte    %10.0g                Censoring (0 = censored, 1 = died\n                                                of CHD)\njobgrade        byte    %10.0g                Job grade\n-------------------------------------------------------------------------------\nSorted by: \n\n. list cigs map age ht gradd1 in 1/3\n\n     +-----------------------------------------+\n     | cigs        map   age       ht   gradd1 |\n     |-----------------------------------------|\n  1. |    0         97    46   154.94        0 |\n  2. |    0   97.66666    55   179.07        1 |\n  3. |    0         82    43   173.99        1 |\n     +-----------------------------------------+\n\n. \n\n\n\n\n\n\n\n. frause smoking, clear\n(A.C.Cameron & P.K.Trivedi (2022): Microeconometrics Using Stata, 2e)\n\n. des\n\nContains data from https://friosavila.github.io/playingwithstata/data2/smoking.\n&gt; dta\n Observations:         1,209                  A.C.Cameron & P.K.Trivedi\n                                                (2022): Microeconometrics Using\n                                                Stata, 2e\n    Variables:             7                  5 Oct 2022 13:08\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nstate           byte    %14.0g     state      State no\nyear            int     %9.0g                 Year\ncigsale         float   %9.0g                 Cigarette sale per capita (in\n                                                packs)\nlnincome        float   %9.0g                 Log state per capita GDP\nbeer            float   %9.0g                 Beer consumption per capita\nage15to24       float   %9.0g                 Percent of state population aged\n                                                15–24 years\nretprice        float   %9.0g                 Retail price of cigarettes\n-------------------------------------------------------------------------------\nSorted by: year\n\n. list in 1/3\n\n     +-----------------------------------------------------------------------+\n     |        state   year   cigsale   lnincome   beer   age15~24   retprice |\n     |-----------------------------------------------------------------------|\n  1. | Rhode Island   1970     123.9          .      .   .1831579       39.3 |\n  2. |    Tennessee   1970      99.8          .      .   .1780438       39.9 |\n  3. |      Indiana   1970     134.6          .      .   .1765159       30.6 |\n     +-----------------------------------------------------------------------+\n\n. \n\n\n\n\n\n\n\n\n\nSummary Statistics are essential before starting basic analysis. Stata gives you many options. Although not all of them are easy to export.\n\n\n\n\n. frause oaxaca, clear\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n. *summarize [varlist] [if] [in] [weight] [, options]\n. summarize if female==1, sep(0)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      lnwage |        683    3.266761    .5700236    .507681   5.259097\n        educ |        888    11.06025     2.26024          5       17.5\n       exper |        683    12.13769     8.32663          0   41.91667\n      tenure |        683    6.605051    6.727475          0   40.08333\n        isco |        683    3.920937    1.762983          1          9\n      female |        888           1           0          1          1\n         lfp |        888    .7691441    .4216179          0          1\n         age |        888    39.88401    10.72665         18         62\n       agesq |        888    1705.666    879.4667        324       3844\n      single |        888    .3018018    .4592984          0          1\n     married |        888    .5236486    .4997219          0          1\n    divorced |        888    .1745495    .3797953          0          1\n       kids6 |        888     .286036    .6726639          0          4\n     kids714 |        888        .375    .7538888          0          4\n          wt |        888    .9883364    .4030975   .5302977   3.181786\n\n. \n\n\n\n\n\n. *   tabstat varlist [if] [in] [weight] [, options]\n. tabstat educ exper tenure age married, by(female)\n\nSummary statistics: Mean\nGroup variable: female (sex of respondent (1=female))\n\n  female |      educ     exper    tenure       age   married\n---------+--------------------------------------------------\n       0 |   11.8004  14.07684  9.003107  38.51647  .5230567\n       1 |  11.06025  12.13769  6.605051  39.88401  .5236486\n---------+--------------------------------------------------\n   Total |  11.40134  13.15324  7.860937  39.25379  .5233758\n------------------------------------------------------------\n\n. tabstat educ exper tenure , by(female) stats(p10 p50 p90)\n\nSummary statistics: p10, p50, p90\nGroup variable: female (sex of respondent (1=female))\n\n  female |      educ     exper    tenure\n---------+------------------------------\n       0 |      10.5  1.416667  .4166667\n         |      10.5  11.08333      6.25\n         |      17.5  31.33333     23.25\n---------+------------------------------\n       1 |         9      2.25  .3333333\n         |      10.5     10.75      4.25\n         |      12.5    23.375     15.75\n---------+------------------------------\n   Total |         9  1.833333  .4166667\n         |      10.5  10.91667  5.291667\n         |        15  28.08333  19.41667\n----------------------------------------\n\n. \n\n\n\n\n\n. ssc install table1\nchecking table1 consistency and verifying not already installed...\nall files already exist and are up to date.\n\n. * see help table1\n. table1, by(female) vars(lnwage contn %3.2f \\ age contn %2.1f \\ married bin)\n  +--------------------------------------------------------------------+\n  | Factor                         female = 0    female = 1    p-value |\n  |--------------------------------------------------------------------|\n  | N                              759           888                   |\n  |--------------------------------------------------------------------|\n  | log hourly wages, mean (SD)    3.44 (0.48)   3.27 (0.57)    &lt;0.001 |\n  |--------------------------------------------------------------------|\n  | age of respondent, mean (SD)   38.5 (11.3)   39.9 (10.7)     0.012 |\n  |--------------------------------------------------------------------|\n  | married                        397 (52.3%)   465 (52.4%)      0.98 |\n  +--------------------------------------------------------------------+\n\n. qui:table1, by(female) vars(lnwage contn %3.2f \\ age contn %2.1f \\ married bi\n&gt; n) saving(m1.xls)\nfile m1.xls already exists\nr(602);\n\n\nYou can see the file here\n\nThere are other options from Stata as well. see help dtable and help table.\nOr you could construct some yourself with the help of estout and esttab.\nSee here for a quick guide on tables.\n\n\n\n\n\nTwo main commands:\n\ngenerate (or gen for short): Creates new variables as a function of others in the data. One can apply system functions. Example:\n\ngen var1 = 1\ngen var2 = _n\ngen wage = exp(lnwage)\ngen age_educ = age * educ\n\nreplace: replaces values in an already existing variable.\n\nreplace wage = 0 if wage==.\nreplace age_educ = . if female==1\n\negen: Advanced variable generating function. It applies a single function to a variable or list of variables to create a third one.\n\negen wage_mean=mean(exp(lnwage)), by(female)\negen wage_p10=pctile(lnwage), by(female) p(10)\n\n\nTo delete a variable, you can use drop varname/varlist or drop2 varname/varlist\n\ndrop is the official. Stops if the variable does not exist.\ndrop2 an addon. Will still work even if a variable name does not exist.\nRequires using full variable name.\n\n\n\n\n\n. gen var1 = exp(lnwage)\n(213 missing values generated)\n\n. gen xar2 = exp(lnwage)+married\n(213 missing values generated)\n\n. drop x\n\n. des var1 xar2\nvariable xar2 not found\nr(111);\n\n\n\n\n\n. gen xar2 = exp(lnwage)+married\n(213 missing values generated)\n\n. drop2 x\nvariable x not found\n\n. \n\n\n\n\n\n\nStata is case sensitive.\n\nYou can create variables with names one, One, OnE, ONE, etc.\n“file addresses” and commands are also case sensitive\n\nIn Stata, variable names cannot can only start with a letter or “_“. Otherwise, it will give you an error.\nOnce variables are created, you could “label” them\nlabel var variable_name \"Description\"\nYou can name other components of a dataset as well. See help label\n\n\n\n\n\nStata can create figures and plots for data exploration\n\n\n\n\n\n\n. webuse dui, clear\n(Fictional data on monthly drunk driving citations)\n\n. scatter citations fines\n\n. \n\n\n\n\n\n\n\n\n\n\n\n\n. two (scatter citations fines if csize==1)  ///\n&gt;     (scatter citations fines if csize==2), ///\n&gt;     legend(order(1 \"Small\" 2 \"Medium\"))\n\n. \n\n\n\n\n\n\n\n\n\n\nThe limitation. User written plotting commands do not interact well with Official plotting commands.\n\n\n\n\n\n\nCommand Window is effective to provide interactive analysis\nAt the end of your session, you can recover everything you did, clicking on the History Section, and save everythig, or just specific commands.\nThe best approach, however, is to ALWAYS use a do-file.\nFirst of all: Create a working directory. A folder in your computer that will hold your project, work, paper, homework, etc. (highly recommended)\nCreate a dofile: For simple projects a single file will suffice, but multiple may be needed for larger ones.\nTo start a dofile, simply type doedit \"filename\" in your command window.\n\nIf file exists in your “working directory” (type cd to see where you “are”), it will open it.\nOtherwise, a new file will be created\n\ndo-files are the best approach to save your work, and keep track of your analysis.\nGeneral Suggestion: Allways add comments to it, to know what you are doing\n\n\n*  You can always Start a command like this\n// Or like this\n/*\nBut you can always add a large comment using \"/*\" to start\nand \"*/\" to end it\n*/\n\n/* You could also add comments at the end of a command */\nsysuse auto, clear  // Loading Auto Dataset, after \"clearing\" the one currently in memory\n\n// Or as I did before, break a long command in various lines using \"///\"\n// Comments after \"///\" are possible\nregress price     /// Dep variable\n        mpg       /// indep variable \n        i.foreign, /// Foreign Dummy\n        robust     // Request Robust Standard errors.\n        \n** Last one has only two \"/\", because line ends there\n\n(1978 automobile data)\n\nLinear regression                               Number of obs     =         74\n                                                F(2, 71)          =      12.72\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.2838\n                                                Root MSE          =     2530.9\n\n------------------------------------------------------------------------------\n             |               Robust\n       price | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -294.1955   60.33645    -4.88   0.000     -414.503   -173.8881\n             |\n     foreign |\n    Foreign  |   1767.292   607.7385     2.91   0.005     555.4961    2979.088\n       _cons |   11905.42   1362.547     8.74   0.000     9188.573    14622.26\n------------------------------------------------------------------------------\n\n\n\n\n\n\nMost commands in Stata have the following syntax:\n\n[by varlist:/prefix] command [varlist]  [if exp] [in range] [weight] [using filename] [,options]\n\neverything in [] are optional.\n[by varlist:/prefix]: by is used to execute the command by groups.\nprefix to request additional manipulation (advanced use)\ncommand: The command itself that will process the data\nvarlist: For Estimation commands include the dependent (first) and independent variables (everything else)\n[if exp] [in range]: To restrict samples\n[weight]: Request the use of weights ie: [fw = wgt_var] or [pw = wgt_var]\n[using filename]: Some commands allow you to use this to work with not-yet loaded datasets or files.\n[options]: Options requesting specific behaivior, statistics, etc\n\n\n\n\n\nUse github, as an additional data-repository\nCombine Stata, python and nbstat to create Jupyter notebooks.\nYou can also use Quarto to create full dynamic reports."
  },
  {
    "objectID": "Stata_Basics.html#command-window",
    "href": "Stata_Basics.html#command-window",
    "title": "Stata-Basics",
    "section": "",
    "text": "You can use command window to type and excute commands directly into Stata.\nGreat for interactive exploration and analysis…\nBut highly recommended that Final analysis is “always” done in a “do-file”\n\n\n\n\n. display \"Hola\"\nHola\n\n. display \"2+2=\" 2+2\n2+2=4\n\n. display \"The probability that z &gt;1.95 is \" %5.3f 1-normal(1.95)\nThe probability that z &gt;1.95 is 0.026\n\n."
  },
  {
    "objectID": "Stata_Basics.html#help",
    "href": "Stata_Basics.html#help",
    "title": "Stata-Basics",
    "section": "",
    "text": "99% of Stata commands come with Extensive help.\nIf you do not know how to use a command, or about a test just “ask for help”\n\nhelp help\n[R] help -- Display help in Stata\n-------------------------------------------------------------------------------\nStata's help system\n    There are several kinds of help available to the Stata user. For more\n    information, see Advice on getting help.  The information below is\n    technical details about Stata's help command.\n-------------------------------------------------------------------------------\nSyntax\n        help [command_or_topic_name] [, nonew name(viewername)\n              marker(markername)]\nMenu\n    Help &gt; Stata command...\nDescription\n    The help command displays help information about the specified command or\n    topic.  help launches a new Viewer to display help for the specified\n    command or topic or displays help on the console in Stata for\n    Unix(console).  If help is not followed by a command or a topic name,\n    Stata displays advice for using the help system and documentation.\nFor estimation commands, and specialized tests, help even provides links to the manuals.\nThe manuals have extensive detailed information on methods, formulas, references, and examples.\n\n\nOf course there is 1% that is “documented/undocumented” or truly undocumented.\nMost Community-contributed commands also have helpfiles, but are not always fully documented.\nYou could also ask for helps on “topics”: help sample selection"
  },
  {
    "objectID": "Stata_Basics.html#installing-programs",
    "href": "Stata_Basics.html#installing-programs",
    "title": "Stata-Basics",
    "section": "",
    "text": "Stata, for all practical purposes, is self-contained.\n\nYou do not need outside sources to analyze your data, estimate models, create tables, etc.\n\nHowever, many users provide add-ons that may help to make your work “easier”\n\nMain Stata repository : Boston College Statistical Software Components (SSC) archive\n\n\n** For using Wooldridge Book Datasets\nssc install frause, replace \n** For Easiy tables\nnet install estout, replace from(https://raw.githubusercontent.com/benjann/estout/master/)\n** My own installer for extra utilities\nnet install fra, replace from(https://friosavila.github.io/stpackages) \nfra install fra_tools, replace\n\nIf at any point there is code that produces an error, and there is no help, let me know."
  },
  {
    "objectID": "Stata_Basics.html#loading-data",
    "href": "Stata_Basics.html#loading-data",
    "title": "Stata-Basics",
    "section": "",
    "text": "Stata Files have format dta.\nLoading Stata-data into Stata is very easy.\n\nDouble-click (opens a new Stata)\nDrag and Drop into your Stata instance\nLoad it from menu File&gt;open\nor using a do-file or command window\n\nOther Formats required extra work.\n\nUse other software to “translate” it into Stata\nMenu: File&gt;import&gt; many choices\n\n\n\n\n\n\n\n. ** Most Stata example files\n. ** Syntax:   sysuse  [filename], [clear]\n. sysuse dir\n  abortion.dta    citytemp.dta    nlsw88.dta      titanic.dta\n  auto.dta        citytemp4.dta   nlswide1.dta    tsline1.dta\n  auto16.dta      educ99gdp.dta   oaxaca.dta      tsline2.dta\n  auto2.dta       gapminder.dta   pop2000.dta     uslifeexp.dta\n  autornd.dta     gnp96.dta       ri.dta          uslifeexp2.dta\n  avocado.dta     lifeexp.dta     sandstone.dta   voter.dta\n  bplong.dta      mortgages.dta   scorecard.dta   xtline1.dta\n  bpwide.dta      mroz.dta        snow.dta        yule.dta\n  cancer.dta      network1.dta    sp500.dta\n  castle.dta      network1a.dta   surface.dta\n  census.dta      nhefs.dta       texas.dta\n\n. \n\n\n\n\n\n** Web data from Stata\nwebuse \"data-file-address\", clear\n** From other sites\nwebuse set [webaddress]\nwebuse data-file-address, clear \nwebuse set \n** from frause and Wooldrige\nfrause , dir\nfrause wage1, clear\n** from anyadress\nuse \"filename-adress\", clear\nuse \"https://friosavila.github.io/playingwithstata/data2/wage1.dta\", clear"
  },
  {
    "objectID": "Stata_Basics.html#basic-data-description",
    "href": "Stata_Basics.html#basic-data-description",
    "title": "Stata-Basics",
    "section": "",
    "text": "sysusewebusefrause\n\n\n\n\n\n. sysuse auto, clear\n(1978 automobile data)\n\n. des\n\nContains data from C:\\Program Files\\Stata17/ado\\base/a/auto.dta\n Observations:            74                  1978 automobile data\n    Variables:            12                  13 Apr 2020 17:45\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nmake            str18   %-18s                 Make and model\nprice           int     %8.0gc                Price\nmpg             int     %8.0g                 Mileage (mpg)\nrep78           int     %8.0g                 Repair record 1978\nheadroom        float   %6.1f                 Headroom (in.)\ntrunk           int     %8.0g                 Trunk space (cu. ft.)\nweight          int     %8.0gc                Weight (lbs.)\nlength          int     %8.0g                 Length (in.)\nturn            int     %8.0g                 Turn circle (ft.)\ndisplacement    int     %8.0g                 Displacement (cu. in.)\ngear_ratio      float   %6.2f                 Gear ratio\nforeign         byte    %8.0g      origin     Car origin\n-------------------------------------------------------------------------------\nSorted by: foreign\n\n. list in 1/3\n\n     +------------------------------------------------------------------------+\n  1. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Concord | 4,099 |  22 |     3 |      2.5 |    11 |  2,930 |    186 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       40     |          121     |         3.58     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n     +------------------------------------------------------------------------+\n  2. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Pacer   | 4,749 |  17 |     3 |      3.0 |    11 |  3,350 |    173 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       40     |          258     |         2.53     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n     +------------------------------------------------------------------------+\n  3. | make        | price | mpg | rep78 | headroom | trunk | weight | length |\n     | AMC Spirit  | 3,799 |  22 |     . |      3.0 |    12 |  2,640 |    168 |\n     |------------------------------------------------------------------------|\n     |     turn     |     displa~t     |     gear_r~o     |      foreign      |\n     |       35     |          121     |         3.08     |     Domestic      |\n     +------------------------------------------------------------------------+\n\n. \n\n\n\n\n\n\n\n. webuse smoking, clear\n(Smoking and mortality data)\n\n. des\n\nContains data from https://www.stata-press.com/data/r17/smoking.dta\n Observations:        17,260                  Smoking and mortality data\n    Variables:            16                  27 Dec 2020 15:21\n                                              (_dta has notes)\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nserno           int     %10.0g                Serial number\ncigs            byte    %10.0g                Daily cigarette consumption\nsysbp           int     %10.0g                Systolic blood pressure (mm Hg)\ndiasbp          float   %10.0g                Diastolic blood pressure (mm Hg)\nmap             float   %9.0g                 Mean arterial pressure (mm Hg)\nage             byte    %10.0g                Age (years)\nht              double  %10.0g                Height (cm)\nwt              double  %10.0g                Weight (kg)\nchol            double  %10.0g                Cholesterol (mmol/l)\ngradd1          byte    %8.0g                 Job grade 1\ngradd2          byte    %8.0g                 Job grade 2\ngradd3          byte    %8.0g                 Job grade 3\nall10           byte    %8.0g                 Ten year mortality\npyar            double  %10.0g                Years of follow-up\nchd             byte    %10.0g                Censoring (0 = censored, 1 = died\n                                                of CHD)\njobgrade        byte    %10.0g                Job grade\n-------------------------------------------------------------------------------\nSorted by: \n\n. list cigs map age ht gradd1 in 1/3\n\n     +-----------------------------------------+\n     | cigs        map   age       ht   gradd1 |\n     |-----------------------------------------|\n  1. |    0         97    46   154.94        0 |\n  2. |    0   97.66666    55   179.07        1 |\n  3. |    0         82    43   173.99        1 |\n     +-----------------------------------------+\n\n. \n\n\n\n\n\n\n\n. frause smoking, clear\n(A.C.Cameron & P.K.Trivedi (2022): Microeconometrics Using Stata, 2e)\n\n. des\n\nContains data from https://friosavila.github.io/playingwithstata/data2/smoking.\n&gt; dta\n Observations:         1,209                  A.C.Cameron & P.K.Trivedi\n                                                (2022): Microeconometrics Using\n                                                Stata, 2e\n    Variables:             7                  5 Oct 2022 13:08\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nstate           byte    %14.0g     state      State no\nyear            int     %9.0g                 Year\ncigsale         float   %9.0g                 Cigarette sale per capita (in\n                                                packs)\nlnincome        float   %9.0g                 Log state per capita GDP\nbeer            float   %9.0g                 Beer consumption per capita\nage15to24       float   %9.0g                 Percent of state population aged\n                                                15–24 years\nretprice        float   %9.0g                 Retail price of cigarettes\n-------------------------------------------------------------------------------\nSorted by: year\n\n. list in 1/3\n\n     +-----------------------------------------------------------------------+\n     |        state   year   cigsale   lnincome   beer   age15~24   retprice |\n     |-----------------------------------------------------------------------|\n  1. | Rhode Island   1970     123.9          .      .   .1831579       39.3 |\n  2. |    Tennessee   1970      99.8          .      .   .1780438       39.9 |\n  3. |      Indiana   1970     134.6          .      .   .1765159       30.6 |\n     +-----------------------------------------------------------------------+\n\n."
  },
  {
    "objectID": "Stata_Basics.html#summary-statistics",
    "href": "Stata_Basics.html#summary-statistics",
    "title": "Stata-Basics",
    "section": "",
    "text": "Summary Statistics are essential before starting basic analysis. Stata gives you many options. Although not all of them are easy to export.\n\n\n\n\n. frause oaxaca, clear\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n. *summarize [varlist] [if] [in] [weight] [, options]\n. summarize if female==1, sep(0)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      lnwage |        683    3.266761    .5700236    .507681   5.259097\n        educ |        888    11.06025     2.26024          5       17.5\n       exper |        683    12.13769     8.32663          0   41.91667\n      tenure |        683    6.605051    6.727475          0   40.08333\n        isco |        683    3.920937    1.762983          1          9\n      female |        888           1           0          1          1\n         lfp |        888    .7691441    .4216179          0          1\n         age |        888    39.88401    10.72665         18         62\n       agesq |        888    1705.666    879.4667        324       3844\n      single |        888    .3018018    .4592984          0          1\n     married |        888    .5236486    .4997219          0          1\n    divorced |        888    .1745495    .3797953          0          1\n       kids6 |        888     .286036    .6726639          0          4\n     kids714 |        888        .375    .7538888          0          4\n          wt |        888    .9883364    .4030975   .5302977   3.181786\n\n. \n\n\n\n\n\n. *   tabstat varlist [if] [in] [weight] [, options]\n. tabstat educ exper tenure age married, by(female)\n\nSummary statistics: Mean\nGroup variable: female (sex of respondent (1=female))\n\n  female |      educ     exper    tenure       age   married\n---------+--------------------------------------------------\n       0 |   11.8004  14.07684  9.003107  38.51647  .5230567\n       1 |  11.06025  12.13769  6.605051  39.88401  .5236486\n---------+--------------------------------------------------\n   Total |  11.40134  13.15324  7.860937  39.25379  .5233758\n------------------------------------------------------------\n\n. tabstat educ exper tenure , by(female) stats(p10 p50 p90)\n\nSummary statistics: p10, p50, p90\nGroup variable: female (sex of respondent (1=female))\n\n  female |      educ     exper    tenure\n---------+------------------------------\n       0 |      10.5  1.416667  .4166667\n         |      10.5  11.08333      6.25\n         |      17.5  31.33333     23.25\n---------+------------------------------\n       1 |         9      2.25  .3333333\n         |      10.5     10.75      4.25\n         |      12.5    23.375     15.75\n---------+------------------------------\n   Total |         9  1.833333  .4166667\n         |      10.5  10.91667  5.291667\n         |        15  28.08333  19.41667\n----------------------------------------\n\n. \n\n\n\n\n\n. ssc install table1\nchecking table1 consistency and verifying not already installed...\nall files already exist and are up to date.\n\n. * see help table1\n. table1, by(female) vars(lnwage contn %3.2f \\ age contn %2.1f \\ married bin)\n  +--------------------------------------------------------------------+\n  | Factor                         female = 0    female = 1    p-value |\n  |--------------------------------------------------------------------|\n  | N                              759           888                   |\n  |--------------------------------------------------------------------|\n  | log hourly wages, mean (SD)    3.44 (0.48)   3.27 (0.57)    &lt;0.001 |\n  |--------------------------------------------------------------------|\n  | age of respondent, mean (SD)   38.5 (11.3)   39.9 (10.7)     0.012 |\n  |--------------------------------------------------------------------|\n  | married                        397 (52.3%)   465 (52.4%)      0.98 |\n  +--------------------------------------------------------------------+\n\n. qui:table1, by(female) vars(lnwage contn %3.2f \\ age contn %2.1f \\ married bi\n&gt; n) saving(m1.xls)\nfile m1.xls already exists\nr(602);\n\n\nYou can see the file here\n\nThere are other options from Stata as well. see help dtable and help table.\nOr you could construct some yourself with the help of estout and esttab.\nSee here for a quick guide on tables."
  },
  {
    "objectID": "Stata_Basics.html#creating-variables",
    "href": "Stata_Basics.html#creating-variables",
    "title": "Stata-Basics",
    "section": "",
    "text": "Two main commands:\n\ngenerate (or gen for short): Creates new variables as a function of others in the data. One can apply system functions. Example:\n\ngen var1 = 1\ngen var2 = _n\ngen wage = exp(lnwage)\ngen age_educ = age * educ\n\nreplace: replaces values in an already existing variable.\n\nreplace wage = 0 if wage==.\nreplace age_educ = . if female==1\n\negen: Advanced variable generating function. It applies a single function to a variable or list of variables to create a third one.\n\negen wage_mean=mean(exp(lnwage)), by(female)\negen wage_p10=pctile(lnwage), by(female) p(10)\n\n\nTo delete a variable, you can use drop varname/varlist or drop2 varname/varlist\n\ndrop is the official. Stops if the variable does not exist.\ndrop2 an addon. Will still work even if a variable name does not exist.\nRequires using full variable name.\n\n\n\n\n\n. gen var1 = exp(lnwage)\n(213 missing values generated)\n\n. gen xar2 = exp(lnwage)+married\n(213 missing values generated)\n\n. drop x\n\n. des var1 xar2\nvariable xar2 not found\nr(111);\n\n\n\n\n\n. gen xar2 = exp(lnwage)+married\n(213 missing values generated)\n\n. drop2 x\nvariable x not found\n\n."
  },
  {
    "objectID": "Stata_Basics.html#variables-management",
    "href": "Stata_Basics.html#variables-management",
    "title": "Stata-Basics",
    "section": "",
    "text": "Stata is case sensitive.\n\nYou can create variables with names one, One, OnE, ONE, etc.\n“file addresses” and commands are also case sensitive\n\nIn Stata, variable names cannot can only start with a letter or “_“. Otherwise, it will give you an error.\nOnce variables are created, you could “label” them\nlabel var variable_name \"Description\"\nYou can name other components of a dataset as well. See help label"
  },
  {
    "objectID": "Stata_Basics.html#plots-in-stata",
    "href": "Stata_Basics.html#plots-in-stata",
    "title": "Stata-Basics",
    "section": "",
    "text": "Stata can create figures and plots for data exploration\n\n\n\n\n\n\n. webuse dui, clear\n(Fictional data on monthly drunk driving citations)\n\n. scatter citations fines\n\n. \n\n\n\n\n\n\n\n\n\n\n\n\n. two (scatter citations fines if csize==1)  ///\n&gt;     (scatter citations fines if csize==2), ///\n&gt;     legend(order(1 \"Small\" 2 \"Medium\"))\n\n. \n\n\n\n\n\n\n\n\n\n\nThe limitation. User written plotting commands do not interact well with Official plotting commands."
  },
  {
    "objectID": "Stata_Basics.html#saving-your-work",
    "href": "Stata_Basics.html#saving-your-work",
    "title": "Stata-Basics",
    "section": "",
    "text": "Command Window is effective to provide interactive analysis\nAt the end of your session, you can recover everything you did, clicking on the History Section, and save everythig, or just specific commands.\nThe best approach, however, is to ALWAYS use a do-file.\nFirst of all: Create a working directory. A folder in your computer that will hold your project, work, paper, homework, etc. (highly recommended)\nCreate a dofile: For simple projects a single file will suffice, but multiple may be needed for larger ones.\nTo start a dofile, simply type doedit \"filename\" in your command window.\n\nIf file exists in your “working directory” (type cd to see where you “are”), it will open it.\nOtherwise, a new file will be created\n\ndo-files are the best approach to save your work, and keep track of your analysis.\nGeneral Suggestion: Allways add comments to it, to know what you are doing\n\n\n*  You can always Start a command like this\n// Or like this\n/*\nBut you can always add a large comment using \"/*\" to start\nand \"*/\" to end it\n*/\n\n/* You could also add comments at the end of a command */\nsysuse auto, clear  // Loading Auto Dataset, after \"clearing\" the one currently in memory\n\n// Or as I did before, break a long command in various lines using \"///\"\n// Comments after \"///\" are possible\nregress price     /// Dep variable\n        mpg       /// indep variable \n        i.foreign, /// Foreign Dummy\n        robust     // Request Robust Standard errors.\n        \n** Last one has only two \"/\", because line ends there\n\n(1978 automobile data)\n\nLinear regression                               Number of obs     =         74\n                                                F(2, 71)          =      12.72\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.2838\n                                                Root MSE          =     2530.9\n\n------------------------------------------------------------------------------\n             |               Robust\n       price | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         mpg |  -294.1955   60.33645    -4.88   0.000     -414.503   -173.8881\n             |\n     foreign |\n    Foreign  |   1767.292   607.7385     2.91   0.005     555.4961    2979.088\n       _cons |   11905.42   1362.547     8.74   0.000     9188.573    14622.26\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "Stata_Basics.html#estimation-commands",
    "href": "Stata_Basics.html#estimation-commands",
    "title": "Stata-Basics",
    "section": "",
    "text": "Most commands in Stata have the following syntax:\n\n[by varlist:/prefix] command [varlist]  [if exp] [in range] [weight] [using filename] [,options]\n\neverything in [] are optional.\n[by varlist:/prefix]: by is used to execute the command by groups.\nprefix to request additional manipulation (advanced use)\ncommand: The command itself that will process the data\nvarlist: For Estimation commands include the dependent (first) and independent variables (everything else)\n[if exp] [in range]: To restrict samples\n[weight]: Request the use of weights ie: [fw = wgt_var] or [pw = wgt_var]\n[using filename]: Some commands allow you to use this to work with not-yet loaded datasets or files.\n[options]: Options requesting specific behaivior, statistics, etc"
  },
  {
    "objectID": "Stata_Basics.html#adv-options-for-saving-work.",
    "href": "Stata_Basics.html#adv-options-for-saving-work.",
    "title": "Stata-Basics",
    "section": "",
    "text": "Use github, as an additional data-repository\nCombine Stata, python and nbstat to create Jupyter notebooks.\nYou can also use Quarto to create full dynamic reports."
  },
  {
    "objectID": "rmethods2/session_7.html",
    "href": "rmethods2/session_7.html",
    "title": "Research Methods II",
    "section": "",
    "text": "Monte Carlo simulation are a generic name given to methods that use random numbers to simulate a process.\nIn econometrics, Monte Carlo methods are used to study the properties of estimators, and to evaluate the performance of statistical tests.\nThis can be a useful tool to understand some of the properties of estimators, or even problems related to violations of assumptions.\nIt can also be used to evaluate the performance of estimators in finite samples, and to compare different estimators.\n\n\n\n\n\nWhich of this estimators is more robust and efficient, when samples are small ?\nLets setup a program that would simulate this:\n\n\n// define a program\ncapture program drop mean_vs_median\nprogram define mean_vs_median, eclass\n  syntax, [nobs(int 100)]\n  clear\n  ** Set  # of obs\n  set obs `nobs'\n  ** Generate a random variable\n  gen x = rnormal(0,1)\n  ** Calculate mean and median\n  qui:sum x,d\n  ** Store results\n  matrix b = r(mean), r(p50)\n  ** post results\n  matrix colname b = \"mean\" \"median\"\n  ereturn post b\nend\nmean_vs_median\nereturn display\n\n\n\n\n\nNumber of observations (_N) was 0, now 100.\n------------------------------------------------------------------------------\n             | Coefficient\n-------------+----------------------------------------------------------------\n        mean |  -.1445693\n      median |  -.1970271\n------------------------------------------------------------------------------\n\n\nNow that the program is SET, lets run it 1000 times:\n\nset seed 101\nsimulate, reps(1000): mean_vs_median, nobs(500)\nsum\n\n\n      Command: mean_vs_median, nobs(500)\n\nSimulations (1,000): .........10.........20.........30.........40.........50...\n&gt; ......60.........70.........80.........90.........100.........110.........120\n&gt; .........130.........140.........150.........160.........170.........180.....\n&gt; ....190.........200.........210.........220.........230.........240.........2\n&gt; 50.........260.........270.........280.........290.........300.........310...\n&gt; ......320.........330.........340.........350.........360.........370........\n&gt; .380.........390.........400.........410.........420.........430.........440.\n&gt; ........450.........460.........470.........480.........490.........500......\n&gt; ...510.........520.........530.........540.........550.........560.........57\n&gt; 0.........580.........590.........600.........610.........620.........630....\n&gt; .....640.........650.........660.........670.........680.........690.........\n&gt; 700.........710.........720.........730.........740.........750.........760..\n&gt; .......770.........780.........790.........800.........810.........820.......\n&gt; ..830.........840.........850.........860.........870.........880.........890\n&gt; .........900.........910.........920.........930.........940.........950.....\n&gt; ....960.........970.........980.........990.........1,000 done\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     _b_mean |      1,000     .001419     .043875  -.1660358   .1525889\n   _b_median |      1,000    .0000899    .0544004  -.1851489   .1853068\n\n\nConclusion, when N=100, and the distribution is normal, the mean is more efficient than the median.\n\n\n\n// define a program\ncapture program drop mean_vs_median\nprogram define mean_vs_median, eclass\n  syntax, [nobs(int 100) rt(int 5)]\n  clear\n  set obs `nobs'\n  gen x = rt(`rt')\n  qui:sum x,d\n  matrix b = r(mean), r(p50)\n  matrix colname b = \"mean\" \"median\"\n  ereturn post b\nend\n set seed 101\nsimulate, reps(1000) nodots: mean_vs_median, nobs(500) rt(2)\nsum\nsimulate, reps(1000) nodots: mean_vs_median, nobs(500) rt(4)\nsum\nsimulate, reps(1000) nodots: mean_vs_median, nobs(500) rt(6)\nsum\n\n\n\n      Command: mean_vs_median, nobs(500) rt(2)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     _b_mean |      1,000   -.0199475    .2203307  -4.369244   .6070946\n   _b_median |      1,000    .0002406    .0640181  -.1970648   .1889922\n\n      Command: mean_vs_median, nobs(500) rt(4)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     _b_mean |      1,000    .0007863    .0647392  -.2136446   .2056455\n   _b_median |      1,000    .0024002    .0602134  -.1902502   .2014696\n\n      Command: mean_vs_median, nobs(500) rt(6)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     _b_mean |      1,000    .0007195    .0531958  -.1984012   .2054568\n   _b_median |      1,000    .0008841    .0580835  -.1863388   .2035508\n\n\n\n\n\n\n\nMonte Carlo methods can also be used to study the properties of estimators.\nConsider the following example:\n\nWe want to study the properties of the OLS estimator when the error term is heteroskedastic.\n\n\n\\[y_i = \\beta_0 + \\beta_1 x_i + u_i*exp(\\gamma x_i)\\]\n\nWhat are the consequences of heteroskedasticity in the OLS estimator?\nlets set up a simulation to study this.\n\n\n// define a program\ncapture program drop ols_hetero\nprogram define ols_hetero, eclass\n  syntax, [nobs(int 100) b0(real 1) b1(real 1) gamma(real 1)]\n  clear\n  set obs `nobs'\n  gen x = rnormal(0,1)\n  gen u = rnormal(0,1)\n  gen y = `b0' + `b1' * x + u*exp(`gamma'*x)\n  // run regression (under homoskedasticity)\n  qui:reg y x\n  // store results\n  matrix b = _b[_cons], _se[_cons], _b[x], _se[x]\n  matrix colname b = \"b0\" \"se0\" \"b1\" \"se1\"\n  ereturn post b\nend\nsimulate, reps(1000) nodots: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\nsum \n\n\n\n      Command: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       _b_b0 |      1,000    .9979773    .1210228   .6013685   1.434932\n      _b_se0 |      1,000    .1196379    .0220658   .0737395   .2717805\n       _b_b1 |      1,000    .9864205    .2661498   .0342865     1.8922\n      _b_se1 |      1,000    .1195836    .0210792   .0808713   .2756631\n\n\n\n\nWe can correct for heteroskedasticity using robust standard errors.\n\n// define a program\ncapture program drop ols_hetero\nprogram define ols_hetero, eclass\n  syntax, [nobs(int 100) b0(real 1) b1(real 1) gamma(real 1)]\n  clear\n  set obs `nobs'\n  gen x = rnormal(0,1)\n  gen u = rnormal(0,1)\n  gen y = `b0' + `b1' * x + u*exp(`gamma'*x)\n  // run regression (under homoskedasticity)\n  qui:reg y x, robust\n  // store results\n  matrix b = _b[_cons], _se[_cons], _b[x], _se[x]\n  matrix colname b = \"b0\" \"se0\" \"b1\" \"se1\"\n  ereturn post b\nend\nsimulate, reps(1000) nodots: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\nsum \n\n\n\n      Command: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       _b_b0 |      1,000     1.00342    .1180407   .6255065     1.3921\n      _b_se0 |      1,000    .1171892    .0219696   .0807092   .3564852\n       _b_b1 |      1,000     1.00355    .2589393  -.2493259   1.956252\n      _b_se1 |      1,000    .2407364    .0913925   .1092693   1.174903\n\n\nor using weighted least squares.\n\n// define a program\ncapture program drop ols_hetero\nprogram define ols_hetero, eclass\n  syntax, [nobs(int 100) b0(real 1) b1(real 1) gamma(real 1)]\n  clear\n  set obs `nobs'\n  gen x = rnormal(0,1)\n  gen u = rnormal(0,1)\n  gen y = `b0' + `b1' * x + u*exp(`gamma'*x)\n  // run regression (under homoskedasticity)\n  qui:reg y x, \n  predict uhat, resid\n  gen lnuhat2 = ln(uhat^2)\n  reg lnuhat2 x\n  predict lnhx\n  gen hx=exp(lnhx)\n  // store results\n  qui:reg y x [w=1/hx], \n  matrix b = _b[_cons], _se[_cons], _b[x], _se[x]\n  matrix colname b = \"b0\" \"se0\" \"b1\" \"se1\"\n  ereturn post b\nend\nsimulate, reps(1000) nodots: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\nsum \n\n\n\n      Command: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       _b_b0 |      1,000    1.001033    .0465683   .8545606   1.252005\n      _b_se0 |      1,000    .0478398    .0084316   .0288236   .1462429\n       _b_b1 |      1,000    1.001063    .0277128   .8875045   1.301835\n      _b_se1 |      1,000    .0270911    .0091106   .0088032   .1357872\n\n\n\n\n\n\n\nYou can use Monte Carlo simulations to study the properties of new estimators as well. (most often)\nThe structure of the simulation, however, will depend on the estimator you want to study, and may not be fully generalizable.\n\nNotice that in the previous example, we assumed that all data needed to be simulated.\nBut, we could just as well simulate only “parts” of the data, and use observed data for the rest.\n\n\n\nfrause oaxaca, clear\nprobit lfp female educ age agesq married divorced\npredict lfp_xb, xb\nmatrix b=e(b)\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\nIteration 0:  Log likelihood = -634.26553  \nIteration 1:  Log likelihood = -453.80541  \nIteration 2:  Log likelihood = -429.25759  \nIteration 3:  Log likelihood = -428.40991  \nIteration 4:  Log likelihood = -428.40986  \nIteration 5:  Log likelihood = -428.40986  \n\nProbit regression                                       Number of obs =  1,647\n                                                        LR chi2(6)    = 411.71\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -428.40986                             Pseudo R2     = 0.3246\n\n------------------------------------------------------------------------------\n         lfp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -1.644408   .1498206   -10.98   0.000    -1.938051   -1.350765\n        educ |   .1051167   .0251728     4.18   0.000     .0557789    .1544544\n         age |   .1045008   .0385916     2.71   0.007     .0288627    .1801389\n       agesq |  -.0012596   .0004463    -2.82   0.005    -.0021344   -.0003848\n     married |  -1.692076   .1887409    -8.97   0.000    -2.062001    -1.32215\n    divorced |    -.68518   .2319883    -2.95   0.003    -1.139869   -.2304912\n       _cons |   .4413072   .7648821     0.58   0.564    -1.057834    1.940448\n------------------------------------------------------------------------------\n\n\n\n* Latent model LFP =1(lfp_xb + e&gt;0)\ncapture program drop probit_sim\nprogram  probit_sim, eclass\n  capture drop lfp_hat\n  gen lfp_hat=(lfp_xb + rnormal(0,1))&gt;0\n  probit lfp_hat female educ age agesq married divorced, from(b, copy)\nend\nsimulate _b _se, reps(500) nodots: probit_sim\nren lfp_hat* *\nsum ,sep(7)\n\n\n\n      Command: probit_sim\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n   _b_female |        500    -1.67208    .1594671  -2.262682  -1.280128\n     _b_educ |        500     .108218    .0251913   .0453378   .1974491\n      _b_age |        500    .1022977     .038849   -.004108   .2443264\n    _b_agesq |        500   -.0012359    .0004483  -.0029406  -.0000835\n  _b_married |        500   -1.720405    .1903062  -2.425838  -1.247641\n _b_divorced |        500   -.7046396    .2310935  -1.387282   .1018858\n     _b_cons |        500    .5167748    .7899037  -1.920398   2.650355\n-------------+---------------------------------------------------------\n  _se_female |        500    .1548574    .0187049   .1234346   .2547092\n    _se_educ |        500    .0255194    .0012991   .0223761    .029805\n     _se_age |        500    .0389973    .0017384   .0336116   .0466778\n   _se_agesq |        500    .0004507    .0000184   .0003941   .0005336\n _se_married |        500    .1987176    .0234131   .1538517   .3142596\n_se_divorced |        500    .2410041    .0207673    .199707   .3420741\n    _se_cons |        500    .7709869    .0397739   .6426999   .9831291\n\n\n\nTo some extent, this is similar to the imputation methods we have seen before.\nMore complex versions of this can be used to elaborate micro-simulations.\n\n\n\n\n\nMicro-simulation is a technique that is used to make micro units act and interact in a way that it is possible to aggregate to the level of interest.\nA micro simulation model can be seen as a set of rules, which operates on a sample of micro units (individuals, households, firms, etc.) to produce a set of outcomes.\nThe goal is to produce synthetic datasets that can be used to estimate the effects of policy changes.\nBecause micro-simulations are based on micro-data, they have the potential to capture the heterogeneity of the population. (decisions, preferences, etc.)\n\n\n\n\n\nWe can also think about micro-simulations as a way to simulate/predict/impute the behavior of a population of interest.\n\nThus a lot of what you learned in terms of Modeling, imputations, and matching can be applied here.\n\n\n\n\n\nWe need a population of micro units (individuals, households, firms, etc.) that is representative of the population of interest. (survey data)\nDetails on a policy change that we want to simulate. (policy parameters)\nA set of rules that describe how the micro units interact with each other and with the policy change. (model for behavior)\nAn outcome of interest that we want to study. (outcome variable)\n\n\n\n\n\n\nDepending on the type of analysis we want to do, the structure of a micro-simulation can be very simple or very complex.\nConsider the following example:\n\nWe want to study the effect of a policy that aims to increase the minimum wage in the labor market. What effects would this have?\n\nHigher wages ?\nincrease/decrease in employment?\n\nChanges in the distribution of wages?\nChanges in the Economic structure?\n\n\nMore complex models require more sophisticated interactions between the micro/and macro units and the policy change.\nHowever, simpler models can be useful at least to study first order/statistical effects of a policy change.\n\n\n\n\n\nConsider the following. The government wants to increase educational attainment in the population.\nTo do so, they want to evaluate the impact that a 2 additional years for people with less than 12 years of education would have on the population.\nHow do we do this?\nFor simplicilty , lets use the oaxaca dataset\n\n\n\n\nWe can start by modeling the effect of education on wages.\n\\[log(wage)= \\beta X + \\beta_e educ + u\\]\n\nfrause oaxaca, clear\nreg lnwage educ exper tenure female age, \npredict  res, res\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n      Source |       SS           df       MS      Number of obs   =     1,434\n-------------+----------------------------------   F(5, 1428)      =    101.03\n       Model |   105.60186         5   21.120372   Prob &gt; F        =    0.0000\n    Residual |  298.517944     1,428  .209046179   R-squared       =    0.2613\n-------------+----------------------------------   Adj R-squared   =    0.2587\n       Total |  404.119804     1,433  .282009633   Root MSE        =    .45722\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0743986   .0052612    14.14   0.000     .0640782    .0847191\n       exper |   .0022628   .0018958     1.19   0.233    -.0014562    .0059817\n      tenure |   .0021805   .0019783     1.10   0.271    -.0017001    .0060611\n      female |  -.1321951   .0254327    -5.20   0.000    -.1820846   -.0823056\n         age |   .0136344   .0017751     7.68   0.000     .0101523    .0171165\n       _cons |   1.985785   .0732567    27.11   0.000     1.842083    2.129488\n------------------------------------------------------------------------------\n(213 missing values generated)\n\n\nID the policy change\n\nclonevar educ2=educ\nreplace educ=educ+2 if educ&lt;12\npredict yhat2, xb\n\n(1,099 real changes made)\n(213 missing values generated)\n\n\nSo what is the effect on wages? (if there is no selection bias)\n\ngen wage = exp(lnwage)\ngen wage2 = exp(yhat2+res)\ngen wage_diff = wage2-wage\ntabstat wage_diff\nreplace educ = educ2\n\n(213 missing values generated)\n(213 missing values generated)\n(213 missing values generated)\n\n    Variable |      Mean\n-------------+----------\n   wage_diff |  2.947924\n------------------------\n(1,099 real changes made)\n\n\nWage has increased in 2.95.\nBut is this the only effect??\n\n\n\n\nWhat about the effect on employment?\nThis is a more complex model, and we need further assumptions\n\nAssume anyone who wants to work, will find a job.\nThose employed remain employed\nThose non-employed will transition to employment marginally\n\n\n\\[P(lfp=1|X,educ)= \\beta X + \\beta_e educ + u\n\\]\nFirst model the probability of employment\n\nfrause oaxaca, clear\nqui:probit lfp educ female age single married kids6 kids714\npredict lfp_xb, xb\npredict pr_org, pr\n\nreplace lfp_xb = lfp_xb + _b[educ] * 2 if educ&lt;12\ngen plfp = normal(lfp_xb)\n\n** Before and after the policy change\nsum plfp pr_org \n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(1,099 real changes made)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        plfp |      1,647    .8995986    .1614962   .0777393   .9999999\n      pr_org |      1,647    .8707874    .1925782   .0604887   .9999999\n\n\nSo now we have the original probability of employment and the probability of employment after the policy change.\nHow do we know who will transition from not working to working?\n\nOption 1. Assign new workers based on the relative change in the probability of employment.\n\n\ngen dprob = (plfp-pr_org)/pr_org\nclonevar lfp_post1 = lfp\nreplace lfp_post1 =1 if lfp==0 & dprob&gt;runiform()\nsum  lfp_post1 lfp\n\n(35 real changes made)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n   lfp_post1 |      1,647    .8919247    .3105698          0          1\n         lfp |      1,647     .870674    .3356624          0          1\n\n\n\nOption 2. Simulate employment status based on the original and post-policy probability of employment.\n\n\n** Option 2\ndrop2 unf lfp_org lfp_post\ngen unf = runiform()\ngen lfp_org  = pr_org&gt;unf\ngen lfp_post = plfp  &gt;unf\n\nsum lfp_post  lfp_org\n\nvariable unf not found\nvariable lfp_org not found\nvariable lfp_post not found\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n    lfp_post |      1,647    .9046752    .2937523          0          1\n     lfp_org |      1,647    .8737098    .3322771          0          1\n\n\n\nBoth options are valid, but\n\nFirst one requires to impute wages for the new workers only.\nSecond one requires to impute wages for the entire population.\n\n\n\n\n\n\n\n\nWe could assume no selection bias.\nWe may need to use only data available for everyone, or use imputed data (exper tenure) (perhaps at 0?)\n\n\nreg lnwage educ female age single married kids6 kids714, \npredict  res, res\npredict lnwage_hat, xb\n** Simulating Known wages component\nreplace lnwage_hat = lnwage_hat + _b[educ] * 2 if educ&lt;12\n** Simulating random component\n** For those already working:\nreplace lnwage_hat = lnwage_hat + res if lfp==1\n** Simulate unobserved\nqui: sum res, \nreplace lnwage_hat = lnwage_hat + rnormal(0,r(sd)) if lfp_post1==1 & lfp==0\n\ngen wage_post = exp(lnwage_hat) if lfp_post1==1 | lfp==1\ngen wage = exp(lnwage)\nsum wage wage_post\nsgini wage wage_post\n\n\n      Source |       SS           df       MS      Number of obs   =     1,434\n-------------+----------------------------------   F(7, 1426)      =     79.23\n       Model |  113.158257         7  16.1654653   Prob &gt; F        =    0.0000\n    Residual |  290.961547     1,426  .204040355   R-squared       =    0.2800\n-------------+----------------------------------   Adj R-squared   =    0.2765\n       Total |  404.119804     1,433  .282009633   Root MSE        =    .45171\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0709179   .0050262    14.11   0.000     .0610584    .0807774\n      female |  -.1427501   .0244736    -5.83   0.000    -.1907583   -.0947419\n         age |    .016475   .0014033    11.74   0.000     .0137222    .0192277\n      single |  -.0711724   .0443651    -1.60   0.109    -.1582002    .0158554\n     married |  -.0977016   .0379654    -2.57   0.010    -.1721755   -.0232276\n       kids6 |   .1085073   .0239699     4.53   0.000     .0614873    .1555272\n     kids714 |   .0656187    .019681     3.33   0.001      .027012    .1042254\n       _cons |   1.999222   .0902563    22.15   0.000     1.822173    2.176272\n------------------------------------------------------------------------------\n(213 missing values generated)\n(1,099 real changes made)\n(1,434 real changes made)\n(35 real changes made)\n(178 missing values generated)\n(213 missing values generated)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        wage |      1,434    32.39167    16.12498   1.661434   192.3077\n   wage_post |      1,469    35.19396    16.94609   1.873127   221.6129\n\nGini coefficient for wage, wage_post\n\n-----------------------\n    Variable |      v=2\n-------------+---------\n        wage |   0.2460\n   wage_post |   0.2356\n-----------------------\n\n\nThis is equivalent to running a single imputation round. However, we could repeat the process M times to get a measure for the precission of our estimates.\n\n\n\n\nWe could go further and also simulate where would people work, and how would the economy change.\nWe could also account for selection problems\nor make a more explicit model for distributional analysis.\n\n\n\n\n\n\n\n\nWe have just use a simple micro-simulation to study the effect of a policy change (education) on wages and employment.\nThis simulation had many assumptions, and we could have done it in many different ways.\nAmong others, we assume no selection bias, with an instantaneous change in education, and no again of the population.\nWe also made important assumptions regarding the transition from non-employment to employment, and the imputation of wages.\n\n\n\n\n\n\nWhile many micro-simulations are based on stochastic simultions, there are other ways to do it.\nIn Hotckiss et al. (forthcoming), we use a deterministic micro-simulation to study the effect of tax-reforms on welfare changes and its distribution for the - first\nHow did we do it?\n\n\n\n\n\nWe concentrated mostly on couple households with young children (0-18 years old).\nUsing a heckman selection model, we impute wages for the non-working people (based on PMM).\nUsing observed and imputed wages, we impute the tax liability for each household before and after the reform. (TAX-SIM), and estimate after-tax wages.\nEstimate Household Labor Supply based on HH utility and Non-linear Tobit model\nUse HLS models, we make predictions for Labor Supply changes and utility changes given the Reform.\n\nThe Outcome was how much better/worse off would households be after the reform."
  },
  {
    "objectID": "rmethods2/session_7.html#what-is-monte-carlo-simulation",
    "href": "rmethods2/session_7.html#what-is-monte-carlo-simulation",
    "title": "Research Methods II",
    "section": "",
    "text": "Monte Carlo simulation are a generic name given to methods that use random numbers to simulate a process.\nIn econometrics, Monte Carlo methods are used to study the properties of estimators, and to evaluate the performance of statistical tests.\nThis can be a useful tool to understand some of the properties of estimators, or even problems related to violations of assumptions.\nIt can also be used to evaluate the performance of estimators in finite samples, and to compare different estimators."
  },
  {
    "objectID": "rmethods2/session_7.html#example-mean-vs-median",
    "href": "rmethods2/session_7.html#example-mean-vs-median",
    "title": "Research Methods II",
    "section": "",
    "text": "Which of this estimators is more robust and efficient, when samples are small ?\nLets setup a program that would simulate this:\n\n\n// define a program\ncapture program drop mean_vs_median\nprogram define mean_vs_median, eclass\n  syntax, [nobs(int 100)]\n  clear\n  ** Set  # of obs\n  set obs `nobs'\n  ** Generate a random variable\n  gen x = rnormal(0,1)\n  ** Calculate mean and median\n  qui:sum x,d\n  ** Store results\n  matrix b = r(mean), r(p50)\n  ** post results\n  matrix colname b = \"mean\" \"median\"\n  ereturn post b\nend\nmean_vs_median\nereturn display\n\n\n\n\n\nNumber of observations (_N) was 0, now 100.\n------------------------------------------------------------------------------\n             | Coefficient\n-------------+----------------------------------------------------------------\n        mean |  -.1445693\n      median |  -.1970271\n------------------------------------------------------------------------------\n\n\nNow that the program is SET, lets run it 1000 times:\n\nset seed 101\nsimulate, reps(1000): mean_vs_median, nobs(500)\nsum\n\n\n      Command: mean_vs_median, nobs(500)\n\nSimulations (1,000): .........10.........20.........30.........40.........50...\n&gt; ......60.........70.........80.........90.........100.........110.........120\n&gt; .........130.........140.........150.........160.........170.........180.....\n&gt; ....190.........200.........210.........220.........230.........240.........2\n&gt; 50.........260.........270.........280.........290.........300.........310...\n&gt; ......320.........330.........340.........350.........360.........370........\n&gt; .380.........390.........400.........410.........420.........430.........440.\n&gt; ........450.........460.........470.........480.........490.........500......\n&gt; ...510.........520.........530.........540.........550.........560.........57\n&gt; 0.........580.........590.........600.........610.........620.........630....\n&gt; .....640.........650.........660.........670.........680.........690.........\n&gt; 700.........710.........720.........730.........740.........750.........760..\n&gt; .......770.........780.........790.........800.........810.........820.......\n&gt; ..830.........840.........850.........860.........870.........880.........890\n&gt; .........900.........910.........920.........930.........940.........950.....\n&gt; ....960.........970.........980.........990.........1,000 done\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     _b_mean |      1,000     .001419     .043875  -.1660358   .1525889\n   _b_median |      1,000    .0000899    .0544004  -.1851489   .1853068\n\n\nConclusion, when N=100, and the distribution is normal, the mean is more efficient than the median.\n\n\n\n// define a program\ncapture program drop mean_vs_median\nprogram define mean_vs_median, eclass\n  syntax, [nobs(int 100) rt(int 5)]\n  clear\n  set obs `nobs'\n  gen x = rt(`rt')\n  qui:sum x,d\n  matrix b = r(mean), r(p50)\n  matrix colname b = \"mean\" \"median\"\n  ereturn post b\nend\n set seed 101\nsimulate, reps(1000) nodots: mean_vs_median, nobs(500) rt(2)\nsum\nsimulate, reps(1000) nodots: mean_vs_median, nobs(500) rt(4)\nsum\nsimulate, reps(1000) nodots: mean_vs_median, nobs(500) rt(6)\nsum\n\n\n\n      Command: mean_vs_median, nobs(500) rt(2)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     _b_mean |      1,000   -.0199475    .2203307  -4.369244   .6070946\n   _b_median |      1,000    .0002406    .0640181  -.1970648   .1889922\n\n      Command: mean_vs_median, nobs(500) rt(4)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     _b_mean |      1,000    .0007863    .0647392  -.2136446   .2056455\n   _b_median |      1,000    .0024002    .0602134  -.1902502   .2014696\n\n      Command: mean_vs_median, nobs(500) rt(6)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     _b_mean |      1,000    .0007195    .0531958  -.1984012   .2054568\n   _b_median |      1,000    .0008841    .0580835  -.1863388   .2035508"
  },
  {
    "objectID": "rmethods2/session_7.html#properties-of-estimators",
    "href": "rmethods2/session_7.html#properties-of-estimators",
    "title": "Research Methods II",
    "section": "",
    "text": "Monte Carlo methods can also be used to study the properties of estimators.\nConsider the following example:\n\nWe want to study the properties of the OLS estimator when the error term is heteroskedastic.\n\n\n\\[y_i = \\beta_0 + \\beta_1 x_i + u_i*exp(\\gamma x_i)\\]\n\nWhat are the consequences of heteroskedasticity in the OLS estimator?\nlets set up a simulation to study this.\n\n\n// define a program\ncapture program drop ols_hetero\nprogram define ols_hetero, eclass\n  syntax, [nobs(int 100) b0(real 1) b1(real 1) gamma(real 1)]\n  clear\n  set obs `nobs'\n  gen x = rnormal(0,1)\n  gen u = rnormal(0,1)\n  gen y = `b0' + `b1' * x + u*exp(`gamma'*x)\n  // run regression (under homoskedasticity)\n  qui:reg y x\n  // store results\n  matrix b = _b[_cons], _se[_cons], _b[x], _se[x]\n  matrix colname b = \"b0\" \"se0\" \"b1\" \"se1\"\n  ereturn post b\nend\nsimulate, reps(1000) nodots: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\nsum \n\n\n\n      Command: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       _b_b0 |      1,000    .9979773    .1210228   .6013685   1.434932\n      _b_se0 |      1,000    .1196379    .0220658   .0737395   .2717805\n       _b_b1 |      1,000    .9864205    .2661498   .0342865     1.8922\n      _b_se1 |      1,000    .1195836    .0210792   .0808713   .2756631\n\n\n\n\nWe can correct for heteroskedasticity using robust standard errors.\n\n// define a program\ncapture program drop ols_hetero\nprogram define ols_hetero, eclass\n  syntax, [nobs(int 100) b0(real 1) b1(real 1) gamma(real 1)]\n  clear\n  set obs `nobs'\n  gen x = rnormal(0,1)\n  gen u = rnormal(0,1)\n  gen y = `b0' + `b1' * x + u*exp(`gamma'*x)\n  // run regression (under homoskedasticity)\n  qui:reg y x, robust\n  // store results\n  matrix b = _b[_cons], _se[_cons], _b[x], _se[x]\n  matrix colname b = \"b0\" \"se0\" \"b1\" \"se1\"\n  ereturn post b\nend\nsimulate, reps(1000) nodots: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\nsum \n\n\n\n      Command: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       _b_b0 |      1,000     1.00342    .1180407   .6255065     1.3921\n      _b_se0 |      1,000    .1171892    .0219696   .0807092   .3564852\n       _b_b1 |      1,000     1.00355    .2589393  -.2493259   1.956252\n      _b_se1 |      1,000    .2407364    .0913925   .1092693   1.174903\n\n\nor using weighted least squares.\n\n// define a program\ncapture program drop ols_hetero\nprogram define ols_hetero, eclass\n  syntax, [nobs(int 100) b0(real 1) b1(real 1) gamma(real 1)]\n  clear\n  set obs `nobs'\n  gen x = rnormal(0,1)\n  gen u = rnormal(0,1)\n  gen y = `b0' + `b1' * x + u*exp(`gamma'*x)\n  // run regression (under homoskedasticity)\n  qui:reg y x, \n  predict uhat, resid\n  gen lnuhat2 = ln(uhat^2)\n  reg lnuhat2 x\n  predict lnhx\n  gen hx=exp(lnhx)\n  // store results\n  qui:reg y x [w=1/hx], \n  matrix b = _b[_cons], _se[_cons], _b[x], _se[x]\n  matrix colname b = \"b0\" \"se0\" \"b1\" \"se1\"\n  ereturn post b\nend\nsimulate, reps(1000) nodots: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\nsum \n\n\n\n      Command: ols_hetero, nobs(500) b0(1) b1(1) gamma(1)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       _b_b0 |      1,000    1.001033    .0465683   .8545606   1.252005\n      _b_se0 |      1,000    .0478398    .0084316   .0288236   .1462429\n       _b_b1 |      1,000    1.001063    .0277128   .8875045   1.301835\n      _b_se1 |      1,000    .0270911    .0091106   .0088032   .1357872"
  },
  {
    "objectID": "rmethods2/session_7.html#more-on-monte-carlo-simulations",
    "href": "rmethods2/session_7.html#more-on-monte-carlo-simulations",
    "title": "Research Methods II",
    "section": "",
    "text": "You can use Monte Carlo simulations to study the properties of new estimators as well. (most often)\nThe structure of the simulation, however, will depend on the estimator you want to study, and may not be fully generalizable.\n\nNotice that in the previous example, we assumed that all data needed to be simulated.\nBut, we could just as well simulate only “parts” of the data, and use observed data for the rest.\n\n\n\nfrause oaxaca, clear\nprobit lfp female educ age agesq married divorced\npredict lfp_xb, xb\nmatrix b=e(b)\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\nIteration 0:  Log likelihood = -634.26553  \nIteration 1:  Log likelihood = -453.80541  \nIteration 2:  Log likelihood = -429.25759  \nIteration 3:  Log likelihood = -428.40991  \nIteration 4:  Log likelihood = -428.40986  \nIteration 5:  Log likelihood = -428.40986  \n\nProbit regression                                       Number of obs =  1,647\n                                                        LR chi2(6)    = 411.71\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -428.40986                             Pseudo R2     = 0.3246\n\n------------------------------------------------------------------------------\n         lfp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -1.644408   .1498206   -10.98   0.000    -1.938051   -1.350765\n        educ |   .1051167   .0251728     4.18   0.000     .0557789    .1544544\n         age |   .1045008   .0385916     2.71   0.007     .0288627    .1801389\n       agesq |  -.0012596   .0004463    -2.82   0.005    -.0021344   -.0003848\n     married |  -1.692076   .1887409    -8.97   0.000    -2.062001    -1.32215\n    divorced |    -.68518   .2319883    -2.95   0.003    -1.139869   -.2304912\n       _cons |   .4413072   .7648821     0.58   0.564    -1.057834    1.940448\n------------------------------------------------------------------------------\n\n\n\n* Latent model LFP =1(lfp_xb + e&gt;0)\ncapture program drop probit_sim\nprogram  probit_sim, eclass\n  capture drop lfp_hat\n  gen lfp_hat=(lfp_xb + rnormal(0,1))&gt;0\n  probit lfp_hat female educ age agesq married divorced, from(b, copy)\nend\nsimulate _b _se, reps(500) nodots: probit_sim\nren lfp_hat* *\nsum ,sep(7)\n\n\n\n      Command: probit_sim\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n   _b_female |        500    -1.67208    .1594671  -2.262682  -1.280128\n     _b_educ |        500     .108218    .0251913   .0453378   .1974491\n      _b_age |        500    .1022977     .038849   -.004108   .2443264\n    _b_agesq |        500   -.0012359    .0004483  -.0029406  -.0000835\n  _b_married |        500   -1.720405    .1903062  -2.425838  -1.247641\n _b_divorced |        500   -.7046396    .2310935  -1.387282   .1018858\n     _b_cons |        500    .5167748    .7899037  -1.920398   2.650355\n-------------+---------------------------------------------------------\n  _se_female |        500    .1548574    .0187049   .1234346   .2547092\n    _se_educ |        500    .0255194    .0012991   .0223761    .029805\n     _se_age |        500    .0389973    .0017384   .0336116   .0466778\n   _se_agesq |        500    .0004507    .0000184   .0003941   .0005336\n _se_married |        500    .1987176    .0234131   .1538517   .3142596\n_se_divorced |        500    .2410041    .0207673    .199707   .3420741\n    _se_cons |        500    .7709869    .0397739   .6426999   .9831291\n\n\n\nTo some extent, this is similar to the imputation methods we have seen before.\nMore complex versions of this can be used to elaborate micro-simulations."
  },
  {
    "objectID": "rmethods2/session_7.html#what-are-micro-simulations",
    "href": "rmethods2/session_7.html#what-are-micro-simulations",
    "title": "Research Methods II",
    "section": "",
    "text": "Micro-simulation is a technique that is used to make micro units act and interact in a way that it is possible to aggregate to the level of interest.\nA micro simulation model can be seen as a set of rules, which operates on a sample of micro units (individuals, households, firms, etc.) to produce a set of outcomes.\nThe goal is to produce synthetic datasets that can be used to estimate the effects of policy changes.\nBecause micro-simulations are based on micro-data, they have the potential to capture the heterogeneity of the population. (decisions, preferences, etc.)"
  },
  {
    "objectID": "rmethods2/session_7.html#section",
    "href": "rmethods2/session_7.html#section",
    "title": "Research Methods II",
    "section": "",
    "text": "We can also think about micro-simulations as a way to simulate/predict/impute the behavior of a population of interest.\n\nThus a lot of what you learned in terms of Modeling, imputations, and matching can be applied here.\n\n\n\n\n\nWe need a population of micro units (individuals, households, firms, etc.) that is representative of the population of interest. (survey data)\nDetails on a policy change that we want to simulate. (policy parameters)\nA set of rules that describe how the micro units interact with each other and with the policy change. (model for behavior)\nAn outcome of interest that we want to study. (outcome variable)"
  },
  {
    "objectID": "rmethods2/session_7.html#how-do-we-do-it",
    "href": "rmethods2/session_7.html#how-do-we-do-it",
    "title": "Research Methods II",
    "section": "",
    "text": "Depending on the type of analysis we want to do, the structure of a micro-simulation can be very simple or very complex.\nConsider the following example:\n\nWe want to study the effect of a policy that aims to increase the minimum wage in the labor market. What effects would this have?\n\nHigher wages ?\nincrease/decrease in employment?\n\nChanges in the distribution of wages?\nChanges in the Economic structure?\n\n\nMore complex models require more sophisticated interactions between the micro/and macro units and the policy change.\nHowever, simpler models can be useful at least to study first order/statistical effects of a policy change."
  },
  {
    "objectID": "rmethods2/session_7.html#example-the-case-of-higher-education",
    "href": "rmethods2/session_7.html#example-the-case-of-higher-education",
    "title": "Research Methods II",
    "section": "",
    "text": "Consider the following. The government wants to increase educational attainment in the population.\nTo do so, they want to evaluate the impact that a 2 additional years for people with less than 12 years of education would have on the population.\nHow do we do this?\nFor simplicilty , lets use the oaxaca dataset"
  },
  {
    "objectID": "rmethods2/session_7.html#section-1",
    "href": "rmethods2/session_7.html#section-1",
    "title": "Research Methods II",
    "section": "",
    "text": "We can start by modeling the effect of education on wages.\n\\[log(wage)= \\beta X + \\beta_e educ + u\\]\n\nfrause oaxaca, clear\nreg lnwage educ exper tenure female age, \npredict  res, res\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n      Source |       SS           df       MS      Number of obs   =     1,434\n-------------+----------------------------------   F(5, 1428)      =    101.03\n       Model |   105.60186         5   21.120372   Prob &gt; F        =    0.0000\n    Residual |  298.517944     1,428  .209046179   R-squared       =    0.2613\n-------------+----------------------------------   Adj R-squared   =    0.2587\n       Total |  404.119804     1,433  .282009633   Root MSE        =    .45722\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0743986   .0052612    14.14   0.000     .0640782    .0847191\n       exper |   .0022628   .0018958     1.19   0.233    -.0014562    .0059817\n      tenure |   .0021805   .0019783     1.10   0.271    -.0017001    .0060611\n      female |  -.1321951   .0254327    -5.20   0.000    -.1820846   -.0823056\n         age |   .0136344   .0017751     7.68   0.000     .0101523    .0171165\n       _cons |   1.985785   .0732567    27.11   0.000     1.842083    2.129488\n------------------------------------------------------------------------------\n(213 missing values generated)\n\n\nID the policy change\n\nclonevar educ2=educ\nreplace educ=educ+2 if educ&lt;12\npredict yhat2, xb\n\n(1,099 real changes made)\n(213 missing values generated)\n\n\nSo what is the effect on wages? (if there is no selection bias)\n\ngen wage = exp(lnwage)\ngen wage2 = exp(yhat2+res)\ngen wage_diff = wage2-wage\ntabstat wage_diff\nreplace educ = educ2\n\n(213 missing values generated)\n(213 missing values generated)\n(213 missing values generated)\n\n    Variable |      Mean\n-------------+----------\n   wage_diff |  2.947924\n------------------------\n(1,099 real changes made)\n\n\nWage has increased in 2.95.\nBut is this the only effect??"
  },
  {
    "objectID": "rmethods2/session_7.html#section-2",
    "href": "rmethods2/session_7.html#section-2",
    "title": "Research Methods II",
    "section": "",
    "text": "What about the effect on employment?\nThis is a more complex model, and we need further assumptions\n\nAssume anyone who wants to work, will find a job.\nThose employed remain employed\nThose non-employed will transition to employment marginally\n\n\n\\[P(lfp=1|X,educ)= \\beta X + \\beta_e educ + u\n\\]\nFirst model the probability of employment\n\nfrause oaxaca, clear\nqui:probit lfp educ female age single married kids6 kids714\npredict lfp_xb, xb\npredict pr_org, pr\n\nreplace lfp_xb = lfp_xb + _b[educ] * 2 if educ&lt;12\ngen plfp = normal(lfp_xb)\n\n** Before and after the policy change\nsum plfp pr_org \n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(1,099 real changes made)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        plfp |      1,647    .8995986    .1614962   .0777393   .9999999\n      pr_org |      1,647    .8707874    .1925782   .0604887   .9999999\n\n\nSo now we have the original probability of employment and the probability of employment after the policy change.\nHow do we know who will transition from not working to working?\n\nOption 1. Assign new workers based on the relative change in the probability of employment.\n\n\ngen dprob = (plfp-pr_org)/pr_org\nclonevar lfp_post1 = lfp\nreplace lfp_post1 =1 if lfp==0 & dprob&gt;runiform()\nsum  lfp_post1 lfp\n\n(35 real changes made)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n   lfp_post1 |      1,647    .8919247    .3105698          0          1\n         lfp |      1,647     .870674    .3356624          0          1\n\n\n\nOption 2. Simulate employment status based on the original and post-policy probability of employment.\n\n\n** Option 2\ndrop2 unf lfp_org lfp_post\ngen unf = runiform()\ngen lfp_org  = pr_org&gt;unf\ngen lfp_post = plfp  &gt;unf\n\nsum lfp_post  lfp_org\n\nvariable unf not found\nvariable lfp_org not found\nvariable lfp_post not found\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n    lfp_post |      1,647    .9046752    .2937523          0          1\n     lfp_org |      1,647    .8737098    .3322771          0          1\n\n\n\nBoth options are valid, but\n\nFirst one requires to impute wages for the new workers only.\nSecond one requires to impute wages for the entire population."
  },
  {
    "objectID": "rmethods2/session_7.html#section-3",
    "href": "rmethods2/session_7.html#section-3",
    "title": "Research Methods II",
    "section": "",
    "text": "We could assume no selection bias.\nWe may need to use only data available for everyone, or use imputed data (exper tenure) (perhaps at 0?)\n\n\nreg lnwage educ female age single married kids6 kids714, \npredict  res, res\npredict lnwage_hat, xb\n** Simulating Known wages component\nreplace lnwage_hat = lnwage_hat + _b[educ] * 2 if educ&lt;12\n** Simulating random component\n** For those already working:\nreplace lnwage_hat = lnwage_hat + res if lfp==1\n** Simulate unobserved\nqui: sum res, \nreplace lnwage_hat = lnwage_hat + rnormal(0,r(sd)) if lfp_post1==1 & lfp==0\n\ngen wage_post = exp(lnwage_hat) if lfp_post1==1 | lfp==1\ngen wage = exp(lnwage)\nsum wage wage_post\nsgini wage wage_post\n\n\n      Source |       SS           df       MS      Number of obs   =     1,434\n-------------+----------------------------------   F(7, 1426)      =     79.23\n       Model |  113.158257         7  16.1654653   Prob &gt; F        =    0.0000\n    Residual |  290.961547     1,426  .204040355   R-squared       =    0.2800\n-------------+----------------------------------   Adj R-squared   =    0.2765\n       Total |  404.119804     1,433  .282009633   Root MSE        =    .45171\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0709179   .0050262    14.11   0.000     .0610584    .0807774\n      female |  -.1427501   .0244736    -5.83   0.000    -.1907583   -.0947419\n         age |    .016475   .0014033    11.74   0.000     .0137222    .0192277\n      single |  -.0711724   .0443651    -1.60   0.109    -.1582002    .0158554\n     married |  -.0977016   .0379654    -2.57   0.010    -.1721755   -.0232276\n       kids6 |   .1085073   .0239699     4.53   0.000     .0614873    .1555272\n     kids714 |   .0656187    .019681     3.33   0.001      .027012    .1042254\n       _cons |   1.999222   .0902563    22.15   0.000     1.822173    2.176272\n------------------------------------------------------------------------------\n(213 missing values generated)\n(1,099 real changes made)\n(1,434 real changes made)\n(35 real changes made)\n(178 missing values generated)\n(213 missing values generated)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        wage |      1,434    32.39167    16.12498   1.661434   192.3077\n   wage_post |      1,469    35.19396    16.94609   1.873127   221.6129\n\nGini coefficient for wage, wage_post\n\n-----------------------\n    Variable |      v=2\n-------------+---------\n        wage |   0.2460\n   wage_post |   0.2356\n-----------------------\n\n\nThis is equivalent to running a single imputation round. However, we could repeat the process M times to get a measure for the precission of our estimates.\n\n\n\n\nWe could go further and also simulate where would people work, and how would the economy change.\nWe could also account for selection problems\nor make a more explicit model for distributional analysis."
  },
  {
    "objectID": "rmethods2/session_7.html#st-micro-simulation",
    "href": "rmethods2/session_7.html#st-micro-simulation",
    "title": "Research Methods II",
    "section": "",
    "text": "We have just use a simple micro-simulation to study the effect of a policy change (education) on wages and employment.\nThis simulation had many assumptions, and we could have done it in many different ways.\nAmong others, we assume no selection bias, with an instantaneous change in education, and no again of the population.\nWe also made important assumptions regarding the transition from non-employment to employment, and the imputation of wages."
  },
  {
    "objectID": "rmethods2/session_7.html#not-the-only-way-to-do-it",
    "href": "rmethods2/session_7.html#not-the-only-way-to-do-it",
    "title": "Research Methods II",
    "section": "",
    "text": "While many micro-simulations are based on stochastic simultions, there are other ways to do it.\nIn Hotckiss et al. (forthcoming), we use a deterministic micro-simulation to study the effect of tax-reforms on welfare changes and its distribution for the - first\nHow did we do it?"
  },
  {
    "objectID": "rmethods2/session_7.html#tax-reform-on-households-welfare",
    "href": "rmethods2/session_7.html#tax-reform-on-households-welfare",
    "title": "Research Methods II",
    "section": "",
    "text": "We concentrated mostly on couple households with young children (0-18 years old).\nUsing a heckman selection model, we impute wages for the non-working people (based on PMM).\nUsing observed and imputed wages, we impute the tax liability for each household before and after the reform. (TAX-SIM), and estimate after-tax wages.\nEstimate Household Labor Supply based on HH utility and Non-linear Tobit model\nUse HLS models, we make predictions for Labor Supply changes and utility changes given the Reform.\n\nThe Outcome was how much better/worse off would households be after the reform."
  },
  {
    "objectID": "rmethods2/session_7.html#intro",
    "href": "rmethods2/session_7.html#intro",
    "title": "Research Methods II",
    "section": "Intro",
    "text": "Intro\n\nAt Levy, we have also constructed a micro-simulation model to study employment simulations.\nMethod first developed for estimating the impact of the American Recovery and Reinvestment Act of 2009\n\nConvert spending into jobs by industry and occupation (I/O matrix)\nAssign potential workers to jobs\nPredict earnings and hours\n\nFor the work with LIMEW and LIMTIP, this has also been used for distributional analysis of employment assigment and services use."
  },
  {
    "objectID": "rmethods2/session_7.html#limm-step-i",
    "href": "rmethods2/session_7.html#limm-step-i",
    "title": "Research Methods II",
    "section": "LIMM: Step I",
    "text": "LIMM: Step I\n\nJob Creation\n\nConsider a policy: Road construction, Services provision, etc.\n\nCalculate changes in final demand for each industry the policy creates\nUsing I-O tables estimate change in total output for each industry\nUse that change in output to estimate change demand for labor inputs\n\nTransform the changes from labor imputs to generated jobs (consider wages)\n\nDistribute changes across occupations (within industry)\n\nUsing, for example, shares of employment by occupation within industry\n\n\nWith this we have a total number of jobs created by industry and occupation."
  },
  {
    "objectID": "rmethods2/session_7.html#limm-step-ii",
    "href": "rmethods2/session_7.html#limm-step-ii",
    "title": "Research Methods II",
    "section": "LIMM: Step II",
    "text": "LIMM: Step II\n\nJob Assigment\n\nGiven the Total change in Jobs, we need to assign workers to those jobs.\n\nWho are the potential workers?\n\nNot in LF: Potential, but not looking for work (may depend on characteristics). Avoid retired, disabled and students\nUnemployed: Most likely to take a job (pool may not be large enough)\nUnderemployed: Working part-time, but willing to work full-time, or people aiming for better jobs. (May create job openings)\nand Employed: May be willing to change jobs (may create job openings)"
  },
  {
    "objectID": "rmethods2/session_7.html#section-4",
    "href": "rmethods2/session_7.html#section-4",
    "title": "Research Methods II",
    "section": "",
    "text": "Job Assigment II\n\nTwo Steps, modeling job creation and job assigment\nA probit/logit model would be estimated to predict the likelihood of working, and this will be used to assign jobs.\n\nJob assignment is done using a multinomial model (mprobit) to predict likelihoods of working on a given occupation & industry.\nIdeally, you would like to do both at the same time, but that is a very complex model to estimate. Instead, each one is estimated separately.\n\n\n\\[\\begin{aligned}\nI^*(ind = 1 ) &= \\beta_1 X + e_1 \\\\\nI^*(ind = 2 ) &= \\beta_2 X + e_2 \\\\\n&\\vdots \\\\\nI^*(ind = k ) &= \\beta_k X + e_k\n\\end{aligned}\n\\]\nWhere \\(I^*(ind = k )\\) is the latent likelihood of working on industry \\(k\\) given characteristics. Mprobit or Mlogit depends on the distribution of \\(e_k\\).\nYou choose Industry \\(k\\) if \\(I^*(ind = k )\\) is the highest among all industries."
  },
  {
    "objectID": "rmethods2/session_7.html#section-5",
    "href": "rmethods2/session_7.html#section-5",
    "title": "Research Methods II",
    "section": "",
    "text": "Job Assigment III\n\nJob Assigment is done as follows:\n\nFor each potential worker, Calculate Prob of working. Those with the highest probability are assigned jobs first.\nFor Worker, \\(i\\), the individual is assigned to the industry with the highest likelihood of working. (until all jobs are assigned)\n\nCan be done followed by doing similar assigment for occupation within industry.\nOr combine both industry and occupation (p_o * p_i)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nJob allocation must consider jobs available and likehood of working on a given Occupation/Industry.\nMay want to avoid deterministic assignment.\ncould assigned based on random draws from the distribution of likelihoods."
  },
  {
    "objectID": "rmethods2/session_7.html#section-6",
    "href": "rmethods2/session_7.html#section-6",
    "title": "Research Methods II",
    "section": "",
    "text": "Earning contributions\n\nBecause some of the newly created jobs go to people who are employed in Family businesses (Farm and non farm income), one has to account for the “loss” of income from those jobs.\nThis is done by estimating the contribution of each member to the family business, and imputing the loss of income from the now “formally employed” member.\n\nFor example, modeling farm income, predicting that income without the family member, and imputing the difference as the loss of income."
  },
  {
    "objectID": "rmethods2/session_7.html#section-7",
    "href": "rmethods2/session_7.html#section-7",
    "title": "Research Methods II",
    "section": "",
    "text": "Job Assigment IV: Wages and Hours\n\nHours and Wages are imputed using a heckman model, and multiple stage process\n\n\nUse a probit model to predict the likelihood of working.\nGiven the model obtain the inverse mills ratios. \\(imr = \\phi(\\alpha'x)/\\Phi(\\alpha'x)\\)\nModel wages and hours using IMR as a regressor. (Heckit model) (This includes info on imputed occupations/industry)\nUse Imputed wages and hours to Match data to “donor” individuals. (PMM)\nFor people “potentially” leaving family business, compare new wages to family business contributions"
  },
  {
    "objectID": "rmethods2/session_7.html#section-8",
    "href": "rmethods2/session_7.html#section-8",
    "title": "Research Methods II",
    "section": "",
    "text": "Hours of HP re-assigment\n\nPeople taking jobs may have to change their contributions to household production. (less time for HP)\nBut the rest of the family may also have to adjust their contributions to HP. (more time for HP?)\n\nThis is done via matching, where the “donor” are working individuals with similar family characteristics.\nOr All working individuals\nCurrently, “recipients” are all individuals in the household, with atleast one new worker.\n\nMay be more sensible to impute hours\nOne may argue those not working may have to increase hours of HP. Or do nothing\n\n\n\n\n\nOther considerations\n\nAssigment of Child Care services"
  },
  {
    "objectID": "rmethods2/session_7.html#assessing-the-quality-of-the-simulation",
    "href": "rmethods2/session_7.html#assessing-the-quality-of-the-simulation",
    "title": "Research Methods II",
    "section": "Assessing the quality of the simulation",
    "text": "Assessing the quality of the simulation\n\nIn principle, there is no way to know if the simulation is correct.\n\nThere is no “true” value to compare to.\nHowever, one may want to at least ensure the simulation replicates the data.\nOne can potetially use the simulation to predict changes of a past policy, and compare to the actual changes.\n\nWe do want to make “sanity” checks\n\nAre results consistent and plausible?\nAre distributions of post-assigment outcomes consistent with the data?"
  },
  {
    "objectID": "rmethods2/session_7.html#limitations",
    "href": "rmethods2/session_7.html#limitations",
    "title": "Research Methods II",
    "section": "Limitations",
    "text": "Limitations\n\nSince we use existing data, we make the implicit assumption “no behavioral changes” in other aspects\n\nThings change as far as we can model them.\n\nResults are typically point estimates, and do not account for uncertainty.\n\nWe can use multiple imputation, monte carlo simulations, or bootstrapping to account for uncertainty.\n\nThe results are only as good as the data we use and the assumptions we make.\n\nWe can use sensitivity analysis to test the robustness of the results to changes in assumptions."
  },
  {
    "objectID": "rmethods2/session_5.html",
    "href": "rmethods2/session_5.html",
    "title": "Research Methods II",
    "section": "",
    "text": "What is statistical significance?\n\nStatistical significance is a way of determining if an observed effect is due to chance.\n\nWe typically use statistical significance to determine if the results of a study are meaningful, using various criteria.\n\nIs the p-value less than 0.05?\nDoes the 95% confidence interval include zero?\nis the t-statistic greater than 1.96?\n\nBut what exactly does that tell us?\n\n\n\n\n\n\n\n\n\n\n\n\nAssume you are testing for the effectiveness of a medicine that treats the common cold.\n\nHow do you know if the treatment is effective? (i.e., Does reduce the duration of the cold?)\nWe make an hypothesis!\nThe null hypothesis is that there is no effect of the treatment. (H0: no effect)\nYou collect some data and find out that it reduces the duration of the cold by 1 days.\nIs this a significant effect?\n\n\n\n\n\n\n\n\nThe sample size (how many people were in the study?)\n\nIf the sample size is 10,000, then a 1 day reduction in the duration of the cold may be significant.\nBut if the sample size is 10, then a 1 day reduction may be due to chance.\n\nThe effect size\n\nIf the effect size is 0.1 days, may not be significant.\nIf the effect size is 10 days, may be significant.\n\n\n\n\n\n\n\n\n\nStatistical significance is a way of determining if the observed effect is due to chance.\nTo do this, we required assumptions about the distribution under the Null Hypothesis.\n\nAssume that the data is normally distributed.\nAnd that there is a 4% chance you observe a value as extreme as the one you observed\n\nIn that case you would say, the effect is significant.\n\n\nJust couple of caveats:\n\nSignicance can be achieved by either measuring a “large” effect\nor by measuring a “small” effect with a large sample size. (very precisely)\n\nThus, finding no significance could mean the effect is noise or that the sample size is too small.\nRecall Hypothesis can be true or not. We only know if the data is consistent with the hypothesis or not.\n\n\n\n\n\n\n\n\nThe power of a statistical test represents the probability of detecting an effect, given that the effect is real.\nFor example, say a drug has the effect of reducing the duration of the common cold by 1 day. But you do not know this\nInstead you make your hypothesis. How likely is that the effect is significant?\n\nYou find the effect is not significant at 10% level.\n\nWhat is happening?\n\nThe effect was true, yet we find no significance.\nThe power of the test was low. (sample size was too small)\n\nIn General, Setting high significance levels will reduce the power of the test."
  },
  {
    "objectID": "rmethods2/session_5.html#statistical-significance-1",
    "href": "rmethods2/session_5.html#statistical-significance-1",
    "title": "Research Methods II",
    "section": "",
    "text": "What is statistical significance?\n\nStatistical significance is a way of determining if an observed effect is due to chance.\n\nWe typically use statistical significance to determine if the results of a study are meaningful, using various criteria.\n\nIs the p-value less than 0.05?\nDoes the 95% confidence interval include zero?\nis the t-statistic greater than 1.96?\n\nBut what exactly does that tell us?"
  },
  {
    "objectID": "rmethods2/session_5.html#back-to-the-basics",
    "href": "rmethods2/session_5.html#back-to-the-basics",
    "title": "Research Methods II",
    "section": "",
    "text": "Assume you are testing for the effectiveness of a medicine that treats the common cold.\n\nHow do you know if the treatment is effective? (i.e., Does reduce the duration of the cold?)\nWe make an hypothesis!\nThe null hypothesis is that there is no effect of the treatment. (H0: no effect)\nYou collect some data and find out that it reduces the duration of the cold by 1 days.\nIs this a significant effect?"
  },
  {
    "objectID": "rmethods2/session_5.html#section-2",
    "href": "rmethods2/session_5.html#section-2",
    "title": "Research Methods II",
    "section": "",
    "text": "The sample size (how many people were in the study?)\n\nIf the sample size is 10,000, then a 1 day reduction in the duration of the cold may be significant.\nBut if the sample size is 10, then a 1 day reduction may be due to chance.\n\nThe effect size\n\nIf the effect size is 0.1 days, may not be significant.\nIf the effect size is 10 days, may be significant."
  },
  {
    "objectID": "rmethods2/session_5.html#section-3",
    "href": "rmethods2/session_5.html#section-3",
    "title": "Research Methods II",
    "section": "",
    "text": "Statistical significance is a way of determining if the observed effect is due to chance.\nTo do this, we required assumptions about the distribution under the Null Hypothesis.\n\nAssume that the data is normally distributed.\nAnd that there is a 4% chance you observe a value as extreme as the one you observed\n\nIn that case you would say, the effect is significant.\n\n\nJust couple of caveats:\n\nSignicance can be achieved by either measuring a “large” effect\nor by measuring a “small” effect with a large sample size. (very precisely)\n\nThus, finding no significance could mean the effect is noise or that the sample size is too small.\nRecall Hypothesis can be true or not. We only know if the data is consistent with the hypothesis or not."
  },
  {
    "objectID": "rmethods2/session_5.html#section-4",
    "href": "rmethods2/session_5.html#section-4",
    "title": "Research Methods II",
    "section": "",
    "text": "The power of a statistical test represents the probability of detecting an effect, given that the effect is real.\nFor example, say a drug has the effect of reducing the duration of the common cold by 1 day. But you do not know this\nInstead you make your hypothesis. How likely is that the effect is significant?\n\nYou find the effect is not significant at 10% level.\n\nWhat is happening?\n\nThe effect was true, yet we find no significance.\nThe power of the test was low. (sample size was too small)\n\nIn General, Setting high significance levels will reduce the power of the test."
  },
  {
    "objectID": "rmethods2/session_5.html#section-5",
    "href": "rmethods2/session_5.html#section-5",
    "title": "Research Methods II",
    "section": "",
    "text": "If At First You Don’t Succeed, Try, Try Again\n\nRQuestionTestingResults\n\n\n\n\n\n\n\n\n\n\n\n\nCartoon from xkcd, by Randall Munroe"
  },
  {
    "objectID": "rmethods2/session_5.html#section-6",
    "href": "rmethods2/session_5.html#section-6",
    "title": "Research Methods II",
    "section": "",
    "text": "Multiple Hypothesis Testing\n\nIn the previous example, the “SAME” hypothesis was tested multiple times.\nYet, it was still compared to the same significance level. Is this correct?\n\nConsider the following:\n\nYou collect 100 data points, from a normal distribution, with mean 0 and standard deviation 1.\nYou know there is only a 5% chance that the mean is greater (abs) than 0.196 (95% confidence interval)\n\nSo you run the same experiment 100 times - How many times do you expect to find a mean greater than 0.196? - What are the chances of finding a mean greater than 0.196 at least once?"
  },
  {
    "objectID": "rmethods2/session_5.html#section-7",
    "href": "rmethods2/session_5.html#section-7",
    "title": "Research Methods II",
    "section": "",
    "text": "A1: 5% of the time\nA2:\n\nPr of finding any “significant” effect in one experiment: \\(1 - 0.95 = 0.05\\)\nPr of finding any “significant” effect in two experiments: \\(1-0.95^2 = 0.0975\\)\nPr of finding no effect in 100 experiments: \\(1-0.95^{100}  = 0.99408\\)\n\n\nSo if you run the experiment enough times, you are almost certain to find a “significant” effect.\n\nAlso, While a single experiment has a 5% chance of finding a “significant” effect (alpha = 0.05), the “alpha” for 2 experiments is 0.0975!"
  },
  {
    "objectID": "rmethods2/session_5.html#section-9",
    "href": "rmethods2/session_5.html#section-9",
    "title": "Research Methods II",
    "section": "",
    "text": "Controlling for Multiple Hypothesis Testing\n\nThere are various ways to control for multiple hypothesis testing.\nSIDAK = \\(\\alpha_{adj} = (1-\\alpha_{tg})^{1/n}\\)\nBONFERRONI = \\(\\alpha_{adj} = \\alpha_{tg}/n\\)\nHOLM = \\(\\alpha_{adj,i} = \\alpha_{tg}/(n-i+1)\\)\n\nWhere \\(\\alpha_{tg}\\) is the target \\(\\alpha\\) level (e.g., 0.05), and \\(n\\) is the number of tests, and \\(\\alpha_{i,adj}\\) is the adjusted \\(\\alpha\\) level.\n\nThere is also Uniform Confidence Intervals (see here)\n\n\nThere is a caveat. They are designed to control for Type I errors (False positives), but they increase the chances of Type II errors. (Less power)"
  },
  {
    "objectID": "rmethods2/session_5.html#missing-data",
    "href": "rmethods2/session_5.html#missing-data",
    "title": "Research Methods II",
    "section": "Missing Data",
    "text": "Missing Data\n\nMissing data is a common problem in empirical research.\nDue to various reasons, some observations may be missing.\n\nRefusal to answer a question\nData entry errors/ommissions\nData loss\netc.\n\nThis can be a problem for various reasons:\n\nMissing data can produced biased and inconsistent estimates.\nIt may also reduce sample size, and thus power. (Potentially making estimation unfeasible)\n\nSo what can we do?"
  },
  {
    "objectID": "rmethods2/session_5.html#types-of-missing-data",
    "href": "rmethods2/session_5.html#types-of-missing-data",
    "title": "Research Methods II",
    "section": "Types of Missing Data",
    "text": "Types of Missing Data\n\nMissing Completely at Random (MCAR)\n\nThe probability of missing data does not depend on any observed or unobserved data.\nThis is the best case scenario. (this is like sampling)\n\nMissing at Random (MAR)\n\nThe probability of missing data depends on observed data.\nSecond Best: Its possible to address the problem using various methods.\n\nMissing Not at Random (MNAR)\n\nThe probability of missing data depends on unobserved data.\nWorst case scenario: It is usually very difficult to address"
  },
  {
    "objectID": "rmethods2/session_5.html#what-its-done-and-what-can-be-done",
    "href": "rmethods2/session_5.html#what-its-done-and-what-can-be-done",
    "title": "Research Methods II",
    "section": "What its done, and what can be done",
    "text": "What its done, and what can be done\n\nComplete Case Analysis (CCA)\n\nDrop observations with missing data.\nThis is the default in most statistical software.\nThis is a bad idea, unless the data is MCAR.\n\nImputation\n\nReplace missing values with a value.\nThis is a better idea, but it depends on the type of missing data.\nRequires modeling the missing data mechanism, and outcome model.\n\nReweighting\n\nWeight observations to account for missing data.\nRequires modeling the missing data mechanism"
  },
  {
    "objectID": "rmethods2/session_5.html#reweighting",
    "href": "rmethods2/session_5.html#reweighting",
    "title": "Research Methods II",
    "section": "Reweighting",
    "text": "Reweighting\nConsider the following example: \\[\\begin{aligned}\n\\text{Pop}&: y = x\\beta+ \\epsilon \\\\\n\\text{Miss Mech }&: p(nmiss|x) = F(x\\gamma) \\\\\n\\text{Miss Reg }&: m\\times y = m\\times x \\beta + m\\times \\epsilon  \n\\end{aligned}\n\\]\n\nWhere \\(m\\) is an indicator of missingness, and \\(F\\) is the function of missing.\nDefine the Weights as \\(w = \\frac{1}{1-p(nmiss|x)}\\)\nThen, we could use WLS to estimate the model of interest:\n\n\\[w \\times m\\times y = w \\times  m\\times x \\beta + w \\times  m\\times \\epsilon\\]"
  },
  {
    "objectID": "rmethods2/session_5.html#example",
    "href": "rmethods2/session_5.html#example",
    "title": "Research Methods II",
    "section": "Example",
    "text": "Example\n\nCodeResults\n\n\nfrause oaxaca, clear\ndrop if lnwage ==.\n** Modeling Missing\nreg lnwage c.(educ exper tenure female age)## c.(educ exper tenure female age)  \npredict lxb\nqui:sum lxb \nreplace lxb = normal((lxb -r(mean))/r(sd))\ngen lnwage2 = lnwage if lxb &lt;runiform()\ngen dwage = lnwage2!=.\n** Modeling Missing data\nlogit dwage educ exper tenure female age\npredict prw, pr\ngen wgt = 1/prw\n** Estimating the model\nreg lnwage educ exper tenure female age\nreg lnwage2 educ exper tenure female age\nreg lnwage2 educ exper tenure female age [w=wgt]\n** Repeat the process 1000 times"
  },
  {
    "objectID": "rmethods2/session_5.html#imputation-mean-and-predictive-mean",
    "href": "rmethods2/session_5.html#imputation-mean-and-predictive-mean",
    "title": "Research Methods II",
    "section": "Imputation: Mean and Predictive Mean",
    "text": "Imputation: Mean and Predictive Mean\n\nThe second approach is to impute the missing values. AKA Substitute the unobserved values with some prediction we can construct.\n\nConsider the case of a single variable \\(Z\\) with missing values, and assume we have a model for \\(Z\\):\n\\[Z = X\\beta + \\epsilon\n\\]"
  },
  {
    "objectID": "rmethods2/session_5.html#section-10",
    "href": "rmethods2/session_5.html#section-10",
    "title": "Research Methods II",
    "section": "",
    "text": "We could “predict” missing values using the mean of the observed values:\n\n\\[\\hat{Z} = \\bar{Z} = \\frac{1}{n}\\sum_{i=1}^n Z_i\\]\n\nOr we could use the predicted values from the model:\n\n\\[\\hat{Z} = X\\hat{\\beta}\\]\nNeither is a good idea, even under MCAR. (Why?)\n\nWe are getting rid of ALL uncertainty (variance) in the missing values."
  },
  {
    "objectID": "rmethods2/session_5.html#section-12",
    "href": "rmethods2/session_5.html#section-12",
    "title": "Research Methods II",
    "section": "",
    "text": "Better Approach: Stochastic Imputation\n\nA better approach of imputation is to use a model to predict not only the “known” variation (Conditional mean), but also the “unknown” variation (Conditional variance).\nSo, we can use the model to predict the missing values, but we add some noise to the prediction.\n\n\\[\\tilde z = X\\hat{\\beta} + \\hat \\epsilon\\]\n\nWhere \\(\\hat \\epsilon\\) is a “random” residual obtain based on the model assumptions.\n\\(\\tilde z\\) is a stochastic imputation of \\(z\\)."
  },
  {
    "objectID": "rmethods2/session_5.html#section-13",
    "href": "rmethods2/session_5.html#section-13",
    "title": "Research Methods II",
    "section": "",
    "text": "Even Better: Account for the uncertainty in the model\n\nWe can also account for the uncertainty in the model by considering the uncertainty in the model parameters, and the error:\n\n\\[z = X\\beta + \\epsilon\n\\]\n\nUnder normality assumptions, we could estimate the model using MLE, and obtain the variance covariance matrix of the parameters.\n\n\\[\n\\begin{pmatrix}\n\\hat{\\beta} \\\\ \\hat{\\sigma}^2 \\end{pmatrix}\n\\sim N\\left(\\begin{bmatrix} \\beta \\\\\n\\sigma\n\\end{bmatrix}, \\begin{bmatrix}\nV_{\\beta} & 0 \\\\\n0 & V_{\\sigma}\n\\end{bmatrix}\\right)\n\\]\n\nSo, we can get \\(\\tilde \\beta\\) and \\(\\tilde \\sigma\\), from random draws from the distribution above, and then use them to impute the missing values."
  },
  {
    "objectID": "rmethods2/session_5.html#section-14",
    "href": "rmethods2/session_5.html#section-14",
    "title": "Research Methods II",
    "section": "",
    "text": "Even Better than before: Multiple Imputation\n\nThe previous methods assumed you only need one imputation to solve the Imputation problem.\nOne, however, may not be enough to account for the uncertainty in the imputation process.\nSo, we can repeat the imputation process multiple times, and obtain multiple imputed values for each missing data.\nWith multiple imputed values, we can estimate the model of interest multiple times, and then combine the results using Rubin’s rules."
  },
  {
    "objectID": "rmethods2/session_5.html#section-15",
    "href": "rmethods2/session_5.html#section-15",
    "title": "Research Methods II",
    "section": "",
    "text": "Call M the number of imputations, and \\(m\\) the imputation index.\n\n\\[\\beta_{MI} = \\frac{1}{M}\\sum_{m=1}^M \\beta_m\\]\n\\[V_{MI} = \\frac{1}{M}\\sum_{m=1}^M V_m + \\left(\\frac{M+1}{M}\\right)Var(\\beta_m)\\]\n\nWhere \\(V_m\\) is the VCV matrix of the parameters for each imputation, and \\(Var(\\beta_m)\\) is the variance of the parameters across imputations.\n\n\\[df = (M-1) \\left( 1 + \\frac{M}{M+1}\\frac{Var_m}{Var_B}\\right)^2\\]"
  },
  {
    "objectID": "rmethods2/session_5.html#section-17",
    "href": "rmethods2/session_5.html#section-17",
    "title": "Research Methods II",
    "section": "",
    "text": "Example: Stata\n\nfrause oaxaca, clear\ndrop if lnwage ==.\n\n** Modeling Missing\nforeach i in  educ exper tenure age {\n    gen m_`i' = `i' if runiform()&gt;.25\n}\n\n\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n(362 missing values generated)\n(311 missing values generated)\n(387 missing values generated)\n(348 missing values generated)\n\n\nSetting data for -mi- commands\n\nmi set wide\nmi register imputed m_*\nmi impute chain (reg) m_* = lnwage single female, add(10) \n\n\nConditional models:\n           m_exper: regress m_exper m_age m_educ m_tenure lnwage single\n                     female\n             m_age: regress m_age m_exper m_educ m_tenure lnwage single\n                     female\n            m_educ: regress m_educ m_exper m_age m_tenure lnwage single\n                     female\n          m_tenure: regress m_tenure m_exper m_age m_educ lnwage single\n                     female\n\nPerforming chained iterations ...\n\nMultivariate imputation                     Imputations =       10\nChained equations                                 added =       10\nImputed: m=1 through m=10                       updated =        0\n\nInitialization: monotone                     Iterations =      100\n                                                burn-in =       10\n\n            m_educ: linear regression\n           m_exper: linear regression\n          m_tenure: linear regression\n             m_age: linear regression\n\n------------------------------------------------------------------\n                   |               Observations per m             \n                   |----------------------------------------------\n          Variable |   Complete   Incomplete   Imputed |     Total\n-------------------+-----------------------------------+----------\n            m_educ |       1072          362       362 |      1434\n           m_exper |       1123          311       311 |      1434\n          m_tenure |       1047          387       387 |      1434\n             m_age |       1086          348       348 |      1434\n------------------------------------------------------------------\n(Complete + Incomplete = Total; Imputed is the minimum across m\n of the number of filled-in observations.)\n\n\nEstmating the model(s):\n\nmi estimate, post: regress lnwage m_* single female\nest sto m1\nregress lnwage educ exper tenure age single  female\n\n\nMultiple-imputation estimates                   Imputations       =         10\nLinear regression                               Number of obs     =      1,434\n                                                Average RVI       =     0.3607\n                                                Largest FMI       =     0.5511\n                                                Complete DF       =       1427\nDF adjustment:   Small sample                   DF:     min       =      31.27\n                                                        avg       =     293.52\n                                                        max       =   1,277.02\nModel F test:       Equal FMI                   F(   6,  403.1)   =      63.74\nWithin VCE type:          OLS                   Prob &gt; F          =     0.0000\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      m_educ |   .0780927   .0062465    12.50   0.000     .0656678    .0905176\n     m_exper |   .0034867    .002416     1.44   0.154    -.0013503    .0083237\n    m_tenure |   .0023203    .002857     0.81   0.423    -.0035046    .0081451\n       m_age |   .0099868   .0023905     4.18   0.000     .0052212    .0147524\n      single |    -.09733   .0309067    -3.15   0.002    -.1580571    -.036603\n      female |  -.1226849   .0255255    -4.81   0.000    -.1727613   -.0726084\n       _cons |   2.102173   .1059333    19.84   0.000     1.889132    2.315213\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     1,434\n-------------+----------------------------------   F(6, 1427)      =     86.35\n       Model |  107.645555         6  17.9409258   Prob &gt; F        =    0.0000\n    Residual |  296.474249     1,427  .207760511   R-squared       =    0.2664\n-------------+----------------------------------   Adj R-squared   =    0.2633\n       Total |  404.119804     1,433  .282009633   Root MSE        =    .45581\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0753085    .005253    14.34   0.000     .0650041    .0856129\n       exper |   .0026545   .0018941     1.40   0.161     -.001061    .0063701\n      tenure |   .0022932   .0019725     1.16   0.245    -.0015761    .0061625\n         age |   .0111437   .0019397     5.75   0.000     .0073388    .0149486\n      single |  -.0918932   .0292993    -3.14   0.002    -.1493674   -.0344189\n      female |  -.1289592   .0253754    -5.08   0.000    -.1787363   -.0791822\n       _cons |   2.100201   .0816356    25.73   0.000     1.940063     2.26034\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods2/session_5.html#what-about-ldv-models",
    "href": "rmethods2/session_5.html#what-about-ldv-models",
    "title": "Research Methods II",
    "section": "What about LDV models?",
    "text": "What about LDV models?\n\nThe method sketched above (OLS) can also be extended to other models\nConsider Logit models\n\nS1: Estimate Logit model: \\(P(y=1|X) = F(X\\beta)\\)\nS2: Draw \\(\\beta\\) from the distribution, call it \\(\\tilde\\beta\\)\nS3: Draw \\(y\\) from a Bernoulli distribution: \\(y \\sim Bernoulli(F(X\\tilde\\beta))\\)\n\nSimilar procedures can be done for other models."
  },
  {
    "objectID": "rmethods2/session_5.html#other-methods-hotdecking",
    "href": "rmethods2/session_5.html#other-methods-hotdecking",
    "title": "Research Methods II",
    "section": "Other Methods: HotDecking",
    "text": "Other Methods: HotDecking\n\nHotdecking is a method of imputation that uses the observed values of the data to impute the missing values.\nBecause it uses data from the empirical distribution (observed data), it produces “valid” imputations for any kind of data.\nThe idea is to find a pool of potential “donors” for the one with missing data. (similar observations)\nThen select one candidate and use its data to impute the missing values."
  },
  {
    "objectID": "rmethods2/session_5.html#section-18",
    "href": "rmethods2/session_5.html#section-18",
    "title": "Research Methods II",
    "section": "",
    "text": "Definition of a “donor”\n\nDonors are identified as observations with similar characteristics to the one with missing data. (close to the missing observation)\nFinding potential donors is easy when there is low dimensional data, but it becomes more difficult as the number of variables increases."
  },
  {
    "objectID": "rmethods2/session_5.html#example-1",
    "href": "rmethods2/session_5.html#example-1",
    "title": "Research Methods II",
    "section": "Example",
    "text": "Example\nImputing data for wages in oaxaca.\n\nfrause oaxaca, clear\n** ID pool of donors based on age and gender\negen id_pool = group(age female)\n** Now, for each missing observation select a \"random\" donor\ngen misswage =missing(lnwage)\nbysort id_pool (misswage):egen smp = sum(misswage==0)\nbysort id_pool: gen draw = runiformint(1, smp)\nbysort id_pool: replace lnwage = lnwage[draw] if misswage==1\nsum lnwage if misswage==0\nsum lnwage if misswage==1 \nsum lnwage \n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 real changes made)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      lnwage |      1,434    3.357604    .5310458    .507681   5.259097\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      lnwage |        213    3.342183    .5482005    .507681   5.259097\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      lnwage |      1,647     3.35561    .5331507    .507681   5.259097"
  },
  {
    "objectID": "rmethods2/session_5.html#section-19",
    "href": "rmethods2/session_5.html#section-19",
    "title": "Research Methods II",
    "section": "",
    "text": "With many variables, we often estimating some distance measure and/or data reduction to ID “close” observations:\n\nPropensity Score (based on logit/probit/regress).\n\n\\[D(X,X_0) = abs(G(X) - G(X_0))\\]\n\nMahalanobis distance (based on X covariance matrix)\n\n\\[D(X,X_0) = \\sqrt{(X-X_0)'\\Sigma^{-1}(X-X_0)}\\]\n\nAffinity score: based on some linear combination of variables.\n\n\\[D(X,X_0) = \\frac{1}{K}\\sum_{i=1}^K \\alpha_i f\\left(\\frac{X_i - X_{0i}}{h}\\right)\\]"
  },
  {
    "objectID": "rmethods2/session_5.html#section-20",
    "href": "rmethods2/session_5.html#section-20",
    "title": "Research Methods II",
    "section": "",
    "text": "Once distances are estimated, donor pools can be defined based on the distance measure.\n\nSay, all observations with distance less than 0.1.\n\nAnd the donor can be selected randomly from the pool. (or weighted by distance)\nThis approaches could be vary computationally intensive, because it requires estimating \\(N\\times N\\) distances.\nComplexity may be reduced by using data reduction techniques, such as PCA, FA or propensity scores"
  },
  {
    "objectID": "rmethods2/session_5.html#section-21",
    "href": "rmethods2/session_5.html#section-21",
    "title": "Research Methods II",
    "section": "",
    "text": "Example: Stata\nUsing single Score (Data reduction)\n\nfrause oaxaca, clear\ndrop if lnwage ==.\n// 25% of data is missing\ngen mlnwage = lnwage if runiform()&gt;.25\ngen misswage =missing(mlnwage)\nqui:logit misswage  educ age agesq female single married\npredict psc, xb\nqui:reg mlnwage  educ age agesq female single married\npredict lnwh, xb\nqui:pca educ age agesq female single married \nqui:predict pc1, score\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n(351 missing values generated)\n\n\nFor imputation, lets do something simple, Use data from the closet observation (with lower score) as donor.\n\nforeach i in psc lnwh pc1 {\n \n    drop2   lnwage_`i'\n    gen lnwage_`i' = mlnwage    \n    sort `i'\n    replace lnwage_`i'=lnwage_`i'[_n-1] if lnwage_`i'==. & lnwage_`i'[_n-1]!=.\n    *replace lnwage_`i'=lnwage_`i'[_n+1] if lnwage_`i'==. & lnwage_`i'[_n+1]!=.\n    qui:_regress lnwage_`i'  educ age agesq female single married\n    matrix b`i' = e(b)\n    matrix coleq b`i'=`i'\n    \n}\n\nvariable lnwage_psc not found\n(351 missing values generated)\n(351 real changes made)\nvariable lnwage_lnwh not found\n(351 missing values generated)\n(351 real changes made)\nvariable lnwage_pc1 not found\n(351 missing values generated)\n(351 real changes made)\n\n\nEstimate models 1000 times, and lets see results"
  },
  {
    "objectID": "rmethods2/session_5.html#comparison-of-methods",
    "href": "rmethods2/session_5.html#comparison-of-methods",
    "title": "Research Methods II",
    "section": "Comparison of methods",
    "text": "Comparison of methods"
  },
  {
    "objectID": "rmethods2/session_3.html",
    "href": "rmethods2/session_3.html",
    "title": "Research Methods II",
    "section": "",
    "text": "Economic inequality refers to how economic variables are distributed among individuals in a group, among groups in a population, or among countries.\nInequality of What?\n\ninequality of opportunities, for example access to employment or education\ninequality of outcomes, for example material dimensions of human well-being, such as the level of income, educational attainment, health status and so on.\n\nFor now we will focus on income inequality.\n\n\n\n\n\nThere are various approaches that have been used for the analysis of Inequality\n\nIntuitive approach\n\nUnaxiomatic approach used to describe inequality.\n\nNormative approach-Social welfare\n\nUses explicit concepts of welfare functions to quantify inequality\n\nInformation theory\n\nQuantifies inequality treating it as a problem of comparing income distribution probabilities.\n\nAxiomatic approach\n\nUses a series of axioms to create measures of inequality\n\n\n\n\n\n\n\nRegardless of the approach, there are some basic steps required to measure inequality\n\nDefine the population of interest\nDefine the measure of interest\nAdjust for prices (if necessary)\nAdjust for individual heterogeneity (needs) (if necessary)\n\n\n\n\n\n\nLet \\(y_i\\) be the income of individual \\(i\\) in the population. Assume that \\(y_i&gt;&gt;0\\).\nAssume that \\(y_i\\) can be characterized by a probability distribution function \\(f(y)\\).\n\n\\[\\begin{aligned}\ny_i &\\sim f(y) \\rightarrow \\int_{-\\infty}^{z} f(y) dy = F(z) \\\\\nF(0)&=0 \\ \\& \\ F(\\infty)=1 \\\\\nF(Q_y(p)) &=p \\rightarrow Q_y(p) = F^{-1}(p)\n\\end{aligned}\n\\]\nThe \\(p_{th}\\) quantile of \\(y_i\\) is the value \\(Q_y(p)\\) such that \\(p\\) percent of the population has income below \\(Q_y(p)\\).\n\n\n\nMean of Standard of Living:\n\\[\\mu_y = E(y) = \\int_{-\\infty}^{\\infty} y f(y) dy=\\int_0^1Q(p)dp\n\\]\nFinally, the inequality measure can be written as:\n\\[I(y)=I(\\mu_y, f(.)) = I(\\mu_y, F(.))\n\\]\n\n\n\n\nThere are several tools that can be used to visualize income distribution:\n\nDensity Function/Histogram\nPen parade/Cumulative Distribution Function\nLorenz Curve\n\n\n\n\n\n\nDensity/HistogramPlots\n\n\n\nDensity functions and histograms are used to visualize the distribution of income in the population.\nThey could be used to detect multimodality, skewness, etc\nAnd could be used to compare distributions across groups.\nStata Commands\nhistogram varname [weight] [if]\nkdensity varname [weight] [if]\n\n\n\n\n\nCode\nset scheme white2\nset linesize 255\ncolor_style tableau\nqui:frause oaxaca, clear\nsum wt, meanonly\ngen int wt2 = round(wt/r(min))\nqui:two histogram lnwage [fw=wt2] ///\n    || kdensity lnwage [w=wt2], ///\n    ysize(5) xsize(9) xtitle(\"Log Wages\") ///\n    legend(order(1 \"Histogram\" 2 \"Kernel Density\") pos(6) col(2)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPenParade/CDFplot\n\n\n\nA different approach to visualize the distribution.\nThe Pen Parade plots the values of the variable of interest in ascending order.\n\ny-axis: Q(p); x-axis: p\n\nThe CDF plots the cumulative distribution of the variable of interest.\n\ny-axis: p; x-axis: Q(p)\n\nThey give you a sense of the distribution, and easy comparison across high and low values.\n\n\n\n\nCode\nqui:pctile qlnwage = lnwage [w=wt], nq(100)\nqui:gen  qwage = exp(qlnwage)\nqui:gen p = _n if _n&lt;100\nscatter qwage p, connect(l) name(m1, replace) ysize(5) xsize(8) title(\"Pen Parade\")\nscatter p qwage, connect(l) name(m2, replace) ysize(5) xsize(8) title(\"Cumulative Distribution Function\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLorenz CurveCodePlotAlternative:\n\n\n\nPerhaps the most popular tool to visualize income distribution.\nThis curve plots the cumulative share of income vs the cumulative share of population.\n\nHow much of total income is held by the bottom X% of the population ?\n\n“Easy” to read:\n\nThe closest it is to the 45 degree line, the more equal the distribution.\n\nNot easy to implement with negative and zero incomes.\nComparison across groups may be ambiguous.\nits always increasing at an increasing rate respect to X%\n\n\n\nAssume data is sorted by income\n\n\nx-axis: Cum share of population\n\\[P_j = \\frac{\\sum_{i=1}^j w_i}{\\sum_{i=1}^n w_i}\\]\n\ny-axis: Cum share of income\n\\[LC_j = \\frac{\\sum_{i=1}^j y_i w_i}{\\sum_{i=1}^n y_iw_i}\\]\n\n\n\nfrause oaxaca, clear\ngen wage = exp(lnwage)\nsort wage wt  // sort by income and weight \n// Estimate Totals for non missing data\negen twage = sum(wage * wt) if wage!=.\negen tpop  = sum(wt)        if wage!=.\n// get cumulative shares\ngen lc_i = sum( (wage*wt/twage) )*100 if wage!=.\ngen p_i  = sum( (wt/tpop) )*100      if wage!=.\n\n\n\n\n\nCode\ntwo (line lc_i p_i) /// Lorenz Curve\n    ( function x, range(0 100) ) , /// 45 degree line\n    aspect(1) ysize(5) xsize(8) ///\n    xtitle(\"Cumulative Share of Population\") ///\n    ytitle(\"Cumulative Share of Income\") ///\n    legend(off)\n\n\n\n\n\nLorenz Curve\n\n\n\n\n\n\nssc install glcurve // installs command for Generalized Lorenz Curve\nglcurve wage [aw = wt], /// provides variable and weight\n    lorenz // Request ploting the Lorenz Curve\n\n\n\n\n\n\nThere are several measures of inequality. The most popular are:\n\nInterquantile Range (IQR) (or normalizations) \\[IQR(\\#1,\\#2) = Q(\\#2) - Q(\\#1)\n\\]\nInterquantile Share Ratio (Palma ratio (10/40)) \\[ISR(\\#1,\\#2) = \\frac{1-LC(\\#2)}{LC(\\#1)}\n\\]\nCoefficient of Variation (CV) \\[CV = \\frac{\\sigma_y}{\\mu_y}\n\\]\n\n\n\n\n\nLorenz Curve:\n\n\\[LC(p) = \\frac{\\int_0^p Q_y(u)du}{\\int_0^1 Q_y(u)du}\n= \\frac{1}{\\mu_y} \\int_0^p Q_y(u)du\n\\]\n\nProperties 1: Lorenz Curve is a non-decreasing function of \\(p\\).\n\n\\[\\frac{\\partial LC(p)}{\\partial p} = \\frac{Q_y(p)}{\\mu_y} \\geq 0\\]\n\nProperties 2: Lorenz Curve is a concave function of \\(p\\) (increases at a fasterate). \\[\\frac{\\partial^2 LC(p)}{\\partial p^2} = \\frac{1}{\\mu_y f(y)} \\geq 0\\]\n\n\n\n\n\nThe Gini coefficient is the most popular measure of inequality.\nIt is defined as (2x) the area between the Lorenz Curve and the 45 degree line.\n\n\\[Gini(y) = 2 \\int_0^1 (p-LC(p)) dp\\]\n\nwhere \\(p-LC(p)\\) is the “loss” of income the Bottom \\(p\\) percent of the population experiences.\nIt is bounded between 0 (perfect Equality) and 1 (complete Inequality).\nWhen Lorenz do not cross, Gini provides unambiguous ranking of inequality.\n\n\\[Gini(y) = \\frac{2}{\\mu_y} Cov(y_p,p)\\]\n\n\n\n\nStata has plenty of commands that can be used to estimate Gini\n\nsearch gini for few examples\n\nI suggest 3 commands:\n\nfastgini (ssc install fastgini)\nineqdeco (ssc install ineqdeco)\nsgini (ssc install sgini)\nrif (ssc install rif)\n\n\n\n\n\n\ncapture:ssc install sgini\nsgini wage \n\n\nGini coefficient for wage\n\n-----------------------\n    Variable |      v=2\n-------------+---------\n        wage |   0.2460\n-----------------------\n\n\n\ncapture:ssc install ineqdeco\nineqdeco wage [pw=wt]\n\n \nPercentile ratios\n\n----------------------------------------------------------\n  All obs |    p90/p10     p90/p50     p10/p50     p75/p25\n----------+-----------------------------------------------\n          |      3.154       1.694       0.537       1.771\n----------------------------------------------------------\n  \nGeneralized Entropy indices GE(a), where a = income difference\n sensitivity parameter, and Gini coefficient\n\n----------------------------------------------------------------------\n  All obs |     GE(-1)       GE(0)       GE(1)       GE(2)        Gini\n----------+-----------------------------------------------------------\n          |    0.23199     0.14240     0.12282     0.13398     0.26273\n----------------------------------------------------------------------\n   \nAtkinson indices, A(e), where e &gt; 0 is the inequality aversion parameter\n\n----------------------------------------------\n  All obs |     A(0.5)        A(1)        A(2)\n----------+-----------------------------------\n          |    0.06292     0.13273     0.31693\n----------------------------------------------\n\n\n\n\n\n\nThere are other approaches that can be used to measure inequality.\n\nNormative approach-Social welfare: Uses explicit concepts of welfare functions to quantify inequality\n\n\n\\[I_A(y,\\varepsilon) = 1 - \\left( \\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{y_i}{\\mu_y}\\right)^{1-\\varepsilon} \\right)^\\frac{1}{1-\\varepsilon}\n\\]\nwhere is a measure of inequality aversion.\n\nInformation theory: Quantifies inequality treating it as a problem of comparing income distribution probabilities. How far are we from Full Entropy\n\n\\[I_{GE}(Y,\\alpha)=\\frac{1}{\\alpha(1-\\alpha)}\\left[\\frac{1}{N} \\sum \\left(\\frac{y_i}{\\mu_y}\\right)^\\alpha -1\\right]\n\\]\n\nAxiomatic approach\n\nUses a series of axioms to create measures of inequality"
  },
  {
    "objectID": "rmethods2/session_3.html#what-is-inequality",
    "href": "rmethods2/session_3.html#what-is-inequality",
    "title": "Research Methods II",
    "section": "",
    "text": "Economic inequality refers to how economic variables are distributed among individuals in a group, among groups in a population, or among countries.\nInequality of What?\n\ninequality of opportunities, for example access to employment or education\ninequality of outcomes, for example material dimensions of human well-being, such as the level of income, educational attainment, health status and so on.\n\nFor now we will focus on income inequality."
  },
  {
    "objectID": "rmethods2/session_3.html#how-do-you-analyze-measure-inequality",
    "href": "rmethods2/session_3.html#how-do-you-analyze-measure-inequality",
    "title": "Research Methods II",
    "section": "",
    "text": "There are various approaches that have been used for the analysis of Inequality\n\nIntuitive approach\n\nUnaxiomatic approach used to describe inequality.\n\nNormative approach-Social welfare\n\nUses explicit concepts of welfare functions to quantify inequality\n\nInformation theory\n\nQuantifies inequality treating it as a problem of comparing income distribution probabilities.\n\nAxiomatic approach\n\nUses a series of axioms to create measures of inequality"
  },
  {
    "objectID": "rmethods2/session_3.html#preliminaries",
    "href": "rmethods2/session_3.html#preliminaries",
    "title": "Research Methods II",
    "section": "",
    "text": "Regardless of the approach, there are some basic steps required to measure inequality\n\nDefine the population of interest\nDefine the measure of interest\nAdjust for prices (if necessary)\nAdjust for individual heterogeneity (needs) (if necessary)"
  },
  {
    "objectID": "rmethods2/session_3.html#mathematical-preliminaries",
    "href": "rmethods2/session_3.html#mathematical-preliminaries",
    "title": "Research Methods II",
    "section": "",
    "text": "Let \\(y_i\\) be the income of individual \\(i\\) in the population. Assume that \\(y_i&gt;&gt;0\\).\nAssume that \\(y_i\\) can be characterized by a probability distribution function \\(f(y)\\).\n\n\\[\\begin{aligned}\ny_i &\\sim f(y) \\rightarrow \\int_{-\\infty}^{z} f(y) dy = F(z) \\\\\nF(0)&=0 \\ \\& \\ F(\\infty)=1 \\\\\nF(Q_y(p)) &=p \\rightarrow Q_y(p) = F^{-1}(p)\n\\end{aligned}\n\\]\nThe \\(p_{th}\\) quantile of \\(y_i\\) is the value \\(Q_y(p)\\) such that \\(p\\) percent of the population has income below \\(Q_y(p)\\)."
  },
  {
    "objectID": "rmethods2/session_3.html#mathematical-preliminaries-1",
    "href": "rmethods2/session_3.html#mathematical-preliminaries-1",
    "title": "Research Methods II",
    "section": "",
    "text": "Mean of Standard of Living:\n\\[\\mu_y = E(y) = \\int_{-\\infty}^{\\infty} y f(y) dy=\\int_0^1Q(p)dp\n\\]\nFinally, the inequality measure can be written as:\n\\[I(y)=I(\\mu_y, f(.)) = I(\\mu_y, F(.))\n\\]"
  },
  {
    "objectID": "rmethods2/session_3.html#visualization-tools",
    "href": "rmethods2/session_3.html#visualization-tools",
    "title": "Research Methods II",
    "section": "",
    "text": "There are several tools that can be used to visualize income distribution:\n\nDensity Function/Histogram\nPen parade/Cumulative Distribution Function\nLorenz Curve"
  },
  {
    "objectID": "rmethods2/session_3.html#section",
    "href": "rmethods2/session_3.html#section",
    "title": "Research Methods II",
    "section": "",
    "text": "Density/HistogramPlots\n\n\n\nDensity functions and histograms are used to visualize the distribution of income in the population.\nThey could be used to detect multimodality, skewness, etc\nAnd could be used to compare distributions across groups.\nStata Commands\nhistogram varname [weight] [if]\nkdensity varname [weight] [if]\n\n\n\n\n\nCode\nset scheme white2\nset linesize 255\ncolor_style tableau\nqui:frause oaxaca, clear\nsum wt, meanonly\ngen int wt2 = round(wt/r(min))\nqui:two histogram lnwage [fw=wt2] ///\n    || kdensity lnwage [w=wt2], ///\n    ysize(5) xsize(9) xtitle(\"Log Wages\") ///\n    legend(order(1 \"Histogram\" 2 \"Kernel Density\") pos(6) col(2))"
  },
  {
    "objectID": "rmethods2/session_3.html#section-1",
    "href": "rmethods2/session_3.html#section-1",
    "title": "Research Methods II",
    "section": "",
    "text": "PenParade/CDFplot\n\n\n\nA different approach to visualize the distribution.\nThe Pen Parade plots the values of the variable of interest in ascending order.\n\ny-axis: Q(p); x-axis: p\n\nThe CDF plots the cumulative distribution of the variable of interest.\n\ny-axis: p; x-axis: Q(p)\n\nThey give you a sense of the distribution, and easy comparison across high and low values.\n\n\n\n\nCode\nqui:pctile qlnwage = lnwage [w=wt], nq(100)\nqui:gen  qwage = exp(qlnwage)\nqui:gen p = _n if _n&lt;100\nscatter qwage p, connect(l) name(m1, replace) ysize(5) xsize(8) title(\"Pen Parade\")\nscatter p qwage, connect(l) name(m2, replace) ysize(5) xsize(8) title(\"Cumulative Distribution Function\")"
  },
  {
    "objectID": "rmethods2/session_3.html#section-2",
    "href": "rmethods2/session_3.html#section-2",
    "title": "Research Methods II",
    "section": "",
    "text": "Lorenz CurveCodePlotAlternative:\n\n\n\nPerhaps the most popular tool to visualize income distribution.\nThis curve plots the cumulative share of income vs the cumulative share of population.\n\nHow much of total income is held by the bottom X% of the population ?\n\n“Easy” to read:\n\nThe closest it is to the 45 degree line, the more equal the distribution.\n\nNot easy to implement with negative and zero incomes.\nComparison across groups may be ambiguous.\nits always increasing at an increasing rate respect to X%\n\n\n\nAssume data is sorted by income\n\n\nx-axis: Cum share of population\n\\[P_j = \\frac{\\sum_{i=1}^j w_i}{\\sum_{i=1}^n w_i}\\]\n\ny-axis: Cum share of income\n\\[LC_j = \\frac{\\sum_{i=1}^j y_i w_i}{\\sum_{i=1}^n y_iw_i}\\]\n\n\n\nfrause oaxaca, clear\ngen wage = exp(lnwage)\nsort wage wt  // sort by income and weight \n// Estimate Totals for non missing data\negen twage = sum(wage * wt) if wage!=.\negen tpop  = sum(wt)        if wage!=.\n// get cumulative shares\ngen lc_i = sum( (wage*wt/twage) )*100 if wage!=.\ngen p_i  = sum( (wt/tpop) )*100      if wage!=.\n\n\n\n\n\nCode\ntwo (line lc_i p_i) /// Lorenz Curve\n    ( function x, range(0 100) ) , /// 45 degree line\n    aspect(1) ysize(5) xsize(8) ///\n    xtitle(\"Cumulative Share of Population\") ///\n    ytitle(\"Cumulative Share of Income\") ///\n    legend(off)\n\n\n\n\n\nLorenz Curve\n\n\n\n\n\n\nssc install glcurve // installs command for Generalized Lorenz Curve\nglcurve wage [aw = wt], /// provides variable and weight\n    lorenz // Request ploting the Lorenz Curve"
  },
  {
    "objectID": "rmethods2/session_3.html#inequality-measures",
    "href": "rmethods2/session_3.html#inequality-measures",
    "title": "Research Methods II",
    "section": "",
    "text": "There are several measures of inequality. The most popular are:\n\nInterquantile Range (IQR) (or normalizations) \\[IQR(\\#1,\\#2) = Q(\\#2) - Q(\\#1)\n\\]\nInterquantile Share Ratio (Palma ratio (10/40)) \\[ISR(\\#1,\\#2) = \\frac{1-LC(\\#2)}{LC(\\#1)}\n\\]\nCoefficient of Variation (CV) \\[CV = \\frac{\\sigma_y}{\\mu_y}\n\\]"
  },
  {
    "objectID": "rmethods2/session_3.html#inequality-measures-1",
    "href": "rmethods2/session_3.html#inequality-measures-1",
    "title": "Research Methods II",
    "section": "",
    "text": "Lorenz Curve:\n\n\\[LC(p) = \\frac{\\int_0^p Q_y(u)du}{\\int_0^1 Q_y(u)du}\n= \\frac{1}{\\mu_y} \\int_0^p Q_y(u)du\n\\]\n\nProperties 1: Lorenz Curve is a non-decreasing function of \\(p\\).\n\n\\[\\frac{\\partial LC(p)}{\\partial p} = \\frac{Q_y(p)}{\\mu_y} \\geq 0\\]\n\nProperties 2: Lorenz Curve is a concave function of \\(p\\) (increases at a fasterate). \\[\\frac{\\partial^2 LC(p)}{\\partial p^2} = \\frac{1}{\\mu_y f(y)} \\geq 0\\]"
  },
  {
    "objectID": "rmethods2/session_3.html#inequality-measures-gini-coefficient",
    "href": "rmethods2/session_3.html#inequality-measures-gini-coefficient",
    "title": "Research Methods II",
    "section": "",
    "text": "The Gini coefficient is the most popular measure of inequality.\nIt is defined as (2x) the area between the Lorenz Curve and the 45 degree line.\n\n\\[Gini(y) = 2 \\int_0^1 (p-LC(p)) dp\\]\n\nwhere \\(p-LC(p)\\) is the “loss” of income the Bottom \\(p\\) percent of the population experiences.\nIt is bounded between 0 (perfect Equality) and 1 (complete Inequality).\nWhen Lorenz do not cross, Gini provides unambiguous ranking of inequality.\n\n\\[Gini(y) = \\frac{2}{\\mu_y} Cov(y_p,p)\\]"
  },
  {
    "objectID": "rmethods2/session_3.html#implementation",
    "href": "rmethods2/session_3.html#implementation",
    "title": "Research Methods II",
    "section": "",
    "text": "Stata has plenty of commands that can be used to estimate Gini\n\nsearch gini for few examples\n\nI suggest 3 commands:\n\nfastgini (ssc install fastgini)\nineqdeco (ssc install ineqdeco)\nsgini (ssc install sgini)\nrif (ssc install rif)"
  },
  {
    "objectID": "rmethods2/session_3.html#section-3",
    "href": "rmethods2/session_3.html#section-3",
    "title": "Research Methods II",
    "section": "",
    "text": "capture:ssc install sgini\nsgini wage \n\n\nGini coefficient for wage\n\n-----------------------\n    Variable |      v=2\n-------------+---------\n        wage |   0.2460\n-----------------------\n\n\n\ncapture:ssc install ineqdeco\nineqdeco wage [pw=wt]\n\n \nPercentile ratios\n\n----------------------------------------------------------\n  All obs |    p90/p10     p90/p50     p10/p50     p75/p25\n----------+-----------------------------------------------\n          |      3.154       1.694       0.537       1.771\n----------------------------------------------------------\n  \nGeneralized Entropy indices GE(a), where a = income difference\n sensitivity parameter, and Gini coefficient\n\n----------------------------------------------------------------------\n  All obs |     GE(-1)       GE(0)       GE(1)       GE(2)        Gini\n----------+-----------------------------------------------------------\n          |    0.23199     0.14240     0.12282     0.13398     0.26273\n----------------------------------------------------------------------\n   \nAtkinson indices, A(e), where e &gt; 0 is the inequality aversion parameter\n\n----------------------------------------------\n  All obs |     A(0.5)        A(1)        A(2)\n----------+-----------------------------------\n          |    0.06292     0.13273     0.31693\n----------------------------------------------"
  },
  {
    "objectID": "rmethods2/session_3.html#other-inequality-measures",
    "href": "rmethods2/session_3.html#other-inequality-measures",
    "title": "Research Methods II",
    "section": "",
    "text": "There are other approaches that can be used to measure inequality.\n\nNormative approach-Social welfare: Uses explicit concepts of welfare functions to quantify inequality\n\n\n\\[I_A(y,\\varepsilon) = 1 - \\left( \\frac{1}{N} \\sum_{i=1}^N \\left(\\frac{y_i}{\\mu_y}\\right)^{1-\\varepsilon} \\right)^\\frac{1}{1-\\varepsilon}\n\\]\nwhere is a measure of inequality aversion.\n\nInformation theory: Quantifies inequality treating it as a problem of comparing income distribution probabilities. How far are we from Full Entropy\n\n\\[I_{GE}(Y,\\alpha)=\\frac{1}{\\alpha(1-\\alpha)}\\left[\\frac{1}{N} \\sum \\left(\\frac{y_i}{\\mu_y}\\right)^\\alpha -1\\right]\n\\]\n\nAxiomatic approach\n\nUses a series of axioms to create measures of inequality"
  },
  {
    "objectID": "rmethods2/session_3.html#significance-test",
    "href": "rmethods2/session_3.html#significance-test",
    "title": "Research Methods II",
    "section": "Significance test",
    "text": "Significance test\n\nAs discussed in Session 1, we can use a t-test to compare means.\n\nThis requires estimating the standard error of the mean, use mean command, or regress\n\nSimilarly, it may be as important to test whether two distributions (or inequality measures) are different.\n\nThis requires estimating the standard error of the inequality measure.\nThis is not as straightforward as the mean.\n\nEasiest methods:\n\nBootstrap: requires bootstrap weights for survey data.\nInfluence function: requires deriving the influence function of the inequality measure."
  },
  {
    "objectID": "rmethods2/session_3.html#bootstrap",
    "href": "rmethods2/session_3.html#bootstrap",
    "title": "Research Methods II",
    "section": "Bootstrap",
    "text": "Bootstrap\n\nBootstrap its a non-parametric method to estimate the standard error of a statistic. Its based on Resampling and re-estimating data.\n\nbootstrap gini=r(coeff): sgini wage\n\n\n\nwarning: sgini does not set e(sample), so no observations will be excluded from the resampling because of missing values or other reasons. To exclude observations, press Break, save the data, drop any observations that are to be excluded, and rerun\n         bootstrap.\n\nBootstrap results                                        Number of obs = 1,647\n                                                         Replications  =    50\n\n      Command: sgini wage\n         gini: r(coeff)\n\n------------------------------------------------------------------------------\n             |   Observed   Bootstrap                         Normal-based\n             | coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        gini |   .2460329   .0063639    38.66   0.000     .2335599    .2585059\n------------------------------------------------------------------------------\n\n\n\nRIF (Recentered Influence Function) is a method that uses the moment conditions to estimate the standard error of a statistic.\n\nrifhdreg wage , rif(gini)\n\n\n\nLinear regression                               Number of obs     =      1,434\n                                                F(0, 1433)        =       0.00\n                                                Prob &gt; F          =          .\n                                                R-squared         =     0.0000\n                                                Root MSE          =     .24613\n\n------------------------------------------------------------------------------\n             |               Robust\n        wage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _cons |   .2460329   .0064995    37.85   0.000     .2332833    .2587825\n------------------------------------------------------------------------------\nDistributional Statistic: gini\nSample Mean    RIF gini :  .24603\n\n\n\nBetter yet, because you can use regressions (RIF-regressions), you can use weights, and test for differences in inequality across groups.\n\nrifhdreg wage ibn.female [pw=wt], rif(gini) over(female) noconstant\n\n\n\nLinear regression                               Number of obs     =      1,434\n                                                F(2, 1432)        =     640.56\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.5056\n                                                Root MSE          =     .25631\n\n------------------------------------------------------------------------------\n             |               Robust\n        wage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |\n          0  |   .2377884   .0084103    28.27   0.000     .2212907    .2542862\n          1  |   .2820059   .0128488    21.95   0.000     .2568015    .3072103\n------------------------------------------------------------------------------\nDistributional Statistic: gini\nSample Mean    RIF gini :  .25805\n\n\nrifhdreg wage i.female [pw=wt], rif(gini) over(female)\n\n\n\nLinear regression                               Number of obs     =      1,434\n                                                F(1, 1432)        =       8.29\n                                                Prob &gt; F          =     0.0040\n                                                R-squared         =     0.0073\n                                                Root MSE          =     .25631\n\n------------------------------------------------------------------------------\n             |               Robust\n        wage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    1.female |   .0442175   .0153565     2.88   0.004     .0140938    .0743412\n       _cons |   .2377884   .0084103    28.27   0.000     .2212907    .2542862\n------------------------------------------------------------------------------\nDistributional Statistic: gini\nSample Mean    RIF gini :  .25805"
  },
  {
    "objectID": "rmethods2/session_3.html#introduction",
    "href": "rmethods2/session_3.html#introduction",
    "title": "Research Methods II",
    "section": "Introduction",
    "text": "Introduction\n\nSome times, we may be interested in determining what factors, and what extent, explain inequality.\n\nBut what do we mean by “explain”?\n\nThere are several approaches to decompose inequality.\n\nby sources: Explain how inequality is related to source of income\nby groups: How much of the inequality is explained by inequality within groups, and between groups.\n\nWe could also consider how inequality gaps are related to characteristics or returns to such characteristics."
  },
  {
    "objectID": "rmethods2/session_3.html#decompose-by-sources",
    "href": "rmethods2/session_3.html#decompose-by-sources",
    "title": "Research Methods II",
    "section": "Decompose by sources",
    "text": "Decompose by sources\n\nSome inequality indices are well suited to decompose inequality by sources.\n\nVariance Decomposition (Shorrocks 1982)\nGini Decomposition by source (Lerman and Yitzhaki 1985)\n\nThe idea is assess inequality (or concentration) of each source of income, and then combine them to obtain their contribution to overall inequality."
  },
  {
    "objectID": "rmethods2/session_3.html#section-4",
    "href": "rmethods2/session_3.html#section-4",
    "title": "Research Methods II",
    "section": "",
    "text": "Variance Decomposition\nSetup: \\[\\begin{aligned}\nY &= y_1 + y_2 + \\cdots + y_n  \\\\\nI_V(Y) &= V(Y) = Cov(Y,Y) \\\\\n&= Cov(Y,y_1) + Cov(Y,y_2) + \\cdots \\\\\n\\end{aligned}\n\\]\nCovariance, however could be rewritten as: \\(Cov(Y,y_k)=\\rho_k \\sigma_Y \\sigma_{y_k}\\), thus\n\\[\\begin{aligned}\nV(y) = \\sigma^2_Y &= \\rho_1 \\sigma_Y \\sigma_{y_1} + \\rho_2 \\sigma_Y \\sigma_{y_2} + \\cdots \\\\\n\\sigma_Y&=   \\rho_1 \\sigma_{y_1} + \\rho_2 \\sigma_{y_2} + \\cdots  \\\\\n\\end{aligned}\n\\]\nFinally, if we divide all by \\(\\mu_y\\) and multiply by \\(\\mu_{yk}/\\mu_{yk}\\), we get:\n\\[\\begin{aligned}\n\\frac{\\sigma_Y}{\\mu_y} = CV(y) &=   \\rho_1 \\frac{1}{\\mu_{y}} \\frac{\\mu_{y1}}{\\mu_{y1}}\\sigma_{y_1} + \\rho_2 \\frac{1}{\\mu_{y}} \\frac{\\mu_{y2}}{\\mu_{y2}}  \\sigma_{y_2} + \\cdots  \\\\\n&= \\rho_1 sh_1 CV(y_1) + \\rho_2 sh_2 CV(y_2) + \\cdots  \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_3.html#example-stata",
    "href": "rmethods2/session_3.html#example-stata",
    "title": "Research Methods II",
    "section": "Example Stata",
    "text": "Example Stata\nssc install ineqfac\n\nfrause limew1972.dta, clear\ncapture: ssc install ineqfac\nineqfac basemi inchomewealth incnonhomewealth govtr_n pubcon tx_n valhp [aw=hhwgt]\n\n \nInequality decomposition by factor components (Shorrocks' method)\n-------------------------------------------------------------------------------\nFactor              |  100*s_f     S_f   100*m_f/m  rho_f   CV_f CV_f/CV(Total)\n--------------------+----------------------------------------------------------\nbasemi              |   29.115    0.284   44.434   0.711   0.900    0.922\ninchomewealth       |    1.872    0.018    4.302   0.318   1.335    1.367\nincnonhomewealth    |   43.136    0.421    7.905   0.754   7.065    7.235\ngovtr_n             |   -0.979   -0.010    5.868  -0.107   1.528    1.565\npubcon              |    2.091    0.020    8.303   0.238   1.035    1.060\ntx_n                |   15.629    0.153   11.462   0.754   1.767    1.809\nvalhp               |    9.136    0.089   17.726   0.502   1.003    1.027\n--------------------+----------------------------------------------------------\nTotal               |  100.000    0.977  100.000   1.000   0.977    1.000\n-------------------------------------------------------------------------------\nNote: s_f is the proportionate contribution of factor f\n to inequality of Total, where ...\n s_f = rho_f*sd(f)/sd(Total) \n     = rho_f*[m(factor_f)/m(totvar)]*[CV(factor_f)/CV(totvar)].\n S_f = s_f*CV(Total). rho_f = corr(f,Total). \n m_f = mean(f). sd(f) = std.dev. of f. CV_f = sd(f)/m_f."
  },
  {
    "objectID": "rmethods2/session_3.html#section-5",
    "href": "rmethods2/session_3.html#section-5",
    "title": "Research Methods II",
    "section": "",
    "text": "Gini Decomposition by source\n\nThe Gini index could also be decomposed by source of income.\n\n\\[\\begin{aligned}\nGini(y) &= \\frac{2}{\\mu_y} Cov(y,F(y)) = \\frac{2}{\\mu_y} Cov\\left(\\sum y_k,F(y)\\right) =\\frac{2}{\\mu_y} \\sum Cov\\left( y_k,F(y)\\right)\n\\end{aligned}\n\\]\n\nSo Gini is the sum of the covariance of each source of income with \\(F(y)\\)\n\n\\[\\begin{aligned}\nGini(y) &= \\sum \\frac{2}{\\mu_y} \\times Cov( y_k,F(y)) \\times \\frac{\\mu_{yk}}{\\mu_{yk}} \\times \\frac{Cov( y_k,F(y_k))}{Cov ( y_k,F(y_k))} \\\\\n&=\\sum \\frac{Cov ( y_k,F(y) )}{Cov ( y_k,F(y_k) )} \\times \\frac{2Cov( y_k,F(y_k)}{\\mu_{yk}} \\times \\frac{\\mu_{yk}}{\\mu_y}  \\\\\n&= \\sum R_k \\times G_k \\times sh_k\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_3.html#section-6",
    "href": "rmethods2/session_3.html#section-6",
    "title": "Research Methods II",
    "section": "",
    "text": "Gini Decomposition by source\n\\[\\begin{aligned}\nGini(y)&= \\sum R_k \\times G_k \\times sh_k\n\\end{aligned}\n\\]\n\n\\(R_k\\) = Gini Correlation; \\(G_k\\) = Gini of source \\(k\\); \\(sh_k\\) = share of source \\(k\\) in total income.\n\\(R_k\\time G_k\\) is the Concentration Index of \\(Y_k\\)"
  },
  {
    "objectID": "rmethods2/session_3.html#example-stata-1",
    "href": "rmethods2/session_3.html#example-stata-1",
    "title": "Research Methods II",
    "section": "Example Stata",
    "text": "Example Stata\nYou probably already have it installed\nssc install sgini\n\nsgini basemi inchomewealth incnonhomewealth govtr_n pubcon tx_n valhp [aw=hhwgt], source\n\n\nGini coefficient for basemi, inchomewealth, incnonhomewealth, govtr_n, pubcon, tx_n, valhp\n\nNote: inchomewealth has 690 negative observations (used in calculations).\nNote: incnonhomewealth has 10359 negative observations (used in calculations).\n-----------------------\n    Variable |      v=2\n-------------+---------\n      basemi |   0.4787\ninchomewea~h |   0.6161\nincnonhome~h |   0.9503\n     govtr_n |   0.7099\n      pubcon |   0.4913\n        tx_n |   0.6036\n       valhp |   0.5021\n-----------------------\n\nDecomposition by source:\n  TOTAL =  basemi +  inchomewealth +  incnonhomewealth +  govtr_n +  pubcon +  tx_n +  valhp\n\n\nParameter: v=2\n--------------------------------------------------------------------------------\n             |    Share   Coeff.    Corr.    Conc.  Contri.  %Contri. Elasticity\n    Variable |        s        g        r    c=g*r    s*g*r   s*g*r/G  s*g*r/G-s\n-------------+------------------------------------------------------------------\n      basemi |   0.4443   0.4787   0.8870   0.4246   0.1887   0.4868    0.0425\ninchomewea~h |   0.0430   0.6161   0.4617   0.2845   0.0122   0.0316   -0.0114\nincnonhome~h |   0.0790   0.9503   0.7724   0.7340   0.0580   0.1497    0.0707\n     govtr_n |   0.0587   0.7099  -0.2298  -0.1631  -0.0096  -0.0247   -0.0834\n      pubcon |   0.0830   0.4913   0.4593   0.2257   0.0187   0.0483   -0.0347\n        tx_n |   0.1146   0.6036   0.8776   0.5297   0.0607   0.1567    0.0420\n       valhp |   0.1773   0.5021   0.6601   0.3314   0.0587   0.1516   -0.0257\n-------------+------------------------------------------------------------------\n       TOTAL |   1.0000   0.3876   1.0000   0.3876   0.3876   1.0000    0.0000\n--------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods2/session_3.html#decompose-by-groups",
    "href": "rmethods2/session_3.html#decompose-by-groups",
    "title": "Research Methods II",
    "section": "Decompose by groups",
    "text": "Decompose by groups\n\nThere is a second type of decomposition one may be interested in.\n\nHow much of the inequality is explained by inequality within groups, and between groups.\n\nFor example, consider two cases:\n\nTwo (eq size) groups that have access to the same level of income, but within each group, all resources are held by one individual.\nTwo (eq size) groups, one has 80% of the income, and the other 20%, but within groups income is equally distributed.\n\nIt is possible to understand the source of inequality by decomposing it by groups.\n\nEntropy Indices (and Atkinson) are well suited for this type of decomposition. (see help ineqdeco)\nGINI is not as straight forward but possible."
  },
  {
    "objectID": "rmethods2/session_3.html#section-7",
    "href": "rmethods2/session_3.html#section-7",
    "title": "Research Methods II",
    "section": "",
    "text": "GINI Decomposition by groups\n\nDecomposition of the GINI coefficient by groups Milanovic and Yitzhaki (2002)\nThe method: To decompose the GINI by groups, one can use the following:\n\n\\[\\begin{aligned}\nGini(y) &= \\sum_{k=1}^K s_k O_k Gini(y_k) + Gini_{bw}\n\\end{aligned}\n\\]\n\nwhere \\(s_i\\) is the share of group \\(i\\) in total income, \\(Gini(y_k)\\) is the Gini of group \\(k\\), \\(O_k\\) is a measure of overlapping across groups, and \\(Gini_{bw}\\) is the Gini between groups.\n\n\\[Gini_{bw} = \\frac{2}{\\mu_y} Cov(\\mu_i,\\bar F_i)\\]"
  },
  {
    "objectID": "rmethods2/session_3.html#section-8",
    "href": "rmethods2/session_3.html#section-8",
    "title": "Research Methods II",
    "section": "",
    "text": "Overlapping\n\nOverlapping \\(O_k\\) measures to what extend the distribution of income in group \\(k\\) overlaps with the distribution of income in other groups.\nIf there is no overlapping, then \\(O_k=p_i\\) (the population share of group k, and incomes are fully stratified).\nOtherwise, this adjustment factors ensures that the sum of the Gini of each group is equal to the Gini of the total population + Between Gini."
  },
  {
    "objectID": "rmethods2/session_3.html#example-stata-2",
    "href": "rmethods2/session_3.html#example-stata-2",
    "title": "Research Methods II",
    "section": "Example Stata",
    "text": "Example Stata\nssc install anogi (Tom Masterson is one of the authors)\nssc install moremata (needed for anogi)\n\ncapture:ssc install anogi\ncapture:ssc install moremata\nanogi limew [aw= hhwgt ], by(educl) detail\n\n\nAnalysis of Gini\n\n--------------------------------------------------\n                          |      Coef.          %\n--------------------------+-----------------------\nOverall Gini              |   .3875623     100.00\n                          |\nG_wo = sum s_i*G_i*O_i    |   .3405469      87.87\nG_b                       |   .0470154      12.13\n                          |\nIG   = sum s_i*G_i        |   .3657639      94.38\nIGO  = sum s_i*G_i(O_i-1) |   -.025217      -6.51\nBGp  = G_bp               |   .1317698      34.00\nBGO  = G_b - G_bp         |  -.0847544     -21.87\n--------------------------+-----------------------\nMean of limew             |    20403.6\nN. of obs                 |      44872\nN. of subgroups           |          4\n--------------------------------------------------\n\n\nDetailed statistics for subgroups\n\n-------------------------------------------------------------------------------------------\n             |         N          p       mean          s          G          O          F \n-------------+-----------------------------------------------------------------------------\n           1 |  18244.58   .4065916   15466.07   .3081992   .3844549   .9986868   .3963075 \n           2 |  14552.19   .3243045   21019.76   .3340981    .338758   .9239628   .5363091 \n           3 |  5609.305   .1250068   22705.72   .1391112   .3640736   .9435821   .5546081 \n           4 |  6465.924   .1440971   30951.73   .2185916   .3817625   .8370495   .6634934 \n-------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods2/session_1.html",
    "href": "rmethods2/session_1.html",
    "title": "Research Methods II",
    "section": "",
    "text": "If you are reading this, you are probably interested in the Microeconometrics path of Research methods II.\nThis course assumes you are familiar with the basics of econometrics and statistics.\n\nWe will not cover questions from Research methods I. You probably know the answers to those questions already!\n\nThis course will focus on tools that are commonly used in empirical research in economics:\n\nWe will emphasize the use of Household Surveys (with the issues it entails)\nAnd focus on applied econometrics using cross-sectional data, for distributional analysis and policy simulation.\n\nThus, we will do much more use of empirical tools and software than we did in Research methods I.\nWe will have 7 Sessions, and 2 homeworks ."
  },
  {
    "objectID": "rmethods2/session_1.html#why-are-we-here",
    "href": "rmethods2/session_1.html#why-are-we-here",
    "title": "Research Methods II",
    "section": "",
    "text": "If you are reading this, you are probably interested in the Microeconometrics path of Research methods II.\nThis course assumes you are familiar with the basics of econometrics and statistics.\n\nWe will not cover questions from Research methods I. You probably know the answers to those questions already!\n\nThis course will focus on tools that are commonly used in empirical research in economics:\n\nWe will emphasize the use of Household Surveys (with the issues it entails)\nAnd focus on applied econometrics using cross-sectional data, for distributional analysis and policy simulation.\n\nThus, we will do much more use of empirical tools and software than we did in Research methods I.\nWe will have 7 Sessions, and 2 homeworks ."
  },
  {
    "objectID": "rmethods2/session_1.html#what-is-a-survey",
    "href": "rmethods2/session_1.html#what-is-a-survey",
    "title": "Research Methods II",
    "section": "What is a Survey?",
    "text": "What is a Survey?\n\nA Survey is a source of data that aims to collect information from a population of interest, to underestand some characteristics, behaviors, or opinions of that population as a whole.\n\nThe population of interest can be individuals, households, firms, etc.\n\nThey can be useful to identify and analyze policy questions.\nHowever, they are secondary data, and thus have limitations in terms of the questions that can be answered with them.\n\nYou cannot answer questions that require data that was not collected.\nThey can also be limited to Interviewee recall, or willingness to answer.\nOr how accessible the population of interest is."
  },
  {
    "objectID": "rmethods2/session_1.html#example-of-surveys",
    "href": "rmethods2/session_1.html#example-of-surveys",
    "title": "Research Methods II",
    "section": "Example of Surveys",
    "text": "Example of Surveys\n\nCurrent Population Survey (CPS)\n\nMonthly survey of 60,000 households in the US.\n\nAmerican Community Survey (ACS)\n\nAnnual survey of 3.5 million households in the US.\n\nAmerican Time Use Survey (ATUS)\n\nAnnual survey of 6,000 individuals in the US.\n\nEnterprise Surveys - WB (ES)\n\nSurvey of firms in developing countries. Different years of Collection"
  },
  {
    "objectID": "rmethods2/session_1.html#what-makes-a-good-survey",
    "href": "rmethods2/session_1.html#what-makes-a-good-survey",
    "title": "Research Methods II",
    "section": "What makes a good survey?",
    "text": "What makes a good survey?\n\nA Good survey is one that allows you to obtain estimates of statistics of interest for the population with “Tolerable” levels of Accuracy.\nTo do this, you need to have a good sampling design (representation and “independence of the population”) and a good questionnaire design (questions that are clear and easy to answer).\nA good survey needs to be representative of the population of interest. To do this appropriately, data will be collected based on a frame that will be used to select the sample."
  },
  {
    "objectID": "rmethods2/session_1.html#types-of-data-seletion",
    "href": "rmethods2/session_1.html#types-of-data-seletion",
    "title": "Research Methods II",
    "section": "Types of Data Seletion",
    "text": "Types of Data Seletion\n\nSimpleClusteredStratified\n\n\n\nEach observation in the “frame” has the same probability of being selected in the sample.\nIt may be difficult to implement in practice, because of cost and logistics. (distance)\nIt could also have problems of representativeness for small groups. (rare events)\n\n\n\n\nUsing some criteria, location for example, the population is divided into clusters.\nFor the sample selection, certain clusters are selected at random, and “some” observations within each selected.\nThis is more feasible in practice, because takes advantage of the “clustering” of the population.\nHowever, one may need to account for possible “common shocks” that people within the same cluster may have.\n\n\n\n\nSome times, statistics are required to accurately represent certain groups of the population. (by region, race, income level, etc)\nIn such cases, data can be collected in a way that ensures that the sample has enough observations for each group of interest.\n\nIt would be as collecting multiple samples, one for each group of interest.\n\nWithin each Strata, it would also be possible to use a simple or clustered sampling design."
  },
  {
    "objectID": "rmethods2/session_1.html#what-to-be-aware-of",
    "href": "rmethods2/session_1.html#what-to-be-aware-of",
    "title": "Research Methods II",
    "section": "What to be aware of?",
    "text": "What to be aware of?\n\nThe sampling design is important to ensure that the sample is representative of the population of interest.\nHowever, there are limitations:\n\nNot every-one selected will respond to the survey. (Is it random? )\nRarely one assumes equal probability of selection. (Different sampling weights)\nUse of Stratatification and Clustering may require special treatment.\n\nSomething else: Panel data\n\nBecause of attrition, it can be difficult to analyze if representativeity is required.\nHowever, it can be useful to analyze dynamics."
  },
  {
    "objectID": "rmethods2/session_1.html#descriptive-statistics",
    "href": "rmethods2/session_1.html#descriptive-statistics",
    "title": "Research Methods II",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\nOnce you have your data, you can start analyzing it by simply applying your survey weights.\n\nPoint estimates are straightforward to obtain.\n\nHowever, when considering the estimation of precision of the estimates (Variance and Standard Errors), there are two approaches that are important to consider:\nFinite Population Approach:\n\nAssociated with data description\nAssumes that the population is finite, and thus Selection probabilities are not independent.\n\nSuperpopulation Approach:\n\nAssociated with data modeling.\nPopulation is infinite. Selection probabilties are independent.\n\nFor practical purposes, the difference between the two approaches is not that important.\n\nWith large enough samples, he “finite sample correction” is negligible."
  },
  {
    "objectID": "rmethods2/session_1.html#basic-summary-statistics",
    "href": "rmethods2/session_1.html#basic-summary-statistics",
    "title": "Research Methods II",
    "section": "Basic Summary Statistics:",
    "text": "Basic Summary Statistics:\n\n\\(n\\) is sample size. \\(N\\) is population size.\n\\(a_i\\) an indicator of belonging to the sample. \\(\\sum a_i=n\\)\nAssume \\(y_i\\) is the outcome of interest, Say income. \\[\\texttt{mean: }\\hat{\\mu}_y=\\bar{y} = \\frac{1}{n}\\sum_{i=1}^N a_i y_i = \\frac{1}{n}\\sum_{j=1}^n y_j\\] \\[\\texttt{Variance: }\\hat \\sigma^2_y = \\frac{1}{n-1} \\sum_{i=1}^N a_i (y_i - \\hat{\\bar{y}} )^2\\] \\[\\texttt{V of mean: }\\widehat{V}(\\bar{y}) = \\frac{\\sigma^2_y }{n} * fpc\\]\n\nWhere \\(fpc = \\frac{N-n}{N-1}\\) is the finite population correction."
  },
  {
    "objectID": "rmethods2/session_1.html#accounting-for-weights",
    "href": "rmethods2/session_1.html#accounting-for-weights",
    "title": "Research Methods II",
    "section": "Accounting for Weights",
    "text": "Accounting for Weights\n\nThe previous formulas did not account for weights.\nWeights are factors used to “reweight/expand” the sample to make it representative of the population.\nThey can are typically related to the inverse of the probability of selection.\nSimply said, a weight \\(w_i=\\frac{1}{n*\\pi_i}\\) is a measure of how many observations in the population are represented by observation \\(i\\) in the sample.\n\nBut how do we account for weights in the formulas?"
  },
  {
    "objectID": "rmethods2/session_1.html#summary-statistics-with-weights",
    "href": "rmethods2/session_1.html#summary-statistics-with-weights",
    "title": "Research Methods II",
    "section": "Summary Statistics with Weights",
    "text": "Summary Statistics with Weights\nPopulation: \\[\\hat N = \\sum_{i=1}^n w_i\\]\nNormalized weights \\[v_i = \\frac{n w_i}{ \\sum w_i} \\rightarrow E(v_i) = 1\\]\nMean: \\[\\hat{\\mu}_y=\\bar{y} = \\frac{1}{\\hat N}\\sum_{i=1}^n w_i y_i = \\frac{1}{n}\\sum v_i y_i\\]"
  },
  {
    "objectID": "rmethods2/session_1.html#variance-with-weights",
    "href": "rmethods2/session_1.html#variance-with-weights",
    "title": "Research Methods II",
    "section": "Variance with Weights",
    "text": "Variance with Weights\nVariances are a bit more complicated. Normally you would consider:\n\\[Var(\\bar y) = \\frac{1}{n} \\hat \\sigma^2_y =\\frac{1}{n} \\frac{1}{n-1} \\sum_{i=1}^n v_i (y_i - \\bar y)^2\\]\nHowever, with survey weights you need to consider something else: \\[Var(\\bar y) = \\frac{1}{n} \\sum_{i=1}^n v_i^2 (y_i - \\bar y)^2\\]\nWhich is similar to “robust” Standard errors in OLS.\n\nWhat about Clusters and Strata?"
  },
  {
    "objectID": "rmethods2/session_1.html#how-to-account-for-weights-in-stata",
    "href": "rmethods2/session_1.html#how-to-account-for-weights-in-stata",
    "title": "Research Methods II",
    "section": "How to account for weights in Stata?",
    "text": "How to account for weights in Stata?\nLets use data two Examples.\n\nFirst Labor Force Survey: Oaxaca\nSecond National Health and Nutrition Examination Survey (NHANES)\n\n\n frause oaxaca, clear\n** Create Weights &lt;- This will be provided\nsum wt, meanonly\nreplace wt = round(wt/r(min))\ngen wage = exp(lnwage)\ntab wt\n\n\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(1,647 real changes made)\n(213 missing values generated)\n\n   sampling |\n    weights |      Freq.     Percent        Cum.\n------------+-----------------------------------\n          1 |        489       29.69       29.69\n          2 |        924       56.10       85.79\n          3 |        160        9.71       95.51\n          4 |         64        3.89       99.39\n          5 |          8        0.49       99.88\n          6 |          2        0.12      100.00\n------------+-----------------------------------\n      Total |      1,647      100.00\n\n\nWeights Distribution\n\ntab wt\n\n\n   sampling |\n    weights |      Freq.     Percent        Cum.\n------------+-----------------------------------\n          1 |        489       29.69       29.69\n          2 |        924       56.10       85.79\n          3 |        160        9.71       95.51\n          4 |         64        3.89       99.39\n          5 |          8        0.49       99.88\n          6 |          2        0.12      100.00\n------------+-----------------------------------\n      Total |      1,647      100.00\n\n\nSummary Statistics:\n\n** unweighted\nsum wage,d\n** weighted\nsum wage [aw=wt],d\n\n\n                            wage\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%     3.907204       1.661434\n 5%      11.8007       1.873127\n10%      17.4216       2.197802       Obs               1,434\n25%     23.19902       2.442003       Sum of wgt.       1,434\n\n50%     30.08896                      Mean           32.39167\n                        Largest       Std. dev.      16.12498\n75%      38.5662       137.3627\n90%     49.95005       152.6252       Variance       260.0151\n95%     58.71271       164.8352       Skewness       2.486954\n99%     85.47008       192.3077       Kurtosis       18.23702\n\n                            wage\n-------------------------------------------------------------\n      Percentiles      Smallest\n 1%     3.453689       1.661434\n 5%     7.370678       1.873127\n10%     15.83933       2.197802       Obs               1,434\n25%     21.72247       2.442003       Sum of wgt.       2,686\n\n50%     29.48271                      Mean            31.5322\n                        Largest       Std. dev.      16.32811\n75%     38.46154       137.3627\n90%     49.95005       152.6252       Variance       266.6071\n95%     58.11497       164.8352       Skewness       2.081806\n99%     85.47008       192.3077       Kurtosis       14.68647\n\n\nAccounting for weights for summary Statistics\n\n** unweighted\nmean wage \n** weighted\nmean wage [pw=wt]\nmean wage [pw=wt], over(female)\n\n\nMean estimation                          Number of obs = 1,434\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n        wage |   32.39167   .4258186      31.55637    33.22696\n--------------------------------------------------------------\n\nMean estimation                          Number of obs = 1,434\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n        wage |    31.5322   .4765835      30.59733    32.46708\n--------------------------------------------------------------\n\nMean estimation                           Number of obs = 1,434\n\n---------------------------------------------------------------\n              |       Mean   Std. err.     [95% conf. interval]\n--------------+------------------------------------------------\nc.wage@female |\n           0  |   33.87649   .6152059      32.66969    35.08329\n           1  |   28.76133    .722173       27.3447    30.17796\n---------------------------------------------------------------\n\n\nTables and cross tables\n\n** unweighted\ntab educ female\n\ntab educ female [w=wt]\n\n\n           |   sex of respondent\n  years of |      (1=female)\n education |         0          1 |     Total\n-----------+----------------------+----------\n         5 |         6         23 |        29 \n         9 |        47         93 |       140 \n      9.75 |         8         26 |        34 \n        10 |        10         42 |        52 \n      10.5 |       376        447 |       823 \n      11.5 |         7         14 |        21 \n        12 |       111         87 |       198 \n      12.5 |        56         80 |       136 \n        15 |        60         13 |        73 \n      17.5 |        78         63 |       141 \n-----------+----------------------+----------\n     Total |       759        888 |     1,647 \n\n           |   sex of respondent\n  years of |      (1=female)\n education |         0          1 |     Total\n-----------+----------------------+----------\n         5 |        17         45 |        62 \n         9 |       104        181 |       285 \n      9.75 |        14         52 |        66 \n        10 |        22         80 |       102 \n      10.5 |       725        833 |     1,558 \n      11.5 |        19         27 |        46 \n        12 |       201        148 |       349 \n      12.5 |       108        157 |       265 \n        15 |       111         21 |       132 \n      17.5 |       149        111 |       260 \n-----------+----------------------+----------\n     Total |     1,470      1,655 |     3,125 \n\n\nBetter approach: svyset\n\nwebuse nhanes2f, clear\nsvyset psuid /// Cluster\n       [pweight=finalwgt], /// Survey Weight as Inverse of Prob of Selection\n       strata(stratid)     // Strata Identifier\nmean zinc      \nmean zinc [pw=finalwgt]\nsvy: mean zinc      \n\n\nSampling weights: finalwgt\n             VCE: linearized\n     Single unit: missing\n        Strata 1: stratid\n Sampling unit 1: psuid\n           FPC 1: &lt;zero&gt;\n\nMean estimation                          Number of obs = 9,189\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n        zinc |   86.51518   .1510744      86.21904    86.81132\n--------------------------------------------------------------\n\nMean estimation                          Number of obs = 9,189\n\n--------------------------------------------------------------\n             |       Mean   Std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n        zinc |   87.18207   .1828747      86.82359    87.54054\n--------------------------------------------------------------\n(running mean on estimation sample)\n\nSurvey: Mean estimation\n\nNumber of strata = 31            Number of obs   =       9,189\nNumber of PSUs   = 62            Population size = 104,176,071\n                                 Design df       =          31\n\n--------------------------------------------------------------\n             |             Linearized\n             |       Mean   std. err.     [95% conf. interval]\n-------------+------------------------------------------------\n        zinc |   87.18207   .4944827      86.17356    88.19057\n--------------------------------------------------------------"
  },
  {
    "objectID": "rmethods2/session_1.html#testing-significance-across-2-groups",
    "href": "rmethods2/session_1.html#testing-significance-across-2-groups",
    "title": "Research Methods II",
    "section": "Testing Significance across 2 groups",
    "text": "Testing Significance across 2 groups\nConsider two groups with the following characteristics:\n\n\n\nGroup\nN\nmean\nVar\n\n\n\n\n1\n100\n45\n32.56\n\n\n2\n150\n55\n21.97\n\n\n\n\nIs the difference in means statistically significant?\nTest \\(H_0: \\mu_1 = \\mu_2\\) vs \\(H_1: \\mu_1 \\neq \\mu_2\\)\n\n\\[t = \\frac{\\mu_2 - \\mu_1}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\n=\\frac{10}{\\sqrt{\\frac{32.56}{100}+\\frac{21.97}{150}}}=\\frac{10}{\\sqrt{.47207}} = 14.55453\n\\]\n\nIf \\(t\\) is large enough, we can reject the null hypothesis.\nBut this does not work if you have weights…"
  },
  {
    "objectID": "rmethods2/session_1.html#testing-significance-across-2-groups-1",
    "href": "rmethods2/session_1.html#testing-significance-across-2-groups-1",
    "title": "Research Methods II",
    "section": "Testing Significance across 2 groups",
    "text": "Testing Significance across 2 groups\n\nBut you can use OLS to test the difference in means with weights!\n\nMake sure you use “robust” standard errors, or “pw” option.\n\n\n\n frause oaxaca, clear\n** Create Weights &lt;- This will be provided\nsum wt, meanonly\nreplace wt = round(wt/r(min))\ngen wage = exp(lnwage)\nreg wage i.female [pw=wt],\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(1,647 real changes made)\n(213 missing values generated)\n(sum of wgt is 2,686)\n\nLinear regression                               Number of obs     =      1,434\n                                                F(1, 1432)        =      29.05\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.0244\n                                                Root MSE          =     16.133\n\n------------------------------------------------------------------------------\n             |               Robust\n        wage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    1.female |  -5.115156   .9490209    -5.39   0.000    -6.976776   -3.253536\n       _cons |   33.87649   .6154206    55.05   0.000     32.66927    35.08371\n------------------------------------------------------------------------------\n\n\n\nyou can also use svy: regress for complex designs.\n\nsee here for additional examples"
  },
  {
    "objectID": "rmethods2/session_1.html#io-tables-1",
    "href": "rmethods2/session_1.html#io-tables-1",
    "title": "Research Methods II",
    "section": "IO-Tables",
    "text": "IO-Tables\n\nIO tables stand for Input-Output tables. They are a way to represent the production structure of an economy.\nThey provide a Static representation of the Economy\nEach Row represents the production of a sector, and each column represents the use of that production by other sectors as Inputs.\nThe information it contains represent a snapshot of the economy at a given point in time.\nIt can be used to simulate changes in production, labor demand, and total production in the economy, under specific assumptions (production function)"
  },
  {
    "objectID": "rmethods2/session_1.html#section",
    "href": "rmethods2/session_1.html#section",
    "title": "Research Methods II",
    "section": "",
    "text": "Consider a Economy with K=3 sectors: Agriculture, Manufacturing and Services, with a Final consumer agent (households)\nEach sector (i) produces a good (\\(X_i\\)), which is sold to other sectors or the final consumer.\n\\(X_{ij}\\) is the quantity of goods sector \\(i\\) sells to sector \\(j\\), and \\(y_i\\) the final consumption by households.\n\\(X_{ji}\\) is also the quantity of goods sector \\(i\\) uses from sector \\(j\\) as inputs.\n\\(L_i\\) is the amount of labor used by sector \\(i\\).\nIn a simple Economy, (value) Total labor demand is equal to total Household income. \\(L_a + L_m + L_s = L = y_a + y_m + y_s\\)"
  },
  {
    "objectID": "rmethods2/session_1.html#section-1",
    "href": "rmethods2/session_1.html#section-1",
    "title": "Research Methods II",
    "section": "",
    "text": "This simple Economy can be represented as follows:\n\\[\n\\begin{aligned}\nX_{11} + X_{12} + X_{13} + Y_{1}  &= X_{1} \\\\\nX_{21} + X_{22} + X_{23} + Y_{2} &= X_{2} \\\\\nX_{31} + X_{32} + X_{33} + Y_{3} &= X_{3} \\\\\nL_1 + L_2 + L_3 + 0 &= L\n\\end{aligned}\n\\]\n\n\n\n\nS1\nS2\nS3\nHH\n\n\n\n\n\nSupply:\n\n\n\n\n\n\n\nS1\n\\(X_{11}\\)\n\\(X_{12}\\)\n\\(X_{13}\\)\n\\(Y_{1}\\)\n\\(X_{1}\\)\n\n\nS2\n\\(X_{21}\\)\n\\(X_{22}\\)\n\\(X_{23}\\)\n\\(Y_{2}\\)\n\\(X_{2}\\)\n\n\nS3\n\\(X_{31}\\)\n\\(X_{32}\\)\n\\(X_{33}\\)\n\\(Y_{3}\\)\n\\(X_{3}\\)\n\n\nHH\n\\(L_1\\)\n\\(L_2\\)\n\\(L_3\\)\n0\n\\(L\\)"
  },
  {
    "objectID": "rmethods2/session_1.html#section-2",
    "href": "rmethods2/session_1.html#section-2",
    "title": "Research Methods II",
    "section": "",
    "text": "From the consumption/inputs Side, we could also write the equations as:\n\\[\n\\begin{aligned}\nX_{11} &+ X_{21} &+ X_{31} &+ L_{1}  &= X_{1} \\\\\nX_{12} &+ X_{22} &+ X_{32} &+ L_{2} &= X_{2} \\\\\nX_{13} &+ X_{23} &+ X_{33} &+ L_{3} &= X_{3} \\\\\nY_1 &+ Y_2 &+ Y_3 &  &= Y\n\\end{aligned}\n\\]\nAnd from here we can get the technical coefficients:\n\\[\n\\begin{aligned}\na_{11} X_1 &+ a_{21} X_1 &+ a_{31} X_1 &+ \\lambda_{1} X_1 &= X_1 \\\\\na_{12} X_2 &+ a_{22} X_2 &+ a_{32} X_2 &+ \\lambda_{2} X_2 &= X_2 \\\\\na_{13} X_3 &+ a_{23} X_3 &+ a_{33} X_3 &+ \\lambda_{3} X_3 &= X_3 \\\\\n\\delta_1 Y &+ \\delta_2 Y & + \\delta_3 Y &  &= Y\n\\end{aligned}\n\\]\nWhere \\(a_{ij} = \\frac{X_{ij}}{X_j}\\) and \\(\\lambda_i = \\frac{L_i}{X_i}\\)\n\\[a_{1i}+a_{2i}+a_{3i}+\\lambda_i = 1 \\ \\& \\ \\delta_1+\\delta_2+\\delta_3 =1\\]"
  },
  {
    "objectID": "rmethods2/session_1.html#section-3",
    "href": "rmethods2/session_1.html#section-3",
    "title": "Research Methods II",
    "section": "",
    "text": "With this, we can write the IO table\n\\[\n\\begin{aligned}\na_{11} X_1 &+ a_{12} X_2 &+ a_{13} X_3  &+ \\delta_1 Y &= X_{1} \\\\\na_{21} X_1 &+ a_{22} X_2 &+ a_{23} X_3  &+ \\delta_2 Y &= X_{2} \\\\\na_{31} X_1 &+ a_{32} X_2 &+ a_{33} X_3  &+ \\delta_3 Y &= X_{3} \\\\\n\\lambda_1 X_1 &+ \\lambda_2 X_2 &+ \\lambda_3 X_3 &\\ \\ &= L\n\\end{aligned}\n\\]\nOr into Matrix Form\n\\[\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13}  \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33} \\\\\n\\lambda_1 & \\lambda_2 & \\lambda_3\n\\end{pmatrix}\n\\begin{pmatrix}\nX_1 \\\\ X_2 \\\\ X_3\n\\end{pmatrix} +\n\\begin{pmatrix}\nY_1 \\\\ Y_2 \\\\ Y_3 \\\\ 0\n\\end{pmatrix}=\n\\begin{pmatrix}\nX_1 \\\\ X_2 \\\\ X_3 \\\\ L\n\\end{pmatrix}\n\\]"
  },
  {
    "objectID": "rmethods2/session_1.html#section-4",
    "href": "rmethods2/session_1.html#section-4",
    "title": "Research Methods II",
    "section": "",
    "text": "Solve for Production sectors:\n\\[\n\\begin{pmatrix}\na_{11} & a_{12} & a_{13}  \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{pmatrix}\n\\begin{pmatrix}\nX_1 \\\\ X_2 \\\\ X_3\n\\end{pmatrix} +\n\\begin{pmatrix}\nY_1 \\\\ Y_2 \\\\ Y_3\n\\end{pmatrix}=\n\\begin{pmatrix}\nX_1 \\\\ X_2 \\\\ X_3\n\\end{pmatrix}\n\\]\n\\[\n\\begin{pmatrix}\nY_1 \\\\ Y_2 \\\\ Y_3\n\\end{pmatrix}=I \\begin{pmatrix} X_1 \\\\ X_2 \\\\ X_3 \\end{pmatrix} -\nA \\begin{pmatrix} X_1 \\\\ X_2 \\\\ X_3 \\end{pmatrix} = (I-A) \\begin{pmatrix} X_1 \\\\ X_2 \\\\ X_3 \\end{pmatrix}\n\\]\nFinally:\n\\[\n\\begin{pmatrix} X_1 \\\\ X_2 \\\\ X_3 \\end{pmatrix} = (I-A)^{-1} \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ Y_3 \\end{pmatrix}\n\\rightarrow\n\\begin{pmatrix} \\Delta X_1 \\\\ \\Delta X_2 \\\\ \\Delta X_3 \\end{pmatrix} = (I-A)^{-1}\n\\begin{pmatrix} \\Delta Y_1 \\\\ \\Delta Y_2 \\\\ \\Delta Y_3 \\end{pmatrix}\n\\]"
  },
  {
    "objectID": "rmethods2/session_1.html#example",
    "href": "rmethods2/session_1.html#example",
    "title": "Research Methods II",
    "section": "Example",
    "text": "Example\n\nConsider the following IO table for a simple economy with 3 sectors and a final consumer.\n\n\n\n\n\n\n\n\n\n\n\nSector\nAgriculture\nManufacturing\nServices\nFinal Consumer\n\n\n\n\nAgriculture\n102\n103\n153\n129\n\n\nManufacturing\n133\n124\n77\n99\n\n\nServices\n71\n92\n51\n165\n\n\nHouseholds\n181\n114\n98\n0\n\n\n\nS1: What is the total production of each sector?\n\n\\(X_1 = 102 + 103 + 153 + 129 = 487\\)\n\\(X_2 = 133 + 124 + 77 + 99 = 433\\)\n\\(X_3 = 71 + 92 + 51 + 165 = 379\\)\n\nS2: What are the technical coefficients?\n\n\n\n\n\n\n\n\n\n\nSector\nAgriculture\nManufacturing\nServices\nFinal Consumer\n\n\n\n\nAgriculture\n\\(a_{11}\\) = 0.209\n0.238\n0.404\n0.328\n\n\nManufacturing\n0.273\n0.286\n0.203\n0.252\n\n\nServices\n0.146\n0.212\n0.135\n0.420\n\n\nHouseholds\n0.372\n0.263\n0.259\n0.000\n\n\n\nThis captures a snapshot of an economy. And could use to simulate changes in production and labor demand.\nS3. How much would production change if the final consumer demand for Agriculture increases in 20%?\n\\[\\Delta X = (I-A)^-1\n\\begin{pmatrix} 0.2 * 129 \\\\ 0 \\\\ 0 \\end{pmatrix} =\n\\begin{pmatrix} 45.54 \\\\ 21.08 \\\\ 12.84 \\end{pmatrix}\n\\]\nS4: How much would labor demand change if the final consumer demand for Agriculture increases in 20%? \\[\\Delta L_i = \\lambda_i * \\Delta X_i\\]\n\\(\\Delta L_1 = 0.372 * 45.54 = 16.95 ; \\Delta L_2 =  0.263 * 21.08 = 5.544 ; \\Delta L_3 =  0.259 * 12.84 = 3.326\\)"
  },
  {
    "objectID": "rmethods2/session_1.html#example---stata",
    "href": "rmethods2/session_1.html#example---stata",
    "title": "Research Methods II",
    "section": "Example - Stata",
    "text": "Example - Stata\nmata: Input Data\n\nmata: x  = (102, 103, 153 \\ 133, 124, 77 \\ 71, 92, 51)\nmata: y  = (129 \\ 99 \\ 165)\nmata: hh = ( 181 , 114 , 98)\n\nEstimate Total Production\n\nmata: tp = colsum(x):+hh ; tp\n\n         1     2     3\n    +-------------------+\n  1 |  487   433   379  |\n    +-------------------+\n\n\nEstimate Technical Coefficients\n\n// Technical Coefficients\nmata:ai = x:/tp ; ai\n\n                 1             2             3\n    +-------------------------------------------+\n  1 |  .2094455852   .2378752887   .4036939314  |\n  2 |   .273100616   .2863741339   .2031662269  |\n  3 |  .1457905544   .2124711316   .1345646438  |\n    +-------------------------------------------+\n\n\nEstimate Change in Demand: 20% increase in Agriculture\n\n// Technical Coefficients\nmata:dy = y :* (.2 \\ 0 \\ 0); dy\n\n          1\n    +--------+\n  1 |  25.8  |\n  2 |     0  |\n  3 |     0  |\n    +--------+\n\n\nEstimate Change in Production\n\n// Change in Production\nmata:dx = qrinv(I(3)-ai)*dy; dx\n\n                 1\n    +---------------+\n  1 |  45.54130481  |\n  2 |  21.08637814  |\n  3 |  12.84872246  |\n    +---------------+\n\n\nEstimate Change in Labor Demand\n\nmata:dl = (hh:/tp)':*dx; dl\n\n                 1\n    +---------------+\n  1 |   16.9260291  |\n  2 |  5.551609949  |\n  3 |  3.322360954  |\n    +---------------+"
  },
  {
    "objectID": "rmethods2/session_1.html#sam-social-accounting-matrix-1",
    "href": "rmethods2/session_1.html#sam-social-accounting-matrix-1",
    "title": "Research Methods II",
    "section": "SAM: Social Accounting Matrix",
    "text": "SAM: Social Accounting Matrix\n\nSAM can be thought as an upgraded version of IO-tables.\nThey are a way to organize information about the production structure of an economy, but also the distribution of resources.\n\nThis it will not only register production of goods and services, but also transfers of resources between sectors and agents.\n\nYou can also use it as basis for a plausible model of the economy.\n\nPrediction of changes in production, income and distribution."
  },
  {
    "objectID": "rmethods2/session_1.html#example-1",
    "href": "rmethods2/session_1.html#example-1",
    "title": "Research Methods II",
    "section": "Example",
    "text": "Example\n\nClosed Economy No GoV SAM\n\n\n\nProduction\nConsumption\nAccumulation\nTotals\n\n\n\n\nProduction\n\n\\(C\\)\n\\(I\\)\n\\(C+I\\)\n\n\nConsumption\n\\(Y\\)\n\n\n\\(Y\\)\n\n\nAccumulation\n\n\\(S\\)\n\n\\(S\\)\n\n\nTotals\n\\(Y\\)\n\\(C+S\\)\n\\(I\\)\n\n\n\n\n\nGoods/services are Transfered from left to Top-right\nMonetary Transfers are from Top-right to left"
  },
  {
    "objectID": "rmethods2/session_1.html#example-2",
    "href": "rmethods2/session_1.html#example-2",
    "title": "Research Methods II",
    "section": "Example 2",
    "text": "Example 2\n\nOpen Economy with GoV SAM\n\n\n\n\n\n\n\n\n\n\n\n\nS1\nS2\nS3\nS4\nS5\nTotals\n\n\n\n\nS1: Prod\n\n\\(C\\)\n\\(G\\)\n\\(I\\)\n\\(E\\)\n\\(C+G+I+E\\)\n\n\nS2: HH\n\\(Y\\)\n\n\n\n\n\\(Y\\)\n\n\nS3: Gov\n\n\\(Tx\\)\n\n\n\n\\(Tx\\)\n\n\nS4: K acc\n\n\\(S_h\\)\n\\(S_g\\)\n\n\\(S_f\\)\n\\(S_h+S_g+S_f\\)\n\n\nS5: RofW\n\\(M\\)\n\n\n\n\n\\(M\\)\n\n\nTotals\n\\(Y+M\\)\n\\(C+S_h+Tx\\)\n\\(G+S_g\\)\n\\(I\\)\n\\(E+S_f\\)\n\n\n\n\nHere \\(S1\\) and \\(S2\\) is what we had in the IO table. Thus, we could further expand the SAM to include more sectors and agents."
  },
  {
    "objectID": "rmethods2/session_1.html#example-3",
    "href": "rmethods2/session_1.html#example-3",
    "title": "Research Methods II",
    "section": "Example 3",
    "text": "Example 3\n\nMoreDetailed SAM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nS1\nS2\nS3\nS4\nS5\nS6\nS7\nS8\n\n\n\n\nS1: Act\n\n\\(Gds\\)\n\n\n\n\n\n\n\n\nS2: Commod\n\\(IntGds\\)\n\n\n\n\\(C\\)\n\\(G\\)\n\\(I\\)\n\\(E\\)\n\n\nS3: Factors\n\\(VA\\)\n\n\n\n\n\n\n\\(FE\\)\n\n\nS4: Enter\n\n\n\\(Prof\\)\n\n\n\\(ETr\\)\n\n\n\n\nS5: HH\n\n\n\\(Wage\\)\n\\(DProf\\)\n\n\\(Tr\\)\n\n\\(REM\\)\n\n\nS6: Gov\n\\(ITx\\)\n\n\\(FTx\\)\n\\(ETx\\)\n\\(DTx\\)\n\n\n\\(Tarf\\)\n\n\nS7: K acc\n\n\n\n\\(RetY\\)\n\\(S_h\\)\n\\(S_g\\)\n\n\\(KTrM\\)\n\n\nS8: RofW\n\n\\(M\\)\n\\(FM\\)\n\n\\(REMA\\)\n\\(TrA\\)\n\\(KtrA\\)"
  },
  {
    "objectID": "rmethods2/session_1.html#other-extensions",
    "href": "rmethods2/session_1.html#other-extensions",
    "title": "Research Methods II",
    "section": "Other Extensions",
    "text": "Other Extensions\n\nSAM also allow you to do further extensions to include more agents (heterogenous)\n\nGreen-Industry\nInformal Sector\nHouseholds by income level\netc."
  },
  {
    "objectID": "rmethods2/session_1.html#what-to-get-from-today",
    "href": "rmethods2/session_1.html#what-to-get-from-today",
    "title": "Research Methods II",
    "section": "What to get from today?",
    "text": "What to get from today?\n\nHow to use weights in Stata to account for survey design, and how to obtain summary statistics.\nHow to test differences in means across groups.\nHow to use IO tables to simulate changes in production and labor demand.\nUnderstand how SAM can be used to represent an Economy"
  },
  {
    "objectID": "rmethods2/HomeWork2.html",
    "href": "rmethods2/HomeWork2.html",
    "title": "Homework II",
    "section": "",
    "text": "Submit a document containing your answers to the following questions and a do file with the code used in Stata to produce the answers. Ensure your program works as submitted and produces the reported answers for full credit."
  },
  {
    "objectID": "rmethods2/HomeWork2.html#instructions",
    "href": "rmethods2/HomeWork2.html#instructions",
    "title": "Homework II",
    "section": "",
    "text": "Submit a document containing your answers to the following questions and a do file with the code used in Stata to produce the answers. Ensure your program works as submitted and produces the reported answers for full credit."
  },
  {
    "objectID": "rmethods2/HomeWork2.html#part-i-significance-testing-15pts",
    "href": "rmethods2/HomeWork2.html#part-i-significance-testing-15pts",
    "title": "Homework II",
    "section": "Part I: Significance Testing (15pts)",
    "text": "Part I: Significance Testing (15pts)\nIn this section, use simulation to demonstrate: - How rejecting the null hypothesis probability increases with the number of tests. - Benferroni correction application to control for multiple testing. - Joint testing usage to control for multiple testing.\n\nTasks\nRefer to the file hw2.do for a template program simulating data for N individuals and K variables. Modify this program to:\n\nTest the null hypothesis that the mean of each variable is zero with a significance level of 10%. Indicate if you reject the null for any variable.\nRepeat the test in 1 using a modified significance level of 10%/K (Benferroni correction). Indicate if you reject the null for any variable.\nConduct a joint test that the mean of all K variables is zero, using a 10% significance level.\n\nRepeat the above tasks 1000 times and report the proportion of times you reject the null hypothesis in each case (summary tables). Provide an explanation for your results.\nFor the parameters K and N, use the following combinations:\n\nK = 2, N = 20\nK = 5, N = 20\nK = 2, N = 100\nK = 5, N = 100\n\nUsing the code, you need to change the parameters K and N to run each simulation. Create a table with the results for each combination of K and N, and explain."
  },
  {
    "objectID": "rmethods2/HomeWork2.html#part-ii-imputation-and-statistical-matching-30pts",
    "href": "rmethods2/HomeWork2.html#part-ii-imputation-and-statistical-matching-30pts",
    "title": "Homework II",
    "section": "Part II: Imputation and Statistical Matching (30pts)",
    "text": "Part II: Imputation and Statistical Matching (30pts)\nImputation and Statistical Matching are methods for handling missing data. Use any of these methods on the dataset cps_imput_miss.dta, to address the problem of missing wages.\nThe dataset focuses on couple-households with or without children (below 15 years old) and no other family members. Their only source of income comes from wages.\nThe dataset includes variables ending in _1 for the husband and _2 for the wife. Household-level variables and the variable cutoff indicating the poverty line are also provided\n\nThe problem\nThe problem on this dataset is that in 50% of the households, husband’s wages are missing. And we are interested in estimating the poverty rate and for the sample.\n\n\nTasks\n\nImpute missing wages for husbands and estimate the poverty rate for the sample.\n\nWrite a report explaining the steps you have taken for imputation, including model specification. Provide a table with poverty rates for the entire sample, by husband education level, and race. Offer a brief explanation of the results.\n\n\nNote: Include a do-file with the code for imputation and summary statistics."
  },
  {
    "objectID": "rmethods2/HomeWork2.html#part-iii-micro-simulations-55pts",
    "href": "rmethods2/HomeWork2.html#part-iii-micro-simulations-55pts",
    "title": "Homework II",
    "section": "Part III: Micro-Simulations (55pts)",
    "text": "Part III: Micro-Simulations (55pts)\nMicro-simulations are a valuable tool for studying the effects of policy changes. In this section, use micro-simulations to explore the impact of an Employer of Last Resort (ELR) program on the labor market of Tanga-Mandapio, a small Pacific country similar to the United States.\n\nConsiderations\n\nThe ELR program guarantees a job to anyone between 18 and 65 years old who is not employed.\nThe program offers different wages based on education levels, based on the following schedule:\n\n\nWages per hour offered by the ELR program\n\n\nEducation\nWage\n\n\n\n\nLess than High School\n10\n\n\nHigh School\n15\n\n\nSome College\n20\n\n\nCollege\n25\n\n\nGraduate School\n30\n\n\n\n\nAll jobs offered by the program are full time (40 hours per week), full year (48 weeks per year).\nThere is a budget of 100 Million dollars per year for the program.\n\n\n\nTasks\nUse the following steps to simulate the impact of the ELR program:\n\nDetermine eligibility for the ELR program.\nEstimate a logit model to predict employment probability considering only those employed and those who are eligible for the program.\nUse model predictions to assign jobs until the budget is exhausted, prioritizing those with higher employment probability.\nRecalculate total household income accounting for new jobs.\n\nWrite a report explaining the simulation steps, including model specification, and discuss the program’s impact:\n\nHow many people are eligible for the program by education level?\nWhat would the the budget requirement be to guarantee a job to all unemployed individuals.\nJobs created, impact on the unemployment rate by education.\nImpact on poverty and inequality using cutoff and Income Percapita (hhincome/hhsize).\n\nProvide summary statistics for poverty before and after the simulation, by gender and race.\n\nNote: Include a do-file with the code for the simulation and summary statistics.\n\n\n\nRemarks\n\nThe data file is tanga_mandapio. Assume each person has a weight of 1.\nhhincome is total household income, and incwage is individual wage income.\ncutoff is the poverty line for a household.\neduc_g is the aggregated education level.\nExplore the dataset for a better understanding of available variables.\n\n\n\nHints on How to proceed\n\nCreate a dummy emp that is equal to 1 if employed and 0 otherwise only for people age = 18 to 65.\nEstimate your logit model using emp as dependent variable, as a function of characteristics.\nUsing predicted Probabilities, assign jobs to those eligible for the program, using those with highest predicted probability first.\n\nFirst for the unemployed, sort data by predicted probability (highest to lowest) and assign jobs until the budget is exhausted.(each persons anual wage is wagex40x48, where wages depends on his education level)\n\nTo recalculate total household income, you can simply create gen new_hhincome = hhincome + new_wage_elr where new_wage_elr the sum of ELR wages for each household. Some households may have more than one person employed by the ELR program.\nHouseholds are identified with the variable serial."
  },
  {
    "objectID": "rmethods/table/tab1.html",
    "href": "rmethods/table/tab1.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "asd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOz\n\nlbs\n\nKgs\n\nGr\n\n\n\n\n\nmale\n3.123\n\n0.195\n\n0.089\n\n88.605\n\n\n\n\n(1.071)\n[2.916]\n(0.067)\n[2.916]\n(0.030)\n[2.916]\n(30.389)\n[2.916]\n\n\nwhite\n5.404\n\n0.338\n\n0.153\n\n153.346\n\n\n\n\n(1.392)\n[3.882]\n(0.087)\n[3.882]\n(0.039)\n[3.882]\n(39.497)\n[3.882]\n\n\ncigs\n-0.480\n\n-0.030\n\n-0.014\n\n-13.628\n\n\n\n\n(0.091)\n[-5.288]\n(0.006)\n[-5.288]\n(0.003)\n[-5.288]\n(2.577)\n[-5.288]\n\n\nlfaminc\n1.053\n\n0.066\n\n0.030\n\n29.867\n\n\n\n\n(0.632)\n[1.664]\n(0.040)\n[1.664]\n(0.018)\n[1.664]\n(17.946)\n[1.664]\n\n\n_cons\n110.603\n\n6.913\n\n3.138\n\n3138.351\n\n\n\n\n(2.071)\n[53.410]\n(0.129)\n[53.410]\n(0.059)\n[53.410]\n(58.760)\n[53.410]\n\n\nN\n1388\n\n1388\n\n1388\n\n1388\n\n\n\nR2\n0.046\n\n0.046\n\n0.046\n\n0.046"
  },
  {
    "objectID": "rmethods/Midterm.html",
    "href": "rmethods/Midterm.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Midterm\n\nExplain how to implement a Chow test using two separate regressions. and why you cannot use it if the underlying model is heteroskedastic."
  },
  {
    "objectID": "rmethods/homework_3.html",
    "href": "rmethods/homework_3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Consider the dataset labsup (available using frause). This dataset has information on the number of hours worked per week, hourly wage, and years of education for a sample balck and hispanic women. The goal is to determine the effect of an additional child has on the number of hours worked per week.\nTo do this, estimate a model of the form:\n\\[\\begin{aligned}\nhours &=b_0 + b_1 kids + b_2 educ + b_3 age + b_4 age^2 \\\\\n&+ b_5 hispanic + b_6 black+ b_7 nonmomi + u\n\\end{aligned} \\tag{1}\\]\n(see variable description in the datafile)\nQ1-1. (10pts) Estimate the model using OLS. What is the effect of an additional child on the number of hours worked per week? Is this effect statistically significant? What about the rest of the variables?\nQ1-2. (10pts) It is believed that Number of childrens may be correlated with the model error (endogenous). Explain why this may be the case. (You can consult the original paper)\nQ1-3. (20pts) There are two possible candidates for IV in this case. samesex (if the first two children had the same sex), and multi2nd - if the family had twins the second during the second pregnancy. Estimate Equation 1, using each of these variables as IV separately, and both of them together. While doing this answer:\n\nAre the instruments strong individually? Are they strong together? (F-statistic of first stage)\nHow does the effect of kids on Hours worked change when using the different IV’s. Are there any differences?\nBased on all specifications, is there any evidence that the number of children is endogenous?\n\n\n\nConsider the dataset smoke (available using frause). This dataset has information on few demographic characteristics, cigarate prices, income, as well as number of cigarates smoked per day.\nIn this case, “Cigarates smoked per day” is a kind of limited dependent variable, because not everyone will smoke, and the number of cigarates people smoke is an integer. Under this considerations estimate the following:\nQ2-1. (10pts) Estimate a Linear probability model (LPM) and probit model, analyzing the probability of smoking as function of demographics, log of income and log of prices. Analyze the results and compare the two models (magnitudes of the effects). What would you say the impact of restaurant smoking bans is on the probability of smoking?\nQ2-2. (10pts) Now, say that you are also interested in analyzing the relations of the different factors on the number of cigarates smoked per day. Estimate a Poisson model and a Tobit model (with data censored at 0). Analyze the results and compare the two models (magnitudes of the effects). What would you say the impact of restaurant smoking bans is on the number of cigarates smoked per day? what about the impact of a 10% increase in cigarate prices?\n\nNote: For the Tobit model, you can use the tobit, ll(0), and will need to use margins, margins, dydx(*) ystar(0,.) to get the marginal effects that are comparable across models.\n\nQ2-3. (10pts) You are also asked to use the Tobit results to analyze the probability of smoking. To do this, you will need to use the marginal effects from the Tobit model margins, dydx(*) pr(0,.). Compare the results of this exercise with the results from the LPM and probit models. Would you reach the same conclusions? if not, what are the implications for the tobit assumptions of a tobit model?\n\n\n\nConsider the dataset driving (available using frause). This dataset has information at the state level, from 1980 to 2004, on statistics regarding traffic accidents, fatalities, and laws.\nSee datafile for variable description.\nQ3-1. (10pts) Estimate a model of the form:\n\\[\\begin{aligned}\ntotfatrte &= b_0 + b_1 \\mathbb{1}(seatbealt=1) + b_2 \\mathbb{1}(seatbealt=2) + b_3 minage  \\\\\n& + b_4 zerotol + b_5 unem + b_6 perc14\\_24 + b_7 vehicmiles + e\n\\end{aligned} \\tag{2}\\]\nAnd interpret the results. is there any effect that seems unexpected? Explain why this may be the case.\nQ3-2. (5pts) One of the reasons one may be finding unexpected results is because of unobserved heterogeneity across states. Can you explain what kind of factors are there that could be related to both accidents and the laws? (think about the political process of passing laws)\nQ3-3. (10pts) To control for unobserved heterogeneity, estimate three models:\n- Random effect model\n- Fixed effect model\n- Correlated Random effect model\nHow do the results from the Random effect model and Fixed effect model compare to each other? what about compared to Equation 2?\nQ3-4. (5pts) Using the results from the Correlated Random effect model, test for which of the models is more appropriate, between Random Effects and Fixed effects. What do you conclude?"
  },
  {
    "objectID": "rmethods/homework_3.html#part-ii-mle-and-nonlinear-models-30pts",
    "href": "rmethods/homework_3.html#part-ii-mle-and-nonlinear-models-30pts",
    "title": "Homework 3",
    "section": "",
    "text": "Consider the dataset smoke (available using frause). This dataset has information on few demographic characteristics, cigarate prices, income, as well as number of cigarates smoked per day.\nIn this case, “Cigarates smoked per day” is a kind of limited dependent variable, because not everyone will smoke, and the number of cigarates people smoke is an integer. Under this considerations estimate the following:\nQ2-1. (10pts) Estimate a Linear probability model (LPM) and probit model, analyzing the probability of smoking as function of demographics, log of income and log of prices. Analyze the results and compare the two models (magnitudes of the effects). What would you say the impact of restaurant smoking bans is on the probability of smoking?\nQ2-2. (10pts) Now, say that you are also interested in analyzing the relations of the different factors on the number of cigarates smoked per day. Estimate a Poisson model and a Tobit model (with data censored at 0). Analyze the results and compare the two models (magnitudes of the effects). What would you say the impact of restaurant smoking bans is on the number of cigarates smoked per day? what about the impact of a 10% increase in cigarate prices?\n\nNote: For the Tobit model, you can use the tobit, ll(0), and will need to use margins, margins, dydx(*) ystar(0,.) to get the marginal effects that are comparable across models.\n\nQ2-3. (10pts) You are also asked to use the Tobit results to analyze the probability of smoking. To do this, you will need to use the marginal effects from the Tobit model margins, dydx(*) pr(0,.). Compare the results of this exercise with the results from the LPM and probit models. Would you reach the same conclusions? if not, what are the implications for the tobit assumptions of a tobit model?"
  },
  {
    "objectID": "rmethods/homework_3.html#part-iii-panel-data-30pts",
    "href": "rmethods/homework_3.html#part-iii-panel-data-30pts",
    "title": "Homework 3",
    "section": "",
    "text": "Consider the dataset driving (available using frause). This dataset has information at the state level, from 1980 to 2004, on statistics regarding traffic accidents, fatalities, and laws.\nSee datafile for variable description.\nQ3-1. (10pts) Estimate a model of the form:\n\\[\\begin{aligned}\ntotfatrte &= b_0 + b_1 \\mathbb{1}(seatbealt=1) + b_2 \\mathbb{1}(seatbealt=2) + b_3 minage  \\\\\n& + b_4 zerotol + b_5 unem + b_6 perc14\\_24 + b_7 vehicmiles + e\n\\end{aligned} \\tag{2}\\]\nAnd interpret the results. is there any effect that seems unexpected? Explain why this may be the case.\nQ3-2. (5pts) One of the reasons one may be finding unexpected results is because of unobserved heterogeneity across states. Can you explain what kind of factors are there that could be related to both accidents and the laws? (think about the political process of passing laws)\nQ3-3. (10pts) To control for unobserved heterogeneity, estimate three models:\n- Random effect model\n- Fixed effect model\n- Correlated Random effect model\nHow do the results from the Random effect model and Fixed effect model compare to each other? what about compared to Equation 2?\nQ3-4. (5pts) Using the results from the Correlated Random effect model, test for which of the models is more appropriate, between Random Effects and Fixed effects. What do you conclude?"
  },
  {
    "objectID": "rmethods/homework_1.html",
    "href": "rmethods/homework_1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Read Chapter 2 of -The Effect- by Nick Hungtington-Klein. here\nWrite a small research proposal that will answer answer the following:\n\nResearch question,\nIdentify your main dependent variable, and the variable(s) you want to analyze the causal effect of.\nDescribe relevant factors that may need to be considered for the analysis (Economic model).\nDescribe how those factors may relate to the outcome, and your variable(s) of interest.\nDescribe an ideal Experiment you may run to identify the effect. Is it a feasible experiment?\nIs there any data you could use to answer this question?"
  },
  {
    "objectID": "rmethods/homework_1.html#note",
    "href": "rmethods/homework_1.html#note",
    "title": "Homework 1",
    "section": "Note",
    "text": "Note\nSearch among all datasets available in frause, and use the data from these datasets to decide what variables to analyze, and controls to use in your homework.\nExamples in the textbook can be useful as guides to consider here.\nYou are free to explore other sources. If so, include the data along with your homework.\nParticularly unique answers (based on completeness, detail, novelty) may receive extra points."
  },
  {
    "objectID": "rmethods/8_iv2sls.html",
    "href": "rmethods/8_iv2sls.html",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "As I mentioned at the beginning, one of the most important assumptions required to analyze data and obtain correct estimations and draw inference was A4: No endogeneity or \\(E(e|X)=0\\)\n\nEndogeneity is a problem that occurs because the error is related to \\(X\\).\nThis is a proble,, because we can no longer assume the error is, in average, constant when analyzing changes in \\(X's\\)\n\n\n\n\n\n\nWhy did it happen?\n\nUsually because important variables are omitted\n\nAdd them back, or at least proxies?\n\nIncorrect functional form\n\nTry making it more flexible?\n\nData has measurement error\n\nGet better data?\n\nSample is endogenous (other treatments are necessary)\nReverse causality (you dont know which cause which)\nSimultenaity, similar to omitted variables. There is another factor that caused both the outcome and explanatory variable\n\n\n\n\n\n\nToday we will cover one approach that could help with many (not all) the situations that could cause endogeneity.\nTo apply this approach, however, we need:\n\nMore data (more variables with specific properties)\nMore information on how the “system” works.\n\nThese methods are:\n\nIV - Instrumental variable\n2sls - Two-stage Least squares\n\n\n\nNOTE: These two methods are almost interchangable.\n\nIV refers to cases with 1 endogenous variable and 1 “instrument”\n2sls refers to cases with 1+ endogenous variables and “instruments”\n\n\n\n\n\n\nI have mentioned a few times the word “instrument” But what are they really?\n\nInstruments: the heros/variables that will “save us” of endogeneity.\n\nThey have at least 2 properties:\n\nInstruments should be exogenous to the model\n\n\nDoes not appear in the specification, thus \\(Z\\) has no DIRECT effect on the outcome \\(y\\).\n\nEffect exists only through the endogenous variable.\n\nAlso that there is no correlation between the model error and \\(Z\\).\n\n\nThe instrument is relevant and related to \\(x\\) (endogenous variable)\n\n\nPreferably, you need a variable that is not only correlated with \\(X\\) but determines changes in \\(X\\).\nWe also need this effect to be monotonic!\n\nIn many instances we even want an instrument that is just as good as [conditionally] random.\n\n\n\n\nThe problem \\(corr(x_1,e) \\neq 0\\): \\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + e || \\tilde w = w-\\bar w  \\\\\n\\tilde \\beta_1 &=\\frac{\\sum \\tilde x_1 \\tilde y}{\\sum \\tilde x_1^2}=\n\\frac{\\sum \\tilde x_1 (\\beta_1 \\tilde x_1 + e)}{\\sum \\tilde x_1^2} \\\\\n& = \\beta_1 + \\frac{\\sum \\tilde x_1 e}{\\sum \\tilde x_1^2}\n\\end{aligned}\n\\]\nHow IV works \\(corr(z_1,e) \\neq 0\\):\n\\[\\begin{aligned}\n\\hat \\beta_1 &=\\frac{\\sum \\tilde z_1 \\tilde y}{\\sum \\tilde z_1 \\tilde x_1}=\n\\frac{\\sum \\tilde z_1 (\\beta_1 \\tilde x_1 + e)}{\\sum \\tilde z_1 \\tilde x_1} \\\\\n& = \\beta_1 + \\frac{\\sum \\tilde z_1 e}{\\sum \\tilde z_1 \\tilde x_1}\n\\end{aligned}\n\\]\n\n\n\n\nOne way of thinking about how IV works is by realizing not ALL changes in \\(X_1\\) are endogenous. Some are due to \\(e\\), but some are due other factors.\n\nwe just can’t differentiate them\nIf we could use (in the regression), only exogenous changes, (or omit endogenous ones), we could estimate our models correctly.\n\nWhat IV’s do is to identify Part of the exogenous component (the one related to \\(Z\\)), and use only THAT variation to identify coefficients.\n\nThis would do a reasonable work, as long as the instrument is relevant, and instrument exogenous.\n\n\n\n\n\n\\[\\begin{aligned}\ne &\\sim chi(2)-2 ; z = chi(2)-2 ; x = chi(2)-2+z+e \\\\\ny &=1+x+e\n\\end{aligned}\n\\]\n\n** Montecarlo Simulation\nset linesize 255\nclear\nset seed 10101\nset obs 1000\nqui:mata:\nk = 1000; n=1000\nb1=bc = b = J(k,2,0)\nfor(i=1;i&lt;=k;i++){\n    e = rchi2(n,1,2):-2\n    e1 = rchi2(n,1,2):-2\n    z = rchi2(n,1,2):-2,J(n,1,1)\n    x = rchi2(n,1,2):-2:+z[,1]:+e ,J(n,1,1)\n    x1 = rchi2(n,1,2):-2:+z[,1]:+e1,J(n,1,1)\n    y = 1:+x[,1]:+e\n    y1 = 1:+x[,1]:+e1\n    xx = cross(x,x)\n    b[i,] = (invsym(xx)*cross(x,y))'\n    bc[i,] = (invsym(cross(z,x))*cross(z,y))'\n    b1[i,] = (invsym(xx)*cross(x,y1))'\n}\nend\ngetmata bb*=b\ngetmata bc*=bc\ngetmata b_*=b1\nset scheme white2\ncolor_style tableau\ntwo kdensity bb1 ||  kdensity bc1  || kdensity b_1, ///\nlegend(order(1 \"X Endogenous\" 2 \"IV\" 3 \"X Exogenous\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSE have a different structure compared to OLS.\nIn the simplest case (one dep variable that is endogenous), and under the assumption of Homoskedasticity, SE for \\(\\beta_1\\) are given by:\n\n\\[\\begin{aligned}Var_{iv}(\\beta_1) = \\frac{\\hat\\sigma^2_e}{SST_x R^2_{x|z}} \\\\\n\\hat\\sigma^2_e =\\frac{ \\sum (y-\\hat\\beta_0-\\hat\\beta_1 x)^2}{n-2}\n\\end{aligned}\n\\]\n\nOnce they are obtained t-stats can be used as usual\n\n\n\n\n\n\nEducation:\n\nFathers Education\nDistance to School\n# Siblins\n\n\nVeteran Status:\n\nVietnam Lottery Ticket\n\nOther:\n\nRain\nShift-Share\nJudge FE\n\n\n\n\nIV SE will be necessarily larger than OLS, because there is less variation of \\(x\\) used to identify \\(\\beta\\).\nIf \\(x\\) is used as its own instrument, the estimator will be that of OLS."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#the-problem-endogeneity",
    "href": "rmethods/8_iv2sls.html#the-problem-endogeneity",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "As I mentioned at the beginning, one of the most important assumptions required to analyze data and obtain correct estimations and draw inference was A4: No endogeneity or \\(E(e|X)=0\\)\n\nEndogeneity is a problem that occurs because the error is related to \\(X\\).\nThis is a proble,, because we can no longer assume the error is, in average, constant when analyzing changes in \\(X's\\)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section",
    "href": "rmethods/8_iv2sls.html#section",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "Why did it happen?\n\nUsually because important variables are omitted\n\nAdd them back, or at least proxies?\n\nIncorrect functional form\n\nTry making it more flexible?\n\nData has measurement error\n\nGet better data?\n\nSample is endogenous (other treatments are necessary)\nReverse causality (you dont know which cause which)\nSimultenaity, similar to omitted variables. There is another factor that caused both the outcome and explanatory variable"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#a-pair-of-solutions",
    "href": "rmethods/8_iv2sls.html#a-pair-of-solutions",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "Today we will cover one approach that could help with many (not all) the situations that could cause endogeneity.\nTo apply this approach, however, we need:\n\nMore data (more variables with specific properties)\nMore information on how the “system” works.\n\nThese methods are:\n\nIV - Instrumental variable\n2sls - Two-stage Least squares\n\n\n\nNOTE: These two methods are almost interchangable.\n\nIV refers to cases with 1 endogenous variable and 1 “instrument”\n2sls refers to cases with 1+ endogenous variables and “instruments”"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#what-is-an-instrumental-variable",
    "href": "rmethods/8_iv2sls.html#what-is-an-instrumental-variable",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "I have mentioned a few times the word “instrument” But what are they really?\n\nInstruments: the heros/variables that will “save us” of endogeneity.\n\nThey have at least 2 properties:\n\nInstruments should be exogenous to the model\n\n\nDoes not appear in the specification, thus \\(Z\\) has no DIRECT effect on the outcome \\(y\\).\n\nEffect exists only through the endogenous variable.\n\nAlso that there is no correlation between the model error and \\(Z\\).\n\n\nThe instrument is relevant and related to \\(x\\) (endogenous variable)\n\n\nPreferably, you need a variable that is not only correlated with \\(X\\) but determines changes in \\(X\\).\nWe also need this effect to be monotonic!\n\nIn many instances we even want an instrument that is just as good as [conditionally] random."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#how-instruments-work-the-math",
    "href": "rmethods/8_iv2sls.html#how-instruments-work-the-math",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "The problem \\(corr(x_1,e) \\neq 0\\): \\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + e || \\tilde w = w-\\bar w  \\\\\n\\tilde \\beta_1 &=\\frac{\\sum \\tilde x_1 \\tilde y}{\\sum \\tilde x_1^2}=\n\\frac{\\sum \\tilde x_1 (\\beta_1 \\tilde x_1 + e)}{\\sum \\tilde x_1^2} \\\\\n& = \\beta_1 + \\frac{\\sum \\tilde x_1 e}{\\sum \\tilde x_1^2}\n\\end{aligned}\n\\]\nHow IV works \\(corr(z_1,e) \\neq 0\\):\n\\[\\begin{aligned}\n\\hat \\beta_1 &=\\frac{\\sum \\tilde z_1 \\tilde y}{\\sum \\tilde z_1 \\tilde x_1}=\n\\frac{\\sum \\tilde z_1 (\\beta_1 \\tilde x_1 + e)}{\\sum \\tilde z_1 \\tilde x_1} \\\\\n& = \\beta_1 + \\frac{\\sum \\tilde z_1 e}{\\sum \\tilde z_1 \\tilde x_1}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#how-instruments-work-the-intuition",
    "href": "rmethods/8_iv2sls.html#how-instruments-work-the-intuition",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "One way of thinking about how IV works is by realizing not ALL changes in \\(X_1\\) are endogenous. Some are due to \\(e\\), but some are due other factors.\n\nwe just can’t differentiate them\nIf we could use (in the regression), only exogenous changes, (or omit endogenous ones), we could estimate our models correctly.\n\nWhat IV’s do is to identify Part of the exogenous component (the one related to \\(Z\\)), and use only THAT variation to identify coefficients.\n\nThis would do a reasonable work, as long as the instrument is relevant, and instrument exogenous."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#example",
    "href": "rmethods/8_iv2sls.html#example",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "\\[\\begin{aligned}\ne &\\sim chi(2)-2 ; z = chi(2)-2 ; x = chi(2)-2+z+e \\\\\ny &=1+x+e\n\\end{aligned}\n\\]\n\n** Montecarlo Simulation\nset linesize 255\nclear\nset seed 10101\nset obs 1000\nqui:mata:\nk = 1000; n=1000\nb1=bc = b = J(k,2,0)\nfor(i=1;i&lt;=k;i++){\n    e = rchi2(n,1,2):-2\n    e1 = rchi2(n,1,2):-2\n    z = rchi2(n,1,2):-2,J(n,1,1)\n    x = rchi2(n,1,2):-2:+z[,1]:+e ,J(n,1,1)\n    x1 = rchi2(n,1,2):-2:+z[,1]:+e1,J(n,1,1)\n    y = 1:+x[,1]:+e\n    y1 = 1:+x[,1]:+e1\n    xx = cross(x,x)\n    b[i,] = (invsym(xx)*cross(x,y))'\n    bc[i,] = (invsym(cross(z,x))*cross(z,y))'\n    b1[i,] = (invsym(xx)*cross(x,y1))'\n}\nend\ngetmata bb*=b\ngetmata bc*=bc\ngetmata b_*=b1\nset scheme white2\ncolor_style tableau\ntwo kdensity bb1 ||  kdensity bc1  || kdensity b_1, ///\nlegend(order(1 \"X Endogenous\" 2 \"IV\" 3 \"X Exogenous\"))"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#se-and-statistical-inference",
    "href": "rmethods/8_iv2sls.html#se-and-statistical-inference",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "SE have a different structure compared to OLS.\nIn the simplest case (one dep variable that is endogenous), and under the assumption of Homoskedasticity, SE for \\(\\beta_1\\) are given by:\n\n\\[\\begin{aligned}Var_{iv}(\\beta_1) = \\frac{\\hat\\sigma^2_e}{SST_x R^2_{x|z}} \\\\\n\\hat\\sigma^2_e =\\frac{ \\sum (y-\\hat\\beta_0-\\hat\\beta_1 x)^2}{n-2}\n\\end{aligned}\n\\]\n\nOnce they are obtained t-stats can be used as usual"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#examples-of-ivs",
    "href": "rmethods/8_iv2sls.html#examples-of-ivs",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "Education:\n\nFathers Education\nDistance to School\n# Siblins\n\n\nVeteran Status:\n\nVietnam Lottery Ticket\n\nOther:\n\nRain\nShift-Share\nJudge FE\n\n\n\n\nIV SE will be necessarily larger than OLS, because there is less variation of \\(x\\) used to identify \\(\\beta\\).\nIf \\(x\\) is used as its own instrument, the estimator will be that of OLS."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#weak-and-endogenous-instruments",
    "href": "rmethods/8_iv2sls.html#weak-and-endogenous-instruments",
    "title": "Instrumental Variables and 2SLS",
    "section": "Weak and endogenous instruments:",
    "text": "Weak and endogenous instruments:\n\nWhile instruments can be used to address Endogeneity problems, finding good instruments can be hard.\n\nThey can be “weak-instruments”\nor not fully exogenous\n\nIf this happens, IV can be worse than endogeneity:\n\n\\[\\begin{aligned}\nplim \\beta_{ols} &= \\beta_1 + corr(x,u) \\frac{\\sigma_u}{\\sigma_x} \\\\\nplim \\beta_{iv}  &= \\beta_1 + \\frac{corr(z,u)}{corr(z,x)} \\frac{\\sigma_u}{\\sigma_x}\n\\end{aligned}\n\\]\n\nwe will talk about the “weak-instruments” later today."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#iv-with-mlr",
    "href": "rmethods/8_iv2sls.html#iv-with-mlr",
    "title": "Instrumental Variables and 2SLS",
    "section": "IV with MLR",
    "text": "IV with MLR\nAdding controls…is easy!\n\\[y = \\beta_0 + \\gamma_1 y_1 + X\\beta + e ; z \\text{ instrument for } y\n\\]\nWe still assume 1 endogenous variable (\\(y_1\\)) and one instrument (\\(z\\))\n\\[X =\\begin{bmatrix} 1 & y_1 & x_1 & x_2 & x_3 \\end{bmatrix} ;\nZ =\\begin{bmatrix} 1 & z & x_1 & x_2 & x_3 \\end{bmatrix}\n\\]\nthen \\(\\hat\\beta_{iv}\\) is given by\n\\[\\hat\\beta_{iv} = (Z'X)^{-1}{Z'y}\n\\]\n\nSame assumptions needed, except that instrument strength is measured by the \\(corr(y_1-E[y_1|X], z-E[z|X] )\\)\n\nMulticollinearity problem can be a problem here"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#iv-and-treatment-effects",
    "href": "rmethods/8_iv2sls.html#iv-and-treatment-effects",
    "title": "Instrumental Variables and 2SLS",
    "section": "IV and Treatment Effects",
    "text": "IV and Treatment Effects\nOne small note:\n\nWhen analyzing the model of interest, we could also try to analyze the reduced form model:\n\n\\[y = \\beta_0 + \\lambda z + X \\beta + e\\]\n\nThis model will not give you the effect of \\(y_1\\) (endogenous variable), but can be just as interesting, specially when instrument and endogenous variables are dummies.\n\nIn such case \\(\\lambda\\) will represent the “intention-to-treat” effect, rather than “treatment” effect."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#sls-many-ys-many-zs",
    "href": "rmethods/8_iv2sls.html#sls-many-ys-many-zs",
    "title": "Instrumental Variables and 2SLS",
    "section": "2SLS: Many Ys many Zs",
    "text": "2SLS: Many Ys many Zs\n\nThere could be situations where you have not one, but many instruments.\n\nThis may be rare, as even finding a single instrument can be hard\n\nYou may also have the situation where there is more than one endogenous variable!\n\nIn such case, you need at least as many IVs as endogenous variables\n\n\n\nThe same assumptions as before apply. IV’s need to be exogenous, but relevant to explain the endogenous variables.\nContrary to intuition, ALL instruments are used to analyze ALL exogenous variables"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#sls-estimation",
    "href": "rmethods/8_iv2sls.html#sls-estimation",
    "title": "Instrumental Variables and 2SLS",
    "section": "2SLS: Estimation",
    "text": "2SLS: Estimation\nConsider the following:\n\n\\(y\\) is the variable of interest\n\\(x1, x2\\) set of exogenous variables\n\\(y1, y2\\) set of endogenous variables\n\\(z1,z2,z3\\) set of instruments for \\(y1\\) and \\(y2\\)\n\n\\(X's\\) and \\(Z's\\) are exogenous:"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section-1",
    "href": "rmethods/8_iv2sls.html#section-1",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "Model of interesed: \\[y = a_0 + a_1 y_1 + a_2 y_2 + b_1 x_1 + b_2 x_2 +  b_3 x_3 + e\n\\]\nFirst Stage\n\\[\\begin{aligned}\ny_1 = \\gamma^1_0 + \\gamma^1_1 z_1+ \\gamma^1_2 z_2+ \\gamma^1_3 z_3+\\lambda^1_1 x_1 + \\lambda^1_2 x_2 +  \\lambda^1_3 x_3  + v_1 \\rightarrow \\hat y_1 \\\\\ny_2 = \\gamma^2_0 + \\gamma^2_1 z_1+ \\gamma^2_2 z_2+ \\gamma^2_3 z_3+\\lambda^2_1 x_1 + \\lambda^2_2 x_2 +  \\lambda^2_3 x_3  + v_2 \\rightarrow \\hat y_1\n\\end{aligned}\n\\]\nSecond Stage:\n\\[y = a_0 + a_1 \\hat y_1 + a_2 \\hat y_2 + b_1 x_1 + b_2 x_2 +  b_3 x_3 + e\n\\]\nThis last model should work, because \\(\\hat y_1\\) and \\(\\hat y_2\\) are exogenous (if \\(Zs\\) are).\nStandard Errors, however, need to be adjusted appropietly. (all two-step estimators need this)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#matrix-math",
    "href": "rmethods/8_iv2sls.html#matrix-math",
    "title": "Instrumental Variables and 2SLS",
    "section": "Matrix Math",
    "text": "Matrix Math\n\\[\\begin{aligned}\nX &= \\begin{bmatrix} 1 & y_k & x  \\end{bmatrix} \\\\\nZ &=\\begin{bmatrix} 1 & z   & x \\end{bmatrix}\n\\end{aligned}\n\\]\nThen\n\\[\\begin{aligned}\n\\beta_{2sls} &= [X'Z(Z'Z)^{-1}Z'X]^{-1}[X'Z(Z'Z)^{-1}Z'y] \\\\\n\\beta_{2sls} &= [X'P_z X]^{-1}[X'P_z y] \\\\\n\\beta_{2sls} &= [\\hat X'X]^{-1}[\\hat X'y]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#multicolinearity-and-2sls",
    "href": "rmethods/8_iv2sls.html#multicolinearity-and-2sls",
    "title": "Instrumental Variables and 2SLS",
    "section": "Multicolinearity and 2sls",
    "text": "Multicolinearity and 2sls\n\nWhen Appplying 2sls, the problem of Multicolinearity could be stronger.\n\nBecause of MCL, Standard errors will increase (less individual variation.)\nBecause of IV, only a fraction of the variation is used for the analysis.\nBotton line: Combined have conisderate results."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#weak-instruments",
    "href": "rmethods/8_iv2sls.html#weak-instruments",
    "title": "Instrumental Variables and 2SLS",
    "section": "Weak Instruments",
    "text": "Weak Instruments\n\nWeak Instruments create problems when using IV’s and 2SLS.\n\nA weak instrument is one that doesnt have much explanatory power on dep variable, once all other controls are taken into account.\nIf the instrument is too weak, the bias it generates could larger than OLS.\nThe distribution of coefficent is no longer normal, so its harder to make inference.\n\n\n\nHow do we test for it?"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section-2",
    "href": "rmethods/8_iv2sls.html#section-2",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "We usually test for weak instruments when analyzing the “first-stage” regression.\n\\[y_2 = \\gamma_0 + \\gamma_1 z_1 + \\gamma_2 z_2 + \\gamma_3 x_1 + \\gamma_4 x_2 + e\n\\]\n\nThe null is: \\(H_0: \\gamma_1 = \\gamma_2 =0\\), or the instruments are weak.\nBased on Stock and Yogo (2005), the general recommendation is to get an F&gt;10, to reject the Null.\nHowever, new evidence and research suggest that F=10 is not large enough.\n\nOne should either use F&gt;104 (To keep the same t) (Lee McCrary Moreira Porter, 2020)\nor use an alternative t-critica (3.4 for an F=10)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#examples",
    "href": "rmethods/8_iv2sls.html#examples",
    "title": "Instrumental Variables and 2SLS",
    "section": "Examples",
    "text": "Examples\n\n/*\n* This code is only to show the process. Too long to run in quarto\ncapture program drop simx \nprogram simx, eclass\nclear\nlocal N `1'\nlocal F `2'\nlocal sig = sqrt(`N'/ `F')\nset obs `N'\ngen e = rnormal()\ngen z = rnormal() \ngen x = 1 + z + (e+rnormal())*sqrt(.5)*`sig'\n*sqrt(10) \ngen y = 1 + (e+rnormal())*sqrt(.5)\nreg x z, \nmatrix b=(_b[z]/_se[z])^2\nivreg y (x=z), \nmatrix b=b,_b[x],_b[x]/_se[x]\nereturn post b\nend\n\ntempfile f1 f2 f3 f4 f5\nparallel initialize 14\nparallel sim, reps(5000): simx 500 10\ngen F=10\nsave `f1'\nparallel sim, reps(5000): simx 500 20\ngen F=20\nsave `f2'\nparallel sim, reps(5000): simx 500 40\ngen F=40\nsave `f3'\nparallel sim, reps(5000): simx 500 80\ngen F=80\nsave `f4'\nparallel sim, reps(5000): simx 500 160\ngen F=160\nsave `f5'\n\nclear \nappend using `f1'\nappend using `f2'\nappend using `f3'\nappend using `f4'\nappend using `f5'\n\nren (*) (f_stat b_coef t_stat)\n*/\nuse mdata/ivweak, clear\nset scheme white2\ncolor_style tableau\n\njoy_plot t_stat, over(F) xline(-1.96 1.96) xtitle(t-Stat) dadj(2)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#iv-for-measurement-errors-two-wrongs-make-one-right",
    "href": "rmethods/8_iv2sls.html#iv-for-measurement-errors-two-wrongs-make-one-right",
    "title": "Instrumental Variables and 2SLS",
    "section": "IV for Measurement errors: Two wrongs make one right",
    "text": "IV for Measurement errors: Two wrongs make one right\n\nAs described previously when independent variables have measurement errors (Classical), using the variable with errors will produced biased coefficients.\nIf you have multiple variables with Mesurement error, however, its possible to use IV to correct the problem.\n\nThis is done by using one variable as the instrument of the other.\n\nFOr this strategy to work, we need the error to be classical. That is, Uncorrelated across each other, and unrelated to the model error.\n\nConsider the following:\n\\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + e \\\\\n\\check x_1 &= x_1 + u_1 \\\\\n\\tilde x_1 &= x_1 + u_2 \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section-3",
    "href": "rmethods/8_iv2sls.html#section-3",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "If we use each model, independently, we will have biased coefficients\nIf we combined the, Biases will remain present (albeit lower)\nIf we use one as instrument of the other tho:\n\n\\[\n\\begin{aligned}\n\\beta_{1,iv} &= \\frac{cov(\\check x_1, y)}{cov(\\check x_1, \\tilde x_1)} or \\beta_{1,iv} = \\frac{cov(\\tilde x_1, y)}{cov(\\check x_1, \\tilde x_1)} \\\\\n& = \\frac{cov(x_1 + u_1, \\beta_0 + \\beta_1 x_1 + e)}{cov(x_1 + u_2, x_1 + u_1)} \\\\\n& = \\frac{\\beta_1 cov(x_1,x_1) + cov(x_1, e) + \\beta_1 cov(u_1, x_1) + cov(x_1,e)}{cov(x_1 , x_1 )} \\\\\n& = \\beta_1\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#example-1",
    "href": "rmethods/8_iv2sls.html#example-1",
    "title": "Instrumental Variables and 2SLS",
    "section": "Example",
    "text": "Example\n\nclear\nset obs 1000\ngen x = rnormal()\ngen y = 1 + x + rnormal() \ngen x1 = x + rnormal()\ngen x2 = x + rnormal()\nqui: regress y x\nest sto m1\nqui: regress y x1\nest sto m2\nqui: regress y x2\nest sto m3\ngen x1_x2 = (x1 + x2)/2\nqui: regress y x1_x2\nest sto m3b\nqui:ivregress 2sls  y (x1=x2)\nest sto m4\nqui:ivregress 2sls  y (x2=x1)\nest sto m5\nesttab m1 m2 m3 m3b m4 m5, se\n\nNumber of observations (_N) was 0, now 1,000.\n\n------------------------------------------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)             (5)             (6)   \n                        y               y               y               y               y               y   \n------------------------------------------------------------------------------------------------------------\nx                   1.006***                                                                                \n                 (0.0320)                                                                                   \n\nx1                                  0.520***                                        1.018***                \n                                 (0.0277)                                        (0.0640)                   \n\nx2                                                  0.502***                                        1.038***\n                                                 (0.0277)                                        (0.0654)   \n\nx1_x2                                                               0.682***                                \n                                                                 (0.0301)                                   \n\n_cons               1.010***        0.996***        0.988***        0.983***        0.974***        0.956***\n                 (0.0313)        (0.0380)        (0.0384)        (0.0360)        (0.0438)        (0.0451)   \n------------------------------------------------------------------------------------------------------------\nN                    1000            1000            1000            1000            1000            1000   \n------------------------------------------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#testing-for-endogeneity-and-overidentifying-restrictions",
    "href": "rmethods/8_iv2sls.html#testing-for-endogeneity-and-overidentifying-restrictions",
    "title": "Instrumental Variables and 2SLS",
    "section": "Testing for Endogeneity and Overidentifying restrictions",
    "text": "Testing for Endogeneity and Overidentifying restrictions\n\nIn general, Instrumental variables are a great tool to deal with A4 violations.\n\nbut, it can be hard to find the perfect IV, and you may still have problems if its Weak.\n\nIn that case, it may be useful to asnwer…Do you have an Endogeneity problem? (empirically rather than theoretically)\n\nTest:\n\nEstimate first stage, and save predicted residuals \\(\\hat v\\):\n\n\\[y_2 = \\gamma_0 + \\gamma_1 z_1 + \\gamma_2 x_1 + \\gamma_3 x_2 + v\n\\]\nYou expect residuals to be endogenous\n\nEstimate main model “adding” the residuals first stage\n\n\\[y = \\beta_0 + \\beta_1 y_2 + \\beta_2 x_1 + \\beta_3 x_2 + \\theta \\hat v + e\n\\]\nTest for \\(H_0: \\theta=0\\)."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section-4",
    "href": "rmethods/8_iv2sls.html#section-4",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "If the model was endogenous, Then \\(\\theta\\) will be different from zero, and \\(\\beta's\\) different from the case without controlling for it.\nOtherwise, we reject presence of endogeneity.\nThis method has the added advantage:\nFor the simple and exactly identified case, 2sls and adding residuals provide the same solution, except for SE. (its called Control function approach)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#overidentifying-restrictions",
    "href": "rmethods/8_iv2sls.html#overidentifying-restrictions",
    "title": "Instrumental Variables and 2SLS",
    "section": "overidentifying restrictions",
    "text": "overidentifying restrictions\n\nSome times, you may have access to multiple potential instruments.\n\nBut what if this instruments give you different results?\nIf you expect/believe a single effect exists, then there may be a problem. (one of they may be endogenous)\n\nSo how do we test if the instruments are exogenous?\n\nFirst you need more instruments than endogenous variables."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#section-5",
    "href": "rmethods/8_iv2sls.html#section-5",
    "title": "Instrumental Variables and 2SLS",
    "section": "",
    "text": "S1. Estimate Structural Equation \\[y=\\beta_0 + \\gamma y_2 + \\beta_1 x_1 + e | y_2 \\sim z_1, z_2\n\\]\nS2. Auxiliary equation \\[\n\\hat e = \\delta_0 + \\delta_1 z_1 + \\delta_2 z2 + \\delta_3 x_1 + v\n\\]\nS3. Test for Overall Fitness. \\(nR^2\\sim \\chi^2(q_{iv})\\) with \\(q_{iv} = \\# over IVs\\)"
  },
  {
    "objectID": "rmethods/8_iv2sls.html#ivs-as-lates",
    "href": "rmethods/8_iv2sls.html#ivs-as-lates",
    "title": "Instrumental Variables and 2SLS",
    "section": "IV’s as LATES",
    "text": "IV’s as LATES\nNOTE\n\n\n\n\n\n\nImportant\n\n\n\nWhen describing this test, I mentioned that one would be typically worried if observing multiple coefficients when using different IV’s.\nHowever, IV’s can identify different effects, because different IV’s may affect different people differently.\n\\(Z1\\) may affect, say, only men. \\(z_2\\) only women, \\(z_3\\) only highly educated, etc.\nWhen analyzing IV’s will be important to undertand who would be affected by the instrument the most, because that may explain why effects vary."
  },
  {
    "objectID": "rmethods/8_iv2sls.html#example-2",
    "href": "rmethods/8_iv2sls.html#example-2",
    "title": "Instrumental Variables and 2SLS",
    "section": "Example",
    "text": "Example\nIgnoring Potential endogeneity\n\nfrause mroz, clear\ndrop if lwage==.\n** But with Endogeneity\nreg lwage educ exper expersq\n\n(325 observations deleted)\n\n      Source |       SS           df       MS      Number of obs   =       428\n-------------+----------------------------------   F(3, 424)       =     26.29\n       Model |  35.0222967         3  11.6740989   Prob &gt; F        =    0.0000\n    Residual |  188.305144       424  .444115906   R-squared       =    0.1568\n-------------+----------------------------------   Adj R-squared   =    0.1509\n       Total |  223.327441       427  .523015084   Root MSE        =    .66642\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .1074896   .0141465     7.60   0.000     .0796837    .1352956\n       exper |   .0415665   .0131752     3.15   0.002     .0156697    .0674633\n     expersq |  -.0008112   .0003932    -2.06   0.040    -.0015841   -.0000382\n       _cons |  -.5220406   .1986321    -2.63   0.009    -.9124667   -.1316144\n------------------------------------------------------------------------------\n\n\nFirst Stage for different IVs\n\nqui: reg educ fatheduc exper expersq\npredict r1 , res\ntest fatheduc\nadde scalar fiv = r(F)\nadde scalar pfiv = r(p)\nest sto m1\nqui: reg educ motheduc exper expersq\npredict r2 , res\ntest motheduc\nadde scalar fiv = r(F)\nadde scalar pfiv = r(p)\nest sto m2\nqui: reg educ fatheduc motheduc exper expersq\npredict r3 , res\ntest motheduc fatheduc\nadde scalar fiv = r(F)\nadde scalar pfiv = r(p)\nest sto m3\n\nesttab m1 m2 m3 , scalar(fiv pfiv) sfmt(%5.2f %5.3f) order(fatheduc motheduc exper expersq) se ///\nstar(* 0.1 ** 0.05 *** 0.01)\n\n\n ( 1)  fatheduc = 0\n\n       F(  1,   424) =   87.74\n            Prob &gt; F =    0.0000\n\n ( 1)  motheduc = 0\n\n       F(  1,   424) =   73.95\n            Prob &gt; F =    0.0000\n\n ( 1)  motheduc = 0\n ( 2)  fatheduc = 0\n\n       F(  2,   423) =   55.40\n            Prob &gt; F =    0.0000\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                     educ            educ            educ   \n------------------------------------------------------------\nfatheduc            0.271***                        0.190***\n                 (0.0289)                        (0.0338)   \n\nmotheduc                            0.268***        0.158***\n                                 (0.0311)        (0.0359)   \n\nexper              0.0468          0.0489          0.0452   \n                 (0.0411)        (0.0417)        (0.0403)   \n\nexpersq          -0.00115        -0.00128        -0.00101   \n                (0.00123)       (0.00124)       (0.00120)   \n\n_cons               9.887***        9.775***        9.103***\n                  (0.396)         (0.424)         (0.427)   \n------------------------------------------------------------\nN                     428             428             428   \nfiv                 87.74           73.95           55.40   \npfiv                0.000           0.000           0.000   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01\n\n\nUsing parents education as instruments\n\n* SMALL requests Df adjustment\nqui:ivregress 2sls lwage (educ= fatheduc ) exper expersq, small\nest sto m1\nqui:ivregress 2sls lwage (educ= motheduc ) exper expersq, small\nest sto m2\nqui:ivregress 2sls lwage (educ= fatheduc motheduc) exper expersq, small\nest sto m3\nesttab m1 m2 m3 , se mtitle(IV:Father IV:Mother IV:Parents) ///\nstar(* 0.1 ** 0.05 *** 0.01)\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                IV:Father       IV:Mother      IV:Parents   \n------------------------------------------------------------\neduc               0.0702**        0.0493          0.0614*  \n                 (0.0344)        (0.0374)        (0.0314)   \n\nexper              0.0437***       0.0449***       0.0442***\n                 (0.0134)        (0.0136)        (0.0134)   \n\nexpersq         -0.000882**     -0.000922**     -0.000899** \n               (0.000401)      (0.000406)      (0.000402)   \n\n_cons             -0.0611           0.198          0.0481   \n                  (0.436)         (0.473)         (0.400)   \n------------------------------------------------------------\nN                     428             428             428   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01\n\n\n** Testing for endogeneity\n\n* Instrumenting education\nqui:reg  lwage educ  exper expersq r1\nest sto m1\nqui:reg  lwage educ  exper expersq r2\nest sto m2\nqui:reg  lwage educ  exper expersq r3\nest sto m3\n\nesttab m1 m2 m3 , se mtitle(IV:Father IV:Mother IV:Parents) ///\nstar(* 0.1 ** 0.05 *** 0.01)\n\n*see -estat endogenous- after ivregress 2sls\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                IV:Father       IV:Mother      IV:Parents   \n------------------------------------------------------------\neduc               0.0702**        0.0493          0.0614** \n                 (0.0341)        (0.0366)        (0.0310)   \n\nexper              0.0437***       0.0449***       0.0442***\n                 (0.0133)        (0.0133)        (0.0132)   \n\nexpersq         -0.000882**     -0.000922**     -0.000899** \n               (0.000397)      (0.000398)      (0.000396)   \n\nr1                 0.0450                                   \n                 (0.0375)                                   \n\nr2                                 0.0684*                  \n                                 (0.0397)                   \n\nr3                                                 0.0582*  \n                                                 (0.0348)   \n\n_cons             -0.0611           0.198          0.0481   \n                  (0.433)         (0.463)         (0.395)   \n------------------------------------------------------------\nN                     428             428             428   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01\n\n\nOverid Test\n\nqui:ivregress 2sls lwage (educ= fatheduc motheduc) exper expersq, small\npredict resid, res\nestat overid \nreg resid fatheduc motheduc exper expersq, notable robust\n\n\n  Tests of overidentifying restrictions:\n\n  Sargan (score) chi2(1) =  .378071  (p = 0.5386)\n  Basmann chi2(1)        =  .373985  (p = 0.5408)\n\nLinear regression                               Number of obs     =        428\n                                                F(4, 423)         =       0.11\n                                                Prob &gt; F          =     0.9791\n                                                R-squared         =     0.0009\n                                                Root MSE          =     .67521"
  },
  {
    "objectID": "rmethods/6_Heteros.html",
    "href": "rmethods/6_Heteros.html",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Mathematically: \\[Var(e|x=c_1)\\neq Var(e|x=c_2)\\]\nThis means: the conditional variance of the errors is not constant across control characteristics."
  },
  {
    "objectID": "rmethods/6_Heteros.html#what-is-heteroskedasticity",
    "href": "rmethods/6_Heteros.html#what-is-heteroskedasticity",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Mathematically: \\[Var(e|x=c_1)\\neq Var(e|x=c_2)\\]\nThis means: the conditional variance of the errors is not constant across control characteristics."
  },
  {
    "objectID": "rmethods/6_Heteros.html#consequences",
    "href": "rmethods/6_Heteros.html#consequences",
    "title": "Multiple Regression Analysis",
    "section": "Consequences",
    "text": "Consequences\nWhat happens when you have heteroskedastic errors?\n\nIn terms of \\(\\beta's\\) and \\(R^2\\) and \\(R^2_{adj}\\), nothing. Coefficients and Goodness of fit are still unbiased and consistent.\nBut, Coefficients standard errors are based on the simplifying assumption of normality. Thus Variances will be bias!.\n\nIf variances are biased, then all statistics will be wrong."
  },
  {
    "objectID": "rmethods/6_Heteros.html#section",
    "href": "rmethods/6_Heteros.html#section",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "How bad can it be?\nSetup:\n\\(y = e\\) where \\(e \\sim N(0,\\sigma_e^2h(x))\\)\n\\(x = uniform(-1,1)\\)\n\n\nCode\n/*capture program drop sim_het\nprogram sim_het, eclass\n    clear\n    set obs 500 \n    gen x = runiform(-1,1)\n    gen u = rnormal()\n    ** Homoskedastic\n    gen y_1 = u*2\n    ** increasing first, decreasing later\n    gen y_4 = u*sqrt(9*abs(x))\n    replace x = x-2\n    reg y_1 x\n    matrix b=_b[x],_b[x]/_se[x]\n    reg y_4 x\n    matrix b=b,_b[x],_b[x]/_se[x]\n    matrix coleq   b = h0 h0 h3 h3 \n    matrix colname b = b  t  b  t \n    ereturn post b\nend\nqui:simulate , reps(1000) dots(100):sim_het\nsave mdata/simulate.dta, replace*/\nuse mdata/simulate.dta, replace\ntwo (kdensity h0_b_t) (kdensity h3_b_t) ///\n    (function y = normalden(x), range(-4 4) lw(2) color(gs5%50)), ///\n    legend(order(3 \"Normal\" 1 \"With Homoskedasticty\" 2 \"with Heteroskedasticity\"))\ngraph export images/fig6_1.png, replace height(1000)"
  },
  {
    "objectID": "rmethods/6_Heteros.html#what-to-do-about-it-1",
    "href": "rmethods/6_Heteros.html#what-to-do-about-it-1",
    "title": "Multiple Regression Analysis",
    "section": "What to do about it?",
    "text": "What to do about it?\n\nSo, If errors are heteroskedastic, then all statistics (t-stats, F-stats, chi2’s) are wrong.\nBut, there are solutions…many solutions\n\nGLS: Generalized Least Squares\nWLS: Weighted Least Squares\nFGLS: Feasible Generealized Least Squares\nWFLS: Weighted FGLS\nHC0-HC3: Heteroskedasticity consistent SE\n\nSome of them are more involved than others.\nBut before trying to do that, lets first ask…do we have a problem?"
  },
  {
    "objectID": "rmethods/6_Heteros.html#detecting-the-problem",
    "href": "rmethods/6_Heteros.html#detecting-the-problem",
    "title": "Multiple Regression Analysis",
    "section": "Detecting the Problem",
    "text": "Detecting the Problem\n\nConsider the model:\n\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +\\beta_3 x_3 +e\n\\]\n\nWe usually start with the assumption that errors are homoskedastic \\(Var(e|x's)=\\sigma^2_c\\).\nHowever, now we want to allow for the possibility of heteroskedasiticity. ie, that variance is some function of X.\n\nWe have to test if the conditional variance is a function that varies with \\(x\\):\n\n\n\\[Var(e|x)=f(x_1,x_2,\\dots,x_k) \\sim a_0+a_1x_1 + a_2 x_2 + \\dots + a_k x_k+v\\]"
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-1",
    "href": "rmethods/6_Heteros.html#section-1",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "\\[Var(e|x)=f(x_1,x_2,\\dots,x_k) \\sim a_0+a_1x_1 + a_2 x_2 + \\dots + a_k x_k+v\\]\n\nThis expression says the conditional variance can vary with \\(X's\\).\nIt could be as flexible as needed, but linear is usually enough.\n\nWith this the Null hypothesis is: \\[H_0: a_1 = a_2 = \\dots = a_k=0 \\text{ vs } H_1: H_0 \\text{ is false}\n\\]\nEasy enough, but do we KNOW \\(Var(e|x)\\) ? can we model the equation?"
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-2",
    "href": "rmethods/6_Heteros.html#section-2",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "We don’t!.\n\nBut we can use \\(\\hat e^2\\) instead. The assumption is that \\(\\hat e^2\\) is a good enough approximation for the condional variance \\(Var(e|x)\\).\nWith this, the test for heteroskedasticty can be implemented using the following recipe.\n\n\nEstimate \\(y=x\\beta+e\\) and obtain predicted model errors \\(\\hat e\\).\nModel \\(\\hat e^2 = \\color{green}{h(x)}+v\\), as a proxy for the variance model.\n\n\\(h(x)\\) could be estimated using some linear or nonlinear functional forms.\n\nTest if conditional variance changes with respect to any explanatory variables.\n\nThe null is H0: Errors are Homoskedastic. Rejection the error suggests you have Heteroskedasticity.\nNote: Depending on Model specification, and test used, there are various Heteroskedasticity tests."
  },
  {
    "objectID": "rmethods/6_Heteros.html#heteroskedasticity-tests",
    "href": "rmethods/6_Heteros.html#heteroskedasticity-tests",
    "title": "Multiple Regression Analysis",
    "section": "Heteroskedasticity tests:",
    "text": "Heteroskedasticity tests:\n\\[\\begin{aligned}\n\\text{Model}: y &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + e \\\\\n\\hat e & = y - (\\hat \\beta_0 + \\hat\\beta_1 x_1 +\\hat \\beta_2 x_2 +\\hat \\beta_3 x_3)\n\\end{aligned}\n\\]\n\nBreusch-Pagan test:\n\\[\\begin{aligned}\n\\hat e^2 & = \\gamma_0 + \\gamma_1 x_1 +\\gamma_2 x_2 +\\gamma_3 x_3 + v \\\\\nH_0 &: \\gamma_1=\\gamma_2=\\gamma_3=0 \\\\\nF &= \\frac{R^2_{\\hat e^2}/k}{(1-R^2_{\\hat e^2})/(n-k-1)} \\\\\nLM &=N R^2_{\\hat e^2} \\sim \\chi^2(k) \\leftarrow BP-test\n\\end{aligned}\n\\]\n\nEasy and simple, but only considers “linear” Heteroskedasticity"
  },
  {
    "objectID": "rmethods/6_Heteros.html#heteroskedasticity-tests-1",
    "href": "rmethods/6_Heteros.html#heteroskedasticity-tests-1",
    "title": "Multiple Regression Analysis",
    "section": "Heteroskedasticity tests:",
    "text": "Heteroskedasticity tests:\n\\[\\begin{aligned}\n\\text{Model}: y &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + e \\\\\n\\hat e & = y - (\\hat \\beta_0 + \\hat\\beta_1 x_1 +\\hat \\beta_2 x_2 +\\hat \\beta_3 x_3)\n\\end{aligned}\n\\]\n\nWhite:\n\\[\\begin{aligned}\n\\hat e^2 & = \\gamma_0 + \\sum \\gamma_{1,k} x_k + \\sum \\gamma_{2,k} x_k^2 + \\sum_k \\sum_{j\\neq k} \\gamma_{3,j,k} x_j x_k + v \\\\\nH_0 &: \\text{ All } \\gamma's =0 \\\\\nF &= \\frac{R^2_{\\hat e^2}/q}{(1-R^2_{\\hat e^2})/(n-q-1)} \\\\\nLM &=N R^2_{\\hat e^2} \\sim \\chi^2(q)\n\\end{aligned}\n\\]\n\\(q\\) is the total number of coefficients in the model (not counting the intercept.)\n\nAccounts for nonlinearities, but gets “messy” with more variables."
  },
  {
    "objectID": "rmethods/6_Heteros.html#heteroskedasticity-tests-2",
    "href": "rmethods/6_Heteros.html#heteroskedasticity-tests-2",
    "title": "Multiple Regression Analysis",
    "section": "Heteroskedasticity tests:",
    "text": "Heteroskedasticity tests:\n\\[\\begin{aligned}\n\\text{Model}: y &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + e \\\\\n\\hat e & = y - (\\hat \\beta_0 + \\hat\\beta_1 x_1 +\\hat \\beta_2 x_2 +\\hat \\beta_3 x_3)\n\\end{aligned}\n\\]\n\nModified White:\n\\[\\begin{aligned}\n\\hat y &= y - \\hat e \\\\\n\\hat e^2 & = \\gamma_0 + \\gamma_1 \\hat y + \\gamma_2 \\hat y^2 + \\dots + v \\\\\nH_0 &: \\gamma_1 = \\gamma_2 = \\dots =0 \\\\\nF &= \\frac{R^2_{\\hat e^2}/ h }{(1-R^2_{\\hat e^2})/(n-h-1)} \\\\\nLM &=N R^2_{\\hat e^2} \\sim \\chi^2(h)\n\\end{aligned}\n\\]\n\\(h\\) is the total number of coefficients in the model (not counting the intercept.)\n\nAccounts for nonlinearities (because of how \\(\\hat y\\) is constructed), and is simpler to implement.\nBut, nonlinearity is restricted."
  },
  {
    "objectID": "rmethods/6_Heteros.html#example",
    "href": "rmethods/6_Heteros.html#example",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\nHousing prices:\n\\[\\begin{aligned}\nprice &= \\beta_0 + \\beta_1 lotsize + \\beta_2 sqft + \\beta_3 bdrms + e_1 \\\\\nlog(price) &= \\beta_0 + \\beta_1 log(lotsize) + \\beta_2 log(sqft) + \\beta_3 bdrms + e_2 \\\\\n\\end{aligned}\n\\]\n\nfrause hprice1, clear\nreg price lotsize sqrft bdrms \npredict res, res\npredict price_hat\ngen res2=res^2\ndisplay \"BP-test\"\nreg res2  lotsize sqrft bdrms, notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\ndisplay \"White Test\"\nreg res2  c.(lotsize sqrft bdrms)##c.(lotsize sqrft bdrms), notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\ndisplay \"MWhite Test\"\nreg res2  price_hat c.price_hat#c.price_hat, notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(3, 84)        =     57.46\n       Model |  617130.701         3  205710.234   Prob &gt; F        =    0.0000\n    Residual |  300723.805        84   3580.0453   R-squared       =    0.6724\n-------------+----------------------------------   Adj R-squared   =    0.6607\n       Total |  917854.506        87  10550.0518   Root MSE        =    59.833\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     lotsize |   .0020677   .0006421     3.22   0.002     .0007908    .0033446\n       sqrft |   .1227782   .0132374     9.28   0.000     .0964541    .1491022\n       bdrms |   13.85252   9.010145     1.54   0.128    -4.065141    31.77018\n       _cons |  -21.77031   29.47504    -0.74   0.462    -80.38466    36.84405\n------------------------------------------------------------------------------\n(option xb assumed; fitted values)\nBP-test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(3, 84)        =      5.34\n       Model |   701213780         3   233737927   Prob &gt; F        =    0.0020\n    Residual |  3.6775e+09        84  43780003.5   R-squared       =    0.1601\n-------------+----------------------------------   Adj R-squared   =    0.1301\n       Total |  4.3787e+09        87  50330276.7   Root MSE        =    6616.6\n\nnR^2: 14.092386\np(chi2) 0.003\nWhite Test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(9, 78)        =      5.39\n       Model |  1.6784e+09         9   186492378   Prob &gt; F        =    0.0000\n    Residual |  2.7003e+09        78    34619265   R-squared       =    0.3833\n-------------+----------------------------------   Adj R-squared   =    0.3122\n       Total |  4.3787e+09        87  50330276.7   Root MSE        =    5883.8\n\nnR^2: 33.731659\np(chi2) 0.000\nMWhite Test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(2, 85)        =      9.64\n       Model |   809489395         2   404744697   Prob &gt; F        =    0.0002\n    Residual |  3.5692e+09        85  41991113.9   R-squared       =    0.1849\n-------------+----------------------------------   Adj R-squared   =    0.1657\n       Total |  4.3787e+09        87  50330276.7   Root MSE        =    6480.1\n\nnR^2: 16.268416\np(chi2) 0.000\n\n\n\nfrause hprice1, clear\nreg lprice llotsize lsqrft bdrms \npredict res, res\npredict price_hat\ngen res2=res^2\ndisplay \"BP-test\"\nreg res2  llotsize lsqrft bdrms, notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\ndisplay \"White Test\"\nreg res2  c.(llotsize lsqrft bdrms)##c.(llotsize lsqrft bdrms), notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\ndisplay \"MWhite Test\"\nreg res2  price_hat c.price_hat#c.price_hat, notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(3, 84)        =     50.42\n       Model |  5.15504028         3  1.71834676   Prob &gt; F        =    0.0000\n    Residual |  2.86256324        84  .034078134   R-squared       =    0.6430\n-------------+----------------------------------   Adj R-squared   =    0.6302\n       Total |  8.01760352        87  .092156362   Root MSE        =     .1846\n\n------------------------------------------------------------------------------\n      lprice | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    llotsize |   .1679667   .0382812     4.39   0.000     .0918404     .244093\n      lsqrft |   .7002324   .0928652     7.54   0.000     .5155597    .8849051\n       bdrms |   .0369584   .0275313     1.34   0.183    -.0177906    .0917074\n       _cons |  -1.297042   .6512836    -1.99   0.050    -2.592191    -.001893\n------------------------------------------------------------------------------\n(option xb assumed; fitted values)\nBP-test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(3, 84)        =      1.41\n       Model |  .022620168         3  .007540056   Prob &gt; F        =    0.2451\n    Residual |  .448717194        84  .005341871   R-squared       =    0.0480\n-------------+----------------------------------   Adj R-squared   =    0.0140\n       Total |  .471337362        87  .005417671   Root MSE        =    .07309\n\nnR^2: 4.2232484\np(chi2) 0.238\nWhite Test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(9, 78)        =      1.05\n       Model |  .051147864         9  .005683096   Prob &gt; F        =    0.4053\n    Residual |  .420189497        78  .005387045   R-squared       =    0.1085\n-------------+----------------------------------   Adj R-squared   =    0.0057\n       Total |  .471337362        87  .005417671   Root MSE        =     .0734\n\nnR^2: 9.5494489\np(chi2) 0.388\nMWhite Test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(2, 85)        =      1.73\n       Model |  .018464046         2  .009232023   Prob &gt; F        =    0.1830\n    Residual |  .452873315        85  .005327921   R-squared       =    0.0392\n-------------+----------------------------------   Adj R-squared   =    0.0166\n       Total |  .471337362        87  .005417671   Root MSE        =    .07299\n\nnR^2: 3.4472889\np(chi2) 0.178\n\n\nCan you do this in Stata? Yes, estat hettest. But look into the options. There are many more options in that command."
  },
  {
    "objectID": "rmethods/6_Heteros.html#what-do-you-do-when-you-have-heteroskedasticity",
    "href": "rmethods/6_Heteros.html#what-do-you-do-when-you-have-heteroskedasticity",
    "title": "Multiple Regression Analysis",
    "section": "What do you do when you have Heteroskedasticity?",
    "text": "What do you do when you have Heteroskedasticity?\nWe need to fix!\n\nRecall, the problem is that \\(Var(e|X)\\neq c\\)\nThis affects how standard errors are estimated (we required homoskedasticity). But what happens when Homoskedasticity doesnt hold?\n\nWe can “fix/change” the model, so its no longer heteroskedastic, and Standard Inference works. (FGLS, WLS)\nWe neec to account for heteroskedasticity when estimating the variance covariance model.\n\n\nSo lets learn to Fix it first"
  },
  {
    "objectID": "rmethods/6_Heteros.html#how-do-we-fix-heteroskedasticity",
    "href": "rmethods/6_Heteros.html#how-do-we-fix-heteroskedasticity",
    "title": "Multiple Regression Analysis",
    "section": "How do we Fix Heteroskedasticity?",
    "text": "How do we Fix Heteroskedasticity?\n\nIn order to address the problem of heteroskedasticity, we require knowledge of why the model is heteroskedastic, or what is generating it.\n\n\\[Var(e|X)=h(x)\\sigma^2_e\n\\]\n\nWhere \\(h(x)\\) is the “source” of heteroskedasticity, which may be a known or estimated function of \\(x\\).\n\nWhich should be an strictly possitive function of \\(x's\\)."
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-4",
    "href": "rmethods/6_Heteros.html#section-4",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Knowledge is power\n\nIf you know \\(h(x)\\), correcting heteroskedasticity is “easy”. Consider the following:\n\n\\[\\begin{aligned}\ny &= b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3 +e \\\\\nVar(e|x) &=x_1 \\sigma_e^2 || h(x)=x_1\n\\end{aligned}\n\\]\nYou can correct Heteroskedasticity in two ways:\n\nTransform model by dividing everything by \\(\\sqrt{h(x)}\\): \\[\\begin{aligned}\n\\frac{y}{\\sqrt{x_1}} &= b_0 \\frac{1}{\\sqrt{x_1}}+ b_1 \\sqrt{x_1} + b_2 \\frac{x_2}{\\sqrt{x_1}} + b_3 \\frac{x_3}{\\sqrt{x_1}} +\\frac{e}{\\sqrt{x_1}} \\\\\nVar\\left(\\frac{e}{\\sqrt{x_1}}|x\\right) &= \\frac{1}{x_1} x_1\\sigma_e^2=\\sigma_e^2\n\\end{aligned}\n\\]\n\nThe new error is Homoskedastic (but has no constant)!"
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-5",
    "href": "rmethods/6_Heteros.html#section-5",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Estimate the model using by \\(\\frac{1}{h(x)}\\) as weights: \\[\\begin{aligned}\n\\beta=\\min_\\beta \\sum \\frac{1}{h(x)} (y-(b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3))^2\n\\end{aligned}\n\\]\n\n\nSame solution as before, and there is no need to “transform” data, or keep track of a constant.\nThis is often called WLS (weighted least squares) or GLS (Generalized Least Squares)."
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-6",
    "href": "rmethods/6_Heteros.html#section-6",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Interestingly: These approaches are more efficient than Standard OLS.\n\nUses more information (heteroskedasticity)\nMakes better use of information (More weight to better data) Standard errors are smaller.\n\nt-stats, F-stats, etc now are valid.\nCoefficients will NOT be the same as before.\n\\(R^2\\) is less useful\nHeteroskedasticty test on transformed data may required added work."
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-7",
    "href": "rmethods/6_Heteros.html#section-7",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "FGLS: We do not know \\(h(x)\\), but we can guess\n\nIf \\(h(x)\\) is not known, we can use an auxiliary model to estimate it:\n\n\\[\\begin{aligned}\nVar(e|x) &= \\sigma^2 \\exp(\\delta_0 + \\delta_1 x_1 + \\delta_2 x_2 +\\dots) \\exp v \\\\\n\\hat e^2 &= \\sigma^2 \\exp(\\delta_0 + \\delta_1 x_1 + \\delta_2 x_2 +\\dots) \\exp v \\\\\nlog(\\hat e^2) &= \\delta_0 + \\delta_1 x_1 + \\delta_2 x_2 +\\dots+ v \\\\\nlog(\\hat e^2) &= \\delta_0 + \\delta_1 \\hat y + \\delta_2 \\hat y^2 + \\dots+ v \\\\\n\\widehat{\\log h(x)} &= \\hat \\delta_0 + \\hat \\delta_1 x_1 + \\hat \\delta_2 x_2 + \\dots = x \\hat \\delta \\\\\n\\hat h(x) &= \\exp (x \\hat \\delta) \\text{ or } \\hat h(x)=e^{x \\hat \\delta}\n\\end{aligned}\n\\]\n\nProceed as before (weighted or transformed)\nIts call Feasible GLS, because we need to estimate \\(h(x)\\)."
  },
  {
    "objectID": "rmethods/6_Heteros.html#do-not-correct-account-for-it-gls-and-fgls",
    "href": "rmethods/6_Heteros.html#do-not-correct-account-for-it-gls-and-fgls",
    "title": "Multiple Regression Analysis",
    "section": "Do not Correct, account for it: GLS and FGLS",
    "text": "Do not Correct, account for it: GLS and FGLS\nRecall “Long” variance formula:\n\\[Var(\\beta)=\\color{brown}{(X'X)^{-1}}\\color{green}{X}'\\color{red}{Var(e|X)}\\color{green}{X}\\color{brown}{(X'X)^{-1}}\n\\]\n\nThe red part is a \\(N\\times N\\) VCOV matrix of ALL erros. It can be Simplified with what we know!\n\n\\[\\begin{aligned}\nVar_{gls/fgls}(\\beta)&=\\sigma^2_{\\tilde e} \\color{brown}{(X'X)^{-1}}\\color{green}{X}'\\color{red}{ \\Omega_h(x) }\\color{green}{X}\\color{brown}{(X'X)^{-1}} \\\\\n\\sigma^2_{\\tilde e} &= \\frac{1}{N-k-1} \\sum \\frac{\\hat e^2}{h(x)} \\\\\n\\Omega_h(x) [i,j] &= h(x_i) & \\text{ if } i=j \\\\\n& = 0 & \\text{ if } i\\neq j \\\\\n\\end{aligned}\n\\]\n\nSE are corrected, but coefficients remain the same!"
  },
  {
    "objectID": "rmethods/6_Heteros.html#do-not-correct-account-for-it-white-sandwich-formula",
    "href": "rmethods/6_Heteros.html#do-not-correct-account-for-it-white-sandwich-formula",
    "title": "Multiple Regression Analysis",
    "section": "Do not Correct, account for it: White Sandwich Formula",
    "text": "Do not Correct, account for it: White Sandwich Formula\n\nWhat if we do not want to even try guessing \\(h(x)\\)?\nyou can use Robust Standard errors!\n\nHeteroskedastic Consistent SE to Heterosedasticity of unknown form.\n\n\nLet me present to you, the Sandwitch Formula: \\[Var(\\beta)=c \\color{brown}{(X'X)^{-1}}\\color{green}{X}'\\color{red}{\\Omega}\\color{green}{X}\\color{brown}{(X'X)^{-1}}\n\\]\n\\[\\begin{aligned}\n\\Omega [i,j] &= \\hat e_i^2 & \\text{ if } i=j \\\\\n& = 0 & \\text{ if } i\\neq j \\\\\n\\end{aligned}\n\\]\n\nThe best approximation to conditional variance is equal to \\(\\hat e_i^2\\). (plus assuming no correlation)\nValid in large samples, but can be really bad in smaller ones.\nThere are other versions. See HC0 HC1 HC2 HC3."
  },
  {
    "objectID": "rmethods/6_Heteros.html#what-if-did-hx-and-it-was-wrong",
    "href": "rmethods/6_Heteros.html#what-if-did-hx-and-it-was-wrong",
    "title": "Multiple Regression Analysis",
    "section": "What if did \\(h(x)\\), and it was wrong",
    "text": "What if did \\(h(x)\\), and it was wrong\n\nUsing FGLS will change coefficients a bit. If they change a lot, It could indicate other assumptions in the model are incorrect. (functional form or exogeneity)\nIn either case, you could always combine FGLS with Robust Standard Errors!"
  },
  {
    "objectID": "rmethods/6_Heteros.html#statistical-inference",
    "href": "rmethods/6_Heteros.html#statistical-inference",
    "title": "Multiple Regression Analysis",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nIf applying GLS or FGLS via transformations or reweighting. All we did before is valid.\nIf using Robust standard errors (HC), t-stats are constructed as usual, but\nF-stats formulas are no longer valid.\n\nInstead…use the long formula\n\\[\\begin{aligned}\nH_0: & R_{q,k+1}\\beta_{k+1,1}=c_{q,1} \\\\\n\\Sigma_R &= R_{q,k+1} V^r_\\beta R'_{q,k+1} \\\\\nF-stat &= \\frac 1 q (R\\beta-c)' \\Sigma_R^{-1} (R\\beta-c)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/6_Heteros.html#section-8",
    "href": "rmethods/6_Heteros.html#section-8",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Extra: Prediction and SE\nPrediction SE:\n\nIf you are using GLS, Formulas seen before apply with the following modification: \\(Var(e|X=x_0)=\\sigma^2_{\\tilde e} h(x_0)\\)\nIf you are using FGLS, its not that simple because of the two-step modeling\n\nFor Prediction with Logs\n\nYou need to take into account Heteroskedasticity\n\n\\[\\hat y_i = \\exp \\left( \\widehat{log y_i}+\\hat \\sigma_{\\tilde e}^2 \\hat h_i /2 \\right)\n\\]"
  },
  {
    "objectID": "rmethods/6_Heteros.html#example-1",
    "href": "rmethods/6_Heteros.html#example-1",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\n\nfrause smoke, clear\ngen age_40sq=(age-40)^2\n** Default\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn\nest sto m1\npredict cig_hat\npredict cig_res,res\n** GLS: h(x)=lincome Weighted\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn [aw=1/lincome]\nest sto m2\n** FGLS: h(x) = f(cigs_hat)\ngen lcres=log(cig_res^2)\nqui:reg lcres c.cig_hat##c.cig_hat##c.cig_hat \npredict aux\ngen hx=exp(aux)\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn [aw=1/hx]\nest sto m3\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn , robust\nest sto m4\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn [aw=1/lincome], robust\nest sto m5\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn [aw=1/hx], robust\nest sto m6\nset linesize 255\n\n\nesttab m1 m2 m3 m4 m5 m6, gaps mtitle(default GLS FGLS Rob GLS-Rob FGLS-Rob) ///\nnonum cell( b( fmt( 3) star ) se( par(( )) ) p( par([ ]) ) ) ///\nstar(* .1 ** 0.05 *** 0.01  )\n\n\n------------------------------------------------------------------------------------------------------------\n                  default             GLS            FGLS             Rob         GLS-Rob        FGLS-Rob   \n                   b/se/p          b/se/p          b/se/p          b/se/p          b/se/p          b/se/p   \n------------------------------------------------------------------------------------------------------------\nlincome             0.880           0.926           1.005**         0.880           0.926*          1.005   \n                  (0.728)         (0.672)         (0.422)         (0.596)         (0.559)         (0.651)   \n                  [0.227]         [0.169]         [0.017]         [0.140]         [0.098]         [0.123]   \n\nlcigpric           -0.751          -1.525          -4.572          -0.751          -1.525          -4.572   \n                  (5.773)         (5.696)         (4.260)         (6.035)         (6.067)         (9.651)   \n                  [0.897]         [0.789]         [0.284]         [0.901]         [0.802]         [0.636]   \n\neduc               -0.501***       -0.477***       -0.610***       -0.501***       -0.477***       -0.610***\n                  (0.167)         (0.166)         (0.115)         (0.162)         (0.159)         (0.115)   \n                  [0.003]         [0.004]         [0.000]         [0.002]         [0.003]         [0.000]   \n\nage                 0.049           0.048           0.041           0.049*          0.048           0.041   \n                  (0.034)         (0.033)         (0.026)         (0.030)         (0.029)         (0.032)   \n                  [0.146]         [0.147]         [0.108]         [0.099]         [0.101]         [0.197]   \n\nage_40sq           -0.009***       -0.009***       -0.007***       -0.009***       -0.009***       -0.007***\n                  (0.002)         (0.002)         (0.001)         (0.001)         (0.001)         (0.002)   \n                  [0.000]         [0.000]         [0.000]         [0.000]         [0.000]         [0.001]   \n\nrestaurn           -2.825**        -2.776**        -3.383***       -2.825***       -2.776***       -3.383***\n                  (1.112)         (1.108)         (0.722)         (1.008)         (0.992)         (0.696)   \n                  [0.011]         [0.012]         [0.000]         [0.005]         [0.005]         [0.000]   \n\n_cons              10.797          13.184          25.712          10.797          13.184          25.712   \n                 (24.145)        (23.656)        (17.120)        (25.401)        (25.478)        (41.539)   \n                  [0.655]         [0.577]         [0.134]         [0.671]         [0.605]         [0.536]   \n------------------------------------------------------------------------------------------------------------\nN                     807             807             807             807             807             807   \n------------------------------------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/6_Heteros.html#lpm-revised",
    "href": "rmethods/6_Heteros.html#lpm-revised",
    "title": "Multiple Regression Analysis",
    "section": "LPM revised",
    "text": "LPM revised\n\nWhat was wrong with LPM?\n\nFixed marginal effects (depends on functional form)\nMay predict p&gt;1 or p&lt;0\nIt is Heteroskedastic by construction\n\nBut now we know how to deal with this! GLS (why not FGLS) and Robust\nIn LPM: \\(Var(y|x)=p(x)(1-p(x)) = \\hat y (1-\\hat y)\\)\n\nWe can use this to transform or weight the data!\nOnly works if \\(0&lt;p(x)&lt;1\\)."
  },
  {
    "objectID": "rmethods/6_Heteros.html#lpm-example",
    "href": "rmethods/6_Heteros.html#lpm-example",
    "title": "Multiple Regression Analysis",
    "section": "LPM Example",
    "text": "LPM Example\n\nfrause gpa1, clear\n** LPM\ngen parcoll = (fathcoll | mothcoll)\nreg pc hsgpa act parcoll\npredict res_1, res\n\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =      1.98\n       Model |  1.40186813         3  .467289377   Prob &gt; F        =    0.1201\n    Residual |  32.3569971       137  .236182461   R-squared       =    0.0415\n-------------+----------------------------------   Adj R-squared   =    0.0205\n       Total |  33.7588652       140  .241134752   Root MSE        =    .48599\n\n------------------------------------------------------------------------------\n          pc | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .0653943   .1372576     0.48   0.635    -.2060231    .3368118\n         act |   .0005645   .0154967     0.04   0.971    -.0300792    .0312082\n     parcoll |   .2210541    .092957     2.38   0.019      .037238    .4048702\n       _cons |  -.0004322   .4905358    -0.00   0.999     -.970433    .9695686\n------------------------------------------------------------------------------\n\n\n\npredict pchat\ngen hx = pchat*(1-pchat)\nsum pchat hx\n\n(option xb assumed; fitted values)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       pchat |        141    .3971631    .1000667   .1700624   .4974409\n          hx |        141    .2294822    .0309768   .1411412   .2499934\n\n\n\nreg pc hsgpa act parcoll [w=1/hx]\npredict res_2, res\n\n(analytic weights assumed)\n(sum of wgt is 628.1830743667746)\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =      2.22\n       Model |  1.54663033         3  .515543445   Prob &gt; F        =    0.0882\n    Residual |  31.7573194       137  .231805251   R-squared       =    0.0464\n-------------+----------------------------------   Adj R-squared   =    0.0256\n       Total |  33.3039497       140  .237885355   Root MSE        =    .48146\n\n------------------------------------------------------------------------------\n          pc | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .0327029   .1298817     0.25   0.802    -.2241292     .289535\n         act |    .004272   .0154527     0.28   0.783    -.0262847    .0348286\n     parcoll |   .2151862   .0862918     2.49   0.014       .04455    .3858224\n       _cons |   .0262099   .4766498     0.05   0.956    -.9163323    .9687521\n------------------------------------------------------------------------------\n\n\n\n** Testing for Heteroskedasticity\nreplace res_1 = res_1^2\nreplace res_2 = res_2^2/hx\ndisplay \"Default\"\nreg res_1 hsgpa act parcoll, notable\ndisplay \"Weighted\"\nreg res_2 hsgpa act parcoll, notable\n\n(141 real changes made)\n(141 real changes made)\nDefault\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =      2.82\n       Model |  .133163365         3  .044387788   Prob &gt; F        =    0.0412\n    Residual |  2.15497574       137   .01572975   R-squared       =    0.0582\n-------------+----------------------------------   Adj R-squared   =    0.0376\n       Total |  2.28813911       140  .016343851   Root MSE        =    .12542\n\nWeighted\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =      0.63\n       Model |  .874194807         3  .291398269   Prob &gt; F        =    0.5980\n    Residual |  63.5472068       137  .463848225   R-squared       =    0.0136\n-------------+----------------------------------   Adj R-squared   =   -0.0080\n       Total |  64.4214016       140  .460152868   Root MSE        =    .68106"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html",
    "href": "rmethods/4_MLRM_IA.html",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Last time, we talk a bit about the estimation of MLRM. For those who do not remember:\n\n\\[\\hat\\beta=(X'X)^{-1}X'y\n\\]\n\nWe also defined how, under A5 (homoskedasticity), we can estimate the variance covariance of coefficients:\n\n\\[Var(\\beta) = \\frac{\\sum \\hat e^2}{N-K-1} (X'X)^{-1}\n\\]\n\nThe next question: how to know how precise your estimates are?\nThat should be simple, just divide coefficient by its Standard error. The larger this is, the more precise, and more significant.\nIs this enough to say something about the population coefficients?\n\n(lets assume A1-A5 holds)\n\n\n\n\nThe right answer is…Perhaps.\nUnless you know something about the distribution of \\(\\beta's\\), it would be hard to make any inferences from the estimates. Why?\nBecause not all distributions are made equal!\n\n\n\nCode\nclear\nrange x -4 4 1000\ngen funiform = 0 \nreplace funiform = 1/(2*sqrt(3)) if inrange(x,-sqrt(3),sqrt(3))\n\ngen fnormal = 0 \nreplace fnormal = normalden(x)\n\ngen fchi2 = 0 \nreplace fchi2 = sqrt(8)*chi2den(4,x*sqrt(8)+4)\n\ninteg funiform x, gen(F1)\ninteg fnormal x, gen(F2)\ninteg fchi2 x, gen(F3)\n\nset scheme white2\ncolor_style egypt\nreplace x = x + 1.5\ntwo (area funiform x           , pstyle(p1) color(%20)) ///\n    (area funiform x if F1&lt;0.05, pstyle(p1) color(%80)) /// \n    (area funiform x if F1&gt;0.95, pstyle(p1) color(%80)) /// \n    (area fnormal  x           , pstyle(p2) color(%20)) ///  \n    (area fnormal  x if F2&lt;0.05, pstyle(p2) color(%80)) /// \n    (area fnormal  x if F2&gt;0.95, pstyle(p2) color(%80)) /// \n    (area fchi2    x           , pstyle(p3) color(%20)) /// \n    (area fchi2    x if F3&gt;0.95, pstyle(p3) color(%80)) /// \n    (area fchi2    x if F3&lt;0.05, pstyle(p3) color(%80)), ///\n    xlabel(-4 / 4) legend(order(2 \"Uniform\" 5 \"Normal\" 8 \"C-Chi2\")) /// \n    xtitle(\"Beta hat Distribution\") ///\n    xline( 0, lstyle(1) lwidth(1)) xline(1.5)\n\ngraph export images/f4_1.png, replace width(1200)  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA6: Errors are normal \\(e\\sim N(0,\\sigma^2_e)\\).\n\nA1-A6 are the Classical Linear Model Assumption\nThis assumes the outcome is “conditionally” normal. \\(y|X \\sim N(X\\beta,\\sigma^2_e)\\)\nAnd with this assumption OLS is no longer blue. Its now BUE!\n\n\n\n\n\n\n\n\nIf you combine two variables with the same distributions, the combined variable will not have the same distributions as the “parents”\nExcept with normals! if you add two -normal- distributions together. The outcome will also be normal. (Dont believe me try it)\n\nRecall:\n\\[\\hat \\beta=\\beta + (X'X)^{-1}X'e\n\\]\nIf \\(e\\) is normal, then \\(\\beta's\\) will also be normal\nAnd this works for ANY Sample size!\n\n\n\n\n\n\n\nIf \\(\\hat \\beta's\\) are normal, then we can use this distribution to make inferences about \\(\\beta's\\) using normal distribution.\nThis is good, because we know how to do math with Normal distributions. And can used the modified Ratio:\n\n\\[z_j = \\frac{\\hat \\beta_j - \\beta_j}{sd(\\hat\\beta)}\\sim N(0,1)\n\\]\n\nWhere \\(\\beta_j\\) is what you think the True Population parameter is (your hypothesis), and \\(\\hat\\beta_j\\) is what you estimate in your data.\nDepending on the size of this, you can either reject your hypothesis, or not Reject it.\n\nbut do we “know” \\(sd(\\beta)\\)?\n\n\n\n\n\n\nWe don’t, which is why we can use a normal directly. Instead we use a t-distribution, which uses \\(se(\\hat\\beta )\\)\n\\[t_j = \\frac{\\hat \\beta_j - \\beta_j}{se(\\hat\\beta)}\\sim t_{N-k-1}\n\\]\nThen\n\nIf \\(e\\) is normal, \\(\\beta\\) will be normal.\nWhen Samples are “small” Standardized \\(\\beta\\) will follow a t-distribution\nBut, as \\(N\\rightarrow \\infty\\), \\(t_{N-k-1}\\sim N(0,1)\\)\n\n\n\n\n\n\nThe idea of hypothesis testing is contrasting the “evidence” from your data (estimates) with the beliefs we have about the population.\n\n\\[y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + e\n\\]\nSay I have two hypothesis.\n\n\\(x_2\\) has no effect on \\(y\\). ie \\(H_0: \\beta_2 = 0\\)\n\\(x_1\\) has an effect equal to 1. ie \\(H_0: \\beta_1 = 1\\)\n\nNotice we make hypothesis about the population coefficients not the estimates\n\n\nI can “test” each hypothesis separately using a “t-statistic”\n\\[ \\color{green}{t_2=\\frac{\\hat \\beta_2 - 0}{se(\\hat \\beta_2)}} ;\nt_1=\\frac{\\hat \\beta_1 - 1}{se(\\hat \\beta_1)}\n\\]\n\n\n\nWhen talking about hypothesis testing there are two types:\n\nOne sided: when your alternative hypothesis compares your null to something either strictly larger, or strictly smaller than your hypothesis.\n\nEducation has no effect on wages vs Returns to education are positive.\nSkipping class has no effect on grades vs Skiping class reduces grades.\n\nTwo sided: When your alternative hypothesis is to say, “its different than”\n\nReturns to education is 10%, vs is not 10%\nSkipping class reduces grades in 0.5 points, vs not 0.5points\n\n\nIn both cases, you use the same t-statistic.\n\\[t_\\beta=\\frac{\\hat\\beta - \\beta_{hyp}}{se(\\hat \\beta)} \\sim t_{N-k-1}\n\\]\n\n\n\nWhat changes are the “thresholds” to Judge something significant or not.\n\n\n\\[\\begin{aligned}\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k&gt;\\beta^{hyp}_k \\\\\n& t_{\\beta_k}&gt;t_{N-k-1}(1-\\alpha) \\\\\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k&lt;\\beta^{hyp}_k \\\\\n& t_{\\beta_k}&lt;-t_{N-k-1}(1-\\alpha)\n\\end{aligned}\n\\]\n\nWhere \\(\\alpha\\) is your level of significance, and \\(t_{N-k-1}(1-\\alpha)\\) is the critical value.\n\\(\\alpha\\) determines the “risk” of commiting an error type I: Rejecting the Null when its true.\nIntuitively, the smaller \\(\\alpha\\) is, the more possitive (negative) “t” needs to be reject the Null.\n\n\n\n\n\n\n\n\\[\\begin{aligned}\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k \\neq \\beta^{hyp}_k \\\\\n& | t_{\\beta_k} | &gt;t_{N-k-1}(1-\\alpha/2)\n\\end{aligned}\n\\]\n\nSimilar to before, except the one needs to consider both tails of the distribution to determine critical values (see \\(t_{N-k-1}(1-\\alpha/2)\\))\nIntuitively, the smaller \\(\\alpha\\) is, the larger the absolute value of “t” needs to be reject the Null."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#how-do-you-know-if-what-you-see-is-relevant",
    "href": "rmethods/4_MLRM_IA.html#how-do-you-know-if-what-you-see-is-relevant",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Last time, we talk a bit about the estimation of MLRM. For those who do not remember:\n\n\\[\\hat\\beta=(X'X)^{-1}X'y\n\\]\n\nWe also defined how, under A5 (homoskedasticity), we can estimate the variance covariance of coefficients:\n\n\\[Var(\\beta) = \\frac{\\sum \\hat e^2}{N-K-1} (X'X)^{-1}\n\\]\n\nThe next question: how to know how precise your estimates are?\nThat should be simple, just divide coefficient by its Standard error. The larger this is, the more precise, and more significant.\nIs this enough to say something about the population coefficients?\n\n(lets assume A1-A5 holds)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#distribution-of-coefficients",
    "href": "rmethods/4_MLRM_IA.html#distribution-of-coefficients",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "The right answer is…Perhaps.\nUnless you know something about the distribution of \\(\\beta's\\), it would be hard to make any inferences from the estimates. Why?\nBecause not all distributions are made equal!\n\n\n\nCode\nclear\nrange x -4 4 1000\ngen funiform = 0 \nreplace funiform = 1/(2*sqrt(3)) if inrange(x,-sqrt(3),sqrt(3))\n\ngen fnormal = 0 \nreplace fnormal = normalden(x)\n\ngen fchi2 = 0 \nreplace fchi2 = sqrt(8)*chi2den(4,x*sqrt(8)+4)\n\ninteg funiform x, gen(F1)\ninteg fnormal x, gen(F2)\ninteg fchi2 x, gen(F3)\n\nset scheme white2\ncolor_style egypt\nreplace x = x + 1.5\ntwo (area funiform x           , pstyle(p1) color(%20)) ///\n    (area funiform x if F1&lt;0.05, pstyle(p1) color(%80)) /// \n    (area funiform x if F1&gt;0.95, pstyle(p1) color(%80)) /// \n    (area fnormal  x           , pstyle(p2) color(%20)) ///  \n    (area fnormal  x if F2&lt;0.05, pstyle(p2) color(%80)) /// \n    (area fnormal  x if F2&gt;0.95, pstyle(p2) color(%80)) /// \n    (area fchi2    x           , pstyle(p3) color(%20)) /// \n    (area fchi2    x if F3&gt;0.95, pstyle(p3) color(%80)) /// \n    (area fchi2    x if F3&lt;0.05, pstyle(p3) color(%80)), ///\n    xlabel(-4 / 4) legend(order(2 \"Uniform\" 5 \"Normal\" 8 \"C-Chi2\")) /// \n    xtitle(\"Beta hat Distribution\") ///\n    xline( 0, lstyle(1) lwidth(1)) xline(1.5)\n\ngraph export images/f4_1.png, replace width(1200)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#new-assumption",
    "href": "rmethods/4_MLRM_IA.html#new-assumption",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "A6: Errors are normal \\(e\\sim N(0,\\sigma^2_e)\\).\n\nA1-A6 are the Classical Linear Model Assumption\nThis assumes the outcome is “conditionally” normal. \\(y|X \\sim N(X\\beta,\\sigma^2_e)\\)\nAnd with this assumption OLS is no longer blue. Its now BUE!"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-1",
    "href": "rmethods/4_MLRM_IA.html#section-1",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "If you combine two variables with the same distributions, the combined variable will not have the same distributions as the “parents”\nExcept with normals! if you add two -normal- distributions together. The outcome will also be normal. (Dont believe me try it)\n\nRecall:\n\\[\\hat \\beta=\\beta + (X'X)^{-1}X'e\n\\]\nIf \\(e\\) is normal, then \\(\\beta's\\) will also be normal\nAnd this works for ANY Sample size!"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-2",
    "href": "rmethods/4_MLRM_IA.html#section-2",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "If \\(\\hat \\beta's\\) are normal, then we can use this distribution to make inferences about \\(\\beta's\\) using normal distribution.\nThis is good, because we know how to do math with Normal distributions. And can used the modified Ratio:\n\n\\[z_j = \\frac{\\hat \\beta_j - \\beta_j}{sd(\\hat\\beta)}\\sim N(0,1)\n\\]\n\nWhere \\(\\beta_j\\) is what you think the True Population parameter is (your hypothesis), and \\(\\hat\\beta_j\\) is what you estimate in your data.\nDepending on the size of this, you can either reject your hypothesis, or not Reject it.\n\nbut do we “know” \\(sd(\\beta)\\)?"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-3",
    "href": "rmethods/4_MLRM_IA.html#section-3",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "We don’t, which is why we can use a normal directly. Instead we use a t-distribution, which uses \\(se(\\hat\\beta )\\)\n\\[t_j = \\frac{\\hat \\beta_j - \\beta_j}{se(\\hat\\beta)}\\sim t_{N-k-1}\n\\]\nThen\n\nIf \\(e\\) is normal, \\(\\beta\\) will be normal.\nWhen Samples are “small” Standardized \\(\\beta\\) will follow a t-distribution\nBut, as \\(N\\rightarrow \\infty\\), \\(t_{N-k-1}\\sim N(0,1)\\)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#testing-hypothesis",
    "href": "rmethods/4_MLRM_IA.html#testing-hypothesis",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "The idea of hypothesis testing is contrasting the “evidence” from your data (estimates) with the beliefs we have about the population.\n\n\\[y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + e\n\\]\nSay I have two hypothesis.\n\n\\(x_2\\) has no effect on \\(y\\). ie \\(H_0: \\beta_2 = 0\\)\n\\(x_1\\) has an effect equal to 1. ie \\(H_0: \\beta_1 = 1\\)\n\nNotice we make hypothesis about the population coefficients not the estimates\n\n\nI can “test” each hypothesis separately using a “t-statistic”\n\\[ \\color{green}{t_2=\\frac{\\hat \\beta_2 - 0}{se(\\hat \\beta_2)}} ;\nt_1=\\frac{\\hat \\beta_1 - 1}{se(\\hat \\beta_1)}\n\\]"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#types-of-hypothesis",
    "href": "rmethods/4_MLRM_IA.html#types-of-hypothesis",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "When talking about hypothesis testing there are two types:\n\nOne sided: when your alternative hypothesis compares your null to something either strictly larger, or strictly smaller than your hypothesis.\n\nEducation has no effect on wages vs Returns to education are positive.\nSkipping class has no effect on grades vs Skiping class reduces grades.\n\nTwo sided: When your alternative hypothesis is to say, “its different than”\n\nReturns to education is 10%, vs is not 10%\nSkipping class reduces grades in 0.5 points, vs not 0.5points\n\n\nIn both cases, you use the same t-statistic.\n\\[t_\\beta=\\frac{\\hat\\beta - \\beta_{hyp}}{se(\\hat \\beta)} \\sim t_{N-k-1}\n\\]"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-4",
    "href": "rmethods/4_MLRM_IA.html#section-4",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "What changes are the “thresholds” to Judge something significant or not.\n\n\n\\[\\begin{aligned}\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k&gt;\\beta^{hyp}_k \\\\\n& t_{\\beta_k}&gt;t_{N-k-1}(1-\\alpha) \\\\\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k&lt;\\beta^{hyp}_k \\\\\n& t_{\\beta_k}&lt;-t_{N-k-1}(1-\\alpha)\n\\end{aligned}\n\\]\n\nWhere \\(\\alpha\\) is your level of significance, and \\(t_{N-k-1}(1-\\alpha)\\) is the critical value.\n\\(\\alpha\\) determines the “risk” of commiting an error type I: Rejecting the Null when its true.\nIntuitively, the smaller \\(\\alpha\\) is, the more possitive (negative) “t” needs to be reject the Null."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-5",
    "href": "rmethods/4_MLRM_IA.html#section-5",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "\\[\\begin{aligned}\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k \\neq \\beta^{hyp}_k \\\\\n& | t_{\\beta_k} | &gt;t_{N-k-1}(1-\\alpha/2)\n\\end{aligned}\n\\]\n\nSimilar to before, except the one needs to consider both tails of the distribution to determine critical values (see \\(t_{N-k-1}(1-\\alpha/2)\\))\nIntuitively, the smaller \\(\\alpha\\) is, the larger the absolute value of “t” needs to be reject the Null."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#why-we-never-accept",
    "href": "rmethods/4_MLRM_IA.html#why-we-never-accept",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Why we never accept?:",
    "text": "Why we never accept?:\n\nAs stated few times before, \\(\\hat \\beta\\) are just approximations to the true \\(\\beta\\) coefficients. Its the “evidence” you have based on the data available.\nWith this evidence, you can reject some hypothesis. (Some more strongly than others)\nHowever, there could exists many scenarios that would fit the evidence."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-7",
    "href": "rmethods/4_MLRM_IA.html#section-7",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Code\nclear\nrange x -5 5 1000\ngen fx = normalden(x) \nset scheme white2\ncolor_style tableau\ngen xx = x+1\ntwo (area fx x , pstyle(p1) color(%20)) ///\n    (area fx x if x&lt;invnormal(.025), pstyle(p1) color(%80) ) ///\n    (area fx x if x&gt;invnormal(.975), pstyle(p1) color(%80) ) ///\n    (area fx xx , pstyle(p2) color(%20)) ///\n    (area fx xx if x&lt;invnormal(.025), pstyle(p2) color(%80) ) /// \n    (area fx xx if x&gt;invnormal(.975), pstyle(p2) color(%80) ) ///\n    , xline(1.8) legend(order(1 \"H0: b=0\" 4 \"H0: b=1\"))  ///\n    xlabel(-4(2)4)  ylabel(0(.1).5)  xsize(8) ysize(4)\ngraph export images/f4_2.png, height(1000)  replace"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#what-about-type-error-i-and-ii",
    "href": "rmethods/4_MLRM_IA.html#what-about-type-error-i-and-ii",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "What about Type error I and II?",
    "text": "What about Type error I and II?\n\n\nBecause we do not know the truth, we are bound to commit errors in our assessment of the data.\nSo given the data evidence and the hypothesis, there could be 2 scenarios:\n\nGOOD: You either reject when \\(H_0\\) is false, or not reject when \\(H_0\\) is true.\n\\(TE-I\\): You reject \\(H_0\\) when it is true,\n\\(TE-II\\): Not reject \\(H_0\\) when it is false (Something else was true)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-8",
    "href": "rmethods/4_MLRM_IA.html#section-8",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Code\nclear\nrange x -5 5 1000\ngen fx = normalden(x) \n\ngen xxx=x+3 \ntwo (area fx x , pstyle(p1) color(%20)) ///\n    (area fx xxx, pstyle(p2) color(%20)) ///\n    (area fx x if x&gt;2, pstyle(p1) color(%80)) ///\n    (area fx xxx if xxx &lt;2, pstyle(p2) color(%80))  ///\n    ,legend(order(1 \"H0\"3 \"Type I \" 2 \"H1\"  4 \"Type II \") cols(2))   ///\n    xlabel(-4(2)7)  ylabel(0(.1).5) xsize(8) ysize(4)  \ngraph export images/f4_3.png, height(1000)  replace"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#example-determinants-of-college-gpa",
    "href": "rmethods/4_MLRM_IA.html#example-determinants-of-college-gpa",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Example: Determinants of College GPA",
    "text": "Example: Determinants of College GPA\n\nfrause gpa1, clear\nreg colgpa hsgpa act skipped\n\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =     13.92\n       Model |  4.53313314         3  1.51104438   Prob &gt; F        =    0.0000\n    Residual |  14.8729663       137  .108561798   R-squared       =    0.2336\n-------------+----------------------------------   Adj R-squared   =    0.2168\n       Total |  19.4060994       140  .138614996   Root MSE        =    .32949\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .4118162   .0936742     4.40   0.000     .2265819    .5970505\n         act |   .0147202   .0105649     1.39   0.166    -.0061711    .0356115\n     skipped |  -.0831131   .0259985    -3.20   0.002    -.1345234   -.0317028\n       _cons |   1.389554   .3315535     4.19   0.000     .7339295    2.045178\n------------------------------------------------------------------------------\n\n\n\nHypothesis: Skipping classes has no effect on College GPA.\n\n\\[H_0: \\beta_{skip} = 0 \\text{ vs } H_1: \\beta_{skip} \\neq 0\n\\]\n\nTest, \\(a=95%\\), \\(|t_{skip}|=3.2\\) vs \\(t_{n-k-1}(0.975)\\):\n\ndisplay invt(141-4,0.975)\n1.9774312\n\nConclusion: \\(H_0\\) is rejected."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-9",
    "href": "rmethods/4_MLRM_IA.html#section-9",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Hyp: Skipping college has no effect on College GPA vs has a negative effect\n\n\\[H_0: \\beta_{skip} = 0 \\text{ vs } H_1: \\beta_{skip}&lt;0\n\\]\n\nTest, \\(a=95\\%\\), \\(|t_{skip}|=3.2\\) vs \\(t_{n-k-1}(0.95)=1.6560\\)\nAlso Reject \\(H_0\\)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-10",
    "href": "rmethods/4_MLRM_IA.html#section-10",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "\\(t_{ACT}=1.39\\)\nHyp: ACT has no effect on College GPA vs It has a non-zero effect\nHyp: ACT has no effect on College GPA vs it has a positive effect\nCritical:\n\n\\(t_{137}(0.95)=1.6560\\) Donot Reject\\(H_0\\) with \\(\\alpha = 5\\%\\)\n\\(t_{137}(0.90)=1.2878\\) Reject \\(H_0\\) with \\(\\alpha = 10\\%\\)\n\nEach GPA point in highschool translates into half a point in College GPA. vs Is less than .5\n\n\ntest hsgpa = 0.5\nlincom hsgpa - 0.5\n\n\n ( 1)  hsgpa = .5\n\n       F(  1,   137) =    0.89\n            Prob &gt; F =    0.3482\n\n ( 1)  hsgpa = .5\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |  -.0881838   .0936742    -0.94   0.348    -.2734181    .0970505\n------------------------------------------------------------------------------\n\n\n\nCritical at 5%: \\(t=-1.6560\\)\nCannot Reject \\(H_0\\)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#p-values",
    "href": "rmethods/4_MLRM_IA.html#p-values",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "p-values",
    "text": "p-values\n\nSomething you may or may not have noticed. The significance level \\(\\alpha\\) can be choosen by the researcher.\n\nConventional levels are 10%, 5% and 1%.\n\nThis may lead to researchers choosing any value that would make their theory fit.\n\nThere is a better alternative. Using \\(p-values\\) to capture the smallest significance level that you could use to reject your Null.\n\n\\[p-value = P(|t|&gt;|t-stat|) \\text{ or } p-value = 2*P(|t|&gt;|t-stat|)\n\\]\n\nThe smallest the better! (for rejection)\nHow?\n\nOne tail : display 1-t(df = n-k-1, |t-stat|)\ntwo tails: display 2-2*t(df = n-k-1, |t-stat|)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-11",
    "href": "rmethods/4_MLRM_IA.html#section-11",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "clear\nrange x -5 5 1000\ngen fx = normalden(x) \nlocal p1:display %5.1f 100*2*(1-normal(1.5)) \nlocal p2:display %5.1f 100*2*(1-normal(2.5)) \ntwo (area fx x , pstyle(p1) color(%20)) ///\n    (area fx x if x&lt;-1.5, pstyle(p3) color(%80) ) ///\n    (area fx x if x&gt;+1.5, pstyle(p3) color(%80) ) ///\n    (area fx x if x&lt;invnormal(.025), pstyle(p1) color(%80) ) ///\n    (area fx x if x&gt;invnormal(.975), pstyle(p1) color(%80) ) ///\n    (area fx x if x&lt;-2.5, pstyle(p2) color(%80) ) ///\n    (area fx x if x&gt;+2.5, pstyle(p2) color(%80) ) , ///\n    xline(-1.5 2.5) legend(order(4 \"{&alpha}=5%\" 6 \"p-value = `p2'%\" 2 \"p-value = `p1'%\")) ///\n    ylabel(0(.1).5) xlabel(-5 0 5 -1.5 2.5)\ngraph export images/f4_4.png, replace width(1200)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#note-on-statistical-significance",
    "href": "rmethods/4_MLRM_IA.html#note-on-statistical-significance",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Note on Statistical Significance",
    "text": "Note on Statistical Significance\n\nStatistically significant doesnt mean meaninful. And lack of it, doesnt mean is not important\n\nKeep in mind that SE may be larger or smaller due to other factors (N or Mcollinearity)\n\nBe careful of discussing the effect size. (a 1US increase in min wage is different from 1chp in min Wage)\nIf non-significant, pay attention to the magnitude and relevance for your research. Does it have the correct sign?\nIncorrect signs with significant results. Either there is something wrong, or you found something interesting."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#confidence-intervals",
    "href": "rmethods/4_MLRM_IA.html#confidence-intervals",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nThis is the third approach to assess how precise or significant an estimate is. You provide a Range of possible values, given the level of coverage, and SE.\n\n\\[CI(\\beta_i) = [\\hat \\beta_i - \\hat \\sigma_{\\beta_i} t_{n-k-1}(1-\\alpha),\\hat \\beta_i +\\hat \\sigma_{\\beta_i} t_{n-k-1}(1-\\alpha)]\n\\]\n\nInterpretation:\n\nIf we were to draw M samples, the true beta would be in this interval \\(1-\\alpha\\%\\) of the time.\n\nIt allows you to see what other “hypothesis” would be consistent with the evidence of the estimate (you wouldnt be able to reject the Null)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#ci-vs-t-critical-and-p-values",
    "href": "rmethods/4_MLRM_IA.html#ci-vs-t-critical-and-p-values",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "CI vs T-critical and P values",
    "text": "CI vs T-critical and P values\n\nt-stat and p-values are calculated based on standardized coefficients (ratio of coefficient and SE)\nCI are calculated based on the actual coefficient and SE.\n\nIf the p-value of a t-statistic is exactly 0.05, then the 95% CI will not include 0 (at the limit), and the t-critical (\\(\\alpha=5\\%\\)) will be the same as the t-statistic.\nIn other words. If you use the same \\(\\alpha\\), your conclusions would be the same regardless of using t-stat, p-value or CI."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-13",
    "href": "rmethods/4_MLRM_IA.html#section-13",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "clear\nrange x -5 5 1000\ngen fx = normalden(x) \nlocal p1:display %5.1f 100*2*(1-normal(1.5)) \nlocal p2:display %5.1f 100*2*(1-normal(2.5)) \ngen xx = x+2\n\ntwo (area fx x , pstyle(p1) color(%10)) ///\n    (area fx x if x&lt;invnormal(.025), pstyle(p1) color(%60) ) ///\n    (area fx x if x&gt;invnormal(.975), pstyle(p1) color(%60) ) ///\n    (area fx xx , color(gs1%10) ) ///\n    (area fx xx if x&lt;invnormal(.005),  color(gs1%80) ) ///\n    (area fx xx if x&gt;invnormal(.995),  color(gs1%80) ) ///\n    (area fx xx if x&lt;invnormal(.025),  color(gs1%60) ) ///\n    (area fx xx if x&gt;invnormal(.975),  color(gs1%60) ) ///\n    (area fx xx if x&lt;invnormal(.05),  color(gs1%40) ) ///\n    (area fx xx if x&gt;invnormal(.95),  color(gs1%40) ), ///\n    xline(2) legend(order(5 \"CI-1%\" 7 \"CI-5%\" 9 \"CI-10%\")) ///\n    ylabel(0(.1).5) xlabel(-5 0 5 )  \ngraph export images/f4_5.png, replace width(1000)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#testing-linear-combinations",
    "href": "rmethods/4_MLRM_IA.html#testing-linear-combinations",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Testing Linear Combinations:",
    "text": "Testing Linear Combinations:\n\nYou may be interested in testing particular linear combinations of coefficients:\n\n\\(b_1 - b_2 =0 ; b_2+b_3=1 ; 2*b_4-b_5=b_6\\)\n\nDoing this is “simple”. Because is a single linear combination, you can still use “t-stat”.\n\n\\(t-stat = \\frac{2*\\hat b_4 -\\hat b_5 -\\hat b_6}{se(2*\\hat b_4 -\\hat b_5 -\\hat b_6)}\\)\n\nJust need SE for combined coefficients (requires knowing Variances and Covariances)\nEasy way, you could use Stata:\n\nreg y x1 x2 x3 x4 x5 x6\nlincom x1-x2 or lincom 2*x4-x5-x6\ntest (x1-x2=0) (x2+x3=1) (2*x4-x5=x6), mtest"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-14",
    "href": "rmethods/4_MLRM_IA.html#section-14",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Harder Way: (if you dare)\nMatrix Multiplication\nAssume Constant is the last coefficient: \\[V( 2*b_4-b_5- b_6) = R' V R ; R = [0,0,0,2,-1,-1]\n\\]\nwhere R are the restrictions, and V is the variance covariance matrix of \\(\\beta's\\).\nThen your t-stat\n\\[t-stat = \\frac{2*b_4-b_5- b_6}{\\sqrt{V(2*b_4-b_5- b_6)}}\n\\]"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-15",
    "href": "rmethods/4_MLRM_IA.html#section-15",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Alternative: Substitution\n\nOne can manipulate the regression model to consider a model with the contrained coefficient.\nOnce model is estimated, it simplifies testing:\n\n\\[\\begin{aligned}\n& y = b_0 + b_1 x_1 + b_2  x_2 + b_3 x_3 + e  \\\\\nh0: & b_1 - 2b_2 +b_3=0 \\rightarrow \\theta = b_1 - 2b_2 +b_3 \\rightarrow b_1 = \\theta + 2b_2 - b_3 \\\\\n& y = b_0 + ( \\theta + 2b_2 - b_3) x_1 + b_2 x_2 + b_3 x_3 + e \\\\\n& y = b_0 +  \\theta x_1 + b_2( x_2 +2 x_1) + b_3 (x_3-x_1) + e \\\\\n& y = b_0 +  \\theta x_1 + b_2 \\tilde x_2 + b_3 \\tilde x_3 + e \\\\\n\\end{aligned}\n\\]\nHere testing for \\(\\theta=0\\) is the same as testing for \\(b_1 - 2b_2 +b_3\\) in the original model.\n\n\n\n\n\n\nNote\n\n\n\nAlways ask something like this in Midterm, so brush up your math."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#testing-multiple-restrictions",
    "href": "rmethods/4_MLRM_IA.html#testing-multiple-restrictions",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Testing Multiple Restrictions",
    "text": "Testing Multiple Restrictions\nWhat if you are interested in testing multiple restrictions:\n\\[\\begin{aligned}\ny &= b_0 + b_1 x_1 + b_2 x_2 +b_3 x_3 + e \\\\\n& H_0: b_1 = 0 ; b_2 - b_3 =0 \\\\\n& H_1: H_0 \\text{ is false}\n\\end{aligned}\n\\]\nEasy way: Stata command test allows you to do this\nOtherwise, you can do it by hand:"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-16",
    "href": "rmethods/4_MLRM_IA.html#section-16",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Estimate unrestricted model (original) and “save” \\(SSR_{ur}\\) or \\(R_{ur}^2\\)\nImpose restrictions on the model and “save” \\(SSR_r\\) or \\(R_{r}^2\\)\nEstimate F-stat:\n\n\\[F_{q,n-k-1} = \\frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(n-k-1)}  \\text{ or }\n\\frac{(R^2_{ur}-R^2_r)/q}{(1-R^2_{ur})/(n-k-1)} \\sim F(q,n-k-1)\n\\]\n\\(SSR\\) Sum of Squared Residuals, \\(q\\) number of restrictions\n\nIdea, you are comparing how the overall fitness of the model changes with restrictions.\nIf restrictions slightly decreases the model Fitness, you cannot be rejected them.\nOtherwise, They are rejected! (you just dont know which)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#overall-model-significance",
    "href": "rmethods/4_MLRM_IA.html#overall-model-significance",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Overall Model Significance",
    "text": "Overall Model Significance\nOne test, we often don’t do anymore, is testing the overall fitness of a model:\n\\[H_0: x_1, x_2, \\dots , x_k \\text{ do not explain y}\n\\]\n\\[H_0: \\beta_1=\\beta_2=\\dots=\\beta_k =0\n\\]\nWhere we kind of suggest that a model with only an intercept is better than the one with covariates.\n\\[F_{q,n-k-1} = \\frac{(R^2_{ur}-\\color{red}{R^2_r})/q}{(1-R^2_{ur})/(n-k-1)} \\sim F(q,n-k-1)\n\\]\nIn this case \\(\\color{red}{R^2_r}=0\\)"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#not-for-the-faint-for-heart",
    "href": "rmethods/4_MLRM_IA.html#not-for-the-faint-for-heart",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Not For the faint for heart",
    "text": "Not For the faint for heart\nMatrix form for F-Stat! Restrictions:\n\\[H_0: R_{q,k+1}\\beta_{k+1,1}=c_{q,1}\n\\]\nFirst. Define matrix with all Matrix Restriction \\[\n\\Sigma_R = R_{q,k+1} V_\\beta R'_{q,k+1}\n\\]\nSecond: F-statistic\n\\[\nF-stat = \\frac 1 q (R\\beta-c)' \\Sigma_R^{-1} (R\\beta-c)\n\\]"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#example",
    "href": "rmethods/4_MLRM_IA.html#example",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Example",
    "text": "Example\n\nfrause hprice1, clear\nreg lprice lasses bdrms llotsize lsqrft\n\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(4, 83)        =     70.58\n       Model |  6.19607473         4  1.54901868   Prob &gt; F        =    0.0000\n    Residual |  1.82152879        83   .02194613   R-squared       =    0.7728\n-------------+----------------------------------   Adj R-squared   =    0.7619\n       Total |  8.01760352        87  .092156362   Root MSE        =    .14814\n\n------------------------------------------------------------------------------\n      lprice | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     lassess |   1.043065    .151446     6.89   0.000     .7418453    1.344285\n       bdrms |   .0338392   .0220983     1.53   0.129    -.0101135    .0777918\n    llotsize |   .0074379   .0385615     0.19   0.848    -.0692593    .0841352\n      lsqrft |  -.1032384   .1384305    -0.75   0.458     -.378571    .1720942\n       _cons |    .263743   .5696647     0.46   0.645    -.8692972    1.396783\n------------------------------------------------------------------------------\n\n\n\ntest (lasses=1)\ntest (lasses=1) (bdrms=llotsize=lsqrft=0)\n\n\n ( 1)  lassess = 1\n\n       F(  1,    83) =    0.08\n            Prob &gt; F =    0.7768\n\n ( 1)  lassess = 1\n ( 2)  bdrms - llotsize = 0\n ( 3)  bdrms - lsqrft = 0\n ( 4)  bdrms = 0\n\n       F(  4,    83) =    0.67\n            Prob &gt; F =    0.6162\n\n\n\nfrause mlb1, clear\nreg lsalary years gamesyr bavg hrunsyr rbisy\n\n\n      Source |       SS           df       MS      Number of obs   =       353\n-------------+----------------------------------   F(5, 347)       =    117.06\n       Model |  308.989208         5  61.7978416   Prob &gt; F        =    0.0000\n    Residual |  183.186327       347  .527914487   R-squared       =    0.6278\n-------------+----------------------------------   Adj R-squared   =    0.6224\n       Total |  492.175535       352  1.39822595   Root MSE        =    .72658\n\n------------------------------------------------------------------------------\n     lsalary | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       years |   .0688626   .0121145     5.68   0.000     .0450355    .0926898\n     gamesyr |   .0125521   .0026468     4.74   0.000     .0073464    .0177578\n        bavg |   .0009786   .0011035     0.89   0.376    -.0011918     .003149\n     hrunsyr |   .0144295    .016057     0.90   0.369    -.0171518    .0460107\n      rbisyr |   .0107657    .007175     1.50   0.134    -.0033462    .0248776\n       _cons |   11.19242   .2888229    38.75   0.000     10.62435    11.76048\n------------------------------------------------------------------------------\n\n\n\ntest bavg hrunsyr rbisy\n\n\n ( 1)  bavg = 0\n ( 2)  hrunsyr = 0\n ( 3)  rbisyr = 0\n\n       F(  3,   347) =    9.55\n            Prob &gt; F =    0.0000"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#introduction",
    "href": "rmethods/4_MLRM_IA.html#introduction",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Introduction",
    "text": "Introduction\n\nWhen considering the topic of asymptotic theory, there are few concepts that are important ton consider.\n\nAsymtotics refer to properties of OLS when \\(N\\rightarrow \\infty\\)\nWhen samples grow, we are more concern about consistency rather than “just” unbiased estimators.\nWe are also concern with how flexible is the normality assumption when samples grow large."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#what-is-consistency",
    "href": "rmethods/4_MLRM_IA.html#what-is-consistency",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "What is consistency?",
    "text": "What is consistency?\n\nUp until now, we have been concerned with Unbiased estimates\n\n\\[E(\\hat\\beta)=\\beta\n\\]\n\nIn large samples, this is no longer enough. One requires Consistency!\n\nConsistency says that as \\(N\\rightarrow \\infty\\) then \\(plim \\hat \\beta = \\beta\\).\n\\(p(|\\hat \\beta - \\beta|&lt;\\varepsilon) = 1\\) or that The variance shrinks to zero, or we can estimate \\(\\beta\\) almost surely.\nThis is also known as asymptotic unbiasness.\n\nIn linear regression analysis, consistency can be achieved with a weaker A4’: \\(Cov(e,x)=0\\), assuming that we require only linear independence."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#consistency-vs-bias",
    "href": "rmethods/4_MLRM_IA.html#consistency-vs-bias",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Consistency vs Bias",
    "text": "Consistency vs Bias\n\nConsistent and UnbiasedConsistent and Biased"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#what-about-normality-assumption",
    "href": "rmethods/4_MLRM_IA.html#what-about-normality-assumption",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "What about Normality Assumption?",
    "text": "What about Normality Assumption?\n\nEverything we have seen so far was possible under the normality assumption of the errors.\n\nif \\(e\\) is normal, then \\(b\\) is normal (even in small samples), thus we can use \\(t\\), \\(F\\), etc\n\nBut what if this assumption fails? would we care?\n\n\n\nPerhaps. If your sample is small, \\(b\\) will not be normal, and standard procedures will not work.\nIn large Samples, however, \\(\\beta's\\) will be normal, even if \\(e\\) is not. Thanks to CLT"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#section-18",
    "href": "rmethods/4_MLRM_IA.html#section-18",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "",
    "text": "Good news\n\nBottom line, when \\(N\\) is large, you do not need \\(e\\) to be normal.\nif A1-A5 hold, you can rely on asymptotic normality!\nThus you can still use t’s and F’s, but you can also use LM"
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#lm-lagrange-multiplier",
    "href": "rmethods/4_MLRM_IA.html#lm-lagrange-multiplier",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "LM-Lagrange Multiplier",
    "text": "LM-Lagrange Multiplier\n\nWhile you can still use t-stat and F-stat to draw inference from your model, there is a better test (given the large sample): Lagrange Multiplier Statistic\nThe idea: Does impossing restrictions affect the model Fitness?\n\n\nRegress \\(y\\) on restricted \\(x_1,\\dots,x_{k-q}\\), and obtain \\(\\tilde e\\)\nRegress \\(\\tilde e\\) on all \\(x's\\), and obtain \\(R^2_e\\).\n\nIf the excluded regressors were not significant, the \\(R^2_e\\) should be very small.\n\nCompare \\(nR^2_e\\) with \\(\\chi^2(n,1-\\alpha)\\), and draw conclusions."
  },
  {
    "objectID": "rmethods/4_MLRM_IA.html#example-1",
    "href": "rmethods/4_MLRM_IA.html#example-1",
    "title": "Multiple Regression Analysis: Inference and Asymptotics",
    "section": "Example:",
    "text": "Example:\n\nfrause crime1, clear\nqui: reg narr86 pcnv avgsen tottime ptime86 qemp86\n** H0: avgsen=0 and tottime=0\ntest (avgsen=0) (tottime=0)\n\n\n ( 1)  avgsen = 0\n ( 2)  tottime = 0\n\n       F(  2,  2719) =    2.03\n            Prob &gt; F =    0.1310\n\n\n\nqui: reg narr86 pcnv                ptime86 qemp86\n* Predict residuals of constrained model\npredict u_tilde , res\n* regress residuals againts all variables\nreg u_tilde  pcnv avgsen tottime ptime86 qemp86\n\n\n      Source |       SS           df       MS      Number of obs   =     2,725\n-------------+----------------------------------   F(5, 2719)      =      0.81\n       Model |  2.87904835         5  .575809669   Prob &gt; F        =    0.5398\n    Residual |  1924.39392     2,719  .707757969   R-squared       =    0.0015\n-------------+----------------------------------   Adj R-squared   =   -0.0003\n       Total |  1927.27297     2,724  .707515773   Root MSE        =    .84128\n\n------------------------------------------------------------------------------\n     u_tilde | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        pcnv |  -.0012971    .040855    -0.03   0.975    -.0814072    .0788129\n      avgsen |  -.0070487   .0124122    -0.57   0.570     -.031387    .0172897\n     tottime |   .0120953   .0095768     1.26   0.207    -.0066833     .030874\n     ptime86 |  -.0048386   .0089166    -0.54   0.587    -.0223226    .0126454\n      qemp86 |   .0010221   .0103972     0.10   0.922    -.0193652    .0214093\n       _cons |  -.0057108   .0331524    -0.17   0.863    -.0707173    .0592956\n------------------------------------------------------------------------------\n\n\ndisplay \"Chi2(2)=\" `=e(N)*e(r2)'\ndisplay \"Its p-value=\" %5.4f `=1-chi2(2, `=e(N)*e(r2)' )'\nChi2(2)=4.0707294 Its p-value=0.1306\nTry making it a program if you “dare”"
  },
  {
    "objectID": "rmethods/2_SRA.html",
    "href": "rmethods/2_SRA.html",
    "title": "Simple Regression Model",
    "section": "",
    "text": "As we saw in the previous slides, one of the important steps when doing empirical analysis is to develop a model that describes reality.\nThis model is quite abstract, as it rarely provides guidance regarding on How should you build your econometric model.\nIn this chapter, we introduce the first (boring) tool to solve this problem. The simple Regression model or SRL"
  },
  {
    "objectID": "rmethods/2_SRA.html#the-simple-regression-model",
    "href": "rmethods/2_SRA.html#the-simple-regression-model",
    "title": "Simple Regression Model",
    "section": "",
    "text": "As we saw in the previous slides, one of the important steps when doing empirical analysis is to develop a model that describes reality.\nThis model is quite abstract, as it rarely provides guidance regarding on How should you build your econometric model.\nIn this chapter, we introduce the first (boring) tool to solve this problem. The simple Regression model or SRL"
  },
  {
    "objectID": "rmethods/2_SRA.html#what-is-a-simple-regression-model-srm",
    "href": "rmethods/2_SRA.html#what-is-a-simple-regression-model-srm",
    "title": "Simple Regression Model",
    "section": "What is a Simple Regression Model (SRM) ?",
    "text": "What is a Simple Regression Model (SRM) ?\n\nA Simple regression model is known as such because it aims to capture the relationship between two variables.\nIt does not mean it ignores other factors, but rather, bundles them together as part of a Bag of Holding or error. In its most flexible setup, a simple regression model can be written as:\n\n\\[y = f(x,u)\n\\]\nThis model simply says that there is some relationship between:\n\n\\(y\\), your outcome, dependent, explained, response, variable\nand \\(x\\), your independent, explanatory, regression, variable\n\nwhereas everything else not considered is assumed to be part of the unobserved \\(u\\)."
  },
  {
    "objectID": "rmethods/2_SRA.html#from-abstract-to-concrete",
    "href": "rmethods/2_SRA.html#from-abstract-to-concrete",
    "title": "Simple Regression Model",
    "section": "From Abstract to Concrete",
    "text": "From Abstract to Concrete\n\nA good reason why one should start thinking about the model as shown earlier is to acknowledge that we Do not know the functional form between \\(x\\) and \\(y\\).\nFurther, we don’t even know how \\(u\\) interacts with \\(x\\).\n\nThis brings us to the first step one should do (almost always) when analyzing data…Create a plot to see if there is any relationship in the data"
  },
  {
    "objectID": "rmethods/2_SRA.html#simple-scatter-1",
    "href": "rmethods/2_SRA.html#simple-scatter-1",
    "title": "Simple Regression Model",
    "section": "Simple Scatter 1",
    "text": "Simple Scatter 1\n\n** To download all Wooldrige Files\nqui: ssc install frause, replace\n** for some additional color schemes\nqui: ssc install color_style\nset scheme white2\ncolor_style tableau\n** Loads file wage1\nfrause wage1, clear\nscatter wage educ\n\n\n\n\n\n\n\n\n\nWe observe a positive relationship between Wages and years of education\nThis relationship does not seem to be linear"
  },
  {
    "objectID": "rmethods/2_SRA.html#simple-scatter-2",
    "href": "rmethods/2_SRA.html#simple-scatter-2",
    "title": "Simple Regression Model",
    "section": "Simple Scatter 2",
    "text": "Simple Scatter 2"
  },
  {
    "objectID": "rmethods/2_SRA.html#even-more-concrete",
    "href": "rmethods/2_SRA.html#even-more-concrete",
    "title": "Simple Regression Model",
    "section": "Even more Concrete",
    "text": "Even more Concrete\n\nThis first “model” provides little guidance for the modeling itself.\nThe Simple Linear Regression Model corrects for that, establishing a specific relationship between the variables of interest and the error: \\[y = \\beta_0 + \\beta_1 x + u\\]\n\nThis model has a lot packed in.\n\nIt imposes a relationship between \\(y\\) and \\(x\\) (linear)\nAnd addresses the fact that there could be other factors not considered \\(u\\). Impossing the assumption they are additive errors.\n\nIt also assumes the population relationships: \\[E(y|x) = \\beta_0 + \\beta_1 x\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#what-can-we-learn-from-it",
    "href": "rmethods/2_SRA.html#what-can-we-learn-from-it",
    "title": "Simple Regression Model",
    "section": "What can we learn from it?",
    "text": "What can we learn from it?\n\\[E(y|x) = \\beta_0 + \\beta_1 x\\]\nThis is your Population Regresson function. To interpret it, we need to assume \\(u\\) is fixed (ceteris paribus). This implies that \\[E(u|x)=c=0\\]\nWhich says that the errors are mean independent of \\(x\\). Thus, for all practical purposes, when \\(x\\) changes, we will assume \\(u\\) is as good as fixed.\nUnder these conditions, we can interpret the coefficients:\n\n\\(\\beta_0\\) is the constant, or expected outcome when \\(x=0\\).\n\\(\\beta_1\\) is the slope of \\(x\\), or the expected change in \\(y\\) when \\(x\\) changes in 1 unit:\n\n\\[\\Delta y = \\beta_1 \\Delta x \\rightarrow \\frac{\\Delta y}{\\Delta x} = \\beta_1\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#example",
    "href": "rmethods/2_SRA.html#example",
    "title": "Simple Regression Model",
    "section": "Example",
    "text": "Example\n\nSoybean and Yield Fertilizer:\n\n\\[yield = \\beta_0 + \\beta_1 fertilizer + u\\]\n\\(\\beta_1\\) Effect of Fertilizer (an additional dosage) on Soybean Yield\n\nSimple wage equation\n\n\\[wage = \\beta_0 + \\beta_1 educ + u\n\\]\n\\(\\beta_1\\) Change in wages given an additional year of education."
  },
  {
    "objectID": "rmethods/2_SRA.html#deriving-coefficients-ordinary-least-squares---ols",
    "href": "rmethods/2_SRA.html#deriving-coefficients-ordinary-least-squares---ols",
    "title": "Simple Regression Model",
    "section": "Deriving Coefficients: Ordinary Least Squares - OLS",
    "text": "Deriving Coefficients: Ordinary Least Squares - OLS\n\nThere are an infinite number of candiates for \\(\\beta_0 \\& \\beta_1\\).\nOLS, is one of the multiple methods that allows us to estimate the coefficients of a SLRM1.\nThe goal is to Choose parameters \\(\\beta={\\beta_0,\\beta_1}\\) that “minimizes” the Squared of the residuals.\n\nIn other words, OLS aims to maximize Explantion power by minimizing errors."
  },
  {
    "objectID": "rmethods/2_SRA.html#visualization",
    "href": "rmethods/2_SRA.html#visualization",
    "title": "Simple Regression Model",
    "section": "Visualization",
    "text": "Visualization\n\nset seed 10\nclear\nrange x -2 2 20\ngen y = 1 + x + rnormal()\ncolor_style tableau\ntwo (scatter  y x) ///\n    (function y = 0.5 + 2*x, range(-2 2)) ///\n    (function y = 2 + 0.5*x, range(-2 2)) ///\n    (function y = 1 + 1*x  , range(-2 2)) , ///\n    legend(order(2 \"y=0.5+2x\" 3 \"y=2+0.5x\" 4 \"y=1+1x\"))\ngraph export images/fig2_1.png , replace width(1000)\n\ngen y1=.5+2*x\ngen y2=2+0.5*x\ngen y3=+1+1*x\n\n\negen u21 = sum((y-y1)^2)\negen u22 = sum((y-y2)^2)\negen u23 = sum((y-y3)^2)\nreg y x\negen u24 = sum((y-_b[_cons] - _b[x]*x)^2)\nlabel var u21 \"SSR-model 1\"\nlabel var u22 \"SSR-model 2\"\nlabel var u23 \"SSR-model 3\"\nlabel var u24 \"SSR-model 4\"\n\ntwo (scatter y x) ///\n    (function y = 0.5 + 2*x, range(-2 2)) ///\n    (rspike y y1 x), ylabel(-4/6) ytitle(y) ///\n    legend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))\ngraph export images/fig2_2.png , replace width(1000) \n    \ntwo (scatter y x) ///\n    (function y = 2 + 0.5*x, range(-2 2)) ///\n    (rspike y y2 x), ylabel(-4/6) ytitle(y) ///\n    legend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))\ngraph export images/fig2_3.png , replace width(1000) \n    \ntwo (scatter y x) ///\n    (function y = 1 + 1*x, range(-2 2)) ///\n    (rspike y y3 x) , ylabel(-4/6) ytitle(y) ///\n    legend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))\ngraph export images/fig2_4.png , replace width(1000)    \n\nreg y x\npredict yh\ntwo (scatter y x) ///\n    (function y = _b[_cons] + _b[x]*x, range(-2 2)) ///\n    (rspike y yh x) , ylabel(-4/6) ytitle(y) ///\n    legend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))    \ngraph export images/fig2_5.png , replace width(1000)    \nclonevar u24x=u24\ngraph bar u24 u21 u22 u23 u24x , ///\n   legend(order( 2 \"SSR-Model 1\" 3 \"SSR-Model 2\" 4 \"SSR-Model 3\" 5 \"Sample Fit\") ///\n   ring(0) pos(2)) xsize(5) ysize(8) scale(1.5) bar(1, color(gs0%0))\n   \ngraph export images/fig2_5x.png , replace width(300)    \n\n\nOptionsOpt1Opt2Opt3Opt4"
  },
  {
    "objectID": "rmethods/2_SRA.html#just-a-minimization-problem",
    "href": "rmethods/2_SRA.html#just-a-minimization-problem",
    "title": "Simple Regression Model",
    "section": "Just a Minimization Problem",
    "text": "Just a Minimization Problem\n\\[y_i =\\beta_0 + \\beta_1 x_i + u_i \\rightarrow u_i = y_i - \\beta_0 - \\beta_1 x_i\n\\]\n\\[{\\hat\\beta_0,\\hat\\beta_1} = \\min_{\\beta_0,\\beta_1} = SSR =\\sum_{i=1}^N u_i^2 = \\sum_{i=1}^N (y-\\beta_0 - \\beta_1 x_i)^2 \\\\\n\\]\nFirst Order Conditions:\n\\[\n\\begin{aligned}\n\\frac{\\partial SSR}{\\partial \\beta_0} &= -2 \\sum (y_i-\\beta_0 - \\beta_1 x_i) = -2 \\sum u_i =0 \\\\\n\\frac{\\partial SSR}{\\partial \\beta_1} &= -2 \\sum x_i (y_i-\\beta_0 - \\beta_1 x_i) =- 2 \\sum x_i u_i =0\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#just-a-minimization-problem-1",
    "href": "rmethods/2_SRA.html#just-a-minimization-problem-1",
    "title": "Simple Regression Model",
    "section": "Just a Minimization Problem",
    "text": "Just a Minimization Problem\nSimilar conditions as before (but now Mathematically):\n\\[\\begin{aligned}\n\\sum u_i &=0 \\rightarrow nE(e) = 0 \\\\\n\\sum x_i u_i &=0 \\rightarrow nE(x*e) \\rightarrow  n Cov(x,e) =0  \n\\end{aligned}\n\\]\nAnd the First Order Conditions simply provide a system of \\(k+1\\) equations with \\(k+1\\) unknowns.\n\\[\\begin{aligned}\n\\hat\\beta_0 &= \\bar y - \\beta_1 \\bar x \\\\\n\\hat\\beta_1 &= \\frac{\\sum (x_i-\\bar x)(y_i-\\bar y)}{\\sum (x_i-\\bar x)^2}\n= \\frac{\\hat \\rho \\hat \\sigma_x \\hat \\sigma_y}{\\hat \\sigma_x^2}\n= \\frac{\\hat \\rho \\hat \\sigma_y}{\\hat \\sigma_y}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#interpretation",
    "href": "rmethods/2_SRA.html#interpretation",
    "title": "Simple Regression Model",
    "section": "Interpretation?",
    "text": "Interpretation?\n\\[\\hat\\beta_0 = \\bar y - \\hat \\beta_1 \\bar x\\]\n\n\\(\\beta_0\\) is usually estimated as a “residual”, thus is often of little of no interest.\n\nExpected outcome when \\(X=0\\)\n\n\n\\[\n\\hat\\beta_1 = \\frac {cov(x,y)}{var(x)}= \\hat \\rho\\frac{ \\hat \\sigma_x \\hat \\sigma_y}{\\hat \\sigma_x^2}\n= \\hat \\rho\\frac{ \\hat \\sigma_y}{\\hat \\sigma_x}\n\\]\n\n\\(\\beta_1\\) is a slope, which is directly related to the correlation between \\(y\\) and \\(x\\).\n\nIt can only be estimated if \\(\\sigma_x\\)&gt;&gt;0\n\n\nAlso, this \\(\\hat y = \\hat \\beta_0 + \\hat \\beta_1 x\\) becomes your sample regression function\n\nwhere \\(\\hat y\\) is the fitted value of \\(y\\) (proyection or prediction), given some value of \\(x\\)."
  },
  {
    "objectID": "rmethods/2_SRA.html#visualization-1",
    "href": "rmethods/2_SRA.html#visualization-1",
    "title": "Simple Regression Model",
    "section": "Visualization",
    "text": "Visualization\n\ngen y0 = 0\ntwo (scatter y x) ///\n    (function y = _b[_cons] + _b[x]*x, range(-2.1 2.1)) ///\n    (rspike y y0 x, color(gs9%50) lw(1)) ///\n    , ylabel(-4/6) ytitle(y) yline(0) ///\n    title(\"Data\")   legend(off)\ngraph export images/fig2_6.png, replace width(1000)\ntwo (scatter y x) ///\n    (function y = _b[_cons] + _b[x]*x, range(-2.1 2.1)) ///\n    (rspike yh y0 x, color(gs9%50) lw(1)) ///\n    , ylabel(-4/6) ytitle(y) yline(0) ///\n    title(\"Prediction\") legend(off)\ngraph export images/fig2_7.png, replace width(1000) \ntwo (scatter y x) ///\n    (function y = _b[_cons] + _b[x]*x, range(-2.1 2.1)) ///\n    (rspike y yh x, color(gs9%50) lw(1)) ///\n    , ylabel(-4/6) ytitle(y) yline(0) ///\n    title(\"Residual\")   legend(off)\ngraph export images/fig2_8.png, replace width(1000) \n\n\nDataPredictionResiduals"
  },
  {
    "objectID": "rmethods/2_SRA.html#properties-of-the-estimator",
    "href": "rmethods/2_SRA.html#properties-of-the-estimator",
    "title": "Simple Regression Model",
    "section": "Properties of the Estimator",
    "text": "Properties of the Estimator\n\nBased on F.O.C., we know the following:\n\n\\[\n\\sum_i^n \\hat u_i = 0 \\ \\& \\ \\sum_i^n x_i \\hat u_i = 0\n\\]\nIn average \\(u_i\\) is zero, and uncorrelated with \\(x\\), and \\(\\bar y , \\bar x\\) is on the regression line\n\nBy construction \\(y_i = \\hat y_i + \\hat u_i\\), so that\n\n\\[\n\\begin{aligned}\n    \\sum_{i=1}^n(y_i-\\bar y)^2 &=\n    \\sum_{i=1}^n(y_i-\\hat y)^2  +\n    \\sum_{i=1}^n(\\hat y-\\bar y)^2  \\\\\n    SST &= SSE + SSR\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#properties-of-the-estimator-1",
    "href": "rmethods/2_SRA.html#properties-of-the-estimator-1",
    "title": "Simple Regression Model",
    "section": "Properties of the Estimator",
    "text": "Properties of the Estimator\n\nGoodness of FIT is defined as\n\n\\[R^2= 1-\\frac {SSR} {SST}=\\frac {SSE} {SST}\\]\n\nHow much of the Data variation (SST) is explained by the model (SSE), or\n1-amount not explained by the model"
  },
  {
    "objectID": "rmethods/2_SRA.html#some-discussion",
    "href": "rmethods/2_SRA.html#some-discussion",
    "title": "Simple Regression Model",
    "section": "Some Discussion",
    "text": "Some Discussion\nWe now know how to estimate coefficients given some data, but we need to ask the questions:\n\nHow do we know if the estimated coefficients are indeed appropriate for the population parameters?\nHow can we know the precision (or lack there off) of the estimates\nRemember, \\(\\hat \\beta's\\) depend on the sample. Different Samples will lead to different estimates. Thus \\(\\hat \\beta's\\) are random.\nIn repeated sampling scenarios, we could empirically obtain the distribution of the estimated parameters, and verify if estimations are unbiased.\nHowever, we can also do that based on analytical solutions. Lets see those assumptions"
  },
  {
    "objectID": "rmethods/2_SRA.html#assumption-1",
    "href": "rmethods/2_SRA.html#assumption-1",
    "title": "Simple Regression Model",
    "section": "Assumption 1:",
    "text": "Assumption 1:\n\n\n\n\n\n\nLinear in Parameters\n\n\n\nWe need to assume the population model is linear in parameters:\n\\[y_i = \\beta_0 + \\beta_1 x_i + u_i\\]\n\n\nIn other words, we need to assume that the model we chose is a good representation of what the true population model is.\n\nAdditive error, with a linear relationship between \\(x_i\\) on \\(y_i\\).\nWe can make it more flexible using some transformations of \\(x_i\\)."
  },
  {
    "objectID": "rmethods/2_SRA.html#assumption-2",
    "href": "rmethods/2_SRA.html#assumption-2",
    "title": "Simple Regression Model",
    "section": "Assumption 2:",
    "text": "Assumption 2:\n\n\n\n\n\n\nRandom Sampling\n\n\n\nThe data we are using is collected from a Random sample of the population, for which the linear model is valid.\n\n\n\nData should be representative from the population (for whom the Linear model Holds)\nThe Data Sampling should not depend the data collected, specially the dependent variable.\nAlso helps to ensure units “unobservables” \\(u's\\) are independent from each other."
  },
  {
    "objectID": "rmethods/2_SRA.html#assumption-3",
    "href": "rmethods/2_SRA.html#assumption-3",
    "title": "Simple Regression Model",
    "section": "Assumption 3:",
    "text": "Assumption 3:\n\n\n\n\n\n\nThere is variation in the explanatory variable\n\n\n\n\\[\\sum_{i=1}^n (x_i - \\bar x)^2 &gt;0\n\\]\n\n\nIf there is no variation in the data, there are no slopes to estmate, and a solution cannot be found to the linear model."
  },
  {
    "objectID": "rmethods/2_SRA.html#assumption-4",
    "href": "rmethods/2_SRA.html#assumption-4",
    "title": "Simple Regression Model",
    "section": "Assumption 4:",
    "text": "Assumption 4:\n\n\n\n\n\n\nZero Conditional Mean\n\n\n\n\\[E(u_i)= E(u_i|x_i) = 0\n\\]\n\n\n\nWe expect unobserved factors \\(u_i\\) to have a zero average effect on the outcome. This helps identify the constant \\(\\beta_0\\).\nWe also expect that the expected value of \\(u_i\\) to be zero for any value of \\(x\\)."
  },
  {
    "objectID": "rmethods/2_SRA.html#unbiased-coefficients",
    "href": "rmethods/2_SRA.html#unbiased-coefficients",
    "title": "Simple Regression Model",
    "section": "Unbiased Coefficients:",
    "text": "Unbiased Coefficients:\nIf Assumptions 1-4 Hold, then OLS allows you to estimate the coefficents of the linear Regression model.\n\\[\\hat \\beta_1 = \\frac {\\sum \\tilde x_i \\tilde y_i}{\\sum \\tilde x_i^2} , \\tilde x_i=x_i - \\bar x\n\\]\n\\[\\begin{aligned}\n\\hat \\beta_1 &= \\frac {\\sum \\tilde x_i (\\beta_1 \\tilde x_i +e)}{\\sum \\tilde x_i^2} = \\beta_1 \\frac {\\sum  \\tilde x_i^2 }{\\sum \\tilde x_i^2} + \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2}  \\\\  \nE(\\hat \\beta_1) &= \\beta_1\n\\end{aligned}\n\\]\nWhile coefficients can be different for each sample, In average, they will be the same as the true parameters."
  },
  {
    "objectID": "rmethods/2_SRA.html#variance-of-ols-estimators",
    "href": "rmethods/2_SRA.html#variance-of-ols-estimators",
    "title": "Simple Regression Model",
    "section": "Variance of OLS Estimators",
    "text": "Variance of OLS Estimators\nHow precise are the estimates?\n\\[\\hat \\beta_1 = \\beta_1 + \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2}\n\\]\n\nIf we assume \\(x's\\) are assume fixed, the distribution from \\(\\beta's\\) will depend only on the variation of the error \\(u_i\\).\nThus we need to impose an additional assumption on this errors, to estimate the variance of \\(\\beta's\\). (at least for convinience)"
  },
  {
    "objectID": "rmethods/2_SRA.html#assumption-5",
    "href": "rmethods/2_SRA.html#assumption-5",
    "title": "Simple Regression Model",
    "section": "Assumption 5:",
    "text": "Assumption 5:\n\n\n\n\n\n\nErrors are Homoskedastic\n\n\n\n\\[E(u_i^2)= E(u_i ^2 | x_i) = \\sigma_u ^2\n\\]\n\n\n\nThis simplifying assumption states that the “distribution” of the errors is constant, regardless of \\(x\\)."
  },
  {
    "objectID": "rmethods/2_SRA.html#visualization-2",
    "href": "rmethods/2_SRA.html#visualization-2",
    "title": "Simple Regression Model",
    "section": "Visualization",
    "text": "Visualization\n\nclear\nset scheme white2\nset obs 1000\ngen x = runiform(-2 , 2)    \ngen u = rnormal()\ngen y1 = x + u\ngen y2 = x + u*abs(x)\ngen y3 = x + u*(2-abs(x))\ngen y4 = x + u*(sin(2*x))\n\n\ntwo scatter y1 x, name(m1, replace)\nscatter y2 x, name(m2, replace)\nscatter y3 x, name(m3, replace)\nscatter y4 x, name(m4, replace)\ngraph combine m1 m2 m3 m4\ngraph export images/fig2_9.png, replace width(1000)"
  },
  {
    "objectID": "rmethods/2_SRA.html#sampling-variance-of-ols",
    "href": "rmethods/2_SRA.html#sampling-variance-of-ols",
    "title": "Simple Regression Model",
    "section": "Sampling Variance of OLS",
    "text": "Sampling Variance of OLS\nWe Start with:\n\\[\n\\hat \\beta_1 - \\beta_1 = \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2}\n\\]\nAnd apply the Variance operator:\n\\[\\begin{aligned}\nVar(\\hat \\beta_1 - \\beta_1) &= Var \\left( \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2} \\right)=\n            \\frac{\\tilde x_1 Var(u_i)}{(\\sum x_i^2)^2}+\\frac{\\tilde x_2^2 Var(u_i)}{(\\sum x_i^2)^2}+...+\\frac{\\tilde x_n^2 Var(u_i)}{(\\sum x_i^2)^2} \\\\\n            &= \\frac {\\sum  \\tilde x_i^2 Var( u_i) }{(\\sum \\tilde x_i^2)^2} =\\frac {\\sum  \\tilde x_i^2 \\sigma_u^2 }{(\\sum \\tilde x_i^2)^2} \\\\\n            &= \\sigma_u^2 \\frac {\\sum  \\tilde x_i^2 }{(\\sum \\tilde x_i^2)^2} = \\frac{\\sigma_u^2}{\\sum \\tilde x_i^2}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#last-piece-of-the-puzze-sigma2_u",
    "href": "rmethods/2_SRA.html#last-piece-of-the-puzze-sigma2_u",
    "title": "Simple Regression Model",
    "section": "Last Piece of the Puzze \\(\\sigma^2_u\\)",
    "text": "Last Piece of the Puzze \\(\\sigma^2_u\\)\nTo estimate \\(Var(\\beta's)\\) we also need \\(\\sigma^2_u\\). But \\(u\\) is not observed, since we only observe \\(\\hat u\\).\n\\[\\hat u_i = u_i + (\\beta_0 - \\hat \\beta_0) + (\\beta_1 - \\hat \\beta_1)*x_i\n\\]\nAnd since to estimate \\(\\hat u_i\\) we need to estimate \\(\\beta_0\\) and \\(\\beta_1\\), we “lose” degrees of freedom that will require adjustment.\nSo, we use the following:\n\\[\\hat \\sigma^2_u = \\frac {1}{N-2} \\sum_{i=1}^N \\hat u_i^2\n\\]"
  },
  {
    "objectID": "rmethods/2_SRA.html#examples",
    "href": "rmethods/2_SRA.html#examples",
    "title": "Simple Regression Model",
    "section": "Examples",
    "text": "Examples\nDeriving OLS \\(\\beta's\\):\n\n** Wage and Education: Example 2.7\n\nfrause wage1, clear\nmata: y = st_data(.,\"wage\")\nmata: x = st_data(.,\"educ\")\nmata: b1 = sum( (x :- mean(x)) :* (y :- mean(y)) ) / sum( (x :- mean(x)):^2 ) \nmata: b0 = mean(y)-mean(x)*b1\nmata: b1, b0\n\n                 1             2\n    +-----------------------------+\n  1 |  .5413592547   -.904851612  |\n    +-----------------------------+\n\n\nSST = SSE + SSR\n\nmata: yh  = b0:+b1*x\nmata: sst = sum( (y:-mean(y)):^2 )\nmata: sse = sum( (yh:-mean(y)):^2 )\nmata: ssr = sum( (y:-yh):^2 )\nmata: sst, sse, ssr, sse + ssr\n\n                 1             2             3             4\n    +---------------------------------------------------------+\n  1 |  7160.414291   1179.732036   5980.682255   7160.414291  |\n    +---------------------------------------------------------+\n\n\n\\(R^2\\):\n\nmata: sse/sst , 1-ssr/sst\n\n                1            2\n    +---------------------------+\n  1 |  .164757511   .164757511  |\n    +---------------------------+\n\n\n\\(\\hat\\sigma_\\beta\\)\n\nmata: sig2_u = ssr / (rows(y)-2)\nmata: sst_x  = sum( (x:-mean(x)):^2 )\nmata: sig_b1 = sqrt( sig2_u / sst_x )\nmata: sig_b0 = sqrt( sig2_u * mean(x:^2) / sst_x ) \nmata: sig_b1 , sig_b0\n\n                 1             2\n    +-----------------------------+\n  1 |  .0532480368   .6849678211  |\n    +-----------------------------+\n\n\nStata command:\n\nregress wage educ\n\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =    103.36\n       Model |  1179.73204         1  1179.73204   Prob &gt; F        =    0.0000\n    Residual |  5980.68225       524  11.4135158   R-squared       =    0.1648\n-------------+----------------------------------   Adj R-squared   =    0.1632\n       Total |  7160.41429       525  13.6388844   Root MSE        =    3.3784\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .5413593    .053248    10.17   0.000     .4367534    .6459651\n       _cons |  -.9048516   .6849678    -1.32   0.187    -2.250472    .4407687\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/2_SRA.html#expanding-on-slrm-units-of-measure",
    "href": "rmethods/2_SRA.html#expanding-on-slrm-units-of-measure",
    "title": "Simple Regression Model",
    "section": "Expanding on SLRM: Units of Measure",
    "text": "Expanding on SLRM: Units of Measure\nFirst thing you should always consider doing is obtaining some summary statistics.\nwithout that its difficult o understand the magnitud of the coefficients and their effects.\n\nfrause ceosal1, clear\ndisplay \"***Variables Description***\"\ndes salary roe\ndisplay \"***Summary Statistics***\"\nsum salary roe\ndisplay \"***Simple Regression***\"\nreg salary roe\n\n***Variables Description***\n\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nsalary          int     %9.0g                 1990 salary, thousands $\nroe             float   %9.0g                 return on equity, 88-90 avg\n***Summary Statistics***\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      salary |        209     1281.12    1372.345        223      14822\n         roe |        209    17.18421    8.518509         .5       56.3\n***Simple Regression***\n\n      Source |       SS           df       MS      Number of obs   =       209\n-------------+----------------------------------   F(1, 207)       =      2.77\n       Model |  5166419.04         1  5166419.04   Prob &gt; F        =    0.0978\n    Residual |   386566563       207  1867471.32   R-squared       =    0.0132\n-------------+----------------------------------   Adj R-squared   =    0.0084\n       Total |   391732982       208  1883331.64   Root MSE        =    1366.6\n\n------------------------------------------------------------------------------\n      salary | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         roe |   18.50119   11.12325     1.66   0.098    -3.428196    40.43057\n       _cons |   963.1913   213.2403     4.52   0.000     542.7902    1383.592\n------------------------------------------------------------------------------\n\n\nNow, Changing scales has no effect on \\(R^2\\), nor the coefficient to Standard error ratio (\\(t-stat\\))\nIt could allow for easier interpretation of the results.\n\ngen saldol=salary*1000\ngen roedec=roe / 100\nqui: reg salary roe\nest sto m1\nqui: reg saldol roe\nest sto m2\nqui: reg salary roedec\nest sto m3\nesttab m1 m2 m3, se r2\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                   salary          saldol          salary   \n------------------------------------------------------------\nroe                 18.50         18501.2                   \n                  (11.12)       (11123.3)                   \n\nroedec                                             1850.1   \n                                                 (1112.3)   \n\n_cons               963.2***     963191.3***        963.2***\n                  (213.2)      (213240.3)         (213.2)   \n------------------------------------------------------------\nN                     209             209             209   \nR-sq                0.013           0.013           0.013   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/2_SRA.html#expanding-on-slrm-nonlinearities",
    "href": "rmethods/2_SRA.html#expanding-on-slrm-nonlinearities",
    "title": "Simple Regression Model",
    "section": "Expanding on SLRM: Nonlinearities",
    "text": "Expanding on SLRM: Nonlinearities\nIt is possible to incorporate some nonlinearities by using “log” transformations:\n\\[\n\\begin{aligned}\nlog(y) &= \\beta_0 + \\beta_1 x + e \\rightarrow & 100\\beta_1 \\simeq \\frac{\\% \\Delta y}{\\Delta x} \\\\\ny &= \\beta_0 + \\beta_1 log(x) + e \\rightarrow & \\frac {\\beta_1}{100} \\simeq \\frac{\\Delta y}{\\% \\Delta x} \\\\\nlog(y) &= \\beta_0 + \\beta_1 log(x) + e \\rightarrow & \\beta_1 =\\frac{\\% \\Delta y}{\\% \\Delta x}\n\\end{aligned}\n\\]\n\nThis allows us to estimate other interesting relationships with a the SRM. Specifically Semi-elasticities and Elasticities.\nThis transformation compresses the distribution of a variable, potentially addressing problems of Heteroskedasticity (non-constant variance)\n\n\n*** Example 2.10\nfrause wage1, clear\ngen lnwage = log(wage)\ngen lneduc = log(educ)\ntwo scatter wage educ     || lfit wage educ    , ///\n    name(m1, replace) title(lin-lin) legend(off)\ntwo scatter lnwage educ   || lfit lnwage educ  , ///\n    name(m2, replace) title(log-lin) legend(off)\ntwo scatter wage lneduc   || lfit wage lneduc  , ///\n    name(m3, replace) title(lin-log) legend(off)\ntwo scatter lnwage lneduc || lfit lnwage lneduc, ///\n    name(m4, replace) title(log-log) legend(off)\ngraph combine m1 m2 m3 m4, \ngraph export images/fig2_10.png, width(1000) replace\n\n\n\n\n\nset linesize 255\nqui: reg wage educ\nest sto m1\nqui: reg lnwage educ\nest sto m2\nqui: reg wage lneduc\nest sto m3\nqui: reg lnwage lneduc\nest sto m4\nesttab m1 m2 m3 m4, se r2 nonumber  compress nostar nogaps\n\n\n--------------------------------------------------\n                wage    lnwage      wage    lnwage\n--------------------------------------------------\neduc           0.541    0.0827                    \n            (0.0532) (0.00757)                    \nlneduc                             5.330     0.825\n                                 (0.608)  (0.0864)\n_cons         -0.905     0.584    -7.460    -0.445\n             (0.685)  (0.0973)   (1.532)   (0.218)\n--------------------------------------------------\nN                526       526       524       524\nR-sq           0.165     0.186     0.128     0.149\n--------------------------------------------------\nStandard errors in parentheses\n\n\n\n\nAn additional year of education\n\n1. Increases hourly wages in 54cnts\n\n2. Increases hourly wages in 8.3%\n\n3. A 1% increase in years of education (about 1.5months) increases wages in 5.3cnts\n\n4. A 1% increase in years of education would increase wages in 0.82%."
  },
  {
    "objectID": "rmethods/2_SRA.html#expanding-on-slrm-using-dummies",
    "href": "rmethods/2_SRA.html#expanding-on-slrm-using-dummies",
    "title": "Simple Regression Model",
    "section": "Expanding on SLRM: Using Dummies",
    "text": "Expanding on SLRM: Using Dummies\n\nA SLRM can also be done using Dummy variables. (Those that take only two values: 0 or 1)\nThis type of modeling may be observed when evaluating programs (Were you treated?(Tr=1) or not (Tr=0))\nAnd can be used to Easily compare means across two groups:\n\n\\[wage = \\beta_0 + \\beta_1 female + e\n\\]\nIn this particular case, both \\(\\beta_0 \\& \\beta_1\\) have clear interpretation:\n\\[\n\\begin{aligned}\n\\beta_0 &= E(wage|female=0) \\\\\n\\beta_1 &= E(wage|female=1) - E(wage|female=0)\n\\end{aligned}\n\\]\nIn most Software, you need to either Create the new variable explicitly, or use internal code to make it for you:\n\nfrause wage1, clear\n** verify Coding\nssc install fre, replace\nfre female\n** create your own\ngen is_male = female==0\n** Regression using Newly created variable\nreg wage is_male\n** Regression using Stata \"factor notation\"\nreg wage i.female\n\nchecking fre consistency and verifying not already installed...\nall files already exist and are up to date.\n\nfemale -- =1 if female\n-----------------------------------------------------------\n              |      Freq.    Percent      Valid       Cum.\n--------------+--------------------------------------------\nValid   0     |        274      52.09      52.09      52.09\n        1     |        252      47.91      47.91     100.00\n        Total |        526     100.00     100.00           \n-----------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =     68.54\n       Model |  828.220467         1  828.220467   Prob &gt; F        =    0.0000\n    Residual |  6332.19382       524  12.0843394   R-squared       =    0.1157\n-------------+----------------------------------   Adj R-squared   =    0.1140\n       Total |  7160.41429       525  13.6388844   Root MSE        =    3.4763\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     is_male |    2.51183   .3034092     8.28   0.000     1.915782    3.107878\n       _cons |   4.587659   .2189834    20.95   0.000     4.157466    5.017852\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =     68.54\n       Model |  828.220467         1  828.220467   Prob &gt; F        =    0.0000\n    Residual |  6332.19382       524  12.0843394   R-squared       =    0.1157\n-------------+----------------------------------   Adj R-squared   =    0.1140\n       Total |  7160.41429       525  13.6388844   Root MSE        =    3.4763\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    1.female |   -2.51183   .3034092    -8.28   0.000    -3.107878   -1.915782\n       _cons |   7.099489   .2100082    33.81   0.000     6.686928     7.51205\n------------------------------------------------------------------------------\n\n\nif the Dummy is a treatment, and Assumption 4 Holds, then you can use this to estimate Average Treatment Effects (ATE) aka Average Casual Effects. (Usually requires randomization)\n\\[\n\\begin{aligned}\ny_i &= y_i(0)(1-D) + y_i(1)D  \\\\\ny_i &= y_i(0) + (y_i(1)-y_i(0))*D \\\\\ny_i &= \\bar y_0 + u_i(0) + (\\bar y_1 - \\bar y_0)*D + (u_i(1)-u_i(0))*D \\\\\ny_i &= \\alpha_0 + \\tau_{ate} D + u_i\n\\end{aligned}\n\\]\nBut we expect \\(u_i(1)-u_i(0)=0\\)\n\n** Example 2.14\nfrause jtrain2, clear\n** Training was Randomly assigned\nreg re78 i.train\n\n\n      Source |       SS           df       MS      Number of obs   =       445\n-------------+----------------------------------   F(1, 443)       =      8.04\n       Model |  348.013451         1  348.013451   Prob &gt; F        =    0.0048\n    Residual |  19177.6432       443  43.2903909   R-squared       =    0.0178\n-------------+----------------------------------   Adj R-squared   =    0.0156\n       Total |  19525.6566       444  43.9767041   Root MSE        =    6.5795\n\n------------------------------------------------------------------------------\n        re78 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     1.train |   1.794343   .6328536     2.84   0.005     .5505748    3.038111\n       _cons |   4.554802    .408046    11.16   0.000     3.752856    5.356749\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/2_SRA.html#footnotes",
    "href": "rmethods/2_SRA.html#footnotes",
    "title": "Simple Regression Model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSimple Linear Regression Model↩︎"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#last-week",
    "href": "rmethods/13_advtimeseries.html#last-week",
    "title": "Times Series Part-II",
    "section": "Last week",
    "text": "Last week\n\nWhat we learned last week a few methodologies for analyzing time series data.\n\nStatic model, dynamic models, use of trends and seasonality, etc.\n\nAll those models, however, were based on very strong assumptions.\n\nStrict Exogeneity, strict Homoskedasticity, and no serial correlation are difficult to defend.\nStatistical inference validity depends on normality assumption of the error.\n\nCan we relax this assumptions?\n\nYes, but we need to impose additional assumtions to the data"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#this-week",
    "href": "rmethods/13_advtimeseries.html#this-week",
    "title": "Times Series Part-II",
    "section": "This week",
    "text": "This week\n\nOne way to relax the assumption of Strict Exogeneiety is assume time series are stationary and weakly dependent.\n\nWith this assumptions, LLN and CLT will hold for TS model.\n\nWhat do these two assumptions mean? (stationary and weakly dependent?)\nNamely, these assumptions impose restrictions on how data SHOULD behaive, so we can learn something from the data.\n\nThis means that what we observe in the data are based on data that has stable patterns. Otherwise, if they are unpredictable, we cannot learn anything. (This is the idea of stationarity)\nAt the same time, data cannot depend on its own past too much. So if we look at two periods that are farther apart, its like they are independent for most practical purposes."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#can-you-recognized-this-type-of-data",
    "href": "rmethods/13_advtimeseries.html#can-you-recognized-this-type-of-data",
    "title": "Times Series Part-II",
    "section": "Can you recognized this type of data?",
    "text": "Can you recognized this type of data?\n\nTrendsDensity"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#stationary-process",
    "href": "rmethods/13_advtimeseries.html#stationary-process",
    "title": "Times Series Part-II",
    "section": "Stationary Process",
    "text": "Stationary Process\n\nA Stationary process is one where the characteristics of the distribution remains Stable across time.\n\nSo we can learn something about it\n\n\nFor example consider the series \\(x=[x_1,x_2,\\dots,x_T]\\), where \\(T\\) can be vary large (infinite)\n\nIf the series is stationary, then:\n\n\\[f(x_t, x_{t+k}) = f(x_s, x_{s+k}) \\forall s, k\\]\n\nIn other words, the distribution across time does not change. (is stable, thus stationary)\nOtherwise, if the distribution function changes constantly, we cannot learn about it, thus making sense of regressions would be even more difficult."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#testing-for-stationarity",
    "href": "rmethods/13_advtimeseries.html#testing-for-stationarity",
    "title": "Times Series Part-II",
    "section": "Testing for Stationarity",
    "text": "Testing for Stationarity\n\nTesting for stationarity is not easy. (how do you test for stability of densities?)\nInstead, from an empirical point of view, we focus on the first 2 moments of the distribution: mean, variance and covariance.\n\nThus:\nA series \\(x=[x_1,x_2,\\dots,x_T]\\) is stationary if:\n\n\\(E(x_t)=E(x_s)=\\mu_x\\). Constant mean\n\\(E(x_t^2)&lt;\\infty\\). Finite variance\n\\(Var(x_t)=Var(x_s)=\\sigma_x^2\\). Constant variance\n\\(cov(x_t,x_{t+k})=cov(x_s,x_{s+k})\\). Constant covariance\n\nThe Series repeats itself."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#weakly-dependent-series",
    "href": "rmethods/13_advtimeseries.html#weakly-dependent-series",
    "title": "Times Series Part-II",
    "section": "Weakly Dependent Series",
    "text": "Weakly Dependent Series\n\nDependence should be understood as a measure of how much a series depends on its own past.\n\nWeakly dependent series are those that depend on its own past, but not too much.\n\nFrom the technical point of view:\n\nIf \\(x_t\\) is weakly dependent, then \\(\\lim_{k\\rightarrow \\infty} corr(x_t,x_{t+k})=0\\) sufficiently fast.\n\nIf this happens, LLN and CLT will hold for TS data.\nWhy?\n\nOmiting \\(x_{t+1}\\) would typically generate an Omitted Variable Bias.\nIf data are weakly dependent, however, we won’t suffer from OMB\nThis is because omitting \\(x_{t+1}\\) is like omitting a unrelated variable.\n\n\n\nNOTE: Weakly dependent variables can be non-stationary. (we call them stationary around a trend)"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#visualizing-weakly-dependent-vs-strongly-dependent",
    "href": "rmethods/13_advtimeseries.html#visualizing-weakly-dependent-vs-strongly-dependent",
    "title": "Times Series Part-II",
    "section": "Visualizing weakly dependent vs strongly dependent",
    "text": "Visualizing weakly dependent vs strongly dependent"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#example-of-weakly-dependent-series",
    "href": "rmethods/13_advtimeseries.html#example-of-weakly-dependent-series",
    "title": "Times Series Part-II",
    "section": "Example of Weakly Dependent Series",
    "text": "Example of Weakly Dependent Series\n\nAR(1) process: \\(x_t = \\rho x_{t-1} + \\epsilon_t\\)\n\nMean in constant (not depent on time)\nVariance is constant (not depent on time)\nCovariance does not depend on time (just on lag)\nIf \\(|\\rho|&lt;1\\), then \\(x_t\\) is weakly dependent.\n\nMA(1) process: \\(x_t = \\epsilon_t + \\theta \\epsilon_{t-1}\\)\n\nConstant mean and variance.\nCovariance does not depend on time.\nCovariance bettween \\(x_t\\) and \\(x_{t+k}\\) is zero for \\(k&gt;1\\).\n\nARMA(1,1) \\(x_t = \\rho x_{t-1} + \\epsilon_t + \\theta \\epsilon_{t-1}\\)"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#special-case-ar1-with-rho1",
    "href": "rmethods/13_advtimeseries.html#special-case-ar1-with-rho1",
    "title": "Times Series Part-II",
    "section": "Special Case AR(1) with \\(\\rho=1\\)",
    "text": "Special Case AR(1) with \\(\\rho=1\\)\n\nIf \\(\\rho=1\\), we have a random walk process \\(x_t = x_{t-1} + \\epsilon_t\\)\nThis is a process that holds grudges (it remembers its past)\n\n\\[x_t = x_{t=0}+e_1+e_2+\\dots+e_t\\]\n\nThis process has constant mean (\\(x_0\\)), but variance and covariance are not constant. With a correlation that dissapears slowly.\n\n\\[Var(x_t) = t\\sigma^2 \\text{ and } cov(x_t,x_{t+h}) = t\\sigma^2\\]\n\\[corr(x_t,x_{t+h}) = \\frac{t \\sigma^2}{\\sqrt{t(t+h)}\\sigma^2}=\\sqrt{\\frac{t}{t+h}}\\]\nEconomics: With weakly dependent data, policies are transitory, with persistent data effects are longlasting."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#special-case-non-stationary-weakly-dependent",
    "href": "rmethods/13_advtimeseries.html#special-case-non-stationary-weakly-dependent",
    "title": "Times Series Part-II",
    "section": "Special Case: non-stationary weakly dependent",
    "text": "Special Case: non-stationary weakly dependent\n\nThere are few cases where a series is weakly dependent but non-stationary.\n\n\\[x_t = \\rho x_{t-1} + \\delta t + \\epsilon_t\\]\n\n\\(E(x_t)\\) is not constant, but this series is still weakly dependent if \\(|\\rho|&lt;1\\).\nIf the data is detrended, however, it becomes stationary."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#example-stationary-around-a-trend",
    "href": "rmethods/13_advtimeseries.html#example-stationary-around-a-trend",
    "title": "Times Series Part-II",
    "section": "Example: Stationary around a trend",
    "text": "Example: Stationary around a trend\n\\(x_t = 0.5*x_{t-1}+0.1 t + u_t\\)"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#example-random-walks-look-with-a-drift",
    "href": "rmethods/13_advtimeseries.html#example-random-walks-look-with-a-drift",
    "title": "Times Series Part-II",
    "section": "Example: Random Walks look with a drift",
    "text": "Example: Random Walks look with a drift\n\\[x_t = x_{t-1} + 0.1 + \\epsilon_t\\]"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#how-do-things-change-assumptions",
    "href": "rmethods/13_advtimeseries.html#how-do-things-change-assumptions",
    "title": "Times Series Part-II",
    "section": "How do things change: Assumptions",
    "text": "How do things change: Assumptions\nA1. Linear in Parameters (same as before), but all variables are stationary and weakly dependent.\nA2. No Perfect Colinearity\nA3. Zero Conditional Mean: \\(E(u_t|x_t)=0\\) Contemporaneous exogeneity!\nOmitting lags of \\(x_t\\) is not a problem, because they are weakly “independent” of \\(x_t\\).\nA1-A3 OLS is consistent.\nWhy does this matter??\n\nBecause, under Strict exogeneity, we cannot allow for Lags of the outcome to be included in the model.\nUnder weak exogeneity, lags can be included."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#how-do-things-change-assumptions-1",
    "href": "rmethods/13_advtimeseries.html#how-do-things-change-assumptions-1",
    "title": "Times Series Part-II",
    "section": "How do things change: Assumptions",
    "text": "How do things change: Assumptions\nA4. Homoskedasticity: \\(Var(u_t|x_t)=\\sigma^2\\) (also we just need contemporaenous homoskedasticity)\nA5. No Serial Correlation: \\(cov(u_t,u_s|x_t)=0\\) for \\(t\\neq s\\)\nThese two assumptions that make “life” easier for estimating standard errors because:\n\nUnder A1-A5, OLS estimators are asymptotically normal, and all Standard Statistics are applicable"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#order-of-integration",
    "href": "rmethods/13_advtimeseries.html#order-of-integration",
    "title": "Times Series Part-II",
    "section": "Order of Integration",
    "text": "Order of Integration\n\nAs you may expect, many interesting time series are not stationary. However, we may want to use for analysis\n\nto do so, we need to understand their taxonomy (in TS) so we can make them stationary.\n\nA series is said to be integrated of order \\(d\\), denoted \\(I(d)\\), if it can be made stationary by taking \\(d\\) differences.\nA weakly dependent series is an \\(I(0)\\) process. (is already stationary)\nA random walk is an \\(I(1)\\) process. It could be made stationary by taking first differences.\n\n\\[x_t = x_{t-1} + \\epsilon_t \\rightarrow \\Delta x_t = \\epsilon_t\\]\n\nA series that is \\(I(2)\\) would required two differences to be made stationary.\n\n\\[x_t =2x_{t-1} - x_{t-2} + \\epsilon_t \\rightarrow \\Delta^2 x_t = \\epsilon_t\\]"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#unit-roots-and-spurious-regressions",
    "href": "rmethods/13_advtimeseries.html#unit-roots-and-spurious-regressions",
    "title": "Times Series Part-II",
    "section": "Unit roots and Spurious Regressions",
    "text": "Unit roots and Spurious Regressions\n\nIf a series is \\(I(1)\\), it is said to have a unit root. an \\(I(2)\\) series has two unit roots, etc.\nData that are \\(I(1)\\) tend to look like data with Trends\n\nIf we analyze this data, we may find spurious relationships. t-stats may be high, as well as \\(R^2\\).\nThis may lead to incorrect conclusions (unless other stronger assumptions are made)\nIn this cases, using trends will not help."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#spurious-regressions-example",
    "href": "rmethods/13_advtimeseries.html#spurious-regressions-example",
    "title": "Times Series Part-II",
    "section": "Spurious Regressions: Example",
    "text": "Spurious Regressions: Example\n\\(x_t = x_{t-1} + v_t\\) & \\(y_t = y_{t-1} + u_t\\)"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#naive-approach.-look-into-rho",
    "href": "rmethods/13_advtimeseries.html#naive-approach.-look-into-rho",
    "title": "Times Series Part-II",
    "section": "Naive Approach. Look into \\(\\rho\\)",
    "text": "Naive Approach. Look into \\(\\rho\\)\n\nNaive approach: Look at auto correlation:\n\nif \\(corr(x_t,x_{t-1})&gt;0.9\\) then \\(x_t\\) is highly persistent, and probably \\(I(1)\\)\nDifferentiate data and look at auto correlation again."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#formal-approach-dickey-fuller-test-for-unit-root",
    "href": "rmethods/13_advtimeseries.html#formal-approach-dickey-fuller-test-for-unit-root",
    "title": "Times Series Part-II",
    "section": "Formal Approach: Dickey-Fuller Test for Unit Root",
    "text": "Formal Approach: Dickey-Fuller Test for Unit Root\nModel: \\(y_t = \\alpha + \\rho y_{t-1} + e_t\\) AModel: \\(\\Delta y_t = \\alpha + \\theta y_{t-1} + e_t\\)\n\n\\(H_0: \\rho=1\\) (has a unit root) vs \\(H_1: \\rho&lt;1\\) (is stationary)\n\\(H_0: \\theta=0\\) (has a unit root) vs \\(H_1: \\theta&lt;0\\) (is stationary)\n\nIts a one tail test, however, the statistic of interest does not follow a t-distribution, but a DF distribution\n\n\n\nSL\n1%\n2.5%\n5%\n10%\n\n\n\n\nDF\n-3.43\n-3.12\n-2.86\n-2.57"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#augmented-dickey-fuller-test",
    "href": "rmethods/13_advtimeseries.html#augmented-dickey-fuller-test",
    "title": "Times Series Part-II",
    "section": "Augmented Dickey-Fuller Test",
    "text": "Augmented Dickey-Fuller Test\nAllowing for serial correlation, and uses same critial values as before:\nModel: \\(\\Delta y_t = \\alpha + \\theta y_{t-1} + \\lambda_1 \\Delta y_{t-1} + \\dots + \\lambda_k \\Delta y_{t-k} + e_t\\)\nSame as before. But in practice the additional lags should be choosen based on information criteria."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#adf-with-a-trend",
    "href": "rmethods/13_advtimeseries.html#adf-with-a-trend",
    "title": "Times Series Part-II",
    "section": "ADF with a trend",
    "text": "ADF with a trend\nModel: \\(\\Delta y_t = \\alpha + \\delta t +\\theta y_{t-1} + \\lambda_1 \\Delta y_{t-1} + \\dots + \\lambda_k \\Delta y_{t-k} + e_t\\)\n\nThis allows for even more flexibility, or if you believe data is stationary around a trend.\nMain difference… critical values are even larger:\n\n\n\n\nSL\n1%\n2.5%\n5%\n10%\n\n\n\n\nDF\n-3.96\n-3.66\n-3.41\n-3.12\n\n\n\nAs before, If you find evidence of unit root, Differentiate and test again."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#example",
    "href": "rmethods/13_advtimeseries.html#example",
    "title": "Times Series Part-II",
    "section": "Example",
    "text": "Example\n\nqui:frause fertil3, clear\n** Setup as time series\ntsset year\n\n\nTime variable: year, 1913 to 1984\n        Delta: 1 unit\n\n\nThe model: \\(gfr = \\alpha + \\delta_0 pe_t + \\delta_1 pe_{t-1} + \\delta_2 pe_{t-2}+ e_t\\)\n\nreg gfr pe l.pe l2.pe  \n\n\n      Source |       SS           df       MS      Number of obs   =        70\n-------------+----------------------------------   F(3, 66)        =      0.14\n       Model |  159.461148         3   53.153716   Prob &gt; F        =    0.9383\n    Residual |  25832.9717        66  391.408663   R-squared       =    0.0061\n-------------+----------------------------------   Adj R-squared   =   -0.0390\n       Total |  25992.4329        69  376.701926   Root MSE        =    19.784\n\n------------------------------------------------------------------------------\n         gfr | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          pe |\n         --. |  -.0158445    .140256    -0.11   0.910    -.2958747    .2641856\n         L1. |  -.0213365   .2152292    -0.10   0.921    -.4510555    .4083826\n         L2. |   .0539005   .1381132     0.39   0.698    -.2218513    .3296524\n             |\n       _cons |   93.15791   4.499654    20.70   0.000     84.17406    102.1418\n------------------------------------------------------------------------------\n\n\nAre \\(gfr\\) and \\(pe\\) Stationary?\nNaive approach: Look at auto correlation:\n\n** Naive apprach\ncorr pe l.pe gfr l.gfr\n\n(obs=71)\n\n             |                 L.                L.\n             |       pe       pe      gfr      gfr\n-------------+------------------------------------\n          pe |\n         --. |   1.0000\n         L1. |   0.9636   1.0000\n         gfr |\n         --. |   0.0086   0.0188   1.0000\n         L1. |  -0.0300  -0.0296   0.9765   1.0000\n\n\n\nFormal Approach: Dickey-Fuller Test for Unit Root\n\nreg d.pe l.pe, nohead\nreg d.gfr l.gfr, nohead\n\n------------------------------------------------------------------------------\n        D.pe | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          pe |\n         L1. |  -.0521147   .0316692    -1.65   0.104    -.1152931    .0110637\n             |\n       _cons |   6.426196   3.808601     1.69   0.096    -1.171754    14.02415\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\n       D.gfr | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         gfr |\n         L1. |  -.0222798   .0260053    -0.86   0.395     -.074159    .0295994\n             |\n       _cons |   1.304937   2.548821     0.51   0.610    -3.779822    6.389695\n------------------------------------------------------------------------------\n\n\nSame conclusions. Are their differences stationary?\n\ngen dpe = d.pe \ngen dgfr = d.gfr\nreg d.dpe l.dpe, nohead\nreg d.dgfr l.dgfr, nohead\n\n(1 missing value generated)\n(1 missing value generated)\n------------------------------------------------------------------------------\n       D.dpe | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         dpe |\n         L1. |  -.7666022   .1181882    -6.49   0.000    -1.002443   -.5307613\n             |\n       _cons |   .8901863   2.103074     0.42   0.673    -3.306432    5.086804\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\n      D.dgfr | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        dgfr |\n         L1. |   -.713481   .1158143    -6.16   0.000    -.9445848   -.4823772\n             |\n       _cons |  -.6332004   .5027213    -1.26   0.212    -1.636365    .3699644\n------------------------------------------------------------------------------\n\n\nNow it should be better to use model in differences for analysi:\n\nreg dgfr dpe l.dpe l2.dpe  \n\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(3, 65)        =      6.56\n       Model |  293.259859         3  97.7532864   Prob &gt; F        =    0.0006\n    Residual |  968.199959        65   14.895384   R-squared       =    0.2325\n-------------+----------------------------------   Adj R-squared   =    0.1971\n       Total |  1261.45982        68  18.5508797   Root MSE        =    3.8595\n\n------------------------------------------------------------------------------\n        dgfr | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         dpe |\n         --. |  -.0362021   .0267737    -1.35   0.181     -.089673    .0172687\n         L1. |  -.0139706   .0275539    -0.51   0.614    -.0689997    .0410584\n         L2. |   .1099896   .0268797     4.09   0.000     .0563071    .1636721\n             |\n       _cons |  -.9636787   .4677599    -2.06   0.043     -1.89786   -.0294976\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#the-problem-of-serial-correlation",
    "href": "rmethods/13_advtimeseries.html#the-problem-of-serial-correlation",
    "title": "Times Series Part-II",
    "section": "The Problem of Serial Correlation",
    "text": "The Problem of Serial Correlation\n\nUp to this point, we have assumed that the error term is uncorrelated across time. (no serial correlation)\nAs with RC analysis, violation of this assumption does not lead to biased estimators of the coefficients (under usual situations), but it does lead to biased standard errors.\nWhy? If errors are correlated across time, (say possitively) then the variance of the OLS estimator is biased downwards.\n\n\\[Var(u_t+u_{t+h})=2\\sigma^2 + \\color{red}{2\\rho_{t,t+h}} \\sigma^2\\]"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#serial-correlation-and-lags",
    "href": "rmethods/13_advtimeseries.html#serial-correlation-and-lags",
    "title": "Times Series Part-II",
    "section": "Serial Correlation and Lags",
    "text": "Serial Correlation and Lags\n\nIf one has a model with Lags, then serial correlation is likely to happen.\n\n\\[y_t = \\beta_0 + \\beta_1 y_{t-1} + u_t\\]\n\nThis model simply assumes that \\(y_{t-1}\\) should be uncorrelated with \\(u_t\\). But, it may be that \\(y_{t-2}\\) is correlated with \\(u_t\\).\nIf that is the case then \\(Corr(u_t, u_{t-1})\\neq 0\\) because it may be picking up that correlation.\nOn the other hand, if \\(u_t\\) is serially correlated, then \\(y_{t-1}\\) is correlated with \\(u_t\\), causing OLS to be inconsistent.\n\nThis, however, may also indicate that one needs to consider a different model:\n\n\n\\[y_t = \\alpha_0 + \\alpha_1 y_{t-1} + \\alpha_2 y_{t-2} + e_t\\]\n\nWhere \\(e_t\\) is not serially correlated, not correlated with \\(y_{t-1}\\), nor \\(y_{t-2}\\)."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#test-for-serial-correlation",
    "href": "rmethods/13_advtimeseries.html#test-for-serial-correlation",
    "title": "Times Series Part-II",
    "section": "Test for Serial Correlation",
    "text": "Test for Serial Correlation\n\nStrictly Exogenous Regressors\nModel: \\(y_t = \\beta_0 + \\beta_1 x_{1,t} + \\dots + \\beta_k x_{k,t} + u_t\\) and: \\(u_t=\\rho u_{t-1}+e_t\\)\nIf there is no serial correlation, then we simply need to test if \\(\\rho=0\\), using a t-statistic.\n\n\nDurbin Watson Test\nUnder Classical assumptions, one could also use the DW statistic:\n\\[DW = \\frac{\\sum_{t=2}^T (u_t-u_{t-1})^2}{\\sum_{t=1}^T u_t^2}\\]\nwhere \\(DW\\simeq 2(1-\\hat{\\rho})\\).\n\nif there is no serial correlation, then \\(DW\\simeq 2\\).\nIf there is possitive serial correlation, then \\(DW&lt;2\\).\nA less practical test, but valid in small samples"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#test-for-serial-correlation-1",
    "href": "rmethods/13_advtimeseries.html#test-for-serial-correlation-1",
    "title": "Times Series Part-II",
    "section": "Test for Serial Correlation",
    "text": "Test for Serial Correlation\n\nWeakly Exogenous Regressors\nModel: \\(y_t = \\beta_0 + \\beta_1 x_{1,t} + \\dots + \\beta_k x_{k,t} + u_t\\) and: \\(u_t=\\rho u_{t-1}+\\gamma_1 x_{1,t} + \\dots + \\gamma_k x_{k,t}+e_t\\)\n\\(H0: \\rho=0\\) vs \\(H1: \\rho\\neq 0\\)\n\n\nTesting for higher order correlation\nand: \\(u_t=\\rho_1 u_{t-1}+\\rho_2 u_{t-2}+\\gamma_1 x_{1,t} + \\dots + \\gamma_k x_{k,t}+e_t\\)\n\\(H0: \\rho_1=0 \\& \\rho_2=0\\) vs \\(H1: \\text{one is not equal to }0\\)\nThis test is called the Breusch-Godfrey test."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#correcting-for-serial-correlation",
    "href": "rmethods/13_advtimeseries.html#correcting-for-serial-correlation",
    "title": "Times Series Part-II",
    "section": "Correcting for Serial Correlation:",
    "text": "Correcting for Serial Correlation:\nThere are two ways to correct for Serial Correlation:\n\nPrais-Winsten and Cochrane-Orcutt regression (Feasible GLS)\n\nRequires variables to be strictly exogenous regressors (no lagged dependent variables)\n\nNewey-West Standard Errors (this is the equivalent to Robust)\n\nGeneral setup."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#prais-winsten-and-cochrane-orcutt-regression",
    "href": "rmethods/13_advtimeseries.html#prais-winsten-and-cochrane-orcutt-regression",
    "title": "Times Series Part-II",
    "section": "Prais-Winsten and Cochrane-Orcutt regression",
    "text": "Prais-Winsten and Cochrane-Orcutt regression\nConsider the model:\n\\[y_t = \\beta_0 + \\beta_1 x_{1,t} + \\beta_2 x_{2,t} + u_t\\]\nwhere \\(u_t=\\rho u_{t-1}+e_{t}\\)\nThis model has serial correlation, which will affect the standard errors of the OLS estimator.\n\nif we know (or estimate) \\(\\rho\\), we can transform the data and eliminate the serial correlation\n\n\\[\\begin{aligned}\ny_t &= \\beta_0 + \\beta_1 x_{1,t} + \\beta_2 x_{2,t} + u_t \\\\\\\n\\rho y_{t-1} &= \\rho \\beta_0 + \\rho \\beta_1 x_{1,t-1} + \\rho \\beta_2 x_{2,t-1} + \\rho  u_{t-1} \\\\\\\n\\tilde y_t &= \\beta_0 (1-\\rho) + \\beta_1 \\tilde x_{1,t} + \\beta_2 \\tilde x_{2,t} +  e_t\n\\end{aligned}\n\\]\n\nFrom here, we can obtain the errors \\(e_t\\) and \\(u_t\\), re estimate \\(\\rho\\), and re estimate the model, until \\(\\rho\\) no longer changes.\nThis is called the Cochrane-Orcutt procedure."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#prais-winsten-and-cochrane-orcutt-regression-1",
    "href": "rmethods/13_advtimeseries.html#prais-winsten-and-cochrane-orcutt-regression-1",
    "title": "Times Series Part-II",
    "section": "Prais-Winsten and Cochrane-Orcutt regression",
    "text": "Prais-Winsten and Cochrane-Orcutt regression\n\nThe Prais-Winsten procedures is also similar to the Cochrane-Orcutt procedure, but you do not “loose” the first observation.\nSpecifically, the first observation is estimated as:\n\n\\[(1-\\rho^2)^{1/2} y_1 =(1-\\rho^2)^{1/2}\\beta_0 + (1-\\rho^2)^{1/2}\\beta_1 x_{1,1} + (1-\\rho^2)^{1/2}\\beta_2 x_{2,1} + (1-\\rho^2)^{1/2} u_1\\]\n\nOther features:\n\nPW can be more efficient than CO, because of the “saved observation”\nBoth can be used when serial correlation is of higher order, but only CO can be used if order is 3 or higher\nBoth methods are iterative\n\n\n\nfrause phillips, clear\ntsset year\nreg inf unem\n\n\nTime variable: year, 1948 to 2003\n        Delta: 1 unit\n\n      Source |       SS           df       MS      Number of obs   =        56\n-------------+----------------------------------   F(1, 54)        =      3.58\n       Model |   31.599858         1   31.599858   Prob &gt; F        =    0.0639\n    Residual |  476.815691        54   8.8299202   R-squared       =    0.0622\n-------------+----------------------------------   Adj R-squared   =    0.0448\n       Total |  508.415549        55  9.24391907   Root MSE        =    2.9715\n\n------------------------------------------------------------------------------\n         inf | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |   .5023782   .2655624     1.89   0.064    -.0300424    1.034799\n       _cons |   1.053566   1.547957     0.68   0.499    -2.049901    4.157033\n------------------------------------------------------------------------------\n\n\n\nprais inf unem, corc\n\nprais inf unem, \n\n\nIteration 0:   rho = 0.0000\nIteration 1:   rho = 0.5721\nIteration 2:   rho = 0.7204\nIteration 3:   rho = 0.7683\nIteration 4:   rho = 0.7793\nIteration 5:   rho = 0.7815\nIteration 6:   rho = 0.7819\nIteration 7:   rho = 0.7820\nIteration 8:   rho = 0.7820\nIteration 9:   rho = 0.7820\nIteration 10:  rho = 0.7820\n\nCochrane–Orcutt AR(1) regression with iterated estimates\n\n      Source |       SS           df       MS      Number of obs   =        55\n-------------+----------------------------------   F(1, 53)        =      5.09\n       Model |  23.3857044         1  23.3857044   Prob &gt; F        =    0.0282\n    Residual |  243.353574        53  4.59157686   R-squared       =    0.0877\n-------------+----------------------------------   Adj R-squared   =    0.0705\n       Total |  266.739278        54  4.93961626   Root MSE        =    2.1428\n\n------------------------------------------------------------------------------\n         inf | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |  -.6639584   .2942027    -2.26   0.028    -1.254054   -.0738626\n       _cons |   7.287078   2.163178     3.37   0.001     2.948291    11.62586\n-------------+----------------------------------------------------------------\n         rho |   .7820093\n------------------------------------------------------------------------------\nDurbin–Watson statistic (original)    = 0.801482\nDurbin–Watson statistic (transformed) = 1.600203\n\nIteration 0:   rho = 0.0000\nIteration 1:   rho = 0.5721\nIteration 2:   rho = 0.7350\nIteration 3:   rho = 0.7792\nIteration 4:   rho = 0.7871\nIteration 5:   rho = 0.7883\nIteration 6:   rho = 0.7885\nIteration 7:   rho = 0.7885\nIteration 8:   rho = 0.7885\nIteration 9:   rho = 0.7885\n\nPrais–Winsten AR(1) regression with iterated estimates\n\n      Source |       SS           df       MS      Number of obs   =        56\n-------------+----------------------------------   F(1, 54)        =      8.39\n       Model |   38.377534         1   38.377534   Prob &gt; F        =    0.0054\n    Residual |  246.917431        54  4.57254502   R-squared       =    0.1345\n-------------+----------------------------------   Adj R-squared   =    0.1185\n       Total |  285.294965        55  5.18718118   Root MSE        =    2.1384\n\n------------------------------------------------------------------------------\n         inf | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |  -.7139659   .2897858    -2.46   0.017    -1.294951   -.1329804\n       _cons |   7.999443   2.048343     3.91   0.000     3.892762    12.10612\n-------------+----------------------------------------------------------------\n         rho |   .7885234\n------------------------------------------------------------------------------\nDurbin–Watson statistic (original)    = 0.801482\nDurbin–Watson statistic (transformed) = 1.913928"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#newey-west-standard-errors",
    "href": "rmethods/13_advtimeseries.html#newey-west-standard-errors",
    "title": "Times Series Part-II",
    "section": "Newey-West Standard Errors",
    "text": "Newey-West Standard Errors\n\nThe Newey-West standard errors are similar to the robust standard errors, but they take into account the serial correlation of the error term.\nThe idea is to estimate an inflation factor that corrects Standard errors for serial correlation.\n\n\\(\\hat v = \\sum_{t=1}^T \\hat a_t^2 + 2 \\sum_{h=1}^g \\left[1-\\frac{h}{g+1}\\right] \\left( \\sum_{t=h+1}^{T} \\hat a_t \\hat a_{t-h}\\right)\\)\nwith \\(\\hat a_t = \\hat r_t \\hat u_t\\)\n\nThen \\(SE_c (\\beta) = \\sqrt{\\hat v} \\left(\\frac{SE(\\beta)}{\\sigma}\\right)^2\\)\nIn other words, this kind of corrects for the fact that the error term is correlated across time.\n\n\nnewey inf unem, lag(0)\nnewey inf unem, lag(1)\nnewey inf unem, lag(2)\n\n\nRegression with Newey–West standard errors      Number of obs     =         56\nMaximum lag = 0                                 F(  1,        54) =       4.11\n                                                Prob &gt; F          =     0.0477\n\n------------------------------------------------------------------------------\n             |             Newey–West\n         inf | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |   .5023782   .2479249     2.03   0.048     .0053187    .9994378\n       _cons |   1.053566   1.379772     0.76   0.448    -1.712711    3.819842\n------------------------------------------------------------------------------\n\nRegression with Newey–West standard errors      Number of obs     =         56\nMaximum lag = 1                                 F(  1,        54) =       3.25\n                                                Prob &gt; F          =     0.0769\n\n------------------------------------------------------------------------------\n             |             Newey–West\n         inf | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |   .5023782    .278577     1.80   0.077    -.0561351    1.060892\n       _cons |   1.053566   1.464589     0.72   0.475    -1.882759     3.98989\n------------------------------------------------------------------------------\n\nRegression with Newey–West standard errors      Number of obs     =         56\nMaximum lag = 2                                 F(  1,        54) =       3.13\n                                                Prob &gt; F          =     0.0827\n\n------------------------------------------------------------------------------\n             |             Newey–West\n         inf | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |   .5023782   .2841794     1.77   0.083    -.0673673    1.072124\n       _cons |   1.053566   1.424115     0.74   0.463    -1.801613    3.908744\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#cointegration",
    "href": "rmethods/13_advtimeseries.html#cointegration",
    "title": "Times Series Part-II",
    "section": "Cointegration",
    "text": "Cointegration\n\nAs previously mentioned, most interesting time series are not stationary.\nAnd, when using non-stationary data, we may find spurious relationships. But what if the relation is not spurious?\nConsider the following model:\n\\(y_t = \\beta_0 + \\beta_1 x_{1,t} + \\beta_2 x_{2,t} + u_t\\)\nIf \\(y_t\\) and \\(x_{1,t}\\) are \\(I(1)\\), then the model is likely to be spurious (common trends). However, it may be possible that there is a causal relationship between these variables.\nIf they indeed have a causal relationship, then they are said to be cointegrated."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#cointegration-1",
    "href": "rmethods/13_advtimeseries.html#cointegration-1",
    "title": "Times Series Part-II",
    "section": "Cointegration",
    "text": "Cointegration\n\nBut how do we know if two variables are cointegrated?\n\ns1: Check if all variables are \\(I(1)\\). If they are, then you can check for cointegration.\ns2: Estimate the model, and obtain the residuals \\(\\hat u_t\\).\ns3: Test if \\(\\hat u_t\\) is \\(I(0)\\).\n\nIf that is the case, then the variables are cointegrated (Share a long term relationship)\nIf not, the relationship is spurious\nHow do we test if \\(\\hat u_t\\) is \\(I(0)\\)? \\(\\rightarrow\\) Unit Root test!\n\n\n\n\nSL\n1%\n2.5%\n5%\n10%\n\n\n\n\nNo Trend\n-3.9\n-3.59\n-3.34\n-3.04\n\n\nWith Trend\n-4.32\n-4.03\n-3.78\n-3.50"
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#error-correction-models",
    "href": "rmethods/13_advtimeseries.html#error-correction-models",
    "title": "Times Series Part-II",
    "section": "Error Correction Models",
    "text": "Error Correction Models\n\nIf two variables are cointegrated, then they share a long term relationship.\nHowever, you may also be interested in the short term dynamics of the relationship.\nTo do this, you can use an Error Correction Model (ECM)\n\n\\(\\Delta y_t = \\beta_0 + \\beta_1 \\Delta x_{1,t} + \\beta_2 \\Delta x_{2,t} + \\gamma \\hat u_{t-1}+ e_t\\)\nWhere \\(\\gamma\\) is the short term correction term."
  },
  {
    "objectID": "rmethods/13_advtimeseries.html#other-topics-of-interest",
    "href": "rmethods/13_advtimeseries.html#other-topics-of-interest",
    "title": "Times Series Part-II",
    "section": "Other topics of interest",
    "text": "Other topics of interest\n\nForcasting\n\nARIMA models, and VAR models (Vector Autoregressions) can be used for forcasting.\nForcating implies making predictions about the future, based on the past, accounting for the errors propagation.\nVariable selection, temporal causation (Granger Causality), and other techniques are used for this."
  },
  {
    "objectID": "rmethods/11_advpanel.html",
    "href": "rmethods/11_advpanel.html",
    "title": "Advanced Panel Data",
    "section": "",
    "text": "Last class we introduce the basic panel data model:\n\n\\[y_{it} = \\alpha + \\beta x_{it} + \\alpha_i + \\delta_t + \\epsilon_{it}\\]\n\nThis model could be estimated using First Differences approach.\n\n\\[\\Delta y_{it} = \\beta \\Delta x_{it} + \\delta_t + \\Delta \\epsilon_{it}\\]\n\nIt elimitates the \\(\\alpha_i\\), and constrains how \\(\\delta_t\\) is estimated.\nIt allows you to related how changes in \\(x_{it}\\) are related to changes in \\(y_{it}\\).\n\nThus Fixed variables across time cannot be identified.\n\nIt requires strong assumptions of strict exogeneity and no serial correlation."
  },
  {
    "objectID": "rmethods/11_advpanel.html#the-old-way-last-class",
    "href": "rmethods/11_advpanel.html#the-old-way-last-class",
    "title": "Advanced Panel Data",
    "section": "",
    "text": "Last class we introduce the basic panel data model:\n\n\\[y_{it} = \\alpha + \\beta x_{it} + \\alpha_i + \\delta_t + \\epsilon_{it}\\]\n\nThis model could be estimated using First Differences approach.\n\n\\[\\Delta y_{it} = \\beta \\Delta x_{it} + \\delta_t + \\Delta \\epsilon_{it}\\]\n\nIt elimitates the \\(\\alpha_i\\), and constrains how \\(\\delta_t\\) is estimated.\nIt allows you to related how changes in \\(x_{it}\\) are related to changes in \\(y_{it}\\).\n\nThus Fixed variables across time cannot be identified.\n\nIt requires strong assumptions of strict exogeneity and no serial correlation."
  },
  {
    "objectID": "rmethods/11_advpanel.html#the-new-way-this-class",
    "href": "rmethods/11_advpanel.html#the-new-way-this-class",
    "title": "Advanced Panel Data",
    "section": "The new way (This class)",
    "text": "The new way (This class)\n\nToday we are going to describe the use of three other methods:\n\nFixed Effects (FE)\nRandom Effects (RE)\nCorrelated Random effects (CRE) \\(\\simeq\\) FE+RE\n\nThis method require their own methods, but could be used in more flexible scenarios.\nWhat do we mean Fixed effects? Random Effects? Correlated Random Effects?\n\nAll this will be estimation methods that relate to the same model.\nHowever, in all cases, we assume the unobserved are fixed factors across time. We simply identify them differently."
  },
  {
    "objectID": "rmethods/11_advpanel.html#fixed-effects-estimation",
    "href": "rmethods/11_advpanel.html#fixed-effects-estimation",
    "title": "Advanced Panel Data",
    "section": "Fixed Effects Estimation",
    "text": "Fixed Effects Estimation\nLets consider the following model: \\[y_{it} = \\beta_1 x_{it} + \\beta_2 z_{it} + \\alpha_i + \\epsilon_{it}\\]\nwhich doesnt include a time-fixed effect, nor time-invariante factors.\n\nThis could be estiamted simply adding dummies for each individual in the data set. (too many dummies). Instead consider the following\nNow, for each person, lets estimate the average characteristics \\(\\bar w = \\frac{1}{T} \\sum_{t=1}^T w_{it}\\). We could apply this to the model above an dobtain:\n\n\\[\\bar y_{i} = \\beta_1 \\bar x_{i} + \\beta_2 \\bar z_{i} + \\alpha_i + \\bar \\epsilon_{i}\\]\n\nThis no longer change across time.\nIt is a model interesting on itself. It captures Between Effects."
  },
  {
    "objectID": "rmethods/11_advpanel.html#section",
    "href": "rmethods/11_advpanel.html#section",
    "title": "Advanced Panel Data",
    "section": "",
    "text": "Now, lets substract this from the original model:\n\n\\[y_{it}-\\bar y_{i} = \\beta_1 (x_{it}- \\bar x_{i}) + \\beta_2 (z_{it}- \\bar z_{i}) + e_{it} - \\bar \\epsilon_{i}\\] \\[\\tilde y_{it} = \\beta_1 \\tilde  x_{it} + \\beta_2 \\tilde  z_{it} + \\tilde \\epsilon_{it}\\]\n\nWhat we have just done is apply the within transformation. The model above now captures the relationship between \\(X's\\) and \\(Y's\\) using only changes within each individual.\n\nThis “ignores” variation across individuals.\n\nThis within transformation eliminates all time-invariant factors, including \\(\\alpha_i\\).\n\nAlso of interest: * This model could now be estimated using OLS * Its an application of the FWL theorem. (we partial out the time-invariant factors) * If done by OLS, you need to correct the Degrees of freedom. (NT-N-k)"
  },
  {
    "objectID": "rmethods/11_advpanel.html#expanding-the-model-time-fixed-effects",
    "href": "rmethods/11_advpanel.html#expanding-the-model-time-fixed-effects",
    "title": "Advanced Panel Data",
    "section": "Expanding the model: Time fixed effects",
    "text": "Expanding the model: Time fixed effects\n\nNow, lets consider the following model:\n\n\\[y_{it} = \\beta_1 x_{it} + \\beta_2 z_{it} + \\alpha_i + \\delta_t + \\epsilon_{it}\\]\n\nWe could apply the same transformation as before, but now we need to consider the \\(\\delta_t\\).\n\nTypically, the number of time periods is small, and we could control for them using dummies. (need to be explicit about it)\nAltenativelly, One may need to use a Douple Demeaning approach.\n\n\n\\[\\tilde y_{it} = y_{it} - \\bar y_i - \\bar y_t + \\bar y\\]\nwhere \\(\\bar y_t\\) is the average across individuals, and \\(\\bar y\\) is the overall average of \\(y_{it}\\).\n\nThis will work as intended if the panel is balanced."
  },
  {
    "objectID": "rmethods/11_advpanel.html#when-panel-is-not-balanced",
    "href": "rmethods/11_advpanel.html#when-panel-is-not-balanced",
    "title": "Advanced Panel Data",
    "section": "When Panel is not balanced:",
    "text": "When Panel is not balanced:\n\nIf panel is not balanced, you need to demean data using interative methods.\nLets assume that \\(\\bar y=0\\). We would need to demean the data many times as follows:\n\n\\[ \\overline{ty}_{it} = y_{it} - \\bar y_i - \\bar y_t \\] \\[ \\overline{tty}_{it} = \\overline{ty}_{it} - \\overline{ty}_i - \\overline{ty}_t \\] \\[ \\overline{ttty}_{it} = \\overline{tty}_{it} - \\overline{tty}_i - \\overline{tty}_t \\]\nSo on and so forth, until there is no more variation in the transformed data.\n\ni.e. \\(\\overline{t \\dots ty}_i = \\overline{t\\dots t y}_t=0\\)\n\nNOTE: There are more efficient ways to do this."
  },
  {
    "objectID": "rmethods/11_advpanel.html#fe-vs-fd-balance-panel",
    "href": "rmethods/11_advpanel.html#fe-vs-fd-balance-panel",
    "title": "Advanced Panel Data",
    "section": "FE vs FD: Balance Panel",
    "text": "FE vs FD: Balance Panel\n\nFE and FD both aim to estimate the model by “eliminating” individual effects \\(\\alpha_i\\).\nWith \\(T=2\\), both will give you the same results.\nWith \\(T\\geq 3\\), you may need to choose based on assumptions on the error\n\nif \\(e_{it}\\) is serially uncorrelated, then FE is more efficient. Otherwise, FD may be better (if correlation is strong)\n\nOtherwise, typical suggestionis to try both, and evaluate the results.\nIn general, there are few arguments to choose between FD and FE.\n\nEmpirically, People use FE, because its the default in most software."
  },
  {
    "objectID": "rmethods/11_advpanel.html#fe-vs-fd-unbalanced-panel",
    "href": "rmethods/11_advpanel.html#fe-vs-fd-unbalanced-panel",
    "title": "Advanced Panel Data",
    "section": "FE vs FD: Unbalanced Panel",
    "text": "FE vs FD: Unbalanced Panel\n\nUnbalance panel data occures when different units are observed over different time periods.\n\nSome periods may or may not overlap, some may skip periods, etc\nIt may be more important understanding why one observes this kind of missing data problem.\n\nIf this is the case FD may be more difficult to use, because it requires data with regular time gaps. (Observations with missing data may be dropped)\nWith FE, you make most use of available data. Only those with “singletons” (units observed only once) are dropped."
  },
  {
    "objectID": "rmethods/11_advpanel.html#random-effects-models",
    "href": "rmethods/11_advpanel.html#random-effects-models",
    "title": "Advanced Panel Data",
    "section": "Random Effects Models",
    "text": "Random Effects Models\n\nEven if not done by hand, FE estimation is very computationally intensive and inneficient, because it requires estimating a large set of coefficients for indivuals.\n\nThis, however, its important if we believe that \\(\\alpha_i\\) are correlated with \\(x_{it}\\).\n\nIf \\(\\alpha_i\\) were uncorrelated with \\(x_{it}\\), then we could use a more efficient Approach: Random Effects model.\n\n\\(Corr(\\alpha_i, x_{it})=0\\) can be a very hard assumption to make.\n\nIf this is the case, we could estimate the model using OLS or Pool-OLS. Both would be consistent.\n\nHowever, the standard errors would be biased, because of the correlation across errors.\n\n\n\\[Corr(e_{it}+a_i,e{is}+a_i)=\\frac{\\sigma^2_a}{\\sigma^2_a+\\sigma^2_e}\\]"
  },
  {
    "objectID": "rmethods/11_advpanel.html#random-effects-models-se-estimation",
    "href": "rmethods/11_advpanel.html#random-effects-models-se-estimation",
    "title": "Advanced Panel Data",
    "section": "Random Effects Models: SE estimation",
    "text": "Random Effects Models: SE estimation\n\nThere are two ways to estimate the standard errors in a Random Effects model:\n\nOne could be to apply “clustered-robust” standard errors, using the individual as the cluster.\n\nIts a genereric solution to Clustering…Specially appropriate if we do not know how Clustering happens. (but we know\n\nThe second One is apply GLS. Since we know the “theoretical” correlation across errors, we could use this to transform the data, and estimate SE."
  },
  {
    "objectID": "rmethods/11_advpanel.html#section-1",
    "href": "rmethods/11_advpanel.html#section-1",
    "title": "Advanced Panel Data",
    "section": "",
    "text": "First, define:\n\\[\\theta = 1- \\left[ \\frac{\\sigma^2_e}{\\sigma^2_e+T \\sigma^2_a}\\right]^{1/2}\\]\n\nAll variables in the model (inclulding the constant) should be transformed using a quasi-differentiation as follows:\n\n\\[\\tilde w_{it} = w_{it} - \\theta \\bar w_i\\]\n\nThis transformation will eliminate the correlation across errors, and allow us to estimate the model using OLS. \\[y_{it}-\\theta \\bar y_i = \\beta_0 (1-\\theta) + b_1 (x_{it} - \\theta \\bar x_i) + b_2 (z_{it} - \\theta \\bar x_i) + v_{it} - \\theta \\bar v_i\\]"
  },
  {
    "objectID": "rmethods/11_advpanel.html#section-2",
    "href": "rmethods/11_advpanel.html#section-2",
    "title": "Advanced Panel Data",
    "section": "",
    "text": "Last pieces of the puzzle:\n\nEstimate the main model using pool OLS. \\(y_{it}=\\beta_0 + \\beta_1 x_{it} + v_{it}\\)\nEstimate \\(\\sigma^2_a\\) as: \\(\\hat \\sigma^2_a = \\frac{\\sum_{i=1}^N \\sum_{t=1}^{T-1}\\sum_{s=t+1}^{T} \\hat v_{it} \\hat v_{is}}{NT(T-1)/2 - (k+1)}\\)\nEstimate \\(\\sigma^2_e\\) as: \\(\\hat \\sigma_e^2 = \\hat\\sigma^2_v - \\hat \\sigma^2_a\\)\n\n\nBiggest Advantage of RE model is that you can now obtain effects for Time-invariant variables.\nIt is also more efficient, because you do not need to estimate individual effects, just capture the distribution of \\(\\alpha_i\\)."
  },
  {
    "objectID": "rmethods/11_advpanel.html#example",
    "href": "rmethods/11_advpanel.html#example",
    "title": "Advanced Panel Data",
    "section": "Example",
    "text": "Example\nIn Stata, you could estimate the panel models using the xtreg command.\nThis command has options for Fixed Effects, Between Effects and Random Effects.\n\nfrause wagepan, clear\n** Good idea to Set the data as panel data\nxtset nr year\n\n\n\n\n\nPanel variable: nr (strongly balanced)\n Time variable: year, 1980 to 1987\n         Delta: 1 unit\n\n\n\n** Pool OLS\nqui: reg lwage educ black hisp exper expersq married union,\nest sto m1\n** pool OLS with Clustered SE\nqui: reg lwage educ black hisp exper expersq married union, cluster(nr)\nest sto m2\n** Random Effects: Default\nqui:xtreg lwage educ black hisp exper expersq married union, re\nest sto m3\n** Fixed Effects\nqui:xtreg lwage educ black hisp exper expersq married union, fe\nest sto m4\nesttab m1 m2 m3 m4, se b(4) noomit nonumber mtitle(OLS OLS_CL RE FE)\n\n\n----------------------------------------------------------------------------\n                      OLS          OLS_CL              RE              FE   \n----------------------------------------------------------------------------\neduc               0.0994***       0.0994***       0.1012***                \n                 (0.0047)        (0.0092)        (0.0089)                   \n\nblack             -0.1438***      -0.1438**       -0.1441**                 \n                 (0.0236)        (0.0501)        (0.0476)                   \n\nhisp               0.0157          0.0157          0.0202                   \n                 (0.0208)        (0.0392)        (0.0426)                   \n\nexper              0.0892***       0.0892***       0.1121***       0.1168***\n                 (0.0101)        (0.0124)        (0.0083)        (0.0084)   \n\nexpersq           -0.0028***      -0.0028**       -0.0041***      -0.0043***\n                 (0.0007)        (0.0009)        (0.0006)        (0.0006)   \n\nmarried            0.1077***       0.1077***       0.0628***       0.0453*  \n                 (0.0157)        (0.0261)        (0.0168)        (0.0183)   \n\nunion              0.1801***       0.1801***       0.1074***       0.0821***\n                 (0.0171)        (0.0276)        (0.0178)        (0.0193)   \n\n_cons             -0.0347         -0.0347         -0.1075          1.0649***\n                 (0.0646)        (0.1201)        (0.1107)        (0.0267)   \n----------------------------------------------------------------------------\nN                    4360            4360            4360            4360   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/11_advpanel.html#pool-ols-vs-re-vs-fe",
    "href": "rmethods/11_advpanel.html#pool-ols-vs-re-vs-fe",
    "title": "Advanced Panel Data",
    "section": "Pool OLS vs RE vs FE",
    "text": "Pool OLS vs RE vs FE\n\nWe now know how to analyze panel data using three Stretegies: Pool OLS, Fixed Effects and, Random Effects.\n\nFE is usually prefered to RE, because is more consistent by relaxing the assumption of no correlation between \\(a_i\\) and \\(x_{it}\\) (explicit control). Its less efficient.\nRE may be prefer to FE if the correlation between \\(a_i\\) and \\(x_{it}\\) is small. Its more efficient, and allows to estimate effects for time-invariant variables.\nRE and POLS will be consistent under the same assumptions. However, RE will remove some of the serial correlation, and may have less bias than OLS (even if \\(a_i\\) and \\(x_{it})\\) are correlated.\n\nChoosing between RE and POLs is rarely considered. (RE would be the default in most cases)\nHowever, Choosing between FE vs RE is common: Hausman Test"
  },
  {
    "objectID": "rmethods/11_advpanel.html#hausman-test",
    "href": "rmethods/11_advpanel.html#hausman-test",
    "title": "Advanced Panel Data",
    "section": "Hausman Test",
    "text": "Hausman Test\n\nHausman test is used to determine which model to use between two estimators.\n\nYou assume FE is consistent (but not efficient).\nYou estimate the model using RE. If RE estimates are close to FE, then RE is consistent and efficient (preferred)\nOthewise, we suspect RE are inconsistent, and we use FE.\n\nFor most applied work, however, FE is generally prefered to RE\n\n\n** Hausman Test\n*** Consistent model FE\nqui:xtreg lwage educ black hisp exper expersq married union, fe\nest sto fe\n*** Efficient under H0\nqui:xtreg lwage educ black hisp exper expersq married union, re\nest sto re\nhausman fe re\n\n\n                 ---- Coefficients ----\n             |      (b)          (B)            (b-B)     sqrt(diag(V_b-V_B))\n             |       fe           re         Difference       Std. err.\n-------------+----------------------------------------------------------------\n       exper |    .1168467     .1121195        .0047272        .0016276\n     expersq |   -.0043009    -.0040689        -.000232        .0001269\n     married |    .0453033     .0627951       -.0174918        .0073427\n       union |    .0820871     .1073789       -.0252917        .0073636\n------------------------------------------------------------------------------\n                          b = Consistent under H0 and Ha; obtained from xtreg.\n           B = Inconsistent under Ha, efficient under H0; obtained from xtreg.\n\nTest of H0: Difference in coefficients not systematic\n\n    chi2(4) = (b-B)'[(V_b-V_B)^(-1)](b-B)\n            =  31.45\nProb &gt; chi2 = 0.0000"
  },
  {
    "objectID": "rmethods/11_advpanel.html#correlated-random-effects",
    "href": "rmethods/11_advpanel.html#correlated-random-effects",
    "title": "Advanced Panel Data",
    "section": "Correlated Random Effects",
    "text": "Correlated Random Effects\n\nThe CRE model is an alternative approach that combines some of the features of RE and FE estimators. \\[y_{it} = \\beta_1 x_{it} + \\beta_2 z_{i} + \\alpha_i + \\epsilon_{it}\\]\nOne way to look at the “individual” fixed effect is to model it as a function of fixed effects:\n\n\\[\\alpha_i = \\alpha + \\gamma_1 \\bar x_{i} + \\gamma_2 z_{i} + r_i\\]\n\nIn this case, we assume the fixed unobserved effect \\(\\alpha_i\\) could be written as a function of avg observed characteristics, and fixed factors.\nAnd we assume that \\(r_i\\) would be uncorrelated with \\(x_{it}\\) , \\(\\bar x_i\\) and \\(z_{i}\\)."
  },
  {
    "objectID": "rmethods/11_advpanel.html#section-3",
    "href": "rmethods/11_advpanel.html#section-3",
    "title": "Advanced Panel Data",
    "section": "",
    "text": "If we combine this with the main regression we have:\n\n\\[y_{it} = \\beta_1 x_{it} + \\beta_2 z_{i} + \\alpha + \\gamma_1 \\bar x_{i} + \\gamma_2 z_{i} + r_i + \\epsilon_{it}\\] \\[y_{it} = \\alpha + \\beta_1 x_{it} + \\beta_2 z_{i} + \\gamma_1 \\bar x_{i} + \\underbrace{\\nu_{it}}_{ r_i + \\epsilon_{it}}\\]\n\nWhich we could now estimate using Pool OLS or RE. There is no more need to worry about correlation between \\(r_i\\) and \\(x_{it}\\).\nDifferences and Advantages:\n\nWe must include individual level average characteristics.\nWe can now estimate effects for time-invariant variables.\nWe can test for FE vs RE models."
  },
  {
    "objectID": "rmethods/11_advpanel.html#correlated-random-effects-fe-vs-re",
    "href": "rmethods/11_advpanel.html#correlated-random-effects-fe-vs-re",
    "title": "Advanced Panel Data",
    "section": "Correlated Random Effects: FE vs RE",
    "text": "Correlated Random Effects: FE vs RE\n\nCRE estimates for Time varying variables are identical to FE. \\[\\hat \\beta_{cre}=\\hat \\beta_{fe}\\]\nCRE estimates shows clearly why RE are more efficient (RE imposes \\(\\gamma=0\\))\nThus, we can test for FE vs RE using the following test:\n\n\\[H_0: \\gamma = 0 \\text{ or } RE \\] \\[H_a: \\gamma \\neq 0 \\text{ or } FE \\]"
  },
  {
    "objectID": "rmethods/11_advpanel.html#example-1",
    "href": "rmethods/11_advpanel.html#example-1",
    "title": "Advanced Panel Data",
    "section": "Example",
    "text": "Example\n\nforeach i of varlist exper expersq married union {\n  bysort nr: egen mn_`i'=mean(`i')\n}\nxtreg lwage educ black hisp exper expersq married union mn_*, re\nest sto m2\n** FE vs RE\ntest mn_exper mn_expersq mn_married mn_union\n\n\nRandom-effects GLS regression                   Number of obs     =      4,360\nGroup variable: nr                              Number of groups  =        545\n\nR-squared:                                      Obs per group:\n     Within  = 0.1780                                         min =          8\n     Between = 0.2192                                         avg =        8.0\n     Overall = 0.2002                                         max =          8\n\n                                                Wald chi2(11)     =     976.27\ncorr(u_i, X) = 0 (assumed)                      Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0946036   .0109043     8.68   0.000     .0732315    .1159757\n       black |  -.1388124   .0488709    -2.84   0.005    -.2345977   -.0430271\n        hisp |   .0047758   .0426925     0.11   0.911    -.0788999    .0884515\n       exper |   .1168467   .0084197    13.88   0.000     .1003444     .133349\n     expersq |  -.0043009   .0006053    -7.11   0.000    -.0054872   -.0031146\n     married |   .0453033   .0183097     2.47   0.013      .009417    .0811896\n       union |   .0820871   .0192907     4.26   0.000      .044278    .1198963\n    mn_exper |  -.1672838    .051032    -3.28   0.001    -.2673046    -.067263\n  mn_expersq |   .0094254   .0032684     2.88   0.004     .0030195    .0158312\n  mn_married |   .0983604   .0450837     2.18   0.029     .0099979    .1867228\n    mn_union |   .1885894   .0504022     3.74   0.000     .0898029    .2873759\n       _cons |    .492309   .2210094     2.23   0.026     .0591386    .9254794\n-------------+----------------------------------------------------------------\n     sigma_u |  .32456727\n     sigma_e |  .35125535\n         rho |  .46057172   (fraction of variance due to u_i)\n------------------------------------------------------------------------------\n\n ( 1)  mn_exper = 0\n ( 2)  mn_expersq = 0\n ( 3)  mn_married = 0\n ( 4)  mn_union = 0\n\n           chi2(  4) =   27.27\n         Prob &gt; chi2 =    0.0000\n\n\nComparing across models:\n\nqui:xtreg lwage educ black hisp exper expersq married union, fe\nest sto m1\nqui:xtreg lwage educ black hisp exper expersq married union, re\nest sto m3\nesttab m1 m2 m3, se b(4) noomit nonumber mtitle(FE CRE RE)\n\n\n------------------------------------------------------------\n                       FE             CRE              RE   \n------------------------------------------------------------\nexper              0.1168***       0.1168***       0.1121***\n                 (0.0084)        (0.0084)        (0.0083)   \n\nexpersq           -0.0043***      -0.0043***      -0.0041***\n                 (0.0006)        (0.0006)        (0.0006)   \n\nmarried            0.0453*         0.0453*         0.0628***\n                 (0.0183)        (0.0183)        (0.0168)   \n\nunion              0.0821***       0.0821***       0.1074***\n                 (0.0193)        (0.0193)        (0.0178)   \n\neduc                               0.0946***       0.1012***\n                                 (0.0109)        (0.0089)   \n\nblack                             -0.1388**       -0.1441** \n                                 (0.0489)        (0.0476)   \n\nhisp                               0.0048          0.0202   \n                                 (0.0427)        (0.0426)   \n\nmn_exper                          -0.1673**                 \n                                 (0.0510)                   \n\nmn_expersq                         0.0094**                 \n                                 (0.0033)                   \n\nmn_married                         0.0984*                  \n                                 (0.0451)                   \n\nmn_union                           0.1886***                \n                                 (0.0504)                   \n\n_cons              1.0649***       0.4923*        -0.1075   \n                 (0.0267)        (0.2210)        (0.1107)   \n------------------------------------------------------------\nN                    4360            4360            4360   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/11_advpanel.html#cre-implementation",
    "href": "rmethods/11_advpanel.html#cre-implementation",
    "title": "Advanced Panel Data",
    "section": "CRE Implementation",
    "text": "CRE Implementation\n\nAs shown above, in Stata, you could estimate the CRE panel models using the xtreg with RE option.\n\nYou just need to be careful when estimating the averages of all variables in the model.\nThis is particularly relevant for unbalance panel data.\nIn those cases, you could use cre (from fra install)\n\nYou could also extend this to using Multiple fixed effects (time and individual), but some equivalences are lost.\n\n\nfra install cre\ncre , abs(nr): xtreg lwage educ black hisp exper expersq married union, re \n\nchecking cre consistency and verifying not already installed...\nall files already exist and are up to date.\n\nRandom-effects GLS regression                   Number of obs     =      4,360\nGroup variable: nr                              Number of groups  =        545\n\nR-squared:                                      Obs per group:\n     Within  = 0.1780                                         min =          8\n     Between = 0.2192                                         avg =        8.0\n     Overall = 0.2002                                         max =          8\n\n                                                Wald chi2(11)     =     976.27\ncorr(u_i, X) = 0 (assumed)                      Prob &gt; chi2       =     0.0000\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0946036   .0109043     8.68   0.000     .0732315    .1159757\n       black |  -.1388124   .0488709    -2.84   0.005    -.2345977   -.0430271\n        hisp |   .0047758   .0426925     0.11   0.911    -.0788999    .0884515\n       exper |   .1168467   .0084197    13.88   0.000     .1003444     .133349\n     expersq |  -.0043009   .0006053    -7.11   0.000    -.0054872   -.0031146\n     married |   .0453033   .0183097     2.47   0.013      .009417    .0811896\n       union |   .0820871   .0192907     4.26   0.000      .044278    .1198963\n    m1_exper |  -.1672838    .051032    -3.28   0.001    -.2673046    -.067263\n  m1_expersq |   .0094254   .0032684     2.88   0.004     .0030195    .0158312\n  m1_married |   .0983604   .0450837     2.18   0.029     .0099979    .1867228\n    m1_union |   .1885894   .0504022     3.74   0.000     .0898029    .2873759\n       _cons |  -.0330167   .1330654    -0.25   0.804    -.2938202    .2277867\n-------------+----------------------------------------------------------------\n     sigma_u |  .32456727\n     sigma_e |  .35125535\n         rho |  .46057172   (fraction of variance due to u_i)\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/11_advpanel.html#final-words",
    "href": "rmethods/11_advpanel.html#final-words",
    "title": "Advanced Panel Data",
    "section": "Final words",
    "text": "Final words\n\nAll estimation methodologies presented here could also be used in other contexts.\nExamples:\n\nGeronimus and Korenman (1992): Analysis of data siblings outcome accounting for family fixed effects.\nAshenfelder and Kruger (1994): Return to education using Twins Data.\n\nOne may also use the principles of Panel data with linked data in high dimensional data sets.\n\nEducation data and School FE\nHealth data and Hospital FE\nWages and Firm FE\netc.\n\nIn this cases, one may need to also considering explicit clustering in addition to “fixed effects”."
  },
  {
    "objectID": "rm-data/homework.html",
    "href": "rm-data/homework.html",
    "title": "HomeWorks",
    "section": "",
    "text": "This document provides the homework for the each week of the course."
  },
  {
    "objectID": "rm-data/homework.html#hw01",
    "href": "rm-data/homework.html#hw01",
    "title": "HomeWorks",
    "section": "Homework 1",
    "text": "Homework 1\nChoose one of the following topics, and create a PDF version of it using Quarto. For this, create a new repository in your GitHub account, this repository should be named homework_1, and contain the html file that you will use to create the PDF.\nFor this homework use the following resources\n\nTemplate: template\n\nJust copy the Heading of this file in your html file\n\nBibliography: reference.bib\n\nAdd this in the same folder as your html file\n\nAll figures, if any, can be saved as PNG or JPG files from the linked pages.\nTables, if any, may have to be replicated using markdown tables.\n\n\nTopics\n\nThe Impact of Resource Management in StarCraft: A Strategic Analysis html pdf\nThe Strategic Depth of StarCraft and Its Esports Legacy html pdf\nThe Mathematics of Dungeons and Dragons: A Statistical Adventure html pdf\nThe Rise of LitRPG: Blending Literature and Gaming html pdf\nThe Impact of ‘The Good Guys’ on Modern Fantasy Literature html pdf\nEconomic Dynamics in Eric Ugland’s ‘The Good Guys’ Series html pdf\nThe Impact of House Allegiances on Power Dynamics in Westeros html pdf"
  },
  {
    "objectID": "quizes/quiz_2.html",
    "href": "quizes/quiz_2.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Name: ________________________________________________________\n\nConsider the following regression equation: \\(y=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+u\\). What does \\(\\beta_1\\) imply?\n\n\nIt measures the ceteris paribus effect of \\(x_1\\) on \\(x_2\\) .\nIt measures the ceteris paribus effect of \\(y\\) on \\(x_1\\)\nIt measures the ceteris paribus effect of \\(x_1\\) on \\(y\\)\nIt measures the ceteris paribus effect of \\(x_1\\) on \\(u\\)\n\n\nIn econometrics, the general partialling out result is usually called the _____.\n\n\nGauss-Markov assumption\nBest linear unbiased estimator\nFrisch-Waugh-Lovell theorem\nGauss-Markov theorem\n\n\nIf an independent variable in a multiple linear regression model is an exact linear combination of other independent variables, the model suffers from the problem of _____.\n\n\nperfect collinearity\nhomoskedasticity\nheteroskedasticty\nomitted variable bias\n\n\nThe term “linear” in a multiple linear regression model means that the equation is linear in parameters, not in terms of variables.\n\n\nTrue\nFalse\n\n\nThe coefficient of determination (R2) decreases when an independent variable is added to a multiple regression model.\n\n\nTrue\nFalse"
  },
  {
    "objectID": "quizes/quiz_2.html#quiz-2",
    "href": "quizes/quiz_2.html#quiz-2",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Name: ________________________________________________________\n\nConsider the following regression equation: \\(y=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+u\\). What does \\(\\beta_1\\) imply?\n\n\nIt measures the ceteris paribus effect of \\(x_1\\) on \\(x_2\\) .\nIt measures the ceteris paribus effect of \\(y\\) on \\(x_1\\)\nIt measures the ceteris paribus effect of \\(x_1\\) on \\(y\\)\nIt measures the ceteris paribus effect of \\(x_1\\) on \\(u\\)\n\n\nIn econometrics, the general partialling out result is usually called the _____.\n\n\nGauss-Markov assumption\nBest linear unbiased estimator\nFrisch-Waugh-Lovell theorem\nGauss-Markov theorem\n\n\nIf an independent variable in a multiple linear regression model is an exact linear combination of other independent variables, the model suffers from the problem of _____.\n\n\nperfect collinearity\nhomoskedasticity\nheteroskedasticty\nomitted variable bias\n\n\nThe term “linear” in a multiple linear regression model means that the equation is linear in parameters, not in terms of variables.\n\n\nTrue\nFalse\n\n\nThe coefficient of determination (R2) decreases when an independent variable is added to a multiple regression model.\n\n\nTrue\nFalse"
  },
  {
    "objectID": "quarto/table1.html",
    "href": "quarto/table1.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Male\nFemale\n\n\n\n\nN\n759 (46.1%)\n888 (53.9%)\n\n\nlog hourly wages\n3.440 (0.479)\n3.267 (0.570)\n\n\nyears of education\n11.800 (2.444)\n11.060 (2.260)\n\n\nyears of work experience\n14.077 (11.180)\n12.138 (8.327)\n\n\nyears of job tenure\n9.003 (9.061)\n6.605 (6.727)\n\n\nage of respondent\n38.516 (11.341)\n39.884 (10.727)\n\n\nMarital Status\n\n\n\n\nSingle\n297 (39.1%)\n268 (30.2%)\n\n\nMarried\n397 (52.3%)\n465 (52.4%)\n\n\nDivorced\n65 (8.6%)\n155 (17.5%)"
  },
  {
    "objectID": "quarto/regress.html",
    "href": "quarto/regress.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "(1)\n(2)\n(3)\n(4)\n\n\n\n\nyears of education\n0.0885***\n0.0794***\n0.0554***\n0.0854***\n\n\n\n(0.00519)\n(0.00522)\n(0.00613)\n(0.00876)\n\n\nyears of work\n0.0153***\n0.00399*\n-0.00483*\n0.00874*\n\n\nexperience\n(0.00126)\n(0.00188)\n(0.00203)\n(0.00356)\n\n\nyears of job tenure\n\n0.00407*\n-0.000553\n0.00150\n\n\n\n\n(0.00196)\n(0.00212)\n(0.00368)\n\n\nage of respondent\n\n0.0114***\n0.0249***\n0.00576*\n\n\n\n\n(0.00174)\n(0.00224)\n(0.00275)\n\n\nConstant\n2.136***\n1.915***\n1.903***\n1.965***\n\n\n\n(0.0654)\n(0.0727)\n(0.0784)\n(0.126)\n\n\nObservations\n1434\n1434\n751\n683"
  },
  {
    "objectID": "mathref/math_3.html",
    "href": "mathref/math_3.html",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "",
    "text": "A random variable is a variable whose value is determined by the outcome of a random experiment. For example, if we toss a coin, the outcome is random, but the possible values of \\(X\\) are 0 and 1. If we roll a die, the outcome is random with possible values 1, 2, 3, 4, 5, and 6.\nThere are two kinds of random variables:\n\nDiscrete random variables can only take on a finite number of values. For example, the number of heads in 10 coin tosses is a discrete random variable.\nContinuous random variables can take on any value in a range. For example, the height of a randomly selected person is a continuous random variable.\n\nIf \\(X\\) is discrete random variable, then \\(P(X=c)\\) is the probability that \\(X\\) takes on the value \\(c\\). It can be any value between 0 and 1.\nBy definition, the sum of all probabilities for all feasible values of \\(X\\) is 1. That is, \\(\\sum_{c} P(X=c)=1\\).\nIf \\(X\\) is continuous random variable, then \\(P(X=c)=0\\) for any value \\(c\\). The probability to observe a particular number is zero. Instead, when using continuous data, we focus on the probability of observing a value in a range. For example, \\(P(1.7 \\leq X \\leq 1.8)\\) is the probability that \\(X\\) is between 1.7 and 1.8, which can be any value between 0 and 1.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Statistics and Probability"
    ]
  },
  {
    "objectID": "mathref/math_3.html#random-variables",
    "href": "mathref/math_3.html#random-variables",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "",
    "text": "A random variable is a variable whose value is determined by the outcome of a random experiment. For example, if we toss a coin, the outcome is random, but the possible values of \\(X\\) are 0 and 1. If we roll a die, the outcome is random with possible values 1, 2, 3, 4, 5, and 6.\nThere are two kinds of random variables:\n\nDiscrete random variables can only take on a finite number of values. For example, the number of heads in 10 coin tosses is a discrete random variable.\nContinuous random variables can take on any value in a range. For example, the height of a randomly selected person is a continuous random variable.\n\nIf \\(X\\) is discrete random variable, then \\(P(X=c)\\) is the probability that \\(X\\) takes on the value \\(c\\). It can be any value between 0 and 1.\nBy definition, the sum of all probabilities for all feasible values of \\(X\\) is 1. That is, \\(\\sum_{c} P(X=c)=1\\).\nIf \\(X\\) is continuous random variable, then \\(P(X=c)=0\\) for any value \\(c\\). The probability to observe a particular number is zero. Instead, when using continuous data, we focus on the probability of observing a value in a range. For example, \\(P(1.7 \\leq X \\leq 1.8)\\) is the probability that \\(X\\) is between 1.7 and 1.8, which can be any value between 0 and 1.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Statistics and Probability"
    ]
  },
  {
    "objectID": "mathref/math_3.html#probability-distributions",
    "href": "mathref/math_3.html#probability-distributions",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nA probability distribution is a function that assigns probabilities to the values of a random variable. For discrete random variables, we can use a table to describe the probability distribution. For example, the probability distribution of the number of heads in 5 coin tosses is:\n\n\n\nNumber of heads\nProbability\n\n\n\n\n0\n0.03125\n\n\n1\n0.15625\n\n\n2\n0.3125\n\n\n3\n0.3125\n\n\n4\n0.15625\n\n\n5\n0.03125\n\n\n\nIn this case, the sum of all probabilities is 1.\nFor continuous random variables, we can use a function to describe the probability distribution. For example, we can say that the probability distribution of the height of a randomly selected person is:\n\\[f(x)\\]\nThis function has important properties:\n\n\\(f(x) \\geq 0\\) for all \\(x\\).\n\\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\).\n\\(P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) dx\\).\n\\(P(X \\leq a) + P(X &gt; a) = 1\\).\n\\(P(a \\leq X \\leq b) = P(X &lt; b) - P(X &lt; a)\\).",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Statistics and Probability"
    ]
  },
  {
    "objectID": "mathref/math_3.html#joint-probability-distributions",
    "href": "mathref/math_3.html#joint-probability-distributions",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Joint Probability Distributions",
    "text": "Joint Probability Distributions\nThe joint probability distribution of \\(X\\) and \\(Y\\) is a function that assigns probabilities to the values of \\(X\\) and \\(Y\\). For discrete random variables, we can use a table to describe the joint probability distribution. For continuous variables, it must be the case that:\n\\[\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x,y) dx dy = 1\\]",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Statistics and Probability"
    ]
  },
  {
    "objectID": "mathref/math_3.html#marginal-probability-distributions",
    "href": "mathref/math_3.html#marginal-probability-distributions",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Marginal Probability Distributions",
    "text": "Marginal Probability Distributions\nThe marginal probability distribution of \\(X\\) is the probability distribution of \\(X\\) ignoring the values of \\(Y\\). This can be expressed as:\n\\[P(x) = \\sum_{z=-\\infty}^{\\infty} P(x,y=z)\\]\nit still must be the case that\n\\[ \\sum_{w=-\\infty}^{\\infty}\\sum_{z=-\\infty}^{\\infty} P(x=w,y=z)=1\\]\nFor continuous random variables, we have the following \\[\\int_{-\\infty}^{\\infty} f(x) dx = 1\\]\nwhere \\(f(x)\\) is the marginal probability distribution of \\(X\\). What is left after we “integrate out” \\(Y\\) is the marginal probability distribution of \\(X\\).\n\\[f(x) = \\int_{-\\infty}^{\\infty} f(x,y) dy\\]",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Statistics and Probability"
    ]
  },
  {
    "objectID": "mathref/math_3.html#independence",
    "href": "mathref/math_3.html#independence",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Independence",
    "text": "Independence\nTwo random variables \\(X\\) and \\(Y\\) are independent if and only if:\n\\[P(x,y) = P(x)P(y) or f(x,y)=f(x)*f(y)\\]",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Statistics and Probability"
    ]
  },
  {
    "objectID": "mathref/math_3.html#conditional-probability",
    "href": "mathref/math_3.html#conditional-probability",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\nThe conditional probability of \\(X\\) given \\(Y\\) is:\n\\[P(x|y) = \\frac{P(x,y)}{P(y)}\\]\nor, the conditional probabilty density function:\n\\[f(x|y) = \\frac{f(x,y)}{f(y)}\\]\nAnd if \\(X\\) and \\(Y\\) are independent, then:\n\\(P(x|y) = P(x)\\) or \\(f(x|y) = f(x)\\).",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Statistics and Probability"
    ]
  },
  {
    "objectID": "mathref/math_3.html#mean-and-variance",
    "href": "mathref/math_3.html#mean-and-variance",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Mean, and variance",
    "text": "Mean, and variance\nThe mean of a random variable \\(X\\) is:\n\\[E(X) = \\sum_{x} xP(x)\\] or \\[E(X) = \\int_{-\\infty}^{\\infty} xf(x) dx\\]\nWhich is a weighted sum of all possible values of \\(X\\), and where the weights are the probabilities (or densities) of each value. It can also be written or referred as:\n\\[E(X), \\mu_x , \\bar x\\]\nThis measure is also called the expected value of \\(X\\), and proviveds a measure of the “center” of the distribution of \\(X\\). It can be very sensitive to outliers.\nThe variance of a random variable \\(X\\) is:\n\\[Var(X) = E[(X-E(X))^2] \\]\n\\[Var(X) = \\sum_{x} (X-E(X))^2 P(x) \\] or\n\\[Var(X) = \\int_{x} (X-E(X))^2 f(x) dx \\]\nWhich is the expected value of the squared difference between \\(X\\) and its mean. It provides a measure of average the “spread” of the distribution of \\(X\\).\nIt could also be defined as follows:\n\\[\\sigma^2_x = Var(x) = E(X^2) - [E(X)]^2\\]\nThere are other measures that can be used to characterize a distribution, such as the median, the mode, the skewness, and the kurtosis. They are defined as follows:\n\nThe median is the value of \\(X\\) such that \\(P(X \\leq x) = 0.5\\).\nThe mode is the value of \\(X\\) that maximizes \\(P(X=x)\\).\nThe skewness is a measure of the asymmetry of the distribution of \\(X\\). It is defined as:\n\n\\[\\frac{E[(X-E(X))^3]}{[Var(X)]^{3/2}}\\]\n\nThe kurtosis is a measure of the “peakedness” of the distribution of \\(X\\). It is defined as:\n\n\\[\\frac{E[(X-E(X))^4]}{[Var(X)]^{2}}\\]\n\nThe quantiles of a distribution are values that divide the distribution into equal parts. For example, the 0.25 quantile is the value of \\(X\\) such that \\(P(X \\leq x) = 0.25\\).\n\nFor a normal distribution, the mean, median, and mode are all equal. The skewness is 0, and the kurtosis is 3.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Statistics and Probability"
    ]
  },
  {
    "objectID": "mathref/math_3.html#covariance-and-correlation",
    "href": "mathref/math_3.html#covariance-and-correlation",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Covariance and Correlation",
    "text": "Covariance and Correlation\nThe covariance of two random variables \\(X\\) and \\(Y\\) is:\n\\[Cov(X,Y) = E[(X-E(X))(Y-E(Y))]\\]\n\\[Cov(X,Y) = \\sum_{x}\\sum_{y} (x-E(X))(y-E(Y))P(x,y)\\]\n\\[Cov(X,Y) = \\int_{x}\\int_{y} (x-E(X))(y-E(Y))f(x,y)\\]\nThe covariance measures the linear association between \\(X\\) and \\(Y\\). If \\(X\\) and \\(Y\\) are independent, then \\(Cov(X,Y)=0\\). However, if \\(Cov(X,Y)=0\\), then \\(X\\) and \\(Y\\) are not necessarily independent. For example \\(y=(x-E(X))^2\\) and \\(x\\) are not independent, but \\(Cov(y,x)=0\\).\nThis measure is scale dependent. For example, if we measure \\(X\\) in meters, and \\(Y\\) in centimeters, then \\(Cov(X,Y)\\) will be 100 times larger than if we measure \\(X\\) in meters and \\(Y\\) in kilometers.\nAn alternative measure of association is the correlation coefficient, which is defined as:\n\\[Corr(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}\\] \\[\\rho_{X,Y} = \\frac{\\sigma_{X,Y}}{\\sigma_x \\sigma_y}\\]\nThis statistics is always between -1 and 1, regardless of the scale of \\(x\\) or \\(y\\).",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Statistics and Probability"
    ]
  },
  {
    "objectID": "mathref/math_3.html#propeties-of-mean-variance-and-covariance",
    "href": "mathref/math_3.html#propeties-of-mean-variance-and-covariance",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Propeties of Mean, Variance and Covariance",
    "text": "Propeties of Mean, Variance and Covariance\nConsider two random variables \\(X\\) and \\(Y\\), and let \\(a\\), \\(b\\), \\(c\\) and \\(d\\) be constants. Then:\n\n\\(Var(aX+b) = a^2Var(X)\\)\n\\(Cov(aX+b,cY+d) = acCov(X,Y)\\)\n\\(Var(aX+bY) = a^2Var(X) + b^2Var(Y) + 2abCov(X,Y)\\)\n\\(Cov(X,Y) = E(XY) - E(X)E(Y)\\)\n\\(Cov(X,X) = Var(X)\\)\n\nFor the mean:\n\n\\(E(aX+b) = aE(X)+b\\)\n\\(E(aX+bY) = aE(X) + bE(Y)\\)",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Statistics and Probability"
    ]
  },
  {
    "objectID": "mathref/math_3.html#some-useful-distributions",
    "href": "mathref/math_3.html#some-useful-distributions",
    "title": "Math Refresher: Basic Statistics and Probability",
    "section": "Some useful distributions",
    "text": "Some useful distributions\n\nDiscrete distributions\n\nBernoulli distribution: \\(X \\sim Bernoulli(p)\\), where \\(p=P(X=1)\\) and \\(1-p=P(X=0)\\). \\(E(X)=p\\) and variance \\(Var(X)=p(1-p)\\). Flip a coin with probability \\(p\\) of getting heads.\nBinomial distribution: \\(X \\sim Binomial(n,p)\\), where \\(p=P(X=1)\\) and \\(1-p=P(X=0)\\). \\(E(x)=np\\) and \\(Var(X)=np(1-p)\\). The binomial distribution is the distribution of the number of successes in \\(n\\) independent Bernoulli trials.\nPoisson distribution: \\(X \\sim Poisson(\\lambda)\\), where \\(\\lambda=E(X)=Var(x)\\). Typically used for counts. For example, the number of customers arriving at a store in a given hour.\n\n\n\nContinuous distributions\n\nUniform distribution: \\(X \\sim Uniform(a,b)\\), where \\(f(x)=\\frac{1}{b-a}\\) for \\(a \\leq x \\leq b\\), and \\(f(x)=0\\) otherwise. \\(E(X)=\\frac{a+b}{2}\\) and \\(Var(X)=\\frac{(b-a)^2}{12}\\). Time between bus arrivals.\nNormal distribution: \\(X \\sim Normal(\\mu,\\sigma^2)\\), where \\(f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\). \\(E(X)=\\mu\\) and \\(Var(X)=\\sigma^2\\). For example, the height of a randomly selected person.\nt-distribution: \\(X \\sim t(\\nu)\\), where \\(f(x)=\\frac{\\Gamma(\\frac{\\nu+1}{2})}{\\sqrt{\\nu\\pi}\\Gamma(\\frac{\\nu}{2})}(1+\\frac{x^2}{\\nu})^{-\\frac{\\nu+1}{2}}\\). \\(E(X)=0\\) if \\(\\nu&gt;1\\), and \\(Var(X)=\\frac{\\nu}{\\nu-2}\\) if \\(\\nu&gt;2\\). For example, the distribution of the sample mean of a small sample from a normal distribution.\nAlternatively. \\(X \\sim t(\\nu)\\), where \\(X=\\frac{Z}{\\sqrt{V/\\nu}}\\), where \\(Z \\sim Normal(0,1)\\) and \\(V \\sim \\chi^2(\\nu)\\), and \\(Z\\) and \\(V\\) are independent.\nChi-squared distribution: \\(X \\sim \\chi^2(\\nu)\\), where \\(f(x)=\\frac{1}{2^{\\nu/2}\\Gamma(\\nu/2)}x^{\\nu/2-1}e^{-x/2}\\). \\(E(X)=\\nu\\) and \\(Var(X)=2\\nu\\).\nAlternatively, \\(X \\sim \\chi^2(\\nu)\\), where \\(X=Z_1^2+Z_2^2+...+Z_\\nu^2\\), where \\(Z_i \\sim Normal(0,1)\\), and \\(Z_1\\), \\(Z_2\\), …, \\(Z_\\nu\\) are independent.\nF-distribution:\n\\(X \\sim F(\\nu_1,\\nu_2)\\), where \\(f(x)=\\frac{\\Gamma(\\frac{\\nu_1+\\nu_2}{2})}{\\Gamma(\\frac{\\nu_1}{2})\\Gamma(\\frac{\\nu_2}{2})}(\\frac{\\nu_1}{\\nu_2})^{\\nu_1/2}x^{\\nu_1/2-1}(1+\\frac{\\nu_1}{\\nu_2}x)^{-(\\nu_1+\\nu_2)/2}\\). \\(E(X)=\\frac{\\nu_2}{\\nu_2-2}\\) if \\(\\nu_2&gt;2\\), and \\(Var(X)=\\frac{2\\nu_2^2(\\nu_1+\\nu_2-2)}{\\nu_1(\\nu_2-2)^2(\\nu_2-4)}\\) if \\(\\nu_2&gt;4\\).\nAlternatively, \\(X \\sim F(\\nu_1,\\nu_2)\\), where \\(X=\\frac{V_1/\\nu_1}{V_2/\\nu_2}\\), where \\(V_1 \\sim \\chi^2(\\nu_1)\\) and \\(V_2 \\sim \\chi^2(\\nu_2)\\), and \\(V_1\\) and \\(V_2\\) are independent.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Statistics and Probability"
    ]
  },
  {
    "objectID": "mathref/math_1.html",
    "href": "mathref/math_1.html",
    "title": "Math Refresher: Basic Calculus",
    "section": "",
    "text": "This is a refresher on basic calculus. It is not meant to be a substitute for a full course on calculus, but rather a quick review of the basic concepts and techniques that will be used in this semester.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Calculus"
    ]
  },
  {
    "objectID": "mathref/math_1.html#introduction",
    "href": "mathref/math_1.html#introduction",
    "title": "Math Refresher: Basic Calculus",
    "section": "",
    "text": "This is a refresher on basic calculus. It is not meant to be a substitute for a full course on calculus, but rather a quick review of the basic concepts and techniques that will be used in this semester.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Calculus"
    ]
  },
  {
    "objectID": "mathref/math_1.html#limits",
    "href": "mathref/math_1.html#limits",
    "title": "Math Refresher: Basic Calculus",
    "section": "Limits",
    "text": "Limits\nThe limit of a function \\(f(x)\\) as \\(x\\) approaches \\(a\\) is the value that \\(f(x)\\) approaches as \\(x\\) gets closer and closer to \\(a\\). We write this as:\n\\[\\lim_{x \\to a} f(x) = L\\]\nIn this case, the limit of the function \\(f(x)\\) as \\(x\\) approaches \\(a\\) is \\(L\\). For example, consider the function \\(f(x) = x^2\\). The limit of \\(f(x)\\) as \\(x\\) approaches \\(2\\) is \\(4\\):",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Calculus"
    ]
  },
  {
    "objectID": "mathref/math_1.html#limits-to-derivatives",
    "href": "mathref/math_1.html#limits-to-derivatives",
    "title": "Math Refresher: Basic Calculus",
    "section": "Limits to Derivatives",
    "text": "Limits to Derivatives\nLimits can also be used to estimate derivatives. The derivative of a function \\(f(x)\\) is the slope of the function at a given point. The derivative of \\(f(x)\\) at \\(x = a\\) is written as \\(f'(a)\\). The derivative of \\(f(x)\\) is defined as:\n\\[f'(a) = \\lim_{h \\to 0} \\frac{f(a+h) - f(a)}{h}\\]\nIn other words, the deriviative is the slope of a function at a particular point \\(a\\). This can be proxied using derivatives, by choosing a very small value for \\(h\\).\nFor example, consider the function \\(f(x) = x^2\\). The derivative of \\(f(x)\\) at \\(x = a\\) is:\n\\[\\begin{aligned}\nf'(a) &= \\lim_{h \\to 0} \\frac{f(a+h) - f(a)}{h} \\\\\n&=\\lim_{h \\to 0} \\frac{(a+h)^2 - (a)^2}{h} \\\\\n&= \\lim_{h \\to 0} \\frac{a^2 + 2ah + h^2 - a^2}{h} \\\\\n&= \\lim_{h \\to 0} 2a + h= 2a\n\\end{aligned}\n\\]\nIf anything else fails, one can always rely on numerical differentiation.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Calculus"
    ]
  },
  {
    "objectID": "mathref/math_1.html#derivative-of-common-functions",
    "href": "mathref/math_1.html#derivative-of-common-functions",
    "title": "Math Refresher: Basic Calculus",
    "section": "Derivative of common functions",
    "text": "Derivative of common functions\nFor most common functions, the derivative can be calculated using the following rules:\n\nThe derivative of a constant is zero\nThe derivative of \\(x^n\\) is \\(nx^{n-1}\\)\nThe derivative of \\(ln(x)\\) is \\(\\frac{1}{x}\\)\nThe derivative of \\(e^x\\) is \\(e^x\\)\nThe derivative of \\(a^x\\) is \\(a^x \\ln a\\)\n\nThere are other rules for derivatives, but these are the ones that will be used most often.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Calculus"
    ]
  },
  {
    "objectID": "mathref/math_1.html#derivative-of-composite-functions",
    "href": "mathref/math_1.html#derivative-of-composite-functions",
    "title": "Math Refresher: Basic Calculus",
    "section": "Derivative of composite functions",
    "text": "Derivative of composite functions\nThe derivative of a composite function \\(f(g(x))\\) is given by the chain rule:\n\\[\\frac{d}{dx} f(g(x)) = f'(g(x)) g'(x)\\]\nFor example, consider the function \\(f(x) = \\ln(x^2)\\). The derivative of \\(f(x)\\) is:\n\\[\\begin{aligned}\n\\frac{d}{dx} \\ln(x^2) &= \\frac{1}{x^2} \\frac{d}{dx} x^2 \\\\\n&= \\frac{1}{x^2} 2x \\\\\n&= \\frac{2}{x}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Calculus"
    ]
  },
  {
    "objectID": "mathref/math_1.html#derivative-of-sums-and-products",
    "href": "mathref/math_1.html#derivative-of-sums-and-products",
    "title": "Math Refresher: Basic Calculus",
    "section": "Derivative of sums and products",
    "text": "Derivative of sums and products\nThe derivative of a sum of functions is the sum of the derivatives of the functions.\n\\[\\frac{d}{dx} (f(x) + g(x)) = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x)\\]\nThe derivative of a product of functions is given by the product rule:\n\\[\\frac{d}{dx} (f(x) g(x)) = f'(x) g(x) + f(x) g'(x)\\]\nThe derivative of a quotient of functions is given by the quotient rule:\n\\[\\frac{d}{dx} \\frac{f(x)}{g(x)} = \\frac{f'(x) g(x) - f(x) g'(x)}{g(x)^2}\\]\nWhich is a special case of th product rule.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Calculus"
    ]
  },
  {
    "objectID": "mathref/math_1.html#optimization-with-multiple-variables",
    "href": "mathref/math_1.html#optimization-with-multiple-variables",
    "title": "Math Refresher: Basic Calculus",
    "section": "Optimization with multiple variables",
    "text": "Optimization with multiple variables\nWhen considering multiple variables, we also need to rely on the first and second order conditions to find minimum and maximum values. Consider a function \\(f(x,y)\\). The first order conditions are:\n\\[\\begin{aligned}\n\\frac{\\partial}{\\partial x} f(x,y) &= 0 \\\\\n\\frac{\\partial}{\\partial y} f(x,y) &= 0\n\\end{aligned}\n\\]\nThis conditions now say that, in the direction of \\(x\\) and \\(y\\), the function \\(f(x,y)\\) is not changing anymore. Thus we have a potential maximum or minimum. Now, to identify a minimum, we need second order conditions to be:\n\\[\\begin{aligned}\nH=\\begin{bmatrix}\nf_{xx} & f_{xy} \\\\\nf_{xy} & f_{yy} \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\]\nIf \\(Det(H)&gt;0\\) and \\(f_{xx}&gt;0\\) then we have a minimum. If \\(Det(H)&gt;0\\) and \\(f_{xx}&lt;0\\) then we have a maximum. If \\(Det(H)&lt;0\\) then we have a saddle point. And if \\(Det(H)=0\\) then we have an inconclusive result.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Calculus"
    ]
  },
  {
    "objectID": "mathref/math_1.html#optimization-with-constraints",
    "href": "mathref/math_1.html#optimization-with-constraints",
    "title": "Math Refresher: Basic Calculus",
    "section": "Optimization with constraints",
    "text": "Optimization with constraints\nWhen optimizing a function with constraints, we can use the method of Lagrange multipliers. Consider a function \\(f(x,y)\\) subject to the constraint \\(g(x,y) = z\\). The Lagrangian is:\n\\[\\begin{aligned}\nL(x,y,\\lambda) = f(x,y) + \\lambda (z - g(x,y))\n\\end{aligned}\n\\]\nNotice that the Lagrangian is the function \\(f(x,y)\\) plus the constraint \\(g(x,y)\\) multiplied by a constant \\(\\lambda\\). The constant \\(\\lambda\\) is called the Lagrange multiplier. The constrain is written as the difference between the constant \\(z\\) and the function \\(g(x,y)\\). The Lagrangian is then optimized with respect to \\(x\\), \\(y\\), and \\(\\lambda\\). This are the equivalent of the first order conditions:\n\\[\\begin{aligned}\n\\frac{\\partial}{\\partial x} L(x,y,\\lambda) &= 0 \\\\\n\\frac{\\partial}{\\partial y} L(x,y,\\lambda) &= 0 \\\\\n\\frac{\\partial}{\\partial \\lambda} L(x,y,\\lambda) &= z - g(x,y)=0\n\\end{aligned}\n\\]\nThe last condition is the constraint, and it implies that the constraint must be satisfied. The second order conditions are the same as before.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Calculus"
    ]
  },
  {
    "objectID": "imewld/chapter3.html",
    "href": "imewld/chapter3.html",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "Model:\n\\[colGPA = \\beta_0 + \\beta_1hsGPA + \\beta_2ACT + u\\]\n\nfrause gpa1, clear\nregress colgpa hsgpa act\nregress colgpa act\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(2, 138)       =     14.78\n       Model |  3.42365506         2  1.71182753   Prob &gt; F        =    0.0000\n    Residual |  15.9824444       138  .115814814   R-squared       =    0.1764\n-------------+----------------------------------   Adj R-squared   =    0.1645\n       Total |  19.4060994       140  .138614996   Root MSE        =    .34032\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .4534559   .0958129     4.73   0.000     .2640047    .6429071\n         act |    .009426   .0107772     0.87   0.383    -.0118838    .0307358\n       _cons |   1.286328   .3408221     3.77   0.000      .612419    1.960237\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(1, 139)       =      6.21\n       Model |  .829558811         1  .829558811   Prob &gt; F        =    0.0139\n    Residual |  18.5765406       139  .133644177   R-squared       =    0.0427\n-------------+----------------------------------   Adj R-squared   =    0.0359\n       Total |  19.4060994       140  .138614996   Root MSE        =    .36557\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         act |    .027064   .0108628     2.49   0.014     .0055862    .0485417\n       _cons |   2.402979   .2642027     9.10   0.000     1.880604    2.925355\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter3.html#example-3.1-determinants-of-college-gpa",
    "href": "imewld/chapter3.html#example-3.1-determinants-of-college-gpa",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "Model:\n\\[colGPA = \\beta_0 + \\beta_1hsGPA + \\beta_2ACT + u\\]\n\nfrause gpa1, clear\nregress colgpa hsgpa act\nregress colgpa act\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(2, 138)       =     14.78\n       Model |  3.42365506         2  1.71182753   Prob &gt; F        =    0.0000\n    Residual |  15.9824444       138  .115814814   R-squared       =    0.1764\n-------------+----------------------------------   Adj R-squared   =    0.1645\n       Total |  19.4060994       140  .138614996   Root MSE        =    .34032\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .4534559   .0958129     4.73   0.000     .2640047    .6429071\n         act |    .009426   .0107772     0.87   0.383    -.0118838    .0307358\n       _cons |   1.286328   .3408221     3.77   0.000      .612419    1.960237\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(1, 139)       =      6.21\n       Model |  .829558811         1  .829558811   Prob &gt; F        =    0.0139\n    Residual |  18.5765406       139  .133644177   R-squared       =    0.0427\n-------------+----------------------------------   Adj R-squared   =    0.0359\n       Total |  19.4060994       140  .138614996   Root MSE        =    .36557\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         act |    .027064   .0108628     2.49   0.014     .0055862    .0485417\n       _cons |   2.402979   .2642027     9.10   0.000     1.880604    2.925355\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter3.html#example-3.2-hourly-wage-equation",
    "href": "imewld/chapter3.html#example-3.2-hourly-wage-equation",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Example 3.2: Hourly Wage Equation",
    "text": "Example 3.2: Hourly Wage Equation\nModel:\n\\[log(wage)=\\beta_0 + \\beta_1educ + \\beta_2exper + \\beta_3tenure + u\\]\n\nfrause wage1, clear\ngen logwage = log(wage)\nreg logwage educ exper tenure\n\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(3, 522)       =     80.39\n       Model |  46.8741776         3  15.6247259   Prob &gt; F        =    0.0000\n    Residual |  101.455574       522  .194359337   R-squared       =    0.3160\n-------------+----------------------------------   Adj R-squared   =    0.3121\n       Total |  148.329751       525   .28253286   Root MSE        =    .44086\n\n------------------------------------------------------------------------------\n     logwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |    .092029   .0073299    12.56   0.000     .0776292    .1064288\n       exper |   .0041211   .0017233     2.39   0.017     .0007357    .0075065\n      tenure |   .0220672   .0030936     7.13   0.000     .0159897    .0281448\n       _cons |   .2843595   .1041904     2.73   0.007     .0796756    .4890435\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter3.html#partialling-out-interpretation-of-multiple-regression",
    "href": "imewld/chapter3.html#partialling-out-interpretation-of-multiple-regression",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Partialling Out Interpretation of Multiple Regression",
    "text": "Partialling Out Interpretation of Multiple Regression\nModel:\n\\[log(wage)=\\beta_0 + \\beta_1educ + \\beta_2exper + \\beta_3tenure + u\\]\nWe could estimate the same models with the followin:\n\\[educ=\\gamma_0 + \\gamma_1exper + \\gamma_2tenure + v\\]\n\\[log(wage)=\\beta_0 + \\beta_1 \\hat v + u\\]\n\nqui:reg logwage exper tenure\npredict logwage_res, resid\nqui:reg educ exper tenure\npredict educ_res, resid\nreg logwage_res educ_res\n\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =    158.24\n       Model |  30.6376772         1  30.6376772   Prob &gt; F        =    0.0000\n    Residual |  101.455574       524  .193617507   R-squared       =    0.2319\n-------------+----------------------------------   Adj R-squared   =    0.2305\n       Total |  132.093251       525  .251606192   Root MSE        =    .44002\n\n------------------------------------------------------------------------------\n logwage_res | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    educ_res |    .092029   .0073159    12.58   0.000     .0776568    .1064011\n       _cons |  -6.25e-10   .0191858    -0.00   1.000    -.0376905    .0376905\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter3.html#example-3.4-determinants-of-college-gpa",
    "href": "imewld/chapter3.html#example-3.4-determinants-of-college-gpa",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Example 3.4 : Determinants of College GPA",
    "text": "Example 3.4 : Determinants of College GPA\nSee example 3.1"
  },
  {
    "objectID": "imewld/chapter3.html#example-3.5-explaining-arrest-records",
    "href": "imewld/chapter3.html#example-3.5-explaining-arrest-records",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Example 3.5 : Explaining Arrest Records",
    "text": "Example 3.5 : Explaining Arrest Records\n\nfrause crime1, clear\nregress narr86 pcnv  ptime86 qemp86\n\n\n      Source |       SS           df       MS      Number of obs   =     2,725\n-------------+----------------------------------   F(3, 2721)      =     39.10\n       Model |  83.0741941         3   27.691398   Prob &gt; F        =    0.0000\n    Residual |  1927.27296     2,721  .708295833   R-squared       =    0.0413\n-------------+----------------------------------   Adj R-squared   =    0.0403\n       Total |  2010.34716     2,724  .738012906   Root MSE        =     .8416\n\n------------------------------------------------------------------------------\n      narr86 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        pcnv |  -.1499274   .0408653    -3.67   0.000    -.2300576   -.0697973\n     ptime86 |  -.0344199    .008591    -4.01   0.000    -.0512655   -.0175744\n      qemp86 |   -.104113   .0103877   -10.02   0.000    -.1244816   -.0837445\n       _cons |   .7117715   .0330066    21.56   0.000      .647051     .776492\n------------------------------------------------------------------------------\n\n\n\nregress narr86 pcnv avgsen ptime86 qemp86\n\n\n      Source |       SS           df       MS      Number of obs   =     2,725\n-------------+----------------------------------   F(4, 2720)      =     29.96\n       Model |  84.8242895         4  21.2060724   Prob &gt; F        =    0.0000\n    Residual |  1925.52287     2,720  .707912819   R-squared       =    0.0422\n-------------+----------------------------------   Adj R-squared   =    0.0408\n       Total |  2010.34716     2,724  .738012906   Root MSE        =    .84138\n\n------------------------------------------------------------------------------\n      narr86 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        pcnv |  -.1508319   .0408583    -3.69   0.000    -.2309484   -.0707154\n      avgsen |   .0074431   .0047338     1.57   0.116    -.0018392    .0167254\n     ptime86 |  -.0373908   .0087941    -4.25   0.000    -.0546345   -.0201471\n      qemp86 |   -.103341   .0103965    -9.94   0.000    -.1237268   -.0829552\n       _cons |   .7067565   .0331515    21.32   0.000     .6417519     .771761\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter3.html#variance-inflation-factors",
    "href": "imewld/chapter3.html#variance-inflation-factors",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Variance Inflation Factors",
    "text": "Variance Inflation Factors\n\nqui:regress narr86 pcnv avgsen ptime86 qemp86\nestat vif\n\n\n    Variable |       VIF       1/VIF  \n-------------+----------------------\n     ptime86 |      1.13    0.883693\n      qemp86 |      1.08    0.927081\n      avgsen |      1.06    0.942363\n        pcnv |      1.00    0.996771\n-------------+----------------------\n    Mean VIF |      1.07\n\n\n\nqui:regress pcnv avgsen ptime86 qemp86\ndisplay \"VIF for pcnv:   \" 1/(1-e(r2))\nqui:regress avgsen ptime86 qemp86 pcnv\ndisplay \"VIF for avgsen: \" 1/(1-e(r2))\n\nVIF for pcnv: 1.003239\nVIF for avgsen: 1.0611622"
  },
  {
    "objectID": "imewld/chapter3.html#example-3.7-evaluating-a-job-training-program",
    "href": "imewld/chapter3.html#example-3.7-evaluating-a-job-training-program",
    "title": "Chapter 3: Multiple Regression Analysis: Estimation",
    "section": "Example 3.7 : Evaluating a Job Training Program",
    "text": "Example 3.7 : Evaluating a Job Training Program\n\nfrause jtrain98, clear\nregress earn98 train\n\n\n      Source |       SS           df       MS      Number of obs   =     1,130\n-------------+----------------------------------   F(1, 1128)      =     17.91\n       Model |  1054.41369         1  1054.41369   Prob &gt; F        =    0.0000\n    Residual |  66408.4778     1,128   58.872764   R-squared       =    0.0156\n-------------+----------------------------------   Adj R-squared   =    0.0148\n       Total |  67462.8915     1,129   59.754554   Root MSE        =    7.6729\n\n------------------------------------------------------------------------------\n      earn98 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       train |  -2.050053   .4844142    -4.23   0.000    -3.000507   -1.099599\n       _cons |    10.6099    .279429    37.97   0.000     10.06164    11.15816\n------------------------------------------------------------------------------\n\n\n\nregress earn98 train earn96 educ age married  \n\n\n      Source |       SS           df       MS      Number of obs   =     1,130\n-------------+----------------------------------   F(5, 1124)      =    152.99\n       Model |  27320.1797         5  5464.03593   Prob &gt; F        =    0.0000\n    Residual |  40142.7118     1,124  35.7141564   R-squared       =    0.4050\n-------------+----------------------------------   Adj R-squared   =    0.4023\n       Total |  67462.8915     1,129   59.754554   Root MSE        =    5.9761\n\n------------------------------------------------------------------------------\n      earn98 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       train |   2.410547   .4352625     5.54   0.000     1.556528    3.264565\n      earn96 |   .3725384   .0186262    20.00   0.000     .3359923    .4090845\n        educ |   .3628329    .064047     5.67   0.000     .2371678    .4884979\n         age |   -.181046    .018875    -9.59   0.000    -.2180803   -.1440118\n     married |   2.481719   .4262625     5.82   0.000      1.64536    3.318079\n       _cons |   4.667042   1.145283     4.08   0.000     2.419908    6.914176\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter2.html",
    "href": "imewld/chapter2.html",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "",
    "text": "Model:\n\\[salary = \\beta_0 + \\beta_1 roe + u\\]\n\n\n\n\n\n\n\nfrause ceosal1, clear\nregress salary roe    \n\ndisplay \"Prediction: \" _b[_cons] \" +\" _b[roe] \"*30\" \"=\" _b[_cons] + _b[roe] *30\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =       209\n-------------+----------------------------------   F(1, 207)       =      2.77\n       Model |  5166419.04         1  5166419.04   Prob &gt; F        =    0.0978\n    Residual |   386566563       207  1867471.32   R-squared       =    0.0132\n-------------+----------------------------------   Adj R-squared   =    0.0084\n       Total |   391732982       208  1883331.64   Root MSE        =    1366.6\n\n------------------------------------------------------------------------------\n      salary | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         roe |   18.50119   11.12325     1.66   0.098    -3.428196    40.43057\n       _cons |   963.1913   213.2403     4.52   0.000     542.7902    1383.592\n------------------------------------------------------------------------------\nPrediction: 963.19134 +18.501186*30=1518.2269"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.3-ceo-salary-and-return-on-equity",
    "href": "imewld/chapter2.html#example-2.3-ceo-salary-and-return-on-equity",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "",
    "text": "Model:\n\\[salary = \\beta_0 + \\beta_1 roe + u\\]\n\n\n\n\n\n\n\nfrause ceosal1, clear\nregress salary roe    \n\ndisplay \"Prediction: \" _b[_cons] \" +\" _b[roe] \"*30\" \"=\" _b[_cons] + _b[roe] *30\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =       209\n-------------+----------------------------------   F(1, 207)       =      2.77\n       Model |  5166419.04         1  5166419.04   Prob &gt; F        =    0.0978\n    Residual |   386566563       207  1867471.32   R-squared       =    0.0132\n-------------+----------------------------------   Adj R-squared   =    0.0084\n       Total |   391732982       208  1883331.64   Root MSE        =    1366.6\n\n------------------------------------------------------------------------------\n      salary | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         roe |   18.50119   11.12325     1.66   0.098    -3.428196    40.43057\n       _cons |   963.1913   213.2403     4.52   0.000     542.7902    1383.592\n------------------------------------------------------------------------------\nPrediction: 963.19134 +18.501186*30=1518.2269"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.4-wage-and-education",
    "href": "imewld/chapter2.html#example-2.4-wage-and-education",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.4: Wage and Education",
    "text": "Example 2.4: Wage and Education\n\n\n\n\n\n\n\nfrause wage1, clear\nregress wage educ\n\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =    103.36\n       Model |  1179.73204         1  1179.73204   Prob &gt; F        =    0.0000\n    Residual |  5980.68225       524  11.4135158   R-squared       =    0.1648\n-------------+----------------------------------   Adj R-squared   =    0.1632\n       Total |  7160.41429       525  13.6388844   Root MSE        =    3.3784\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .5413593    .053248    10.17   0.000     .4367534    .6459651\n       _cons |  -.9048516   .6849678    -1.32   0.187    -2.250472    .4407687\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.5-voting-outcomes-and-campaign-expenditures",
    "href": "imewld/chapter2.html#example-2.5-voting-outcomes-and-campaign-expenditures",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.5: Voting Outcomes and Campaign Expenditures",
    "text": "Example 2.5: Voting Outcomes and Campaign Expenditures\n\n\n\n\n\n\n\nfrause vote1, clear\nregress votea sharea\n\n\n      Source |       SS           df       MS      Number of obs   =       173\n-------------+----------------------------------   F(1, 171)       =   1017.66\n       Model |  41486.2307         1  41486.2307   Prob &gt; F        =    0.0000\n    Residual |  6971.01783       171  40.7661862   R-squared       =    0.8561\n-------------+----------------------------------   Adj R-squared   =    0.8553\n       Total |  48457.2486       172  281.728189   Root MSE        =    6.3848\n\n------------------------------------------------------------------------------\n       votea | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      sharea |   .4638269   .0145397    31.90   0.000     .4351266    .4925272\n       _cons |   26.81221   .8872146    30.22   0.000     25.06091    28.56352\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.6-ceo-salary-and-return-on-equity",
    "href": "imewld/chapter2.html#example-2.6-ceo-salary-and-return-on-equity",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.6: CEO Salary and Return on Equity",
    "text": "Example 2.6: CEO Salary and Return on Equity\n\n\n\n\n\n\n\nfrause ceosal1, clear\nqui:regress salary roe  \npredict salaryhat, xb\npredict uhat , resid\n\nlist roe salary salaryhat uhat  in 1/10\n\n\n     +--------------------------------------+\n     |  roe   salary   salary~t        uhat |\n     |--------------------------------------|\n  1. | 14.1     1095   1224.058   -129.0581 |\n  2. | 10.9     1001   1164.854   -163.8543 |\n  3. | 23.5     1122   1397.969   -275.9692 |\n  4. |  5.9      578   1072.348   -494.3483 |\n  5. | 13.8     1368   1218.508    149.4923 |\n     |--------------------------------------|\n  6. |   20     1145   1333.215   -188.2151 |\n  7. | 16.4     1078   1266.611   -188.6108 |\n  8. | 16.3     1094   1264.761   -170.7607 |\n  9. | 10.5     1237   1157.454     79.5462 |\n 10. | 26.3      833   1449.773   -616.7725 |\n     +--------------------------------------+"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.7-wage-and-education",
    "href": "imewld/chapter2.html#example-2.7-wage-and-education",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.7: Wage and Education",
    "text": "Example 2.7: Wage and Education\n\n\n\n\n\n\n\nfrause wage1, clear\nqui:regress wage educ\nsum wage educ\ndisplay \"b0+b1*E(educ)= \" _b[_cons] + _b[educ]*r(mean)\n\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        wage |        526    5.896103    3.693086        .53      24.98\n        educ |        526    12.56274    2.769022          0         18\nb0+b1*E(educ)= 5.8961027"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.8-ceo-salary-and-return-on-equity",
    "href": "imewld/chapter2.html#example-2.8-ceo-salary-and-return-on-equity",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.8: CEO Salary and Return on Equity",
    "text": "Example 2.8: CEO Salary and Return on Equity\n\nfrause ceosal1, clear\ngen one=1\nmata:y = st_data(., \"salary\")\nmata:x = st_data(., \"one roe\")\nmata:b = invsym(x'*x)*x'*y\nmata:yhat = x*b\nmata:uhat = y - yhat\nmata:sst = sum((y :- mean(y)):^2)\nmata:sse = sum((yhat :- mean(y)):^2)\nmata:ssr = sum((y :- yhat):^2)\nmata:rsq = 1 - ssr/sst;rsq\nmata:rsq = sse/sst;rsq\n\n  .0131886241\n  .0131886241"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.10-a-log-wage-model",
    "href": "imewld/chapter2.html#example-2.10-a-log-wage-model",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.10: a log wage model",
    "text": "Example 2.10: a log wage model\nModel\n\\[log(wage) = \\beta_0 + \\beta_1 educ + u\\]\n\n\n\n\n\n\n\nfrause wage1, clear\ngen logwage = log(wage)\nregress logwage educ\n\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =    119.58\n       Model |  27.5606288         1  27.5606288   Prob &gt; F        =    0.0000\n    Residual |  120.769123       524  .230475425   R-squared       =    0.1858\n-------------+----------------------------------   Adj R-squared   =    0.1843\n       Total |  148.329751       525   .28253286   Root MSE        =    .48008\n\n------------------------------------------------------------------------------\n     logwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0827444   .0075667    10.94   0.000     .0678796    .0976091\n       _cons |   .5837727   .0973358     6.00   0.000     .3925563    .7749891\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.11-ceo-salary-and-firms-sales",
    "href": "imewld/chapter2.html#example-2.11-ceo-salary-and-firms-sales",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.11: CEO Salary and Firms Sales",
    "text": "Example 2.11: CEO Salary and Firms Sales\nModel:\n\\[log(salary) = \\beta_0 + \\beta_1 log(sales) + u\\]\n\n\n\n\n\n\n\nfrause ceosal1, clear\ngen logsalary = log(salary)\ngen logsales = log(sales)\nreg logsalary logsales\n\n\n      Source |       SS           df       MS      Number of obs   =       209\n-------------+----------------------------------   F(1, 207)       =     55.30\n       Model |  14.0661688         1  14.0661688   Prob &gt; F        =    0.0000\n    Residual |  52.6559944       207  .254376785   R-squared       =    0.2108\n-------------+----------------------------------   Adj R-squared   =    0.2070\n       Total |  66.7221632       208  .320779631   Root MSE        =    .50436\n\n------------------------------------------------------------------------------\n   logsalary | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    logsales |   .2566717   .0345167     7.44   0.000     .1886224    .3247209\n       _cons |   4.821997   .2883396    16.72   0.000     4.253538    5.390455\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter2.html#example-2.14-evaluating-a-job-training-program",
    "href": "imewld/chapter2.html#example-2.14-evaluating-a-job-training-program",
    "title": "Chapter 2: The Simple Regression Model",
    "section": "Example 2.14: Evaluating a Job Training Program",
    "text": "Example 2.14: Evaluating a Job Training Program\n\n\n\n\n\n\n\nfrause jtrain2, clear\nreg re78 train\n\n\n      Source |       SS           df       MS      Number of obs   =       445\n-------------+----------------------------------   F(1, 443)       =      8.04\n       Model |  348.013451         1  348.013451   Prob &gt; F        =    0.0048\n    Residual |  19177.6432       443  43.2903909   R-squared       =    0.0178\n-------------+----------------------------------   Adj R-squared   =    0.0156\n       Total |  19525.6566       444  43.9767041   Root MSE        =    6.5795\n\n------------------------------------------------------------------------------\n        re78 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       train |   1.794343   .6328536     2.84   0.005     .5505748    3.038111\n       _cons |   4.554802    .408046    11.16   0.000     3.752856    5.356749\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter4.html",
    "href": "imewld/chapter4.html",
    "title": "Chapter 4: Multiple Regression Analysis: Inference",
    "section": "",
    "text": "Model:\n\\[log(wage)=\\beta_0 +\\beta_1 jc + \\beta_2 univ + \\beta_3 exper + u\\]\nHypothesis\n\\[H_0: \\beta_1 = \\beta_2 \\rightarrow \\beta_1 - \\beta_2 = 0\\] \\[H_1: \\beta_1 &lt; \\beta_2 \\rightarrow \\beta_1 - \\beta_2 &lt; 0\\]\n\nfrause twoyear, clear\nreg lwage jc univ exper\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =     6,763\n-------------+----------------------------------   F(3, 6759)      =    644.53\n       Model |  357.752575         3  119.250858   Prob &gt; F        =    0.0000\n    Residual |  1250.54352     6,759  .185019014   R-squared       =    0.2224\n-------------+----------------------------------   Adj R-squared   =    0.2221\n       Total |  1608.29609     6,762  .237843255   Root MSE        =    .43014\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          jc |   .0666967   .0068288     9.77   0.000     .0533101    .0800833\n        univ |   .0768762   .0023087    33.30   0.000     .0723504    .0814021\n       exper |   .0049442   .0001575    31.40   0.000     .0046355    .0052529\n       _cons |   1.472326   .0210602    69.91   0.000     1.431041     1.51361\n------------------------------------------------------------------------------\n\n\n\ntest jc = univ\n\n\n ( 1)  jc - univ = 0\n\n       F(  1,  6759) =    2.15\n            Prob &gt; F =    0.1422\n\n\nManual transformation:\n\\[\\theta = \\beta_1 - \\beta_2 \\rightarrow \\beta_1 = \\theta + \\beta_2 \\]\n\\[log(wage)=\\beta_0 +(\\theta + \\beta_2) jc + \\beta_2 univ + \\beta_3 exper + u\\] \\[log(wage)=\\beta_0 +\\theta jc + \\beta_2 (univ+jc) + \\beta_3 exper + u\\]\n\ngen univjc = univ + jc\nreg lwage jc univjc exper\n\n\n      Source |       SS           df       MS      Number of obs   =     6,763\n-------------+----------------------------------   F(3, 6759)      =    644.53\n       Model |  357.752575         3  119.250858   Prob &gt; F        =    0.0000\n    Residual |  1250.54352     6,759  .185019014   R-squared       =    0.2224\n-------------+----------------------------------   Adj R-squared   =    0.2221\n       Total |  1608.29609     6,762  .237843255   Root MSE        =    .43014\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          jc |  -.0101795   .0069359    -1.47   0.142    -.0237761     .003417\n      univjc |   .0768762   .0023087    33.30   0.000     .0723504    .0814021\n       exper |   .0049442   .0001575    31.40   0.000     .0046355    .0052529\n       _cons |   1.472326   .0210602    69.91   0.000     1.431041     1.51361\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "imewld/chapter4.html#testing-hypotheses-about-single-linear-combinations-of-the-parameters",
    "href": "imewld/chapter4.html#testing-hypotheses-about-single-linear-combinations-of-the-parameters",
    "title": "Chapter 4: Multiple Regression Analysis: Inference",
    "section": "",
    "text": "Model:\n\\[log(wage)=\\beta_0 +\\beta_1 jc + \\beta_2 univ + \\beta_3 exper + u\\]\nHypothesis\n\\[H_0: \\beta_1 = \\beta_2 \\rightarrow \\beta_1 - \\beta_2 = 0\\] \\[H_1: \\beta_1 &lt; \\beta_2 \\rightarrow \\beta_1 - \\beta_2 &lt; 0\\]\n\nfrause twoyear, clear\nreg lwage jc univ exper\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =     6,763\n-------------+----------------------------------   F(3, 6759)      =    644.53\n       Model |  357.752575         3  119.250858   Prob &gt; F        =    0.0000\n    Residual |  1250.54352     6,759  .185019014   R-squared       =    0.2224\n-------------+----------------------------------   Adj R-squared   =    0.2221\n       Total |  1608.29609     6,762  .237843255   Root MSE        =    .43014\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          jc |   .0666967   .0068288     9.77   0.000     .0533101    .0800833\n        univ |   .0768762   .0023087    33.30   0.000     .0723504    .0814021\n       exper |   .0049442   .0001575    31.40   0.000     .0046355    .0052529\n       _cons |   1.472326   .0210602    69.91   0.000     1.431041     1.51361\n------------------------------------------------------------------------------\n\n\n\ntest jc = univ\n\n\n ( 1)  jc - univ = 0\n\n       F(  1,  6759) =    2.15\n            Prob &gt; F =    0.1422\n\n\nManual transformation:\n\\[\\theta = \\beta_1 - \\beta_2 \\rightarrow \\beta_1 = \\theta + \\beta_2 \\]\n\\[log(wage)=\\beta_0 +(\\theta + \\beta_2) jc + \\beta_2 univ + \\beta_3 exper + u\\] \\[log(wage)=\\beta_0 +\\theta jc + \\beta_2 (univ+jc) + \\beta_3 exper + u\\]\n\ngen univjc = univ + jc\nreg lwage jc univjc exper\n\n\n      Source |       SS           df       MS      Number of obs   =     6,763\n-------------+----------------------------------   F(3, 6759)      =    644.53\n       Model |  357.752575         3  119.250858   Prob &gt; F        =    0.0000\n    Residual |  1250.54352     6,759  .185019014   R-squared       =    0.2224\n-------------+----------------------------------   Adj R-squared   =    0.2221\n       Total |  1608.29609     6,762  .237843255   Root MSE        =    .43014\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          jc |  -.0101795   .0069359    -1.47   0.142    -.0237761     .003417\n      univjc |   .0768762   .0023087    33.30   0.000     .0723504    .0814021\n       exper |   .0049442   .0001575    31.40   0.000     .0046355    .0052529\n       _cons |   1.472326   .0210602    69.91   0.000     1.431041     1.51361\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "mathref/math_2.html",
    "href": "mathref/math_2.html",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "",
    "text": "A vector is a list of numbers. We can think of a vector as a point in space, or as an arrow pointing from the origin to that point. For example, the vector\n\\[\\vec{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\] is a vector in \\(\\mathbb{R}^3\\) (three-dimensional space) that points from the origin to the point \\((1, 2, 3)\\).\nWe can add vectors together by adding their corresponding elements. For example,\n\\[\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 7 \\\\ 9 \\end{bmatrix}\\]\nWe can also multiply a vector by a scalar (a single number) by multiplying each element of the vector by that number. For example,\n\\[2 \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \\end{bmatrix}\n\\]",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Linear Algebra"
    ]
  },
  {
    "objectID": "mathref/math_2.html#vectors",
    "href": "mathref/math_2.html#vectors",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "",
    "text": "A vector is a list of numbers. We can think of a vector as a point in space, or as an arrow pointing from the origin to that point. For example, the vector\n\\[\\vec{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\] is a vector in \\(\\mathbb{R}^3\\) (three-dimensional space) that points from the origin to the point \\((1, 2, 3)\\).\nWe can add vectors together by adding their corresponding elements. For example,\n\\[\\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} + \\begin{bmatrix} 4 \\\\ 5 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 7 \\\\ 9 \\end{bmatrix}\\]\nWe can also multiply a vector by a scalar (a single number) by multiplying each element of the vector by that number. For example,\n\\[2 \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \\end{bmatrix}\n\\]",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Linear Algebra"
    ]
  },
  {
    "objectID": "mathref/math_2.html#matrices",
    "href": "mathref/math_2.html#matrices",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Matrices",
    "text": "Matrices\nA matrix is a two-dimensional array of numbers. We can think of a matrix as a list of vectors. For example, the matrix:\n\\[A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} =\n\\begin{bmatrix} 1 \\\\ 4  \\end{bmatrix},\n\\begin{bmatrix} 2 \\\\ 5  \\end{bmatrix},\n\\begin{bmatrix} 3 \\\\ 6  \\end{bmatrix} \\]\nis a matrix that concatenates 3 \\(\\mathbb{R}^2\\) vectors together.\nMatrices can have different dimensions. For example, the matrix:\n\\[B = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\]\nis a square matrix that concatenates 3 \\(\\mathbb{R}^3\\) vectors together.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Linear Algebra"
    ]
  },
  {
    "objectID": "mathref/math_2.html#matrix-dimensions",
    "href": "mathref/math_2.html#matrix-dimensions",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Matrix Dimensions",
    "text": "Matrix Dimensions\nMatrices are often denoted by their dimensions. For example, the matrix \\(A\\) above is a \\(2 \\times 3\\) matrix, because it has 2 rows and 3 columns. The matrix \\(B\\) above is a \\(3 \\times 3\\) matrix, because it has 3 rows and 3 columns.\nIn general, we can denote a matrix \\(M\\) with \\(r\\) rows and \\(c\\) columns as an \\(r \\times c\\) matrix. For Notation, I will usually refer to this like \\(M_{r \\times c}\\). In this case we have \\(A_{2 \\times 3}\\) and \\(B_{3 \\times 3}\\).\nWe can denote the element in the \\(i\\)th row and \\(j\\)th column of \\(M\\) as \\(M_{ij}\\). For example, the element in the 2nd row and 3rd column of \\(B\\) is \\(B_{23} = 6\\).",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Linear Algebra"
    ]
  },
  {
    "objectID": "mathref/math_2.html#special-matrices",
    "href": "mathref/math_2.html#special-matrices",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Special Matrices",
    "text": "Special Matrices\nThere are a few special matrices that we will use often. The zero matrix is a matrix where all of the elements are 0. For example, the zero matrix with 2 rows and 3 columns is:\n\\[Zero=\\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\\]\nA square matrix is a matrix where the number of rows is equal to the number of columns. For example, \\(B\\) is a square matrix.\nThe identity matrix is a square matrix where all of the elements are 0, except for the elements along the diagonal, which are 1. For example, the identity matrix with 3 rows and 3 columns is:\n\\[I_{3}=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\]\nFor simplicitly, we will use the subscript to denote the size of the identity matrix. For example, \\(I_{3}\\) is a 3x3 identity matrix, and \\(I_{5}\\) is a 5x5 identity matrix.\nA \\(1\\times c\\) matrix is called a row vector. Wheras a \\(r \\times 1\\) matrix is called a column vector.\nA diagonal matrix is a square matrix where all of the elements off the diagonal are 0. For example, the following matrix is a diagonal matrix:\n\\[\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix}\\]\nThe identify matrix is a special case of a diagonal matrix.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Linear Algebra"
    ]
  },
  {
    "objectID": "mathref/math_2.html#matrix-operations",
    "href": "mathref/math_2.html#matrix-operations",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Matrix Operations",
    "text": "Matrix Operations\nWe can add matrices together by adding their corresponding elements. For example,\n\\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} + \\begin{bmatrix} 7 & 8 & 9 \\\\ 10 & 11 & 12 \\end{bmatrix} = \\begin{bmatrix} 8 & 10 & 12 \\\\ 14 & 16 & 18 \\end{bmatrix}\\]\nHowever, both matrices must have the same dimensions. For example, we cannot add the following matrices together:\n\\[\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}_{2\\times 3} + \\begin{bmatrix} 7 & 8 \\\\ 10 & 11 \\end{bmatrix}_{2\\times 2}\\]",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Linear Algebra"
    ]
  },
  {
    "objectID": "mathref/math_2.html#matrix-scalar-multiplication",
    "href": "mathref/math_2.html#matrix-scalar-multiplication",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Matrix Scalar Multiplication",
    "text": "Matrix Scalar Multiplication\nWe can multiply a matrix by a scalar by multiplying each element of the matrix by that scalar. For example,\n\\[ a \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} = \\begin{bmatrix} 2a & 4a & 6a \\\\ 8a & 10a & 12a \\end{bmatrix}\\]",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Linear Algebra"
    ]
  },
  {
    "objectID": "mathref/math_2.html#matrix-multiplication",
    "href": "mathref/math_2.html#matrix-multiplication",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Matrix Multiplication",
    "text": "Matrix Multiplication\nWe can multiple two matrices together by taking the dot product of each row of the first matrix with each column of the second matrix. For example:\n\\[\\begin{aligned}\n\\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}_{2\\times 3} \\begin{bmatrix} 7 & 8 \\\\ 10 & 11 \\\\ 13 & 14 \\end{bmatrix}_{3\\times 2} &= \\begin{bmatrix} 1*7 + 2*10 + 3*13 & 1*8 + 2*11 + 3*14 \\\\ 4*7 + 5*10 + 6*13 & 4*8 + 5*11 + 6*14 \\end{bmatrix}_{2\\times 2} \\\\\n&= \\begin{bmatrix} 66 & 82 \\\\ 156 & 199 \\end{bmatrix}\n\\end{aligned}\n\\]\nA good way of remembering this is to follow the flow: \\(\\rightarrow  \\times \\downarrow\\).\nNote that the number of columns in the first matrix must be equal to the number of rows in the second matrix. For example, we cannot multiply the following matrices together:\n\\[\\begin{pmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{pmatrix}_{2\\times 3} \\begin{pmatrix} 7 & 8 \\\\ 10 & 11 \\end{pmatrix}_{2\\times 2}\\]\nIn general, given two matrixes \\(A_{a\\times b}\\) and \\(B_{c\\times d}\\), we can multiply them together if and only if \\(b=c\\). The resulting matrix will be \\(AB_{a\\times d}\\).\nSome properties of matrix multiplication:\n\nMatrix multiplication is not commutative. That is, \\(AB \\neq BA\\) in general.\nMatrix multiplication is associative. That is, \\(A(BC) = (AB)C\\).\nAny matrix multiplied by \\(I\\) is equal to itself. That is, \\(AI = IA = A\\).",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Linear Algebra"
    ]
  },
  {
    "objectID": "mathref/math_2.html#transpose",
    "href": "mathref/math_2.html#transpose",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Transpose",
    "text": "Transpose\nThe transpose of a matrix is a matrix where the rows and columns are swapped. For example, if the matrix \\(A\\) is defined as:\n\\[A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\]\nthen the transpose of \\(A\\), denoted \\(A^T\\), is:\n\\[A^T = \\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}\\]\nNote that if \\(A_{a\\times b}\\), then \\(A^T_{b\\times a}\\).\nSome properties of the transpose:\n\n\\((A^T)^T = A\\)\n\\((AB)^T = B^TA^T\\)\n\\((A+B)^T = A^T + B^T\\)\n\\((aA)^T = aA^T\\)\n\\((A^T)^{-1} = (A^{-1})^T\\)",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Linear Algebra"
    ]
  },
  {
    "objectID": "mathref/math_2.html#inverse",
    "href": "mathref/math_2.html#inverse",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Inverse",
    "text": "Inverse\nThe inverse of a square matrix is a matrix that, when multiplied by the original matrix (\\(A A^{-1} = I\\)), results in the identity matrix. For example, if the matrix \\(A\\) is defined as:\n\\[A = \\begin{bmatrix} 1 & 2 \\\\ 4   & 6 \\end{bmatrix}\\]\nthen the inverse of \\(A\\), denoted \\(A^{-1}\\), is:\n\\[A^{-1} = \\begin{bmatrix} -3 & 1 \\\\ 2 & -.5 \\end{bmatrix}\\]\nFor a \\(2 \\times 2\\) matrix, the inverse is defined as:\n\\[\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}\\]\nIf a matrix has determinant 0, then it is not invertible.\nSome properties of the inverse:\n\n\\((A^{-1})^{-1} = A\\)\n\\((AB)^{-1} = B^{-1}A^{-1}\\)\n\\((A^T)^{-1} = (A^{-1})^T\\)\n\\((aA)^{-1} = \\frac{1}{a}A^{-1}\\)",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Linear Algebra"
    ]
  },
  {
    "objectID": "mathref/math_2.html#determinant",
    "href": "mathref/math_2.html#determinant",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Determinant",
    "text": "Determinant\nThe determinant of a square matrix is a scalar value that is a function of the elements of the matrix. The determinant of a \\(2 \\times 2\\) matrix is defined as:\n\\[\\begin{vmatrix} a & b \\\\ c & d \\end{vmatrix} = ad - bc\\]\nThe determinant of a \\(3 \\times 3\\) matrix is defined as:\n\\[\\begin{vmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\end{vmatrix} = aei+dhc+gbf-ceg-fha-ibd\\]",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Linear Algebra"
    ]
  },
  {
    "objectID": "mathref/math_2.html#rank-and-linear-independence",
    "href": "mathref/math_2.html#rank-and-linear-independence",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Rank and linear independence",
    "text": "Rank and linear independence\nThe rank of a matrix is the number of linearly independent rows or columns in the matrix. In a rectangular matrix, the rank cannot be larger than the smaller of the rows or columns.\nIf we consider each column, or rows, of a matrix as a vector, then the rank of the matrix is the number of linearly independent vectors in the matrix. If a set of vectors are not linearly independent, then one of the vectors can be expressed as a linear combination of the other vectors. For example, the following vectors are not linearly independent:\n\\[a_1 \\vec x_1 + a_2 \\vec x_2 + a_3 \\vec x_3 = 0\\]",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Linear Algebra"
    ]
  },
  {
    "objectID": "mathref/math_2.html#eigenvalues-and-eigenvectors",
    "href": "mathref/math_2.html#eigenvalues-and-eigenvectors",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "Eigenvalues and eigenvectors",
    "text": "Eigenvalues and eigenvectors\nThe eigenvalues and eigenvectors of a matrix are scalars and vectors that satisfy the following equation:\n\\[A \\vec x = \\lambda \\vec x\\]\nwhere \\(A\\) is a square matrix, \\(\\vec x\\) is the an eigen a vector, and \\(\\lambda\\) is a scalar. In other words, multiplying a vector by a matrix is the same as multiplying the vector by a scalar.\nThe eigenvalues of a matrix are the values of \\(\\lambda\\) that satisfy this equation. The eigenvectors of a matrix are the vectors \\(\\vec x\\) that satisfy this equation.\nThe eigenvalues and eigenvectors of a matrix can be found by solving the following equation:\n\\[det(A - \\lambda I) = 0\\]\nwhere \\(I\\) is the identity matrix.\nIf the matrix A is of dimension \\(n \\times n\\), then there are \\(n\\) eigenvalues and \\(n\\) eigenvectors.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Linear Algebra"
    ]
  },
  {
    "objectID": "mathref/math_2.html#system-of-linear-equations",
    "href": "mathref/math_2.html#system-of-linear-equations",
    "title": "Math Refresher: Basic Linear Algebra",
    "section": "System of linear equations",
    "text": "System of linear equations\nA system of linear equations is a set of equations that can be expressed in the form:\n\\[\\begin{aligned}\na_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n &= b_1 \\\\\na_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n &= b_2 \\\\\n\\vdots \\\\\na_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nn}x_n &= b_n \\\\\n\\end{aligned}\\]\nwhere \\(a_{ij}\\) and \\(b_i\\) are constants, and \\(x_i\\) are variables.\nThis system of equations can be written in matrix form as:\n\\[A_{n\\times n}   X_{n\\times 1} =   b_{n\\times 1}\\]\nif the system has a unique solution, then the matrix \\(A\\) is invertible, and the solution is given by:\n\\[X = A^{-1}b\\]\nThus if there is no solution, then \\(A\\) is not invertible. If the determinant of \\(A\\) is 0, then \\(A\\) is not invertible.",
    "crumbs": [
      "Home",
      "Math Refresher",
      "Math Refresher: Basic Linear Algebra"
    ]
  },
  {
    "objectID": "Projectos_Ecuador.html",
    "href": "Projectos_Ecuador.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Idea paper Migracion Pobreza Ecuador 5-7K\nComo identificar?\nComo impacto Migracion en Pobreza? 2007-2022 Que lugares recibieron mas migracion?\nMecanismos\nComo afecto pobreza? sector? servicios? Violencia\nAsset? wealth indirect?\n\nTiempos? 1 semestre… Research agenda.\n\nShift Share -&gt; Q3-Q4 ()\n\n\nBid -&gt; Mineria Ilegal Pobreza Colombia : Posible?"
  },
  {
    "objectID": "quarto/stata_basics.html",
    "href": "quarto/stata_basics.html",
    "title": "Stata-output",
    "section": "",
    "text": "Analyzing Oaxaca dataset\nsmaller\n\n\n\n\n\n\n\n\n\n\n\n\n(1)\n&gt; (2)\n\n\n\n\n\nyears of education\n0.0885***\n0 &gt; .0794***\n\n\n\n(0.00519)\n(0.0 &gt; 0522)\n\n\n \n\n\nyears of work\n0.0153***\n0. &gt; 00399*\n\n\nexperience\n(0.00126)\n(0.0 &gt; 0188)\n\n\n \n\n\nyears of job tenure\n\n0. &gt; 00407*\n\n\n\n\n(0.0 &gt; 0196)\n\n\n \n\n\nage of respondent\n\n0 &gt; .0114***\n\n\n\n\n(0.0 &gt; 0174)\n\n\n \n\n\nConstant\n2.136***\n&gt; 1.915***\n\n\n\n(0.0654)\n(0. &gt; 0727)\n\n\n\n\n\nObservations\n1434\n&gt; 1434"
  },
  {
    "objectID": "quizes/quiz_1.html",
    "href": "quizes/quiz_1.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Name: ________________________________________________________\n\nHousehold Budget surveys are an example of:\n\n\npanel data\nexperimental data\ntime series data\ncross-sectional data\n\n\nA dependent variable is also known as a(n) _____.\n\n\nexplanatory variable\ncontrol variable\npredictor variable\nresponse variable\n\n\nThe Zero conditional mean assumption means\n\n\n\\(E(u│x)=0\\)\n\\(E(\\hat u | x)=0\\)\n\\(E(u^2│x)=0\\)\n\\(E(\\hat u ^2│x)=0\\)\n\n\nThe explained sum of squares (SSE) for the regression function, \\(y=\\beta_0 + \\beta_1 x + u\\), is defined as _____.\n\n\n\\(\\sum(\\hat y_i - \\bar{y})^2\\)\n\\(\\sum(y_i - \\hat y_i )^2\\)\n\\(\\sum \\hat u\\)\n\\(\\sum \\hat u^2\\)\n\n\nThe error term in a regression equation is said to exhibit homoskedasticty if _____.\n\n\nit has zero conditional mean\nit has the same variance for all values of the explanatory variable\nit has the same value for all values of the explanatory variable\nif the error term has a value of one given any value of the explanatory variable\n\n\nBonus: Why do we care about the Zero Conditional mean assumption?"
  },
  {
    "objectID": "quizes/quiz_1.html#quiz-1",
    "href": "quizes/quiz_1.html#quiz-1",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Name: ________________________________________________________\n\nHousehold Budget surveys are an example of:\n\n\npanel data\nexperimental data\ntime series data\ncross-sectional data\n\n\nA dependent variable is also known as a(n) _____.\n\n\nexplanatory variable\ncontrol variable\npredictor variable\nresponse variable\n\n\nThe Zero conditional mean assumption means\n\n\n\\(E(u│x)=0\\)\n\\(E(\\hat u | x)=0\\)\n\\(E(u^2│x)=0\\)\n\\(E(\\hat u ^2│x)=0\\)\n\n\nThe explained sum of squares (SSE) for the regression function, \\(y=\\beta_0 + \\beta_1 x + u\\), is defined as _____.\n\n\n\\(\\sum(\\hat y_i - \\bar{y})^2\\)\n\\(\\sum(y_i - \\hat y_i )^2\\)\n\\(\\sum \\hat u\\)\n\\(\\sum \\hat u^2\\)\n\n\nThe error term in a regression equation is said to exhibit homoskedasticty if _____.\n\n\nit has zero conditional mean\nit has the same variance for all values of the explanatory variable\nit has the same value for all values of the explanatory variable\nif the error term has a value of one given any value of the explanatory variable\n\n\nBonus: Why do we care about the Zero Conditional mean assumption?"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "The goal of the class is for you to become familiar and proficient with some essential tools that are used in most empirical analysis.\nWhile learning to implement all the methods we cover by hand is a great excercise to learn what they do, and how they work, it may not be a feasible practice in most real-world work, unless you decide to follow that path (Econometrics/applied Econometrics).\nFor this purpose, the main software we will use in this class (as evidence from all the code shared in the slides) its Stata. A self contained program that is yet flexible enough to add custom add on programs/commands.\nNevertheless, if you are new to Stata, there are quite few resources you may want to look into using this software"
  },
  {
    "objectID": "resources.html#stata",
    "href": "resources.html#stata",
    "title": "Resources",
    "section": "Stata",
    "text": "Stata\n\nStata Free-Webinars: https://www.stata.com/training/webinar/\nStata Past Recorded Webinars: https://www.stata.com/training/webinar_series/past-webinar-recordings/\nStata Video-Tutorials: https://www.stata.com/links/video-tutorials/\nGeneral Learning resources: https://www.stata.com/links/resources-for-learning-stata/\nExcellent Stata tutorial for beginners: https://grodri.github.io/stata/index\nOur own Tutorial! Stata Basics\nAlso, you may want to check the following for a quick reference on\n\nData Management\nPublication Ready Tables\nGraphs\n\n\nBut of course, Stata is not free. There are other resources you may want to explore, if you are interested in doing econometric analysis, but no longer have access to Stata. These are R, Python and Julia."
  },
  {
    "objectID": "resources.html#r-julia-python",
    "href": "resources.html#r-julia-python",
    "title": "Resources",
    "section": "R, Julia, Python",
    "text": "R, Julia, Python\nThese software are free, but usually require add-ons from different sources to estimate specialized models. They also have a steep, or rather steep-er (than Stata) learning curve. However, it is smart to learn other languages, at least to implement basic analysis. One resource you may find very convinient is the following:\n\nR, Python, Julia: http://www.upfie.net/\n\nThis site and its author(s) have put together a set of companion books to go along with the Textbook “Introductory Econometrics: A Modern Approach”. These books are rather inexpensive, providing some of the authors own insights, with full code in all three languages, that replicate the examples in the textbook.\n\nExample Codes: http://www.upfie.net/code.html\n\nThe authors also suggest other resources that could be of interest\n\nFurther Resources: http://www.upfie.net/links.html"
  },
  {
    "objectID": "resources.html#quarto",
    "href": "resources.html#quarto",
    "title": "Resources",
    "section": "Quarto",
    "text": "Quarto\nQuarto is not a programming language. Rather an interpreter that converts plain text to nicely formating documents, presentations, websites, etc. This site, for instance, was built using Quarto.\nBecause of this, I’m encouraging the use of Quarto, combined with nbstata/python, to produce answers to ALL homeworks or group works. So it will be easy to check and cross check your work with the code.\nTo use this, you need to have R-Studio here, or Visual Studio Code here with Quarto plug-in (if you use VSC) in your computers. You will also need python and nbstata.\nA good place to start learning how to use Quarto for dynamic documents its here (for R-studio) or here (for VCS).\nI also have a small example using Quarto with Stata here.\nTry it on, and let me know if you have any problems."
  },
  {
    "objectID": "resources.html#other-resources-data",
    "href": "resources.html#other-resources-data",
    "title": "Resources",
    "section": "Other Resources: Data",
    "text": "Other Resources: Data\nIf you are looking for small and easy to work datasets for your projects, you may want to check the following:\n\nfrause: Its a repository I created to load data from the web into Stata. Contains all datasets used in Intro to Ecometrics book by Wooldridge.\ndatasciencedojo: This is a nice website that provides access to raw data that could be used for your projects. See their webpage."
  },
  {
    "objectID": "rmethods/10_pooldata.html",
    "href": "rmethods/10_pooldata.html",
    "title": "Pool Cross-section and Panel Data",
    "section": "",
    "text": "Up to this point, we have cover the analysis of cross-section data.\n\nMany individuals at a single point in time.\n\nTowards the end of the semester, We will also cover the analysis of time series data.\n\nA single individual across time.\n\nToday, we will cover the analysis of panel data and repeated crossection: Many individuals across time.\nThis type of data, also known as longitudinal data, has advantages over crossection, as it provides more information that helps dealing with the unknown of \\(e\\).\nAnd its often the only way to answer certain questions."
  },
  {
    "objectID": "rmethods/10_pooldata.html#pooling-data-together-cross-section-and-panel-data",
    "href": "rmethods/10_pooldata.html#pooling-data-together-cross-section-and-panel-data",
    "title": "Pool Cross-section and Panel Data",
    "section": "",
    "text": "Up to this point, we have cover the analysis of cross-section data.\n\nMany individuals at a single point in time.\n\nTowards the end of the semester, We will also cover the analysis of time series data.\n\nA single individual across time.\n\nToday, we will cover the analysis of panel data and repeated crossection: Many individuals across time.\nThis type of data, also known as longitudinal data, has advantages over crossection, as it provides more information that helps dealing with the unknown of \\(e\\).\nAnd its often the only way to answer certain questions."
  },
  {
    "objectID": "rmethods/10_pooldata.html#pooling-independent-crossections",
    "href": "rmethods/10_pooldata.html#pooling-independent-crossections",
    "title": "Pool Cross-section and Panel Data",
    "section": "Pooling independent crossections",
    "text": "Pooling independent crossections\n\nWe first consider the case of independent crossections.\n\nWe have access to surveys that may be collected regularly. (Household budget surveys)\nWe assume that individuals across this surveys are independent from each other (no panel structure).\n\nThis scenario is typically used for increasing sample-sizes and thus power of analysis (larger N smaller SE)\nOnly minor considerations are needed when analyzing this type of data.\n\nWe need to account for the fact Data comes from different years. This can be done by including year dummies.\nMay need to Standardize variables to make them comparable across years. (inflation adjustments, etc.)"
  },
  {
    "objectID": "rmethods/10_pooldata.html#example",
    "href": "rmethods/10_pooldata.html#example",
    "title": "Pool Cross-section and Panel Data",
    "section": "Example",
    "text": "Example\nLets use the data fertil1 to estimate the changes in fertility rates across time. This data comes from the General Social Survey.\n\nfrause fertil1, clear\nregress kids educ age agesq black east northcen west farm othrural town smcity i.year, robust  \n\n\nLinear regression                               Number of obs     =      1,129\n                                                F(17, 1111)       =      10.19\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.1295\n                                                Root MSE          =     1.5548\n\n------------------------------------------------------------------------------\n             |               Robust\n        kids | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |  -.1284268    .021146    -6.07   0.000    -.1699175   -.0869362\n         age |   .5321346   .1389371     3.83   0.000     .2595258    .8047433\n       agesq |   -.005804   .0015791    -3.68   0.000    -.0089024   -.0027056\n       black |   1.075658   .2013188     5.34   0.000     .6806496    1.470666\n        east |    .217324    .127466     1.70   0.088    -.0327773    .4674252\n    northcen |    .363114   .1167013     3.11   0.002     .1341342    .5920939\n        west |   .1976032   .1626813     1.21   0.225     -.121594    .5168003\n        farm |  -.0525575   .1460837    -0.36   0.719    -.3391886    .2340736\n    othrural |  -.1628537   .1808546    -0.90   0.368    -.5177087    .1920014\n        town |   .0843532   .1284759     0.66   0.512    -.1677295    .3364359\n      smcity |   .2118791   .1539645     1.38   0.169    -.0902149    .5139731\n             |\n        year |\n         74  |   .2681825   .1875121     1.43   0.153    -.0997353    .6361003\n         76  |  -.0973795   .1999339    -0.49   0.626    -.4896701    .2949112\n         78  |  -.0686665   .1977154    -0.35   0.728    -.4566042    .3192713\n         80  |  -.0713053   .1936553    -0.37   0.713    -.4512767    .3086661\n         82  |  -.5224842   .1879305    -2.78   0.006    -.8912228   -.1537456\n         84  |  -.5451661   .1859289    -2.93   0.003    -.9099776   -.1803547\n             |\n       _cons |  -7.742457   3.070656    -2.52   0.012     -13.7674   -1.717518\n------------------------------------------------------------------------------\n\n\n\nThis allow us to see how fertility rates have changed across time.\nOne could even interact the year dummies with other variables to see how the effect of other variables have changed across time.\n\n\nfrause cps78_85, clear\nregress lwage i.year##c.(educ i.female) exper expersq union, robust cformat(%5.4f)\n\n\nLinear regression                               Number of obs     =      1,084\n                                                F(8, 1075)        =     110.48\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.4262\n                                                Root MSE          =      .4127\n\n------------------------------------------------------------------------------\n             |               Robust\n       lwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     85.year |     0.1178     0.1239     0.95   0.342      -0.1253      0.3609\n        educ |     0.0747     0.0060    12.40   0.000       0.0629      0.0865\n    1.female |    -0.3167     0.0347    -9.12   0.000      -0.3848     -0.2486\n             |\n year#c.educ |\n         85  |     0.0185     0.0095     1.94   0.053      -0.0002      0.0371\n             |\n year#female |\n       85 1  |     0.0851     0.0518     1.64   0.101      -0.0165      0.1866\n             |\n       exper |     0.0296     0.0037     8.10   0.000       0.0224      0.0368\n     expersq |    -0.0004     0.0001    -5.11   0.000      -0.0006     -0.0002\n       union |     0.2021     0.0293     6.89   0.000       0.1446      0.2597\n       _cons |     0.4589     0.0855     5.37   0.000       0.2911      0.6267\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/10_pooldata.html#good-old-friend-chow-test",
    "href": "rmethods/10_pooldata.html#good-old-friend-chow-test",
    "title": "Pool Cross-section and Panel Data",
    "section": "Good old Friend: Chow test",
    "text": "Good old Friend: Chow test\n\nThe Chow test can be used to test whether the coefficients of a regression model are the same across two groups.\n\nwe have seen this test back when we were discussing dummy variables.\n\nWe can also use this test to check if coefficients of a regression model are the same across two time periods. (Has the wage structure changed across time?)\n\nThis is the case of interest here.\n\nNot much changes with before. Although it can be a bit more tedious to code."
  },
  {
    "objectID": "rmethods/10_pooldata.html#example-1",
    "href": "rmethods/10_pooldata.html#example-1",
    "title": "Pool Cross-section and Panel Data",
    "section": "Example",
    "text": "Example\n\nfrause cps78_85, clear\nregress lwage i.year##c.(educ i.female exper expersq i.union), robust\n\n\nLinear regression                               Number of obs     =      1,084\n                                                F(11, 1072)       =      82.83\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.4276\n                                                Root MSE          =     .41278\n\n------------------------------------------------------------------------------\n             |               Robust\n       lwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     85.year |   .1219978   .1521927     0.80   0.423    -.1766315    .4206271\n        educ |   .0768148   .0063312    12.13   0.000     .0643918    .0892378\n    1.female |  -.3155108   .0348402    -9.06   0.000    -.3838737    -.247148\n       exper |   .0249177   .0042985     5.80   0.000     .0164833    .0333522\n     expersq |  -.0002844   .0000918    -3.10   0.002    -.0004645   -.0001043\n     1.union |   .2039824   .0381315     5.35   0.000     .1291616    .2788033\n             |\n year#c.educ |\n         85  |    .013927   .0103252     1.35   0.178    -.0063329    .0341869\n             |\n year#female |\n       85 1  |   .0846136   .0524618     1.61   0.107    -.0183258     .187553\n             |\nyear#c.exper |\n         85  |   .0095289   .0073767     1.29   0.197    -.0049454    .0240033\n             |\n        year#|\n   c.expersq |\n         85  |  -.0002399   .0001592    -1.51   0.132    -.0005522    .0000724\n             |\n  year#union |\n       85 1  |  -.0018095   .0594387    -0.03   0.976    -.1184389      .11482\n             |\n       _cons |    .458257     .09386     4.88   0.000     .2740868    .6424271\n------------------------------------------------------------------------------\n\n\n\ntest 85.year#c.educ 85.year#1.female 85.year#c.exper   85.year#c.expersq 85.year#1.union\n\n\n ( 1)  85.year#c.educ = 0\n ( 2)  85.year#1.female = 0\n ( 3)  85.year#c.exper = 0\n ( 4)  85.year#c.expersq = 0\n ( 5)  85.year#1.union = 0\n\n       F(  5,  1072) =    1.65\n            Prob &gt; F =    0.1443"
  },
  {
    "objectID": "rmethods/10_pooldata.html#using-pool-crossection-for-causal-inference",
    "href": "rmethods/10_pooldata.html#using-pool-crossection-for-causal-inference",
    "title": "Pool Cross-section and Panel Data",
    "section": "Using Pool Crossection for Causal Inference",
    "text": "Using Pool Crossection for Causal Inference\n\nOne advantage of pooling crossection data is that it could to be used to estimate causal effects using a method known as Differences in Differences (DnD)\nConsider the following case:\n\nThere was a project regarding the construction of an incinerator in a city. You are asked to evaluate what the impact of this was on the prices of houses around the area.\nYou have access to data for two years: 1978 and 1981.\nIn 1978, there was no information about the project. In 1981, the project was announced, but it only began operations in 1985."
  },
  {
    "objectID": "rmethods/10_pooldata.html#section",
    "href": "rmethods/10_pooldata.html#section",
    "title": "Pool Cross-section and Panel Data",
    "section": "",
    "text": "we could start estimating the project using the simple model: \\[rprice = \\beta_0 + \\beta_1 nearinc + e\\]\n\nusing only 1981 data. But this would not be a good idea. Why?\n\nfrause kielmc, clear\nregress rprice nearinc if year == 1981, robust\n\n\nLinear regression                               Number of obs     =        142\n                                                F(1, 140)         =      24.35\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.1653\n                                                Root MSE          =      31238\n\n------------------------------------------------------------------------------\n             |               Robust\n      rprice | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     nearinc |  -30688.27   6219.265    -4.93   0.000     -42984.1   -18392.45\n       _cons |   101307.5   2951.195    34.33   0.000     95472.84    107142.2\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/10_pooldata.html#section-1",
    "href": "rmethods/10_pooldata.html#section-1",
    "title": "Pool Cross-section and Panel Data",
    "section": "",
    "text": "We could also estimate the model using only 1971 data. What would this be showing us?\n\n\nregress rprice nearinc if year == 1978, robust\n\n\nLinear regression                               Number of obs     =        179\n                                                F(1, 177)         =       9.87\n                                                Prob &gt; F          =     0.0020\n                                                R-squared         =     0.0817\n                                                Root MSE          =      29432\n\n------------------------------------------------------------------------------\n             |               Robust\n      rprice | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     nearinc |  -18824.37   5992.564    -3.14   0.002    -30650.44   -6998.302\n       _cons |   82517.23   1881.165    43.86   0.000     78804.83    86229.63\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/10_pooldata.html#section-2",
    "href": "rmethods/10_pooldata.html#section-2",
    "title": "Pool Cross-section and Panel Data",
    "section": "",
    "text": "So, using 1981 data we capture the Total price difference between houses near and far from the incinerator.\n\nThis captures both the announcement effect of the project, but also other factors (where would an incinerator be built?).\n\nUsing 1978 data we capture the price difference between houses near and far from the incinerator in the absence of the project.\n\nThis captures the effect of other factors that may be correlated with the incinerator project.\n\nUse both to see the impact!\n\n\\[Effect = -30688.27-(-18824.37)= -11863.9\\]\n\nThis is in essence a DnD model"
  },
  {
    "objectID": "rmethods/10_pooldata.html#difference-in-differences",
    "href": "rmethods/10_pooldata.html#difference-in-differences",
    "title": "Pool Cross-section and Panel Data",
    "section": "Difference in Differences",
    "text": "Difference in Differences\n\n\n\n\n\n\n\n\n\n\nControl\nTreatment\nTreat-Control\n\n\n\n\nPre-\n\\(\\bar y_{00}\\)\n\\(\\bar y_{10}\\)\n\\(\\bar y_{10}\\)-\\(\\bar y_{00}\\)\n\n\nPost-\n\\(\\bar y_{01}\\)\n\\(\\bar y_{11}\\)\n\\(\\bar y_{10}\\)-\\(\\bar y_{00}\\)\n\n\nPost-pre\n\\(\\bar y_{01}\\)-\\(\\bar y_{00}\\)\n\\(\\bar y_{11}\\)-\\(\\bar y_{10}\\)\nDD\n\n\n\n\nPost-Pre:\n\nTrend changes for the control\nTrend changes for the treated: A mix of the impact of the treatment and the trend change.\n\nTreat-Control:\n\nBaseline difference when looking at Pre-period\nTotal Price differentials when looking at Post-period: Mix of the impact of the treatment and the baseline difference.\n\nTake the Double Difference and you get the treatment effect."
  },
  {
    "objectID": "rmethods/10_pooldata.html#difference-in-differences-regression",
    "href": "rmethods/10_pooldata.html#difference-in-differences-regression",
    "title": "Pool Cross-section and Panel Data",
    "section": "Difference in Differences: Regression",
    "text": "Difference in Differences: Regression\n\nThis could also be achieved using a regression model:\n\n\\[ y = \\beta_0 + \\beta_1 post + \\beta_2 treat + \\beta_3 post*treat + e\\]\nWhere \\(\\beta_3\\) is the treatment effect. (only for 2x2 DD)\n\nregress rprice nearinc##y81, robust\n\n\nLinear regression                               Number of obs     =        321\n                                                F(3, 317)         =      17.75\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.1739\n                                                Root MSE          =      30243\n\n------------------------------------------------------------------------------\n             |               Robust\n      rprice | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   1.nearinc |  -18824.37    5996.47    -3.14   0.002    -30622.28   -7026.461\n       1.y81 |   18790.29   3498.376     5.37   0.000     11907.32    25673.26\n             |\n nearinc#y81 |\n        1 1  |   -11863.9   8635.585    -1.37   0.170    -28854.21    5126.401\n             |\n       _cons |   82517.23   1882.391    43.84   0.000     78813.67    86220.79\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/10_pooldata.html#difference-in-differences-regression-controls",
    "href": "rmethods/10_pooldata.html#difference-in-differences-regression-controls",
    "title": "Pool Cross-section and Panel Data",
    "section": "Difference in Differences: Regression + controls",
    "text": "Difference in Differences: Regression + controls\n\nOne advantage of DD is that it can control for those unobserved factors that may be correlated with outcome.\n\nWithout controls, however, estimates may not have enough precision.\n\nBut, we could add controls!\n\n\\[ y = \\beta_0 + X \\gamma + \\beta_1 post + \\beta_2 treat + \\beta_3 post*treat + e\\]\n\nBut its not as easy as it may seem! (just adding regressions is not a good approach)\nThis method requires other assumptions! (\\(\\gamma\\) is fixed), which may be very strong.\n\n\nNote: For DD to work, you need to assume the two groups follow the same path in the absence of the treatment. (Parallel trends assumption)\nOtherwise, you are just using trend differences!"
  },
  {
    "objectID": "rmethods/10_pooldata.html#diff-in-diff-in-diff",
    "href": "rmethods/10_pooldata.html#diff-in-diff-in-diff",
    "title": "Pool Cross-section and Panel Data",
    "section": "Diff in Diff in Diff",
    "text": "Diff in Diff in Diff\nAn Alternative approach is to use a triple difference model.\nSetup:\n\nYou still have two groups: Control and Treatment (which are easily identifiable)\nYou have two time periods: Pre and Post (which are also easily identifiable)\nYou have a different sample, where you can identify controls and treatment, as well as the pre- and post- periods. This sample was not treated!\n\nEstimation:\n\nEstimate the DD for the Original Sample, and the new untreated sample.\nObtaining the difference between these two estimates will give you the triple difference.\n\nExample: Smoking ban analysis based on age. (DD) But using both treated and untreated States (DDD)"
  },
  {
    "objectID": "rmethods/10_pooldata.html#general-framework-and-pseudo-panels",
    "href": "rmethods/10_pooldata.html#general-framework-and-pseudo-panels",
    "title": "Pool Cross-section and Panel Data",
    "section": "General Framework and Pseudo Panels",
    "text": "General Framework and Pseudo Panels\n\nOne general Structure for Policy analysis is the use of Pseudo Panels structure.\n\nPseudo panels are a way to use repeated crossection data, but controlling for some unobserved heterogeneity across specific groups. (the pseudo panels)\n\nFor Pseudo-panels, we need to identify a group that could be followed across time.\n\nThis cannot be a group of individuals (repeated crosection).\nBut we could use groups of states, cohorts (year of birth), etc.\n\nIn this case, the data would look like this: \\[y_{igt} = \\lambda_t + \\alpha_g + \\beta x_{gt} + z_{igt}\\gamma +  e_{igt}\\]\nWhere \\(g\\) is the group, \\(t\\) is the time, and \\(i\\) is the individual.\nThis model can be estimated by using dummies. (one dummy for each group and time-period)\nAnd \\(\\beta\\) is the coefficient of interest. (impact of the Policy \\(x_{gt}\\)).\n\nThis may ony work if we assume \\(\\beta\\) is constant across time and groups."
  },
  {
    "objectID": "rmethods/10_pooldata.html#alternative",
    "href": "rmethods/10_pooldata.html#alternative",
    "title": "Pool Cross-section and Panel Data",
    "section": "Alternative",
    "text": "Alternative\n\nWe could also use a more general model: \\[y_{igt} = \\lambda_{gt}+ \\beta x_{gt} + z_{igt}\\gamma +  e_{igt}\\]\nwhere \\(\\lambda_{gt}\\) is a group-time fixed effect. (Dummy for each group-time combination)\n\nNevertheless, while more flexible, this also imposes other types of assumptions, and might even be unfeasible if we have a large number of groups and time periods.\n\nStill, we require \\(\\beta\\) to be homogenous. If that is not the case, you may still suffer from contamination bias."
  },
  {
    "objectID": "rmethods/10_pooldata.html#period-panel-data",
    "href": "rmethods/10_pooldata.html#period-panel-data",
    "title": "Pool Cross-section and Panel Data",
    "section": "2-period Panel data",
    "text": "2-period Panel data\n\nPanel Data, or longitudinal data, is a type of data that has information about the same individual across time.\nThe simplest Structure is one where individuals are followed over only 2 periods.\nThe main advantage of panel data (even two periods version) is that it allows us to control for unobserved heterogeneity across individuals.\n\nBut only if you want to assume fixed effects are constant across time."
  },
  {
    "objectID": "rmethods/10_pooldata.html#section-3",
    "href": "rmethods/10_pooldata.html#section-3",
    "title": "Pool Cross-section and Panel Data",
    "section": "",
    "text": "So how does this reflects in the model specification?\n\n\\[y_{it} = \\beta_0 + \\beta_1 x_{it} + \\beta_2 z_{t} + \\beta_3 w_{i} + e_i + e_t + e_{it}\\]\n\nWhere \\(i\\) refers to individuals or panel units, and \\(t\\) refers to time periods.\nAlso, \\(X's\\), \\(X's\\) \\(W's\\) are variables that vary across individual and time, across time or across individuals.\nThere are also three types of errors. Those that contains unobserved that vary across individuals \\(e_i\\), across time \\(e_t\\), and across individuals and time \\(e_{it}\\) (Idiosyncratic error).\n\\(e_i\\) is usually referred to as the individual fixed effect, and \\(e_t\\) as the time fixed effect.\nIn a 2 period panel, controlling for time-effects is may not be necessary (its just one dummy)\nWhat is more concerning is the unobserved individual fixed effect.\n\nThis is pretty similar to the generalized Pooling model we saw before."
  },
  {
    "objectID": "rmethods/10_pooldata.html#how-estimation-changes",
    "href": "rmethods/10_pooldata.html#how-estimation-changes",
    "title": "Pool Cross-section and Panel Data",
    "section": "How estimation changes",
    "text": "How estimation changes\nFor time use, we assume we control with a single dummy.\n\nYou can choose to “ignore” individual effects.\n\n\\[y_{it} = \\beta_0 + \\beta_1 x_{it} + \\beta_2 w_{i} + \\delta t + (v_{it} = e_i + e_{it})\\]\n\nRequires \\(e_i\\) to be uncorrelated with \\(x_{it}\\) (otherwise is biased), and Standard Errors will need to be clustered at the individual level.\n\n\nYou can aim to estimate all individual fixed effects using dummies (FE estimator). \\[y_{it} = \\beta_0 + \\beta_1 x_{it} + \\delta t + \\sum \\alpha_i D_i + e_{it})\\]\n\nTime fixed variables cannot be estimated anymore"
  },
  {
    "objectID": "rmethods/10_pooldata.html#section-4",
    "href": "rmethods/10_pooldata.html#section-4",
    "title": "Pool Cross-section and Panel Data",
    "section": "",
    "text": "You can estimate the model in differences (FD estimator)\n\n\\[\\begin{aligned}\ny_{i1} &= \\beta_0 + \\beta_1 x_{i1} + \\delta + e_i + e_{i1} \\\\\ny_{i0} &= \\beta_0 + \\beta_1 x_{i0} + e_i + e_{i0} \\\\\n\\Delta y_{i} &= \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta_1 \\Delta x_{i1} + \\delta + \\Delta e_{i}\n\\end{aligned}\n\\]\n\nNow you have only 1 observation per panel, instead of 2. And the result would be identical to FE estimator."
  },
  {
    "objectID": "rmethods/10_pooldata.html#example-2",
    "href": "rmethods/10_pooldata.html#example-2",
    "title": "Pool Cross-section and Panel Data",
    "section": "Example",
    "text": "Example\n\n** This data is in wide format\nfrause slp75_81, clear\n** Lets reshape it so its in standard long format\ngen id = _n\nreshape long educ gdhlth marr slpnap totwrk yngkid, i(id) j(year)\ngen time = year==81\nxtset id time\n** Regression as Pool Crossection\nqui: reg slpnap time totwrk educ marr yngkid gdhlth male, cluster(id)\nest sto m1\n** using FE\nqui: areg slpnap time totwrk educ marr yngkid gdhlth male, absorb(id) cluster(id)\nest sto m2\n** using FD\nqui: reg d.slpnap d.time d.totwrk d.educ d.marr d.yngkid d.gdhlth d.male, robust\nest sto m3\n\n(j = 75 81)\n\nData                               Wide   -&gt;   Long\n-----------------------------------------------------------------------------\nNumber of observations              239   -&gt;   478         \nNumber of variables                  21   -&gt;   16          \nj variable (2 values)                     -&gt;   year\nxij variables:\n                          educ75 educ81   -&gt;   educ\n                      gdhlth75 gdhlth81   -&gt;   gdhlth\n                          marr75 marr81   -&gt;   marr\n                      slpnap75 slpnap81   -&gt;   slpnap\n                      totwrk75 totwrk81   -&gt;   totwrk\n                      yngkid75 yngkid81   -&gt;   yngkid\n-----------------------------------------------------------------------------\n\nPanel variable: id (strongly balanced)\n Time variable: time, 0 to 1\n         Delta: 1 unit"
  },
  {
    "objectID": "rmethods/12_timeseries.html#the-nature-of-time-series-data",
    "href": "rmethods/12_timeseries.html#the-nature-of-time-series-data",
    "title": "Times Series Part-I",
    "section": "The nature of time series data",
    "text": "The nature of time series data\n\nTime series “works” different from Repeated crossection.\n\nYou do not have access to a random sample. (Window of time if fixed)\nYou have access to a single “random” time line\n\nAnd in time series, one needs to be quite aware that Data has Baggage…What you see today is the product of everything that happens in the far past.\nThis is what we call Past Dependent, or simple serial correlation.\n\nAnd is why we need to be careful when we use time series data."
  },
  {
    "objectID": "rmethods/12_timeseries.html#the-nature-of-time-series-data-1",
    "href": "rmethods/12_timeseries.html#the-nature-of-time-series-data-1",
    "title": "Times Series Part-I",
    "section": "The nature of time series data",
    "text": "The nature of time series data\n\nData cannot not be arbitrarily reordered. (Past affect future)\n\nTypical features: serial correlation/nonindependence of observations\n\nRandomness of the data comes from the uncertainty of shocks that affects a variable over time, not from sampling.\nYour “Sample” is one realized path that you observe in a narrow window of time.\nBecause observations are no longer independent, we will need to worry about correlation across time.\nIn fact, because data may be strongly correlated across time (say your age), it may generate some problems when applying OLS.\n\nHighly correlated data (high innertia) will have common “trends” that do not necessarity reflect the causal relationship between variables.\n\nSo, we must learn “new” tools to deal with this problem."
  },
  {
    "objectID": "rmethods/12_timeseries.html#static-model",
    "href": "rmethods/12_timeseries.html#static-model",
    "title": "Times Series Part-I",
    "section": "1: Static model",
    "text": "1: Static model\n\nThe static model is the simplest model for analyzing time series data. (like SLRM)\nA Static model aims to find correlations between contemporaneous variables.\n\nImplicity, this assumes there are no dynamic interactions among variables\n\n\n\\[GDP_t = a_0 + a_1 educ_t + a_2 Invest_t + a_3 Unemp_t + u_t\\]\nEducation, investment and Unemployment rate are assumed to affect GDP contemporaneously. But Lags of Leads of the data has no effect on GDP.\n\nThese models are not useful for Forecasting, and Only produces reasonable estimates under very strong assumptions (we will see this later)."
  },
  {
    "objectID": "rmethods/12_timeseries.html#finite-distributed-lag-model-fdl",
    "href": "rmethods/12_timeseries.html#finite-distributed-lag-model-fdl",
    "title": "Times Series Part-I",
    "section": "2: Finite Distributed Lag model (FDL)",
    "text": "2: Finite Distributed Lag model (FDL)\n\nThe FDL model is a simple extension of the static model that allows for dynamic interactions of independent variables.\n\nFinite Because we choose How far back (lags) to add to the model\nDistributed Because each lag will have a different effect on the dependent variable.\n\nSimple Example: \\[fr_t = a_0 + a_1 te_t +e_t\\] \\[fr_t = a_0 + a_1 te_t + a_2 te_{t-1}+ a_3 te_{t-2}+e_t\\]\n\n\\(fr_t\\): Fertility Rate; \\(te_t\\): Tax exemption\nThis is an FDL model with 2 lags."
  },
  {
    "objectID": "rmethods/12_timeseries.html#section",
    "href": "rmethods/12_timeseries.html#section",
    "title": "Times Series Part-I",
    "section": "",
    "text": "More Generality, FDL of order q is defined as:\n\n\\[y_t = a_0 + \\sum_{k=0}^q \\delta_k z_{t-k} + e_t\\]\n\nYou can choose Lags using F-statistic, but also considering the “loss” of Degrees of freedom.\n\nMore lags, less data to estimate the coefficients, more coefficients to estimate\nCoefficients may suffer from High Collinearity\nAllow us to draw inference on Duration of effects.\n\nTwo Types of Effects:\n\nTransitory effects \\(\\frac{\\partial y_t}{\\partial z_{t-q}}=\\delta_q\\)\nPermanent effect \\(\\frac{\\partial y_t}{\\partial z}=\\sum \\frac{\\partial y_t}{\\partial z_{t-q}}=\\sum \\delta_k\\)"
  },
  {
    "objectID": "rmethods/12_timeseries.html#section-1",
    "href": "rmethods/12_timeseries.html#section-1",
    "title": "Times Series Part-I",
    "section": "",
    "text": "What do you expect to see?\n\n\n\n\n\n\n\n\n\nTransitory\n\n\n\n\n\n\n\nPermanent\n\n\n\n\n\n\nTransitory effects measure the short-term effect on outcome (Only of the additional unit)\nPermanent effects measure the long-term effect on outcome (adding up Transitory effects)"
  },
  {
    "objectID": "rmethods/12_timeseries.html#infinite-distributed-lag-model-idl",
    "href": "rmethods/12_timeseries.html#infinite-distributed-lag-model-idl",
    "title": "Times Series Part-I",
    "section": "3: Infinite Distributed Lag model (IDL)",
    "text": "3: Infinite Distributed Lag model (IDL)\n\nThis is a more advanced model that allows for the effects of independent variables to last forever, but how?\n\nA model with infinite number of lags cannot be estimated…unless some restrictions are imposed.\n\n\n\\[ \\text{Wrong: } y_t = a_0 + \\sum_{k=0}^{\\infty} \\delta_k z_{t-k} + e_t\\] \\[ \\text{Better: } y_t = a_0 + \\sum_{k=0}^{\\infty} \\gamma \\delta^k z_{t-k} + e_t\\]\n\nSo we went from pretending to estimate an infinite number of coefficients \\(\\delta_k\\) to estimating only two parameters \\(\\gamma\\) and \\(\\delta\\).\n\nThis is called the Geometric Distributed Lag model."
  },
  {
    "objectID": "rmethods/12_timeseries.html#section-2",
    "href": "rmethods/12_timeseries.html#section-2",
    "title": "Times Series Part-I",
    "section": "",
    "text": "GDL requires an additional “Trick”:\n\n\\[\\begin{aligned}\ny_t &= a_0 + \\gamma z_t + \\gamma \\rho z_{t-1} + \\dots + e_t \\\\\ny_{t-1} &= a_0 + \\gamma z_{t-1} + \\gamma \\rho z_{t-2} + \\dots + e_{t-1}\n\\end{aligned}\n\\]\n\nSubtracting the second equation (times \\(\\rho\\) ) from the first one, we get:\n\n\\[y_t =  \\rho y_{t-1} + a_0 (1-\\rho) + \\gamma z_t  + v_{t}\\]\nWhich requires really strong assumptions!\n\nThe short and Long effects are:\n\n\\[\\text{Short}\\frac{\\partial y_t}{\\partial z_{t-k}}=\\gamma \\rho^k \\text{ and }\n\\text{Long}\\frac{\\partial y_t}{\\partial z}=\\frac{\\gamma}{1-\\rho}\\]"
  },
  {
    "objectID": "rmethods/12_timeseries.html#rational-distributed-lag-model-rdl",
    "href": "rmethods/12_timeseries.html#rational-distributed-lag-model-rdl",
    "title": "Times Series Part-I",
    "section": "4: Rational Distributed Lag model (RDL)",
    "text": "4: Rational Distributed Lag model (RDL)\n\nBecause IDL imposes strong assumptions on coefficients, we can relax them by allowing for lags. This is called the RDL model. \\[y_t = a_0 + \\gamma_0 z_t + \\gamma_1 z_t  +\\delta y_{t-1} + e_t- \\rho e_{t-1}\\]\nWhich has the following short and long effects:\n\n\\[\\text{ Short:}\\frac{\\partial y_t}{\\partial z_t} = \\gamma_0  \\] \\[\\text{ Short:}\\frac{\\partial y_t}{\\partial z_{t-k}} = \\rho^{k-1}(\\rho \\gamma_0 + \\gamma_1) \\] \\[\\text{ Long:}\\frac{\\partial y_t}{\\partial z} = \\frac{\\gamma_0 + \\gamma_1}{1-\\rho}\n\\]"
  },
  {
    "objectID": "rmethods/12_timeseries.html#assumptions-m1-and-m2",
    "href": "rmethods/12_timeseries.html#assumptions-m1-and-m2",
    "title": "Times Series Part-I",
    "section": "Assumptions: M1 and M2",
    "text": "Assumptions: M1 and M2\nA1. Linear in Parameters: Same old, same old, \\(y_t = \\beta_0 + \\beta_1 x_{1t} + \\dots + \\beta_k x_{kt} + u_t\\)\nA2. No Perfect Collinearity: Also Same old, same old"
  },
  {
    "objectID": "rmethods/12_timeseries.html#section-3",
    "href": "rmethods/12_timeseries.html#section-3",
    "title": "Times Series Part-I",
    "section": "",
    "text": "The Stronger ones\n\\[X=\\begin{pmatrix}\nx_{11} & x_{12} & \\dots & x_{1k} \\\\\nx_{21} & x_{22} & \\dots & x_{2k} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{T1} & x_{T2} & \\dots & x_{Tk}\n\\end{pmatrix}\n\\]\nA3. Zero Conditional Mean\n\\[E(u_t|X)=0\n\\]\nSo that \\(X\\) is strictly Exogenous (across all possible times).\nNot only \\(x_t\\) should not be affected by \\(u_t\\), but neither should \\(x_{t-1}\\) nor \\(x_{t+1}\\)\nA1-A3 will guarantee that OLS is unbiased."
  },
  {
    "objectID": "rmethods/12_timeseries.html#what-about-std-errors",
    "href": "rmethods/12_timeseries.html#what-about-std-errors",
    "title": "Times Series Part-I",
    "section": "What about Std Errors?",
    "text": "What about Std Errors?\nA4: Strong Homoskedasticity\n\\[Var(u_t|X)=\\sigma^2\n\\]\nA5: No Serial Correlation (Correlation across time of the errors)\n\\[Corr(u_t,u_s|X)=0 \\text{ for all } t\\neq s\n\\]\nAlso difficult to fulfill, because unobserved may have inertia, and depend on past values."
  },
  {
    "objectID": "rmethods/12_timeseries.html#section-4",
    "href": "rmethods/12_timeseries.html#section-4",
    "title": "Times Series Part-I",
    "section": "",
    "text": "Nevertheless, A1-A5: Standard errors can be estimated using the usual formula:\n\\[\\begin{aligned}\n\\hat{Var}(\\hat{\\beta}) &= \\hat{\\sigma}^2(X'X)^{-1} \\\\\n\\hat{Var}(\\hat{\\beta_k}) &= \\frac{\\hat{\\sigma}^2}{SST_k(1-R^2_k)} \\\\\n\\hat \\sigma^2 &= \\frac{1}{T-k-1}\\sum_{t=1}^T \\hat{u}_t^2\n\\end{aligned}\n\\]\nWhich are BLUE! (Best Linear Unbiased Estimators)\nA6: Normality, The \\(\\beta\\)’s are normally distributed, and F-tests and t-tests are valid."
  },
  {
    "objectID": "rmethods/12_timeseries.html#example-the-effet-of-inflation-and-deficit-on-interest-rates",
    "href": "rmethods/12_timeseries.html#example-the-effet-of-inflation-and-deficit-on-interest-rates",
    "title": "Times Series Part-I",
    "section": "Example: The effet of inflation and Deficit on Interest rates",
    "text": "Example: The effet of inflation and Deficit on Interest rates\nModel: \\(i_t = \\beta_0 + \\beta_1 inf_t + \\beta_2 def_t + u_t\\)\nA1: \\(\\checkmark\\) (but questionable)\nA2: \\(\\checkmark\\) (almost never a problem)\nA3: NO! Deficits and inflation today may affect adjustments in the future (\\(u_{t+1}\\)), Similarly, \\(u_t\\) may have to be adjusted in the future using Deficits and inflation.\nA4: Perhaps? Usually there is a direct relationship between deficit and uncertainty, which will generate heteroskedasticity.\nA5: NO! There could be many things in \\(u_t\\) that are correlated across time. (taxes?)\nA6: NO…the errors are almost never normal\n\nfrause intdef, clear\nreg i3 inf def\n\n\n\n\n\n      Source |       SS           df       MS      Number of obs   =        56\n-------------+----------------------------------   F(2, 53)        =     40.09\n       Model |  272.420338         2  136.210169   Prob &gt; F        =    0.0000\n    Residual |  180.054275        53  3.39725047   R-squared       =    0.6021\n-------------+----------------------------------   Adj R-squared   =    0.5871\n       Total |  452.474612        55  8.22681113   Root MSE        =    1.8432\n\n------------------------------------------------------------------------------\n          i3 | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         inf |   .6058659   .0821348     7.38   0.000     .4411243    .7706074\n         def |   .5130579   .1183841     4.33   0.000     .2756095    .7505062\n       _cons |   1.733266    .431967     4.01   0.000     .8668497    2.599682\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/12_timeseries.html#event-studies",
    "href": "rmethods/12_timeseries.html#event-studies",
    "title": "Times Series Part-I",
    "section": "1: Event Studies",
    "text": "1: Event Studies\n\nWe can use Dummies to represent Transitory shocks (events) on the outcome\n\nDummies for the impact of Covid (if we assume effect was transitory), 0 for all periods except for months we were at home.\n\nOr use Dummies to capture permanent changes in the outcome\n\nDummies for ChatGPT. 0 before the introduction, 1 after\n\nPossible to use Lags of Dummies to see the dynamics of the impact.\n\nWith Time series may not be as useful, because its easy to mix event effects with trends, although one could also directly control for trends.\n\n\n\\[FRate_t = 98.7 + 0.08 PE_t - 24.24 WW2 - 31.6 Pill_t+ e_t\\]\n\n\\(Pill_t\\), \\(WW2_t\\) are dummies for the introduction of the pill (permanent) and WW2 (transitory) effects on Fertility rate."
  },
  {
    "objectID": "rmethods/12_timeseries.html#logs-and-growth-models",
    "href": "rmethods/12_timeseries.html#logs-and-growth-models",
    "title": "Times Series Part-I",
    "section": "2: Logs and Growth models",
    "text": "2: Logs and Growth models\n\nVery Similar to what was done in Cross Sectional Models.\nUsing Logs of the Dep variable changes the interpretation of the coefficients.\n\n\\[\\Delta log(x)\\simeq \\%\\Delta x\\]\n\nBecause of that, you can use “log-models” and a trend to estimate the growth rates.\n\nreg log_gdp year\nThe coefficient of year should give you the average growth rate of GDP.\n\nBut the model can also be used in levels to identify trends."
  },
  {
    "objectID": "rmethods/12_timeseries.html#trends-and-seasonality",
    "href": "rmethods/12_timeseries.html#trends-and-seasonality",
    "title": "Times Series Part-I",
    "section": "3: Trends and Seasonality",
    "text": "3: Trends and Seasonality\n\nTrends are very common in time series data.\n\nBecause of the “inertia” of the data, its very common to see variables sharing common trends even if they are completely unrelated. (GDP and age)\n\nIgnoring this may cause problems, as one may identify spurious relationships. (things that look to have significant effects, even tho they are not related)\nConsider the following model (investment on housing, and housing prices):\n\n\nqui:frause hseinv,clear\nreg  linvpc lprice\n\n\n      Source |       SS           df       MS      Number of obs   =        42\n-------------+----------------------------------   F(1, 40)        =     10.53\n       Model |  .254364468         1  .254364468   Prob &gt; F        =    0.0024\n    Residual |  .966255566        40  .024156389   R-squared       =    0.2084\n-------------+----------------------------------   Adj R-squared   =    0.1886\n       Total |  1.22062003        41   .02977122   Root MSE        =    .15542\n\n------------------------------------------------------------------------------\n      linvpc | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      lprice |   1.240943   .3824192     3.24   0.002     .4680452    2.013841\n       _cons |  -.5502345   .0430266   -12.79   0.000    -.6371945   -.4632746\n------------------------------------------------------------------------------\n\n\n\nIf we estimate this model, we find a very strong relationship, perhaps because of common trends. Adding a trend, however, may change the results.\n\n\\[E(log(invpc_t)|x) = -20.04 -0.38 log(price) + 0.009 year \\]\n\nreg linvpc lprice  year\n\n\n      Source |       SS           df       MS      Number of obs   =        42\n-------------+----------------------------------   F(2, 39)        =     10.08\n       Model |  .415945108         2  .207972554   Prob &gt; F        =    0.0003\n    Residual |  .804674927        39   .02063269   R-squared       =    0.3408\n-------------+----------------------------------   Adj R-squared   =    0.3070\n       Total |  1.22062003        41   .02977122   Root MSE        =    .14364\n\n------------------------------------------------------------------------------\n      linvpc | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      lprice |  -.3809612   .6788352    -0.56   0.578    -1.754035    .9921125\n        year |   .0098287   .0035122     2.80   0.008     .0027246    .0169328\n       _cons |  -20.03976   6.964526    -2.88   0.006    -34.12684   -5.952675\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/12_timeseries.html#trends-and-seasonality-1",
    "href": "rmethods/12_timeseries.html#trends-and-seasonality-1",
    "title": "Times Series Part-I",
    "section": "3: Trends and Seasonality",
    "text": "3: Trends and Seasonality\n\nJust as time series are characterized by trends, they are also characterized by seasonality.\n\nSeasonality is the presence of regular patterns in the data that repeat over fixed periods of time.\nSeasonality is a form of “deterministic” variation, because it is predictable.\n\nFor example, If you look at Public expenditure, you will see that its higher the last year that a president is in office. (election year)\nSimilarly, you will see higher expenditure in December, because of Christmas.\nAs with trends, this may cause spurious relations, thus, its recommended to control for seasonality adding dummies.\n\nquarter, month, day of the week, year after election, etc.\n\nAs simple as adding dummies for each month, or quarter, etc."
  },
  {
    "objectID": "rmethods/12_timeseries.html#r2-and-spurious-regressions",
    "href": "rmethods/12_timeseries.html#r2-and-spurious-regressions",
    "title": "Times Series Part-I",
    "section": "4: \\(R^2\\) and Spurious Regressions",
    "text": "4: \\(R^2\\) and Spurious Regressions\n\nOne of the consequences of spurious regressions is that the \\(R^2\\) will be inflated. (caputred by the common trend or seasonality\nEven if we add trends or seaonalities, the default \\(R^2\\) will be too large. (Because it still describes ALL variation)\nA better approach to understand the true explanatory power of the model is to use an \\(R^2\\) that adjusts for trends and seasonality.\n\n\\[y_t = \\beta_0 + \\beta_1 x_{1t} + \\beta_2 x_{2t} + \\theta \\times t + \\sum \\gamma_k \\times D_k + u_t\\]\nWhere \\(D_k\\) are dummies for seasonality, and \\(\\theta \\times t\\) is a trend."
  },
  {
    "objectID": "rmethods/12_timeseries.html#r2-and-spurious-regressions-1",
    "href": "rmethods/12_timeseries.html#r2-and-spurious-regressions-1",
    "title": "Times Series Part-I",
    "section": "4: \\(R^2\\) and Spurious Regressions",
    "text": "4: \\(R^2\\) and Spurious Regressions\n\nTo estimate the adjusted \\(R^2\\) it may be better to use de-trended and de-seasonalized data.\n\n\\[\\tilde w_t = w_t - E(w_t| t , D_1, D_2, \\dots, D_k) \\forall w \\in {y, x_1, x_2}\\]\n\nEstimate model\n\n\\[\\tilde y_t = \\beta_1 \\tilde x_{1t} + \\beta_2 \\tilde x_{2t} + u_t\\]\n\nCalculate the \\(R^2\\) using the “correct” \\(SST\\) and \\(SSE\\) using the demeaned data.\n\n\\[aR^2 = 1-\\frac{\\sum \\hat u^2_t}{\\sum \\tilde y^2_t}\\]"
  },
  {
    "objectID": "rmethods/1_introduction.html",
    "href": "rmethods/1_introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Econometrics is an amalgamation of Statistics and Economics, that typically analysis nonexperimental data.\n\nStatistics: Because we make use of numerous properties and mathematical properties to obtain derive Statitics related to our data\nEconomics: Because we aknowledge that we use data that comes from Agents interactions, and as such as subject to erros.\n\nWe use both tools to analyze data from the world around us.\n\nYour Economic intuition to make sense and explain relationships that you find, and mathematics/statistics to obtain estimates that are statistically sound."
  },
  {
    "objectID": "rmethods/1_introduction.html#what-is-econometrics",
    "href": "rmethods/1_introduction.html#what-is-econometrics",
    "title": "Introduction",
    "section": "",
    "text": "Econometrics is an amalgamation of Statistics and Economics, that typically analysis nonexperimental data.\n\nStatistics: Because we make use of numerous properties and mathematical properties to obtain derive Statitics related to our data\nEconomics: Because we aknowledge that we use data that comes from Agents interactions, and as such as subject to erros.\n\nWe use both tools to analyze data from the world around us.\n\nYour Economic intuition to make sense and explain relationships that you find, and mathematics/statistics to obtain estimates that are statistically sound."
  },
  {
    "objectID": "rmethods/1_introduction.html#when-is-it-useful",
    "href": "rmethods/1_introduction.html#when-is-it-useful",
    "title": "Introduction",
    "section": "When is it useful?",
    "text": "When is it useful?\nEconometrics is useful whenever we aim to:\n\nTest theories, Explore theoretical relationships, Verify Predictions\n\nBut also\n\nWe use Econometrics when we want to evaluate policies, or provide evidence for policy makers. Find that Causal effect.\n\nCaveat: We may not have the best data for this, but we can come up with cleaver designs to still do our job!"
  },
  {
    "objectID": "rmethods/1_introduction.html#where-does-econometric-fits-in-empirical-research",
    "href": "rmethods/1_introduction.html#where-does-econometric-fits-in-empirical-research",
    "title": "Introduction",
    "section": "Where does Econometric Fits in Empirical Research",
    "text": "Where does Econometric Fits in Empirical Research\n\nObvious answer\n\nYou will need Econometrics methods to analyze your data, however, (just as a reminder) you should be aware of “HOW” an Empirical Research should be made:\n\nS1. Research Question:\n\nA question that is Answerable (within bounderies of Time/Money/and availabilty)\nThat should help us understand a Topic Better\nThat is Specific Enough to be feasible, but General Enough to be of interest.\n\nKeep it simple"
  },
  {
    "objectID": "rmethods/1_introduction.html#where-does-econometric-fits-in-empirical-research-1",
    "href": "rmethods/1_introduction.html#where-does-econometric-fits-in-empirical-research-1",
    "title": "Introduction",
    "section": "Where does Econometric Fits in Empirical Research",
    "text": "Where does Econometric Fits in Empirical Research\nS2. Construct an Economic model\n\nTo understand what the problem before you start analyzing the research question. May not need a formal modeling (Heavy Math), but enough to have some understanding of the problem.\n\nS3. Decide on the Econometric model\n\nYou need to decide what data is needed and is available your model.\nHow you will estimate the model (assumptions on methods)\n\nS4. Estimate model, and Analyze Data\n\nEstimate the model, using Economtric tools and Methods fitted to the data and the research question.\nExplain results in light of your Economic Model, and theoretical predictions. The Economist in you!"
  },
  {
    "objectID": "rmethods/1_introduction.html#so-you-need-data",
    "href": "rmethods/1_introduction.html#so-you-need-data",
    "title": "Introduction",
    "section": "So you need Data",
    "text": "So you need Data"
  },
  {
    "objectID": "rmethods/1_introduction.html#need-for-data",
    "href": "rmethods/1_introduction.html#need-for-data",
    "title": "Introduction",
    "section": "Need for Data",
    "text": "Need for Data\nDifferent types of data may allow for using different econometric methodologies, and answer different types of questions.\n\nKeep in mind you will only have access to SAMPLES, never the Population\nThere will be instances that you come close to Population data, ie Census.\nBut Even Census data is not the Population (or Super Population we use in Econometrics)."
  },
  {
    "objectID": "rmethods/1_introduction.html#types-of-data",
    "href": "rmethods/1_introduction.html#types-of-data",
    "title": "Introduction",
    "section": "Types of Data:",
    "text": "Types of Data:\nCross-Section: Sample of the population collects data on Many individuals in a single point in time.\nTime Series Data: Data collected on a single individual across time.\nPanel Data: Data collected for Many individuals who are followed across time.\nRepeated Cross-Section: Pooled Cross-Section Data for different individuals collected at different points in time. Individuals are not followed across time."
  },
  {
    "objectID": "rmethods/1_introduction.html#visually",
    "href": "rmethods/1_introduction.html#visually",
    "title": "Introduction",
    "section": "Visually",
    "text": "Visually\n\nCrossectionTime SeriesPanel DataRepeated Crossection"
  },
  {
    "objectID": "rmethods/1_introduction.html#before-the-break",
    "href": "rmethods/1_introduction.html#before-the-break",
    "title": "Introduction",
    "section": "Before the Break:",
    "text": "Before the Break:\n\nCausality, Ceteris Paribus, and Counterfactuals\nThee important concepts for the Class\n\nCausality: This is what most applied research aims to identify. A causal effect is a change the variable interest experiences, only because a second variable changed, while all other factors remained FIXED.\n\nThis is different from associations or correlations.\n\nCeteris Paribus: In Econometric analysis, ceteris paribus implies that all factors, except the one analyzed, are assumed constant (There is no change), thus leading to causality\nCounterfactual: It is the consideration of what would have been if only a single factor changed in the analysis (for a given observation).\n\nWhat if didn’t apply to the MSC at Levy? If you got miss your plane to the US? etc."
  },
  {
    "objectID": "rmethods/1_introduction.html#thinking-about-counterfactuals-is-key",
    "href": "rmethods/1_introduction.html#thinking-about-counterfactuals-is-key",
    "title": "Introduction",
    "section": "Thinking about Counterfactuals is Key",
    "text": "Thinking about Counterfactuals is Key\nFor empirical work that aims to identify Causal Effects, it is important to understand the concept of counterfactual.\n\nIt will help you understand what is what you need to analyze,\nHow could those effects be identified in ideal scenarios (Experiments)\nWhat the limitation of those scenarios are\nAnd what alternatives are there to void those limitations"
  },
  {
    "objectID": "rmethods/1_introduction.html#example-causal-effect-of-fertilizer-on-crops",
    "href": "rmethods/1_introduction.html#example-causal-effect-of-fertilizer-on-crops",
    "title": "Introduction",
    "section": "Example: Causal effect of Fertilizer on Crops",
    "text": "Example: Causal effect of Fertilizer on Crops\nRQ: By how much will the production of soybeans increase if one increases the amount of fertilizer applied to the ground?\nCF: Same Piece of Land with and without Fertilizer (Impossible)\nEXP: Randomly Use Fertilizers Across different Plots of Land (Expensive but feasible)\nEA: Use Regressions to keep other all factors that can affect Land productivity fixed when Analyzing Expost Data (Inexpensive)"
  },
  {
    "objectID": "rmethods/1_introduction.html#example-causal-effect-of-smoking-on-babies-health",
    "href": "rmethods/1_introduction.html#example-causal-effect-of-smoking-on-babies-health",
    "title": "Introduction",
    "section": "Example: Causal Effect of Smoking on Babie’s health",
    "text": "Example: Causal Effect of Smoking on Babie’s health\nRQ: Does Smoking during Pregnancy decreases birthweight?\nCF: We consider the same woman. In one case she smokes through pregnancy, in the other she doesnt. Compare Babies Weight.\nEXP: Select a random sample of Pregnant Women and randomly select those who will be “forced” to smoke during pregnancy.\nEXP1: Select a Randome sample of PW with history of smoking. Randomly offer them a voucher and Counceling to quit smoking.\nEA: Consider women with similar characteristics, except for smoking, and compare their babies outcomes."
  },
  {
    "objectID": "rmethods/3_MLRM.html",
    "href": "rmethods/3_MLRM.html",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "The SLRM we cover last week is a powerful tool to understand the mechanics behind regression analysis, however is too limited.\n\nUse one control? to fix everything ?!\n\nThe Natural alternative is to relax the assumption and Make things more flexible.\n\nIn other words…Allow for adding More controls\n\n\nThus, instead of:\n\\[y_i = \\beta_0 + \\beta_1 x_i + e_i\n\\]\nwe have to consider:\n\\[y_i = \\beta_0 + \\beta_1 x_{1i} +\\beta_2 x_{2i} + \\dots + \\beta_k x_{ki} + u_i\n\\]\nHow many can we add? and why does it help?\n\n\n\n\nOne more explicitly accounts for variables that before were hidden in \\(e_i\\).\nWe add \\(x_{2i},x_{3i},\\dots,x_{ki}\\) to the model model, and is no longer in \\(e_i\\)\nAllows for richer model specifications and nonlinearities:\nBefore: \\(y_i = \\beta_0 + \\beta_1 x_{1i} + e_i\\)\nNow : \\(y_i = \\beta_0 + \\beta_1 x_{1i} +\\beta_2 x^2_{1i} + \\beta_3 x^{1/2}_{1i} + \\beta_4 x^{-1}_{1i} + \\beta_5 x_{2i}+\\dots+e_i\\)\n\nThus, we can get closer to the unknown Population function, and explicitly handle some endogeneity problems (we control for it).\n\n\n\n\n\n\nWith great power…\n\n\n\nBeing able to add more controls is good, but:\n\nMay make things worse (bad controls)\nOr might not be feasible (small Sample)\nOr may be difficult to interpret (unless you know how to)"
  },
  {
    "objectID": "rmethods/3_MLRM.html#why-stay-with-1-when-you-can-use-many-why-not",
    "href": "rmethods/3_MLRM.html#why-stay-with-1-when-you-can-use-many-why-not",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "The SLRM we cover last week is a powerful tool to understand the mechanics behind regression analysis, however is too limited.\n\nUse one control? to fix everything ?!\n\nThe Natural alternative is to relax the assumption and Make things more flexible.\n\nIn other words…Allow for adding More controls\n\n\nThus, instead of:\n\\[y_i = \\beta_0 + \\beta_1 x_i + e_i\n\\]\nwe have to consider:\n\\[y_i = \\beta_0 + \\beta_1 x_{1i} +\\beta_2 x_{2i} + \\dots + \\beta_k x_{ki} + u_i\n\\]\nHow many can we add? and why does it help?"
  },
  {
    "objectID": "rmethods/3_MLRM.html#the-power-of-mlr-why-do-more-controls-help",
    "href": "rmethods/3_MLRM.html#the-power-of-mlr-why-do-more-controls-help",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "One more explicitly accounts for variables that before were hidden in \\(e_i\\).\nWe add \\(x_{2i},x_{3i},\\dots,x_{ki}\\) to the model model, and is no longer in \\(e_i\\)\nAllows for richer model specifications and nonlinearities:\nBefore: \\(y_i = \\beta_0 + \\beta_1 x_{1i} + e_i\\)\nNow : \\(y_i = \\beta_0 + \\beta_1 x_{1i} +\\beta_2 x^2_{1i} + \\beta_3 x^{1/2}_{1i} + \\beta_4 x^{-1}_{1i} + \\beta_5 x_{2i}+\\dots+e_i\\)\n\nThus, we can get closer to the unknown Population function, and explicitly handle some endogeneity problems (we control for it).\n\n\n\n\n\n\nWith great power…\n\n\n\nBeing able to add more controls is good, but:\n\nMay make things worse (bad controls)\nOr might not be feasible (small Sample)\nOr may be difficult to interpret (unless you know how to)"
  },
  {
    "objectID": "rmethods/3_MLRM.html#mostly-the-same",
    "href": "rmethods/3_MLRM.html#mostly-the-same",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Mostly the same",
    "text": "Mostly the same\n\nLinear in Parameters: \\(y = X\\beta + e\\) (And this is the pop function)\nRandom Sampling from the population of interest. (So errors \\(e_i\\) is independent from \\(e_j\\))\nNo Perfect Collinearity:\nThis is the alternative to \\(Var(x)&gt;0\\) (SLRM), and deserves more attention.\n\n\nWe want each variable in \\(X\\) to have some independent variation, from all other variables in the model.\n\nIn the SLRM, the independent variation idea was with respect to the constant.\n\nIf a variable was a linear combination of others, then \\(\\beta's\\) cannot be identified. You need to choose what to keep:\n\n\\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1+X_2) + e \\\\\n&=  \\beta_0 + (\\beta_1+\\beta_3) X_1 + (\\beta_2+\\beta_3) X_2 + e  \n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#section-1",
    "href": "rmethods/3_MLRM.html#section-1",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "Zero Conditional mean (Exogeneity): \\(E(e_i|X)=0\\)\nRequires that the errors and the explanatory variables are uncorrelated. This is “easier” to achieve, because we can now move variables form the error to the model.\nHowever, there could be things you can’t controls for (and remain lurking in your errors)\n\n\nI call this the most important assumption, because is the hardest to deal with\n\n\nIf A1-A4 Hold, then your estimates will be unbiased!\n\nHomoskedasticity Same as before. Errors dispersion does not change with respect to all \\(X's\\). \\[Var(e|X)=c\n\\]\n\nJust as with SLRM, this assumption will help with the estimation of Standard Errors."
  },
  {
    "objectID": "rmethods/3_MLRM.html#mlrm-estimation",
    "href": "rmethods/3_MLRM.html#mlrm-estimation",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "MLRM estimation",
    "text": "MLRM estimation\nAs before, not much has changed. We are still interested in finding \\(\\beta's\\) that Minimizes the (squared) error of the model when compared to the observed data:\n\\[\\hat \\beta = \\min_\\beta \\sum (y_i-X_i'\\beta)^2 = \\min_\\beta \\sum (y_i-\\beta_0-\\beta_1 x_{1i}-\\dots-\\beta_k x_{ki})^2\n\\]\nThe corresponding FOC generate \\(K+1\\) equations to identify \\(K+1\\) parameters:\n\\[\\begin{aligned}\n\\sum (y_i-X_i'\\beta) &= 0  \\\\\n\\sum x_{1i}(y_i-X_i'\\beta) &= 0 \\\\\n\\sum x_{2i}(y_i-X_i'\\beta) &= 0 \\\\ \\dots \\\\\\\n\\sum x_{ki}(y_i-X_i'\\beta) &= 0\n\\end{aligned} \\rightarrow X'(y-X\\beta) =0 \\rightarrow \\hat \\beta = (X'X)^{-1}X'y\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#mata-interlute-for-those-curious",
    "href": "rmethods/3_MLRM.html#mata-interlute-for-those-curious",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "mata Interlute (for those curious)",
    "text": "mata Interlute (for those curious)\n\nfrause gpa1, clear\ngen one =1 \nmata: y=st_data(.,\"colgpa\"); mata: x=st_data(.,\"hsgpa act one\")\nmata: xx=x'x ; ixx=invsym(xx) ; xy = x'y \nmata: b = ixx * xy ; b\n\n                 1\n    +---------------+\n  1 |  .4534558853  |\n  2 |  .0094260123  |\n  3 |  1.286327767  |\n    +---------------+"
  },
  {
    "objectID": "rmethods/3_MLRM.html#you-got-the-betas-how-do-you-interpret-them",
    "href": "rmethods/3_MLRM.html#you-got-the-betas-how-do-you-interpret-them",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "You got the \\(\\beta's\\), how do you interpret them?",
    "text": "You got the \\(\\beta's\\), how do you interpret them?\nInterpretation of MLRM is similar to the SLRM. For most cases, you simply look into the coefficients, and interpret effects in terms of Changes:\n\\[\\begin{aligned}\ny_i = \\hat\\beta_0 + \\hat\\beta_1 x_{1i}  + \\hat\\beta_2 x_{2i} + e_i \\\\\n\\Delta y_i =  \\hat\\beta_1 \\Delta  x_{1i}  + \\hat\\beta_2 \\Delta  x_{2i} + \\Delta e_i\n\\end{aligned}\n\\]\nUnder A1-A5 I can make use the above to make interpretations\n\n\\(\\hat \\beta_0\\) has no effect on “changes” of \\(y\\). Only its levels.\n\\(\\hat \\beta_1\\) indicates how much \\(\\Delta y_i\\) will be if \\(\\Delta x_{1i}\\) increases in 1 unit, if both \\(\\Delta x_{2i}\\) and \\(\\Delta e_i\\) remain constant (Ceteris Paribus)\n\n\\(\\Delta e_i=0\\) by assumption, and \\(\\Delta x_{2i}=0\\) because we are explicitly controlling for it (We impute this based on extrapolations)\nYou could also analyze the effect of \\(\\Delta x_{1i}\\) and \\(\\Delta x_{2i}\\) Simultaneously!"
  },
  {
    "objectID": "rmethods/3_MLRM.html#example",
    "href": "rmethods/3_MLRM.html#example",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Example",
    "text": "Example\nqui: frause wage1, clear\nqui: reg lwage educ exper tenure\nlocal b0:display %5.3f _b[_cons]\nlocal b1:display %5.3f _b[educ]\nlocal b2:display %5.3f _b[exper]\nlocal b3:display %5.3f _b[tenure]\ndisplay \"\\$log(wage) = `b0' + `b1' educ + `b2' exper + `b3' tenure$\"\n\\(log(wage) = 0.284 + 0.092 educ + 0.004 exper + 0.022 tenure\\)\n\n\\(\\beta_0\\) has no effect on changes, but level.\n\nIf someone has no education, experience or tenure, log(wages) will be 0.284. Why not wages? and Does it make sense to assume 0 education, experience and tenure?\n\n\\(\\beta_1\\): An additional year of education increases wages in 0.092log points or about 9.2%, if Experience and tenure do not change (ceteris paribus).\n\nNotes:\n\nThink of Interpretations as counterfactual: \\(y_{post} - y_{pre}\\)\nAssumption: Other factors (unobserved \\(e\\)) remain fixed (is it always credible??)\nEffects can be combined. What if a person gains 1 year of education but losses 3 of tenure?"
  },
  {
    "objectID": "rmethods/3_MLRM.html#more-on-interpretation",
    "href": "rmethods/3_MLRM.html#more-on-interpretation",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "More on Interpretation",
    "text": "More on Interpretation\nUnder A1-A5, you can still interpret results as “counterfactual” at the individual level. However, its more common to do it based on Conditional means:\n\\[\\frac {\\Delta E(y|X)}{\\Delta X_k} \\simeq E(y|X_{-k},X_k+1)-E(y|X)\n\\]\nWhich mostly changes Language.\n\nThe expected effect of an increase in \\(X\\) in one unit."
  },
  {
    "objectID": "rmethods/3_MLRM.html#alternative-interpretation-partialling-out",
    "href": "rmethods/3_MLRM.html#alternative-interpretation-partialling-out",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Alternative Interpretation: Partialling out",
    "text": "Alternative Interpretation: Partialling out\n\nAn alternative way of interpreting (and understanding) MLRM is to think about partialling out interpretation.\nThis interpretation is based on the Frisch-Waugh-Lowell Theorem, which states that the following models should give you the SAME \\(\\beta's\\):\n\n\\[\\begin{aligned}\ny &= \\color{blue}{\\beta_1 } X_1 + \\beta_2 X_2 + e \\\\\n(I-P_{X^c_2}) y &= \\color{green}{\\beta_1} (I-P_{X^c_2}) X_1 + e \\\\\nP_{X^c_2} &= X^c_2 (X'^{c}_2  X^{c}_2) X'^{c}_2 : \\text{Projection Matrix}\n\\end{aligned}\n\\]\n\n\n\n\n\n\nPartialling out\n\n\n\n\\(\\beta_1\\) can be interpreted as the effect of \\(X_1\\) on \\(y\\), after all variation related to \\(X_2\\) has been “eliminated”.\nThus \\(\\beta_1\\) is the effect uniquely driven by \\(X_1\\)."
  },
  {
    "objectID": "rmethods/3_MLRM.html#example-1",
    "href": "rmethods/3_MLRM.html#example-1",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Example",
    "text": "Example\n\nqui {\n  frause oaxaca, clear\n  drop if lnwage==.\n  reg lnwage educ exper tenure\n  est sto m1\n  reg educ        exper tenure\n  predict r_educ , res\n  reg lnwage      exper tenure\n  predict r_lnwage , res\n  reg r_lnwage r_educ\n  est sto m2\n  reg lnwage educ\n  est sto m3\n}\nesttab m1 m2 m3, se  \n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                   lnwage        r_lnwage          lnwage   \n------------------------------------------------------------\neduc               0.0870***                       0.0800***\n                (0.00516)                       (0.00539)   \n\nexper              0.0113***                                \n                (0.00154)                                   \n\ntenure            0.00837***                                \n                (0.00188)                                   \n\nr_educ                             0.0870***                \n                                (0.00516)                   \n\n_cons               2.140***     8.93e-10           2.434***\n                 (0.0650)        (0.0124)        (0.0636)   \n------------------------------------------------------------\nN                    1434            1434            1434   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/3_MLRM.html#estimator-properties-unbiased",
    "href": "rmethods/3_MLRM.html#estimator-properties-unbiased",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Estimator Properties: Unbiased",
    "text": "Estimator Properties: Unbiased\nRecall, the estimator of \\(\\beta's\\) when you have multiple dependent variables:\n\\[\\begin{aligned}\n0  &: \\hat \\beta = (X'X)^{-1} X'y \\\\\nA1 \\text{ & }  A2 &: \\hat \\beta = (X'X)^{-1} X'(X\\beta + e) \\\\\n1  &: \\hat \\beta = (X'X)^{-1} X'X\\beta + (X'X)^{-1} X'e \\\\\nA3 &: det(X'X)\\neq 0 \\rightarrow (X'X)^{-1} \\text{ exists} \\\\\n2  &: \\hat \\beta = \\beta + (X'X)^{-1} X'e \\\\\nA4 &: E(e|X)=0 \\rightarrow E[(X'X)^{-1} X'e]=0 \\\\\n3  &: E(\\hat\\beta)= \\beta \\text{ unbiased}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#estimator-properties-variance-under-homoskedasticity",
    "href": "rmethods/3_MLRM.html#estimator-properties-variance-under-homoskedasticity",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Estimator Properties: Variance under Homoskedasticity",
    "text": "Estimator Properties: Variance under Homoskedasticity\nLets start with (2). \\(\\beta's\\) are random functions of the errors. Thus its variance will depend on \\(e\\).\n\\[\\begin{aligned}\n1 &: \\hat \\beta = \\beta + (X'X)^{-1} X'e \\\\\n2 &:\\hat \\beta - \\beta = (X'X)^{-1} X'e \\\\\n3 &: Var(\\hat \\beta - \\beta) = Var((X'X)^{-1} X'e) \\\\\n4 &: Var(\\hat \\beta - \\beta) = (X'X)^{-1} X' Var(e) X (X'X)^{-1}  \\\\\n\\end{aligned}\n\\]\n\\(Var(e)\\) considers variance and covariance of each \\(e_i\\) and its combinations."
  },
  {
    "objectID": "rmethods/3_MLRM.html#section-2",
    "href": "rmethods/3_MLRM.html#section-2",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "By assumption A2, \\(cov(e_i,e_j)=0\\). And by assumption A5 \\(Var(e_i)=Var(e_j)\\).\n\\[\\begin{aligned}\nVar(\\hat \\beta - \\beta) &= (X'X)^{-1} X' \\sigma_e^2 I X (X'X)^{-1} \\\\\nVar(\\hat \\beta - \\beta) &= \\sigma_e^2 (X'X)^{-1} \\\\\nVar(\\hat \\beta_j - \\beta_j) &= \\frac{\\sigma_e^2}{SST_j (1-R^2_j)}\n\\end{aligned}\n\\]\nBut we do not know \\(\\sigma^2_e\\). Thus, we also “estimate it”\n\\[\\hat \\sigma^2_e = \\frac{\\sum \\hat e^2}{N-K-1}\n\\]\nWhich is unbiased estimator for \\(\\sigma^2_e\\) if A1-A5 hold."
  },
  {
    "objectID": "rmethods/3_MLRM.html#section-3",
    "href": "rmethods/3_MLRM.html#section-3",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "\\[\\begin{aligned}\nVar(\\hat \\beta - \\beta) &= \\sigma_e^2 (X'X)^{-1} \\\\\nVar(\\hat \\beta_j - \\beta_j) &= \\frac{\\sigma_e^2}{SST_j (1-R^2_j)}  \\\\\\\n& = \\frac{\\sigma_e^2}{(N-1)Var(X_j) (1-R^2_j)} = \\frac{\\sigma_e^2}{(N-1)Var(X_j)}VIF_j\n\\end{aligned}\n\\]\nTo consider:\n\n\\(Var(\\beta)\\) increases with \\(\\sigma_e^2\\). More variation in the error, more variation of the coefficients.\n\\(Var(\\beta)\\) decreases with Sample size \\(N\\)\n\\(Var(\\beta)\\) also decreases with Variation in \\(X\\)\nHowever, it increases if there is less unique variation (Multicolinearity problem and VIF)"
  },
  {
    "objectID": "rmethods/3_MLRM.html#quick-note",
    "href": "rmethods/3_MLRM.html#quick-note",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Quick Note",
    "text": "Quick Note\n\n\\(R^2\\) are the same as SLRM: How much of variation is explained by the model.\n\nAlso \\(R^2 = corr(y,\\hat y)^2\\)\n\nThe fitted line goes over the “mean” of all variables\nMLRM Fits hyper-planes to the data\nRegression through the origin still a bad idea\nAlso, under A1-A5 OLS is the Best Linear Unbiased Estimator (BLUE)"
  },
  {
    "objectID": "rmethods/3_MLRM.html#ignoring-variables",
    "href": "rmethods/3_MLRM.html#ignoring-variables",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Ignoring Variables",
    "text": "Ignoring Variables\nIn the MLRM framework, its easier to see what happens when important variables are ignored.\n\\[\\text{True: } y = b_0 + b_1 x_1 + b_2 x_2 + e\n\\]\nBut instead you estimate the following :\n\\[\\text{Estimated: }y = g_0 + g_1 x_1 + v\n\\]\nUnless stronger assumptions are imposed, \\(g_1\\) will be a biased estimate of \\(b_1\\).\n\\[\\begin{aligned}\n\\hat g_1 &= \\frac{\\sum \\tilde x_1 \\tilde y}{\\sum \\tilde x_1^2}\n         = \\frac{\\sum \\tilde x_1 (b_1 \\tilde x_1 +\\tilde b_2 \\tilde x_2 + e) }{\\sum \\tilde x_1^2} \\\\\n         &= \\frac{b_1 \\sum \\tilde x_1^2}{\\sum \\tilde x_1^2}\n          + b_2 \\frac{\\sum \\tilde x_1\\tilde x_2}{\\sum \\tilde x_1^2}\n          +\\frac{\\sum \\tilde x_1 e}{\\sum \\tilde x_1^2} \\\\\n         &= b_1+b_2 \\delta_1 +\\frac{\\sum \\tilde x_1 e}{\\sum \\tilde x_1^2} \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#section-6",
    "href": "rmethods/3_MLRM.html#section-6",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "",
    "text": "This implies that \\(g_1\\) is biased:\n\\[E(\\hat g_1) = b_1+b_2 \\delta_1\n\\]\nWhere \\(\\delta_1\\) is the coefficient in \\(x_2=\\delta_0+\\delta_1 x_1 + v\\).\nImplications:\n\nUnless\n\n\\(\\delta_1\\) is zero (\\(x_1\\) and \\(x_2\\) are linearly independent) or,\n\\(b_2\\) is zero (\\(x_2\\) was irrelevant)\n\nignoring \\(x_2\\) will generate biased (and inconsistent) estimates for \\(b_1\\).\n\nIn models with more controls, the direction of the biases will be harder to define, but similar rule’s of thumb can be used."
  },
  {
    "objectID": "rmethods/3_MLRM.html#adding-irrelevant-controls",
    "href": "rmethods/3_MLRM.html#adding-irrelevant-controls",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Adding irrelevant controls",
    "text": "Adding irrelevant controls\nAdding irrelevant controls will have no effect on bias and consistency.\nif your model is:\n\\[y=b_0+b_1 x_1 +e\n\\]\nbut you estimate:\n\\[y=g_0+g_1 x_1+g_2 x_2 +v\n\\]\nyour model is still unbiased:\n\\[\\begin{aligned}\ng &= (X'X)^{-1}X'(X \\beta^+ + e) \\\\\n    \\beta^+ &= [\\beta \\ ; 0] \\\\\ng &=  \\beta^+ + (X'X)^{-1}X'e \\rightarrow E(g) = \\beta^+\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#adding-bad-controls",
    "href": "rmethods/3_MLRM.html#adding-bad-controls",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Adding “bad” Controls",
    "text": "Adding “bad” Controls\nThe worst case, yet hard to see, is when you add “bad” Controls, also known as Colliers.\nFor example:\n\nSay you want to analyze the effect of education on wages, and you control for occupation. Will it create an unbiased estimate for education?\n\nNo. Your education affects your occupation choice. So some of the effect of education will be “absorbed” by occupation.\n\nSay you want to see the impact of health expenditure on health, and you control for “#visits to the doctor”\n\nThis may also affect your estimates, as expenditure may change how many times you Visits are highly related.\n\n\nIn general, you want to avoid using “channels” as Controls."
  },
  {
    "objectID": "rmethods/3_MLRM.html#what-about-standard-errors",
    "href": "rmethods/3_MLRM.html#what-about-standard-errors",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "What about Standard Errors",
    "text": "What about Standard Errors\n\nCase 1Case 2Case 3\n\n\nOmitting relevant variables that are correlated to \\(X's\\)\nWe wont talk about this. It violates A4, and creates endogeneity\n\n\nOmitting relevant variables that are uncorrelated to \\(X's\\)\n\nOmitted variables will be in the error \\(e\\). Thus variance of coefficients will be larger\n\n\\[\\begin{aligned}\nTrue: & y = b_0 + b_1 x_1 + b_2 x_2 + e  \\\\\nEstimated: & y = g_0 + g_1 x_1 + v   \\\\\n& Var(e)&lt;Var(v) \\rightarrow Var(b_1)&lt;Var(g_1)\n\\end{aligned}\n\\]\nThus Adding controls in Randomized experiements is still a good idea!\n\n\nAdding Irrelevant controls (related to X’s)\nCoefficients are unbiased, and \\(\\sigma^2_e\\) will also be unbiased.\nHowever, you may increase Multicolinearity in the model increasing \\(R_j^2\\) and \\(VIF_j\\).\nVariance of relevant coefficients will be larger.\n\\[\\begin{aligned}\nTrue: & y = b_0 + b_1 x_1  + e  \\\\\nEstimated: & y = g_0 + g_1 x_1 + g_2 x_2 + v   \\\\\n& Var(b_1)&lt;Var(g_1)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/3_MLRM.html#prediction",
    "href": "rmethods/3_MLRM.html#prediction",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Prediction",
    "text": "Prediction\n\nYou can use MLRM to obtain predictions of outcomes.\nThey will be subject to the model specification.\nFor prediction you do not need to worry about “endogeneity” as much. Just on Predictive power (how ??)\n\n\nqui:frause oaxaca, clear\ngen wage = exp(lnwage)\nqui:reg wage educ female age agesq single married\npredict wage_hat\nlist wage wage_hat educ female age agesq single married in 1/5\n\n(213 missing values generated)\n(option xb assumed; fitted values)\n\n     +----------------------------------------------------------------------+\n     |     wage   wage_hat   educ   female   age   agesq   single   married |\n     |----------------------------------------------------------------------|\n  1. | 41.80602   25.61872      9        1    37    1369        1         0 |\n  2. | 36.63003   31.70813      9        0    62    3844        0         1 |\n  3. | 23.54788   30.71257   10.5        1    40    1600        0         1 |\n  4. | 29.76191    42.7976     12        0    55    3025        0         0 |\n  5. | 44.95504   35.76914     12        0    36    1296        0         1 |\n     +----------------------------------------------------------------------+"
  },
  {
    "objectID": "rmethods/3_MLRM.html#efficient-market",
    "href": "rmethods/3_MLRM.html#efficient-market",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Efficient Market",
    "text": "Efficient Market\n\nWe could use MLRM to test theories, like the Efficient Market Theory.\nFor housing, the Assessed price of a house should be all information needed to assess the price of the house. (other ammenities should not matter)\n\nfrause hprice1, clear\nqui:reg price assess bdrms llotsize lsqrft colonial\nmodel_display\nprice_hat = 206.645 + 1.007 assess + 11.404 bdrms + 1.363 llotsize - 38.335 lsqrft + 9.297 colonial\nN= 88 R2=0.831\nqui:reg lprice lassess bdrms llotsize lsqrft colonial\nmodel_display\nlprice_hat = 0.210 + 1.036 lassess + 0.025 bdrms + 0.008 llotsize - 0.092 lsqrft + 0.045 colonial\nN= 88 R2=0.777"
  },
  {
    "objectID": "rmethods/3_MLRM.html#testing-for-discrimination-cp",
    "href": "rmethods/3_MLRM.html#testing-for-discrimination-cp",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Testing for Discrimination (CP)",
    "text": "Testing for Discrimination (CP)\n\nWe could test for discrimination: Unexplained differences in outcomes once other factors are kept fixed.\nIt does require that groups are similar in terms of unobservables.\n\nqui: frause oaxaca, clear\nqui:reg lnwage female \nmodel_display\nlnwage_hat = 3.440 - 0.173 female\nN= 1434 R2=0.027\nqui:reg lnwage female educ age agesq single married exper tenure\nmodel_display\nlnwage_hat = 0.383 - 0.160 female + 0.064 educ + 0.113 age - 0.001 agesq - 0.072 single - 0.094 married - 0.000 exper + 0.007 tenure\nN= 1434 R2=0.345"
  },
  {
    "objectID": "rmethods/3_MLRM.html#treatment-evaluation",
    "href": "rmethods/3_MLRM.html#treatment-evaluation",
    "title": "Multiple Regression Analysis: Estimation",
    "section": "Treatment Evaluation",
    "text": "Treatment Evaluation\n\nUnder Random Assingment SRM was enough to estimate ATTs.\nBut if assigment was conditionally random, a better approach would be using MLRM\n\nfrause jtrain98, clear\nqui:reg earn98 train \nmodel_display\nearn98_hat = 10.610 - 2.050 train\nN= 1130 R2=0.016\nqui:reg earn98 train earn96 educ age married\nmodel_display\nearn98_hat = 4.667 + 2.411 train + 0.373 earn96 + 0.363 educ - 0.181 age + 2.482 married\nN= 1130 R2=0.405"
  },
  {
    "objectID": "rmethods/5_FXMRA.html",
    "href": "rmethods/5_FXMRA.html",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Multiple Linear Regression models (MLRM), estimated via OLS, have very good properties, if all Assumptions (A1-A5,A6’) Hold.\nUp until now, we have discussed how to estimate them, and analyze them under “optimal” assumptions, in simplified cases.\nToday we will be adding other “minor” Features to MLR, and aim to better understand its features"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#introduction",
    "href": "rmethods/5_FXMRA.html#introduction",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Multiple Linear Regression models (MLRM), estimated via OLS, have very good properties, if all Assumptions (A1-A5,A6’) Hold.\nUp until now, we have discussed how to estimate them, and analyze them under “optimal” assumptions, in simplified cases.\nToday we will be adding other “minor” Features to MLR, and aim to better understand its features"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#scaling-and-shifting",
    "href": "rmethods/5_FXMRA.html#scaling-and-shifting",
    "title": "Multiple Regression Analysis",
    "section": "Scaling and shifting",
    "text": "Scaling and shifting\n\nSomething that we do not emphasize enough. Before analyzing your data, its important to analyze the nature of the data (summary stats, ranges, scales)\nWhen I talk about Scaling and shifting, I refer exclusibly to affine transormations of the following type:\n\n\\[x^* = a*x+c \\text{ or } x^* = a*(x+c1)+c2\n\\]\nThey either Shift, or change the scale of the data. Not the shape! (logs change shape)\n\nIf one applies affine transformations to the data, it will have NO effect on your model what-so-ever. (Same t’s same F’s, same \\(R^2\\))\nBut, your \\(\\beta's\\) will change. This could help understading and explaining the results."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#example",
    "href": "rmethods/5_FXMRA.html#example",
    "title": "Multiple Regression Analysis",
    "section": "Example:",
    "text": "Example:\n\nset linesize 255\nfrause bwght, clear\ngen bwkg = bwghtlbs*0.454\ngen bwgr = bwkg*1000\nregress bwght male white cigs lfaminc \nest sto m1\nregress bwghtlbs male white cigs lfaminc\nest sto m2\nregress bwkg male white cigs lfaminc\nest sto m3\nregress bwgr male white cigs lfaminc\nest sto m4\n\n\nBirthweight and Cig\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOz\n\nlbs\n\nKgs\n\nGr\n\n\n\n\n\nmale\n3.123***\n\n0.195***\n\n0.089***\n\n88.605***\n\n\n\n\n(1.071)\n[2.916]\n(0.067)\n[2.916]\n(0.030)\n[2.916]\n(30.389)\n[2.916]\n\n\nwhite\n5.404***\n\n0.338***\n\n0.153***\n\n153.346***\n\n\n\n\n(1.392)\n[3.882]\n(0.087)\n[3.882]\n(0.039)\n[3.882]\n(39.497)\n[3.882]\n\n\ncigs\n-0.480***\n\n-0.030***\n\n-0.014***\n\n-13.628***\n\n\n\n\n(0.091)\n[-5.288]\n(0.006)\n[-5.288]\n(0.003)\n[-5.288]\n(2.577)\n[-5.288]\n\n\nlfaminc\n1.053*\n\n0.066*\n\n0.030*\n\n29.867*\n\n\n\n\n(0.632)\n[1.664]\n(0.040)\n[1.664]\n(0.018)\n[1.664]\n(17.946)\n[1.664]\n\n\n_cons\n110.603***\n\n6.913***\n\n3.138***\n\n3138.351***\n\n\n\n\n(2.071)\n[53.410]\n(0.129)\n[53.410]\n(0.059)\n[53.410]\n(58.760)\n[53.410]\n\n\nN\n1388\n\n1388\n\n1388\n\n1388\n\n\n\nR2\n0.046\n\n0.046\n\n0.046\n\n0.046"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#scaling-xs-and-ys",
    "href": "rmethods/5_FXMRA.html#scaling-xs-and-ys",
    "title": "Multiple Regression Analysis",
    "section": "Scaling X’s and Y’s",
    "text": "Scaling X’s and Y’s\n\nRe-scaling \\(y\\) will affect the all coefficients.\n\nReducing Scale, reduces scale of coefficients\n\nRe-scaling \\(x's\\) will only affect its coefficient and possible the constant.\n\nReducing (increasing) Scale will increase (reduce) Scale of coefficient\n\nIn both cases, Shifting the variable only affects the constant.\n\n\n\n\n\n\n\nImportant\n\n\n\nRe-Scaling is an important tool/trick that can be used for interpreting more complex models."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#beta-or-standardized-coefficients",
    "href": "rmethods/5_FXMRA.html#beta-or-standardized-coefficients",
    "title": "Multiple Regression Analysis",
    "section": "Beta or Standardized Coefficients",
    "text": "Beta or Standardized Coefficients\n\nIn some fields (health), making inferences based on default scales can be difficult (the impact of 1microgram ?).\nTo avoid this type of problem researchers may opt to use Standardized or Beta coefficients.\n\nHow a \\(sd\\) change in \\(X's\\) affect the outcome (in \\(sd\\))\n\nGetting these coefficient is similar to applying the following transformation to all variables:\n\n\\[\\tilde w = \\frac{w-\\bar w}{\\sigma_w} \\rightarrow E(\\tilde w)=0 \\text{ and } Var(\\tilde w) = 1\n\\]\nreg y x1 x2 x3, beta\nest sto m1\nesttab m1, beta \n\nIt also helps you make comparison of the relative importance of each covariate explanatory power."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-single-dummies",
    "href": "rmethods/5_FXMRA.html#functional-forms-single-dummies",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Single Dummies",
    "text": "Functional Forms: Single Dummies\n\nDummies are variables that take only two values (preferably 0 and 1).\nThey are used to capture qualitative (binary) characteristics (ie Democrat, Union worker, etc)\nWhen used in regression analysis, they represent “shifts” in the Intercept: \\[y = b_0 + b_1 male + b_2 x_1 + b_3 x_2 + e\n\\]\n\nHere, \\(b_0\\) would be the “intercept” for “women” (base) while \\(b_0+b_1\\) would be the intercept for men.\n\nUnder A4, \\(b_1\\) is the expected outcome difference men have over women, everything else constant.\n\n\nUnless further restrictions are used, you can’t add Dummies for both categories in the model.\n\n* Stata Code\nreg y x1 x2 d    &lt;-- Possible if d = 0 or 1\nreg y x1 x2 i.d  &lt;-- Better"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-multiple-dummies",
    "href": "rmethods/5_FXMRA.html#functional-forms-multiple-dummies",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Multiple Dummies",
    "text": "Functional Forms: Multiple Dummies\n\nWe can use dummies to represent multiple (nonoverlapping) characteristics like Race, ranking or age group).\nOne needs a “base” or comparison group to analyze coefficients (or more).\nOrdered variables can be used as continuous, but using them as dummies requires creating dummies for each category.\n\n\\[\\begin{aligned}\ny &= b_0 + b_1 black + b_2 hispanic + b_3 other + b_4 x + e & || Base = White \\\\\ny &= b_0 + b_1 young + b_2 old + b_3 x + e & || Base = Adult\n\\end{aligned}\n\\]\n\nWhen using with ordered data, multiple dummies may create somewhat counterintuitive results\n\ntab race, gen(race_)  &lt;- creates dummies\nreg y i.race x1 x2 x3 &lt;- generally uses first group as base\nreg y ib2.race x1 x2 x3 &lt;- indicates a particular \"base\""
  },
  {
    "objectID": "rmethods/5_FXMRA.html#example-1",
    "href": "rmethods/5_FXMRA.html#example-1",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\n\nfrause beauty, clear\n** Union also a dummy. \n** looks as Continous\nqui:reg lwage exper union educ female looks\nest sto m1\ngen looks_good = looks&gt;=4 if !missing(looks)\nqui:reg lwage exper union educ female looks_good\nest sto m2\nqui:reg lwage exper union educ female i.looks\nest sto m3\nqui:reg lwage exper union educ female ib3.looks\nest sto m4\nesttab m1 m2 m3 m4, se star( * 0.1 ** 0.05 *** 0.01  ) nogaps nomtitle\ndisplay _n \"Exact Change Union : \" %5.3f (exp(_b[union])-1)*100 \"%\"\n\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n----------------------------------------------------------------------------\nexper              0.0137***       0.0134***       0.0135***       0.0135***\n                (0.00119)       (0.00120)       (0.00120)       (0.00120)   \nunion               0.201***        0.201***        0.196***        0.196***\n                 (0.0305)        (0.0307)        (0.0306)        (0.0306)   \neduc               0.0737***       0.0750***       0.0735***       0.0735***\n                (0.00528)       (0.00528)       (0.00528)       (0.00528)   \nfemale             -0.448***       -0.450***       -0.446***       -0.446***\n                 (0.0293)        (0.0294)        (0.0293)        (0.0293)   \nlooks              0.0555***                                                \n                 (0.0201)                                                   \nlooks_good                         0.0276                                   \n                                 (0.0299)                                   \n1.looks                                                 0          -0.266** \n                                                      (.)         (0.134)   \n2.looks                                             0.146          -0.121***\n                                                  (0.139)        (0.0439)   \n3.looks                                             0.266**             0   \n                                                  (0.134)             (.)   \n4.looks                                             0.264*       -0.00255   \n                                                  (0.136)        (0.0312)   \n5.looks                                             0.422**         0.156   \n                                                  (0.173)         (0.111)   \n_cons               0.408***        0.565***        0.338**         0.604***\n                 (0.0968)        (0.0774)         (0.149)        (0.0781)   \n----------------------------------------------------------------------------\nN                    1260            1260            1260            1260   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01\n\nExact Change Union : 21.598%"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-logarithms",
    "href": "rmethods/5_FXMRA.html#functional-forms-logarithms",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Logarithms",
    "text": "Functional Forms: Logarithms\n\nUsing Logarithms can help modeling some nonlinearities in the data.\nBecause it changes the “shape” of variables, it also changes the interpretation (Changes vs %Changes)\nBy reducing dispersion of dep. variable, CLM assumptions may hold.\n\nBut:\n\nCannot or should not be applied to all data types (ie Dummies, negatives, shares)\n(log-lin model): It is often better to use the exact percentage change rather than approximation: \\[\\begin{aligned}\nlog(y) &= b_0 + b_1 x_1 + b_2 x_2 + b_3 D + e \\\\\n\\frac{\\% \\Delta y}{\\Delta D} &= 100 (exp(b_3)-1)\\%\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-polynomials-x2-x3-etc",
    "href": "rmethods/5_FXMRA.html#functional-forms-polynomials-x2-x3-etc",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Polynomials (\\(x^2, x^3, etc\\))",
    "text": "Functional Forms: Polynomials (\\(x^2, x^3, etc\\))\n\nUp to this point, we have only considered linear models (\\(X's\\) enter asis or in logs). This almost always works! (Taylor expansion justification)\n\nSpecially if interested in Average Effects\n\nSome times, you may be interest in capturing some heterogeneity for \\(dy/dx\\). That can be done just adding “ANY” transformation of \\(X\\) in the model (\\(sin(x), 1/x, \\sqrt x\\), etc)\nFor practical, and theoretical purposes, however, we usually concentrate on quadratic terms.\n\nFor example: Increasing returns with decreasing marginal returns\nWe may be interested in “turning” points\n\nHowever, we now need to be careful about marginal effects!"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section",
    "href": "rmethods/5_FXMRA.html#section",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "\\[\\begin{aligned}\ny &=b_0+b_1 x_1 + b_2 x_1^2 + b_3 x_2 + e \\\\\n\\frac{dy}{dx_1} &= b_1+2b_2 x_1 =0 \\\\\nx_1^* &= - \\frac{b_1}{2b_2} x_1\n\\end{aligned}\n\\]\n\n\n\n\n\n\nTo consider\n\n\n\n\nMarginal effects are no longer constant. You need an \\(x_1\\) value to obtain them (mean? average?)\nWith Quadratic models, there is ALWAYS a turning point (but may not be relevant)\nMFX can be positive or negative for some value of \\(x_1\\) (but may not be relevant)\nUnless something else is done, coefficients may not make sense on their own.\n\n\n\n\nWhy not add further polynomials?\n\nEstimating them is easy (except for numerical precision), but adds complexity for interpretation. Nothing else."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#example-2",
    "href": "rmethods/5_FXMRA.html#example-2",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\n\nfrause hprice2, clear\ngen rooms2=rooms*rooms\nqui:reg lprice lnox dist rooms \nest sto m0\nqui:reg lprice lnox dist rooms rooms2\nest sto m1\nqui:reg lprice lnox dist c.rooms c.rooms#c.rooms\nest sto m2\nesttab m0 m1 m2, se varwidth(20) star(* 0.1 ** 0.05 *** 0.01) nogaps\n\n\n--------------------------------------------------------------------\n                              (1)             (2)             (3)   \n                           lprice          lprice          lprice   \n--------------------------------------------------------------------\nlnox                       -0.968***       -0.975***       -0.975***\n                          (0.110)         (0.106)         (0.106)   \ndist                      -0.0291***      -0.0223**       -0.0223** \n                         (0.0102)       (0.00995)       (0.00995)   \nrooms                       0.302***       -0.724***       -0.724***\n                         (0.0189)         (0.171)         (0.171)   \nrooms2                                     0.0794***                \n                                         (0.0131)                   \nc.rooms#c.rooms                                            0.0794***\n                                                         (0.0131)   \n_cons                       9.793***        13.05***        13.05***\n                          (0.271)         (0.599)         (0.599)   \n--------------------------------------------------------------------\nN                             506             506             506   \n--------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.1, ** p&lt;0.05, *** p&lt;0.01\n\n\n\nNegative coefficient for \\(rooms\\), so is there a problem?\n\nFind “turnpoint” and summary Stats\n\n\nTurn point: 4.55\n\n\n\n    Variable |       Min        p1        p5       p10       p25       p50       p75       p90       p99       Max\n-------------+----------------------------------------------------------------------------------------------------\n       rooms |      3.56      4.52       5.3      5.59      5.88      6.21      6.62      7.15      8.34      8.78\n------------------------------------------------------------------------------------------------------------------\n\n\n\nDoes it make a difference how we estimate the model?\n\n\nqui:reg lprice lnox dist rooms rooms2\nmargins, dydx(rooms)\nqui:reg lprice lnox dist c.rooms c.rooms#c.rooms\nmargins, dydx(rooms) \n\n\nAverage marginal effects                                   Number of obs = 506\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  rooms\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       rooms |  -.7236433   .1706763    -4.24   0.000    -1.058973   -.3883139\n------------------------------------------------------------------------------\n\nAverage marginal effects                                   Number of obs = 506\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  rooms\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       rooms |   .2747106   .0188463    14.58   0.000     .2376831    .3117382\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-interactions-i-d1d2",
    "href": "rmethods/5_FXMRA.html#functional-forms-interactions-i-d1d2",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Interactions I (\\(d1*d2\\))",
    "text": "Functional Forms: Interactions I (\\(d1*d2\\))\n\nIt is possible to use multiple (unrelated) dummy variables.\nDummy interactions are feasible to allow for differential means across groups combined groups.\nYou still need a reference group that should be identified: \\[\\begin{aligned}\ny &= a_0 + a_1 female + a_2 union + a_3 female \\times union + e \\\\\ny &= b_0 + b_1 female \\times nonunion + b_2 male \\times union +b_3 female \\times union + e\n\\end{aligned}\n\\] Both models are equivalent. Also \\[\\begin{aligned}\n& E(y|male,nonunion)    && =a_0  &&= b_0   \\\\\n& E(y|female,nonunion) && = a_0 + a_1 && = b_0 + b_1 \\\\\n& E(y|male,union) && =a_0+a_2  &&= b_0+b_2 \\\\\n& E(y|female,union)  && = a_0 + a_1 + a_2 + a_3  &&= b_0 + b_3\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-1",
    "href": "rmethods/5_FXMRA.html#section-1",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "For this, you may need to use manual dummy creation, or use explicit interactions:\nreg y i.d1 i.d2 i.d1#i.d2\nreg y i.d1##i.d2\nreg y i.d1#i.d2\n\nYou set the interactions\nSimilar to one, but Stata does it for you\nCreates full set of interactions, as in 2nd model before\n\nOptions 1 and 3 will allow you using margins. For overall groups (all women, all unions) you need to decide how to get representative samples."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-interactions-ii-x1x2",
    "href": "rmethods/5_FXMRA.html#functional-forms-interactions-ii-x1x2",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Interactions II (\\(x1*x2\\))",
    "text": "Functional Forms: Interactions II (\\(x1*x2\\))\n\nYou may be interested in allowing for some interaction across continuous variables.\n\nie Interacted effect of household size and number of bedrooms\n\nAs with Polynomials, this allows for heterogeneity, thus effects are not constant.\n\n\\[\\begin{aligned}\ny &= a_0 + a_1 x_1 + a_2 x_2 + a_3 x_1 x_2 + e \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_1} &= a_1  + a_3 x_2 \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_2} &= a_2  + a_3 x_1\n\\end{aligned}\n\\]\n\nThus, coefficients, on their own, are difficult to interpret, unless \\(x_1\\) or \\(x_2\\) are zero"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-2",
    "href": "rmethods/5_FXMRA.html#section-2",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Shortcut: Affine transformation\n\nThere is a trick that could help easy and direct interpretation. re-scaling variables:\n\n\\[\\begin{aligned}\ny &= b_0 + b_1 x_1 + b_2 x_2 + b_3 (x_1-\\bar x_1)(x_2-\\bar x_2) + e \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_1} &= b_1  + b_3 (x_2-\\bar x_2) \\simeq b_1 \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_2} &= b_2  + b_3 (x_1-\\bar x_1) \\simeq b_2\n\\end{aligned}\n\\]\n\nAlso works with quadratic terms!\n\n\\[\\begin{aligned}\ny &= b_0 + b_1 x_1 + b_2 (x_1-\\bar x_1)^2 + b_3 x_2 + e \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_1} &= b_1  + 2 b_2 (x_1-\\bar x_1) \\simeq b_1 \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-interactions-iii-d1x1",
    "href": "rmethods/5_FXMRA.html#functional-forms-interactions-iii-d1x1",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Interactions III (\\(d1*x1\\))",
    "text": "Functional Forms: Interactions III (\\(d1*x1\\))\n\nDummy variables allows for shifts to the constant (intercept).\nInteracting with continuous variables allows for shifts in slopes!.\n\nThis can be useful to testing hypothesis: differences in returns to education by gender.\n\n\n\\[wage=b_0 + b_1 female + b_2 educ + b_3 educ \\times female + e\n\\]\n\n\\(b_1\\): Baseline wage differential between men and women.\n\\(b_2+b_3\\): Returns to education for women.\n\\(b_1 + b_3 \\overline{educ}\\): Average wage difference between men and women.\n\nStata:\nreg y x1 i.d c.x1#i.d"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#functional-forms-full-interactions-with-dummies",
    "href": "rmethods/5_FXMRA.html#functional-forms-full-interactions-with-dummies",
    "title": "Multiple Regression Analysis",
    "section": "Functional Forms: Full Interactions (with dummies)",
    "text": "Functional Forms: Full Interactions (with dummies)\n\nIt is possible to estimate models where all variables are interacted with a single dummy. This allows you to test the hypothesis if two groups have the same underlying parameters.\n\nDo men and women have the same wage structure?\n\nFull interactions is equivalent to estimating separate models:\n\n\\[\\begin{aligned}\nFT: & y = b_0 + b_1 x_1 + b_2 x_2 + g_0 d +g_1 x_1 d +g_2 x_2 d +e \\\\\nD0: & y = b_0 + b_1 x_1 + b_2 x_2  +e  && \\text{ if d=0 } \\\\\nD1: & y = (b_0+g_0) + (b_1+g_1) x_1 + (b_2+g_2) x_2 +e && \\text{ if d=1 } \\\\\n    & y = a_0 + a_1 x_1 + a_1 x_2 +e && \\text{ if d=1 } \\\\\nCS1: & H_0: g_0=g_1=g_2=0\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-3",
    "href": "rmethods/5_FXMRA.html#section-3",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Chow test\n\n\\(CS1\\) can be tested using F-stat for multiple hypothesis.\nBut, under homoskedasticty, one could also use what is known as the Chow test\n\n\\[\\begin{aligned}\nM1 &: y = b_0 + b_1 x_1 + b_2 x_2 + e \\\\\nM2 &: y = b_0 + b_1 x_1 + b_2 x_2 + b_3 d + e \\\\\nif \\ D=0 &: y = b_{00} + b_{01} x_1 + b_{02} x2 + e_0 \\\\\nif \\ D=1 &: y = b_{10} + b_{11} x_1 + b_{12} x2 + e_1\n\\end{aligned}\n\\]\nF-Stat (similar to before):\n\\[\\begin{aligned}\nF_{M1} = \\frac{(SSR_{M1}-SSR_0-SSR_1)/(k+1)}{(SSR_0+SSR_1)/(n - 2(k+1))} \\\\\nF_{M2} = \\frac{(SSR_{M2}-SSR_0-SSR_1)/k}{(SSR_0+SSR_1)/(n - 2(k+1))}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#example-3",
    "href": "rmethods/5_FXMRA.html#example-3",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\n\n frause gpa3, clear\ndrop if cumgpa==0\nreplace sat = sat /100\nqui:reg cumgpa sat hsperc tothrs\nest sto m1\nqui:reg cumgpa sat hsperc tothrs female\nest sto m2\nqui:reg cumgpa sat hsperc tothrs if female==0\nest sto m3\nqui:reg cumgpa sat hsperc tothrs if female==1\nest sto m4\nqui:reg cumgpa i.female##c.(sat hsperc tothrs)\nest sto m5\nesttab m1 m2 m3 m4 m5, mtitle( Simple With_fem Men Women Full_int) ///\nse star(* .1 ** 0.05 *** 0.01) nogaps noomitted \n\n(98 observations deleted)\nvariable sat was int now float\n(634 real changes made)\n\n--------------------------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)             (5)   \n                   Simple        With_fem             Men           Women        Full_int   \n--------------------------------------------------------------------------------------------\nsat                0.0933***       0.0938***       0.0679***        0.177***       0.0679***\n                 (0.0133)        (0.0130)        (0.0151)        (0.0244)        (0.0146)   \nhsperc           -0.00865***     -0.00730***     -0.00748***     -0.00869***     -0.00748***\n                (0.00105)       (0.00106)       (0.00119)       (0.00219)       (0.00116)   \ntothrs          -0.000599       -0.000586        -0.00155**       0.00141        -0.00155** \n               (0.000662)      (0.000647)      (0.000771)       (0.00111)      (0.000748)   \nfemale                              0.277***                                                \n                                 (0.0493)                                                   \n0.female                                                                                0   \n                                                                                      (.)   \n1.female                                                                           -0.855** \n                                                                                  (0.333)   \n1.female#c~t                                                                        0.109***\n                                                                                 (0.0310)   \n1.female#c~c                                                                     -0.00121   \n                                                                                (0.00271)   \n1.female#c~s                                                                      0.00296** \n                                                                                (0.00145)   \n_cons               1.900***        1.782***        2.070***        1.215***        2.070***\n                  (0.149)         (0.147)         (0.173)         (0.257)         (0.168)   \n--------------------------------------------------------------------------------------------\nN                     634             634             483             151             634   \n--------------------------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;.1, ** p&lt;0.05, *** p&lt;0.01\n\n\n\ntest 1.female#c.sat 1.female#c.hsperc 1.female#c.tothrs\ntest 1.female 1.female#c.sat 1.female#c.hsperc 1.female#c.tothrs\nmargins female, dydx(sat hsperc tothrs)\n\n\n ( 1)  1.female#c.sat = 0\n ( 2)  1.female#c.hsperc = 0\n ( 3)  1.female#c.tothrs = 0\n\n       F(  3,   626) =    6.26\n            Prob &gt; F =    0.0003\n\n ( 1)  1.female = 0\n ( 2)  1.female#c.sat = 0\n ( 3)  1.female#c.hsperc = 0\n ( 4)  1.female#c.tothrs = 0\n\n       F(  4,   626) =   12.75\n            Prob &gt; F =    0.0000\n\nAverage marginal effects                                   Number of obs = 634\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  sat hsperc tothrs\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nsat          |\n      female |\n          0  |   .0679302    .014607     4.65   0.000     .0392454    .0966149\n          1  |   .1772582   .0273126     6.49   0.000     .1236229    .2308936\n-------------+----------------------------------------------------------------\nhsperc       |\n      female |\n          0  |    -.00748   .0011573    -6.46   0.000    -.0097526   -.0052073\n          1  |  -.0086922   .0024524    -3.54   0.000    -.0135081   -.0038763\n-------------+----------------------------------------------------------------\ntothrs       |\n      female |\n          0  |  -.0015482   .0007477    -2.07   0.039    -.0030165   -.0000798\n          1  |    .001412   .0012472     1.13   0.258    -.0010371    .0038612\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#avg-partial-effects-vs-partial-effects-at-x_c",
    "href": "rmethods/5_FXMRA.html#avg-partial-effects-vs-partial-effects-at-x_c",
    "title": "Multiple Regression Analysis",
    "section": "Avg Partial effects vs Partial effects at \\(X_c\\)",
    "text": "Avg Partial effects vs Partial effects at \\(X_c\\)\n\nWhenever you have interactions, higher order polynomials (or any nonlinear transformation of \\(X\\)), marginal effects are no longer constant, and may depend on additional information:\n\n\\[y = b_0 + b_1 x_1 + b_2 x_1^2 + e \\rightarrow \\frac{dy}{dx} = b_1 + 2b_2 x_1\n\\]\n\nWhat to do in this cases?\n\nEstimate Average marginal effects: \\(AME = E\\left(\\frac{dy}{dx}\\right) = b_1 + 2b_2 \\overline{x}_1\\)\nEstimate Marginal effects at means: \\(MEM = \\frac{dy}{dx}\\Big|_{x=\\bar x} = b_1 + 2b_2 \\overline{x}_1\\)\nEstimate Marginal effects at relevant values\nReport ALL marginal effects"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-4",
    "href": "rmethods/5_FXMRA.html#section-4",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "In Stata you can do this only for interactions. For constructed variables you need f_able, or do it by hand.\n\nreg y c.x1##c.x1##c.x1\nmargins, dydx(x1) &lt;-- Default is Average Marginal Effects\nmargins, dydx(x1) atmeans &lt;-- Request marginal effects at means\nmargins, dydx(x1) at(x1=(1/5)) &lt;-- Request marginal effects at specific values of x1\n* and plot afterwards\nmarginsplot"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#goodness-of-fit-r2-vs-r2_adj",
    "href": "rmethods/5_FXMRA.html#goodness-of-fit-r2-vs-r2_adj",
    "title": "Multiple Regression Analysis",
    "section": "Goodness of Fit: \\(R^2\\) vs \\(R^2_{adj}\\)",
    "text": "Goodness of Fit: \\(R^2\\) vs \\(R^2_{adj}\\)\n\nWith Great power…\nIMPORTANT: Low \\(R^2\\) does not mean a bad model, nor high \\(R^2\\) mean a good one.\n\nIf \\(N\\) is constant, adding more variables to your model will increase the Goodness of fit \\(R^2\\) (even if marginally)\n\nThis may lead to the incorrect intuition of choosing models with the highest \\(R^2\\)\nThis is wrong because \\(R^2\\) only measures in-sample fitness.\n\nAlternative, the Adjusted \\(R^2\\) (\\(R_{adj}^2\\)), which penalizes using multiple controls\n\n\\[R^2_{adj} = 1-\\frac{SSR/(n-k-1)}{SST/(n-1)}=1-(1-R^2)\\frac{n-1}{n-k-1}\n\\]\n\nMore controls \\(k\\) will not always increase \\(R^2_{adj}\\)"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-5",
    "href": "rmethods/5_FXMRA.html#section-5",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "\\(R^2_{adj}\\) and Model Selection\n\n\\(R^2_{adj}\\) can be used to choose between nested models.\n\nIf adding variables improves \\(R_{adj}^2\\), then choose that model.\n\nBut it can also be used to choose between non-nested models:\n\n\\[\\begin{aligned}\nM1: & y = b_0 + b_1 x_1 + b_2 x_2 + e \\\\\nM2: & y = b_0 + b_1 x_1 + b_3 x_3 + e \\\\\nM3: & y = b_0 + b_1 ln(x_1) + b_2 ln(x_2) + e \\\\\nM4: & y = b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3 + e\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#log-models-and-prediction",
    "href": "rmethods/5_FXMRA.html#log-models-and-prediction",
    "title": "Multiple Regression Analysis",
    "section": "log models and Prediction",
    "text": "log models and Prediction\n\nTransforming the Depvariable with logs is quite useful for interpretation, and addressing overdispersion\nHowever, obtaining predictions from such models is not straight forward:\n\n\\[\\begin{aligned}\nln(y) &= a_0 + a_1 x_1 + a_2 x_2 + \\varepsilon \\\\\ny &= exp(a_0 + a_1 x_1 + a_2 x_2 + \\varepsilon ) \\\\\nE(y|x_1,x_2) &=E(e^{a_0 + a_1 x_1 + a_2 x_2}) \\times E(e^ \\varepsilon ) \\\\\n& E(e^ \\varepsilon )\\neq 1\n\\end{aligned}\n\\]\n\nTo make Predictions in a log model we need some approximation for \\(E(e^ \\varepsilon )\\)"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-6",
    "href": "rmethods/5_FXMRA.html#section-6",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "We have Options:\nLets call \\(E(e^ \\varepsilon ) = \\alpha_0\\)\nOption 1 : \\(\\alpha_0 = n^{-1} \\sum( \\exp {\\hat\\varepsilon})\\)\nOption 2 : Under Normality of \\(\\varepsilon\\), \\(\\alpha_0 = \\exp(\\hat \\sigma^2/2)\\)\nOption 3 : Call \\(\\hat m = \\exp(a_0 + a_1 x_1 + a_2 x_2)\\).\nRegress \\(y\\) on \\(\\hat m\\) without intercept. \\(\\alpha_0 = \\frac{\\hat m'y}{\\hat m'\\hat m}\\)\n\nYour \\(\\hat y\\) prediction can now be used to estimate a comparable \\(R^2\\)\n\n\\[R^2 = Corr(y,\\hat y)^2 \\text{ or } 1-\\frac{\\sum(y_i-\\alpha_0 \\hat m_i)^2}{\\sum(y-\\bar y)^2}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-7",
    "href": "rmethods/5_FXMRA.html#section-7",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Example\n\nfrause oaxaca, clear\ndrop if lnwage==.\ngen wage = exp(lnwage)\nqui:reg lnwage educ exper tenure female married divorced\npredict lnw_hat\npredict lnw_res, res\n** Case 1:\negen alpha_01 = mean( exp(lnw_res))\n** Case 2:\nqui:sum lnw_res\ngen alpha_02 = exp(r(Var)/2)\ngen elnw_hat = exp(lnw_hat)\nqui: reg wage elnw_hat, nocons\ngen alpha_03 = _b[elnw_hat]\ngen wage_1 = elnw_hat\ngen wage_2 = elnw_hat*alpha_01\ngen wage_3 = elnw_hat*alpha_02\ngen wage_4 = elnw_hat*alpha_03\nmata:  y = st_data(.,\"wage\"); my = mean(y)\nmata:  yh = st_data(.,\"wage_1 wage_2 wage_3 wage_4\")\nmata:\"R2_1 \"; 1 - sum((y:-yh[,1]):^2)/sum( (y:-my):^2 )\nmata:\"R2_2 \"; 1 - sum((y:-yh[,2]):^2)/sum( (y:-my):^2 )\nmata:\"R2_3 \"; 1 - sum((y:-yh[,3]):^2)/sum( (y:-my):^2 )\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n(option xb assumed; fitted values)\n  R2_1 \n  .1569552664\n  R2_2 \n  .1692562931\n  R2_3 \n  .1658805115"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#limited-dependent-variables",
    "href": "rmethods/5_FXMRA.html#limited-dependent-variables",
    "title": "Multiple Regression Analysis",
    "section": "Limited Dependent variables",
    "text": "Limited Dependent variables\n\nSo far, we have impliclity assumed your dep. variable is continuous and unbounded.\nHowever, OLS imposes no distributional assumptions (A6 is more convinience)\nThis means that LRM using OLS can be used for variables with limited distribution!\n\nlike Dummies or count variables"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#linear-probability-model---lpm",
    "href": "rmethods/5_FXMRA.html#linear-probability-model---lpm",
    "title": "Multiple Regression Analysis",
    "section": "Linear Probability Model - LPM",
    "text": "Linear Probability Model - LPM\n\nLPM can be used when the dep.variable is a dummy, and the goal is to explain the Likelihood of something to happen.\n\n\\[\\begin{aligned}\nD &= b_0 + b_1 x_1 + b_2 x_2 +b_3 x_3 + e \\\\\nE(D|Xs) &= P(D=1|Xs) \\\\\n        &= b_0 + b_1 x_1 + b_2 x_2 +b_3 x_3\n\\end{aligned}\n\\]\nNote:\n\nFor marginal effects, we no longer consider effects at the individual level.\nInstead we look into conditional means, and likelihood\n\nfrause mroz, clear\nqui: reg inlf  age educ exper kidsge6 kidslt6 nwifeinc \nmodel_display\nE(inlf|X) = 0.707 - 0.018 age + 0.040 educ + 0.023 exper + 0.013 kidsge6 - 0.272 kidslt6 - 0.003 nwifeinc\nN = 753 R^2 = 0.254"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-8",
    "href": "rmethods/5_FXMRA.html#section-8",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Problems with LPM\n\nLPM are easy to estimate and interpret but it has some problems:\n\nPredictions could fall below 0 or above 1 (what does it mean?)\nUnless more flexible functional forms are allowed, mfx are fixed.\nThe model is, by construction, Heteroskedastic:\n\n\n\\[Var(y|x)=p(x)*(1-p(x))\n\\]\nThus SE will be incorrect, affecting inference"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#modeling-count-data",
    "href": "rmethods/5_FXMRA.html#modeling-count-data",
    "title": "Multiple Regression Analysis",
    "section": "Modeling Count Data",
    "text": "Modeling Count Data\n\nYou could also use LRM (via OLS) to model count data.\n\nCount data is always possitive, but with discrete values\n\n\n\\[Children = b_0 + b_1 age + b_2 education + e\\]\n\nNothing changes for estimation, but its useful to change language:\n\nfrause fertil2, clear\nqui reg children age educ\nmodel_display\nE(children|X) = -1.997 + 0.175 age - 0.090 educ\nN = 4361 R^2 = 0.560\n\n1 year of education decreases # of children in .09.\n1 year of education decreases Fertility .09 children per women.\nEvery 100 women, If they were 1 year more educated, we would expect to see 9 fewer children among them."
  },
  {
    "objectID": "rmethods/5_FXMRA.html#prediction-policy-and-shifting",
    "href": "rmethods/5_FXMRA.html#prediction-policy-and-shifting",
    "title": "Multiple Regression Analysis",
    "section": "Prediction, Policy and Shifting",
    "text": "Prediction, Policy and Shifting\n\nAs mentioned before, intercepts, or constant in model regressions are usually meaningless.\n\nBecause \\(a_0 = E(y|X=0)\\) (does it make sense)\n\nConstant, however, can be useful if we apply some transformations to the data. \\[y = b_0 +  b_1 (x_1 - c_1) +  b_2 (x_2 - c_2) +  b_3 (x_3 - c_3) +e\n\\]\n\nIn this case \\(b_0\\) is the expected value of \\(y\\) when \\(x_1=c_1\\), \\(x_2=c_2\\) and \\(x_3=c_3\\). Thus, its now Useful!\n\nUsing this affine transformation, we can easily make predictions (and get SE) for any specific values of interest.\n\nGranted, you could also use “margins”\n\n\n\nfrause gpa2, clear\ngen sat0=sat-1200\ngen hsperc0=hsperc-30\ngen hsize0=hsize-5\ngen hsize20=hsize^2-25\nqui:reg colgpa sat hsperc c.hsize##c.hsize\nmargins, at(sat = 1200 hsperc = 30 hsize = 5)\nreg colgpa sat0 hsperc0 hsize0 hsize20\n\n\nAdjusted predictions                                     Number of obs = 4,137\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\nAt: sat    = 1200\n    hsperc =   30\n    hsize  =    5\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _cons |   2.700075   .0198778   135.83   0.000     2.661104    2.739047\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     4,137\n-------------+----------------------------------   F(4, 4132)      =    398.02\n       Model |  499.030503         4  124.757626   Prob &gt; F        =    0.0000\n    Residual |  1295.16517     4,132  .313447524   R-squared       =    0.2781\n-------------+----------------------------------   Adj R-squared   =    0.2774\n       Total |  1794.19567     4,136  .433799728   Root MSE        =    .55986\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        sat0 |   .0014925   .0000652    22.89   0.000     .0013646    .0016204\n     hsperc0 |  -.0138558    .000561   -24.70   0.000    -.0149557   -.0127559\n      hsize0 |  -.0608815   .0165012    -3.69   0.000    -.0932328   -.0285302\n     hsize20 |   .0054603   .0022698     2.41   0.016     .0010102    .0099104\n       _cons |   2.700075   .0198778   135.83   0.000     2.661104    2.739047\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-9",
    "href": "rmethods/5_FXMRA.html#section-9",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Policy Evaluation\n\nWhen modeling \\(y = b_0 + \\delta \\ trt + b_1 x_1 + b_2 x_2 + e\\) the treatment effect \\(\\delta\\) was estimated under homogeneity assumption (only intercept shift)\nThis assumption can be relaxed by estimating separate models or using interactions.\nEffects can be estimated manually (separate models), margins or using shifts!\n\nUsing Separate models: \\[\\begin{aligned}\ny &= b^0_0 +  b^0_1 x_1 + b^0_2 x_2 + e^0 \\text{ if trt=0} \\\\\ny &= b^1_0 +  b^1_1 x_1 + b^1_2 x_2 + e^1 \\text{ if trt=1} \\\\\n& ATE = E(\\hat y_1 - \\hat y_0 ) \\\\\n& ATT = E(\\hat y_1 - \\hat y_0 | trt=1) \\\\\n& ATU = E(\\hat y_1 - \\hat y_0 | trt=0)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/5_FXMRA.html#section-10",
    "href": "rmethods/5_FXMRA.html#section-10",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Or using Model Shits\n\\[\\begin{aligned}\ny  &= b_0 + \\delta_{ate} trt + b_1 x_1 + g_1 trt (x_1- E(x_1)) + e \\\\\ny  &= b_0 + \\delta_{att} trt + b_1 x_1 + g_1 trt (x_1- E(x_1|trt=1)) + e \\\\\ny  &= b_0 + \\delta_{atu} trt + b_1 x_1 + g_1 trt (x_1- E(x_1|trt=0)) + e \\\\\n\\end{aligned}\n\\]\n\nfrause jtrain98, clear\nforeach i in earn96 educ age married {\n  sum `i' if train==0, meanonly\n  gen atu_`i' = (`i' - r(mean))*train\n  sum `i' if train==1, meanonly\n  gen att_`i' = (`i' - r(mean))*train\n  sum `i' , meanonly\n  gen ate_`i' = (`i' - r(mean))*train\n}\nqui:reg earn98 train earn96 educ age married\nest sto m1\nqui:reg earn98 train earn96 educ age married ate*\nest sto m2\nqui:reg earn98 train earn96 educ age married atu*\nest sto m3\nqui:reg earn98 train earn96 educ age married att*\nest sto m4\n\nesttab m1 m2 m3 m4, keep(train) mtitle(Homogenous ATE ATU ATT) se\n\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n               Homogenous             ATE             ATU             ATT   \n----------------------------------------------------------------------------\ntrain               2.411***        3.106***        3.533***        2.250***\n                  (0.435)         (0.532)         (0.667)         (0.449)   \n----------------------------------------------------------------------------\nN                    1130            1130            1130            1130   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/7_spec.html",
    "href": "rmethods/7_spec.html",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "There are various kinds of model specification we will talk about.\n\nThere are important variables you did not include in your model: Endogeneity\nYou added all relevant variables…just not in the right way.\nYou added proxies for variables you had no access to (Question change)\nYou have all relevant data, but with errors.\nYou have some missing data"
  },
  {
    "objectID": "rmethods/7_spec.html#what-do-we-mean-with-model-miss-specification",
    "href": "rmethods/7_spec.html#what-do-we-mean-with-model-miss-specification",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "There are various kinds of model specification we will talk about.\n\nThere are important variables you did not include in your model: Endogeneity\nYou added all relevant variables…just not in the right way.\nYou added proxies for variables you had no access to (Question change)\nYou have all relevant data, but with errors.\nYou have some missing data"
  },
  {
    "objectID": "rmethods/7_spec.html#section",
    "href": "rmethods/7_spec.html#section",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Simple linear functions work in almost ALL cases. They can be thought as first order Taylor expansions: \\[\\begin{aligned}\ny &= f(x) + e \\\\\nf(x) &\\simeq f(x_0)\n+\\frac{\\partial f(x)}{\\partial x}|_{x=x_0}\n(x-x_0)+R+e \\\\\nf(x) &\\simeq \\color{red}{ f(x_0)}\n\\color{red}{-\\frac{\\partial f(x)}{\\partial x}|_{x=x_0} x_0}\n+\\frac{\\partial f(x)}{\\partial x}|_{x=x_0} x+R+e \\\\\ny &= \\color{red}{\\beta_0}+\\beta_1 x + R+ e\n\\end{aligned}\n\\]\n\nSo, for “reasonable” values of X, or when analyzing average marginal effects \\(R\\) should be small enough to be ignored.\n\nIn other words, for Overall effects Simple linear model works reasonably well! (most of the time)"
  },
  {
    "objectID": "rmethods/7_spec.html#section-1",
    "href": "rmethods/7_spec.html#section-1",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "If you are interested in individuals (or alike people), you may need flexiblity!\nIgnoring functional form misspecification imposes unwanted assumptions (homogeneity), that could create further problems.\n\nSpecially if data is skewed\n\nBut how flexible is flexible enough?\n\nWe will only consider quadratic terms and interactions,\nbut there is a large literature on making very flexible estimations (non-paramatric analysis)\n\n\n\nclear\nset seed 10\nset obs 1000\ngen p = (2*_n-1)/(2*_N) \ngen x = invchi2(5, p)/2\ngen y = 1 + x + (x-2.5)^2 + rnormal()  \nreg y x\ndisplay \"Quadratic\"\nqui:reg y c.x##c.x\nmargins, dydx(x)\ndisplay \"Cubic\"\nqui:reg y c.x##c.x##c.x\nmargins, dydx(x)\n\nNumber of observations (_N) was 0, now 1,000.\n\n      Source |       SS           df       MS      Number of obs   =     1,000\n-------------+----------------------------------   F(1, 998)       =   1287.16\n       Model |  22189.0552         1  22189.0552   Prob &gt; F        =    0.0000\n    Residual |  17204.2788       998  17.2387563   R-squared       =    0.5633\n-------------+----------------------------------   Adj R-squared   =    0.5628\n       Total |  39393.3339       999  39.4327667   Root MSE        =     4.152\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |   2.983973   .0831722    35.88   0.000      2.82076    3.147185\n       _cons |  -1.467351   .2458875    -5.97   0.000    -1.949866   -.9848348\n------------------------------------------------------------------------------\nQuadratic\n\nAverage marginal effects                                 Number of obs = 1,000\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  x\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |   1.033401   .0258934    39.91   0.000     .9825894    1.084213\n------------------------------------------------------------------------------\nCubic\n\nAverage marginal effects                                 Number of obs = 1,000\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  x\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |   1.041387   .0292892    35.56   0.000     .9839117    1.098863\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/7_spec.html#reset-ramsey-test",
    "href": "rmethods/7_spec.html#reset-ramsey-test",
    "title": "Multiple Regression Analysis",
    "section": "Reset Ramsey test",
    "text": "Reset Ramsey test\n\nIntuition: If the model is misspecified, perhaps we need to control for more non-linearities and interactions.\nNaive test: Add more controls (quadratics and interactions) (like White test, this will grow fast)\nReset - Ramsey test: Get predictions from original model, and add it as control\n\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\delta_1 \\hat y^2 + \\delta_2 \\hat y^3 +e\n\\]\n\\(H_0: \\delta_1 = \\delta_2 = 0\\): (everything is awesome)\n\\(H_1: H_0\\) is false: we need to fix the problem\n\nRRT does not tell you “How” to fix the problem.\n\nestat ovtest\n(bad name tho)"
  },
  {
    "objectID": "rmethods/7_spec.html#davidson-mackinnon-test",
    "href": "rmethods/7_spec.html#davidson-mackinnon-test",
    "title": "Multiple Regression Analysis",
    "section": "Davidson-MacKinnon test",
    "text": "Davidson-MacKinnon test\nTwo non-tested models:\n\\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + e \\\\\ny &= \\gamma_0 + \\gamma_1 log(x_1) + \\gamma_2 log(x_2) + e \\\\\n\\end{aligned}\n\\]\n\nWhich one is more appropriate? eq1? or eq2? This are non-nested models, so its difficult to say.\n\nYou could nest them:\n\n\n\\[y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 log(x_1) + \\theta_4 log(x_2) + e\n\\]\nand test \\(\\theta_1=\\theta_2=0\\) or \\(\\theta_3=\\theta_4=0\\)."
  },
  {
    "objectID": "rmethods/7_spec.html#section-2",
    "href": "rmethods/7_spec.html#section-2",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "or the “true” Davidson-MacKinnon test:\n\nFirst Obtain predictions from competing models: \\[\\begin{aligned}\n\\hat y &= \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 \\\\\n\\check y &= \\hat \\gamma_0 + \\hat\\gamma_1 log(x_1) + \\hat\\gamma_2 log(x_2) \\\\\n\\end{aligned}\n\\]\nThen add the predictions as added controls in the alternative model: \\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\theta_1 \\check y +e \\\\\ny &= \\gamma_0 + \\gamma_1 log(x_1) + \\gamma_2 log(x_2) + \\theta_1 \\hat y + e \\\\\n\\end{aligned}\n\\]\n\nUnfortunately, you may ended up with conflicting results."
  },
  {
    "objectID": "rmethods/7_spec.html#a-re-tell-of-omitted-variable-bias",
    "href": "rmethods/7_spec.html#a-re-tell-of-omitted-variable-bias",
    "title": "Multiple Regression Analysis",
    "section": "A re-tell of Omitted variable Bias",
    "text": "A re-tell of Omitted variable Bias\n\nWe know this. If a variable that SHOULD be in the model is not added, it will generate an OMV, unless it was uncorrelated to the model error.\n\nLesson: add important variables!\n\nWhat if those variables are not available? how do you solve the problem?\n\nIV (we will talk about that later) or\nProxy Variable (a bandaid)"
  },
  {
    "objectID": "rmethods/7_spec.html#proxies",
    "href": "rmethods/7_spec.html#proxies",
    "title": "Multiple Regression Analysis",
    "section": "Proxies",
    "text": "Proxies\nConsider: \\[log(wages) = \\beta_0 + \\color{blue}{\\beta_1} exper + \\color{blue}{\\beta_2} educ + \\beta_3 skill + e\n\\]\nWhere you are really interested in \\(\\beta_1 \\And \\beta_2\\).\n\nSince we dont have \\(skill\\), and omitting it will bias our coefficients, we can use a proxy \\(ASVAB\\).\n\n\\[log(wages) = \\beta_0 + \\color{blue}{\\beta_1} exper + \\color{blue}{\\beta_2} educ + \\gamma_3 ASVAB + e\n\\]\n\nand done?"
  },
  {
    "objectID": "rmethods/7_spec.html#section-3",
    "href": "rmethods/7_spec.html#section-3",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Using a Proxy will work only under the following condition:\n\nConditioning on the observed variable and proxy, the unobserved variable has to be uncorrelated to other variables in the model:\n\n\\[\\begin{aligned}\nE(x_3^*|x_1,x_2,x_3)&=\\alpha_0 + \\alpha_1 x_3 \\\\\nE(skill|exper,educ,ASVAB)&=\\alpha_0 + \\alpha_1 ASVAB\n\\end{aligned}\n\\]\nIf this happens, you can still estimate \\(\\beta_1 \\And \\beta_2\\), although the constant and slope of the proxy varible will be biased for the proxied variable.\n\\[\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x^*_3 + e \\ ; \\\n\\color{blue}{x^*_3 =  \\delta_0 + \\delta_1 x_3 + v} \\\\\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (\\delta_0 + \\delta_1 x_3 + v) + e \\\\\n&= \\color{brown}{\\beta_0 +\\beta_3\\delta_0} \\color{black}{+ \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 \\delta_1 x_3 +} \\color{green}{\\beta_3 v + e} \\\\\n&=\\color{brown}{\\alpha_0} + \\beta_1 x_1 + \\beta_2 x_2 + \\alpha_1 x_3 + \\color{green}{u} \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/7_spec.html#section-4",
    "href": "rmethods/7_spec.html#section-4",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "What about Lags (of dep variable)?\n\nIncreses Data requirements (panel? pseudo panel?)\nFurther assumptions are required (Past exogenous of present)\nBut allows controlling for underlying factors or historical factors\n\n\nfrause crime2, clear\nqui:reg crmrte unem llawexpc if year == 87\nest sto m1\nqui:reg crmrte unem llawexpc lcrmrt_1 if year == 87\nest sto m2\nqui:reg ccrmrte unem llawexpc if year==87  \nest sto m3\nesttab m1 m2 m3, se star(* .1 ** 0.05 *** 0.01) b(3) ///\nmtitle(crimert crimert change_crrt)\n\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                  crimert         crimert     change_crrt   \n------------------------------------------------------------\nunem               -3.659           0.346          -0.125   \n                  (3.471)         (2.127)         (2.152)   \n\nllawexpc           16.452         -20.059*        -10.377   \n                 (18.531)        (11.842)        (11.487)   \n\nlcrmrt_1                          127.111***                \n                                 (14.399)                   \n\n_cons              10.655        -337.106***       79.288   \n                (134.223)        (89.507)        (83.200)   \n------------------------------------------------------------\nN                      46              46              46   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;.1, ** p&lt;0.05, *** p&lt;0.01\n\n\nNote: Skip 9-2c and 9-3"
  },
  {
    "objectID": "rmethods/7_spec.html#why-is-x-not-the-real-x",
    "href": "rmethods/7_spec.html#why-is-x-not-the-real-x",
    "title": "Multiple Regression Analysis",
    "section": "Why is \\(X\\) not the real \\(X\\)?",
    "text": "Why is \\(X\\) not the real \\(X\\)?\n\nOften we treat data as if it they were perfect measures of the true data. But is that the case?\n\nAge: Do you report age in years, months, days, hours, minutes, etc\nWeight and Height: Even if measured, how accurate it can be? and do they make mistakes?\nIncome: Do people report income accurately? or they Lie? why?\n\nDepending on the type of error, magnitude, and if the affected variable is dep or indep, it may have diffrent consequences for OLS.\nFor now we will concentrate on a specific kind of measurement error: Classical measurement error\n\n\\[\\begin{aligned}\ny_{obs} &= y_{true} + \\varepsilon \\\\\nE(\\varepsilon) &=0; cov(\\varepsilon,y_{true})=0; cov(\\varepsilon,X's)=0\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/7_spec.html#error-in-y-dep-variable",
    "href": "rmethods/7_spec.html#error-in-y-dep-variable",
    "title": "Multiple Regression Analysis",
    "section": "Error in \\(y\\) (dep variable)",
    "text": "Error in \\(y\\) (dep variable)\n\nInstead of: \\(y^* = x\\beta + e\\)\nWe estimate \\(y^*+\\varepsilon = x\\beta + e \\rightarrow y^* = x\\beta + e-\\varepsilon\\)\nThis implies that \\(\\beta's\\) can still be unbiased when applying OLS.\nHowever variance will be larger than when using true data:\n\n\nqui: frause oaxaca, clear\nset seed 101\ngen lnwage2=lnwage + rnormal(2) \nqui:reg lnwage educ exper female\nest sto m1\nqui:reg lnwage2 educ exper female\nest sto m2\nesttab m1 m2, se\n\n(213 missing values generated)\n\n--------------------------------------------\n                      (1)             (2)   \n                   lnwage         lnwage2   \n--------------------------------------------\neduc               0.0858***       0.0902***\n                (0.00521)        (0.0120)   \n\nexper              0.0147***       0.0171***\n                (0.00126)       (0.00291)   \n\nfemale            -0.0949***      -0.0759   \n                 (0.0251)        (0.0580)   \n\n_cons               2.219***        4.132***\n                 (0.0687)         (0.159)   \n--------------------------------------------\nN                    1434            1434   \n--------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/7_spec.html#error-in-x-indep-variable",
    "href": "rmethods/7_spec.html#error-in-x-indep-variable",
    "title": "Multiple Regression Analysis",
    "section": "Error in \\(X\\) (indep variable)",
    "text": "Error in \\(X\\) (indep variable)\n\nInstead of: \\(y = \\beta_0 + \\beta_1 x^* + e\\)\nWe estimate \\(y = \\gamma_0 + \\gamma_1 (x^* + \\varepsilon) + v\\)\nBy adding an error \\(\\varepsilon\\) that has a zero relationship with \\(y\\), the “average” coefficient \\(\\gamma_1\\) will be between the true \\(\\beta_1\\) and 0. \\[\\begin{aligned}\n\\gamma_1 &=\\frac{\\sum (y-\\bar y)(x^* + \\varepsilon - \\bar x)}{\\sum (x^* + \\varepsilon - \\bar x)^2} =\\frac{\\sum (y-\\bar y)(x^* - \\bar x)+ \\sum (y-\\bar y) \\varepsilon}{\\sum (x^* - \\bar x)^2 + \\sum \\varepsilon^2} \\\\\n&= \\frac{\\sum (y-\\bar y)(x^* - \\bar x)}{\\sum (x^* - \\bar x)^2 + \\sum \\varepsilon^2} \\frac{\\sum (x^* - \\bar x)^2}{\\sum (x^* - \\bar x)^2} \\\\\n& =\\beta_1 \\frac{\\sigma^2_x}{\\sigma^2_x + \\sigma^2_\\varepsilon}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/7_spec.html#section-5",
    "href": "rmethods/7_spec.html#section-5",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "frause oaxaca, clear\nqui:sum educ\ngen educ_error = educ + rnormal()*r(sd)\nsum educ educ_error\nqui:reg lnwage educ\nest sto m1\nqui:reg lnwage educ_error\nest sto m2\nesttab m1 m2, se\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        educ |      1,647    11.40134    2.374952          5       17.5\n  educ_error |      1,647    11.36352    3.400767   .6707422   26.90462\n\n--------------------------------------------\n                      (1)             (2)   \n                   lnwage          lnwage   \n--------------------------------------------\neduc               0.0800***                \n                (0.00539)                   \n\neduc_error                         0.0399***\n                                (0.00395)   \n\n_cons               2.434***        2.898***\n                 (0.0636)        (0.0475)   \n--------------------------------------------\nN                    1434            1434   \n--------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/7_spec.html#missing-data-assume-sample-is-complete",
    "href": "rmethods/7_spec.html#missing-data-assume-sample-is-complete",
    "title": "Multiple Regression Analysis",
    "section": "Missing Data (Assume Sample is complete)",
    "text": "Missing Data (Assume Sample is complete)\n\nWhat is it? you dont have data! Your \\(N\\) falls.\n\nSome data for some observations are missing.\nWe may or may not know why they are missing\nand they maybe missing at random, or following unknown patterns.\n\nIf we are Missing data, and we do not know why, its a problem. We cant know if the sample represents the population, thus cannot be used for analysis."
  },
  {
    "objectID": "rmethods/7_spec.html#section-6",
    "href": "rmethods/7_spec.html#section-6",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "How to deal with it?\n\nif Missing completely at random (MCAR), analysis can be done as usual (no effects except smaller N)\nif Missing at random (MAR), the analysis can be done, often using standard methods:\n\nMissingness depends on observed factors (\\(X's\\)).\nIt is also known as exogenous sample selection.\nIntuitively, because all factors that determine selection are exogenous, you can identify who in the population is identified (Regression for men, women, high education, etc)\n\nIf Missing not at random (MNAR), you cant address the problem with standard analysis.\n\nSome methods such as Heckman selection or truncated regression, could be used. (advanced)\nOther wise, you can’t analyze the data (in a satisfactory manner)\nIntuitively, missingness is determined by unobserved factors, which also determines the outcome. (ie Analyze high wage population only)"
  },
  {
    "objectID": "rmethods/7_spec.html#outliers-and-influencers",
    "href": "rmethods/7_spec.html#outliers-and-influencers",
    "title": "Multiple Regression Analysis",
    "section": "Outliers and influencers",
    "text": "Outliers and influencers\n\nNot all data is made equal, and not all data has the same weight when estimating regressions.\nObservations with high Influence are those with outliers based on the conditional distribution (\\(y|x\\)).\n\nWhile outliers are not necessarily bad for analysis, it is important to understand how sensitive your results are to excluding some observations.\n\nObservations with high leverage are those with unusual characteristics.(\\(X's\\))\nCombination of both may have strong impacts on the regression analysis."
  },
  {
    "objectID": "rmethods/7_spec.html#section-7",
    "href": "rmethods/7_spec.html#section-7",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Leverage of an observation is determined by the following:\n\nDefine \\(H = X(X'X)^{-1}X'\\)\nLeverage \\(h_i = H[i,i]\\)\nHigh \\(h_i\\) denotes more influence in the model. (sensitive)\n\nInfluence is typically detected based on “studentized” residuals\n\n\\[r_i =  \\frac{\\hat e}{s_{-i}\\sqrt{1-h_i}}\n\\]"
  },
  {
    "objectID": "rmethods/7_spec.html#example",
    "href": "rmethods/7_spec.html#example",
    "title": "Multiple Regression Analysis",
    "section": "Example",
    "text": "Example\n\nqui:{\nfrause oaxaca, clear\ndrop if lnwage==.\nreg lnwage educ exper tenure female age\npredict lev, lev\nsum lev, meanonly\nreplace lev=lev/r(mean)\npredict rst, rstud\n}\nset scheme white2\ncolor_style tableau\nscatter lev rst"
  },
  {
    "objectID": "rmethods/7_spec.html#solutions",
    "href": "rmethods/7_spec.html#solutions",
    "title": "Multiple Regression Analysis",
    "section": "Solutions",
    "text": "Solutions\n\nThe problem with OLS is that it provides “too much weight” to outliers.\nThis is similar to the mean, which may not be very stable with extreme distributions.\n\nThere are at least two solutions to problems with outliers.\n\nRobust Regression (different from regression with robust Standard errors)\n\nThe idea is to penalize outliers, to reduce the impact on the estimated coefficients."
  },
  {
    "objectID": "rmethods/7_spec.html#section-8",
    "href": "rmethods/7_spec.html#section-8",
    "title": "Multiple Regression Analysis",
    "section": "",
    "text": "Quantile (median) Regression\n\nModifies the objective function to be minized:\n\n\n\\[\\beta's=\\min_\\beta \\sum |y-x\\beta|\n\\]\n\nInstead of using the squared of errors, it uses the absolute value.\n\nby doing this, coefficients are not sensitive to outliers! (as the median is better than the mean to capture typical values)\nDrawbacks: Its slower than OLS, and it can be difficult to interpret\n\n\nrreg &lt;- Robust Regression\nqreg &lt;- Quantile Regression"
  },
  {
    "objectID": "rmethods/9_ldvm.html",
    "href": "rmethods/9_ldvm.html",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "clear\nset obs 2000\ngen r1 = runiform()\ngen r2 = rchi2(5)/5 \ngen r3 = round(rchi2(3))*3\ngen r4 = rnormal()\nset scheme white2\ncolor_style tableau\nhistogram r1, name(m1, replace) \nhistogram r2, name(m2, replace)\nhistogram r3, name(m3, replace) width(1)  \nhistogram r4, name(m4, replace)\ngraph combine m1 m2 m3 m4\ngraph export images/fig9_1.png, width(1000) replace\n\n\n\n\nLimited Dependent variables"
  },
  {
    "objectID": "rmethods/9_ldvm.html#what-do-we-mean-limited",
    "href": "rmethods/9_ldvm.html#what-do-we-mean-limited",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "clear\nset obs 2000\ngen r1 = runiform()\ngen r2 = rchi2(5)/5 \ngen r3 = round(rchi2(3))*3\ngen r4 = rnormal()\nset scheme white2\ncolor_style tableau\nhistogram r1, name(m1, replace) \nhistogram r2, name(m2, replace)\nhistogram r3, name(m3, replace) width(1)  \nhistogram r4, name(m4, replace)\ngraph combine m1 m2 m3 m4\ngraph export images/fig9_1.png, width(1000) replace\n\n\n\n\nLimited Dependent variables"
  },
  {
    "objectID": "rmethods/9_ldvm.html#section",
    "href": "rmethods/9_ldvm.html#section",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "What do we mean Limited??\n\nWhen we think about “limited dependent variable” models, we refer to models when the distribution of the dep.variable is “limited”\n\nIn other words. The values it can take are restricted! (positive, or only integer), within a range, etc\n\nCan you still use LRM for them?\nWill anything change if you do?\nDo we care?"
  },
  {
    "objectID": "rmethods/9_ldvm.html#no-we-dont-but..",
    "href": "rmethods/9_ldvm.html#no-we-dont-but..",
    "title": "Limited Dependent Variable Models",
    "section": "No we dont, but..",
    "text": "No we dont, but..\n\nWe dont really care. In fact we have already use LRM on that fashion:\n\nLPM: Dep variable was a Dummy\nWages: Always positive\n# Children: Countable\n\nBut, there are couple of things one should consider.\n\nModels of this kind are usually heteroskedastic by construction. (robust? Weighted?)\nPredictions could made no sense.\nThere are better models we could use to analyze the data\n\n\nBetter under some assumptions\n\nHowever, this models cannot be estimated using OLS (there is no “close form solution”)\nWe may need to learn a new method: Maximum Likelihood"
  },
  {
    "objectID": "rmethods/9_ldvm.html#probits-and-logits",
    "href": "rmethods/9_ldvm.html#probits-and-logits",
    "title": "Limited Dependent Variable Models",
    "section": "Probits and Logits",
    "text": "Probits and Logits\n\nLPM are easy, fast, and good for most data analysis (exploration). But they have some limitations.\nMost limitations can be overcome with alternative models: Logit or Probit\nIn constrast with LPM (which aims to explain individual outcomes), Logit/probit aims to explain Conditional Probabilities:\n\n\\[p(y=1|x) = G(x\\beta)\\]\n\nwhere the function \\(G()\\) makes sure the predicted outcome is always between 0 and 1.\nCaveat: Because \\(G()\\) is nonlinear, this is a nonlinear model, and marginal effects are harder to estimate."
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-1",
    "href": "rmethods/9_ldvm.html#section-1",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "What to use for \\(G()\\)\n\nTwo leading options:\n\n\\[logit: G(x\\beta) = \\frac{\\exp{x\\beta}}{1+\\exp{x\\beta}}\\] \\[probit: G(x\\beta) = \\Phi(x\\beta)=\\int_{-\\infty}^{x\\beta}\\phi(z)dz\\]\n\nBut in practice Either will work. Then why the difference?"
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-2",
    "href": "rmethods/9_ldvm.html#section-2",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "Probits and Logits: Latent variables\n\nIt all comes down to the Latent variable!\nAssumption:\n\nEverybody has a latent score on every “binary” decision: The value to a decision \\(y^*\\) \\[y^* = x\\beta + e \\]\nIf \\(y^*\\) is above certain threshold (\\(y^*&gt;0\\)), you “do” something (\\(y=1\\)). If not you dont (\\(y=0\\)).\n\nThus the choice between logit and probit depends on the distribution of \\(e\\).\n\n\\(e\\) is normal, then probit\n\\(e\\) is logistic, then logit"
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-3",
    "href": "rmethods/9_ldvm.html#section-3",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "Some Math\nLatent Model:\n\\[ y^* = x\\beta + e \\]\nWe aim to measure the probablity of a positive latent.\n\\[\\begin{aligned}\nP(y^*&gt;0|x) & = P(x\\beta + e&gt;0|x) \\\\\n& = P( e&gt;- x\\beta|x) \\\\\n& = 1 - P( e &lt; - x\\beta|x) = 1-G( - x\\beta|x) \\\\\n& = G(x\\beta)\n\\end{aligned}\n\\]\nlast step valid only if \\(G()\\) is symetrical."
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-4",
    "href": "rmethods/9_ldvm.html#section-4",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "Marginal Effects?\n\nSame as before. The partial derivative!\n\n\\[\\begin{aligned}\np(y=1|x) &= G(\\beta_0 + \\beta_1 x_1 +\\beta_2 x_2 ) \\\\\n\\frac{\\partial p(y=1|x)}{\\partial x_1} = G'(x\\beta)\\beta_1=g(x\\beta)\\beta_1\n\\end{aligned}\n\\]\n\nBut if variables are dummies, we need to estimate true effect.\n\n\\[\\begin{aligned}\np(y=1|x) &= G(\\beta_0 + \\beta_1 x_1 +\\beta_2 D_2 ) \\\\\n\\frac{\\partial p(y=1|x)}{\\partial D_2} = G(\\beta_0 + \\beta_1 x_1 +\\beta_2 )-G(\\beta_0 + \\beta_1 x_1 )\n\\end{aligned}\n\\]\nand yes, you could also have interactions, polynomials, etc"
  },
  {
    "objectID": "rmethods/9_ldvm.html#mle-how-does-this-work",
    "href": "rmethods/9_ldvm.html#mle-how-does-this-work",
    "title": "Limited Dependent Variable Models",
    "section": "MLE: How does this work?",
    "text": "MLE: How does this work?\n\nMLE: Maximum Likelihood Estimator, is an alternative method to OLS that allows you to estimate parameters in nonlinear models.\nThe idea of the method is to “model” the conditional distribution of the data \\(F(y|x,\\theta)\\) or \\(f(y|x,\\theta)\\), assuming \\(X's\\) are given and modifying values of \\(\\theta\\) (distribution parameters).\n\\(LRM\\) could be estimated via MLE, but you will need More assumptions:\n\nThe error \\(e\\) is normal.\n\nThen “simply” find the parameters for the mean and variance that “maximizes” the probability that data Comes a given distribution.\nIn the case of Probit/logit, there is “only” one paramter we need to identify. The conditional probabilty \\(p(y=1|X)\\).\n\nExcept that we allow this to vary by \\(X\\)"
  },
  {
    "objectID": "rmethods/9_ldvm.html#likelihood-function-for-logitprobit",
    "href": "rmethods/9_ldvm.html#likelihood-function-for-logitprobit",
    "title": "Limited Dependent Variable Models",
    "section": "Likelihood function for Logit/probit",
    "text": "Likelihood function for Logit/probit\n\\[L_i = G(x\\beta)^{y=1}*(1-G(x\\beta))^{y=0}\n\\]\nUnder Independence:\n\\[L_D = L_1 \\times L_2 \\times \\dots L_N\n\\]\nThus we need to find the \\(\\beta's\\) that make \\(L_D\\) the largest.\nBut because we like sums over products:\n\\[LL_D = \\sum_{i=1}^N log(L_i)\n\\]"
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-5",
    "href": "rmethods/9_ldvm.html#section-5",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "clear\n  set obs 25\n  gen r = runiform()&lt;.7\n  mata: \n    r = st_data(.,\"r\")\n    ll = J(99,2,0)\n    for(i=1;i&lt;=99;i++){\n      theta = i/100\n      // Log Properties\n      ll[i,]= theta,exp(sum(log(theta:^(r:==1) :* (1-theta):^(r:==0))))\n    }\n  end\n  qui getmata ll*=ll , force\n  ren ll1 theta\n  ren ll2 likelihood\n  *scatter likelihood theta \n\nNumber of observations (_N) was 0, now 25."
  },
  {
    "objectID": "rmethods/9_ldvm.html#testing",
    "href": "rmethods/9_ldvm.html#testing",
    "title": "Limited Dependent Variable Models",
    "section": "Testing?",
    "text": "Testing?\n\nYou can test two things:\n\nTest coefficients (\\(\\beta\\))\nTest marginal effects (\\(G'(x\\beta)\\beta\\))\n\nBoth test will most likely agree with each other, but some contradictions may arise. ### How?\nz-test and/or Wald test: Similar to t-test and Joint F-test we cover before. But, we now make the assumption of normality (not t-distribution)\nLog-Likelihood test. Similar to F-test for restricted and unrestricted model:\n\nEstimate both Restricted and unrestricted model. And obtain their Log Likelihoods (\\(\\mathcal{L}_ur\\)) and (\\(\\mathcal{L}_r\\)).\n\n\\[LR = 2 (\\mathcal{L}_ur-\\mathcal{L}_r) \\overset{a}\\sim \\chi^2_q\\]"
  },
  {
    "objectID": "rmethods/9_ldvm.html#stata---example",
    "href": "rmethods/9_ldvm.html#stata---example",
    "title": "Limited Dependent Variable Models",
    "section": "Stata - Example",
    "text": "Stata - Example\n\nfrause mroz, clear\n* LPM with Robust Standard errors\nqui:reg inlf nwifeinc educ exper expersq age kidslt6 kidsge6, robust\nest sto m1\nqui:logit inlf nwifeinc educ exper expersq age kidslt6 kidsge6, \nest sto m2a\nqui:margins, dydx(*) post\nest sto m2b\nprobit inlf nwifeinc educ exper expersq age kidslt6 kidsge6, \nest sto m3a\nqui:margins, dydx(*) post\nest sto m3b\n\n\nIteration 0:   log likelihood =  -514.8732  \nIteration 1:   log likelihood = -402.06651  \nIteration 2:   log likelihood = -401.30273  \nIteration 3:   log likelihood = -401.30219  \nIteration 4:   log likelihood = -401.30219  \n\nProbit regression                                       Number of obs =    753\n                                                        LR chi2(7)    = 227.14\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -401.30219                             Pseudo R2     = 0.2206\n\n------------------------------------------------------------------------------\n        inlf | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    nwifeinc |  -.0120237   .0048398    -2.48   0.013    -.0215096   -.0025378\n        educ |   .1309047   .0252542     5.18   0.000     .0814074     .180402\n       exper |   .1233476   .0187164     6.59   0.000     .0866641    .1600311\n     expersq |  -.0018871      .0006    -3.15   0.002     -.003063   -.0007111\n         age |  -.0528527   .0084772    -6.23   0.000    -.0694678   -.0362376\n     kidslt6 |  -.8683285   .1185223    -7.33   0.000    -1.100628    -.636029\n     kidsge6 |    .036005   .0434768     0.83   0.408     -.049208    .1212179\n       _cons |   .2700768    .508593     0.53   0.595    -.7267473    1.266901\n------------------------------------------------------------------------------\n\n\n\nset linesize 255\n*| classes: larger\ndisplay \"Prob Models\"\nesttab m1 m2a m2b m3a m3b, scalar(r2 ll) cell(b(fmt(%5.3f)) ///\nse(par([ ])) p( par(( )) ) )  gap  mtitle(LPM Logit Logit-mfx Probit Probit-mfx)\n\nProb Models\n\n-----------------------------------------------------------------------------\n                      (1)          (2)          (3)          (4)          (5)\n                      LPM        Logit    Logit-mfx       Probit   Probit-mfx\n                   b/se/p       b/se/p       b/se/p       b/se/p       b/se/p\n-----------------------------------------------------------------------------\nmain                                                                         \nnwifeinc           -0.003       -0.021       -0.004       -0.012       -0.004\n                  [0.002]      [0.008]      [0.001]      [0.005]      [0.001]\n                  (0.026)      (0.011)      (0.010)      (0.013)      (0.012)\n\neduc                0.038        0.221        0.039        0.131        0.039\n                  [0.007]      [0.043]      [0.007]      [0.025]      [0.007]\n                  (0.000)      (0.000)      (0.000)      (0.000)      (0.000)\n\nexper               0.039        0.206        0.037        0.123        0.037\n                  [0.006]      [0.032]      [0.005]      [0.019]      [0.005]\n                  (0.000)      (0.000)      (0.000)      (0.000)      (0.000)\n\nexpersq            -0.001       -0.003       -0.001       -0.002       -0.001\n                  [0.000]      [0.001]      [0.000]      [0.001]      [0.000]\n                  (0.002)      (0.002)      (0.001)      (0.002)      (0.001)\n\nage                -0.016       -0.088       -0.016       -0.053       -0.016\n                  [0.002]      [0.015]      [0.002]      [0.008]      [0.002]\n                  (0.000)      (0.000)      (0.000)      (0.000)      (0.000)\n\nkidslt6            -0.262       -1.443       -0.258       -0.868       -0.261\n                  [0.032]      [0.204]      [0.032]      [0.119]      [0.032]\n                  (0.000)      (0.000)      (0.000)      (0.000)      (0.000)\n\nkidsge6             0.013        0.060        0.011        0.036        0.011\n                  [0.014]      [0.075]      [0.013]      [0.043]      [0.013]\n                  (0.337)      (0.422)      (0.421)      (0.408)      (0.407)\n\n_cons               0.586        0.425                     0.270             \n                  [0.152]      [0.860]                   [0.509]             \n                  (0.000)      (0.621)                   (0.595)             \n-----------------------------------------------------------------------------\nN                     753          753          753          753          753\nr2                  0.264                                                    \nll                 -423.9       -401.8                    -401.3             \n-----------------------------------------------------------------------------\n\n\n\ndisplay \"LR test\"\nqui:probit inlf nwifeinc educ exper expersq age kidslt6 kidsge6 motheduc fatheduc, \nest sto unrestricted\nqui:probit inlf nwifeinc educ exper expersq age kidslt6 kidsge6 , \nest sto restricted\nlrtest unrestricted restricted\n\nLR test\n\nLikelihood-ratio test\nAssumption: restricted nested within unrestricted\n\n LR chi2(2) =   0.29\nProb &gt; chi2 = 0.8668"
  },
  {
    "objectID": "rmethods/9_ldvm.html#censored-and-truncated-data",
    "href": "rmethods/9_ldvm.html#censored-and-truncated-data",
    "title": "Limited Dependent Variable Models",
    "section": "Censored and Truncated Data",
    "text": "Censored and Truncated Data\n\nLogits and Probits, are not the only models that require MLE for estimation.\n\nAmong Discrete data models, you also have ologit/oprobit for ordered responses. mlogit/mprobit for unordered ones. Extends on logit/probit.\n\nThere are other interesting cases:\n\nWhen Data is censored.\nWhen Data is truncated."
  },
  {
    "objectID": "rmethods/9_ldvm.html#three-cases",
    "href": "rmethods/9_ldvm.html#three-cases",
    "title": "Limited Dependent Variable Models",
    "section": "Three Cases",
    "text": "Three Cases\n\nCase 1Case 2Case 3\n\n\n\n\\(y\\) is “conditionally-normal” and is Fully Observed.\nYou can estimate the model using OLS or ML\n\n\nqui:{\n  clear\n  set obs 999\n  gen p   = _n/(_N+1)\n  gen fob = invnormal(p)\n}\nqui:histogram fob\n\n\n\n\n\n\n\n\n\n\n\nData is observed for everyone, but is “censored” for some. tobit\n\nEither corner solution (how many hours you study) or Recoded: \\(y_{obs} = max(c,y^*)\\)\n\n\n\nqui: replace fob = -2 if fob&lt;-2\nqui:histogram fob, xlabel(-4 (2) 4)\n\n\n\n\n\n\n\n\n\n\n\nBelow (or above) some threshold, you do not have information on \\(y\\). truncreg \\[y_{obs} = y^* \\text{ if } y^*&gt;c\\]"
  },
  {
    "objectID": "rmethods/9_ldvm.html#estimation-censored-and-corner-solution",
    "href": "rmethods/9_ldvm.html#estimation-censored-and-corner-solution",
    "title": "Limited Dependent Variable Models",
    "section": "Estimation: Censored and Corner Solution",
    "text": "Estimation: Censored and Corner Solution\nIf data is censored or corner solution the estimation strategy is based on:\n\\[\\begin{aligned}\nL_i &= \\frac{1}{\\sigma} \\phi\\left( \\frac{y-x\\beta}{\\sigma} \\right) \\text{ if } y&gt;c \\\\\n    &= 1-\\Phi\\left(\\frac{x\\beta}{\\sigma} \\right) \\text{ if } y\\leq c \\\\\n\\end{aligned}\n\\]\nIf data is truncated, we need to “adjust” the distribution of what is observed\n\\[\\begin{aligned}\nL_i &= \\frac{1}{\\Phi\\left( x\\beta/\\sigma \\right)} \\frac{1}{\\sigma} \\phi\\left( \\frac{y-x\\beta}{\\sigma} \\right) \\text{ if } y&gt;c \\\\  \n\\end{aligned}\n\\]\nWe will put -truncated regression- on the side for now. But see here for an example."
  },
  {
    "objectID": "rmethods/9_ldvm.html#interpretation-it-depends",
    "href": "rmethods/9_ldvm.html#interpretation-it-depends",
    "title": "Limited Dependent Variable Models",
    "section": "Interpretation: It depends!",
    "text": "Interpretation: It depends!\n\nWhat are you interested in analyzing? and what type of data you have?\n\n\nLatent variable\\(P(y&gt;0|x)\\)\\(E(y|y&gt;0,x)\\)\\(E(y|x)\\)\n\n\n\nEasiest Case. Just need to consider the coefficients (as in LRM)\n\n\\[\n\\begin{aligned}\nE(y^*|x) &= x\\beta \\\\\n\\frac{\\partial E(y^*|x)}{\\partial x } &= \\beta_x\n\\end{aligned}\n\\]\n\nThe same applies if model was censored.\n\n\n\n\nIts an alternative approach to Probit models, where you are interest in analyzing why is data Not censored, or why is it above some threshold. (why people work)\nExtensive margin effect. \\[\n\\begin{aligned}\nP(y&gt;0|x) &= \\Phi\\left(\\frac{x\\beta}{\\sigma}\\right) \\\\\n\\frac{\\partial P(y&gt;0|x)}{\\partial x } &= \\frac{\\beta_x}{\\sigma} \\phi\\left(\\frac{x\\beta}{\\sigma}\\right)\n\\end{aligned}\n\\]\n\nNote: Coefficients \\(\\beta\\) need to be Standardized.\n\n\n\nIf corner solution, one may be interested in the effect of those with positive outcomes only.\nThis is the intensive margin effect. \\[\n\\begin{aligned}\nE(y|y&gt;0,x) &= x\\beta + \\sigma \\frac{\\phi(x\\beta / \\sigma )}{\\Phi(x\\beta / \\sigma )} \\\\\n\\frac{\\partial E(y|y&gt;0,x)}{\\partial x } &= \\beta_x\n\\left[ 1-\\frac{\\phi(x\\beta / \\sigma )}{\\Phi(x\\beta / \\sigma )} \\left( \\frac{x\\beta }{\\sigma }+ \\frac{\\phi(x\\beta / \\sigma )}{\\Phi(x\\beta / \\sigma )}\\right) \\right]\n\\end{aligned}\n\\]\n\n\n\n\nIn this case, one may be interested in estimating the expected effect on everyone.\nCombines both Intensive and extensive margin effects. Comparable to OLS.\n\n\\[\n\\begin{aligned}\nE(y|x) &= p(y&gt;0|x)*E(y|y&gt;0,x) + (1-p(y&gt;0|x))*0 \\\\\n\\frac{\\partial E(y|x)}{\\partial x } &= \\beta_x \\Phi(x\\beta)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods/9_ldvm.html#example",
    "href": "rmethods/9_ldvm.html#example",
    "title": "Limited Dependent Variable Models",
    "section": "Example",
    "text": "Example\n\nfrause mroz, clear\nqui:tobit hours nwifeinc educ c.exper##c.exper   age kidslt6 kidsge6 , ll(0)\nqui:emargins, dydx(*) estore(m1)\nqui:emargins, dydx(*) predict(p(0,.)) estore(m2)\nqui:emargins, dydx(*) predict(e(0,.)) estore(m3)\nqui:emargins, dydx(*) predict(ystar(0,.)) estore(m4)\nesttab m1 m2 m3 m4, mtitle(Latent P(y&gt;0) E(y|y&gt;0) E(y) ) b(3) se\n\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                   Latent          P(y&gt;0)        E(y|y&gt;0)            E(y)   \n----------------------------------------------------------------------------\nnwifeinc           -8.814*         -0.002*         -3.969*         -5.189*  \n                  (4.459)         (0.001)         (2.008)         (2.621)   \n\neduc               80.645***        0.022***       36.312***       47.473***\n                 (21.583)         (0.006)         (9.703)        (12.621)   \n\nexper              91.929***        0.026***       37.593***       48.793***\n                  (7.997)         (0.002)         (2.966)         (3.587)   \n\nage               -54.405***       -0.015***      -24.497***      -32.026***\n                  (7.418)         (0.002)         (3.362)         (4.292)   \n\nkidslt6          -894.020***       -0.246***     -402.551***     -526.278***\n                (111.878)         (0.028)        (50.749)        (64.706)   \n\nkidsge6           -16.218          -0.004          -7.303          -9.547   \n                 (38.641)         (0.011)        (17.404)        (22.752)   \n----------------------------------------------------------------------------\nN                     753             753             753             753   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/9_ldvm.html#tobit-has-problems-too",
    "href": "rmethods/9_ldvm.html#tobit-has-problems-too",
    "title": "Limited Dependent Variable Models",
    "section": "Tobit has problems too",
    "text": "Tobit has problems too\n\nThat simple equation, too much aggregation\nHayek (in Fear the Boom and Bust)\n\n\nTobit, when addressing corner solutions, aims to explain two different actions (Engagement and intensity) with the same model. However, this may not be appropriate all the time.\n\nHW-Examples?\n\nWhen this happens, other models may be more appropritate like\n\ntwo part model: (literally model using two equations)\nHurdle Model (craggit or churdle)\n\nAlso…Normality…"
  },
  {
    "objectID": "rmethods/9_ldvm.html#censored-regression",
    "href": "rmethods/9_ldvm.html#censored-regression",
    "title": "Limited Dependent Variable Models",
    "section": "Censored Regression",
    "text": "Censored Regression\n\nApplies to the same cases as Tobit model. But, it usually refers to Censoring at other points of the distribution (upper censoring? mixed censoring?)\nFurthermore, applies to cases with different censoring thresholds!\n\nTypical Example, Unemployment duration\n\n\n\nqui:frause recid, clear\ngen lldur = ldurat             // Lower Limit\ngen uudur = ldurat if cens==0  // upper limit = . if censored.\nintreg lldur uudur workprg priors tserved felon alcohol drugs black married educ age\n\n(893 missing values generated)\n\nFitting constant-only model:\n\nIteration 0:   log likelihood = -2188.8689  \nIteration 1:   log likelihood = -1732.7406  \nIteration 2:   log likelihood = -1680.7927  \nIteration 3:   log likelihood =  -1680.427  \nIteration 4:   log likelihood =  -1680.427  \n\nFitting full model:\n\nIteration 0:   log likelihood = -2116.9831  \nIteration 1:   log likelihood = -1639.9495  \nIteration 2:   log likelihood =  -1597.634  \nIteration 3:   log likelihood = -1597.0592  \nIteration 4:   log likelihood =  -1597.059  \nIteration 5:   log likelihood =  -1597.059  \n\nInterval regression                                 Number of obs     =  1,445\n                                                           Uncensored =    552\n                                                        Left-censored =      0\n                                                       Right-censored =    893\n                                                       Interval-cens. =      0\n\n                                                    LR chi2(10)       = 166.74\nLog likelihood = -1597.059                          Prob &gt; chi2       = 0.0000\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     workprg |  -.0625715   .1200369    -0.52   0.602    -.2978396    .1726965\n      priors |  -.1372529   .0214587    -6.40   0.000    -.1793111   -.0951947\n     tserved |  -.0193305   .0029779    -6.49   0.000    -.0251672   -.0134939\n       felon |   .4439947   .1450865     3.06   0.002     .1596303     .728359\n     alcohol |  -.6349092   .1442166    -4.40   0.000    -.9175686   -.3522499\n       drugs |  -.2981602   .1327356    -2.25   0.025    -.5583171   -.0380033\n       black |  -.5427179   .1174428    -4.62   0.000    -.7729014   -.3125343\n     married |   .3406837   .1398431     2.44   0.015     .0665964    .6147711\n        educ |   .0229196   .0253974     0.90   0.367    -.0268584    .0726975\n         age |   .0039103   .0006062     6.45   0.000     .0027221    .0050984\n       _cons |   4.099386    .347535    11.80   0.000      3.41823    4.780542\n-------------+----------------------------------------------------------------\n    /lnsigma |   .5935864   .0344122    17.25   0.000     .5261398     .661033\n-------------+----------------------------------------------------------------\n       sigma |    1.81047   .0623022                      1.692387    1.936792\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods/9_ldvm.html#truncated",
    "href": "rmethods/9_ldvm.html#truncated",
    "title": "Limited Dependent Variable Models",
    "section": "Truncated",
    "text": "Truncated\n\nIf Data is simply not there, as shown before, one needs to adjust Estimates.\nmarginal effects decisions are similar to Tobit\n\n\nfrause mroz, clear\nqui:truncreg hours nwifeinc educ c.exper##c.exper   age kidslt6 kidsge6 , ll(0)\nemargins, dydx(*) estore(m1b)\nemargins, dydx(*) predict(e(0,.)) estore(m2b)\nesttab m1 m1b m3 m2b, mtitle(Lat-Tobit Lat-Trunc E(y&gt;0)-Tobit E(y&gt;0)-Trunc ) b(3) se\n\n\nAverage marginal effects                                   Number of obs = 428\nModel VCE: OIM\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  nwifeinc educ exper age kidslt6 kidsge6\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    nwifeinc |   .1534399   5.164279     0.03   0.976    -9.968361    10.27524\n        educ |  -29.85254   22.83935    -1.31   0.191    -74.61684    14.91176\n       exper |   48.00824   8.578316     5.60   0.000     31.19504    64.82143\n         age |  -27.44381   8.293458    -3.31   0.001    -43.69869   -11.18893\n     kidslt6 |  -484.7109   153.7881    -3.15   0.002      -786.13   -183.2918\n     kidsge6 |  -102.6574   43.54347    -2.36   0.018    -188.0011   -17.31379\n------------------------------------------------------------------------------\n\nAverage marginal effects                                   Number of obs = 428\nModel VCE: OIM\n\nExpression: E(hours|hours&gt;0), predict(e(0,.))\ndy/dx wrt:  nwifeinc educ exper age kidslt6 kidsge6\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    nwifeinc |   .1094149   3.682546     0.03   0.976    -7.108243    7.327072\n        educ |  -21.28723   16.25065    -1.31   0.190    -53.13793    10.56346\n       exper |   32.66986   5.277772     6.19   0.000     22.32562    43.01411\n         age |  -19.56962   5.823226    -3.36   0.001    -30.98293   -8.156303\n     kidslt6 |  -345.6374   107.9599    -3.20   0.001    -557.2349   -134.0399\n     kidsge6 |  -73.20291   30.80594    -2.38   0.017    -133.5814   -12.82438\n------------------------------------------------------------------------------\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                Lat-Tobit       Lat-Trunc    E(y&gt;0)-Tobit    E(y&gt;0)-Trunc   \n----------------------------------------------------------------------------\nnwifeinc           -8.814*          0.153          -3.969*          0.109   \n                  (4.459)         (5.164)         (2.008)         (3.683)   \n\neduc               80.645***      -29.853          36.312***      -21.287   \n                 (21.583)        (22.839)         (9.703)        (16.251)   \n\nexper              91.929***       48.008***       37.593***       32.670***\n                  (7.997)         (8.578)         (2.966)         (5.278)   \n\nage               -54.405***      -27.444***      -24.497***      -19.570***\n                  (7.418)         (8.293)         (3.362)         (5.823)   \n\nkidslt6          -894.020***     -484.711**      -402.551***     -345.637** \n                (111.878)       (153.788)        (50.749)       (107.960)   \n\nkidsge6           -16.218        -102.657*         -7.303         -73.203*  \n                 (38.641)        (43.543)        (17.404)        (30.806)   \n----------------------------------------------------------------------------\nN                     753             428             753             428   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/9_ldvm.html#poisson",
    "href": "rmethods/9_ldvm.html#poisson",
    "title": "Limited Dependent Variable Models",
    "section": "Poisson",
    "text": "Poisson\n\nSome times, Data may be non-negative, and/or countable. OLS works well, but we could do better\nWith Count data, some data transformations (logs) are not possible, because of the zeroes.\nSo instead of assuming \\(y|x \\sim N(\\mu_x,\\sigma)\\), one could assume \\(y|x \\sim poisson(\\mu_x)\\)\n\n\\[P(y=k,\\mu_x) = \\frac{\\mu_x^k e ^{-\\mu_x}}{k!} \\text{ with } \\mu_x=\\exp(x\\beta)\\]\n\nFor a Poisson:\n\n\\(E(y|x) = \\exp{x\\beta}\\) and \\(Var(y|x) = \\exp{x\\beta}\\)\n\nAs hinted before, Count data is heteroskedastic. And Poisson assumes some structure to that."
  },
  {
    "objectID": "rmethods/9_ldvm.html#section-6",
    "href": "rmethods/9_ldvm.html#section-6",
    "title": "Limited Dependent Variable Models",
    "section": "",
    "text": "Also convinient that Poisson models are very easy to interpret! (just like Log-lin models)\nAfter estimation:\n\n\\[\\frac{\\Delta \\% E(y|x)}{\\Delta x} \\simeq \\beta_x \\times 100 \\text{ or } (\\exp \\beta_x-1)\\times 100 \\]\n\nOther points.\n\nThe variance imposed in Poisson is very restrictive. This is a problem for Variance estimation!\nSolution: use Robust Standard Errors!\nLike LRM, poisson is robust to errors when modeling the conditional mean.\nPoisson is a very good alternative for continuous data too (if using Robust SE)\n\nWage models, trade models"
  },
  {
    "objectID": "rmethods/9_ldvm.html#example-1",
    "href": "rmethods/9_ldvm.html#example-1",
    "title": "Limited Dependent Variable Models",
    "section": "Example",
    "text": "Example\n\nfrause crime1, clear\nqui: reg narr86 pcnv avgsen tottime ptime86 qemp86 inc86 black hispan born60\nest sto m1\nqui:poisson narr86 pcnv avgsen tottime ptime86 qemp86 inc86 black hispan born60, robust\nest sto m2\nqui:emargins, dydx(*) estore(m3)\nesttab m1 m2 m3, se b(3) mtitle(LRM Poisson Poisson-mfx) ///\nkeep(pcnv ptime86  qemp86 inc86 black hispan) label varwidth(20) wrap\n\n\n--------------------------------------------------------------------\n                              (1)             (2)             (3)   \n                              LRM         Poisson     Poisson-mfx   \n--------------------------------------------------------------------\nmain                                                                \nproportion of prior        -0.132**        -0.402***       -0.162***\nconvictions               (0.040)         (0.101)         (0.040)   \n\nmos. in prison             -0.041***       -0.099***       -0.040***\nduring 1986               (0.009)         (0.022)         (0.009)   \n\n# quarters employed,       -0.051***       -0.038          -0.015   \n1986                      (0.014)         (0.034)         (0.014)   \n\nlegal income, 1986,        -0.001***       -0.008***       -0.003***\n$100s                     (0.000)         (0.001)         (0.001)   \n\n=1 if black                 0.327***        0.661***        0.267***\n                          (0.045)         (0.099)         (0.042)   \n\n=1 if Hispanic              0.194***        0.500***        0.202***\n                          (0.040)         (0.092)         (0.038)   \n--------------------------------------------------------------------\nObservations                 2725            2725            2725   \n--------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods/9_ldvm.html#other-methods-of-interest",
    "href": "rmethods/9_ldvm.html#other-methods-of-interest",
    "title": "Limited Dependent Variable Models",
    "section": "Other Methods of interest",
    "text": "Other Methods of interest\n\nMLE opens the door to other methods that may be more approriate to analyze data\nThey may even be able to handle otherwise unsolvable data problems.\n\nologit, oprobit: Ordered qualitative variables\nmlogit, mprobit: Unordered Qualitative variables\nheckman: Endogenous Sample Selection\nfractional regression model: When the depvariable is an index\netc etc\n\nWorth knowing, but not for the exam!"
  },
  {
    "objectID": "rmethods/homework_2.html",
    "href": "rmethods/homework_2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Similar to HW1, propose a concise research project. Specify the dependent variable of interest, along with control variables.\nThe model specification must include both continuous and discrete variables in your model.\nDescribe your economic model and its corresponding econometric model, accompanied by a succinct description of your anticipated findings."
  },
  {
    "objectID": "rmethods/homework_2.html#note",
    "href": "rmethods/homework_2.html#note",
    "title": "Homework 2",
    "section": "Note",
    "text": "Note\nExplore all datasets available in frause and utilize the data from these datasets to determine the variables for analysis and the controls to incorporate into your homework.\nThe examples within the textbook can serve as valuable guidelines for your considerations here.\nYou have the freedom to explore other sources. If you do so, please include the data alongside your homework submission.\nExceptionally unique responses (distinguished by their thoroughness, detail, innovation, and presentation) may merit extra points."
  },
  {
    "objectID": "rmethods/index.html",
    "href": "rmethods/index.html",
    "title": "Research Methods: Econometrics I",
    "section": "",
    "text": "Class syllabus is here.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods: Econometrics I"
    ]
  },
  {
    "objectID": "rmethods/index.html#syllabus",
    "href": "rmethods/index.html#syllabus",
    "title": "Research Methods: Econometrics I",
    "section": "",
    "text": "Class syllabus is here.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods: Econometrics I"
    ]
  },
  {
    "objectID": "rmethods/index.html#zoom-link",
    "href": "rmethods/index.html#zoom-link",
    "title": "Research Methods: Econometrics I",
    "section": "Zoom Link",
    "text": "Zoom Link\nWhen there is need to attend a class online, and you have requested to attend online, use the following link:\nZoom class",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods: Econometrics I"
    ]
  },
  {
    "objectID": "rmethods/index.html#introduction",
    "href": "rmethods/index.html#introduction",
    "title": "Research Methods: Econometrics I",
    "section": "Introduction",
    "text": "Introduction\n\nChapter 1\nOnce Upon a time, why Econometrics?:",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods: Econometrics I"
    ]
  },
  {
    "objectID": "rmethods/index.html#part-i-basic-tools",
    "href": "rmethods/index.html#part-i-basic-tools",
    "title": "Research Methods: Econometrics I",
    "section": "Part I: Basic Tools",
    "text": "Part I: Basic Tools\n\n1. Chapter 2:\nThe Simple Regression Model\n\n\n2. Chapter 3:\nMultiple Regression Analysis: Estimation\n\n\n3. Chapter 4 - 5\nMRA: Inference and Asymptotics\n\n\nHomeWork 1 Also Github link\n\n\nDue Date September 27",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods: Econometrics I"
    ]
  },
  {
    "objectID": "rmethods/index.html#part-ii-addressing-problems-with-mra",
    "href": "rmethods/index.html#part-ii-addressing-problems-with-mra",
    "title": "Research Methods: Econometrics I",
    "section": "Part II: Addressing Problems with MRA",
    "text": "Part II: Addressing Problems with MRA\n\n4. Chapter 6 - 7\nMRA: Scaling, functional forms, Goodness of Fit, and Qualitative Information\n\n\n5. Chapter 8\nHeteroskedasticity\n\n\n6. Chapter 9\nFurther Problems: Functional form, Missing variables, measurement errors, missing data\n\n\nHomeWork 2 Also Github link\n\n\nDue Date October 25 !!\n\n\nReview Class: Wednesday October 18\n\n\nMIDTERM! October 20: 9:30 - 12:50\n\n7. Chapter 15\nInstrumental Variables and 2SLS\n\n\n8. Chapter 17\nLimited Dep Variables",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods: Econometrics I"
    ]
  },
  {
    "objectID": "rmethods/index.html#part-iii-panel-data-methods",
    "href": "rmethods/index.html#part-iii-panel-data-methods",
    "title": "Research Methods: Econometrics I",
    "section": "Part III: Panel Data Methods",
    "text": "Part III: Panel Data Methods\n\n9. Chapter 13\nPool Cross Section and Panel Data\n\n\n10. Chapter 14\nAdvanced Panel Data Methods\n\n\n[HomeWork 3] Github\n\n\nDue Date November 29 at 1pm",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods: Econometrics I"
    ]
  },
  {
    "objectID": "rmethods/index.html#part-iv-time-series",
    "href": "rmethods/index.html#part-iv-time-series",
    "title": "Research Methods: Econometrics I",
    "section": "Part IV: Time Series",
    "text": "Part IV: Time Series\n\n11. Chapter 10 - 11\nBasics of Regression analysis with TSD\n\n\n12. Chapter 12 & 18\nAdvanced TSD Problems\n\n\nFinal\n\nPossible Date December 13",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods: Econometrics I"
    ]
  },
  {
    "objectID": "rmethods/Question 1.html",
    "href": "rmethods/Question 1.html",
    "title": "Econometrics MSC Levy",
    "section": "",
    "text": "Question 1\nconsider the following matrixes\n\\(A=\\begin{pmatrix} 1 & 2 & 3 \\\\\n3 & 3 & 8\n\\end{pmatrix}\\)\n\\(B=\\begin{pmatrix} 3 & 1 & 6 \\\\\n2 & 1 & 2\n\\end{pmatrix}\\)\n\\(C=\\begin{pmatrix} 5 & 1 & 6 \\\\\n1 & 1 & 2 \\\\\n4 & 2 & 6\n\\end{pmatrix}\\)\nConsider the following Operations:\n\\(A + B\\)\n\\(A + B'\\)\n\\(A * B\\)\n\\(Det(C)\\)\n\\(C^{-1}\\)\n\\(A' * B\\)\n\\((A * B')^{-1}\\)\nIndicate if they are valid operations. if not, explain why If they are obtain the resulting matrices.\nQuestion 2\nConsider the following functions:\n\\(y=1+x^2-3x^3\\)\n\\(y=ln(3x^2+1)\\)\n\\(y=exp(x+x*z)\\)\nFor each one estimate: \\(\\frac{\\partial y}{∂ x}\\) and \\(\\frac{\\partial^2 y}{∂ x^2}\\)\nQuestion 3 Solve the following set of equations. If not, indicate why they cant be solved:\nEq1\n\\(\\begin{aligned}\n2x+y  &=  3 \\\\\n2x+2y &= -3\n\\end{aligned}\\)\nEq 2\n\\(\\begin{aligned}\n2x+ y + z &= 1 \\\\\n3x+ 2y - z &= 2 \\\\\nx+ y - 2 z &= 3 \\\\\n\\end{aligned}\\)\nEq 3\n\\(x^2 + x - 2 =0\\)\nQuestion 4: Maximization\n\nConsider the following set of functions:\n\n\\(\\begin{aligned}\ny &= A  x^α z ^\\beta \\\\\nC &= p_x x + P_z z\n\\end{aligned}\\)\nSet up the constrained maximization problem.\n\nconsider the following function:\n\n\\(y = f(z)\\)\nIf you were to maximize this function, what are the first order conditions?\nIf there is a unique solution, what are the conditions to verify it is the maximum.\nQuestion 5\nP1)\nConsider the following set of numbers\n\\(x = \\{10, 4, 5, 7, 3, 6, 9\\}\\)\nEstimate the mean, median, and variance of this data.\np2)\nSay that X is a continuous random variable that ranges from -10 to 10.\n\nWhat is the probability that X=1 (\\(P(x=1)\\))\nTrue or False \\(P(X&gt;1) + p(X&lt;1)=1\\)\nTrue or False \\(P(X&gt;1) + p(-1&lt;X&lt;2)\\) can be larger than 1.\n\nP3)\nSay that Z1 and Z2 are two random variables that follow a bernulli distribution. Assume that the following probabilities are true:\n\np(z1=0 & z2=0) = 0.1\np(z1=1 & z2=1) = 0.2\np(z1=0) = 0.5\np(z2=1) = 0.6\n\nWhat is the probability for:\n\nProbability Z1=1 conditional on z2=0\nunconditional probability of Z1=1\n\np4. Given two random variables x_1 and x_2, with means and variances \\(\\mu_1\\), \\(\\mu_2\\) and \\(\\sigma^2_1\\) and \\(\\sigma^2_2\\).\nif z = x_1 - x_2, what is the mean and variance of Z\nwhat is the covariance between z and x_1."
  },
  {
    "objectID": "rmethods/Syllabus.html",
    "href": "rmethods/Syllabus.html",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "",
    "text": "Instructor: Fernando Rios-Avila\nOffice: Room 307, Blithewood\nOffice Hours: Friday, 9:00 am to 10:00 am or by appointment\nPhone: 845-758-7719\nEmail: friosavi@levy.org\nTime and Location: Wednesday, 9:30 am to 12:50 pm"
  },
  {
    "objectID": "rmethods/Syllabus.html#course-description",
    "href": "rmethods/Syllabus.html#course-description",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Course Description",
    "text": "Course Description\nThe course aims to provide students with a foundation in applied econometrics that is required for the successful completion of the program. The emphasis of the course is on understanding the intuition behind model estimation, hypothesis testing, and economic interpretation of statistical results.\nWe begin by discussing the nature of econometrics and economic data. This is followed by a discussion of estimation and inference in univariate and multivariate regression models of cross-sectional data. We will review some of the consequences of heteroscedasticity, measurement errors, and endogeneity, among other issues of model specification and data measures. The second part of the course will cover advanced regression models such as limited dependent variables, panel data, and time series data.\nThe class is taught through a combination of lectures, discussion, homework, quizzes, and exams. Student involvement and participation in class are highly encouraged."
  },
  {
    "objectID": "rmethods/Syllabus.html#required-text",
    "href": "rmethods/Syllabus.html#required-text",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Required Text",
    "text": "Required Text\nIntroductory Econometrics: A Modern Approach\nby Jeffrey M. Wooldridge\n\nWhile any edition will do, exercises and exam material will be taken from the 7th edition.\n\nSuggested:\nUsing R for Introductory Econometrics (recommended)\nby Florian Heiss\n\nThis book introduces the free programming language and software package R with a focus on the implementation of standard tools and methods used in econometrics. It builds on the textbook “Introductory Econometrics: A Modern Approach” by Jeffrey M. Wooldridge. The book can be accessed online here.\nThe author also provides textbooks using the same structure, introducing Julia and python."
  },
  {
    "objectID": "rmethods/Syllabus.html#course-website",
    "href": "rmethods/Syllabus.html#course-website",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Course website",
    "text": "Course website\nWe will be using Github pages to provide all lectures and homework. It is your responsibility to access the site or communicate with me if any questions should arise.\nThis is where assignments, readings, and other information will be posted."
  },
  {
    "objectID": "rmethods/Syllabus.html#grading",
    "href": "rmethods/Syllabus.html#grading",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Grading",
    "text": "Grading\nGrading will be based on homework assignments, quizzes, and exams. Their grade distribution is as follows:\n\nHomework (55%): Three homework assignments will be given throughout the semester. Homework assignments are prepared for you to implement the methodologies covered in class, as well as encourage you to interpret the results. Each homework will consist of a small research project where you will be asked to answer a series of questions, as if you were writing a research paper.\nThe homework assignments will require using data and the statistical software Stata. You are free to use any of the data sets that come along with the textbook. They can be accessed using frause in Stata. It is encouraged that homework assignments are prepared and submitted in pairs.\nAll homework assignments will be posted on the course website. When submitting your homework, prepare a pdf, html, or doc file with your answers, and a do file with the code used to answer the questions. It is highly encouraged that you use markdown or quarto to prepare your homework, as it easily allows you to incorporate all necessary information to reproduce your results.\nMidterm and Final (40%): Two exams will be given. Each one is prepared to test you on concepts, interpretation, and intuition behind the econometric topics reviewed in class. The exams will be open book and open notes. You are allowed to use any printed material, including the textbook, your notes, and any other material you may find useful. You are not allowed to use any electronic devices, including computers, tablets, or phones, except for the use of a calculator.\nQuestions for the midterm and final will include three sections:\n\nMultiple-choice questions and concept questions.\nAnalytical section, equation solving, and derivations.\nEmpirical section, where you will be asked to interpret results from a regression analysis, as well as implement statistical tests.\n\nAnalytical and empirical sections will be taken from the problem sets and computational exercises in the textbook.\nQuizzes (5%): After each topic, there will be multiple-choice quizzes to test your knowledge of important concepts and ideas seen in class. There will also be open-ended questions or extra projects that will be provided during the semester. This includes 5 extra credit points.\nClass Participation: Class participation is highly encouraged. You are to participate in class discussions. You are also encouraged to ask questions and provide answers to questions asked in class. This counts for up to 5% of extra credit for your final grade."
  },
  {
    "objectID": "rmethods/Syllabus.html#attendance",
    "href": "rmethods/Syllabus.html#attendance",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Attendance:",
    "text": "Attendance:\nClass attendance, in-person or online, is highly recommended. Classes will not be recorded, but for exceptional cases, a link will be provided to attend the class online. Material for exams and homework will come from both class lectures as well as the book.\nThe only acceptable excuses for missing a test are medical reasons or family emergencies. If you have a legitimate excuse, a make-up exam will be issued soon after the date of the original exam. Any issues should be discussed with me before the actual exam takes place."
  },
  {
    "objectID": "rmethods/Syllabus.html#course-software",
    "href": "rmethods/Syllabus.html#course-software",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Course Software",
    "text": "Course Software\nThere are several statistical packages for analyzing data. In this course, we will be using the software Stata to cover all materials in class. Slides are self-replicable, thus you can copy and paste almost all code provided to replicate the results seen in class. The Institute will be providing you with licenses for Stata/BE for the length of the course.\nStata offers many free short webinars and video tutorials that may be useful if you never used Stata before, or even if you have some experience with it. Please see the resources page for more information.\nIf you decide to, you can use R, Julia, or Python to study and work on the course materials and homework. One of the recommended books has nice introductions and code that can help you get started with these software packages. The resources page has additional information on how to get started with these software packages.\nAs with many other skills, the best way to learn is to simply work with the packages, work on the book exercises, and ask any questions to me or your classmates when you find a problem you could not find a solution for."
  },
  {
    "objectID": "rmethods/Syllabus.html#additional-information",
    "href": "rmethods/Syllabus.html#additional-information",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Additional Information:",
    "text": "Additional Information:\nAll students are responsible for knowing Bard’s Policy on Academic Honesty as published in Bard College Student Handbook."
  },
  {
    "objectID": "rmethods/Syllabus.html#syllabus",
    "href": "rmethods/Syllabus.html#syllabus",
    "title": "ECON 529 - Research Methods I: Econometrics",
    "section": "Syllabus:",
    "text": "Syllabus:\n\nIntroduction: What is Econometrics?\n\nPart I: Basic tools\n\nThe Simple Regression Model\nMultiple Regression Analysis: Estimation\nMRA: Inference and Asymptotics\n\nPart II: Addressing Problems with MRA\n\nMRA: Scaling, functional forms, Goodness of Fit, and Qualitative Information\nHeteroskedasticity\nFurther Problems: Functional form, Missing variables, measurement errors, missing data\nInstrumental Variables and 2SLS\nLimited Dep Variables\n\nPart III: Panel Data Methods\n\nPool Cross Section and Panel Data\nAdvanced Panel Data Methods\n\nPart IV: Time Series\n\nBasics of Regression analysis with time series data\nAdvanced TSD Problems"
  },
  {
    "objectID": "rmethods2/HomeWork1.html",
    "href": "rmethods2/HomeWork1.html",
    "title": "Homework I",
    "section": "",
    "text": "Consider the following IO table for a hypothetical economy:\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgriculture\nManufacture\nServices\nConstruction\nFinal Demand\nTotal Output\n\n\n\n\nAgriculture\n160\n230\n260\n290\n340\n\n\n\nManufacture\n210\n190\n450\n170\n340\n\n\n\nServices\n410\n380\n200\n160\n350\n\n\n\nConstruction\n180\n320\n240\n170\n280\n\n\n\nLabor\n320\n240\n350\n400\n\n\n\n\n\n\nCalculate total output by Industry\nProvide the Matrix of technical coefficients and labor coefficients for this Economy\nAssume that Final demand has shifted. There is a 30% increase in demand in Construction, but with a 10% decline in demand for services and Manufacturing. Estimate the changes expected in total ouput for all Sectors, as well as the changes in Labor Inputs."
  },
  {
    "objectID": "rmethods2/HomeWork1.html#part-i-io-tables",
    "href": "rmethods2/HomeWork1.html#part-i-io-tables",
    "title": "Homework I",
    "section": "",
    "text": "Consider the following IO table for a hypothetical economy:\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgriculture\nManufacture\nServices\nConstruction\nFinal Demand\nTotal Output\n\n\n\n\nAgriculture\n160\n230\n260\n290\n340\n\n\n\nManufacture\n210\n190\n450\n170\n340\n\n\n\nServices\n410\n380\n200\n160\n350\n\n\n\nConstruction\n180\n320\n240\n170\n280\n\n\n\nLabor\n320\n240\n350\n400\n\n\n\n\n\n\nCalculate total output by Industry\nProvide the Matrix of technical coefficients and labor coefficients for this Economy\nAssume that Final demand has shifted. There is a 30% increase in demand in Construction, but with a 10% decline in demand for services and Manufacturing. Estimate the changes expected in total ouput for all Sectors, as well as the changes in Labor Inputs."
  },
  {
    "objectID": "rmethods2/HomeWork1.html#sec-part2",
    "href": "rmethods2/HomeWork1.html#sec-part2",
    "title": "Homework I",
    "section": "Part II: MLE",
    "text": "Part II: MLE\nConsider data from the American Time use Survey for 2019 atus_2019.dta. This data contains aggregates on various time use activities for 9K individuals. Because this is survey data, be mindful of the sampling weights. You can use either wt06 or wtfinal as the sampling weight.\n\nAt Levy, Household production activities are typically classified as\n\nCore: Main activities taking care of the household\nProc: Procurement, shopping, and other activities related to the household production\nacare and ccare: Activities related to the care of children and other adults in the household\n\nIn the dataset, these variables contain information on hours spend on these activities per day.\nwith this in mind, what is the average time spent on total household production activities per day? when weighted and when unweighted? why are they different?\nEstimate the average time spend on Total household production between weekends and weekdays (use variable wkend_wkday) Are they statistically different?\nHours of Household production have a large share of zeros (about 11% in the data). Because of this, using a simple Linear model may not be appropriate. Instead estimate a Tobit model and Poisson model using individual and household characteristics (plus others of your choice). Discuss why you choose to control for these variables, and intepret the results.\n\nFor the tobit model answer, is this a problem of corner solution or censoring? How would this affect the estimation of marginal effects?"
  },
  {
    "objectID": "rmethods2/HomeWork1.html#part-iii-inequality-gaps",
    "href": "rmethods2/HomeWork1.html#part-iii-inequality-gaps",
    "title": "Homework I",
    "section": "Part III: Inequality Gaps",
    "text": "Part III: Inequality Gaps\nThe GINI index is commonly used to measure income or wealth inequality. However, you could also use the GINI index to measure inequality in other variables.\n\nProduce a table that decomposes the GINI of total hours of household production by source. That is Core, Procurement, child care and adult care.\nWhich one is the component with the greatest share of household production.\nWhich component shows the greatest concentration?\nWhat is the greatest contributor to overall inequality?"
  },
  {
    "objectID": "rmethods2/HomeWork1.html#part-iv-explaining-gaps",
    "href": "rmethods2/HomeWork1.html#part-iv-explaining-gaps",
    "title": "Homework I",
    "section": "Part IV: Explaining Gaps",
    "text": "Part IV: Explaining Gaps\n\nConsidering the methodology known as Oaxaca-Blinder decomposition. Using this methodology, analyze the gender gap on household production using a similar model specification as you did in Part II. Discuss the results.\n\nInclude the use of weights.\nFor better understanding of the gaps, include summary statistics and model coefficients for the relevant regressions and variables."
  },
  {
    "objectID": "rmethods2/index.html",
    "href": "rmethods2/index.html",
    "title": "Research Methods II",
    "section": "",
    "text": "If you are reading this, you are probably enrolled in the course. This page will contain information about the course, including the syllabus, assignments, and other relevant information.\nAs the course progresses, I will add links to the syllabus, assignments, and other relevant information.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods II"
    ]
  },
  {
    "objectID": "rmethods2/index.html#syllabus",
    "href": "rmethods2/index.html#syllabus",
    "title": "Research Methods II",
    "section": "",
    "text": "If you are reading this, you are probably enrolled in the course. This page will contain information about the course, including the syllabus, assignments, and other relevant information.\nAs the course progresses, I will add links to the syllabus, assignments, and other relevant information.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods II"
    ]
  },
  {
    "objectID": "rmethods2/index.html#sessions",
    "href": "rmethods2/index.html#sessions",
    "title": "Research Methods II",
    "section": "Sessions",
    "text": "Sessions\nSession 1: Surveys, IO and SAM\nSlides: html and pdf\nReadings: link\nSession 2: MLE & Limited Dependent Variables\nSlides: html and pdf\nReadings: link\nSession 3: Measuring Inequality\nSlides: html and pdf\nReadings: link\nSession 4: Analyzing Gaps\nSlides: html and pdf\nReadings: link",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods II"
    ]
  },
  {
    "objectID": "rmethods2/index.html#homework-1",
    "href": "rmethods2/index.html#homework-1",
    "title": "Research Methods II",
    "section": "Homework 1",
    "text": "Homework 1\nThe first homework is due on TBD. You can find the instructions here.\nSession 5: Significance and Missing Data\nSlides: html and pdf\nReadings: link\nSession 6: Imputation: Statistical Matching\nSlides: html and pdf\nReadings: link\nSession 7: Micro-Simulations, and Monte Carlo Methods\nSlides: html and pdf\nReadings: link",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods II"
    ]
  },
  {
    "objectID": "rmethods2/index.html#homework-2",
    "href": "rmethods2/index.html#homework-2",
    "title": "Research Methods II",
    "section": "Homework 2",
    "text": "Homework 2\nThe Second homework is due on TBD. You can find the instructions here.",
    "crumbs": [
      "Home",
      "Courses",
      "Research Methods II"
    ]
  },
  {
    "objectID": "rmethods2/session_2.html",
    "href": "rmethods2/session_2.html",
    "title": "Research Methods II",
    "section": "",
    "text": "This is something we have seen before.\nMLE is a method to estimate parameters of a model.\n\nIt can be used to estimate paramaters of linear and nonlinear models\n\nThe idea is to find the values of the parameters that maximize the likelihood function.\n\nBut what does it mean?\n\n\n\nThe likelihood function is the probability of observing the data given the parameters of the model.\n\nSo, MLE tries to maximize that probability, under the assumption that we know the distribution of the data.\nIn other words, we try to identify distributions! (not only conditional mean functions)"
  },
  {
    "objectID": "rmethods2/session_2.html#introduction-to-maximum-likelihood-estimation",
    "href": "rmethods2/session_2.html#introduction-to-maximum-likelihood-estimation",
    "title": "Research Methods II",
    "section": "",
    "text": "This is something we have seen before.\nMLE is a method to estimate parameters of a model.\n\nIt can be used to estimate paramaters of linear and nonlinear models\n\nThe idea is to find the values of the parameters that maximize the likelihood function.\n\nBut what does it mean?\n\n\n\nThe likelihood function is the probability of observing the data given the parameters of the model.\n\nSo, MLE tries to maximize that probability, under the assumption that we know the distribution of the data.\nIn other words, we try to identify distributions! (not only conditional mean functions)"
  },
  {
    "objectID": "rmethods2/session_2.html#data",
    "href": "rmethods2/session_2.html#data",
    "title": "Research Methods II",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "rmethods2/session_2.html#mle-estimation",
    "href": "rmethods2/session_2.html#mle-estimation",
    "title": "Research Methods II",
    "section": "MLE estimation",
    "text": "MLE estimation\n\nTo identify the parameters of the model, we need to impose assumptions about the distribution of the data.\nFor simplicitly, lets make the assumption that the data is normally distributed.\nThe likelihood function for a single observation is:\n\n\\[L_i(\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2}\\left(\\frac{y_i-\\mu}{\\sigma}\\right)^2 }\\]\n\nAnd under independent observations assumptions, the Likelihood function for the sample is:\n\n\\[LL(\\mu,\\sigma) = \\prod_{i=1}^n L_i(\\mu,\\sigma)\\]"
  },
  {
    "objectID": "rmethods2/session_2.html#graphical-representation",
    "href": "rmethods2/session_2.html#graphical-representation",
    "title": "Research Methods II",
    "section": "Graphical representation",
    "text": "Graphical representation"
  },
  {
    "objectID": "rmethods2/session_2.html#how-good-we-did",
    "href": "rmethods2/session_2.html#how-good-we-did",
    "title": "Research Methods II",
    "section": "How Good we did?",
    "text": "How Good we did?"
  },
  {
    "objectID": "rmethods2/session_2.html#section",
    "href": "rmethods2/session_2.html#section",
    "title": "Research Methods II",
    "section": "",
    "text": "Under that assumption, the likelihood function for a single observation is:\n\n\\[L_i(\\beta,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2}\\left(\\frac{y_i-x_i'\\beta}{\\sigma}\\right)^2 }\\]\n\nWhich can be used to construct the MLE estimator for OLS.\nPlot-twist: The MLE estimator for OLS is the same as the OLS estimator."
  },
  {
    "objectID": "rmethods2/session_2.html#limited-dependent-variables-1",
    "href": "rmethods2/session_2.html#limited-dependent-variables-1",
    "title": "Research Methods II",
    "section": "Limited Dependent Variables",
    "text": "Limited Dependent Variables\n\nLimited dependent variables are variables that are limited in their range of values.\n\nFor example, binary variables, or variables that are bounded between 0 and 1.\nOr variables that are bounded between 0 and some positive number.\nOr variables bounded to take only positive values.\netc\n\nVery Special Case: Endogenous Sample Selection\n\nLooks unbounded, but you only observe a subset of the population."
  },
  {
    "objectID": "rmethods2/session_2.html#binary-data-lpmprobitlogit",
    "href": "rmethods2/session_2.html#binary-data-lpmprobitlogit",
    "title": "Research Methods II",
    "section": "Binary Data: LPM/Probit/logit",
    "text": "Binary Data: LPM/Probit/logit\n\nProbit and logit are two models that are used to model binary dependent variables. (Dummies)\n\nYou can also use OLS (LPM), but has drawbacks\nYou also need to make sure your Dependent Variable is binary!\n\nWhen your Dep variable is binary, your goal is determine the probability of observing a 1 (success) of something to happen given a set of covariates.\n\n\\[P(y_i=1|x_i) = G(x_i'\\beta)\\]\nThe choice of \\(G\\) is what makes the difference between LPM, a probit and logit."
  },
  {
    "objectID": "rmethods2/session_2.html#probitlogit",
    "href": "rmethods2/session_2.html#probitlogit",
    "title": "Research Methods II",
    "section": "Probit/Logit",
    "text": "Probit/Logit\n\n\nLOGIT\n\\[G(Z) = \\frac{e^{Z}}{1+e^{Z}} = \\Lambda(Z)\\]\n\nProbit\n\\[G(Z) = \\int_{-\\infty}^z \\phi(v) dv = \\Phi(Z)\\]\n\n\n\nBoth make sure that \\(0\\leq G(Z) \\leq 1\\), which doesnt happen with LPM (\\(G(Z)=Z\\))\nAnd with this, we can use MLE to estimate the parameters of the model.\n\n\\[LL(\\beta) = \\prod_{i=1}^n G(x_i'\\beta)^{y_i} (1-G(x_i'\\beta))^{1-y_i}\\]"
  },
  {
    "objectID": "rmethods2/session_2.html#section-1",
    "href": "rmethods2/session_2.html#section-1",
    "title": "Research Methods II",
    "section": "",
    "text": "One could also think of the probit and logit as a transformation of a latent variable \\(Y^*\\).\n\\[Y^*_i = x_i'\\beta + \\epsilon_i\\]\n\nThe latent variable is not observed. However, when \\(Y^*_i&gt;0\\), we observe \\(Y_i=1\\).\n\nHere the probabilty of observing a \\(Y_i=1\\) is:\n\\[\\begin{aligned}\nP(y_i=1|x_i) &= P(y^*_i&gt;0|x_i) = P(x_i'\\beta + \\epsilon_i&gt;0|x_i) \\\\\n  &= P( \\epsilon_i&gt;-x_i'\\beta |x_i) = 1-P( \\epsilon_i&lt;-x_i'\\beta |x_i) \\\\\n  &= 1-G(-x_i'\\beta)\n\\end{aligned}\n\\]\nAnd if \\(G'\\) is symetrical (logit/probit/lpm):\n\\[P(y_i=1|x_i) = G(x_i'\\beta)\\]"
  },
  {
    "objectID": "rmethods2/session_2.html#marginal-effects-and-testing",
    "href": "rmethods2/session_2.html#marginal-effects-and-testing",
    "title": "Research Methods II",
    "section": "Marginal Effects and testing",
    "text": "Marginal Effects and testing\n\nLPM estimates can be interpreted Directly as the change in P(y=1|X)\nFor Logit and probit, we need to compute the marginal effects.\n\n\\[P(y_i=1|x_i) = G(x_i'\\beta)\\]\n\\[\\frac{\\partial P(y_i=1|x_i)}{\\partial x_{ij}} = g(x_i'\\beta)\\beta_j\\]\n\nFor testing,\n\nYou can use the t-test (or z-test for logit/probit) for coefficients or marginal effects\nOr use LR test for joint significance of a set of coefficients.\n\n\n\\[LR = 1- 2 (LL_ur - LL_r) \\sim \\chi^2_q\\]"
  },
  {
    "objectID": "rmethods2/session_2.html#example-stata",
    "href": "rmethods2/session_2.html#example-stata",
    "title": "Research Methods II",
    "section": "Example Stata",
    "text": "Example Stata\nLoad the data\n\nwebuse nhanes2d, clear\ndes highbp height weight age female\nsum   highbp height weight age female i.race [w=finalwgt]\n\n\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nhighbp          byte    %8.0g               * High blood pressure\nheight          float   %9.0g                 Height (cm)\nweight          float   %9.0g                 Weight (kg)\nage             byte    %9.0g                 Age (years)\nfemale          byte    %8.0g      female     Female\n\n    Variable |     Obs      Weight        Mean   Std. dev.       Min        Max\n-------------+-----------------------------------------------------------------\n      highbp |  10,351   117157513    .3685423   .4824328          0          1\n      height |  10,351   117157513    168.4599   9.699111      135.5        200\n      weight |  10,351   117157513    71.90064   15.43281      30.84     175.88\n         age |  10,351   117157513    42.25264   15.50249         20         74\n      female |  10,351   117157513    .5206498   .4995975          0          1\n-------------+-----------------------------------------------------------------\n             |\n        race |\n      White  |  10,351   117157513    .8791545   .3259634          0          1\n      Black  |  10,351   117157513    .0955059   .2939267          0          1\n      Other  |  10,351   117157513    .0253396   .1571621          0          1\n\n\nEstimate mode: LPM using weights\n\nreg highbp height weight age female i.race [pw=finalwgt]\n* or svy: reg highbp height weight age female i.race \n\n(sum of wgt is 117,157,513)\n\nLinear regression                               Number of obs     =     10,351\n                                                F(6, 10344)       =     531.97\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.2110\n                                                Root MSE          =     .42864\n\n------------------------------------------------------------------------------\n             |               Robust\n      highbp | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      height |   -.006271   .0008179    -7.67   0.000    -.0078742   -.0046677\n      weight |   .0097675    .000369    26.47   0.000     .0090441    .0104909\n         age |   .0096675   .0002984    32.40   0.000     .0090826    .0102524\n      female |  -.0794025   .0142293    -5.58   0.000    -.1072948   -.0515103\n             |\n        race |\n      Black  |   .0647166   .0170488     3.80   0.000     .0312977    .0981355\n      Other  |   .0869917   .0381158     2.28   0.022     .0122775     .161706\n             |\n       _cons |    .347133   .1403729     2.47   0.013     .0719749     .622291\n------------------------------------------------------------------------------\n\n\nEstimate mode: Logit using weights\n\nlogit highbp height weight age female i.race [pw=finalwgt]\n* or svy: reg highbp height weight age female i.race \n\n\nIteration 0:  Log pseudolikelihood =  -77110184  \nIteration 1:  Log pseudolikelihood =  -63830529  \nIteration 2:  Log pseudolikelihood =  -63604963  \nIteration 3:  Log pseudolikelihood =  -63604252  \nIteration 4:  Log pseudolikelihood =  -63604252  \n\nLogistic regression                                    Number of obs =  10,351\n                                                       Wald chi2(6)  = 1473.91\n                                                       Prob &gt; chi2   =  0.0000\nLog pseudolikelihood = -63604252                       Pseudo R2     =  0.1752\n\n------------------------------------------------------------------------------\n             |               Robust\n      highbp | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      height |  -.0328739   .0045382    -7.24   0.000    -.0417687   -.0239791\n      weight |   .0514503   .0022181    23.20   0.000     .0471028    .0557977\n         age |   .0496323   .0017152    28.94   0.000     .0462706     .052994\n      female |  -.4472131   .0777753    -5.75   0.000    -.5996498   -.2947764\n             |\n        race |\n      Black  |    .351346   .0915423     3.84   0.000     .1719264    .5307656\n      Other  |   .4929785   .1961652     2.51   0.012     .1085017    .8774552\n             |\n       _cons |  -.7501284   .7683899    -0.98   0.329    -2.256145    .7558881\n------------------------------------------------------------------------------\n\n\nJoint significance test\n\ntest 2.race 3.race\n\n\n ( 1)  [highbp]2.race = 0\n ( 2)  [highbp]3.race = 0\n\n           chi2(  2) =   20.25\n         Prob &gt; chi2 =    0.0000\n\n\nMarginal Effects: You need to use margins command\n\nmargins, dydx(*)\n\n\nAverage marginal effects                                Number of obs = 10,351\nModel VCE: Robust\n\nExpression: Pr(highbp), predict()\ndy/dx wrt:  height weight age female 2.race 3.race\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      height |  -.0059971   .0008145    -7.36   0.000    -.0075936   -.0044007\n      weight |    .009386   .0003507    26.76   0.000     .0086985    .0100734\n         age |   .0090543   .0002562    35.34   0.000     .0085522    .0095564\n      female |  -.0815842   .0141075    -5.78   0.000    -.1092344   -.0539339\n             |\n        race |\n      Black  |   .0654291   .0173285     3.78   0.000     .0314659    .0993923\n      Other  |   .0925816     .03772     2.45   0.014     .0186517    .1665115\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n\n\nPredicted Probabilities\n\npredict pr_hat\nhistogram pr_hat  \ngraph export s2fig2.png, replace width(1000)\n\nFrom here, we could also predict HighBP\n\ngen dpr_hat = pr_hat&gt;.5\ntab dpr_hat highbp [w=finalwgt]\n\n\n           |  High blood pressure\n   dpr_hat |         0          1 |     Total\n-----------+----------------------+----------\n         0 |  61819680   20494538 |  82314218 \n         1 |  12160331   22682964 |  34843295 \n-----------+----------------------+----------\n     Total |  73980011   43177502 | 117157513"
  },
  {
    "objectID": "rmethods2/session_2.html#tobit-1",
    "href": "rmethods2/session_2.html#tobit-1",
    "title": "Research Methods II",
    "section": "Tobit",
    "text": "Tobit\n\nTobit models are to analyze data with censored information.\nCensored data means that the data is there… but you dont know the exact value.\nFor example, if you have data on income, but you only know that some people earn less than 10K, but you dont know how much less.\nThe fact that you can see the data, even if you do not know the exact value, helps you to estimate the parameters of the model."
  },
  {
    "objectID": "rmethods2/session_2.html#visualizing-the-problem",
    "href": "rmethods2/session_2.html#visualizing-the-problem",
    "title": "Research Methods II",
    "section": "Visualizing the problem",
    "text": "Visualizing the problem"
  },
  {
    "objectID": "rmethods2/session_2.html#visualizing-the-problem-1",
    "href": "rmethods2/session_2.html#visualizing-the-problem-1",
    "title": "Research Methods II",
    "section": "Visualizing the problem",
    "text": "Visualizing the problem"
  },
  {
    "objectID": "rmethods2/session_2.html#tobit-model",
    "href": "rmethods2/session_2.html#tobit-model",
    "title": "Research Methods II",
    "section": "Tobit Model",
    "text": "Tobit Model\n\nThe idea of the Tobit model is “model” not only why \\(y\\) changes when \\(X\\) changes, but also why y is censored.\n\nAlthough you do that with the same parameters, under normality assumptions.\n\nWhen the data is censored, the likelihood function is similar to a probit model, when the data is not censored, the likelihood function is similar to a linear model:\n\n\\[\\begin{aligned}\nL_i(\\beta,\\sigma) &= \\Phi\\left(\\frac{y^c-x_i'\\beta}{\\sigma}\\right) \\text{if } y_i = y^c \\\\\nL_i(\\beta,\\sigma) &= \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2}\\left(\\frac{y_i-x_i'\\beta}{\\sigma}\\right)^2 } \\text{if } y_i &gt; y^c\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_2.html#estimation-stata",
    "href": "rmethods2/session_2.html#estimation-stata",
    "title": "Research Methods II",
    "section": "Estimation Stata",
    "text": "Estimation Stata\nIn Stata, you can estimate a Tobit model using the tobit command.\ntobit y x1 x2 x3, ll(#)\n\ny: dependent variable\nx1 x2 x3: independent variable\nll(#): is the value of the censoring point."
  },
  {
    "objectID": "rmethods2/session_2.html#visualizing-the-solution",
    "href": "rmethods2/session_2.html#visualizing-the-solution",
    "title": "Research Methods II",
    "section": "Visualizing the solution",
    "text": "Visualizing the solution"
  },
  {
    "objectID": "rmethods2/session_2.html#latent-variable",
    "href": "rmethods2/session_2.html#latent-variable",
    "title": "Research Methods II",
    "section": "Latent Variable",
    "text": "Latent Variable\n\nThe easiest to interpret is the latent variable.\nFor example, say that you are interested in the effect of education on wages, but wages are censored at 10.\nIn this case the coefficients of the Tobit model are the same as the coefficients of the linear model.\n\ntobit y x, ll(0)\n\nuse margins if you have interactions or polynomial terms."
  },
  {
    "objectID": "rmethods2/session_2.html#data-is-corner-solution",
    "href": "rmethods2/session_2.html#data-is-corner-solution",
    "title": "Research Methods II",
    "section": "Data is Corner Solution:",
    "text": "Data is Corner Solution:\n\nIf data is corner solution, then you need to decide what to interpret.\n\nFor example, say you are interested in the effect of education hours of work\nHours of work cannot fall below 0.\nBut you know education has a positive effect (on something)\n\nWould you be interested in the effect on the probability of working?\nThe effect on hours of work for those who work?\nThe overall average effect on hours of work? (some will enter the labor force, some will work more hours)"
  },
  {
    "objectID": "rmethods2/session_2.html#probability-of-working",
    "href": "rmethods2/session_2.html#probability-of-working",
    "title": "Research Methods II",
    "section": "Probability of Working",
    "text": "Probability of Working\n\\[P(y_i&gt;0|x_i) = \\Phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right)\\]\nmargins, dydx(x) predict(pr(0,.))\n\npredict(pr(0,.)) says you are interested in the probability that data was not censored…or in this case that was not a corner solution"
  },
  {
    "objectID": "rmethods2/session_2.html#eyy0x",
    "href": "rmethods2/session_2.html#eyy0x",
    "title": "Research Methods II",
    "section": "E(Y|Y>0,X)",
    "text": "E(Y|Y&gt;0,X)\n\\[\\begin{aligned}\ny_i &= x_i'\\beta + \\epsilon_i \\ || E(|y&gt;0,X) \\\\\nE(y_i|y_i&gt;0,x_i) &= x_i'\\beta + \\sigma\\lambda\\left(\\frac{x_i'\\beta}{\\sigma}\\right) \\\\\n\\lambda(z) &= \\frac{\\phi(z)}{\\Phi(z)}\n\\end{aligned}\n\\]\nThis is the expected value of the latent variable, conditional on the latent variable being positive.\nmargins, dydx(x) predict(e(0,.))\n\npredict(e(0,.)) says you are interested in the expected change only for those who currently work."
  },
  {
    "objectID": "rmethods2/session_2.html#eyx",
    "href": "rmethods2/session_2.html#eyx",
    "title": "Research Methods II",
    "section": "E(Y|X)",
    "text": "E(Y|X)\n\\[\\begin{aligned}\nE(y_i|x_i) &= E(y_i|y_i&gt;0,x_i) * P(y_i&gt;0|x_i) + 0 * (1-P(y_i&gt;0|x_i)) \\\\\nE(y_i|x_i) &= \\Phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right)\n\\left( x_i'\\beta + \\sigma\\lambda\\left(\\frac{x_i'\\beta}{\\sigma}\\right)\\right) \\\\\nE(y_i|x_i) &= \\Phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right) x_i'\\beta + \\sigma \\phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right)\n\\end{aligned}\n\\]\nmargins, dydx(x) predict(ystar(0,.))\n\npredict(ystar(0,.)) says you are interested in the average effect considering those who work and those who do not work."
  },
  {
    "objectID": "rmethods2/session_2.html#visualizing-the-solution-1",
    "href": "rmethods2/session_2.html#visualizing-the-solution-1",
    "title": "Research Methods II",
    "section": "Visualizing the solution",
    "text": "Visualizing the solution"
  },
  {
    "objectID": "rmethods2/session_2.html#exogenous-sample-selection",
    "href": "rmethods2/session_2.html#exogenous-sample-selection",
    "title": "Research Methods II",
    "section": "Exogenous Sample Selection",
    "text": "Exogenous Sample Selection\n\nFirst: Samples already represent a selection of the population.\n\nhowever, because the selection is random, all assumptions of OLS are satisfied. (if they are true for the population.)\n\nSecond: Some times selection may not be random, but based on observed (and control) characteristics\n\nNot a problem either. Since you could at least say something for those you observe. (if you have the right variables)\nThis was exogenous sample selection.\n\n\nie: You want to estimate the effect of education on wages, but you only have data for highly educated people."
  },
  {
    "objectID": "rmethods2/session_2.html#endogenous-sample-selection",
    "href": "rmethods2/session_2.html#endogenous-sample-selection",
    "title": "Research Methods II",
    "section": "Endogenous Sample Selection",
    "text": "Endogenous Sample Selection\n\nThird: Selection may be based on unobserved characteristics.\n\nThis is a problem. The reason why we do not observe data is for unknown reasons (part of the error).\nBecause we cannot control for it, it will bias our estimates. (like omitted variable bias)\nThis is endogenous sample selection.\n\n\nie: - You want to estimate the effect of education on wages, but you only have data for those who work. - Those who work do so because they may have been offer higher wages for unknown reasons. (high skill? high motivation? )"
  },
  {
    "objectID": "rmethods2/session_2.html#heckman-selection-model",
    "href": "rmethods2/session_2.html#heckman-selection-model",
    "title": "Research Methods II",
    "section": "Heckman Selection Model",
    "text": "Heckman Selection Model\n\nThe Heckman selection model is an estimation method that allows you to correct a specific kind of endogenous sample selection.\nConsider the following model:\n\n\\[y_i = x_i'\\beta + \\epsilon_i\\]\n\nIn absence of selection, we can assume standard assumtions and estimate the model using OLS.\nHowever, if we have endogenous selection, usually means that we have a second equation that determines the selection.\n\n\\[s_i^* = z_i'\\gamma + \\eta_i\\]"
  },
  {
    "objectID": "rmethods2/session_2.html#section-4",
    "href": "rmethods2/session_2.html#section-4",
    "title": "Research Methods II",
    "section": "",
    "text": "The Full model:\n\\[\\begin{aligned}\ny_i &= x_i'\\beta + \\epsilon_i \\text{ if } s_i^*&gt;0 \\\\\ns_i^* &= z_i'\\gamma + \\eta_i \\\\\n\\epsilon &\\sim N(0,\\sigma_\\epsilon) \\\\\n\\eta &\\sim N(0,1) \\\\\ncorr(\\epsilon,\\eta) &= \\rho\n\\end{aligned}\n\\]\n\n\\(x_i\\) and \\(z_i\\) are not necessarily the same. But they are exogenous to the error terms.\nThe selection equation depends on observable \\(z_i\\) and unobservable \\(\\eta_i\\) factors.\nThe unobserable factors are correlated with the error term of the main equation.\nThe problem: if \\(\\rho\\neq 0\\) then \\(E(\\epsilon|s_i^*&gt;0,x_i) \\neq 0\\)."
  },
  {
    "objectID": "rmethods2/session_2.html#solution",
    "href": "rmethods2/session_2.html#solution",
    "title": "Research Methods II",
    "section": "Solution",
    "text": "Solution\n\nThe solution is to “control” for the unobserved factors that are correlated with \\(\\epsilon\\).\n\n\\[\\begin{aligned}\ny_i &= x_i'\\beta + \\epsilon_i \\ || \\ E( * |x_i,s_i^*&gt;0) \\\\\nE(y_i|x_i,s_i^*&gt;0) &= x_i'\\beta + E(\\epsilon_i|x,z,\\eta, s_i^*&gt;0) \\\\\n&= x_i'\\beta + E(\\epsilon_i|\\eta,s_i^*&gt;0) \\\\\n  &= x_i'\\beta + \\rho \\frac{\\phi(z_i'\\gamma)}{\\Phi(z_i'\\gamma)} \\\\\n  &= x_i'\\beta + \\rho \\lambda (z_i'\\gamma) \\\\\n\\end{aligned}\n\\]\nThus the new model is:\n\\[y_i  = x_i'\\beta + \\rho \\lambda (z_i'\\gamma) + \\varepsilon_i\\]\nwhere \\(\\gamma\\) is estimated using a probit model."
  },
  {
    "objectID": "rmethods2/session_2.html#implementation",
    "href": "rmethods2/session_2.html#implementation",
    "title": "Research Methods II",
    "section": "Implementation",
    "text": "Implementation\n\nTwo options:\n\nEstimate both outcome and selection equation jointly using MLE.\n\n\nRequires careful setup of the likelihood function.\nImposes the assumption of joint normality of the error terms.\n\n\nEstimate it using a two-step procedure.(Heckit)\n\n\nEstimate the selection equation using probit. \\(z_i'\\gamma\\)\nEstimate the outcome equation using OLS, inclusing inverse mills ratio. \\(\\lambda (z_i'\\gamma)\\)\nStd Errs need to be corrected\n\nConsideration: In contrast with IV, Heckman does not require an instrument, but having one is highly recommended."
  },
  {
    "objectID": "rmethods2/session_2.html#example-stata-1",
    "href": "rmethods2/session_2.html#example-stata-1",
    "title": "Research Methods II",
    "section": "Example Stata",
    "text": "Example Stata\nLets start by loading some data\n\nwebuse womenwk, clear\ndescribe\n\n\nContains data from https://www.stata-press.com/data/r18/womenwk.dta\n Observations:         2,000                  \n    Variables:             6                  3 Mar 2022 07:43\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\ncounty          byte    %9.0g                 County of residence\nage             byte    %8.0g                 Age in years\neducation       byte    %8.0g                 Years of schooling\nmarried         byte    %8.0g                 1 if married spouse present\nchildren        byte    %8.0g                 # of children under 12 years old\nwage            float   %9.0g                 Hourly wage; missing, if not\n                                                working\n-------------------------------------------------------------------------------\nSorted by: \n\n\nIn Stata, we can use command heckman to estimate the Heckman selection model, but lets start by doing this manually\n\ngen works = (wage!=.)\n** Selection model\nprobit works married children educ age\npredict zg, xb\n\n\nIteration 0:  Log likelihood = -1266.2225  \nIteration 1:  Log likelihood = -1031.4962  \nIteration 2:  Log likelihood = -1027.0625  \nIteration 3:  Log likelihood = -1027.0616  \nIteration 4:  Log likelihood = -1027.0616  \n\nProbit regression                                       Number of obs =  2,000\n                                                        LR chi2(4)    = 478.32\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -1027.0616                             Pseudo R2     = 0.1889\n\n------------------------------------------------------------------------------\n       works | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     married |   .4308575    .074208     5.81   0.000     .2854125    .5763025\n    children |   .4473249   .0287417    15.56   0.000     .3909922    .5036576\n   education |   .0583645   .0109742     5.32   0.000     .0368555    .0798735\n         age |   .0347211   .0042293     8.21   0.000     .0264318    .0430105\n       _cons |  -2.467365   .1925635   -12.81   0.000    -2.844782   -2.089948\n------------------------------------------------------------------------------\n\n\nThis selection equation can be interpreted the usual way\nThe outcome model:\n\ngen mill = normalden(zg)/normal(zg)\nreg  wage educ age mill\nest sto hkit\n\n\n      Source |       SS           df       MS      Number of obs   =     1,343\n-------------+----------------------------------   F(3, 1339)      =    173.01\n       Model |  14904.6806         3  4968.22688   Prob &gt; F        =    0.0000\n    Residual |   38450.214     1,339  28.7156191   R-squared       =    0.2793\n-------------+----------------------------------   Adj R-squared   =    0.2777\n       Total |  53354.8946     1,342  39.7577456   Root MSE        =    5.3587\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   education |   .9825259   .0504982    19.46   0.000     .8834616     1.08159\n         age |   .2118695   .0206636    10.25   0.000      .171333     .252406\n        mill |   4.001616   .5771027     6.93   0.000     2.869492    5.133739\n       _cons |   .7340391   1.166214     0.63   0.529    -1.553766    3.021844\n------------------------------------------------------------------------------\n\n\nLets compare the outcome with Stata’s Heckman\n\nset linesize 255\nreg wag educ age\nest sto ols\nheckman wage educ age, select(works = married children educ age) twostep\nest sto hecktwo\nheckman wage educ age, select(works = married children educ age) \nest sto heckmle\n\n\nesttab ols hkit hecktwo heckmle, se(%9.3f) b(%9.3f) star(* 0.10 ** 0.05 *** 0.01) nogaps mtitle(OLS Heckit Hkm-two Hkm-mle) nonum\n\n\n----------------------------------------------------------------------------\n                      OLS          Heckit         Hkm-two         Hkm-mle   \n----------------------------------------------------------------------------\nmain                                                                        \neducation           0.897***        0.983***        0.983***        0.990***\n                  (0.050)         (0.050)         (0.054)         (0.053)   \nage                 0.147***        0.212***        0.212***        0.213***\n                  (0.019)         (0.021)         (0.022)         (0.021)   \nmill                                4.002***                                \n                                  (0.577)                                   \n_cons               6.085***        0.734           0.734           0.486   \n                  (0.890)         (1.166)         (1.248)         (1.077)   \n----------------------------------------------------------------------------\nworks                                                                       \nmarried                                             0.431***        0.445***\n                                                  (0.074)         (0.067)   \nchildren                                            0.447***        0.439***\n                                                  (0.029)         (0.028)   \neducation                                           0.058***        0.056***\n                                                  (0.011)         (0.011)   \nage                                                 0.035***        0.037***\n                                                  (0.004)         (0.004)   \n_cons                                              -2.467***       -2.491***\n                                                  (0.193)         (0.189)   \n----------------------------------------------------------------------------\n/mills                                                                      \nlambda                                              4.002***                \n                                                  (0.607)                   \n----------------------------------------------------------------------------\n/                                                                           \nathrho                                                              0.874***\n                                                                  (0.101)   \nlnsigma                                                             1.793***\n                                                                  (0.028)   \n----------------------------------------------------------------------------\nN                    1343            1343            2000            2000   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.10, ** p&lt;0.05, *** p&lt;0.01"
  },
  {
    "objectID": "rmethods2/session_2.html#interpretation",
    "href": "rmethods2/session_2.html#interpretation",
    "title": "Research Methods II",
    "section": "Interpretation",
    "text": "Interpretation\n\nIt depends…\nbut the most likely scenario is to interpret the outcomes for everyone (thus just look at coefficients of the outcome equation)\nBut you can also obtain effects for those who work, or the average effect.\nThe Mills ratio can be interpreted as the direction of the selection.\n\nIf positive, then those who work are those who earn more\nIf negative, then those who work are those who earn less"
  },
  {
    "objectID": "rmethods2/session_2.html#extra-example",
    "href": "rmethods2/session_2.html#extra-example",
    "title": "Research Methods II",
    "section": "Extra example",
    "text": "Extra example\n\nfrause oaxaca, clear\nreg lnwage educ exper tenure \nest sto ols\nheckman lnwage educ exper tenure  , select(lfp =educ age   married divorced kids6 kids714) \nest sto hk_mle\nheckman lnwage educ exper tenure  , select(lfp =educ age   married divorced kids6 kids714) two\nest sto hk_two\n\n\nesttab ols hk_mle hk_two, se(%9.3f) b(%9.3f) star(* 0.10 ** 0.05 *** 0.01) nogaps mtitle(OLS Heckit Hkm-two) nonum  \n\n\n------------------------------------------------------------\n                      OLS          Heckit         Hkm-two   \n------------------------------------------------------------\nmain                                                        \neduc                0.087***        0.091***        0.094***\n                  (0.005)         (0.005)         (0.006)   \nexper               0.011***        0.011***        0.010***\n                  (0.002)         (0.002)         (0.002)   \ntenure              0.008***        0.008***        0.007***\n                  (0.002)         (0.002)         (0.002)   \n_cons               2.140***        2.079***        2.024***\n                  (0.065)         (0.067)         (0.072)   \n------------------------------------------------------------\nlfp                                                         \neduc                                0.168***        0.183***\n                                  (0.025)         (0.025)   \nage                                -0.028***       -0.029***\n                                  (0.006)         (0.006)   \nmarried                            -0.853***       -0.832***\n                                  (0.191)         (0.185)   \ndivorced                           -0.324          -0.239   \n                                  (0.229)         (0.222)   \nkids6                              -0.590***       -0.573***\n                                  (0.069)         (0.070)   \nkids714                            -0.318***       -0.307***\n                                  (0.057)         (0.058)   \n_cons                               1.454***        1.313***\n                                  (0.336)         (0.355)   \n------------------------------------------------------------\n/                                                           \nathrho                              0.343***                \n                                  (0.082)                   \nlnsigma                            -0.750***                \n                                  (0.020)                   \n------------------------------------------------------------\n/mills                                                      \nlambda                                              0.293***\n                                                  (0.066)   \n------------------------------------------------------------\nN                    1434            1647            1647   \n------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.10, ** p&lt;0.05, *** p&lt;0.01"
  },
  {
    "objectID": "rmethods2/session_4.html",
    "href": "rmethods2/session_4.html",
    "title": "Research Methods II",
    "section": "",
    "text": "What do we mean by decomposition?\n\n\nIn Economics, decomposition refers to the process breaking down an index or aggregate measure into factors that explain it.\nWe have done some of this last week with Decomposition by groups and sources.\nBut there are other types of decompositions that are useful in Economics.\n\n\\[\n\\begin{aligned}\ny &=A L^\\alpha K ^\\beta \\\\\n\\frac{\\Delta y}{y} &=\\frac{\\Delta A}{A} + \\alpha \\frac{\\Delta L}{L} + \\beta \\frac{\\Delta K}{K}\n\\end{aligned}\n\\]\n\nToday we will focus on a different kind of Decomposition: Decomposition of gaps."
  },
  {
    "objectID": "rmethods2/session_4.html#introduction",
    "href": "rmethods2/session_4.html#introduction",
    "title": "Research Methods II",
    "section": "",
    "text": "What do we mean by decomposition?\n\n\nIn Economics, decomposition refers to the process breaking down an index or aggregate measure into factors that explain it.\nWe have done some of this last week with Decomposition by groups and sources.\nBut there are other types of decompositions that are useful in Economics.\n\n\\[\n\\begin{aligned}\ny &=A L^\\alpha K ^\\beta \\\\\n\\frac{\\Delta y}{y} &=\\frac{\\Delta A}{A} + \\alpha \\frac{\\Delta L}{L} + \\beta \\frac{\\Delta K}{K}\n\\end{aligned}\n\\]\n\nToday we will focus on a different kind of Decomposition: Decomposition of gaps."
  },
  {
    "objectID": "rmethods2/session_4.html#section",
    "href": "rmethods2/session_4.html#section",
    "title": "Research Methods II",
    "section": "",
    "text": "Introduction to Oaxaca-Blinder Decomposition\n\nThe most common type of decomposition of gaps is the Oaxaca-Blinder Decomposition.\nThe idea behind is as follows:\n\nThere are two groups you are interested in comparing.\nGroup A has a higher average value of a variable of interest than Group B.\n\nWhat explains the difference??\n\nThe Oaxaca-Blinder Decomposition allows you to answer this question.\n\nThe difference could be due to differences in the characteristics\nOr it could be due to differences in the returns to those characteristics."
  },
  {
    "objectID": "rmethods2/session_4.html#section-1",
    "href": "rmethods2/session_4.html#section-1",
    "title": "Research Methods II",
    "section": "",
    "text": "Introduction to Oaxaca-Blinder Decomposition\n\nIn 1973 Oaxaca and Blinder independently proposed a very similar method to decompose the differences in averages value of a variable of interest between two groups.\n\n\n“Male-Female Wage Differentials in Urban Labor Markets” Oaxaca (IER 1973)\n\n\n“Wage Discrimination: Reduced Form and Structural Estimates” Blinder (JHR 1973)\n\n\nWhich become the basis for what is known as the Oaxaca-Blinder Decomposition.\nHeavily used in Labor Economics, it can be helpful to explain what factors relate to: Union premiums, povery gaps, gender wage gaps, etc."
  },
  {
    "objectID": "rmethods2/session_4.html#section-2",
    "href": "rmethods2/session_4.html#section-2",
    "title": "Research Methods II",
    "section": "",
    "text": "General Framework\n\nSuppose we have two groups: \\(A\\) and \\(B\\), with Data generating processes (DGP) that are defined as:\n\n\\[\\begin{aligned}\nY_a = G_a(X_a,\\epsilon_a) \\\\\nY_b = G_b(X_b,\\epsilon_b) \\\\\n\\end{aligned}\n\\]\n\nDifferences between two groups could be explain by:\n\nDifferences in observed characteristics \\(X_a\\) and \\(X_b\\)\nDifferences in unobserved \\(\\epsilon_a\\) and \\(\\epsilon_b\\)\nDifferences in the Funcional forms \\(G_a\\) and \\(G_b\\)\n\nTo some extent, this suggests something akin to a counterfactual."
  },
  {
    "objectID": "rmethods2/session_4.html#section-3",
    "href": "rmethods2/session_4.html#section-3",
    "title": "Research Methods II",
    "section": "",
    "text": "Imposing Restrictions\nTo implement OB, we need to impose some restrictions on the model.\n1st Functional form: Linear in parameters \\[G_a(X_k,\\epsilon_k) = X_k \\beta_k + \\epsilon_k\n\\]\n2nd Errors are independent by group: \\[\\varepsilon_i \\perp D | X_i\n\\]\nThis makes it possible to have other problems in the model (like endogeneity) and still get aggregate consistent estimates. (but lets assume Zero Conditional Mean )\n3rd Homoskedastic (across groups)\n4th We only care about Differences in means"
  },
  {
    "objectID": "rmethods2/session_4.html#section-4",
    "href": "rmethods2/session_4.html#section-4",
    "title": "Research Methods II",
    "section": "",
    "text": "OB In Action\n\nSuppose the models are given by:\n\n\\[Y_k = X_k \\beta_k + \\epsilon_k\n\\]\n\nThen, the “average” outcome for each group is given by:\n\n\\[\\bar Y_k =\\bar X_k \\beta_k\n\\]\n\nThis is useful, because we could now use it for creating a counterfactual.\n\n\\(\\bar X_a \\beta_a\\) is the average wage of group A\n\\(\\bar X_b \\beta_a\\) is the average wage of group B if “paid like” group A.\n\nor Average Wages of Group A if they had the characteristics of Group B.\n\n\nWith this, we can now obtain the OB Decomposition."
  },
  {
    "objectID": "rmethods2/session_4.html#section-5",
    "href": "rmethods2/session_4.html#section-5",
    "title": "Research Methods II",
    "section": "",
    "text": "\\[\\begin{aligned}\n\\Delta \\bar Y &= \\bar Y_a - \\bar Y_b \\\\\n&= \\bar X_a \\beta_a - \\bar X_b \\beta_b \\\\\n\\end{aligned}\n\\]\n\nNow we need a Counterfactual: What if Group A were paid as of Group B? \\(\\bar X_a \\beta_b\\)\n\n\\[\\begin{aligned}\n\\Delta \\bar Y &= \\bar Y_a - \\bar Y_b + \\bar Y^c_a - \\bar Y^c_a\\\\\n&= \\color{blue}{\\bar X_a \\beta_a} - \\color{red}{\\bar X_b \\beta_b + \\bar X_a \\beta_b} - \\color{blue}{\\bar X_a \\beta_b} \\\\\n&= \\color{blue}{(\\bar X_a \\beta_a- \\bar X_a \\beta_b)}  + \\color{red}{ (\\bar X_a \\beta_b - \\bar X_b \\beta_b)} \\\\\n&= \\color{blue}{\\bar X_a (\\beta_a- \\beta_b)}  + \\color{red}{  (\\bar X_a - \\bar X_b) \\beta_b} \\\\\n&= \\color{blue}{\\bar X_a \\Delta \\beta}  + \\color{red}{  \\Delta \\bar X \\beta_b} \\\\\n\\end{aligned}\n\\]\nThus Differences in averages is decomposed into two parts:\n\nDiff in \\(X's\\) and (weighted by \\(\\beta_b\\))\nDiff in \\(\\beta's\\). (weighted by \\(\\bar X_a\\))"
  },
  {
    "objectID": "rmethods2/session_4.html#section-7",
    "href": "rmethods2/session_4.html#section-7",
    "title": "Research Methods II",
    "section": "",
    "text": "OB: From Aggregate to detailed\n\nThe previous decomposition is for Aggregates.\n\nThey are robust to endogeneity, (if endogeneity is the same across groups)\n\nIf the model is correctly specified, we can also decompose the differences in the detailed level.\n\n\\[\\begin{aligned}\n\\Delta X \\beta_b &= \\beta_{a0}-\\beta_{b0} + \\sum_{j=1}^k \\bar X_{aj}(\\beta_{aj} - \\beta_{bj}) \\\\\n\\bar X_a \\Delta \\beta &= \\sum_{j=1}^k (\\bar X_{aj} - \\bar X_{bj}) \\beta_{bj}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_4.html#section-8",
    "href": "rmethods2/session_4.html#section-8",
    "title": "Research Methods II",
    "section": "",
    "text": "Note:\n\nThe decomposition is not unique.\n\nDepends on the “counterfactual” we choose.\n\nThe decomposition is not causal, but may be indicative of potential causes.\nThe decomposition is not a test of discrimination.\n\nAs it does not assess why the differences in \\(\\beta\\) exist, nor what explains the differences in \\(\\bar X\\)."
  },
  {
    "objectID": "rmethods2/session_4.html#section-9",
    "href": "rmethods2/session_4.html#section-9",
    "title": "Research Methods II",
    "section": "",
    "text": "In a picture\n\nOption 1Option 2note\n\n\n\n\n\n\n\n\n\nNeither option is “correct” or “incorrect”.\nThey are just different ways of measuring the gaps.\nHowever, you may want to consider which one is more appropriate for your research question.\nOr consider other decomposition options\n\nSingle Ref group with 3-way Decomposition"
  },
  {
    "objectID": "rmethods2/session_4.html#section-10",
    "href": "rmethods2/session_4.html#section-10",
    "title": "Research Methods II",
    "section": "",
    "text": "3-way Decomposition\n\nOption 1Option 2note\n\n\n\n\n\n\n\n\n\nMathematically:\n\n\\[\\begin{aligned}\n\\Delta \\bar Y &= \\bar Y_a - \\bar Y_b \\\\\n&= \\bar X_a \\beta_a - \\bar X_b \\beta_b \\\\\n&= \\bar X_a \\Delta \\beta  +   \\Delta \\bar X \\beta_b +   \\Delta \\bar X \\beta_a - \\Delta \\bar X \\beta_a \\\\\n&= {\\bar X_a \\Delta \\beta}  + \\Delta \\bar X \\beta_a - \\Delta \\bar X \\Delta \\beta \\\\\n\\text{ or } \\\\\n&= \\bar X_a \\Delta \\beta  +   \\Delta \\bar X \\beta_b +   \\bar X_b \\Delta \\beta - \\bar X_b \\Delta \\beta  \\\\\n&= \\bar X_b \\Delta \\beta + \\Delta \\bar X \\beta_b + \\Delta X \\Delta \\beta\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_4.html#example",
    "href": "rmethods2/session_4.html#example",
    "title": "Research Methods II",
    "section": "Example:",
    "text": "Example:\n\nfrause oaxaca, clear\ndrop if lnwage == .\n\n\n\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n\n\nFirst things first:\n\nDefine your groups of interest: Men vs Women\nIdentify your model of interest: Simple Specification\nAnd obtain Summary Statistics\n\n\ntabstat lnwage educ exper age married , by(female)\n\n\nSummary statistics: Mean\nGroup variable: female (sex of respondent (1=female))\n\n  female |    lnwage      educ     exper       age   married\n---------+--------------------------------------------------\n       0 |  3.440222  11.81425  14.07684  38.43142  .5219707\n       1 |  3.266761  11.23206  12.13769  39.28697  .4128843\n---------+--------------------------------------------------\n   Total |  3.357604  11.53696  13.15324  38.83891  .4700139\n------------------------------------------------------------\n\n\n\n\nCode\ngen cns = 1\nqui:mean educ exper age married cns if female == 0\nmatrix x_men = e(b)\nqui:mean educ exper age married cns if female == 1\nmatrix x_women = e(b)\n\n\nSecond, estimate models of interest\n\nqui: reg lnwage educ exper age married if female==0\nest sto men\nmatrix b_men = e(b)\nqui: reg lnwage educ exper age married if female==1\nmatrix b_women = e(b)\nest sto women\nesttab men women, nogaps se mtitle(\"Men\" \"Women\")\n\n\n--------------------------------------------\n                      (1)             (2)   \n                      Men           Women   \n--------------------------------------------\neduc               0.0540***       0.0853***\n                (0.00595)       (0.00871)   \nexper            -0.00495*        0.00776*  \n                (0.00197)       (0.00346)   \nage                0.0216***      0.00808** \n                (0.00202)       (0.00271)   \nmarried             0.173***       -0.121** \n                 (0.0293)        (0.0421)   \n_cons               1.950***        1.947***\n                 (0.0757)         (0.124)   \n--------------------------------------------\nN                     751             683   \n--------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n\n\nmatrix DX = x_men - x_women \nmatrix DB = b_men - b_women\n\nmatrix DX_bw = DX * b_women'\nmatrix Xm_Db = x_men * DB'\n\nmatrix DX_bm = DX * b_men'\nmatrix Xw_Db = x_women * DB'\n\nmatrix dDX_bw = vecdiag(DX' * b_women)'\nmatrix dXm_Db = vecdiag(x_men' * DB)'\n\nmatrix dDX_bm = vecdiag(DX' * b_men)'\nmatrix dXw_Db = vecdiag(x_women' * DB)'\n\n** Total Gap Returns\nmatrix result = DX_bw + Xm_Db, DX_bw, Xm_Db, DX_bm, Xw_Db\nmatrix result =result\\ dDX_bw + dXm_Db, dDX_bw, dXm_Db, dDX_bm, dXw_Db\nmatrix colname result = DY  DX_bw Xm_Db DX_bm  Xw_Db\nmatrix coleq result = \"\" op1 op1 op2 op2\n\n\nmatrix list result, format(%9.4f)\n\n\nresult[6,5]\n                      op1:     op1:     op2:     op2:\n              DY    DX_bw    Xm_Db    DX_bm    Xw_Db\n     y1   0.1735   0.0446   0.1289   0.0222   0.1513\n   educ  -0.3192   0.0496  -0.3689   0.0315  -0.3507\n  exper  -0.1638   0.0150  -0.1789  -0.0096  -0.1542\n    age   0.5141  -0.0069   0.5210  -0.0185   0.5326\nmarried   0.1401  -0.0132   0.1533   0.0189   0.1213\n  _cons   0.0023   0.0000   0.0023   0.0000   0.0023\n\n\n\nNegative numbers “Contract” the gap,\nPositive, “Expand” the gap."
  },
  {
    "objectID": "rmethods2/session_4.html#the-oaxaca-way",
    "href": "rmethods2/session_4.html#the-oaxaca-way",
    "title": "Research Methods II",
    "section": "The oaxaca way",
    "text": "The oaxaca way\n\nssc install oaxaca\nqui:oaxaca lnwage educ exper age married, by(female) w(0)  \nest sto m1\nqui:oaxaca lnwage educ exper age married, by(female) w(1)  \nest sto m2\nesttab m1 m2, wide mtitle(bw_Xm Xm_bw)\n\nchecking oaxaca consistency and verifying not already installed...\nall files already exist and are up to date.\n\n----------------------------------------------------------------------\n                      (1)                          (2)                \n                    bw_Xm                        Xm_bw                \n----------------------------------------------------------------------\noverall                                                               \ngroup_1             3.440***     (196.70)        3.440***     (196.70)\ngroup_2             3.267***     (149.41)        3.267***     (149.41)\ndifference          0.173***       (6.20)        0.173***       (6.20)\nexplained          0.0446*         (2.50)       0.0222          (1.28)\nunexplained         0.129***       (4.63)        0.151***       (5.69)\n----------------------------------------------------------------------\nexplained                                                             \neduc               0.0496***       (4.15)       0.0315***       (4.09)\nexper              0.0150          (1.92)     -0.00960*        (-2.08)\nage              -0.00691         (-1.32)      -0.0185         (-1.46)\nmarried           -0.0132*        (-2.36)       0.0189***       (3.40)\n----------------------------------------------------------------------\nunexplained                                                           \neduc               -0.369**       (-2.96)       -0.351**       (-2.96)\nexper              -0.179**       (-3.17)       -0.154**       (-3.18)\nage                 0.521***       (4.00)        0.533***       (4.00)\nmarried             0.153***       (5.62)        0.121***       (5.54)\n_cons             0.00234          (0.02)      0.00234          (0.02)\n----------------------------------------------------------------------\nN                    1434                         1434                \n----------------------------------------------------------------------\nt statistics in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods2/session_4.html#choosing-the-counterfactual",
    "href": "rmethods2/session_4.html#choosing-the-counterfactual",
    "title": "Research Methods II",
    "section": "Choosing the Counterfactual",
    "text": "Choosing the Counterfactual\n\nWhat should be a good counterfactual?\n\nThe one that is most relevant to your research question.\nThe one that is most likely to be true.\nThe one that is not affected by discrimination.\n\nWhile most of the time, decomposition results do not change much respect to the counterfactual, some times they do.\nWhat to do?"
  },
  {
    "objectID": "rmethods2/session_4.html#section-11",
    "href": "rmethods2/session_4.html#section-11",
    "title": "Research Methods II",
    "section": "",
    "text": "Choosing the Counterfactual\n\nDefault counterfactual is to use predict wages for group A, using Wage structure of group B.\nSome times it may make more sense choosing something in between:\n\n\\[\\beta_c = \\omega \\beta_a + (1-\\omega) \\beta_b\n\\]\nThis is the meaning of w() in Oaxaca\n\nSome times, you may want to use \\(\\beta's\\) from a pool model (omega option in Oaxaca)"
  },
  {
    "objectID": "rmethods2/session_4.html#ob-cheat-sheet",
    "href": "rmethods2/session_4.html#ob-cheat-sheet",
    "title": "Research Methods II",
    "section": "OB Cheat Sheet",
    "text": "OB Cheat Sheet\n\nStata Implementation: oaxaca by Jann (2008)\nTypes of Decompositions\n\nTrifold Decomposition: oaxaca y x1 x2 x3 x4, by(group)\nStandard Decompositions: oaxaca y x1 x2 x3 x4, by(group) w(0)  [w(1) ]\nReimiers (1983) Decomposition: oaxaca y x1 x2 x3 x4, by(group) w(0.5)\nCotton(1983) Decomposition: oaxaca y x1 x2 x3 x4, by(group) w(#=Share)\nOaxaca Ransom (1988,1994) and Neumark (1988) Decomposition: oaxaca y x1 x2 x3 x4, by(group) omega\nCain (1986): oaxaca y x1 x2 x3 x4, by(group) pool"
  },
  {
    "objectID": "rmethods2/session_4.html#beyond-microdata---a-note",
    "href": "rmethods2/session_4.html#beyond-microdata---a-note",
    "title": "Research Methods II",
    "section": "Beyond Microdata - a note",
    "text": "Beyond Microdata - a note\n\nThe principles of OB decomposition can also be applied to other types of data, including Macro Data.\nConsider: Between 1990 and 2000 poverty rates fell from 20 to 10%.\nWhat factors explain this change?\n\nComposition changes (Populations with lower poverty rates grew faster)\nPovery rates within groups (Poverty rates within groups fell)\n\n\n\\[\\begin{aligned}\nP_t - P_s &= \\sum_{j=1}^K w_{jt} P_{jt} - \\sum_{j=1}^K w_{js} P_{js} \\\\\n\\Delta P &= \\sum_{j=1}^K w_{jt} ( P_{jt} -  P_{js}) + \\sum_{j=1}^K (w_{jt}-w_{js}) P_{js}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_4.html#ob-decomposition-extensions-1",
    "href": "rmethods2/session_4.html#ob-decomposition-extensions-1",
    "title": "Research Methods II",
    "section": "OB Decomposition: Extensions",
    "text": "OB Decomposition: Extensions\n\nOB Decompositions have two drawbacks\n\nIts meant to analyze differences in average differences\nIt uses differences in mean characteristics\nIts based on a linear model (OLS)\nStrong assumptions on error terms\n\nThere are various extensions (discussed in Firpo, Fortin, Lemieux (2010)) that can be used to address some of this limitations."
  },
  {
    "objectID": "rmethods2/session_4.html#linearity-assumption",
    "href": "rmethods2/session_4.html#linearity-assumption",
    "title": "Research Methods II",
    "section": "Linearity Assumption",
    "text": "Linearity Assumption\n\nBarsky at al (2002) and Dinardo Fortin and Limeux (1996)\nA strong assumption behind OB is that the model is linear in parameters.\n\nThis is important because the decomposition assumes we can make good “extrapolations” of the model.\nThis is a problem of model misspecification. (what to do?)\n\nOne option is to improve model specification\n\nConsider using quadratic terms, interactions, with CENTERED variables.\n\nHowever if Detailed decomposition is not of interest, one could use Re-weighting to get Counterfactuals"
  },
  {
    "objectID": "rmethods2/session_4.html#section-12",
    "href": "rmethods2/session_4.html#section-12",
    "title": "Research Methods II",
    "section": "",
    "text": "Re-weighting\n\nConsidered a more general model: \\(Y_k = G_k(X) + \\epsilon_k\\) and \\(E(Y|X,k) = G_k(X)\\)\nThe overall mean, in this case could be written as:\n\n\\[\\begin{aligned}\nE_k(Y)&=\\int y f_k(y) dy = \\iint (g_k(x) + \\epsilon) f_k(x,\\epsilon) dx d\\epsilon \\\\\n      &= \\int g_k(x) f_k(x) dx  \\\\\n      &= \\bar Y_{g=k, X=k}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_4.html#section-13",
    "href": "rmethods2/session_4.html#section-13",
    "title": "Research Methods II",
    "section": "",
    "text": "Now lets define a counterfactual: \\(\\bar Y^c_{x=A, g=B} = \\int g_B(x) f_A(x) dx\\)\n\nthis is defined as the average outcome of group A if they face the market of group B.\n\nThen, the nonlinear decomposition can be written as:\n\n\\[\\begin{aligned}\n\\Delta \\bar Y &= \\bar Y_{G=A, X=A} - \\bar Y_{G=B, X=B} \\\\\n&= \\bar Y_{G=A, X=A} - \\bar Y_{G=B, X=B} + \\bar Y^c_{G=A, X=B} - \\bar Y^c_{G=A, X=B} \\\\\n&= \\bar Y_{G=A, X=A} - \\bar Y^c_{G=A, X=B} + \\bar Y^c_{G=A, X=B} - \\bar Y_{G=B, X=B} \\\\\n&=                   \\Delta X + \\Delta G\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_4.html#section-14",
    "href": "rmethods2/session_4.html#section-14",
    "title": "Research Methods II",
    "section": "",
    "text": "But how are the counterfactuals weights identified?\nThen the counterfactual can be written as:\n\\[\n\\begin{aligned}\n\\bar Y^c_{G=A, X=B} &= \\int g_A(x) f_B(x) dx  = \\int g_A(x) \\frac{f_B(x)}{f_A(x)} f_A(x) dx \\\\\n&= \\int g_A(x) \\frac{1-P_A(X)}{P_A(X)} f_A(x) dx \\\\\n&= \\int g_A(x) \\widehat{IPW}(X) f_A(x) dx \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "rmethods2/session_4.html#section-15",
    "href": "rmethods2/session_4.html#section-15",
    "title": "Research Methods II",
    "section": "",
    "text": "Example\n\nfrause oaxaca, clear\ndrop if lnwage==.\nqui:logit female c.(educ exper age married)##c.(educ exper age married), nolog\npredict pr_b, pr\n\ngen ipw1 = (1-pr_b)/pr_b if female==1\ngen ipw2 = pr_b/(1-pr_b) if female==0\nqui:{\nmean lnwage educ exper age married if female==0\nest sto m1\nmean lnwage educ exper age married if female==0 [pw=ipw2] \nest sto m2a\nmean lnwage educ exper age married if female==1 [pw=ipw1]\nest sto m2b\nmean lnwage educ exper age married if female==1\nest sto m3\n \n}\nesttab m1 m2a m2b m3, nogaps se ///\nmtitle(Men Men_as_w Wmen_as_m Women)\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n(751 missing values generated)\n(683 missing values generated)\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                      Men        Men_as_w       Wmen_as_m           Women   \n----------------------------------------------------------------------------\nlnwage              3.440***        3.475***        3.277***        3.267***\n                 (0.0175)        (0.0247)        (0.0280)        (0.0218)   \neduc                11.81***        11.35***        11.82***        11.23***\n                 (0.0895)         (0.112)         (0.154)        (0.0900)   \nexper               14.08***        12.26***        15.23***        12.14***\n                  (0.408)         (0.432)         (1.621)         (0.319)   \nage                 38.43***        39.46***        39.47***        39.29***\n                  (0.413)         (0.725)         (1.160)         (0.411)   \nmarried             0.522***        0.393***        0.523***        0.413***\n                 (0.0182)        (0.0252)        (0.0408)        (0.0189)   \n----------------------------------------------------------------------------\nN                     751             751             683             683   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001\n\n\nDecomposition:\n(1,2,4): - \\(DX\\) = (1) vs (2)= 3.440-3.475=-0.035 - \\(DB\\) = (2) vs (4)= 3.475-3.267 =0.208 (1,3,4) - \\(DX\\) = (3) vs (4) = 3.277-3.267 = 0.010 - \\(DB\\) = (1) vs (3) = 3.440-3.277 = 0.163\n\nreplace ipw1=1 if ipw1==.\nreplace ipw2=1 if ipw2==.\nreg lnwage female [pw=ipw1] \nreg lnwage female [pw=ipw2] \n\n(751 real changes made)\n(683 real changes made)\n(sum of wgt is 1,553.12667637155)\n\nLinear regression                               Number of obs     =      1,434\n                                                F(1, 1432)        =      24.62\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.0249\n                                                Root MSE          =     .51221\n\n------------------------------------------------------------------------------\n             |               Robust\n      lnwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |  -.1635866   .0329659    -4.96   0.000    -.2282533   -.0989199\n       _cons |   3.440222   .0174631   197.00   0.000     3.405966    3.474478\n------------------------------------------------------------------------------\n(sum of wgt is 1,349.95921823604)\n\nLinear regression                               Number of obs     =      1,434\n                                                F(1, 1432)        =      39.96\n                                                Prob &gt; F          =     0.0000\n                                                R-squared         =     0.0380\n                                                Root MSE          =     .52454\n\n------------------------------------------------------------------------------\n             |               Robust\n      lnwage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |   -.208271   .0329455    -6.32   0.000    -.2728976   -.1436444\n       _cons |   3.475032   .0246922   140.73   0.000     3.426596    3.523469\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "rmethods2/session_4.html#section-16",
    "href": "rmethods2/session_4.html#section-16",
    "title": "Research Methods II",
    "section": "",
    "text": "Re-weighting and other functions\n\nThe first advantage of re-weighting is that it allows for non-linear relationships between X and y. (via IPW)\nIt also allows you to move away from focusing on differences in means. Any transformation of the outcome is now valid!\n\n\ngen wage = exp(lnwage)\nrifhdreg wage female [pw=ipw1] , rif(gini) over(female)\nrifhdreg wage female [pw=ipw2] , rif(gini) over(female)\n\n\nLinear regression                               Number of obs     =      1,434\n                                                F(1, 1432)        =       2.12\n                                                Prob &gt; F          =     0.1457\n                                                R-squared         =     0.0027\n                                                Root MSE          =     .24103\n\n------------------------------------------------------------------------------\n             |               Robust\n        wage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |   .0252941   .0173772     1.46   0.146    -.0087934    .0593816\n       _cons |   .2207253   .0067667    32.62   0.000     .2074516     .233999\n------------------------------------------------------------------------------\nDistributional Statistic: gini\nSample Mean    RIF gini :  .23379\n\nLinear regression                               Number of obs     =      1,434\n                                                F(1, 1432)        =      11.79\n                                                Prob &gt; F          =     0.0006\n                                                R-squared         =     0.0096\n                                                Root MSE          =     .24693\n\n------------------------------------------------------------------------------\n             |               Robust\n        wage | Coefficient  std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      female |   .0485792   .0141476     3.43   0.001     .0208269    .0763315\n       _cons |   .2187795   .0078984    27.70   0.000     .2032859    .2342732\n------------------------------------------------------------------------------\nDistributional Statistic: gini\nSample Mean    RIF gini :  .24336\n\n\n\nDrawback: requires correction of SE, and accounting for RW error and Specification error\nDoesn’t easily allow for detailed Decompositions."
  },
  {
    "objectID": "rmethods2/session_4.html#going-beyond-the-mean",
    "href": "rmethods2/session_4.html#going-beyond-the-mean",
    "title": "Research Methods II",
    "section": "Going Beyond the mean",
    "text": "Going Beyond the mean\n\nOne additional and useful extension of the OB Decomposition is to use it for analysis of statistics other than the mean.\nThis can be done by using the recentered influence function (RIF) (Firpo, Fortin, Lemieux (2009,2018))\nThe idea: Use RIFs of the statistic of interest to decompose instead as dependent variable.\nOB can then be applied as usual\nConsider Double Decomposition to avoid Reweighting errors"
  },
  {
    "objectID": "rmethods2/session_4.html#example-2",
    "href": "rmethods2/session_4.html#example-2",
    "title": "Research Methods II",
    "section": "Example",
    "text": "Example\n\nfrause oaxaca, clear\ndrop if lnwage==.\ngen wage = exp(lnwage)\nssc install rif\nqui:oaxaca_rif wage educ exper age married, by(female) rif(mean)\nest sto m1\nqui:oaxaca_rif wage educ exper age married, by(female) rif(gini)\nest sto m2\nqui:oaxaca_rif wage educ exper age married, by(female) rif(q(25))\nest sto m3\nqui:oaxaca_rif wage educ exper age married, by(female) rif(iqsr(40 90))\nest sto m4\n \nesttab m1 m2 m3 m4, se nogaps ///\nmtitle(Mean Gini 25th iqsr_4010)\n\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\nchecking rif consistency and verifying not already installed...\nall files already exist and are up to date.\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                     Mean            Gini            25th       iqsr_4010   \n----------------------------------------------------------------------------\noverall                                                                     \ngroup_1             34.34***        0.221***        25.36***        0.713***\n                  (0.518)       (0.00678)         (0.484)        (0.0244)   \ngroup_2             30.25***        0.267***        20.91***        0.942***\n                  (0.682)        (0.0118)         (0.507)        (0.0635)   \ndifference          4.083***      -0.0466***        4.448***       -0.230***\n                  (0.857)        (0.0136)         (0.701)        (0.0681)   \nexplained           0.455         -0.0238**         0.969**        -0.111** \n                  (0.516)       (0.00758)         (0.371)        (0.0403)   \nunexplained         3.628***      -0.0228           3.479***       -0.119   \n                  (0.878)        (0.0153)         (0.733)        (0.0782)   \n----------------------------------------------------------------------------\nexplained                                                                   \neduc                1.220***     -0.00539           0.943***      -0.0238   \n                  (0.312)       (0.00319)         (0.239)        (0.0169)   \nexper              0.0121         -0.0121*          0.313         -0.0560*  \n                  (0.216)       (0.00509)         (0.183)        (0.0260)   \nage                -0.283        -0.00297         -0.0508         -0.0154   \n                  (0.207)       (0.00243)        (0.0659)        (0.0128)   \nmarried            -0.494**      -0.00332          -0.236         -0.0154   \n                  (0.189)       (0.00280)         (0.125)        (0.0150)   \n----------------------------------------------------------------------------\nunexplained                                                                 \neduc               -5.229           0.107          -10.68**         0.528   \n                  (3.910)        (0.0695)         (3.346)         (0.349)   \nexper              -3.666*         0.0476          -2.788           0.317   \n                  (1.774)        (0.0317)         (1.475)         (0.162)   \nage                 14.13***       -0.130           13.76***       -0.745*  \n                  (4.070)        (0.0721)         (3.532)         (0.358)   \nmarried             4.804***     -0.00499           3.654***     -0.00311   \n                  (0.856)        (0.0149)         (0.731)        (0.0747)   \n_cons              -6.410         -0.0420          -0.469          -0.215   \n                  (4.582)        (0.0817)         (3.850)         (0.416)   \n----------------------------------------------------------------------------\nN                                                                           \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rmethods2/session_6.html",
    "href": "rmethods2/session_6.html",
    "title": "Research Methods II",
    "section": "",
    "text": "As we described before, missing data is a problem for micro data analysis.\n\nReduces sample size, statistical power, and may bias estimates. (depending on the type of missingness)\n\nWe have also discussed that there are few ways to deal with missing data.\n\nComplete case analysis\nReweighting\nImputation: Prediction\nImputation: Hotdecking\n\nThis methods allows you solve for missing data if data is MCAR or MAR.\n\nwith MNAR, dealing with missing data is difficult\n\nNevertheless, you can deal with Missing data, because you have some observed data that can be used to impute it.\n\n\n\n\n\nT1T2T3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat would happen if all data is missing?\nExample:\n\nYou are working with the CPS, but are interested in looking at the relationship between income and time use.\n\nCPS does NOT have time use data.\n\n\nWe are going for the -lion hunt-\n\nYou can’t impute time use data\nYou can’t use complete case analysis\nYou can’t use reweighting\nYou can’t use hotdecking\nwhat do we do?\n\n\n\n\n\n\n\n\nOne option would be using a different data set.\n\nIn the US, the American Time Use Survey (ATUS) could be a good option.\nBut…The data has no income information!\n\nWhat if we could combine the two data sets?\nThis changes the problem from Missing all data, to one of Missing Data by design.\n\nSome segment of the population was asked about income, and some other segment was asked about time use.\n\n\n\n\n\n\n\nIf you consider the idea of combining two data sets, you can treat the problem as one of imputation.\n\nYou have a sample (two) that represents the population of interest.\nWe can reasonably assume the data is MCAR. But the data of interest is not observed at the same time.\nThen, we can use the combine data to impute the missing data, using many of the approaches we have discussed before.\n\nAnd there is also another method that is more commonly used (at Levy) to deal with this problem.\n\nStatistical Matching (aka Data Fusion).\n\nWhat does this imply?:\n\nMatch individuals across datasets (“Donor” and “Recipient”)\nTransfer information based on the matching links\n\n\n\n\n\nThere is a lot of work on this topic. Many statistical agencies use this approach to combine data Survey data with administrative data.\n\nAdministrative data is usually more accurate, but it is not collected for the purpose of research.\nSurvey data is collected for research purposes, but may not have accurate data in some areas (income)\nUnless Survey Data was collected with the purpose of being linked with administrative data, one requires methods similar to statistical matching to combine both data sets.\n\n\n\n\n\nAt Levy we have used this approach to produce relevant datasets:\n\nLIMEW: Levy Institute Measure of Economic Well-Being Combines Time use, wealtgh and Survey data (in addition to other aggregate data)\nLIMTIP: Levy Institute Measure of Time and Income Poverty Combines ATUS, with income/consumption data"
  },
  {
    "objectID": "rmethods2/session_6.html#missing-data",
    "href": "rmethods2/session_6.html#missing-data",
    "title": "Research Methods II",
    "section": "",
    "text": "As we described before, missing data is a problem for micro data analysis.\n\nReduces sample size, statistical power, and may bias estimates. (depending on the type of missingness)\n\nWe have also discussed that there are few ways to deal with missing data.\n\nComplete case analysis\nReweighting\nImputation: Prediction\nImputation: Hotdecking\n\nThis methods allows you solve for missing data if data is MCAR or MAR.\n\nwith MNAR, dealing with missing data is difficult\n\nNevertheless, you can deal with Missing data, because you have some observed data that can be used to impute it."
  },
  {
    "objectID": "rmethods2/session_6.html#types-of-missing-data",
    "href": "rmethods2/session_6.html#types-of-missing-data",
    "title": "Research Methods II",
    "section": "",
    "text": "T1T2T3"
  },
  {
    "objectID": "rmethods2/session_6.html#missing-data-1",
    "href": "rmethods2/session_6.html#missing-data-1",
    "title": "Research Methods II",
    "section": "",
    "text": "What would happen if all data is missing?\nExample:\n\nYou are working with the CPS, but are interested in looking at the relationship between income and time use.\n\nCPS does NOT have time use data.\n\n\nWe are going for the -lion hunt-\n\nYou can’t impute time use data\nYou can’t use complete case analysis\nYou can’t use reweighting\nYou can’t use hotdecking\nwhat do we do?"
  },
  {
    "objectID": "rmethods2/session_6.html#section",
    "href": "rmethods2/session_6.html#section",
    "title": "Research Methods II",
    "section": "",
    "text": "One option would be using a different data set.\n\nIn the US, the American Time Use Survey (ATUS) could be a good option.\nBut…The data has no income information!\n\nWhat if we could combine the two data sets?\nThis changes the problem from Missing all data, to one of Missing Data by design.\n\nSome segment of the population was asked about income, and some other segment was asked about time use."
  },
  {
    "objectID": "rmethods2/session_6.html#imputation-and-statistical-matching",
    "href": "rmethods2/session_6.html#imputation-and-statistical-matching",
    "title": "Research Methods II",
    "section": "",
    "text": "If you consider the idea of combining two data sets, you can treat the problem as one of imputation.\n\nYou have a sample (two) that represents the population of interest.\nWe can reasonably assume the data is MCAR. But the data of interest is not observed at the same time.\nThen, we can use the combine data to impute the missing data, using many of the approaches we have discussed before.\n\nAnd there is also another method that is more commonly used (at Levy) to deal with this problem.\n\nStatistical Matching (aka Data Fusion).\n\nWhat does this imply?:\n\nMatch individuals across datasets (“Donor” and “Recipient”)\nTransfer information based on the matching links"
  },
  {
    "objectID": "rmethods2/session_6.html#official-examples",
    "href": "rmethods2/session_6.html#official-examples",
    "title": "Research Methods II",
    "section": "",
    "text": "There is a lot of work on this topic. Many statistical agencies use this approach to combine data Survey data with administrative data.\n\nAdministrative data is usually more accurate, but it is not collected for the purpose of research.\nSurvey data is collected for research purposes, but may not have accurate data in some areas (income)\nUnless Survey Data was collected with the purpose of being linked with administrative data, one requires methods similar to statistical matching to combine both data sets."
  },
  {
    "objectID": "rmethods2/session_6.html#in-house-some-examples",
    "href": "rmethods2/session_6.html#in-house-some-examples",
    "title": "Research Methods II",
    "section": "",
    "text": "At Levy we have used this approach to produce relevant datasets:\n\nLIMEW: Levy Institute Measure of Economic Well-Being Combines Time use, wealtgh and Survey data (in addition to other aggregate data)\nLIMTIP: Levy Institute Measure of Time and Income Poverty Combines ATUS, with income/consumption data"
  },
  {
    "objectID": "rmethods2/session_6.html#what-do-we-need",
    "href": "rmethods2/session_6.html#what-do-we-need",
    "title": "Research Methods II",
    "section": "What do we need?",
    "text": "What do we need?\n\nConsider two data sets: \\(A\\) and \\(B\\).\n\n\\(A\\) has informtion on \\(X\\) and \\(Z\\)\n\\(B\\) has information on \\(Y\\) and \\(Z\\)\nWe want a file that has \\(X\\), \\(Y\\) and \\(Z\\)."
  },
  {
    "objectID": "rmethods2/session_6.html#assumptions",
    "href": "rmethods2/session_6.html#assumptions",
    "title": "Research Methods II",
    "section": "Assumptions",
    "text": "Assumptions\n\n(\\(X,Y,Z\\)) are multivariate random variables with joint distribution \\(f(x,y,z)\\), that represents the population of interest.\nBoth datasets are random samples from the same population of interest.\n\\(\\frac{P_w(D=A|X,Y,Z)}{P_w(D=B|X,Y,Z)} = \\frac{P(D=A)}{P(D=B)} = 1\\)\nConditional Independence assumption:\n\n\\(Y\\) and \\(Z\\) are independent from each other given \\(X\\). \\[f(x,y|z) = f(x|z)f(y|z)\\]\n\nThe goal is to combine the two data sets to produce a file that has data on \\(X\\), \\(Y\\) and \\(Z\\). by identifying \\(f(x,y,z)\\)."
  },
  {
    "objectID": "rmethods2/session_6.html#statistical-matching-limitations",
    "href": "rmethods2/session_6.html#statistical-matching-limitations",
    "title": "Research Methods II",
    "section": "Statistical Matching: Limitations",
    "text": "Statistical Matching: Limitations\n\nThe quality of this identification will depend on how well the conditional independence assumption holds.\nBecause of this, synthetic datasets can’t tell you much about covariances or causal relationships\n\n\\[Cov(z,y,z) = \\begin{pmatrix}\nV(X) & \\color{red}{V(X,Y)} & V(X,Z) \\\\\n\\color{red}{V(X,Y)'} & V(Y) & V(Y,Z) \\\\\nV(X,Z)' & V(Y,Z)' & V(Z)\n\\end{pmatrix}\n\\]\nalbeit, you can impose certain bounderies on the covariance matrix."
  },
  {
    "objectID": "rmethods2/session_6.html#matching-approaches",
    "href": "rmethods2/session_6.html#matching-approaches",
    "title": "Research Methods II",
    "section": "Matching Approaches:",
    "text": "Matching Approaches:\nThere are two types of statistical matching procedures:\n\nUnconstrained MatchingConstraints Matching\n\n\n\nRecords from \\(A\\) and \\(B\\) can be used multiple times (or none) in the matching.\n\nAbsurd case: One observation from \\(A\\) is matched with all observations from \\(B\\).\n\nThis is the most common approach in the literature for policy evaluation\nPros: Uses the “best” candidate for the matching. Cons: It may not transfer the uncoditional distribution of the data.\nDoes not necessarily required \\(A\\) and \\(B\\) to be from the same population. (weighted size)\n\n\n\n\nAll records from \\(A\\) and \\(B\\) are used once and only once in the matching. (without replacement)\nWhen using weighted samples, records are matched until the weights are exhausted.\n\nRequires \\(A\\) and \\(B\\) to be from the same population. (weighted size)\n\nPros: It transfers the unconditional distribution of the data. Cons: My not use “best” candidate for the matching."
  },
  {
    "objectID": "rmethods2/session_6.html#matching-records",
    "href": "rmethods2/session_6.html#matching-records",
    "title": "Research Methods II",
    "section": "Matching Records:",
    "text": "Matching Records:\n\nMatching records, requires defining a measure of similarity between records.\nThis measures can vary depending on the data type, and dimensionality of the data\n\n\\[\\begin{aligned}\n\\text{ Euclidian: } d(r^A,r^b) &= \\sqrt{\\sum_i^k(x^A_i-x^B_i)^2 } \\\\\n\\text{ SdEuclidian: } d(r^A,r^b) &= \\sqrt{\\sum_i^k\\left(\\frac{x^A_i-x^B_i}{\\sigma_j}\\right)^2 } \\\\\n\\text{ Mahalanobis: } d(r^A,r^b) &= \\sqrt{(x^A-x^B)'\\Sigma_x^{-1}(x^A-x^B)} \\\\\n\\end{aligned}\n\\]\n\nAll this measures are useful when one has high dimensional data."
  },
  {
    "objectID": "rmethods2/session_6.html#matching-reducing-dimensionality",
    "href": "rmethods2/session_6.html#matching-reducing-dimensionality",
    "title": "Research Methods II",
    "section": "Matching: Reducing Dimensionality",
    "text": "Matching: Reducing Dimensionality\n\nA second alternative is to reduce data dimensionality before estimating distances.\n\n\nPredictive mean matching:Propensity score matching:PCA\n\n\n\nModel \\(x = z\\beta + \\epsilon\\) using \\(A\\).\nMake predictions \\(z\\hat\\beta\\) for both samples.\nMatch records based on \\(z\\hat\\beta\\)\nGood results to match individuals with similar “predicted” income.\nPuts more “weight” on the variables used to predict the outcome.\n\n\n\n\nModel the likehood of an observation being in \\(A\\) using \\(Z\\). \\[P(D=A|Z) = G(Z\\gamma)\\]\nMake predictions \\(\\hat P\\) or \\(z\\hat\\gamma\\) for both samples.\nMatch records based on \\(\\hat\\pi\\)\nGeneral purpose score.\nMay be problematic if \\(A\\) and \\(B\\) have very similar distributions of \\(Z\\).\n\nPuts more “weight” on the variables with different distributions between \\(A\\) and \\(B\\).\n\n\n\n\nUse PCA to reduce dimensionality of \\(Z\\) into a single index.\n\nCan use either a single dataset or both\n\nMake predictions of the first principal component \\(PC1\\)\nMatch records based on \\(PC1\\)\nPuts more weight on variables that explain most of the variance in \\(Z\\)."
  },
  {
    "objectID": "rmethods2/session_6.html#matching-rank-matching",
    "href": "rmethods2/session_6.html#matching-rank-matching",
    "title": "Research Methods II",
    "section": "Matching: Rank Matching",
    "text": "Matching: Rank Matching\n\nMost of distance based matching is usually feasible with unconstrained matching.\n\nthus, best records are always matched.\n\nWhen considering constrained matching, distance based matching may not be adecuate\n\nWhile first records are matched the best, last records may be matched poorly match.\n\nA balance therefore is to use rank matching.\n\nRank observations based on a single variable (pscore, predicted mean, etc)\nMatch records based on rank.\n\nNo match would be “best”, but reduces changes of poor matches."
  },
  {
    "objectID": "rmethods2/session_6.html#data-harmonization",
    "href": "rmethods2/session_6.html#data-harmonization",
    "title": "Research Methods II",
    "section": "1. Data Harmonization",
    "text": "1. Data Harmonization\n\nBecause Data files come from different data sources, they may have different variables names, coding schemes, or definitions.\nWe need to set \\(Z\\) variables to be defined as identically as possible in both files\nBeyond definition harmonization, one must also be mindful of the distribution of the variables in both files.\n\nIf the distribution of \\(Z\\) is different in both files, the matching may not be adequate.\n\nThe weights schemes in both files should be adjusted to add up to the same population size (typically the “recipient” values)\n\nWeight adjustment could be done by selected strata"
  },
  {
    "objectID": "rmethods2/session_6.html#estimation-of-matching-score",
    "href": "rmethods2/session_6.html#estimation-of-matching-score",
    "title": "Research Methods II",
    "section": "2. Estimation of Matching Score",
    "text": "2. Estimation of Matching Score\n\nEither using full or sub (strata) samples, estimate a matching score\n\nThis could be a propensity score, predicted mean, or first principal component.\n\nYou may want to create “further cells” to improve matching. (not necessarily re estimate the matching score)\n\nFor example, You consider Gender as strata (two scores), but further create cells by “age” (5 groups)"
  },
  {
    "objectID": "rmethods2/session_6.html#perform-the-match",
    "href": "rmethods2/session_6.html#perform-the-match",
    "title": "Research Methods II",
    "section": "3. Perform the match",
    "text": "3. Perform the match\n\nUsing the finest definition of “cells”, rank observations based on Matching Scores\nUsing rank, match observations till all weights are exhausted.(from either Sample)\n“unmatched” observations are left for later rounds using coarser definitions of cells.\nMatching continues until all units (recipients) are matched."
  },
  {
    "objectID": "rmethods2/session_6.html#assessing-the-quality-of-the-match",
    "href": "rmethods2/session_6.html#assessing-the-quality-of-the-match",
    "title": "Research Methods II",
    "section": "4. Assessing the quality of the match",
    "text": "4. Assessing the quality of the match\n\nThe idea is to compare the distribution of the “transfered/imputed” data with the distribution from the “donor” data.\n\nOverall distribution of the data will be the same by construction.\n\nCompare distributions by Strata, smaller cells, or specific variables or interest.\nRule of thumb +/- 10% is acceptable (mean, median, Standard error).\n\nBut it may depend on the variable of interest.\n\nOne may also use other approaches like “regression” to compare all variables at once.\nIf the distribution of the data is not adequate, one may want re-do the matching, with different “cell” definitions or matching scores."
  },
  {
    "objectID": "rmethods2/session_6.html#example",
    "href": "rmethods2/session_6.html#example",
    "title": "Research Methods II",
    "section": "Example",
    "text": "Example\n\nfrause wage2, clear\nset seed 312\nxtile smp = runiform()\nreplace smp=smp==1\ngen wage_s = wage if smp==1\n**three Matching scores\n** Pmm\nreg wage_s hours iq kww educ exper tenure age married black south urban sibs \npredict wageh\n** pscore\nlogit smp hours iq kww educ exper tenure age married black south urban sibs \npredict pscore, xb\n** pca\npca hours iq kww educ exper tenure age married black south urban sibs , comp(1)\npredict pc1\n\nforeach i in wageh pscore pc1 {\n    qui:sum `i'\n    replace `i' = (`i'-r(mean))/r(sd)\n}\n\n\n\n\n(467 real changes made)\n(467 missing values generated)\n\n      Source |       SS           df       MS      Number of obs   =       468\n-------------+----------------------------------   F(12, 455)      =     14.83\n       Model |  24043666.1        12  2003638.84   Prob &gt; F        =    0.0000\n    Residual |  61493527.5       455   135150.61   R-squared       =    0.2811\n-------------+----------------------------------   Adj R-squared   =    0.2621\n       Total |  85537193.6       467  183163.155   Root MSE        =    367.63\n\n------------------------------------------------------------------------------\n      wage_s | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hours |  -4.080234   2.204689    -1.85   0.065    -8.412869    .2524015\n          iq |   3.090831   1.459171     2.12   0.035     .2232806    5.958381\n         kww |   5.637987   2.888128     1.95   0.052    -.0377362    11.31371\n        educ |   53.43222   10.41882     5.13   0.000     32.95724     73.9072\n       exper |   8.816439    5.30953     1.66   0.098    -1.617804    19.25068\n      tenure |   6.327233   3.534248     1.79   0.074    -.6182417    13.27271\n         age |   10.92113   7.195943     1.52   0.130    -3.220278    25.06253\n     married |    143.306   54.35023     2.64   0.009     36.49735    250.1146\n       black |  -144.0597   60.51784    -2.38   0.018    -262.9889    -25.1306\n       south |  -37.37316    37.7745    -0.99   0.323    -111.6073    36.86096\n       urban |   200.3258   38.93558     5.15   0.000       123.81    276.8417\n        sibs |   1.881618   8.468635     0.22   0.824    -14.76087    18.52411\n       _cons |  -842.6706     273.28    -3.08   0.002    -1379.718   -305.6231\n------------------------------------------------------------------------------\n(option xb assumed; fitted values)\n\nIteration 0:  Log likelihood = -648.09208  \nIteration 1:  Log likelihood = -642.21625  \nIteration 2:  Log likelihood = -642.21522  \nIteration 3:  Log likelihood = -642.21522  \n\nLogistic regression                                     Number of obs =    935\n                                                        LR chi2(12)   =  11.75\n                                                        Prob &gt; chi2   = 0.4657\nLog likelihood = -642.21522                             Pseudo R2     = 0.0091\n\n------------------------------------------------------------------------------\n         smp | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hours |   .0153544   .0093201     1.65   0.099    -.0029127    .0336215\n          iq |   .0024677   .0057346     0.43   0.667    -.0087719    .0137072\n         kww |  -.0095774   .0113247    -0.85   0.398    -.0317735    .0126186\n        educ |  -.0255069   .0409702    -0.62   0.534     -.105807    .0547932\n       exper |   .0256009   .0205087     1.25   0.212    -.0145955    .0657973\n      tenure |   .0131078   .0137404     0.95   0.340     -.013823    .0400385\n         age |  -.0033883   .0281317    -0.12   0.904    -.0585256    .0517489\n     married |  -.2382151   .2167632    -1.10   0.272    -.6630632    .1866331\n       black |  -.1214461   .2276112    -0.53   0.594    -.5675559    .3246637\n       south |  -.0370746   .1457381    -0.25   0.799    -.3227161    .2485669\n       urban |  -.0371736   .1495076    -0.25   0.804    -.3302031     .255856\n        sibs |   -.041735   .0313008    -1.33   0.182    -.1030835    .0196135\n       _cons |  -.1246922   1.056515    -0.12   0.906    -2.195424    1.946039\n------------------------------------------------------------------------------\n\nPrincipal components/correlation                 Number of obs    =        935\n                                                 Number of comp.  =          1\n                                                 Trace            =         12\n    Rotation: (unrotated = principal)            Rho              =     0.2112\n\n    --------------------------------------------------------------------------\n       Component |   Eigenvalue   Difference         Proportion   Cumulative\n    -------------+------------------------------------------------------------\n           Comp1 |       2.5348      .622213             0.2112       0.2112\n           Comp2 |      1.91258      .821455             0.1594       0.3706\n           Comp3 |      1.09113     .0297471             0.0909       0.4615\n           Comp4 |      1.06138     .0497587             0.0884       0.5500\n           Comp5 |      1.01162     .0867533             0.0843       0.6343\n           Comp6 |      .924871     .0639678             0.0771       0.7114\n           Comp7 |      .860903     .0907871             0.0717       0.7831\n           Comp8 |      .770116      .144885             0.0642       0.8473\n           Comp9 |      .625231      .119791             0.0521       0.8994\n          Comp10 |       .50544     .0919858             0.0421       0.9415\n          Comp11 |      .413454      .124988             0.0345       0.9760\n          Comp12 |      .288466            .             0.0240       1.0000\n    --------------------------------------------------------------------------\n\nPrincipal components (eigenvectors) \n\n    --------------------------------------\n        Variable |    Comp1 | Unexplained \n    -------------+----------+-------------\n           hours |   0.1320 |       .9558 \n              iq |   0.4921 |       .3863 \n             kww |   0.4220 |       .5486 \n            educ |   0.4555 |        .474 \n           exper |  -0.2161 |       .8817 \n          tenure |   0.0463 |       .9946 \n             age |   0.0527 |        .993 \n         married |   0.0053 |       .9999 \n           black |  -0.3722 |       .6488 \n           south |  -0.2076 |       .8908 \n           urban |   0.0723 |       .9868 \n            sibs |  -0.3411 |       .7051 \n    --------------------------------------\n(score assumed)\n\nScoring coefficients \n    sum of squares(column-loading) = 1\n\n    ------------------------\n        Variable |    Comp1 \n    -------------+----------\n           hours |   0.1320 \n              iq |   0.4921 \n             kww |   0.4220 \n            educ |   0.4555 \n           exper |  -0.2161 \n          tenure |   0.0463 \n             age |   0.0527 \n         married |   0.0053 \n           black |  -0.3722 \n           south |  -0.2076 \n           urban |   0.0723 \n            sibs |  -0.3411 \n    ------------------------\n(935 real changes made)\n(935 real changes made)\n(935 real changes made)\n\n\nNext we create ranks for each observation, assuming no stratification.\n\nbysort smp (wageh) :gen rnk1=_n\nbysort smp (pscore):gen rnk2=_n\nbysort smp (pc1)   :gen rnk3=_n\n\nFinally, the imputation. Simply “matching” information from the donor to the recipient.\n\n* Imputation\nclonevar wage1 = wage_s\nclonevar wage2 = wage_s\nclonevar wage3 = wage_s\n\ngsort -smp rnk1\nreplace wage1 = wage_s[rnk1] if smp==0\n\ngsort -smp rnk2\nreplace wage2 = wage_s[rnk2] if smp==0\n\ngsort -smp rnk3\nreplace wage3 = wage_s[rnk3] if smp==0\n\n(467 missing values generated)\n(467 missing values generated)\n(467 missing values generated)\n(467 real changes made)\n(467 real changes made)\n(467 real changes made)\n\n\nSimple quality assessment.\n\nqui:reg wage hours iq kww educ exper tenure age married black south if smp==0\nest sto m1\nqui:reg wage1 hours iq kww educ exper tenure age married black south if smp==0\nest sto m2\nqui:reg wage2 hours iq kww educ exper tenure age married black south if smp==0\nest sto m3\nqui:reg wage3 hours iq kww educ exper tenure age married black south if smp==0\nest sto m4\nesttab m1 m2 m3 m4 , se mtitle(True Wageh pscore pca)\n\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                     True           Wageh          pscore             pca   \n----------------------------------------------------------------------------\nhours              -3.076          -5.508*          0.208          -0.971   \n                  (2.482)         (2.678)         (3.109)         (2.825)   \n\niq                  2.579           0.343          -2.775           3.653*  \n                  (1.411)         (1.522)         (1.767)         (1.605)   \n\nkww                 5.410*          8.750**         1.954           7.545*  \n                  (2.745)         (2.961)         (3.438)         (3.124)   \n\neduc                46.14***        77.82***        8.331           44.53***\n                  (10.18)         (10.98)         (12.75)         (11.58)   \n\nexper               11.66*          15.17**        -2.006           5.108   \n                  (4.980)         (5.372)         (6.237)         (5.667)   \n\ntenure              3.844           5.238          -0.511          0.0757   \n                  (3.332)         (3.595)         (4.174)         (3.792)   \n\nage                -0.945          -9.667          -10.91          -5.380   \n                  (6.905)         (7.449)         (8.648)         (7.858)   \n\nmarried             187.2***        165.6**         89.99           38.58   \n                  (54.26)         (58.54)         (67.96)         (61.75)   \n\nblack              -64.09          -99.07           46.48          -39.87   \n                  (52.43)         (56.57)         (65.67)         (59.67)   \n\nsouth              -80.79*         -74.14           9.854          -120.9** \n                  (35.19)         (37.96)         (44.07)         (40.04)   \n\n_cons              -254.2          -199.6          1349.4***       -104.9   \n                  (251.2)         (271.0)         (314.6)         (285.9)   \n----------------------------------------------------------------------------\nN                     467             467             467             467   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p&lt;0.05, ** p&lt;0.01, *** p&lt;0.001"
  },
  {
    "objectID": "rm_class.html",
    "href": "rm_class.html",
    "title": "Research Methods: Econometrics I",
    "section": "",
    "text": "Class syllabus is here."
  },
  {
    "objectID": "rm_class.html#syllabus",
    "href": "rm_class.html#syllabus",
    "title": "Research Methods: Econometrics I",
    "section": "",
    "text": "Class syllabus is here."
  },
  {
    "objectID": "rm_class.html#zoom-link",
    "href": "rm_class.html#zoom-link",
    "title": "Research Methods: Econometrics I",
    "section": "Zoom Link",
    "text": "Zoom Link\nWhen there is need to attend a class online, and you have requested to attend online, use the following link:\nZoom class"
  },
  {
    "objectID": "rm_class.html#introduction",
    "href": "rm_class.html#introduction",
    "title": "Research Methods: Econometrics I",
    "section": "Introduction",
    "text": "Introduction\n\nChapter 1\nOnce Upon a time, why Econometrics?:"
  },
  {
    "objectID": "rm_class.html#part-i-basic-tools",
    "href": "rm_class.html#part-i-basic-tools",
    "title": "Research Methods: Econometrics I",
    "section": "Part I: Basic Tools",
    "text": "Part I: Basic Tools\n\n1. Chapter 2:\nThe Simple Regression Model\n\n\n2. Chapter 3:\nMultiple Regression Analysis: Estimation\n\n\n3. Chapter 4 - 5\nMRA: Inference and Asymptotics\n\n\nHomeWork 1 Github"
  },
  {
    "objectID": "rm_class.html#part-ii-addressing-problems-with-mra",
    "href": "rm_class.html#part-ii-addressing-problems-with-mra",
    "title": "Research Methods: Econometrics I",
    "section": "Part II: Addressing Problems with MRA",
    "text": "Part II: Addressing Problems with MRA\n\n4. Chapter 6 - 7\nMRA: Scaling, functional forms, Goodness of Fit, and Qualitative Information\n\n\n5. Chapter 8\nHeteroskedasticity\n\n\n6. Chapter 9\nFurther Problems: Functional form, Missing variables, measurement errors, missing data\n\n\nHomeWork 2 Github\n\n\nMIDTERM!\n\n7. Chapter 15\nInstrumental Variables and 2SLS\n\n\n8. Chapter 17\nLimited Dep Variables"
  },
  {
    "objectID": "rm_class.html#part-iii-panel-data-methods",
    "href": "rm_class.html#part-iii-panel-data-methods",
    "title": "Research Methods: Econometrics I",
    "section": "Part III: Panel Data Methods",
    "text": "Part III: Panel Data Methods\n\n9. Chapter 13\nPool Cross Section and Panel Data\n\n\n10. Chapter 14\nAdvanced Panel Data Methods\n\n\nHomeWork 3"
  },
  {
    "objectID": "rm_class.html#part-iv-time-series",
    "href": "rm_class.html#part-iv-time-series",
    "title": "Research Methods: Econometrics I",
    "section": "Part IV: Time Series",
    "text": "Part IV: Time Series\n\n11. Chapter 10 - 11\nBasics of Regression analysis with TSD\n\n\n12. Chapter 12 & 18\nAdvanced TSD Problems\n\n\nFinal TBD"
  },
  {
    "objectID": "rm-data/hw-material/report7.html",
    "href": "rm-data/hw-material/report7.html",
    "title": "The Impact of House Allegiances on Power Dynamics in Westeros",
    "section": "",
    "text": "This report examines the intricate power dynamics in Westeros, as depicted in George R.R. Martin’s “A Song of Ice and Fire” series and its television adaptation, “Game of Thrones.” We will explore how house allegiances shape the political landscape, analyze key alliances, and discuss their implications for the struggle for the Iron Throne."
  },
  {
    "objectID": "rm-data/hw-material/report7.html#house-allegiances-and-power-distribution",
    "href": "rm-data/hw-material/report7.html#house-allegiances-and-power-distribution",
    "title": "The Impact of House Allegiances on Power Dynamics in Westeros",
    "section": "House Allegiances and Power Distribution",
    "text": "House Allegiances and Power Distribution\nThe distribution of power in Westeros can be modeled using a modified version of the Lanchester equations, which originally described the relative strengths of military forces. In our context, we adapt this to represent the power dynamics between major houses:\n\\[\n\\frac{dR}{dt} = -\\alpha L, \\quad \\frac{dL}{dt} = -\\beta R\n\\tag{1}\\]\nWhere \\(R\\) and \\(L\\) represent the strength of rival houses (e.g., Stark and Lannister), and \\(\\alpha\\) and \\(\\beta\\) are coefficients representing the effectiveness of each house’s strategy and resources."
  },
  {
    "objectID": "rm-data/hw-material/report7.html#visualization-of-house-alliances",
    "href": "rm-data/hw-material/report7.html#visualization-of-house-alliances",
    "title": "The Impact of House Allegiances on Power Dynamics in Westeros",
    "section": "Visualization of House Alliances",
    "text": "Visualization of House Alliances\nTo better understand the complex web of alliances in Westeros, we’ve created a network graph representing the relationships between major houses throughout the series.\n\n\n\n\n\n\n\n\nFigure 1: Network of Major House Alliances in Westeros\n\n\n\n\n\nFigure Figure 1 illustrates the complex network of alliances between major houses in Westeros. The connections between houses play a crucial role in determining the balance of power, as discussed in Martin (2011)."
  },
  {
    "objectID": "rm-data/hw-material/report7.html#key-factors-influencing-house-power",
    "href": "rm-data/hw-material/report7.html#key-factors-influencing-house-power",
    "title": "The Impact of House Allegiances on Power Dynamics in Westeros",
    "section": "Key Factors Influencing House Power",
    "text": "Key Factors Influencing House Power\nSeveral factors contribute to a house’s overall power and influence in Westeros. Table Table 1 summarizes these key elements:\n\n\n\nTable 1: Key Factors Influencing House Power in Westeros\n\n\n\n\n\nFactor\nDescription\nImpact\n\n\n\n\nMilitary Strength\nSize and training of armies\nHigh\n\n\nEconomic Resources\nWealth and control over trade\nHigh\n\n\nPolitical Alliances\nRelationships with other houses\nMedium\n\n\nDragons\nPossession of dragons (Targaryen-specific)\nVery High\n\n\n\n\n\n\nAs seen in Table Table 1, military strength and economic resources are crucial for maintaining power. However, the reintroduction of dragons by House Targaryen significantly alters the balance, as noted in Equation Equation 11."
  },
  {
    "objectID": "rm-data/hw-material/report7.html#footnotes",
    "href": "rm-data/hw-material/report7.html#footnotes",
    "title": "The Impact of House Allegiances on Power Dynamics in Westeros",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe presence of dragons could be represented by an additional term in the Lanchester equations, significantly increasing the α coefficient for the house possessing them.↩︎"
  },
  {
    "objectID": "rm-data/hw-material/report5.html",
    "href": "rm-data/hw-material/report5.html",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "",
    "text": "This report examines the influence of Eric Ugland’s “The Good Guys” series on contemporary fantasy literature. We will explore the unique elements of Ugland’s work, its reception among readers, and its impact on the genre as a whole. The analysis will include quantitative data on book sales, a comparison with other works in the genre, and insights from literary critics."
  },
  {
    "objectID": "rm-data/hw-material/report5.html#uglands-narrative-formula",
    "href": "rm-data/hw-material/report5.html#uglands-narrative-formula",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "Ugland’s Narrative Formula",
    "text": "Ugland’s Narrative Formula\nOne of the key factors contributing to the success of “The Good Guys” series is Ugland’s innovative approach to character progression. This can be represented by the following equation:\n\\[\nP = (E \\times S) + (L \\times C)\n\\tag{1}\\]\nWhere P represents character progression, E is experience gained, S is skill level, L is luck factor, and C is character choices. This formula Equation 1 encapsulates Ugland’s balance between traditional RPG elements and character-driven storytelling."
  },
  {
    "objectID": "rm-data/hw-material/report5.html#sales-performance",
    "href": "rm-data/hw-material/report5.html#sales-performance",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "Sales Performance",
    "text": "Sales Performance\nThe series’ popularity can be visualized through its sales performance over time:\n\n\n\n\n\n\n\n\nFigure 1: Monthly sales of ‘The Good Guys’ series over two years\n\n\n\n\n\nAs shown in Figure 1, the series has experienced steady growth in sales, with periodic spikes coinciding with new book releases."
  },
  {
    "objectID": "rm-data/hw-material/report5.html#genre-comparison",
    "href": "rm-data/hw-material/report5.html#genre-comparison",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "Genre Comparison",
    "text": "Genre Comparison\nTo contextualize the success of “The Good Guys,” we can compare its key metrics with other popular fantasy series:\n\n\n\nTable 1: Comparison of popular LitRPG series\n\n\n\n\n\nSeries\nAvg. Rating\nBooks Published\nTotal Sales (millions)\n\n\n\n\nThe Good Guys\n4.6\n11\n2.5\n\n\nCradle\n4.7\n11\n3.0\n\n\nThe Land\n4.5\n8\n2.0\n\n\n\n\n\n\nThe data in Table 1 demonstrates that “The Good Guys” holds its own against other well-established series in the genre."
  },
  {
    "objectID": "rm-data/hw-material/report5.html#critical-reception",
    "href": "rm-data/hw-material/report5.html#critical-reception",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "Critical Reception",
    "text": "Critical Reception\nLiterary critics have praised Ugland’s work for its fresh take on the LitRPG genre. According to Johnson (2022), “Ugland’s ‘The Good Guys’ series represents a significant evolution in LitRPG storytelling, blending traditional fantasy elements with modern gaming concepts in a uniquely engaging way.”1\nThe series has also been noted for its contribution to the broader fantasy genre. Smith (2023) argues that “The Good Guys” has “pushed the boundaries of what readers expect from fantasy literature, potentially influencing the direction of the genre for years to come.”"
  },
  {
    "objectID": "rm-data/hw-material/report5.html#footnotes",
    "href": "rm-data/hw-material/report5.html#footnotes",
    "title": "The Impact of ‘The Good Guys’ on Modern Fantasy Literature",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis praise is particularly noteworthy given the often-skeptical reception of LitRPG works by mainstream literary critics.↩︎"
  },
  {
    "objectID": "rm-data/hw-material/report3.html",
    "href": "rm-data/hw-material/report3.html",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "",
    "text": "Dungeons and Dragons (D&D) is a popular tabletop role-playing game that has captivated players for decades. This report explores the mathematical underpinnings of D&D, focusing on the probability distributions of dice rolls and their impact on gameplay. We will examine the statistical nature of character abilities, combat outcomes, and skill checks, providing insights into the game’s mechanics through equations, data visualization, and tabular analysis."
  },
  {
    "objectID": "rm-data/hw-material/report3.html#the-probability-of-adventure",
    "href": "rm-data/hw-material/report3.html#the-probability-of-adventure",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "The Probability of Adventure",
    "text": "The Probability of Adventure\nAt the heart of D&D lies the rolling of dice, particularly the iconic twenty-sided die (d20). The probability of rolling any number on a d20 is uniform, but the outcomes of these rolls can be modified by character abilities and situational modifiers. The probability of success for any given action can be expressed as:\n\\[\nP(success) = \\frac{21 - (DC - modifier)}{20}\n\\tag{1}\\]\nWhere DC is the Difficulty Class of the task, and the modifier is the character’s relevant skill or ability modifier. This equation (Equation 1) forms the foundation of many D&D mechanics1."
  },
  {
    "objectID": "rm-data/hw-material/report3.html#visualizing-character-ability-scores",
    "href": "rm-data/hw-material/report3.html#visualizing-character-ability-scores",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "Visualizing Character Ability Scores",
    "text": "Visualizing Character Ability Scores\nCharacter creation in D&D often involves rolling dice to determine ability scores. The most common method is rolling 4d6 and dropping the lowest die. Let’s visualize the distribution of these rolls:\n\n\n\n\n\n\n\n\nFigure 1: Distribution of D&D Ability Scores (4d6 drop lowest)\n\n\n\n\n\nFigure Figure 1 illustrates the distribution of ability scores using the 4d6 drop lowest method. This bell-shaped curve demonstrates why most characters have average abilities, with exceptional scores being rare."
  },
  {
    "objectID": "rm-data/hw-material/report3.html#combat-outcomes",
    "href": "rm-data/hw-material/report3.html#combat-outcomes",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "Combat Outcomes",
    "text": "Combat Outcomes\nCombat in D&D involves a complex interplay of attack rolls, damage calculations, and defensive abilities. Table Table 1 summarizes the average damage output for different weapon types:\n\n\n\nTable 1: Average Damage Output by Weapon Type\n\n\n\n\n\nWeapon Type\nAverage Damage\nCritical Hit Chance\n\n\n\n\nDagger\n2.5\n5%\n\n\nLongsword\n4.5\n5%\n\n\nGreataxe\n6.5\n5%\n\n\n\n\n\n\nAs shown in Table 1, weapon choice significantly impacts potential damage output, with larger weapons generally dealing more damage at the cost of other factors like weight and required strength."
  },
  {
    "objectID": "rm-data/hw-material/report3.html#the-role-of-randomness",
    "href": "rm-data/hw-material/report3.html#the-role-of-randomness",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "The Role of Randomness",
    "text": "The Role of Randomness\nWhile skill and strategy play crucial roles in D&D, the element of chance introduced by dice rolls adds excitement and unpredictability to the game. According to Tormey (2019), this balance between player agency and random chance is what makes D&D both challenging and engaging. The interplay between player decisions and dice rolls creates a unique narrative experience in each game session."
  },
  {
    "objectID": "rm-data/hw-material/report3.html#footnotes",
    "href": "rm-data/hw-material/report3.html#footnotes",
    "title": "The Mathematics of Dungeons and Dragons: A Statistical Adventure",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis simplified equation assumes a linear probability distribution and does not account for critical successes or failures, which are typically represented by rolling a natural 20 or 1, respectively.↩︎"
  },
  {
    "objectID": "rm-data/hw-material/report1.html",
    "href": "rm-data/hw-material/report1.html",
    "title": "The Impact of Resource Management in StarCraft: A Strategic Analysis",
    "section": "",
    "text": "This report examines the crucial role of resource management in the popular real-time strategy game StarCraft. We will explore how effective resource allocation influences gameplay dynamics and strategic decision-making. The analysis will include a mathematical model of resource gathering, a visualization of unit production rates, and a comparison of resource types across different races."
  },
  {
    "objectID": "rm-data/hw-material/report1.html#mathematical-model-of-resource-gathering",
    "href": "rm-data/hw-material/report1.html#mathematical-model-of-resource-gathering",
    "title": "The Impact of Resource Management in StarCraft: A Strategic Analysis",
    "section": "Mathematical Model of Resource Gathering",
    "text": "Mathematical Model of Resource Gathering\nIn StarCraft, the rate of resource accumulation can be modeled using a simple differential equation. If we denote the amount of resources as \\(R\\) and time as \\(t\\), we can express the rate of change of resources as:\n\\[\n\\frac{dR}{dt} = \\alpha N - \\beta P\n\\tag{1}\\]\nWhere \\(\\alpha\\) is the gathering rate per worker, \\(N\\) is the number of workers, \\(\\beta\\) is the consumption rate, and \\(P\\) is the production rate of units or structures. This model, as shown in Equation 1, forms the basis of the game’s economic system (Choi and Kim 2015)."
  },
  {
    "objectID": "rm-data/hw-material/report1.html#unit-production-rates",
    "href": "rm-data/hw-material/report1.html#unit-production-rates",
    "title": "The Impact of Resource Management in StarCraft: A Strategic Analysis",
    "section": "Unit Production Rates",
    "text": "Unit Production Rates\nTo illustrate the impact of resource management on unit production, we’ve created a visualization of unit production rates for different races in StarCraft.\n\n\n\n\n\n\n\n\nFigure 1: Unit Production Rates by Race in StarCraft\n\n\n\n\n\nAs shown in Figure 1, the Zerg race has the highest unit production rate, reflecting their swarm-based strategy. This aligns with the game’s design philosophy, where each race has unique strengths and weaknesses1."
  },
  {
    "objectID": "rm-data/hw-material/report1.html#resource-types-comparison",
    "href": "rm-data/hw-material/report1.html#resource-types-comparison",
    "title": "The Impact of Resource Management in StarCraft: A Strategic Analysis",
    "section": "Resource Types Comparison",
    "text": "Resource Types Comparison\nStarCraft features two primary resource types: minerals and vespene gas. Their availability and usage vary across races:\n\n\n\nTable 1: Resource Usage by Race\n\n\n\n\n\nRace\nMineral Usage\nGas Usage\nResource Dependency\n\n\n\n\nTerran\nHigh\nMedium\nBalanced\n\n\nProtoss\nMedium\nHigh\nGas-heavy\n\n\nZerg\nHigh\nLow\nMineral-heavy\n\n\n\n\n\n\nTable 1 illustrates how different races prioritize resources, influencing their strategic options and tech progression paths."
  },
  {
    "objectID": "rm-data/hw-material/report1.html#footnotes",
    "href": "rm-data/hw-material/report1.html#footnotes",
    "title": "The Impact of Resource Management in StarCraft: A Strategic Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis design approach contributes to StarCraft’s enduring popularity in esports and casual gaming circles.↩︎"
  },
  {
    "objectID": "rm-data/homework.html#hw02",
    "href": "rm-data/homework.html#hw02",
    "title": "HomeWorks",
    "section": "Homework 2",
    "text": "Homework 2"
  },
  {
    "objectID": "rm-data/homework.html#hw03",
    "href": "rm-data/homework.html#hw03",
    "title": "HomeWorks",
    "section": "Homework 3",
    "text": "Homework 3"
  },
  {
    "objectID": "rm-data/homework.html#hw04",
    "href": "rm-data/homework.html#hw04",
    "title": "HomeWorks",
    "section": "Homework 4",
    "text": "Homework 4"
  },
  {
    "objectID": "rm-data/homework.html#hw05",
    "href": "rm-data/homework.html#hw05",
    "title": "HomeWorks",
    "section": "Homework 5",
    "text": "Homework 5"
  },
  {
    "objectID": "rm-data/homework.html#hw06",
    "href": "rm-data/homework.html#hw06",
    "title": "HomeWorks",
    "section": "Homework 6",
    "text": "Homework 6"
  },
  {
    "objectID": "rm-data/homework.html#hw07",
    "href": "rm-data/homework.html#hw07",
    "title": "HomeWorks",
    "section": "Homework 7",
    "text": "Homework 7"
  },
  {
    "objectID": "rm-data/homework.html#hw08",
    "href": "rm-data/homework.html#hw08",
    "title": "HomeWorks",
    "section": "Homework 8",
    "text": "Homework 8"
  },
  {
    "objectID": "rm-data/homework.html#hw09",
    "href": "rm-data/homework.html#hw09",
    "title": "HomeWorks",
    "section": "Homework 9",
    "text": "Homework 9"
  },
  {
    "objectID": "rm-data/homework.html#hw10",
    "href": "rm-data/homework.html#hw10",
    "title": "HomeWorks",
    "section": "Homework 10",
    "text": "Homework 10"
  },
  {
    "objectID": "rm-data/homework.html#hw11",
    "href": "rm-data/homework.html#hw11",
    "title": "HomeWorks",
    "section": "Homework 11",
    "text": "Homework 11"
  },
  {
    "objectID": "rm-data/homework.html#hw12",
    "href": "rm-data/homework.html#hw12",
    "title": "HomeWorks",
    "section": "Homework 12",
    "text": "Homework 12"
  },
  {
    "objectID": "rm-data/homework.html#hw13",
    "href": "rm-data/homework.html#hw13",
    "title": "HomeWorks",
    "section": "Homework 13",
    "text": "Homework 13"
  },
  {
    "objectID": "rm-data/hw-material/report2.html",
    "href": "rm-data/hw-material/report2.html",
    "title": "The Strategic Depth of StarCraft and Its Esports Legacy",
    "section": "",
    "text": "StarCraft, a real-time strategy game developed by Blizzard Entertainment, has left an indelible mark on the esports landscape. This report explores the strategic complexity of StarCraft and its enduring impact on competitive gaming. We will examine the game’s mechanics, its influence on player decision-making, and its role in shaping the modern esports industry."
  },
  {
    "objectID": "rm-data/hw-material/report2.html#player-decision-making-and-apm",
    "href": "rm-data/hw-material/report2.html#player-decision-making-and-apm",
    "title": "The Strategic Depth of StarCraft and Its Esports Legacy",
    "section": "Player Decision-Making and APM",
    "text": "Player Decision-Making and APM\nOne of the key metrics in competitive StarCraft is Actions Per Minute (APM). This measure reflects a player’s ability to execute complex strategies rapidly. Kim et al. (2016) found a strong correlation between APM and player skill level. The following figure illustrates this relationship:\n\n\n\n\n\n\n\n\nFigure 1: Correlation between APM and Player Skill Rating\n\n\n\n\n\nAs shown in Figure 1, there is a clear positive correlation between a player’s skill rating and their APM. This relationship highlights the importance of both strategic thinking and mechanical skill in competitive StarCraft."
  },
  {
    "objectID": "rm-data/hw-material/report2.html#starcraft-tournaments-and-prize-pools",
    "href": "rm-data/hw-material/report2.html#starcraft-tournaments-and-prize-pools",
    "title": "The Strategic Depth of StarCraft and Its Esports Legacy",
    "section": "StarCraft Tournaments and Prize Pools",
    "text": "StarCraft Tournaments and Prize Pools\nThe popularity of StarCraft as an esport is evident in the number and scale of tournaments organized worldwide. The following table shows the top StarCraft tournaments by prize pool:\n\n\n\nTable 1: Top StarCraft Tournaments by Prize Pool\n\n\n\n\n\nTournament Name\nYear\nPrize Pool (USD)\nWinner\n\n\n\n\nWCG 2005\n2005\n$75,000\nLi “Sky” Xiaofeng\n\n\nBlizzCon 2007\n2007\n$100,000\nYoan “ToD” Merlo\n\n\nWCG 2009\n2009\n$200,000\nJae Ho “Moon” Jang\n\n\n\n\n\n\nAs seen in Table 1, the prize pools for StarCraft tournaments have grown significantly over time, reflecting the game’s increasing popularity and the growth of the esports industry as a whole."
  },
  {
    "objectID": "rm-data/hw-material/report2.html#footnotes",
    "href": "rm-data/hw-material/report2.html#footnotes",
    "title": "The Strategic Depth of StarCraft and Its Esports Legacy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGame-tree complexity is a measure used in game theory to quantify the number of possible game states in a given game. It provides a way to compare the complexity of different games mathematically.↩︎"
  },
  {
    "objectID": "rm-data/hw-material/report4.html",
    "href": "rm-data/hw-material/report4.html",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "",
    "text": "This report explores the emerging genre of LitRPG (Literary Role-Playing Game), a unique fusion of literature and gaming elements. We will examine the key characteristics of LitRPG, its growing popularity, and its impact on both the literary and gaming worlds. The report includes an analysis of typical LitRPG progression systems, reader engagement statistics, and future trends in the genre."
  },
  {
    "objectID": "rm-data/hw-material/report4.html#defining-litrpg",
    "href": "rm-data/hw-material/report4.html#defining-litrpg",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "Defining LitRPG",
    "text": "Defining LitRPG\nLitRPG is a literary genre that incorporates elements of role-playing games into the narrative structure. According to Johnson (2021), the genre typically features characters progressing through a game-like world, complete with statistics, levels, and skill trees. This unique blend of storytelling and gaming mechanics has led to a surge in popularity among readers who enjoy both literature and video games."
  },
  {
    "objectID": "rm-data/hw-material/report4.html#character-progression-in-litrpg",
    "href": "rm-data/hw-material/report4.html#character-progression-in-litrpg",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "Character Progression in LitRPG",
    "text": "Character Progression in LitRPG\nOne of the key features of LitRPG is the quantifiable progression of characters. This is often represented by a character’s stats or attributes, which can be expressed mathematically. A common formula for calculating a character’s overall power level in many LitRPG novels is:\n\\[\nPower Level = \\sqrt{(Strength + Agility) \\times Intelligence} \\times Level\n\\tag{1}\\]\nThis equation (Equation 1) demonstrates how various attributes contribute to a character’s overall capabilities, providing readers with a tangible sense of growth and achievement."
  },
  {
    "objectID": "rm-data/hw-material/report4.html#reader-engagement-in-litrpg",
    "href": "rm-data/hw-material/report4.html#reader-engagement-in-litrpg",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "Reader Engagement in LitRPG",
    "text": "Reader Engagement in LitRPG\nThe unique structure of LitRPG novels has led to high levels of reader engagement. To illustrate this, we conducted a survey of 1000 LitRPG readers, asking them to rate their engagement levels compared to traditional fantasy novels.\n\n\n\n\n\n\n\n\nFigure 1: Reader Engagement: LitRPG vs Traditional Fantasy\n\n\n\n\n\nAs shown in Figure 1, LitRPG novels tend to generate higher levels of reader engagement compared to traditional fantasy novels. This increased engagement can be attributed to the interactive elements and clear progression systems inherent in LitRPG stories."
  },
  {
    "objectID": "rm-data/hw-material/report4.html#popular-litrpg-subgenres",
    "href": "rm-data/hw-material/report4.html#popular-litrpg-subgenres",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "Popular LitRPG Subgenres",
    "text": "Popular LitRPG Subgenres\nThe LitRPG genre has spawned several popular subgenres, each with its own unique characteristics and fan base. The table below outlines some of the most prominent subgenres:\n\n\n\nTable 1: Popular LitRPG Subgenres\n\n\n\n\n\nSubgenre\nDescription\nPopularity Rating\n\n\n\n\nDungeon Core\nProtagonist is a dungeon\n8/10\n\n\nVR LitRPG\nSet in virtual reality\n9/10\n\n\nApocalypse LitRPG\nReal world becomes game-like\n7/10\n\n\nCultivation LitRPG\nBased on Eastern cultivation novels\n8/10\n\n\n\n\n\n\nAs seen in Table 1, VR LitRPG is currently the most popular subgenre, likely due to its relatability and connection to current technological trends1."
  },
  {
    "objectID": "rm-data/hw-material/report4.html#footnotes",
    "href": "rm-data/hw-material/report4.html#footnotes",
    "title": "The Rise of LitRPG: Blending Literature and Gaming",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe popularity of VR LitRPG may also be influenced by the growing interest in virtual reality technology in the real world.↩︎"
  },
  {
    "objectID": "rm-data/hw-material/report6.html",
    "href": "rm-data/hw-material/report6.html",
    "title": "Economic Dynamics in Eric Ugland’s ‘The Good Guys’ Series",
    "section": "",
    "text": "This report examines the economic systems and dynamics present in Eric Ugland’s popular LitRPG series, “The Good Guys.” We will explore how the author integrates economic principles into the game-like world, analyzing their impact on character development and plot progression. The report will include a mathematical model of the in-game economy, a visual representation of economic trends, and a comparative analysis of different economic zones within the series."
  },
  {
    "objectID": "rm-data/hw-material/report6.html#the-mathematical-foundation-of-the-in-game-economy",
    "href": "rm-data/hw-material/report6.html#the-mathematical-foundation-of-the-in-game-economy",
    "title": "Economic Dynamics in Eric Ugland’s ‘The Good Guys’ Series",
    "section": "The Mathematical Foundation of the In-Game Economy",
    "text": "The Mathematical Foundation of the In-Game Economy\nIn “The Good Guys,” the in-game economy is governed by a complex system of resource generation, currency valuation, and market dynamics. We can model the basic economic growth within the game using a modified version of the Solow-Swan model, as shown in Equation 1:\n\\[\nY(t) = K(t)^\\alpha (A(t)L(t))^{1-\\alpha} e^{\\beta Q(t)}\n\\tag{1}\\]\nWhere:\n\nY(t) is the total production in the game world at time t\nK(t) is the capital stock\nA(t) is the level of technology\nL(t) is the labor force (players and NPCs)\nQ(t) represents the impact of quests and missions\n\\(\\alpha\\) and \\(\\beta\\) are constants representing the elasticity of output with respect to capital and quests, respectively\n\nThis equation demonstrates how the unique elements of the game world, such as quests (Q), interact with traditional economic factors to drive growth and development."
  },
  {
    "objectID": "rm-data/hw-material/report6.html#economic-trends-across-game-zones",
    "href": "rm-data/hw-material/report6.html#economic-trends-across-game-zones",
    "title": "Economic Dynamics in Eric Ugland’s ‘The Good Guys’ Series",
    "section": "Economic Trends Across Game Zones",
    "text": "Economic Trends Across Game Zones\nTo visualize the economic disparities between different zones in the game world, we’ve compiled data on average player wealth across five major regions. Figure 1 illustrates these differences:\n\n\n\n\n\n\n\n\nFigure 1: Average Player Wealth Across Game Zones\n\n\n\n\n\nAs evident from Figure 1, there is a significant increase in average player wealth as they progress through the game zones. This economic progression serves as a motivator for players to advance in the game, mirroring real-world economic incentives1."
  },
  {
    "objectID": "rm-data/hw-material/report6.html#comparative-analysis-of-economic-systems",
    "href": "rm-data/hw-material/report6.html#comparative-analysis-of-economic-systems",
    "title": "Economic Dynamics in Eric Ugland’s ‘The Good Guys’ Series",
    "section": "Comparative Analysis of Economic Systems",
    "text": "Comparative Analysis of Economic Systems\nThe diverse economic systems present in “The Good Guys” series offer a rich ground for analysis. Table 1 provides a comparison of these systems:\n\n\n\nTable 1: Comparison of Economic Systems in “The Good Guys”\n\n\n\n\n\n\n\n\n\n\n\nEconomic System\nPrimary Currency\nMain Economic Activities\nPlayer Impact\n\n\n\n\nStarting Town\nCopper Coins\nBasic trade, simple quests\nLow\n\n\nMerchant City\nGold Coins\nComplex trade, investments\nHigh\n\n\nDragon’s Lair\nDragon Scales\nRare item trade, high-risk ventures\nVery High\n\n\n\n\n\n\nAs shown in Table 1, the economic systems become more complex and impactful as players progress, offering increasing opportunities for wealth accumulation and economic strategy."
  },
  {
    "objectID": "rm-data/hw-material/report6.html#footnotes",
    "href": "rm-data/hw-material/report6.html#footnotes",
    "title": "Economic Dynamics in Eric Ugland’s ‘The Good Guys’ Series",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis economic progression system is a common feature in many RPGs and LitRPG novels, serving as a form of “gamified capitalism” that keeps players engaged and motivated.↩︎"
  }
]