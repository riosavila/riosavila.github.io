{
  "hash": "31e85b3a185c7d3da5b584f3924361fd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Research Methods II\"\nsubtitle: \"Session 2: MLE & Limited Dependent Variables\"\nauthor: Fernando Rios-Avila\nformat: \n  revealjs: \n    slide-number: true\n    width: 1600\n    height: 900\n    code-fold: true\n    echo: true\n    css: styles.css \n    highlight-style: github\n---\n\n## Introduction to Maximum Likelihood Estimation\n\n- This is something we have seen before. \n- MLE is a method to estimate parameters of a model.\n  - It can be used to estimate paramaters of linear and nonlinear models\n- The idea is to find the values of the parameters that maximize the likelihood function.\n  - But what does it mean?\n\n> The likelihood function is the probability of observing the data given the parameters of the model.\n\nSo, MLE tries to maximize that probability, under the assumption that we know the distribution of the data.\n\nIn other words, we try to identify distributions! (not only conditional mean functions)\n\n# Example\n\n\n\n## Data\n\n::: {#310de7b9 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](session_2_files/figure-revealjs/cell-3-output-1.png){fig-align='center'}\n:::\n:::\n\n\n## MLE estimation\n\n- To identify the parameters of the model, we need to impose assumptions about the distribution of the data.\n- For simplicitly, lets make the assumption that the data is normally distributed.\n\n- The likelihood function for a single observation is:\n\n$$L_i(\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2}\\left(\\frac{y_i-\\mu}{\\sigma}\\right)^2 }$$\n\n- And under independent observations assumptions, the Likelihood function for the sample is:\n\n$$LL(\\mu,\\sigma) = \\prod_{i=1}^n L_i(\\mu,\\sigma)$$\n\n## Graphical representation  \n\n\n\n![](s2_fig1.png)\n\n## How Good we did?\n\n::: {#d0263e02 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](session_2_files/figure-revealjs/cell-5-output-1.png){fig-align='center'}\n:::\n:::\n\n\n# LR as an MLE\n\n- We already know that LR can be easily estimated using OLS.\n$$\\beta = (X'X)^{-1}X'y$$\n\n- But we can also estimate it using MLE.\n\n- Consider the following model:\n$$Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\epsilon_i$$\n\n- To estimate the parameters using MLE, we need to make assumptions about the distribution of the error term or the dependent variable.\n\n$$Y_i \\sim N(x_i'\\beta, \\sigma^2)$$\n\n##\n\n- Under that assumption, the likelihood function for a single observation is:\n\n$$L_i(\\beta,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2}\\left(\\frac{y_i-x_i'\\beta}{\\sigma}\\right)^2 }$$\n\n- Which can be used to construct the MLE estimator for OLS.\n\n- Plot-twist: The MLE estimator for OLS is the same as the OLS estimator.\n\n# Limited Dependent Variables\n\n## Limited Dependent Variables\n\n- Limited dependent variables are variables that are limited in their range of values.\n  - For example, binary variables, or variables that are bounded between 0 and 1.\n  - Or variables that are bounded between 0 and some positive number.\n  - Or variables bounded to take only positive values.\n  - etc\n\n- Very Special Case: Endogenous Sample Selection \n  - Looks unbounded, but you only observe a subset of the population.\n\n## Binary Data: LPM/Probit/logit \n\n- Probit and logit are two models that are used to model binary dependent variables. (Dummies)\n  - You can also use OLS (LPM), but has drawbacks\n  - You also need to make sure your Dependent Variable is binary!\n\n- When your Dep variable is binary, your goal is determine the probability of observing a 1 (success) of something to happen given a set of covariates.\n\n$$P(y_i=1|x_i) = G(x_i'\\beta)$$\n\nThe choice of $G$ is what makes the difference between LPM, a probit and logit.\n\n## Probit/Logit\n\n:::: {.columns}\n::: {.column}\n\n**LOGIT**\n\n$$G(Z) = \\frac{e^{Z}}{1+e^{Z}} = \\Lambda(Z)$$\n\n:::\n::: {.column}\n\n**Probit**\n\n$$G(Z) = \\int_{-\\infty}^z \\phi(v) dv = \\Phi(Z)$$\n:::\n\n::::\n\n- Both make sure that $0\\leq G(Z) \\leq 1$, which doesnt happen with LPM ($G(Z)=Z$)\n\n- And with this, we can use MLE to estimate the parameters of the model.\n\n$$LL(\\beta) = \\prod_{i=1}^n G(x_i'\\beta)^{y_i} (1-G(x_i'\\beta))^{1-y_i}$$\n\n## \n\nOne could also think of the probit and logit as a transformation of a latent variable $Y^*$.\n\n$$Y^*_i = x_i'\\beta + \\epsilon_i$$\n\n- The latent variable is not observed. However, when $Y^*_i>0$, we observe $Y_i=1$.\n\nHere the probabilty of observing a $Y_i=1$ is:\n\n$$\\begin{aligned}\nP(y_i=1|x_i) &= P(y^*_i>0|x_i) = P(x_i'\\beta + \\epsilon_i>0|x_i) \\\\\n  &= P( \\epsilon_i>-x_i'\\beta |x_i) = 1-P( \\epsilon_i<-x_i'\\beta |x_i) \\\\\n  &= 1-G(-x_i'\\beta) \n\\end{aligned}\n$$\n\nAnd if $G'$ is symetrical (logit/probit/lpm):\n\n$$P(y_i=1|x_i) = G(x_i'\\beta)$$\n\n## Marginal Effects and testing\n\n- LPM estimates can be interpreted Directly as the change in P(y=1|X)\n- For Logit and probit, we need to compute the marginal effects.\n\n$$P(y_i=1|x_i) = G(x_i'\\beta)$$\n\n$$\\frac{\\partial P(y_i=1|x_i)}{\\partial x_{ij}} = g(x_i'\\beta)\\beta_j$$\n\n- For testing, \n  - You can use the t-test (or z-test for logit/probit) for coefficients or marginal effects\n  - Or use LR test for joint significance of a set of coefficients.\n  \n$$LR = 1- 2 (LL_ur - LL_r) \\sim \\chi^2_q$$  \n\n## Example `Stata` {.scrollable}\n\nLoad the data\n\n::: {#3c6c9460 .cell .larger execution_count=5}\n``` {.stata .cell-code code-fold=\"false\"}\nwebuse nhanes2d, clear\ndes highbp height weight age female\nsum   highbp height weight age female i.race [w=finalwgt]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nhighbp          byte    %8.0g               * High blood pressure\nheight          float   %9.0g                 Height (cm)\nweight          float   %9.0g                 Weight (kg)\nage             byte    %9.0g                 Age (years)\nfemale          byte    %8.0g      female     Female\n\n    Variable |     Obs      Weight        Mean   Std. dev.       Min        Max\n-------------+-----------------------------------------------------------------\n      highbp |  10,351   117157513    .3685423   .4824328          0          1\n      height |  10,351   117157513    168.4599   9.699111      135.5        200\n      weight |  10,351   117157513    71.90064   15.43281      30.84     175.88\n         age |  10,351   117157513    42.25264   15.50249         20         74\n      female |  10,351   117157513    .5206498   .4995975          0          1\n-------------+-----------------------------------------------------------------\n             |\n        race |\n      White  |  10,351   117157513    .8791545   .3259634          0          1\n      Black  |  10,351   117157513    .0955059   .2939267          0          1\n      Other  |  10,351   117157513    .0253396   .1571621          0          1\n```\n:::\n:::\n\n\nEstimate mode: LPM using weights\n\n::: {#13044d74 .cell .larger execution_count=6}\n``` {.stata .cell-code code-fold=\"false\"}\nreg highbp height weight age female i.race [pw=finalwgt]\n* or svy: reg highbp height weight age female i.race \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(sum of wgt is 117,157,513)\n\nLinear regression                               Number of obs     =     10,351\n                                                F(6, 10344)       =     531.97\n                                                Prob > F          =     0.0000\n                                                R-squared         =     0.2110\n                                                Root MSE          =     .42864\n\n------------------------------------------------------------------------------\n             |               Robust\n      highbp | Coefficient  std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      height |   -.006271   .0008179    -7.67   0.000    -.0078742   -.0046677\n      weight |   .0097675    .000369    26.47   0.000     .0090441    .0104909\n         age |   .0096675   .0002984    32.40   0.000     .0090826    .0102524\n      female |  -.0794025   .0142293    -5.58   0.000    -.1072948   -.0515103\n             |\n        race |\n      Black  |   .0647166   .0170488     3.80   0.000     .0312977    .0981355\n      Other  |   .0869917   .0381158     2.28   0.022     .0122775     .161706\n             |\n       _cons |    .347133   .1403729     2.47   0.013     .0719749     .622291\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nEstimate mode: Logit using weights\n\n::: {#d88b0daf .cell .larger execution_count=7}\n``` {.stata .cell-code code-fold=\"false\"}\nlogit highbp height weight age female i.race [pw=finalwgt]\n* or svy: reg highbp height weight age female i.race \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nIteration 0:   log pseudolikelihood =  -77110184  \nIteration 1:   log pseudolikelihood =  -63830529  \nIteration 2:   log pseudolikelihood =  -63604963  \nIteration 3:   log pseudolikelihood =  -63604252  \nIteration 4:   log pseudolikelihood =  -63604252  \n\nLogistic regression                                    Number of obs =  10,351\n                                                       Wald chi2(6)  = 1473.91\n                                                       Prob > chi2   =  0.0000\nLog pseudolikelihood = -63604252                       Pseudo R2     =  0.1752\n\n------------------------------------------------------------------------------\n             |               Robust\n      highbp | Coefficient  std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      height |  -.0328739   .0045382    -7.24   0.000    -.0417687   -.0239791\n      weight |   .0514503   .0022181    23.20   0.000     .0471028    .0557977\n         age |   .0496323   .0017152    28.94   0.000     .0462706     .052994\n      female |  -.4472131   .0777753    -5.75   0.000    -.5996498   -.2947764\n             |\n        race |\n      Black  |    .351346   .0915423     3.84   0.000     .1719264    .5307656\n      Other  |   .4929785   .1961652     2.51   0.012     .1085017    .8774552\n             |\n       _cons |  -.7501284   .7683899    -0.98   0.329    -2.256145    .7558881\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nJoint significance test\n\n::: {#58d614be .cell .larger execution_count=8}\n``` {.stata .cell-code code-fold=\"false\"}\ntest 2.race 3.race\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n ( 1)  [highbp]2.race = 0\n ( 2)  [highbp]3.race = 0\n\n           chi2(  2) =   20.25\n         Prob > chi2 =    0.0000\n```\n:::\n:::\n\n\nMarginal Effects: You need to use `margins` command\n\n::: {#dee91632 .cell .larger execution_count=9}\n``` {.stata .cell-code code-fold=\"false\"}\nmargins, dydx(*)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nAverage marginal effects                                Number of obs = 10,351\nModel VCE: Robust\n\nExpression: Pr(highbp), predict()\ndy/dx wrt:  height weight age female 2.race 3.race\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      height |  -.0059971   .0008145    -7.36   0.000    -.0075936   -.0044007\n      weight |    .009386   .0003507    26.76   0.000     .0086985    .0100734\n         age |   .0090543   .0002562    35.34   0.000     .0085522    .0095564\n      female |  -.0815842   .0141075    -5.78   0.000    -.1092344   -.0539339\n             |\n        race |\n      Black  |   .0654291   .0173285     3.78   0.000     .0314659    .0993923\n      Other  |   .0925816     .03772     2.45   0.014     .0186517    .1665115\n------------------------------------------------------------------------------\nNote: dy/dx for factor levels is the discrete change from the base level.\n```\n:::\n:::\n\n\nPredicted Probabilities\n\n::: {#9bd3b317 .cell .larger execution_count=10}\n``` {.stata .cell-code code-fold=\"false\"}\npredict pr_hat\nhistogram pr_hat  \ngraph export s2fig2.png, replace width(1000)\n```\n:::\n\n\nFrom here, we could also predict HighBP\n\n::: {#444d11bc .cell .larger execution_count=11}\n``` {.stata .cell-code code-fold=\"false\"}\ngen dpr_hat = pr_hat>.5\ntab dpr_hat highbp [w=finalwgt]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n           |  High blood pressure\n   dpr_hat |         0          1 |     Total\n-----------+----------------------+----------\n         0 |  61819680   20494538 |  82314218 \n         1 |  12160331   22682964 |  34843295 \n-----------+----------------------+----------\n     Total |  73980011   43177502 | 117157513 \n```\n:::\n:::\n\n\n# Tobit\n\n## Tobit\n\n- Tobit models are to analyze data with censored information.\n\n- Censored data means that the data is there... but you dont know the exact value.\n\n- For example, if you have data on income, but you only know that some people earn less than 10K, but you dont know how much less.\n\n- The fact that you can see the data, even if you do not know the exact value, helps you to estimate the parameters of the model.\n\n## Visualizing the problem \n\n\n\n![](s2fig3.png)\n\n## Visualizing the problem\n\n![](s2fig4.png)\n\n## Tobit Model\n\n- The idea of the Tobit model is \"model\" not only why $y$ changes when $X$ changes, but also why y is censored.\n  - Although you do that with the same parameters, under normality assumptions.\n\n- When the data is censored, the likelihood function is similar to a probit model, when the data is not censored, the likelihood function is similar to a linear model:\n\n$$\\begin{aligned}\nL_i(\\beta,\\sigma) &= \\Phi\\left(\\frac{y^c-x_i'\\beta}{\\sigma}\\right) \\text{if } y_i = y^c \\\\\nL_i(\\beta,\\sigma) &= \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{ -\\frac{1}{2}\\left(\\frac{y_i-x_i'\\beta}{\\sigma}\\right)^2 } \\text{if } y_i > y^c \n\\end{aligned}\n$$\n\n## Estimation `Stata`\n\nIn `Stata`, you can estimate a Tobit model using the `tobit` command.\n\n`tobit y x1 x2 x3, ll(#)` \n\n- `y`: dependent variable\n- `x1 x2 x3`: independent variable\n- `ll(#)`: is the value of the censoring point.\n\n## Visualizing the solution\n\n\n\n![](s2fig5.png)\n\n# Tobit: Interpretation\n\n## Latent Variable\n\n- The easiest to interpret is the latent variable. \n- For example, say that you are interested in the effect of education on wages, but wages are censored at 10.\n- In this case the coefficients of the Tobit model are the same as the coefficients of the linear model.\n\n`tobit y x, ll(0)`\n\n- use `margins` if you have interactions or polynomial terms.\n\n## Data is Corner Solution:\n\n- If data is corner solution, then you need to decide what to interpret.\n  - For example, say you are interested in the effect of education hours of work\n  - Hours of work cannot fall below 0.\n  - But you know education has a positive effect (on something)\n\n- Would you be interested in the effect on the probability of working?\n- The effect on hours of work for those who work?\n- The overall average effect on hours of work? (some will enter the labor force, some will work more hours)\n\n## Probability of Working\n\n$$P(y_i>0|x_i) = \\Phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right)$$\n\n`margins, dydx(x) predict(pr(0,.))`\n\n- `predict(pr(0,.))` says you are interested in the probability that data was not censored...or in this case that was not a corner solution\n\n## E(Y|Y>0,X) \n\n$$\\begin{aligned}\ny_i &= x_i'\\beta + \\epsilon_i \\ || E(|y>0,X) \\\\\nE(y_i|y_i>0,x_i) &= x_i'\\beta + \\sigma\\lambda\\left(\\frac{x_i'\\beta}{\\sigma}\\right) \\\\\n\\lambda(z) &= \\frac{\\phi(z)}{\\Phi(z)}\n\\end{aligned}\n$$\n\nThis is the expected value of the latent variable, conditional on the latent variable being positive.\n\n`margins, dydx(x) predict(e(0,.))`\n\n- `predict(e(0,.))` says you are interested in the expected change only for those who currently work.\n\n## E(Y|X) \n\n$$\\begin{aligned}\nE(y_i|x_i) &= E(y_i|y_i>0,x_i) * P(y_i>0|x_i) + 0 * (1-P(y_i>0|x_i)) \\\\\nE(y_i|x_i) &= \\Phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right)\n\\left( x_i'\\beta + \\sigma\\lambda\\left(\\frac{x_i'\\beta}{\\sigma}\\right)\\right) \\\\\nE(y_i|x_i) &= \\Phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right) x_i'\\beta + \\sigma \\phi\\left(\\frac{x_i'\\beta}{\\sigma}\\right)\n\\end{aligned}\n$$ \n\n`margins, dydx(x) predict(ystar(0,.))`\n\n- `predict(ystar(0,.))` says you are interested in the average effect considering those who work and those who do not work.\n\n## Visualizing the solution\n\n\n\n![](s2fig6.png)\n\n# Sample Selection: Heckman\n\n## Exogenous Sample Selection\n\n- First: Samples already represent a selection of the population.\n\n  - however, because the selection is random, all assumptions of OLS are satisfied. (if they are true for the population.)\n  \n- Second: Some times selection may not be random, but based on observed (and control) characteristics\n\n  - Not a problem either. Since you could at least say something for those you observe. (if you have the right variables)\n  - This was exogenous sample selection.\n\nie: You want to estimate the effect of education on wages, but you only have data for highly educated people.\n\n##\n\n\n\n![](s2fig7.png)\n\n## Endogenous Sample Selection\n\n- Third: Selection may be based on unobserved characteristics.\n\n  - This is a problem. The reason why we do not observe data is for unknown reasons (part of the error).\n  - Because we cannot control for it, it will bias our estimates. (like omitted variable bias)\n  - This is endogenous sample selection.\n\nie: \n  - You want to estimate the effect of education on wages, but you only have data for those who work.\n  - Those who work do so because they may have been offer higher wages for unknown reasons. (high skill? high motivation? )\n\n##\n\n\n\n![](s2fig8.png)\n\n## Heckman Selection Model\n\n- The Heckman selection model is an estimation method that allows you to correct a specific kind of endogenous sample selection.\n\n- Consider the following model:\n\n$$y_i = x_i'\\beta + \\epsilon_i$$\n\n- In absence of selection, we can assume standard assumtions and estimate the model using OLS.\n\n- However, if we have endogenous selection, usually means that we have a second equation that determines the selection.\n\n$$s_i^* = z_i'\\gamma + \\eta_i$$\n\n\n## \n\nThe Full model:\n\n$$\\begin{aligned}\ny_i &= x_i'\\beta + \\epsilon_i \\text{ if } s_i^*>0 \\\\\ns_i^* &= z_i'\\gamma + \\eta_i \\\\\n\\epsilon &\\sim N(0,\\sigma_\\epsilon) \\\\\n\\eta &\\sim N(0,1) \\\\\ncorr(\\epsilon,\\eta) &= \\rho\n\\end{aligned}\n$$ \n\n- $x_i$ and $z_i$ are not necessarily the same. But they are exogenous to the error terms.\n- The selection equation depends on observable $z_i$ and unobservable $\\eta_i$ factors.\n- The unobserable factors are correlated with the error term of the main equation. \n- The problem: if $\\rho\\neq 0$ then $E(\\epsilon|s_i^*>0,x_i) \\neq 0$.\n\n## Solution\n\n- The solution is to \"control\" for the unobserved factors that are correlated with $\\epsilon$. \n\n$$\\begin{aligned}\ny_i &= x_i'\\beta + \\epsilon_i \\ || \\ E( * |x_i,s_i^*>0) \\\\\nE(y_i|x_i,s_i^*>0) &= x_i'\\beta + E(\\epsilon_i|x,z,\\eta, s_i^*>0) \\\\\n &= x_i'\\beta + E(\\epsilon_i|\\eta,s_i^*>0) \\\\\n  &= x_i'\\beta + \\rho \\frac{\\phi(z_i'\\gamma)}{\\Phi(z_i'\\gamma)} \\\\\n  &= x_i'\\beta + \\rho \\lambda (z_i'\\gamma) \\\\\n\\end{aligned}\n$$\n\nThus the new model is:\n\n$$y_i  = x_i'\\beta + \\rho \\lambda (z_i'\\gamma) + \\varepsilon_i$$\n\nwhere $\\gamma$ is estimated using a probit model.\n\n## Implementation\n\n- Two options:\n  1. Estimate both outcome and selection equation jointly using MLE.\n    - Requires careful setup of the likelihood function.\n    - Imposes the assumption of joint normality of the error terms.\n  2. Estimate it using a two-step procedure.(Heckit)\n    - Estimate the selection equation using probit. $z_i'\\gamma$\n    - Estimate the outcome equation using OLS, inclusing inverse mills ratio. $\\lambda (z_i'\\gamma)$\n    - Std Errs need to be corrected \n\n- Consideration: In contrast with IV, Heckman does not require an instrument, but having one is highly recommended.\n\n## Example `Stata` {.scrollable}\n\nLets start by loading some data\n\n::: {#9247d23e .cell .larger execution_count=17}\n``` {.stata .cell-code code-fold=\"false\"}\nwebuse womenwk, clear\ndescribe\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nContains data from https://www.stata-press.com/data/r17/womenwk.dta\n Observations:         2,000                  \n    Variables:             6                  3 Mar 2020 07:43\n-------------------------------------------------------------------------------\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\ncounty          byte    %9.0g                 County of residence\nage             byte    %8.0g                 Age in years\neducation       byte    %8.0g                 Years of schooling\nmarried         byte    %8.0g                 1 if married spouse present\nchildren        byte    %8.0g                 # of children under 12 years old\nwage            float   %9.0g                 Hourly wage; missing, if not\n                                                working\n-------------------------------------------------------------------------------\nSorted by: \n```\n:::\n:::\n\n\nIn `Stata`, we can use command `heckman` to estimate the Heckman selection model, but lets start by doing this manually\n\n::: {#1a8a9469 .cell .larger execution_count=18}\n``` {.stata .cell-code code-fold=\"false\"}\ngen works = (wage!=.)\n** Selection model\nprobit works married children educ age\npredict zg, xb\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nIteration 0:   log likelihood = -1266.2225  \nIteration 1:   log likelihood = -1031.4962  \nIteration 2:   log likelihood = -1027.0625  \nIteration 3:   log likelihood = -1027.0616  \nIteration 4:   log likelihood = -1027.0616  \n\nProbit regression                                       Number of obs =  2,000\n                                                        LR chi2(4)    = 478.32\n                                                        Prob > chi2   = 0.0000\nLog likelihood = -1027.0616                             Pseudo R2     = 0.1889\n\n------------------------------------------------------------------------------\n       works | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     married |   .4308575    .074208     5.81   0.000     .2854125    .5763025\n    children |   .4473249   .0287417    15.56   0.000     .3909922    .5036576\n   education |   .0583645   .0109742     5.32   0.000     .0368555    .0798735\n         age |   .0347211   .0042293     8.21   0.000     .0264318    .0430105\n       _cons |  -2.467365   .1925635   -12.81   0.000    -2.844782   -2.089948\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nThis selection equation can be interpreted the usual way\n\nThe outcome model:\n\n::: {#1105a83a .cell .larger execution_count=19}\n``` {.stata .cell-code code-fold=\"false\"}\ngen mill = normalden(zg)/normal(zg)\nreg  wage educ age mill\nest sto hkit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n      Source |       SS           df       MS      Number of obs   =     1,343\n-------------+----------------------------------   F(3, 1339)      =    173.01\n       Model |  14904.6806         3  4968.22688   Prob > F        =    0.0000\n    Residual |   38450.214     1,339  28.7156191   R-squared       =    0.2793\n-------------+----------------------------------   Adj R-squared   =    0.2777\n       Total |  53354.8946     1,342  39.7577456   Root MSE        =    5.3587\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   education |   .9825259   .0504982    19.46   0.000     .8834616     1.08159\n         age |   .2118695   .0206636    10.25   0.000      .171333     .252406\n        mill |   4.001616   .5771027     6.93   0.000     2.869492    5.133739\n       _cons |   .7340391   1.166214     0.63   0.529    -1.553766    3.021844\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nLets compare the outcome with Stata's Heckman\n\n::: {#9700093f .cell .larger execution_count=20}\n``` {.stata .cell-code code-fold=\"false\"}\nset linesize 255\nreg wag educ age\nest sto ols\nheckman wage educ age, select(works = married children educ age) twostep\nest sto hecktwo\nheckman wage educ age, select(works = married children educ age) \nest sto heckmle\n```\n:::\n\n\n::: {#dd7e103e .cell .larger execution_count=21}\n``` {.stata .cell-code code-fold=\"false\"}\nesttab ols hkit hecktwo heckmle, se(%9.3f) b(%9.3f) star(* 0.10 ** 0.05 *** 0.01) nogaps mtitle(OLS Heckit Hkm-two Hkm-mle) nonum\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n----------------------------------------------------------------------------\n                      OLS          Heckit         Hkm-two         Hkm-mle   \n----------------------------------------------------------------------------\nmain                                                                        \neducation           0.897***        0.983***        0.983***        0.990***\n                  (0.050)         (0.050)         (0.054)         (0.053)   \nage                 0.147***        0.212***        0.212***        0.213***\n                  (0.019)         (0.021)         (0.022)         (0.021)   \nmill                                4.002***                                \n                                  (0.577)                                   \n_cons               6.085***        0.734           0.734           0.486   \n                  (0.890)         (1.166)         (1.248)         (1.077)   \n----------------------------------------------------------------------------\nworks                                                                       \nmarried                                             0.431***        0.445***\n                                                  (0.074)         (0.067)   \nchildren                                            0.447***        0.439***\n                                                  (0.029)         (0.028)   \neducation                                           0.058***        0.056***\n                                                  (0.011)         (0.011)   \nage                                                 0.035***        0.037***\n                                                  (0.004)         (0.004)   \n_cons                                              -2.467***       -2.491***\n                                                  (0.193)         (0.189)   \n----------------------------------------------------------------------------\n/mills                                                                      \nlambda                                              4.002***                \n                                                  (0.607)                   \n----------------------------------------------------------------------------\n/                                                                           \nathrho                                                              0.874***\n                                                                  (0.101)   \nlnsigma                                                             1.793***\n                                                                  (0.028)   \n----------------------------------------------------------------------------\nN                    1343            1343            2000            2000   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p<0.10, ** p<0.05, *** p<0.01\n```\n:::\n:::\n\n\n## Interpretation\n\n- It depends...\n\n- but the most likely scenario is to interpret the outcomes for everyone (thus just look at coefficients of the outcome equation)\n\n- But you can also obtain effects for those who work, or the average effect. \n\n- The Mills ratio can be interpreted as the direction of the selection.\n  - If positive, then those who work are those who earn more\n  - If negative, then those who work are those who earn less\n\n## Extra example {.scrollable}\n\n::: {#9bda7db2 .cell .larger execution_count=22}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause oaxaca, clear\nreg lnwage educ exper tenure \nest sto ols\nheckman lnwage educ exper tenure  , select(lfp =educ age   married divorced kids6 kids714) \nest sto hk_mle\nheckman lnwage educ exper tenure  , select(lfp =educ age   married divorced kids6 kids714) two\nest sto hk_two\n```\n:::\n\n\n::: {#6fc14bea .cell .larger execution_count=23}\n``` {.stata .cell-code code-fold=\"false\"}\nesttab ols hk_mle hk_two, se(%9.3f) b(%9.3f) star(* 0.10 ** 0.05 *** 0.01) nogaps mtitle(OLS Heckit Hkm-two) nonum  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n------------------------------------------------------------\n                      OLS          Heckit         Hkm-two   \n------------------------------------------------------------\nmain                                                        \neduc                0.087***        0.091***        0.094***\n                  (0.005)         (0.005)         (0.006)   \nexper               0.011***        0.011***        0.010***\n                  (0.002)         (0.002)         (0.002)   \ntenure              0.008***        0.008***        0.007***\n                  (0.002)         (0.002)         (0.002)   \n_cons               2.140***        2.079***        2.024***\n                  (0.065)         (0.067)         (0.072)   \n------------------------------------------------------------\nlfp                                                         \neduc                                0.168***        0.183***\n                                  (0.025)         (0.025)   \nage                                -0.028***       -0.029***\n                                  (0.006)         (0.006)   \nmarried                            -0.853***       -0.832***\n                                  (0.191)         (0.185)   \ndivorced                           -0.324          -0.239   \n                                  (0.229)         (0.222)   \nkids6                              -0.590***       -0.573***\n                                  (0.069)         (0.070)   \nkids714                            -0.318***       -0.307***\n                                  (0.057)         (0.058)   \n_cons                               1.454***        1.313***\n                                  (0.336)         (0.355)   \n------------------------------------------------------------\n/                                                           \nathrho                              0.343***                \n                                  (0.082)                   \nlnsigma                            -0.750***                \n                                  (0.020)                   \n------------------------------------------------------------\n/mills                                                      \nlambda                                              0.293***\n                                                  (0.066)   \n------------------------------------------------------------\nN                    1434            1647            1647   \n------------------------------------------------------------\nStandard errors in parentheses\n* p<0.10, ** p<0.05, *** p<0.01\n```\n:::\n:::\n\n\n# The End...\nTil next week\n\n",
    "supporting": [
      "session_2_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}