{
  "hash": "92c066500b8df3b67ff04c9524a1d826",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Research Methods II\"\nsubtitle: \"Significance and Missing Data\"\nauthor: Fernando Rios-Avila\nformat: \n  revealjs: \n    slide-number: true\n    width: 1600\n    height: 900\n    code-fold: true\n    echo: true\n    css: styles.css \n    highlight-style: github\nexecute: \n  freeze: auto    \n---\n\n# Statistical Significance\n\n## Statistical Significance\n\n-   What is statistical significance?\n    -   Statistical significance is a way of determining if an observed effect is due to chance.\n-   We typically use statistical significance to determine if the results of a study are meaningful, using various criteria.\n    -   Is the p-value less than 0.05?\n    -   Does the 95% confidence interval include zero?\n    -   is the t-statistic greater than 1.96?\n-   But what exactly does that tell us?\n\n## \n\n### \n\n![](https://imgs.xkcd.com/comics/p_values.png)\n\n## Back to the basics\n\n-   Assume you are testing for the effectiveness of a medicine that treats the common cold.\n    -   How do you know if the treatment is effective? (i.e., Does reduce the duration of the cold?)\n    -   We make an hypothesis!\n    -   The null hypothesis is that there is no effect of the treatment. (H0: no effect)\n    -   You collect some data and find out that it reduces the duration of the cold by 1 days.\n    -   Is this a significant effect?\n\n## \n\n### Depends...\n\n-   The sample size (how many people were in the study?)\n    -   If the sample size is 10,000, then a 1 day reduction in the duration of the cold may be significant.\n    -   But if the sample size is 10, then a 1 day reduction may be due to chance.\n-   The effect size\n    -   If the effect size is 0.1 days, may not be significant.\n    -   If the effect size is 10 days, may be significant.\n\n## \n\n### Then what?\n\n-   Statistical significance is a way of determining if the observed effect is due to chance.\n\n-   To do this, we required assumptions about the distribution under the Null Hypothesis.\n\n    -   Assume that the data is normally distributed.\n    -   And that there is a 4% chance you observe a value as extreme as the one you observed\n        -   In that case you would say, the effect is significant.\n\n-   Just couple of caveats:\n\n    -   Signicance can be achieved by either measuring a \"large\" effect\n    -   or by measuring a \"small\" effect with a large sample size. (very precisely)\n\n-   Thus, finding no significance could mean the effect is noise or that the sample size is too small.\n-   Recall Hypothesis can be true or not. We only know if the data is consistent with the hypothesis or not.\n\n## \n\n### From Significance to Power\n\n-   The power of a statistical test represents the probability of detecting an effect, given that the effect is real.\n\n-   For example, say a drug has the effect of reducing the duration of the common cold by 1 day. But you do not know this\n\n-   Instead you make your hypothesis. How likely is that the effect is significant?\n\n    -   You find the effect is not significant at 10% level.\n\n-   What is happening?\n\n    -   The effect was true, yet we find no significance.\n    -   The power of the test was low. (sample size was too small)\n\n-   In General, Setting high significance levels will reduce the power of the test.\n\n# Multiple Hypothesis Testing\n\n##\n### If At First You Donâ€™t Succeed, Try, Try Again\n\n:::{.panel-tabset}\n\n## RQuestion\n\n![](images/paste-2.png)\n\n## Testing\n\n![](images/paste-3.png)\n\n## Results\n![](images/paste-5.png)\n\n:::\n\n    Cartoon from xkcd, by Randall Munroe\n\n## \n### Multiple Hypothesis Testing\n\n-   In the previous example, the \"SAME\" hypothesis was tested multiple times. \n-   Yet, it was still compared to the same significance level. Is this correct?\n\nConsider the following:\n\n- You collect 100 data points, from a normal distribution, with mean 0 and standard deviation 1.\n- You know there is only a 5% chance that the mean is greater (abs) than 0.196 (95% confidence interval)\n\nSo you run the same experiment 100 times\n    - How many times do you expect to find a mean greater than 0.196?\n    - What are the chances of finding a mean greater than 0.196 at least once?\n\n##\n###\n\n- **A1**: 5% of the time\n- **A2**: \n  - Pr of finding any \"significant\" effect in one experiment: $1 - 0.95 = 0.05$\n  - Pr of finding any \"significant\" effect in two experiments: $1-0.95^2 = 0.0975$\n  - Pr of finding no effect in 100 experiments: $1-0.95^{100}  = 0.99408$\n  \nSo if you run the experiment enough times, you are almost certain to find a \"significant\" effect.\n\n- Also, While a single experiment has a 5% chance of finding a \"significant\" effect (alpha = 0.05),  the \"alpha\" for 2 experiments is 0.0975! \n\n##\n### Controlling for Multiple Hypothesis Testing\n\n- There are various ways to control for multiple hypothesis testing.\n   \n - SIDAK = $\\alpha_{adj} = (1-\\alpha_{tg})^{1/n}$\n - BONFERRONI = $\\alpha_{adj} = \\alpha_{tg}/n$\n - HOLM = $\\alpha_{adj,i} = \\alpha_{tg}/(n-i+1)$\n\nWhere $\\alpha_{tg}$ is the target $\\alpha$ level (e.g., 0.05), and $n$ is the number of tests, and $\\alpha_{i,adj}$ is the adjusted $\\alpha$ level.\n\n> There is also Uniform Confidence Intervals (see [here](https://friosavila.github.io/stata_do/stata_do5.html))\n\n- There is a caveat. They are designed to control for Type I errors (False positives), but they increase the chances of Type II errors. (Less power)\n\n# Missing/incomplete Data\n\n## Missing Data\n\n- Missing data is a common problem in empirical research.\n- Due to various reasons, some observations may be missing.\n  - Refusal to answer a question\n  - Data entry errors/ommissions\n  - Data loss\n  - etc.\n- This can be a problem for various reasons:\n  - Missing data can produced biased and inconsistent estimates. \n  - It may also reduce sample size, and thus power. (Potentially making estimation unfeasible)\n- So what can we do?\n\n## Types of Missing Data\n\n-   Missing Completely at Random (MCAR)\n    -   The probability of missing data does not depend on any observed or unobserved data.\n    -   This is the best case scenario. (this is like sampling)\n-   Missing at Random (MAR)\n    -   The probability of missing data depends on observed data.\n    -   Second Best: Its possible to address the problem using various methods.\n-   Missing Not at Random (MNAR)\n    -   The probability of missing data depends on unobserved data.\n    -   Worst case scenario: It is usually very difficult to address \n\n## What its done, and what can be done\n\n-   Complete Case Analysis (CCA)\n    -   Drop observations with missing data.\n    -   This is the default in most statistical software.\n    -   This is a bad idea, unless the data is MCAR.\n-   Imputation\n    -   Replace missing values with a value.\n    -   This is a better idea, but it depends on the type of missing data.\n    -   Requires modeling the missing data mechanism, and outcome model.\n-   Reweighting\n    -   Weight observations to account for missing data.\n    -   Requires modeling the missing data mechanism\n\n## Reweighting\n\nConsider the following example:\n$$\\begin{aligned}\n\\text{Pop}&: y = x\\beta+ \\epsilon \\\\\n\\text{Miss Mech }&: p(nmiss|x) = F(x\\gamma) \\\\\n\\text{Miss Reg }&: m\\times y = m\\times x \\beta + m\\times \\epsilon  \n\\end{aligned}\n$$\n\n-   Where $m$ is an indicator of missingness, and $F$ is the function of missing.\n\n- Define the Weights as $w = \\frac{1}{1-p(nmiss|x)}$\n\n- Then, we could use WLS to estimate the model of interest:\n\n$$w \\times m\\times y = w \\times  m\\times x \\beta + w \\times  m\\times \\epsilon$$\n\n## Example{.scrollable}\n\n:::{.panel-tabset}\n\n## Code\n\n```stata\nfrause oaxaca, clear\ndrop if lnwage ==.\n** Modeling Missing\nreg lnwage c.(educ exper tenure female age)## c.(educ exper tenure female age)  \npredict lxb\nqui:sum lxb \nreplace lxb = normal((lxb -r(mean))/r(sd))\ngen lnwage2 = lnwage if lxb <runiform()\ngen dwage = lnwage2!=.\n** Modeling Missing data\nlogit dwage educ exper tenure female age\npredict prw, pr\ngen wgt = 1/prw\n** Estimating the model\nreg lnwage educ exper tenure female age\nreg lnwage2 educ exper tenure female age\nreg lnwage2 educ exper tenure female age [w=wgt]\n** Repeat the process 1000 times\n```\n\n## Results\n\n![](s5_fig1.png)\n\n\n:::\n\n## Imputation: Mean and Predictive Mean\n\n- The second approach is to impute the missing values. AKA Substitute the unobserved values with some prediction we can construct. \n\nConsider the case of a single variable $Z$ with missing values, and assume we have a model for $Z$:\n\n$$Z = X\\beta + \\epsilon\n$$\n\n##\n###\n\n- We could \"predict\" missing values using the mean of the observed values:\n\n$$\\hat{Z} = \\bar{Z} = \\frac{1}{n}\\sum_{i=1}^n Z_i$$\n\n- Or we could use the predicted values from the model:\n\n$$\\hat{Z} = X\\hat{\\beta}$$\n\nNeither is a good idea, even under MCAR. (Why?)\n\n- We are getting rid of ALL uncertainty (variance) in the missing values.\n\n## \n### Better Approach: Stochastic Imputation\n\n- A better approach of imputation is to use a model to predict not only the \"known\" variation (Conditional mean), but also the \"unknown\" variation (Conditional variance).\n \n- So, we can use the model to predict the missing values, but we add some noise to the prediction.\n\n$$\\tilde z = X\\hat{\\beta} + \\hat \\epsilon$$\n\n- Where $\\hat \\epsilon$ is a \"random\" residual obtain based on the model assumptions.\n- $\\tilde z$ is a stochastic imputation of $z$.\n\n##\n### Even Better: Account for the uncertainty in the model\n\n- We can also account for the uncertainty in the model by considering the uncertainty in the model parameters, and the error:\n\n$$z = X\\beta + \\epsilon\n$$\n\n- Under normality assumptions, we could estimate the model using MLE, and obtain the variance covariance matrix of the parameters.\n\n$$\n\\begin{pmatrix}\n\\hat{\\beta} \\\\ \\hat{\\sigma}^2 \\end{pmatrix} \n\\sim N\\left(\\begin{bmatrix} \\beta \\\\\n\\sigma\n\\end{bmatrix}, \\begin{bmatrix}\nV_{\\beta} & 0 \\\\\n0 & V_{\\sigma}\n\\end{bmatrix}\\right)\n$$\n\n- So, we can get $\\tilde \\beta$ and $\\tilde \\sigma$, from random draws from the distribution above, and then use them to impute the missing values.\n\n##\n### Even Better than before: Multiple Imputation\n\n- The previous methods assumed you only need one imputation to solve the Imputation problem.\n\n- One, however, may not be enough to account for the uncertainty in the imputation process.\n\n- So, we can repeat the imputation process multiple times, and obtain multiple imputed values for each missing data.\n\n- With multiple imputed values, we can estimate the model of interest multiple times, and then combine the results using Rubin's rules.\n\n##\n###\n\n- Call M the number of imputations, and $m$ the imputation index.\n\n$$\\beta_{MI} = \\frac{1}{M}\\sum_{m=1}^M \\beta_m$$\n\n$$V_{MI} = \\frac{1}{M}\\sum_{m=1}^M V_m + \\left(\\frac{M+1}{M}\\right)Var(\\beta_m)$$\n\n- Where $V_m$ is the VCV matrix of the parameters for each imputation, and $Var(\\beta_m)$ is the variance of the parameters across imputations.\n\n$$df = (M-1) \\left( 1 + \\frac{M}{M+1}\\frac{Var_m}{Var_B}\\right)^2$$\n\n## {.scrollable}\n### Example: `Stata`\n\n::: {#de439134 .cell .larger execution_count=1}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause oaxaca, clear\ndrop if lnwage ==.\n\n** Modeling Missing\nforeach i in  educ exper tenure age {\n    gen m_`i' = `i' if runiform()>.25\n}\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<style>div.jp-Notebook .datagrid-container {min-height: 448px; }</style>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n(362 missing values generated)\n(311 missing values generated)\n(387 missing values generated)\n(348 missing values generated)\n```\n:::\n:::\n\n\nSetting data for -`mi`- commands\n\n::: {#a4f4f7b1 .cell .larger execution_count=2}\n``` {.stata .cell-code code-fold=\"false\"}\nmi set wide\nmi register imputed m_*\nmi impute chain (reg) m_* = lnwage single female, add(10) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nConditional models:\n           m_exper: regress m_exper m_age m_educ m_tenure lnwage single\n                     female\n             m_age: regress m_age m_exper m_educ m_tenure lnwage single\n                     female\n            m_educ: regress m_educ m_exper m_age m_tenure lnwage single\n                     female\n          m_tenure: regress m_tenure m_exper m_age m_educ lnwage single\n                     female\n\nPerforming chained iterations ...\n\nMultivariate imputation                     Imputations =       10\nChained equations                                 added =       10\nImputed: m=1 through m=10                       updated =        0\n\nInitialization: monotone                     Iterations =      100\n                                                burn-in =       10\n\n            m_educ: linear regression\n           m_exper: linear regression\n          m_tenure: linear regression\n             m_age: linear regression\n\n------------------------------------------------------------------\n                   |               Observations per m             \n                   |----------------------------------------------\n          Variable |   Complete   Incomplete   Imputed |     Total\n-------------------+-----------------------------------+----------\n            m_educ |       1072          362       362 |      1434\n           m_exper |       1123          311       311 |      1434\n          m_tenure |       1047          387       387 |      1434\n             m_age |       1086          348       348 |      1434\n------------------------------------------------------------------\n(Complete + Incomplete = Total; Imputed is the minimum across m\n of the number of filled-in observations.)\n```\n:::\n:::\n\n\nEstmating the model(s):\n\n::: {#21da8ebf .cell .larger execution_count=3}\n``` {.stata .cell-code code-fold=\"false\"}\nmi estimate, post: regress lnwage m_* single female\nest sto m1\nregress lnwage educ exper tenure age single  female\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nMultiple-imputation estimates                   Imputations       =         10\nLinear regression                               Number of obs     =      1,434\n                                                Average RVI       =     0.3607\n                                                Largest FMI       =     0.5511\n                                                Complete DF       =       1427\nDF adjustment:   Small sample                   DF:     min       =      31.27\n                                                        avg       =     293.52\n                                                        max       =   1,277.02\nModel F test:       Equal FMI                   F(   6,  403.1)   =      63.74\nWithin VCE type:          OLS                   Prob > F          =     0.0000\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      m_educ |   .0780927   .0062465    12.50   0.000     .0656678    .0905176\n     m_exper |   .0034867    .002416     1.44   0.154    -.0013503    .0083237\n    m_tenure |   .0023203    .002857     0.81   0.423    -.0035046    .0081451\n       m_age |   .0099868   .0023905     4.18   0.000     .0052212    .0147524\n      single |    -.09733   .0309067    -3.15   0.002    -.1580571    -.036603\n      female |  -.1226849   .0255255    -4.81   0.000    -.1727613   -.0726084\n       _cons |   2.102173   .1059333    19.84   0.000     1.889132    2.315213\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     1,434\n-------------+----------------------------------   F(6, 1427)      =     86.35\n       Model |  107.645555         6  17.9409258   Prob > F        =    0.0000\n    Residual |  296.474249     1,427  .207760511   R-squared       =    0.2664\n-------------+----------------------------------   Adj R-squared   =    0.2633\n       Total |  404.119804     1,433  .282009633   Root MSE        =    .45581\n\n------------------------------------------------------------------------------\n      lnwage | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .0753085    .005253    14.34   0.000     .0650041    .0856129\n       exper |   .0026545   .0018941     1.40   0.161     -.001061    .0063701\n      tenure |   .0022932   .0019725     1.16   0.245    -.0015761    .0061625\n         age |   .0111437   .0019397     5.75   0.000     .0073388    .0149486\n      single |  -.0918932   .0292993    -3.14   0.002    -.1493674   -.0344189\n      female |  -.1289592   .0253754    -5.08   0.000    -.1787363   -.0791822\n       _cons |   2.100201   .0816356    25.73   0.000     1.940063     2.26034\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n## What about LDV models?\n\n- The method sketched above (OLS) can also be extended to other models\n \n- Consider Logit models\n\n**S1**: Estimate Logit model: $P(y=1|X) = F(X\\beta)$  \n\n**S2**: Draw $\\beta$ from the distribution, call it $\\tilde\\beta$  \n\n**S3**: Draw $y$ from a Bernoulli distribution: $y \\sim Bernoulli(F(X\\tilde\\beta))$\n\n- Similar procedures can be done for other models.\n\n## Other Methods: HotDecking\n\n-   Hotdecking is a method of imputation that uses the observed values of the data to impute the missing values.\n-   Because it uses data from the empirical distribution (observed data), it produces \"valid\" imputations for any kind of data.\n-   The idea is to find a pool of potential \"donors\" for the one with missing data. (similar observations)\n-   Then select one candidate and use its data to impute the missing values.\n\n##\n### Definition of a \"donor\"\n\n- Donors are identified as observations with similar characteristics to the one with missing data. (close to the missing observation)\n\n- Finding potential donors is easy when there is low dimensional data, but it becomes more difficult as the number of variables increases.\n\n## Example {.scrollable}\n\nImputing data for wages in `oaxaca`.\n\n::: {#af3372a3 .cell .larger execution_count=4}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause oaxaca, clear\n** ID pool of donors based on age and gender\negen id_pool = group(age female)\n** Now, for each missing observation select a \"random\" donor\ngen misswage =missing(lnwage)\nbysort id_pool (misswage):egen smp = sum(misswage==0)\nbysort id_pool: gen draw = runiformint(1, smp)\nbysort id_pool: replace lnwage = lnwage[draw] if misswage==1\nsum lnwage if misswage==0\nsum lnwage if misswage==1 \nsum lnwage \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 real changes made)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      lnwage |      1,434    3.357604    .5310458    .507681   5.259097\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      lnwage |        213    3.342183    .5482005    .507681   5.259097\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      lnwage |      1,647     3.35561    .5331507    .507681   5.259097\n```\n:::\n:::\n\n\n##\n\nWith many variables, we often estimating some distance measure and/or data reduction to ID \"close\" observations:\n\n- Propensity Score (based on logit/probit/regress).\n  \n$$D(X,X_0) = abs(G(X) - G(X_0))$$\n\n- Mahalanobis distance (based on X covariance matrix)\n\n$$D(X,X_0) = \\sqrt{(X-X_0)'\\Sigma^{-1}(X-X_0)}$$\n\n- Affinity score: based on some linear combination of variables.\n\n$$D(X,X_0) = \\frac{1}{K}\\sum_{i=1}^K \\alpha_i f\\left(\\frac{X_i - X_{0i}}{h}\\right)$$\n\n##\n\n- Once distances are estimated, donor pools can be defined based on the distance measure.\n  - Say, all observations with distance less than 0.1.\n- And the donor can be selected randomly from the pool. (or weighted by distance)\n\n- This approaches could be vary computationally intensive, because it requires estimating $N\\times N$ distances.\n\n- Complexity may be reduced by using data reduction techniques, such as PCA, FA or propensity scores\n  \n## {.scrollable}\n### Example: `Stata`\n\n\nUsing single Score (Data reduction)\n\n::: {#6ada87c7 .cell .larger execution_count=5}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause oaxaca, clear\ndrop if lnwage ==.\n// 25% of data is missing\ngen mlnwage = lnwage if runiform()>.25\ngen misswage =missing(mlnwage)\nqui:logit misswage  educ age agesq female single married\npredict psc, xb\nqui:reg mlnwage  educ age agesq female single married\npredict lnwh, xb\nqui:pca educ age agesq female single married \nqui:predict pc1, score\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n(351 missing values generated)\n```\n:::\n:::\n\n\nFor imputation, lets do something simple, Use data from the closet observation (with lower score) as donor.\n\n::: {#920b3399 .cell .larger execution_count=6}\n``` {.stata .cell-code code-fold=\"false\"}\nforeach i in psc lnwh pc1 {\n \n    drop2   lnwage_`i'\n    gen lnwage_`i' = mlnwage    \n\tsort `i'\n\treplace lnwage_`i'=lnwage_`i'[_n-1] if lnwage_`i'==. & lnwage_`i'[_n-1]!=.\n\t*replace lnwage_`i'=lnwage_`i'[_n+1] if lnwage_`i'==. & lnwage_`i'[_n+1]!=.\n    qui:_regress lnwage_`i'  educ age agesq female single married\n    matrix b`i' = e(b)\n\tmatrix coleq b`i'=`i'\n\t\n}\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nvariable lnwage_psc not found\n(351 missing values generated)\n(351 real changes made)\nvariable lnwage_lnwh not found\n(351 missing values generated)\n(351 real changes made)\nvariable lnwage_pc1 not found\n(351 missing values generated)\n(351 real changes made)\n```\n:::\n:::\n\n\nEstimate models 1000 times, and lets see results\n\n## Comparison of methods\n\n![](s5_fig7.png)\n\n\n# Till next time!\nWhat happens when not some but ALL data is missing?\n\n",
    "supporting": [
      "session_5_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}