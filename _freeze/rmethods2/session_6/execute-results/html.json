{
  "hash": "da5857a85c21cf06d2e5c2e17c9bb34b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Research Methods II\"\nsubtitle: \"Session 6: Imputation: Statistical Matching\"\nauthor: Fernando Rios-Avila\nformat: \n  revealjs: \n    slide-number: true\n    width: 1600\n    height: 900\n    code-fold: true\n    echo: true\n    css: styles.css \n    highlight-style: github\n---\n\n# Missing Data vs MISSING DATA\n\n## Missing Data\n\n-   As we described before, missing data is a problem for micro data analysis.\n    -   Reduces sample size, statistical power, and may bias estimates. (depending on the type of missingness)\n-   We have also discussed that there are few ways to deal with missing data.\n    -   Complete case analysis\n    -   Reweighting\n    -   Imputation: Prediction\n    -   Imputation: Hotdecking\n-   This methods allows you solve for missing data if data is MCAR or MAR.\n    -   with MNAR, dealing with missing data is difficult\n-   Nevertheless, you can deal with Missing data, because you have some observed data that can be used to impute it.\n\n## Types of Missing data\n\n::: panel-tabset\n## T1\n\n![](images/paste-8.png){width=\"900\"}\n\n## T2\n\n![](images/paste-9.png){width=\"900\"}\n\n## T3\n\n![](images/paste-10.png){width=\"900\"}\n:::\n\n## MISSING DATA\n\n-   What would happen if all data is missing?\n\n-   Example:\n\n    -   You are working with the CPS, but are interested in looking at the relationship between income and time use.\n        -   CPS does NOT have time use data.\n\n-   We are going for the -lion hunt-\n\n    -   You can't impute time use data\n    -   You can't use complete case analysis\n    -   You can't use reweighting\n    -   You can't use hotdecking\n    -   what do we do?\n\n## \n\n### What do we do?\n\n-   One option would be using a different data set.\n\n    -   In the US, the American Time Use Survey (ATUS) could be a good option.\n    -   But...The data has no income information!\n\n-   What if we could combine the two data sets?\n\n-   This changes the problem from Missing all data, to one of Missing Data by design.\n\n    -   Some segment of the population was asked about income, and some other segment was asked about time use.\n\n## Imputation and Statistical Matching\n\n-   If you consider the idea of combining two data sets, you can treat the problem as one of imputation.\n\n    -   You have a sample (two) that represents the population of interest.\n    -   We can reasonably assume the data is MCAR. But the data of interest is not observed at the same time.\n    -   Then, we can use the combine data to impute the missing data, using many of the approaches we have discussed before.\n\n-   And there is also another method that is more commonly used (at Levy) to deal with this problem.\n\n    -   **Statistical Matching** (aka Data Fusion).\n\n-   What does this imply?:\n\n    -   Match individuals across datasets (\"Donor\" and \"Recipient\")\n    -   Transfer information based on the matching links\n\n## Official examples:\n\nThere is a lot of work on this topic. Many statistical agencies use this approach to combine data Survey data with administrative data.\n\n- Administrative data is usually more accurate, but it is not collected for the purpose of research.\n- Survey data is collected for research purposes, but may not have accurate data in some areas (income)\n- Unless Survey Data was collected with the purpose of being linked with administrative data, one requires methods similar to statistical matching to combine both data sets.\n\n## In house Some examples:\n\n- At Levy we have used this approach to produce relevant datasets:\n  - LIMEW: Levy Institute Measure of Economic Well-Being\n    Combines Time use, wealtgh and Survey data (in addition to other aggregate data)\n\n  - LIMTIP: Levy Institute Measure of Time and Income Poverty\n    Combines ATUS, with income/consumption data \n\n# Framework    \n\n## What do we need?\n\n- Consider two data sets: $A$ and $B$.  \n- $A$ has informtion on $X$ and $Z$\n- $B$ has information on $Y$ and $Z$\n- We want a file that has $X$, $Y$ and $Z$.\n  \n## Assumptions\n\n- ($X,Y,Z$) are multivariate random variables with joint distribution $f(x,y,z)$, that represents the population of interest. \n- Both datasets are random samples from the same population of interest.\n\n  $\\frac{P_w(D=A|X,Y,Z)}{P_w(D=B|X,Y,Z)} = \\frac{P(D=A)}{P(D=B)} = 1$\n\n- Conditional Independence assumption: \n  - $Y$ and $Z$ are independent from each other given $X$.\n  $$f(x,y|z) = f(x|z)f(y|z)$$\n\n-   The goal is to combine the two data sets to produce a file that has data on $X$, $Y$ and $Z$. by  identifying $f(x,y,z)$. \n  \n## Statistical Matching: Limitations\n\n-   The quality of this identification will depend on how well the conditional independence assumption holds.\n   \n-   Because of this, synthetic datasets can't tell you much about covariances or causal relationships  \n   \n$$Cov(z,y,z) = \\begin{pmatrix} \nV(X) & \\color{red}{V(X,Y)} & V(X,Z) \\\\ \n\\color{red}{V(X,Y)'} & V(Y) & V(Y,Z) \\\\ \nV(X,Z)' & V(Y,Z)' & V(Z) \n\\end{pmatrix}\n$$\n    \nalbeit, you can impose certain bounderies on the covariance matrix.\n\n## Matching Approaches:  \n\nThere are two types of **statistical matching** procedures:\n\n::: {.panel-tabset}\n\n## Unconstrained Matching\n\n- Records from $A$ and $B$ can be used multiple times (or none) in the matching.\n  - Absurd case: One observation from $A$ is matched with all observations from $B$. \n- This is the most common approach in the literature for policy evaluation\n  \n  Pros: Uses the \"best\" candidate for the matching.\n  Cons: It may not transfer the uncoditional distribution of the data.\n\n- Does not necessarily required $A$ and $B$ to be from the same population. (weighted size)\n\n## Constraints Matching\n\n- All records from $A$ and $B$ are used once and only once in the matching. (without replacement)\n- When using weighted samples, records are matched until the weights are exhausted.\n  - Requires $A$ and $B$ to be from the same population. (weighted size)\n  \n  Pros: It transfers the unconditional distribution of the data.\n  Cons: My not use \"best\" candidate for the matching.\n  \n:::\n\n## Matching Records:\n\n- Matching records, requires defining a measure of similarity between records.\n \n- This measures can vary depending on the data type, and dimensionality of the data\n\n$$\\begin{aligned}\n\\text{ Euclidian: } d(r^A,r^b) &= \\sqrt{\\sum_i^k(x^A_i-x^B_i)^2 } \\\\\n\\text{ SdEuclidian: } d(r^A,r^b) &= \\sqrt{\\sum_i^k\\left(\\frac{x^A_i-x^B_i}{\\sigma_j}\\right)^2 } \\\\\n\\text{ Mahalanobis: } d(r^A,r^b) &= \\sqrt{(x^A-x^B)'\\Sigma_x^{-1}(x^A-x^B)} \\\\ \n\\end{aligned}\n$$\n\n- All this measures are useful when one has high dimensional data.\n \n## Matching: Reducing Dimensionality\n\n- A second alternative is to reduce data dimensionality before estimating distances.\n\n::: {.panel-tabset}\n\n## Predictive mean matching:\n\n  - Model $x = z\\beta + \\epsilon$ using $A$. \n  - Make predictions $z\\hat\\beta$ for both samples.\n  - Match records based on $z\\hat\\beta$\n  - Good results to match individuals with similar \"predicted\" income.\n  - Puts more \"weight\" on the variables used to predict the outcome.\n\n## Propensity score matching:\n\n  - Model the likehood of an observation being in $A$ using $Z$.\n  $$P(D=A|Z) = G(Z\\gamma)$$\n\n  - Make predictions $\\hat P$ or $z\\hat\\gamma$ for both samples.\n  - Match records based on $\\hat\\pi$\n  - General purpose score. \n  - May be problematic if $A$ and $B$ have very similar distributions of $Z$.  \n  - Puts more \"weight\" on the variables with different distributions between $A$ and $B$.\n     \n## PCA\n\n  - Use PCA to reduce dimensionality of $Z$ into a single index.\n    - Can use either a single dataset or both\n  - Make predictions of the first principal component $PC1$\n  - Match records based on $PC1$\n  - Puts more weight on variables that explain most of the variance in $Z$.\n\n:::\n\n## Matching: Rank Matching\n\n- Most of distance based matching is usually feasible with unconstrained matching.\n  - thus, best records are always matched.\n\n- When considering constrained matching, distance based matching may not be adecuate\n  - While first records are matched the best, last records may be matched poorly match.\n\n- A balance therefore is to use rank matching.\n  - Rank observations based on a single variable (pscore, predicted mean, etc)\n  - Match records based on rank.\n\n- No match would be \"best\", but reduces changes of poor matches.\n \n# Levy Matching Algorithm\n- At Levy, we use a constrained matching algorithm, with stratification and rank matching.\n\n## 1. Data Harmonization \n\n- Because Data files come from different data sources, they may have different variables names, coding schemes, or definitions.\n- We need to set $Z$ variables to be defined as identically as possible  in both files\n- Beyond definition harmonization, one must also be mindful of the distribution of the variables in both files.\n    - If the distribution of $Z$ is different in both files, the matching may not be adequate.\n- The weights schemes in both files should be adjusted to add up to the same population size (typically the \"recipient\" values)\n    - Weight adjustment could be done by selected strata\n\n## 2. Estimation of Matching Score\n\n- Either using full or sub (strata) samples, estimate a matching score\n  - This could be a propensity score, predicted mean, or first principal component.\n- You may want to create \"further cells\" to improve matching. (not necessarily re estimate the matching score)\n  - For example, You consider Gender as strata (two scores), but further create cells by \"age\" (5 groups)\n\n## 3. Perform the match\n\n- Using the finest definition of \"cells\", rank observations based on Matching Scores\n- Using rank, match observations till all weights are exhausted.(from either Sample)\n- \"unmatched\" observations are left for later rounds using coarser definitions of cells.\n- Matching continues until all units (recipients) are matched.\n\n## 4. Assessing the quality of the match\n\n- The idea is to compare the distribution of the \"transfered/imputed\" data with the distribution from the \"donor\" data.\n  - Overall distribution of the data will be the same by construction.\n- Compare distributions by Strata, smaller cells, or specific variables or interest.\n- Rule of thumb +/- 10% is acceptable (mean, median, Standard error).\n  - But it may depend on the variable of interest.\n  \n- One may also use other approaches like \"regression\" to compare all variables at once.\n  \n- If the distribution of the data is not adequate, one may want re-do the matching, with different \"cell\" definitions or matching scores.\n\n## Example\n\n::: {#09d431f1 .cell execution_count=1}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause wage2, clear\nset seed 312\nxtile smp = runiform()\nreplace smp=smp==1\ngen wage_s = wage if smp==1\n**three Matching scores\n** Pmm\nreg wage_s hours iq kww educ exper tenure age married black south urban sibs \npredict wageh\n** pscore\nlogit smp hours iq kww educ exper tenure age married black south urban sibs \npredict pscore, xb\n** pca\npca hours iq kww educ exper tenure age married black south urban sibs , comp(1)\npredict pc1\n\nforeach i in wageh pscore pc1 {\n\tqui:sum `i'\n\treplace `i' = (`i'-r(mean))/r(sd)\n}\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<style>div.jp-Notebook .datagrid-container {min-height: 448px; }</style>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(467 real changes made)\n(467 missing values generated)\n\n      Source |       SS           df       MS      Number of obs   =       468\n-------------+----------------------------------   F(12, 455)      =     14.83\n       Model |  24043666.1        12  2003638.84   Prob > F        =    0.0000\n    Residual |  61493527.5       455   135150.61   R-squared       =    0.2811\n-------------+----------------------------------   Adj R-squared   =    0.2621\n       Total |  85537193.6       467  183163.155   Root MSE        =    367.63\n\n------------------------------------------------------------------------------\n      wage_s | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hours |  -4.080234   2.204689    -1.85   0.065    -8.412869    .2524015\n          iq |   3.090831   1.459171     2.12   0.035     .2232806    5.958381\n         kww |   5.637987   2.888128     1.95   0.052    -.0377362    11.31371\n        educ |   53.43222   10.41882     5.13   0.000     32.95724     73.9072\n       exper |   8.816439    5.30953     1.66   0.098    -1.617804    19.25068\n      tenure |   6.327233   3.534248     1.79   0.074    -.6182417    13.27271\n         age |   10.92113   7.195943     1.52   0.130    -3.220278    25.06253\n     married |    143.306   54.35023     2.64   0.009     36.49735    250.1146\n       black |  -144.0597   60.51784    -2.38   0.018    -262.9889    -25.1306\n       south |  -37.37316    37.7745    -0.99   0.323    -111.6073    36.86096\n       urban |   200.3258   38.93558     5.15   0.000       123.81    276.8417\n        sibs |   1.881618   8.468635     0.22   0.824    -14.76087    18.52411\n       _cons |  -842.6706     273.28    -3.08   0.002    -1379.718   -305.6231\n------------------------------------------------------------------------------\n(option xb assumed; fitted values)\n\nIteration 0:   log likelihood = -648.09208  \nIteration 1:   log likelihood = -642.21625  \nIteration 2:   log likelihood = -642.21522  \nIteration 3:   log likelihood = -642.21522  \n\nLogistic regression                                     Number of obs =    935\n                                                        LR chi2(12)   =  11.75\n                                                        Prob > chi2   = 0.4657\nLog likelihood = -642.21522                             Pseudo R2     = 0.0091\n\n------------------------------------------------------------------------------\n         smp | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hours |   .0153544   .0093201     1.65   0.099    -.0029127    .0336215\n          iq |   .0024677   .0057346     0.43   0.667    -.0087719    .0137072\n         kww |  -.0095774   .0113247    -0.85   0.398    -.0317735    .0126186\n        educ |  -.0255069   .0409702    -0.62   0.534     -.105807    .0547932\n       exper |   .0256009   .0205087     1.25   0.212    -.0145955    .0657973\n      tenure |   .0131078   .0137404     0.95   0.340     -.013823    .0400385\n         age |  -.0033883   .0281317    -0.12   0.904    -.0585256    .0517489\n     married |  -.2382151   .2167632    -1.10   0.272    -.6630632    .1866331\n       black |  -.1214461   .2276112    -0.53   0.594    -.5675559    .3246637\n       south |  -.0370746   .1457381    -0.25   0.799    -.3227161    .2485669\n       urban |  -.0371736   .1495076    -0.25   0.804    -.3302031     .255856\n        sibs |   -.041735   .0313008    -1.33   0.182    -.1030835    .0196135\n       _cons |  -.1246922   1.056515    -0.12   0.906    -2.195424    1.946039\n------------------------------------------------------------------------------\n\nPrincipal components/correlation                 Number of obs    =        935\n                                                 Number of comp.  =          1\n                                                 Trace            =         12\n    Rotation: (unrotated = principal)            Rho              =     0.2112\n\n    --------------------------------------------------------------------------\n       Component |   Eigenvalue   Difference         Proportion   Cumulative\n    -------------+------------------------------------------------------------\n           Comp1 |       2.5348      .622213             0.2112       0.2112\n           Comp2 |      1.91258      .821455             0.1594       0.3706\n           Comp3 |      1.09113     .0297471             0.0909       0.4615\n           Comp4 |      1.06138     .0497587             0.0884       0.5500\n           Comp5 |      1.01162     .0867533             0.0843       0.6343\n           Comp6 |      .924871     .0639678             0.0771       0.7114\n           Comp7 |      .860903     .0907871             0.0717       0.7831\n           Comp8 |      .770116      .144885             0.0642       0.8473\n           Comp9 |      .625231      .119791             0.0521       0.8994\n          Comp10 |       .50544     .0919858             0.0421       0.9415\n          Comp11 |      .413454      .124988             0.0345       0.9760\n          Comp12 |      .288466            .             0.0240       1.0000\n    --------------------------------------------------------------------------\n\nPrincipal components (eigenvectors) \n\n    --------------------------------------\n        Variable |    Comp1 | Unexplained \n    -------------+----------+-------------\n           hours |   0.1320 |       .9558 \n              iq |   0.4921 |       .3863 \n             kww |   0.4220 |       .5486 \n            educ |   0.4555 |        .474 \n           exper |  -0.2161 |       .8817 \n          tenure |   0.0463 |       .9946 \n             age |   0.0527 |        .993 \n         married |   0.0053 |       .9999 \n           black |  -0.3722 |       .6488 \n           south |  -0.2076 |       .8908 \n           urban |   0.0723 |       .9868 \n            sibs |  -0.3411 |       .7051 \n    --------------------------------------\n(score assumed)\n\nScoring coefficients \n    sum of squares(column-loading) = 1\n\n    ------------------------\n        Variable |    Comp1 \n    -------------+----------\n           hours |   0.1320 \n              iq |   0.4921 \n             kww |   0.4220 \n            educ |   0.4555 \n           exper |  -0.2161 \n          tenure |   0.0463 \n             age |   0.0527 \n         married |   0.0053 \n           black |  -0.3722 \n           south |  -0.2076 \n           urban |   0.0723 \n            sibs |  -0.3411 \n    ------------------------\n(935 real changes made)\n(935 real changes made)\n(935 real changes made)\n```\n:::\n:::\n\n\nNext we create ranks for each observation, assuming no stratification.\n\n::: {#ae3f8e27 .cell execution_count=2}\n``` {.stata .cell-code}\nbysort smp (wageh) :gen rnk1=_n\nbysort smp (pscore):gen rnk2=_n\nbysort smp (pc1)   :gen rnk3=_n\n```\n:::\n\n\nFinally, the imputation. Simply \"matching\" information from the donor to the recipient.\n\n::: {#54f7f169 .cell execution_count=3}\n``` {.stata .cell-code code-fold=\"false\"}\n* Imputation\nclonevar wage1 = wage_s\nclonevar wage2 = wage_s\nclonevar wage3 = wage_s\n\ngsort -smp rnk1\nreplace wage1 = wage_s[rnk1] if smp==0\n\ngsort -smp rnk2\nreplace wage2 = wage_s[rnk2] if smp==0\n\ngsort -smp rnk3\nreplace wage3 = wage_s[rnk3] if smp==0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(467 missing values generated)\n(467 missing values generated)\n(467 missing values generated)\n(467 real changes made)\n(467 real changes made)\n(467 real changes made)\n```\n:::\n:::\n\n\nSimple quality assessment. \n\n::: {#29d1a599 .cell execution_count=4}\n``` {.stata .cell-code code-fold=\"false\"}\nqui:reg wage hours iq kww educ exper tenure age married black south if smp==0\nest sto m1\nqui:reg wage1 hours iq kww educ exper tenure age married black south if smp==0\nest sto m2\nqui:reg wage2 hours iq kww educ exper tenure age married black south if smp==0\nest sto m3\nqui:reg wage3 hours iq kww educ exper tenure age married black south if smp==0\nest sto m4\nesttab m1 m2 m3 m4 , se mtitle(True Wageh pscore pca)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                     True           Wageh          pscore             pca   \n----------------------------------------------------------------------------\nhours              -3.076          -5.508*          0.208          -0.971   \n                  (2.482)         (2.678)         (3.109)         (2.825)   \n\niq                  2.579           0.343          -2.775           3.653*  \n                  (1.411)         (1.522)         (1.767)         (1.605)   \n\nkww                 5.410*          8.750**         1.954           7.545*  \n                  (2.745)         (2.961)         (3.438)         (3.124)   \n\neduc                46.14***        77.82***        8.331           44.53***\n                  (10.18)         (10.98)         (12.75)         (11.58)   \n\nexper               11.66*          15.17**        -2.006           5.108   \n                  (4.980)         (5.372)         (6.237)         (5.667)   \n\ntenure              3.844           5.238          -0.511          0.0757   \n                  (3.332)         (3.595)         (4.174)         (3.792)   \n\nage                -0.945          -9.667          -10.91          -5.380   \n                  (6.905)         (7.449)         (8.648)         (7.858)   \n\nmarried             187.2***        165.6**         89.99           38.58   \n                  (54.26)         (58.54)         (67.96)         (61.75)   \n\nblack              -64.09          -99.07           46.48          -39.87   \n                  (52.43)         (56.57)         (65.67)         (59.67)   \n\nsouth              -80.79*         -74.14           9.854          -120.9** \n                  (35.19)         (37.96)         (44.07)         (40.04)   \n\n_cons              -254.2          -199.6          1349.4***       -104.9   \n                  (251.2)         (271.0)         (314.6)         (285.9)   \n----------------------------------------------------------------------------\nN                     467             467             467             467   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n:::\n:::\n\n\n# Next Class: Micro Simulation\njust more imputations\n\n",
    "supporting": [
      "session_6_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}