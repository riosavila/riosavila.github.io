{
  "hash": "1632247a19795a8996c04e547022ff84",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Modeling Probabilities\"\nsubtitle: \"to be or not to be\"\nauthor: \n  - name: Fernando Rios-Avila\n    affiliation: Levy Economics Institute  \ndate: last-modified\ndate-format: long\nformat:\n  revealjs:\n    theme: [ clean2.scss]\n    slide-number: true\n    footer: \"*Rios-Avila and Cia*\"\n    width:  1300\n    height: 675\njupyter: nbstata    \nexecute: \n  cache: true\n  freeze: auto\n---  \n\n\n\n\n## What we will see today\n\n- Linear Probability Model - LPM\n- Logit & probit\n- Goodness of fit\n- Diagnostics\n- Summary\n\n## Motivation\n\n- What are the health benefits of not smoking? Considering the 50+ population, we can investigate if differences in smoking habits are correlated with differences in health status.\n  - good health vs bad health\n  \n## Binary events\n\n- Some outcomes are things that either happen or don't happen, which can be captured by binary variables\n  - e.g. a person is healthy or not, a person is employed or not, a person is a smoker or not. We dont see a person that is half healthy, half employed, or half a smoker.\n- How can we model these events?\n  - We have seen this before. Instead of modeling the value itself, we model the probability of the event happening. \n\n$$E[y] = P[y = 1]$$\n\n- In fact, the average of a 0–1 event is the probability of that event happening. Which can also be estimated as conditional probabilities:\n\n$$E[y|x_1, x_2, ...] = P[y = 1|x_1, x_2, ...]$$\n\n- Good news, we can use the same tools we have been using to model these probabilities.\n  \n# Modelling events: LPM\n\n## LPM: Linear probability model\n\n- Linear Probability Model (LPM) is a **linear regression** with a binary dependent variable\n  - It has the goal of modeling the probability of an event happening\n- A linear regressions with binary dependent variables shows:\n  - differences in expected $y$ by $x$, represent diferences in probability of $y = 1$ by $x$.\n- Introduce notation for probability:\n  $y^P = P[y = 1|x_1, x_2, . . .]$\n- Linear probability model (LPM) regression is\n  $y^P = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$\n\n## LPM: Interpretation\n\n$$y^P = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$$\n\n- So far nothing changes in terms of modelling or estimation. However, the interpretation of the coefficients changes.\n  - $y^P$ denotes the probability that the dependent variable is one, conditional on the right-hand-side variables of the model.\n  - $\\beta_0$ shows the *predicted* probability of $y$ if all $x$ are zero.\n  - $\\beta_1$ shows the difference in the probability that $y = 1$ for observations that are different in $x_1$ but are the same in terms of $x_2$. (ceteris paribus)\n\n## LPM: Modelling\n\n- Linear probability model (**LPM**) can be estimated using OLS. (just like linear regression)\n- We can use all transformations in $x$, that we used before:\n  - Log, Polinomials, Splines, dummies, interactions, etc. They all work.\n- All formulae and interpretations for standard errors, confidence intervals, hypotheses and p-values of tests are the same.\n- [**IMPORTANT**]{.red} Heteroskedasticity robust error are essential in this case!\n  - By construction LPMs are heteroskedastic!\n  - And ignoring this fact will lead to biased standard errors and confidence intervals.\n  \n## LPM: Prediction\n\n- Predicted values - $\\hat{y}_P$ - may be problematic. Although they are calculated the same way, they need to be interpreted as **probabilities**.\n\n$$\\hat{y}^P = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2$$\n\n- Predicted values **need to be** between 0 and 1 because they are probabilities\n- But in LPM, predictions may be below 0 and above 1. No formal bounds in the model.\n  - more likely than certain, less likely than impossible\n  \n## LPM: Prediction\n\n- When to get worried?:\n  - With continuous variables that can take any value (GDP, Population, sales, etc), this could be a serious issue (extrapolation)\n    - We need to check if predictions are within the 0-1 range at least \"in-sample\". But, this is not a guarantee that it will be the case \"out-of-sample\".\n  - With binary variables, no problem ('saturated models') (interpolation)\n    - Not problem because \"simple\" means will always be between 0 and 1.\n- So, a problem if goal is prediction!\n- Not a big issue for inference → uncover patterns of association.\n  - But it may give biased estimates...(in **theory**)\n\n## CS: Does smoking pose a health risk?\n\n> This is on of the few datasets from the book that is not directly available from their website. \n> If interested, you need to go over the repository, and follow the instructions to access the data.\n\nThus, we will use a different dataset to illustrate the concepts.\n\n## CS: Does smoking during pregnancy affect birth weight?\n\n- The question is whether, and by how much, smoking during pregnancy affects the likelihood that a baby is born with low birth weight.\n- We will use \"lbw\" dataset from Stata's example datasets.\n- The dataset contains information on 189 observations of mothers and their newborns.\n  -  `low` is a binary variable indicating whether the baby was born with low birth weight (<2500gr <5.5lbs).\n  -  `smoke` is a binary variable indicating whether the mother smoked during pregnancy.\n     \n## Data\n\n- $low = 1$ if baby was born with low birth weight\n- $low = 0$ if baby was born with normal weight\n  -Some demographic information on all individual\n- We exclude women <15 years old and >40 years old  \n- Also exclude women with Weight > 200lbs (before pregnancy)\n  \n## LPM: in Stata\n\n- Start with a simple univariate model: $P[low|smoke] = \\alpha + \\beta[smoke]$\n\n::: {#92d6978f .cell execution_count=1}\n``` {.stata .cell-code code-fold=\"false\"}\nwebuse lbw, clear\ndrop if age < 15 | age > 40 | lwt > 200\nreg low smoke, robust\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<style>div.jp-Notebook .datagrid-container {min-height: 448px; }</style>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(Hosmer & Lemeshow data)\n(10 observations deleted)\n\nLinear regression                               Number of obs     =        179\n                                                F(1, 177)         =       4.70\n                                                Prob > F          =     0.0316\n                                                R-squared         =     0.0272\n                                                Root MSE          =     .46208\n\n------------------------------------------------------------------------------\n             |               Robust\n         low | Coefficient  std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       smoke |    .157405   .0726412     2.17   0.032     .0140507    .3007593\n       _cons |   .2568807   .0420845     6.10   0.000     .1738289    .3399326\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n## LPM: Interpretation\n\nInterpretation: \n\n- The coefficient on `smoker` shows the difference in the probability of a baby.\n- Babies are **15.7** percentage points more likely to be born with low birth weight if the mother smoked during pregnancy.\n  - Are you comparing Apples to apples?\n  - Lets add additional controls to capture other factors\n\n## LPM: with many regressors I\n\n- Multiple regression – closer to causality\n- Compare women who are very similar in many respects but are different in smoking habits\n- Smokers / non-smokers – different in many other behaviors and conditions:\n  - personal traits (age, race)\n  - behavior pre-pregnancy (Pre-pregnancy weight)\n  - Medical history (History of Hypertension)\n  - background for pregnancy (Number of prenatal visits, Previous premature labor)\n\n## LPM with many regressors II\n\n- May also consider functional form selection or interactions\n- Trial and error, or theory-based\n- Useful to check bivariate relationships (scatter plots, Lpoly, correlations)\n  - For now, assume linear relationships\n   \n## LPM with many regressors III\n\n``` {.stata .cell-code code-fold=\"true\"}\nqui {\ngen any_premature = ptl >0\nren ftv no_of_visits_1tr\nren ht hist_hyper\nren lwt wgt_bef_preg\nreg low smoke age i.race any_premature hist_hyper  no_of_visits_1tr wgt_bef_preg, robust nohead\n}\nest store lpm_results\nesttab lpm_results,   se  wide nonumber ///\ncollabel(b se) md drop(1.race) nomtitle b(3) nonotes\n```\n\n\n|              |            b                  |           se |\n| ------------ | :---------------------------: | :----------: |\n| smoke        |        0.151<sup>\\*</sup>     |      (0.075) |\n| age          |       -0.005                  |      (0.007) |\n| 2.race       |        0.210                  |      (0.112) |\n| 3.race       |        0.126                  |      (0.079) |\n| any\\_premature |        0.275<sup>\\*\\*</sup>   |      (0.097) |\n| hist\\_hyper   |        0.396<sup>\\*\\*</sup>   |      (0.143) |\n| no\\_of\\_visits\\_1tr |       -0.005                  |      (0.034) |\n| wgt\\_bef\\_preg |       -0.002                  |      (0.002) |\n| \\_cons       |        0.464                  |      (0.252) |\n| *N*          |          179                  |              |\n\n\n\n[Robust standard errors in parentheses]{.table-note} \n[<sup>\\*</sup> *p* < 0.05, <sup>\\*\\*</sup> *p* < 0.01, <sup>\\*\\*\\*</sup> *p* < 0.001]{.table-note }\n\n## Detour: Regression Tables\n\n- If need to show many explanatory variables\n- Do not show table 12*2 rows, people will not see it.\n  - Avoid copy pasting from your document! Those tables are unwieldy.\n- Either only show selected variables (smoke + 2-3 others)\n- Or may need to create two columns. (a bit more work)\n  - In my case, Wide format did the trick.\n- Make site you have title, N of observations, footnote on SE, stars.\n- SE, stars: many different notations. Check carefully.\n  - `esttab` default is $p^{***}= p<0.001$, $0.01$ and $0.05$ \n  - In papers there is $p^{***}=p<0.01$, $0.05$ and $0.1$.\n\n## Does smoking pose a health risk for the baby?\n\n- Coefficient on smoking during pregnancy is **-.151**.\n  - Women who smoked during pregnancy are **15.1** percentage points more likely to have a baby with low birth weight.\n- The 95% confidence interval is relatively wide $[0.002, 0.300]$, but it does not contain zero\n- Age, Race?, Nr of Visits and Pre-pregnancy weight do not seem to be factors\n-  Hypertension and previous premature labor are significant factors, increasing the probability of low birth weight by 40pp and 27.5pp, respectively.\n\n## LPM's predicted probabilities\n\n::: {#dd249d47 .cell execution_count=3}\n``` {.stata .cell-code code-fold=\"true\"}\nqui: reg low smoke age i.race any_premature hist_hyper  no_of_visits_1tr wgt_bef_preg, robust nohead\nqui: predict low_hat\nqui:histogram low_hat\nsum low_hat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n     low_hat |        179    .3184358    .1878437  -.0346098   .9483117\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](week08_files/figure-revealjs/cell-4-output-2.png){fig-align='center'}\n:::\n:::\n\n\n## Analysis of LPM's predicted probabilities\n\n:::{.panel-tabset}\n\n## What to do\n\n- Drill down in distribution:\n  - Looking at the composition of people: top vs bottom part of probability distribution\n  - Look at average values of covariates for top and bottom X% of predicted probabilities!\n\n## What we find\n\n::: {#74bb6707 .cell execution_count=4}\n``` {.stata .cell-code code-fold=\"true\"}\nsort low_hat\nset linesize 255\n \nqui: gen flag = 1 if _n<=5\nqui: replace  flag = 2 if _n>=_N-4\nlist low_hat smoke age race any_premature hist_hyper  no_of_visits_1tr wgt_bef_preg   if flag==1\nlist low_hat smoke age race any_premature hist_hyper  no_of_visits_1tr wgt_bef_preg   if flag==2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n     +---------------------------------------------------------------------------------+\n     |   low_hat       smoke   age    race   any_pr~e   hist_h~r   no_of_~r   wgt_be~g |\n     |---------------------------------------------------------------------------------|\n  1. | -.0346098   Nonsmoker    32   White          0          0          2        186 |\n  2. | -.0280454   Nonsmoker    36   White          0          0          0        175 |\n  3. |  .0019138   Nonsmoker    32   White          0          0          0        170 |\n  4. |  .0158307   Nonsmoker    23   White          0          0          0        190 |\n  5. |  .0284496   Nonsmoker    28   White          0          0          0        167 |\n     +---------------------------------------------------------------------------------+\n\n     +--------------------------------------------------------------------------------+\n     |  low_hat       smoke   age    race   any_pr~e   hist_h~r   no_of_~r   wgt_be~g |\n     |--------------------------------------------------------------------------------|\n175. | .7197446      Smoker    34   Black          0          1          0        187 |\n176. | .7369648   Nonsmoker    17   Black          0          1          0        142 |\n177. | .8160616      Smoker    18   Black          1          0          0        110 |\n178. | .8545191   Nonsmoker    26   Other          1          1          1        154 |\n179. | .9483117   Nonsmoker    25   Other          1          1          0        105 |\n     +--------------------------------------------------------------------------------+\n```\n:::\n:::\n\n\n:::\n\n# Modelling events: <br> [log/prob]it \n\n## Probability models: logit and probit\n\n- **Prediction:** predicted probability need to be between 0 and 1\n  - Thus, for prediction, we **must** use non-linear models\n  - Actually, its a quasi-linear model. \n- The model, itself, is linear in the parameters\n- but need to relate this to the probability of the $y = 1$ event, using a nonlinear function that maps the linear index into a 0-1 range: **'Link function'**\n\n$$\\begin{aligned}\nXB &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... \\\\\ny^P &= F(XB) \\rightarrow y^P \\in (0,1)\n\\end{aligned}\n$$\n\n- Two options: Logit and probit – different link function\n\n\n## Link functions I.\n\nCall $XB = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...$\n\n- The Logit:  \n  $y^P = \\Lambda(XB) = \\frac{\\exp(XB)}{1 + \\exp(XB)}$\n\n- The probit:  \n  $y^P = \\Phi(XB) \\rightarrow \\Phi(z) = \\int_{-\\infty}^z \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz$\n\nwhere $\\Lambda()$ is called logistic function, and $\\Phi()$ is the cumulative distribution function (CDF) of the standard normal distribution.\n\n\n## Link functions II.\n\n::: {.columns}\n\n:::{.column width=40%}\n- Both link functions are S-shaped curves bounded between 0 and 1.\n- There is but a small difference between the two.\n- but estimated coefficients will be different.\n:::\n\n:::{.column width=60%}\n\n::: {#874d63fc .cell execution_count=5}\n``` {.stata .cell-code code-fold=\"true\"}\nqui {\nclear\nrange p 0 1 202\ndrop if p==0 | p==1 \ngen x = invnormal(p)\ngen y = (x+rnormal())>0\nreg  y x\npredict y_1\nlogit y x\npredict y_2\nprobit y x\npredict y_3\ndrop if abs(x)>2\ntwo (line y_1 x ) (line y_2 x) (line y_3 x), ///\nlegend(order(1 \"LPM\" 2 \"Logit\" 3 \"Probit\") pos(3) ring(0) col(1)) \n}\n```\n\n::: {.cell-output .cell-output-display}\n![](week08_files/figure-revealjs/cell-6-output-1.png){fig-align='center'}\n:::\n:::\n\n\n:::\n:::\n\n \n\n## Logit and probit interpretation\n\n- Both the probit and the logit transform the $\\beta_0 + \\beta_1 x_1 + ...$ linear combination using a link function that shows an S-shaped curve.\n- The slope of this curve keeps changing as we change whatever is inside, but it's steepest when $y^P = 0.5$ (inflection point)\n- The difference in $y^P$ corresponds to changes in probabilities, between any two values of $x$.\n- To find how much is related to a particular $x$, You need to take the partial derivatives. \n- **Important consequence**: no direct interpretation of the raw coefficient values!\n  - Thus, always know if you are interpreting the raw coefficients or the marginal differences.\n\n## Marginal differences (marginal effects)\n\n> **NOTE** As before, the word \"effect\" should be used with caution. In the book, they use \"marginal differences\" instead. as a more neutral term.\n \n- Link functions makes associates $\\Delta x$ into $\\Delta y_P$. we do not interpret raw coefficients! (except for direction)\n- Instead, transform them into 'marginal differences' for interpretation purposes\n\n- They are also called 'marginal effects' or 'average marginal effects (AME)' or 'average partial effects'.\n \n- Average marginal difference has the same interpretation as the coefficient of linear probability models, but with caveats.\n \n## Marginal differences: Discrete $x$\n\n- if $x$ is a categorical (0-1), the marginal difference is the difference in the predicted probability of $y = 1$, that corresponds to a change from $x = 0$ to $x = 1$.\n\n$$\\Delta y^P = y^P(x = 1) - y^P(x = 0)$$\n\n- Then we simply \"average\" this difference across all observations.\n\n## Marginal differences: continous $x$\n\n- If $x$ is continuous, the marginal difference is calculated as the derivative (for a small change in $x$).\n\n$$\\frac{\\partial y^P}{\\partial x_1} = \\beta_1 \\cdot f(XB)$$\n\n- Which is then averaged across all observations to report a single number.\n\n- In practice, we interpret this as the change in the probability of $y = 1$ for a one-unit change in $x_1$.\n\n## How to estimate this models?\n### Maximum likelihood estimation!\n\n- When estimating a logit or probit model, we use 'maximum likelihood' estimation.\n  - See 11.U2 for details.\n- Idea for maximum likelihood is another way to get coefficient estimates. \n  - **1st** You specify a (conditional) distribution, that you will use during the estimation. \n    - This is logistic for logit and normal for probit model.\n  - **2nd** You maximize this function w.r.t. your $\\beta$ parameters → gives the maximum likelihood for this model.\n- Different from OLS: No closed form solution → need to use search algorithms.\n  - Thus... more computationally intensive.\n\n## Predictions for LMP, Logit and Probit I.\n\n::: {#023916e9 .cell execution_count=6}\n``` {.stata .cell-code code-fold=\"true\"}\nqui {\n  webuse lbw, clear\n  drop if age < 15 | age > 40 | lwt > 200\n  gen any_premature = ptl >0\n  ren ftv no_of_visits_1tr\n  ren ht hist_hyper\n  ren lwt wgt_bef_preg\n  gen black = 2.race\n  gen other = 3.race\n  reg low smoke age i.black i.other i.any_premature hist_hyper , robust nohead\n  predict low_hat_ols\n  est sto lpm_results\n  logit low smoke age black other any_premature hist_hyper , robust nohead\n  predict low_hat_logit\n   probit low smoke age black other any_premature hist_hyper , robust nohead\n  predict low_hat_probit\n   two (scatter  low_hat_logit low_hat_probit low_hat_ols) ///\n      (line low_hat_ols low_hat_ols, sort), ///\n      legend(order(1 \"Logit\" 2 \"Probit\" 3 \"LPM\") pos(3) ring(0) col(1))\n}\n```\n\n::: {.cell-output .cell-output-display}\n![](week08_files/figure-revealjs/cell-7-output-1.png){fig-align='center'}\n:::\n:::\n\n\n## Coefficient results for logit and probit {.smaller}\n\n``` {.stata .cell-code code-fold=\"true\"}\nqui {\n  logit low smoke age i.black i.other i.any_premature hist_hyper , robust nohead\n  est sto logit_results\n  margins, dydx(*) post\n  est sto logit_mfx\n  probit low smoke age i.black i.other i.any_premature hist_hyper , robust nohead  \n  est sto probit_results\n  margins, dydx(*) post\n  est sto probit_mfx\n}\nesttab lpm_results logit_results  logit_mfx probit_results probit_mfx, se  nonumber drop(0.*) ///\nmtitle(LPM Logit Logit_MFX Probit Probit_MFX) collabel(none) md ///\nstar(* 0.10 ** 0.05 *** 0.01) nonotes \n```\n\n\n|              |          LPM    |        Logit    |    Logit\\_MFX    |       Probit    |   Probit\\_MFX    |\n| ------------ | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |\n| main         |                 |                 |                 |                 |                 |\n| smoke        |        0.166**  |        0.925**  |        0.169**  |        0.549**  |        0.169**  |\n|              |     (0.0739)    |      (0.397)    |     (0.0695)    |      (0.235)    |     (0.0697)    |\n| age          |     -0.00703    |      -0.0447    |     -0.00818    |      -0.0271    |     -0.00835    |\n|              |    (0.00669)    |     (0.0377)    |    (0.00688)    |     (0.0221)    |    (0.00679)    |\n| 1.black      |        0.192*   |        1.012*   |        0.202*   |        0.607*   |        0.202*   |\n|              |      (0.111)    |      (0.526)    |      (0.109)    |      (0.314)    |      (0.108)    |\n| 1.other      |        0.150*   |        0.884**  |        0.164**  |        0.524**  |        0.163**  |\n|              |     (0.0770)    |      (0.435)    |     (0.0787)    |      (0.254)    |     (0.0779)    |\n| 1.any\\_premature |        0.284*** |        1.330*** |        0.279*** |        0.810*** |        0.280*** |\n|              |     (0.0972)    |      (0.441)    |     (0.0944)    |      (0.270)    |     (0.0956)    |\n| hist\\_hyper   |        0.370*** |        1.720**  |        0.315*** |        1.063**  |        0.327*** |\n|              |      (0.137)    |      (0.674)    |      (0.117)    |      (0.415)    |      (0.122)    |\n| \\_cons       |        0.270    |       -0.971    |                 |       -0.576    |                 |\n|              |      (0.183)    |      (0.983)    |                 |      (0.577)    |                 |\n| *N*          |          179    |          179    |          179    |          179    |          179    |\n\n\n\n[Standard errors in parentheses]{.table-note} \n[<sup>\\*</sup> *p* < 0.1, <sup>\\*\\*</sup> *p* < 0.04, <sup>\\*\\*\\*</sup> *p* < 0.01]{.table-note }\n\n## Does smoking pose a health risk?– logit and probit\n\n- LPM – interpret the coefficients as usual.\n- Logit, probit - Interpret the marginal differences. Basically the same.\n  - Marginal differences are essentially the same across the logit and the probit.\n  - **Essentially** the same as the corresponding LPM coefficients.\n- Happens often:\n  - Often LPM is good enough for interpretation.\n  - Check if logit/probit very different.\n    - if so, Investigate functional forms if yes.\n\n# Goodness of fit measures\n\n## Goodness of fit\n\n- There is no generally accepted goodness of fit measure\n  - This is because we do not observe probabilities only 1 and 0, so we cannot FIT those probabilities.\n- There are, however, other options to evaluate the quality of the model.\n  - R-squared\n  - Brier score\n  - Pseudo R-squared\n  - log-loss\n\n## Goodness of fit: R-squared\n\n$$R^2 = 1 - \\frac{\\sum_{i=1}^n (\\hat{y}_P^i - y_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$$\n\n- R-squared is not useful for binary outcomes\n  - It can be calculated, but it lacks the interpretation we had for linear models, because we are fitting the probabilities, not the outcomes.\n  \n## Brier score\n$$\\text{Brier} = \\frac{1}{n}\\sum_{i=1}^n (\\hat{y}_P^i - y_i)^2$$\n\n- The Brier score is the average distance (mean squared difference) between predicted probabilities and the actual value of $y$.\n- Smaller the Brier score, the better.\n- When comparing two predictions, the one with the smaller Brier score is the better prediction because it produces less (squared) error on average.\n- Related to a main concept in prediction: mean squared error (MSE)\n\n## Pseudo R2\n\n$$pR^2 = 1 - \\frac{\\text{Log-likelihood}_{\\text{model}}}{\\text{Log-likelihood}_{\\text{intercept only}}}$$\n\n- It is similar to the R-squared, as it measures the goodness of fit, tailored to nonlinear models and binary outcomes.\n  - Most widely used: McFadden's R-squared (`Stata` uses this)\n- Computes the ratio of log-likelihood of the model vs **intercept only**.\n\n- Can be computed for the logit and the probit but not for the linear probability model. (unless you re-define the Log-likelihood)\n\n## Log-loss\n\n$$\\text{Log-loss} = \\frac{1}{n}\\sum_{i=1}^n \\left[y_i \\log(\\hat{y}_P^i) + (1-y_i) \\log(1-\\hat{y}_P^i) \\right]$$\n\n- The log-loss is a measured derived from the log-likelihood function. It measures how much observed data dissagrees with the predicted probabilities.\n\n- The smaller (close to zero) the log-loss, the better the model.\n  \n## Practical use\n\n- There are several measured of model fit, but they often give the same ranking of models.\n- Do not use R-squared. Even for LPM, it has no interpretation.\n- If using probit vs logit: pseudo R-squared may be used to rank logit and probit models.\n- Use, especially for prediction: Brier score is a metric that can be computed for all models and is used in prediction.\n\n## Bias of the predictions\n\n- Post-prediction: we may be interested to study some features of our model\n- One specific goal: evaluating the bias of the prediction.\n  - Probability predictions are unbiased if they are right on average = the average of predicted probabilities is equal to the actual probability of the outcome.\n  - If the prediction is unbiased, the bias is zero.\n- Unless the model is really bad, unconditional bias is not a big issue.\n  - Only Probit will be biased.\n\n## Calibration\n\n- Unbiasedness refers to the whole distribution of probability predictions. \n- A finer and stricter concept is calibration\n- A prediction is well calibrated if the actual probability of the outcome is equal to the predicted probability for each and every value of the predicted probability.\n  $$E(y|y^P) = y^P$$\n    \n- 'Calibration curve' is used to show this.\n- A model may be unbiased (right on average) but not well calibrated\n  - underestimate high probability events and overestimate low probability ones\n\n## Calibration curve\n\n- Horizontal axis shows the values of all predicted probabilities ($\\hat{y}_P$).\n- Vertical axis shows the fraction of $y = 1$ observations for all observations with the corresponding predicted probability.\n- The closer the curve is to the 45-degree line, the better the calibration.\n  \nIn practice:\n\n- Create bins for predicted probabilities and make comparisons of the actual event's probability.\n- Or use non-parametric methods to estimate the calibration curve.\n\n## Calibration curve\n\n::: {#1b07866b .cell execution_count=8}\n``` {.stata .cell-code code-fold=\"true\"}\ntwo (lpoly low low_hat_ols) (lpoly low low_hat_logit) ///\n(lpoly low low_hat_probit) (function y = x, range(0 1) lcolor(black%50)), ///\nlegend(order(1 \"LPM\" 2 \"Logit\" 3 \"Probit\") ) scale(1.5) ///\nxtitle(\"Predicted probability\") ytitle(\"Avg outcome\")\n```\n\n::: {.cell-output .cell-output-display}\n![](week08_files/figure-revealjs/cell-9-output-1.png){fig-align='center'}\n:::\n:::\n\n\n## Probability models summary\n\n- Find patterns when $y$ is binary can be done using model probability with regressions\n- Linear probability model is mostly good enough, easy inference.\n  - to-go for data exploration, quick analysis, and diagnostics\n  - but, predicted values could be below 0, above 1\n- Logit (and probit) - better when aim is prediction, predicted values strictly between 0-1\n- Most often, LPM, logit, probit - similar inference\n- Use marginal (average) differences (with logit/probit)\n- No trivial goodness of fit. Brier score or pseudo-R-Squared.\n- Calibration is important for prediction.\n\n",
    "supporting": [
      "week08_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}