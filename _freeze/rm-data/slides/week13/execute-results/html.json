{
  "hash": "3ad5e35e927cac9cff76333651b92227",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Forecasting from Time Series Data\"\nsubtitle: \"Todays forcast it will be sunny with a chance of rain\"\nauthor: \n  - name: Fernando Rios-Avila\n    affiliation: Levy Economics Institute  \ndate: last-modified\ndate-format: long\nformat:\n  revealjs:\n    theme: [ clean2.scss]\n    slide-number: true\n    footer: \"*Rios-Avila and Cia*\"\n    width:  1280\n    height: 720\n---\n\n\n## Forecast setup\n\n> It is difficult to make predictions, especially about the future. (Niels Bohr; possibly an old Danish proverb)\n\n-   It is fairly easy to make forecasts for variables that have a decently stable pattern over time\n    -   Weekly FastMovingGoods product sales, sensor of gadgets used regularly\n-   It is very hard to make forecasts of really interesting variables\n    -   GDP with Recession, crisis,\n    -   High tech gadgets with new products\n    -   Stock market prices\n-   why???\n    -   Tradition!\n\n## Forecasting basics\n\n-   Forecasting is a special case of prediction.\n-   Forecasting makes use of time series data on $y$, and possibly other variables $x$.\n-   The original data used for forecasting is a time series from 1 through $T$, such as $y_1, y_2, ..., y_T$\n-   The forecast is prepared for time periods after the original data ends, such as $\\hat{y}_{T+1}, \\hat{y}_{T+2}, ..., \\hat{y}_{T+H}$. This is the live time series data.\n\n## Forecast horizon\n\n-   What is it?\n    -   The length of the live time series data (here $H$) = the forecast horizon.\n-   Short-horizon forecasts are carried out for a few observations after the original time series;\n    -   5-10 years of monthly data $\\rightarrow$ forecast a 3-12 months ahead\n    -   10 years of quarterly data $\\rightarrow$ predict ahead of a few quarters\n-   Long-horizon forecasts are carried out for many observations.\n    -   Often: data on activity, operation\n    -   5 years of daily data $\\rightarrow$ forecast daily ahead for a year\n    -   2 months of hourly activity data $\\rightarrow$ predict weeks ahead\n\n## Cross-validation in time series\n\n-   Cross-validation is essential in time series\n    -   The ability to build a model that works well for the future, without overfitting\n-   But it is tricky\n    -   you cannot just randomly split the data.\n        -   Time series is time dependent\n    -   But there are Options\n\n## Cross-validation: Options\n\n-   OP1: Select a \"test set\" of few periods where to evaluate the model\n    -   Use all information before and the test set to build the model\n    -   And a common \"end of sample\" test set\n    -   Good for long-horizon forecasts\n-   OP2: Select a \"test set\" of few periods where to evaluate the model\n    -   Use \"X\" periods before the test set to build the model\n    -   Repeat with the next Window\n    -   Good for short-horizon forecasts\n\n## Cross-validation\n\n::: panel-tabset\n## Option 1: Full sample\n\n![](images/paste-29.png)\n\n## Option 2: Rolling window\n\n![](images/paste-27.png)\n:::\n\n## Message:\n\n-   The strategy for prediction and cross-validation depends on the forecast horizon.\n    -   For short-horizon forecasts, you really need to consider the time dependence of the data.\n    -   For long-horizon forecasts, you try to capture long-term patterns in the data.\n\n# Long-horizon:\n\nSeasonality, trend and predictable events\n\n## Long-horizon forecasting: Seasonality and predictable events\n\n-   Look for aspect of data that matter for long time\n-   Focus on predictable aspects of time series\n    -   Trend(s) + Seasonality + Other regular events\n-   Two options to model trend: estimate average change or trend line\n    -   $y = f(T,S,E)$ or\n    -   $\\%\\Delta y \\text{ or } \\Delta y =g(T,S,E)$\n-   Seasonality: ie model with set of variables (11 months), maybe interactions\n-   Other regular events - set of binary vars (War, Covid, New Policy, etc)\n\n## Long-horizon forecasting: Growth rates\n\nFirst model - estimate average change:\n\n$$\\hat{\\Delta}y = \\hat{\\alpha}$$\n\nFor prediction this means:\n\n$$\n\\begin{aligned}\n\\hat{y}_{T+1} &= y_T + \\hat{\\Delta}y \\\\\n\\hat{y}_{T+2} &= \\hat{y}_{T+1} + \\hat{\\Delta}y = y_T + 2 \\times \\hat{\\Delta}y \\\\\n\\dots \\\\\n\\hat{y}_{T+H} &= y_T xx+ H \\times \\hat{\\Delta}y\n\\end{aligned}\n$$\n\n## Long-horizon forecasting: Trends\n\nEstimate trend line: $$\\hat{y}_t = \\hat{\\alpha} + \\hat{\\delta}t$$\n\n-   $\\hat{\\alpha}$ is predicted $y$ when $t = 0$\n-   $\\hat{\\delta}$ tells us how much predicted $y$ changes if $t$ is increased by one unit.\n-   You can also use more complex trends. (but one should still be concerned about overfitting)\n\n## Long-horizon forecasting: Trends - compare options\n\n-   Difference in models\n    -   Model changes:\n        -   Assumes that $y$ continues from the last observation, and increase by the same amount each time. What if Last observation is unusual?\n    -   Model trend line\n        -   Assumes that $y$ remains close to the trend line. Last unusual observation would not matter for the forecast, because it would be the trend line.\n-   Neither approach is inherently better than the other\n\n## Long-horizon forecasting: Seasonality\n\n-   Capture regular fluctuations\n-   Months, days of the week, hours, combinations\n\n## Case study: ABQ swimming\n\n-   Swimming pool data\n-   Albuquerque (ABQ), New Mexico, USA\n-   Big data, transaction level entry data logged from sales systems\n-   1.5m observations\n\nThis CS:\n\n-   Sample: Single swimming pool\n-   Aggregated: number of ticket sales per day\n-   After some sample design - regular tickets only\n\n## Case study: Modelling\n\n-   Trend is simple – use simple linear trend: $\\alpha t$\n    -   Maybe not really important at all\n    -   Perhaps consider exponential trend? (log-linear model)\n-   Seasonality is important and tricky\n\n## Case study: Daily ticket sales\n\n![Daily Sales 5Yrs](images/paste-30.png)\n\n## Case study: Monthly and daily seasonality\n\n::::: columns\n::: column\n![Month](images/paste-31.png)\n:::\n\n::: column\n![Day of the Week](images/paste-32.png)\n:::\n:::::\n\n## Case study: Daily ticket sales: A heatmap\n\n::::: columns\n::: column\n-   Tool to model seasonality\n-   Each cell is average sales for a given combination of day and month over years\n-   Colors help see pattern\n:::\n\n::: column\n![](images/paste-36.png)\n:::\n:::::\n\n## Case study: Modeling\n\n-   Trend is simple - linear trend\n-   Seasonality is tricky - need to model and simplify\n    -   Months\n    -   Days of the week\n    -   USA holidays (dummies)\n    -   Summer break (depends?)\n    -   Interaction of summer break and day of the week\n    -   Interaction of weekend and month\n\n## Case study: Model features and RMSE\n\n|                 | trend | months | days | holidays | school\\*days | days\\*months | RMSE  |\n|---------|---------|---------|---------|---------|---------|---------|---------|\n| M1              | X     | X      |      |          |              |              | 32.35 |\n| M2              | X     | X      | X    |          |              |              | 31.45 |\n| M3              | X     | X      | X    | X        |              |              | 29.46 |\n| M4              | X     | X      | X    | X        | X            |              | 27.61 |\n| M5              | X     | X      | X    | X        | X            | X            | 26.90 |\n| M6 (log)        | X     | X      | X    | X        | X            |              | 30.99 |\n| M7 (Prophet-ML) | X     | X      | X    | X        | N/A          |              | 29.47 |\n\nNote: Trend is linear trend, days is day-of-the-week, holidays: national US holidays, school\\*days is school holiday (mid-May to mid-August and late December) interacted with days of week.\\\nRMSE is cross-validated.\\\nSource: swim-transactions dataset.\\\nDaily time series, 2010–2016, N=2522 (work set 2010–2015, N=2162).\n\n## Case study: Compared actual vs predicted on holdout set (2016)\n\n![](images/paste-37.png){width=\"80%\"}\n\n## Case study: Diagnostics - holdout set (2016)\n\n::::: columns\n::: column\n### Actual vs predicted August 2016\n\n![](images/paste-38.png)\n:::\n\n::: column\n### Monthly RMSE\n\n![](images/paste-40.png)\n:::\n:::::\n\n## Case study: Diagnostics vs model building\n\n-   Here we used diagnostics to learn about what to expect, strength and weaknesses of the already **selected** model.\n    -   This means, no going back to drawing board.\n-   But, we could have said, lets run some checks on the training set and maybe alter the model accordingly.\n    -   As we normally do with scatterplots, lowess, tabulations, etc\n    -   Here: something weird happening in December.\n-   Act, build a new model and test, etc.\n\n# Short-run horizon, serial correlation and ARIMA\n\nWhat you learn today, you can use tomorrow\n\n## Short-horizon forecasting: what is new?\n\n-   Serial correlation is essential\n\n    -   Data Depends on itself, and Errors Also Depend on Themselves\n\n-   Modeling how a shock fades away $\\leftarrow$ Hopefully things \"return to normal\"\n\n    -   Autoregressive (AR) models, capture the patterns of serial correlation – $y$ at time $t$ is regressed on its lags, that is its past values, $t - 1, t - 2, etc$.\n\n-   The simplest includes one lag only, AR(1): $$y_t^E = \\beta_0 + \\beta_1 y_{t-1}$$\n\n-   Interested in estimating $\\beta_1$ or $\\rho$.\n\n    -   If $\\rho = 1$ is random walk (unpredictable),\n    -   if $\\rho = 0$ is white noise (also unpredictable, but stable).\n\n## Short-horizon forecasting: AR(1)\n\n-   One-period-ahead forecast from an AR(1):\n    -   Its a recursive formula:\n\n$$\\begin{aligned}\n\\hat{y}_{T+1} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 y_T \\\\\n\\hat{y}_{T+2} &= \\hat{\\beta}_0 + \\hat{\\beta}_1 \\hat y_{T+1}  \\\\\n              &=  \\hat{\\beta}_0 + \\hat{\\beta}_1 \\hat{\\beta}_0 + \\hat{\\beta}_1^2  y_T \\\\\n\\dots                \\\\\n\\hat{y}_{T+k} &= \\hat{\\beta}_0 \\sum_{s=1}^{k}  \\hat{\\beta}_1^{s-1} + \\hat{\\beta}_1^{k} y_T \n\\end{aligned}\n$$\n\n# ARIMA\n\nBringing the Big Guns\n\n## Short-horizon forecasting: ARIMA\n\n-   ARIMA(p,d,q) models that are generalizations of the AR(1) model\n    -   can approximate any pattern of serial correlation.\n-   ARIMA models are put together from three parts: AR(p), I(d) and MA(q).\n-   but What are these?\n\n## Short-horizon forecasting: ARIMA(p,d,q)\n\n-   The ARIMA model combines three approaches to modeling time series data:\n    -   AR(p): autoregressive models, We have seen this\n    -   I(d): models of differences, This is new\n        -   Relates to models that you first take the difference before modeling.\n        -   For example $\\Delta y_t = y_t - y_{t-1} = \\alpha$ is a I(1) model\n    -   MA(q) models: moving average models: Concentrates on the error term, and how much it \"depends\" on past errors.\n        -   For example $y_t = e_t + \\theta e_{t-1}$ is a MA(1) model\n        -   $\\theta$ has to be estimated via maximum likelihood, not OLS.\n\n## ARIMA: Mix and Match\n\n-   ARIMA(2,1,0): $\\Delta y_t = \\beta_0 + \\beta_1 \\Delta y_{t-1} + \\beta_2 \\Delta y_{t-2} + e_t$\n-   ARIMA(0,1,1): $\\Delta y_t = \\beta_0 + \\theta e_{t-1} + e_t$\n-   ARIMA(0,2,2): $\\Delta^2 y_t = \\beta_0 + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + e_t$\n-   ARIMA(p,d,q): $\\Delta^q y_t = \\beta_0 + \\sum_{i=1}^{p} \\beta_i \\Delta^q y_{t-i} + \\sum_{j=1}^{q} \\theta_j e_{t-j}+e_t$\n\nIn practice, You rarely see $d>2$, although $p$ and $q$ can be larger (depending on frequency of data)\n\n## How to choose (p,d,q)?\n\n-   Empirical approach\n    -   Whichever works best in a cross-validated exercise!\n    -   Try out a few and pick the one that works best\n    -   \"auto-arima\" - an algo that tries out many options\n    -   keep it simple, $d = 0, 1$ and $p = 0, 1, 2$ and $q = 0, 1, 2$ rarely more\n\n## How to choose (p,d,q)?\n\n-   Box-Jenkins Methodology\n\n-   Step 1: Determine $d$. Typically you test if the data is stationary (ADF test or PP test). You difference the data until it is stationary.\n\n-   Step 2: To determine $p$ and $q$ you look at the ACF and PACF of the differenced (if applicable) data.\n\n    -   ACF or Autocorrelation function\n        -   $corr(y_t, y_{t-1})$, $corr(y_t, y_{t-2})$, etc.\n        -   Helps Identify MA component (based on spikes)\n    -   PACF or Partial autocorrelation function\n        -   $y_t = \\beta_0 + \\beta_1 y_{t-1}+ e_t$\n        -   $y_t = \\beta_0 + \\beta_1 y_{t-1}+\\beta_2{t-2} y_{t-2} e_t$,etc\n        -   Helps Identify the AR component\n\n## Case study: Case- Shiller home price index\n\n-   Case-Shiller home price index, Los Angeles\n-   Monthly index of home prices\n-   Data available: fred.stlouisfed.org\n-   Use 18 years of monthly data\n\n## Case study: Case Shiller home price data\n\n-   18 years of data 2000-2017\n-   work: 2000-2016, holdout is 2017\n-   cross-validate with rolling window, 4-fold\n    -   train is 2000-2012, test is 2013\n    -   ...\n    -   train is 2003-2015, test is 2016\n-   Predict 12 months ahead\n-   RMSE - symmetric and quadratic loss\n-   Assume getting index right matters exactly the same\n\n## Case study: target variable\n\n-   What should be the target variable?\n    -   The price index\n    -   The log of the price index\n    -   First difference \\<- Shortcut to assume I(1) process\n-   We'll try out, and pick via cross-validation\n-   The model should include seasonal dummies (could be more complicated)\n-   The model may include a linear trend or capture it with $\\Delta y$ as target\n-   The model can have any form of ARIMA\n\n## Case study: Case- Shiller home price index - prediction from ARIMA models\n\n| id  | target | ARIMA | trend | season | AR  | I   | MA  | RMSE |\n|-----|--------|-------|-------|--------|-----|-----|-----|------|\n| M1  | p      | NO    | X     | X      |     |     |     | 31.9 |\n| M2  | p      | YES   | 1     | 1      | 2   |     |     | 9.5  |\n| M3  | p      | YES   | X     | 1      | 1   | 1   | 0   | 4.1  |\n| M4  | p      | YES   | X     | X      | 2   | 0   | 0   | 2.3  |\n| M5  | dp     | NO    | X     | X      |     |     |     | 18.8 |\n| M6  | lnp    | YES   | X     | 0      | 2   | 0   | 0   | 7.2  |\n\n## Case study: Prediction with best model M4: Uncertainty\n\n::: {#fig-1}\n![](images/paste-44.png)\n:::\n\n## `Stata` Corner\n\nIn Stata, the easiest way to estimate ARIMA models is using `arima` command:\n\n-   ARIMA(2,2,2) model: `arima y, arima(2,2,2)`\n\nPredictions are a bit tricky. You can use `predict` command with the `dynamic` option to predict out-of-sample values. But also need to indicate \"which\" periods to start using the model predictions\n\n-   `predict yhat, dynamic(ym(2017,1)) [y]`\n\n`yhat` is the new variable, `ym(2017,1)` indicates the first period of 2017, and `[y]` indicates that we want to predict the \"real\" dependent variable. Not the changes or transformations.\n\nNo built-in option for CI predictions.\n\n# Vector Autoregression\n\nWhen you need more than one variable\n\n## VAR\n\n-   Better forecasts with the help of other variables, at least for short forecast horizons.\n-   Need forecasts of the $x$ variable as well – we need a model.\n-   Vector autoregression (VAR), is a method that incorporates other variables in time series regressions and can use those other variables for forecasting $y$.\n-   Technically, you are not \"ONLY\" forecasting $y$ anymore, but a set of time series regressions.\n-   A set of time series regressions.\n\n$${y_t, x_t, z_t} =W_t= \\beta_0 + W_{t-1}\\beta_1 + W_{t-2}\\beta_2 + \\dots + W_{t-p}\\beta_p + e_t\n$$\n\n## VAR: simplest model\n\nThe simplest VAR model has $y$ and one $x$ variable, and it includes one lag of each = VAR(1) model.\n\nIt assumes that all data have the same frequency. $$\\begin{align*}\ny_t^E &= \\beta_{10} + \\beta_{11} y_{t-1} + \\beta_{12} x_{t-1} \\\\\nx_t^E &= \\beta_{20} + \\beta_{21} y_{t-1} + \\beta_{22} x_{t-1}\n\\end{align*}\n$$\n\n## VAR forecast\n\n-   **One-period-ahead** forecast for $y$, only need estimates from the first one: $$\\hat{y}_{T+1} = \\hat{\\beta}_{10} + \\hat{\\beta}_{11} y_T + \\hat{\\beta}_{12} x_T\n    $$\n\n-   For forecasting $y$ further ahead, we do need all coefficient estimates, and forecast values of $x$ as well.\n\n-   A two-period-ahead forecast of $y$ from a VAR(1) is\n\n$$\n\\hat{y}_{T+2} = \\hat{\\beta}_{10} + \\hat{\\beta}_{11} \\hat{y}_{T+1} + \\hat{\\beta}_{12} \\hat{x}_{T+1}\n$$\n\nand $\\hat{x}_{T+1}$ and $\\hat{y}_{T+1}$ are from the first forecast.\n\nForecasts for $T + 3, T + 4, etc.,$ are analogous.\n\n## VAR characteristics\n\nThere are four important characteristics of a VAR:\n\n-   A VAR has a regression for each of the variables.\n-   The right-hand side of each equation has all variables.\n-   Right-hand-side variables are in lags only.\n-   All right-hand-side variables in all regressions have the same number of lags\n\nNote: More often than not, you need the \"system\" to be stable/stationary. This is a bit more complex than just checking the target variable. (You need to see the Matrix of coefficients)\n\n## Case study: Unemployment Rate\n\n::::: columns\n::: column\n### U. Rate\n\n![](images/paste-45.png)\n:::\n\n::: column\n### Change in U. Rate\n\n![](images/paste-46.png)\n:::\n:::::\n\n## Case study: Employed Population\n\n::::: columns\n::: column\n### ln(emp)\n\n![](images/paste-47.png)\n:::\n\n::: column\n### Change in ln(emp)\n\n![](images/paste-48.png)\n:::\n:::::\n\n## Case study: Case- Shiller home price index - Model selection 2\n\nRun the VAR model and compare to previous results.\n\n| id  | target | ARIMA | trend | season | AR  | I   | MA  | RMSE    |\n|-----|--------|-------|-------|--------|-----|-----|-----|---------|\n| M1  | p      | NO    | X     | X      |     |     |     | 31.9    |\n| M2  | p      | YES   | 1     | 1      | 2   |     |     | 9.5     |\n| M3  | p      | YES   | X     | 1      | 1   | 1   | 0   | 4.1     |\n| M4  | p      | YES   | X     | X      | 2   | 0   | 0   | 2.3     |\n| M5  | dp     | NO    | X     | X      |     |     |     | 18.8    |\n| M6  | lnp    | YES   | X     | 0      | 2   | 0   | 0   | 7.2     |\n| M7a | dp     | VAR   |       |        |     |     |     | **7.8** |\n| M7b | dp     | VAR   | X     |        |     |     |     | **4.5** |\n\n-   In this case study, VAR did not improve on ARIMA.\n\n## VAR in `Stata`\n\nStata has a feature for the estimation of VAR models called...Var\n\n`var depvarlist [if] [in] [,lags(#) exog(varlist) Other]`\n\n-   `depvarlist` are all the variables one needs to analyze.\n\n-   `lags(#)` indicates how many (and which) lags to use.\n\nFor prediction look into `fcast`, `predict` or `forecast`\n\n## External validity in time series\n\n-   External validity is about the stability of patterns in the data\n\n    -   Such as trends, seasonality (Do they repeat? or do we need to update?)\n\n-   Stationarity is what we look for:\n\n    -   Distribution of the target, predictors is stable over time\n    -   Correlation patterns also stable over time\n    -   We can then make predictions of the future!\n\n-   External validity is massive risk with time series by design: predict for future\n\n-   What if we update the model with NEW data. How well would it do?\n\n## Case study: Case- Shiller home price index - model fit on test sets\n\nFour test set (in work set) with rolling window CV. RMSE in each test set for each model.\n\n|        | Fold1 | Fold2 | Fold3 | Fold4 | Average |\n|--------|-------|-------|-------|-------|---------|\n| M1     | 14.90 | 17.58 | 34.44 | 48.58 | 31.9    |\n| M2     | 14.83 | 8.39  | 6.23  | 5.52  | 9.5     |\n| M3     | 6.68  | 1.39  | 3.29  | 3.22  | 4.1     |\n| **M4** | 2.22  | 1.96  | 2.88  | 1.20  | 2.2     |\n| M5     | 33.94 | 9.79  | 10.44 | 7.39  | 18.8    |\n| M6     | 2.49  | 4.95  | 9.22  | 9.54  | 7.2     |\n| M7a    | 13.30 | 5.85  | 3.52  | 4.28  | 7.8     |\n| M7b    | 5.24  | 2.51  | 5.18  | 4.75  | 4.5     |\n\n## Case study: Prediction with best model M4 for 2018\n\n<div>\n\n![](images/paste-49.png)\n\n</div>\n\n## Summary\n\n-   Time series prediction is both simple and very hard\n    -   Simple as some basic models work okay\n    -   Model trend as first difference or linear trend\n    -   Model seasonality, regular events\n    -   Some basic method of capturing serial correlation\n-   Time series prediction model building is also very hard\n    -   Getting seasonality, holidays, changing patterns right\n    -   Getting target variable and ARIMA(p,d,q) selection needs competing models\n-   Most importantly: external validity is a huge problem\n    -   Stability may easily break down, and there is nothing we can do.\n\n## Extra: ARIMA with Forcast\n\nNews! I just got word from Stata Technical services regarding Arima and the Forcasting and CI.\n\nThe official word is as follows: \n\n> It is possible to obtain Confidence Intervals with `arima` using the `forecast` command. This uses\n> simulations to obtain the CI. \n\nLets do an example\n\n## Extra Example\n\n**Step 1**: Setup, I will simulate data for an ARIMA(2,1,2) model\n\n::: {#1060963b .cell execution_count=1}\n``` {.stata .cell-code code-fold=\"true\"}\nclear\nset seed 2\nset obs 100\ngen t = _n\ntsset t\ngen ut = rnormal()*0.01\ngen dxt = 0\nreplace dxt =  .5 *l.dxt + .2 *l2.dxt + ut + 0.5*l.ut - 0.25*l2.ut if t>2\ngen x=sum(dxt)\n```\n:::\n\n\n**Step 2**: Estimate the ARIMA(2,1,2) model using t=1 to 80\n\n::: {#41513e99 .cell execution_count=2}\n``` {.stata .cell-code code-fold=\"true\"}\nqui: arima x if t<=80, arima(2,1,2)  \nest sto arima_est\n```\n:::\n\n\n**Step 3**: Forecast the next 20 periods, with 95% CI\n\n::: {#a11d1704 .cell execution_count=3}\n``` {.stata .cell-code code-fold=\"true\"}\ncapture forecast clear        // \"clears\" the last forecast\nforecast create arima_predict // Sets the Forecast setup\nforecast estimates arima_est, names(Dx) // Names the outcome Dx\nforecast identity x = L.x + Dx // reconstructs x (the forecast)\nforecast solve, prefix(f_) begin(81) ///\nsimulate(betas, statistic(stddev, prefix(sd_)) reps(200)) /// Forcast and does the simulation\n```\n:::\n\n\n## Extra: ARIMA with Forcast\n\nStep 4: Plot the forecast\n\n::: {#1a9d64e0 .cell execution_count=4}\n``` {.stata .cell-code code-fold=\"true\"}\nqui replace sd_x = 0 if sd_x ==.\ngenerate low = f_x - invnormal(0.975)*sd_x\ngenerate up  = f_x + invnormal(0.975)*sd_x\n\ncolor_style bay\ntwo (rarea low up t, color(gs12)) (line x t , lwidth(.5) ) ///\n     (line f_x t, lwidth(.5)  ),  ///\n     ytitle(\"x\") xtitle(\"Time\") legend(off) ///\n     note(\"ARIMA(2,1,2) with 95% CI\") // Adds a note\n```\n\n::: {.cell-output .cell-output-display}\n![](week13_files/figure-revealjs/cell-5-output-1.png){}\n:::\n:::\n\n\n",
    "supporting": [
      "week13_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}