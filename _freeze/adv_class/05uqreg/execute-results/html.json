{
  "hash": "59b57d7243728edae18dbca76c7adefe",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Unconditional Quantile Regressions (and RIF's)\"\nsubtitle: \"When we care about everyone\"\nauthor: Fernando Rios-Avila\nformat:\n  revealjs: \n    slide-number: true\n    width: 1600\n    height: 900\n    code-fold: true\n    echo: true\n    css: styles.css  \n    highlight-style: github\n  pdf: default  \nexecute:\n  freeze: true   \n---\n\n## Introduction\n\nAs we saw last class, conditional quantile regressions have only one purpose:\n\n-   Analyze relationships between conditional distributions.\n\nThis is a very useful tool!. As it allows you to move beyond Average relationships.\n\n-   How do people (who are not all average) would be affected by changes in $Xs$\n\nThere is a limitation, however. The effects you may estimate, will depend strongly on model specification.\n\n-   This is similar to OVB. Changing covariates could drastically change the conditional distributions and associated coefficients\n\nWhat if, you are interested in distributional effects across the whole population! Not only a subsample?\n\n## $E(q(y|X))$ is not $Q(y)$\n\n- Common mistake when analyzing QRegressions: Make interpretations as if the average effects on the $qth$ conditional quantiles would be the same as the effect on the \"overall\" $qth$ quantile.\n\n- Except for few cases (when Quantile regressions are not relevant), CQ effects do not translate ***directly*** into Changes into the unconditional quantile.\n\nHowever, as a policy maker, this would be the most relevant estimand you may be interested in :\n\n-   How does improving education affect inequality?\n\n-   Would eliminating Unionization would increase wage inequality?\n\n-   Is there heterogeneity in consumption expenditure?\n\nHowever, going from Conditional to unconditional statistics (not only Q) is not always straight forward.\n\n## âŒšWait...What do we mean unconditional?\n\nOne of the questions I read a lot regarding UQR is what do we mean unconditional?\n\n-   This is perhaps a someone poor choice of words.\n\n-   Anytime we estimate **ANY** statistic, we condition on something.\n\n    -   We condition on all individual characteristics (including errors)\n\n    -   We condition on groups characteristics (CQREG and CEF)\n\n    -   or, We condition on all characteristics (distributions). We happen to call this, unconditional statistics.\n\n-   This, however, ***does*** make a big difference in interpretation.\n\n## From Condition on Individuals,\n\n### to conditioning on Distributions\n\n$$\\begin{aligned}\ny_i &= b_0 + b_1 x_i + e_i + x_i e_i \\\\\n\\frac{dy_i}{dx_i}&=b_1 + e_i \\\\\nE(y_i|x_i=x) &= b_0 + b_1 x  \\\\\n\\frac{dE(y_i|x)}{dx}&=b_1 \\\\\nE(E(y_i|x_i=x))=E(y_i) &= b_0 + b_1 E(x_i)    \\\\\n\\frac{dE(y_i)}{dE(x_i)}&=b_1\n\\end{aligned}\n$$\n\nSame effects, but different interpretations (specially last one)\n\n## How are Unconditional effects Estimated?\n\nConsider any distributional statistic $v$, which takes as arguments, all observations, density distributions $f()$, or cumulative distributions $F()$.\n\n$$\nv = v(F_y) \\ or \\ v(f_y) \\ or \\ v(y_1, y_2, ...,y_n)\n$$\n\nAnd to simplify notation, lets say this function is defined as follows:\n\n$$\nv(f_y) = \\int_{-\\infty}^\\infty h(y,\\theta) f(y)dy \n$$\n\nThis simply considers distributional statistics $v$ that can be estimated by simply integrating a transformation of $h(y,\\theta)$ given a set of parameters $\\theta$.\n\nBut for now, lets consider only the Identify function $h(y,\\theta)=y$\n\nbut...What about Controls??\n\n## Introducing controls\n\nAssume there is a joint distribution of function $f(y,x)$, then\n\n$$\n\\begin{aligned}\nf(y,x)&=f(y|x)f(x) \\\\\nf(y) &= \\int f(y|x) f(x) dx\n\\end{aligned}\n$$\n\nAnd all together:\n\n$$\n\\begin{aligned}\nv(f_y) &= \\int y \\int f(y|x) f(x) dx \\ dy \\\\\nv(f_y) &= \\iint y f(y|x) dy \\ f(x) dx  \\\\\nv(f_y) &= \\int  E(y|X) f(x) dx  \\\\\n\\end{aligned}\n$$\n\n## Or a bit more General\n\n$$\nv(f_y) = \\iint h(y,\\theta) f(y|x)f(x)dxdy\n$$ \n\nSo, the statistic $v$ will change if:\n\n\\- We change the function $h$ or its parameters $\\theta$.\n\n\\- Assume some shocks that change the conditional $f(y|x)$\n\n\\- or the distribution of characteristics change!\n\nNote: \n$$f(y|x) \\sim \\beta \\text{ and }\nf(x) \\sim x\n$$\n\n## Again...How are Unconditional effects Estimated?\n\nIn an ideal scenario, you simple get the data under two regimes (before and after changes in $x$), and do the following:\n\n$$\\Delta v = v(f'_y)-v(f_y)\n$$\n\nThat is, just estimate the statistic in two scenarios ($f'$ and $f$), and calculate the difference. (impossible!)\n\nBut there are (at least) three alternatives:\n\n1.  Using Reweighting approaches to \"reshape\" the data: $f(x)$ (non-parametric)\n2.  Identify $f(y|x)$ so one can simulate how $\\Delta X$ affect y\n3.  Focus on the statistic $v$ and indirectly identify the effects of interest. (RIF!)\n\n## Op1: Re-weighting {.scrollable}\n\nConsider the following\n\n- There is a policy such that you plan to improve education in a country. \n  \n  Every single person will have at least 7 years of education, and will have free access to two additional years of education if they want to.\n\n  In other words, characteristics change from $f(x) \\rightarrow g(x)$ . But you do not see this! \n  \n$$ v(g_y) = \\iint h(y,\\theta) f(y|x) \\color{red}{g(x)}dxdy $$\n\n##\n\nbut perhaps, we could see this:\n\n$$\\hat v(g_y) = \\iint h(y,\\theta) f(y|x) \\color{red}{\\hat w(x)}f(x) dxdy\n$$\n\nif we can come up with a set of weights $\\color{red}{\\hat w(x)}$ such that $f(x)\\hat w(x)=g(x)$\n\n$$\n\\hat w(x) = \\frac{\\hat g(x)}{\\hat f(x)}\n$$\n\n## \n\nSimple, yet hard. Estimation of multivariate densities can be a difficult task.\n\n$$\nf(x) = h(x|s=0) ; g(x) = h(x|s=1) \n$$\n\nThis makes things \"easier\".\n\n$$\n\\begin{aligned}\nh(x|s=k)  &= \\frac{h(x)p(s=k|x)}{p(s=k)} \\\\\n\\hat w(x)  \n         &= \\frac{h(x)p(s=1|x)}{h(x)p(s=0|x)}\\frac{p(s=0)}{p(s=1)} \\\\\n         &=\\frac{p(s|x)}{1-p(s|x)} \\frac{1-p(s)}{p(s)}\n\\end{aligned}\n$$\n\nEasier to estimate conditional probabilities, (logit probit or other) than Densities\n\n## Example {.scrollable}\n\nGoal: Evaluate the impact of an increase in Fines on # of citations. (using reweighting)\n\n::: {#7d11922e .cell .larger execution_count=1}\n``` {.stata .cell-code code-fold=\"false\"}\nwebuse dui, clear\n** Create Fake Sample\ngen id = _n\nexpand 2\nbysort id:gen smp = _n ==2\n** Now you have two of ever person. So lets do some Policy\n** Fines increase lower fines more than higher ones, up to 12\n** Here we have a simulation of a policy that increases fines\nreplace fines = 0.1*(12-fines)+fines if smp==1\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<style>div.jp-Notebook .datagrid-container {min-height: 448px; }</style>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(Fictional data on monthly drunk driving citations)\n(500 observations created)\n(498 real changes made)\n```\n:::\n:::\n\n\nEstimation of Logit (or Probit) to estimate $p(s|x)$\n\nAnd estimate IPW weights\n\n::: {#4e590a22 .cell .larger execution_count=2}\n``` {.stata .cell-code code-fold=\"false\"}\n** Estimate logit \nqui:logit smp c.fines##c.fines taxes i.csize college\npredict pr_smp\ngen wgt = pr_smp / (1-pr_smp) \nreplace wgt = 1 if smp==1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(option pr assumed; Pr(smp))\n(500 real changes made)\n```\n:::\n:::\n\n\nHave the IPW weights helped simulate the policy? \n\n::: {#72a759fe .cell .larger execution_count=3}\n``` {.stata .cell-code code-fold=\"false\"}\nset scheme white2\ncolor_style tableau\nxi:tabstat fines i.csize college  taxes [w=wgt],  by(smp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ni.csize           _Icsize_1-3         (naturally coded; _Icsize_1 omitted)\n(analytic weights assumed)\n\nSummary statistics: Mean\nGroup variable: smp \n\n     smp |     fines  _Icsiz~2  _Icsiz~3   college     taxes\n---------+--------------------------------------------------\n       0 |  10.10573  .2919004  .3571831  .2483835  .7047717\n       1 |  10.10568       .29      .358      .248      .704\n---------+--------------------------------------------------\n   Total |  10.10571  .2909501  .3575916  .2481917  .7043858\n------------------------------------------------------------\n```\n:::\n:::\n\n\nI can now compare the distribution of fines before and after the policy\n\n::: {#8fa3ab59 .cell execution_count=4}\n``` {.stata .cell-code}\ntwo (kdensity citations if smp==0 ) ///\n    (kdensity citations if smp==0 [w=wgt]) /// \n    , legend(order(1 \"Before Policy\" 2 \"After Policy\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(analytic weights assumed)\n(analytic weights assumed)\n(analytic weights assumed)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](05uqreg_files/figure-revealjs/cell-5-output-2.png){}\n:::\n:::\n\n\nSeems to be a contraction of # citations:\n\n::: {#f6968af3 .cell .larger execution_count=5}\n``` {.stata .cell-code}\ndisplay \"Before Policy\"\ntabstat citations if smp ==0, stats(p10 p25 p50 mean p75 p90  )\ndisplay \"After Policy\"\ntabstat citations if smp ==0 [w=wgt],  stats(p10 p25 p50 mean p75 p90  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBefore Policy\n\n    Variable |       p10       p25       p50      Mean       p75       p90\n-------------+------------------------------------------------------------\n   citations |      11.5        15        20    22.018        27      34.5\n--------------------------------------------------------------------------\nAfter Policy\n(analytic weights assumed)\n\n    Variable |       p10       p25       p50      Mean       p75       p90\n-------------+------------------------------------------------------------\n   citations |        11        14        19  20.29751        25        32\n--------------------------------------------------------------------------\n```\n:::\n:::\n\n\n- Increasing fines  may reduce citations in about 1.3., but have almost no effect at the bottom of the distribution.\n\nWhat about Standard errors? Bootstrap! (logit and estimation, probably clustering at individual level)\n\nEasy to extend to other Statistics, but, can only provide results \"within\" support.\n\n## Op2: Model Conditional Distribution\n\nSay that you are interested in the same Policy, but do not trust re-weighting. Instead you want to model the Outcome, using some parametric or nonparametric analysis\n\n1.  Define your model. Should be feasible enough to accommodate changes in the conditional distribution. (one \"model\" for each $X's$ combination?)\n2.  Use the model to make predictions of your outcome (quite a few times). and summarize all results.\n\nOptions for flexible mode?\n\n-   You can use Heteroskedastic OLS $y\\sim N(x\\beta,x\\gamma)$ and predict from here\n\n-   You can use CQregressions to simulate the results.\n\nOne of this is similar to what we do in simulation analysis, and imputation. The other is similar to the work of [Machado Mata (2005)](https://onlinelibrary.wiley.com/doi/10.1002/jae.788) and [Melly(2005)](https://www.sciencedirect.com/science/article/pii/S0927537105000382). Where you invert the whole distribution \"globally\"\n\n## \n### Recipe\n\n- Model $Y=G(X,\\theta)$\n\n- Create a \"policy\" $X'=H(X)$\n\n- Predict $Y'=G(X',\\theta)$ and identify effect:\n  \n  $$\\Delta V(Y) = V(Y')-V(Y)$$\n\n- Repeat many times, and summarize results.\n\n## Example #1: Hetregress {.scrollable}\n\n```stata\n** Example for OPT2\nwebuse dui, clear\n** Modeling OLS with heteroskedastic errors\n    hetregress citations fines i.csize college taxes ,  het(fines i.csize college taxes )\n    \n    \n------------------------------------------------------------------------------\n   citations | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\ncitations    |\n       fines |   -6.18443   .3018298   -20.49   0.000    -6.776006   -5.592855\n             |\n       csize |\n     Medium  |   4.683941   .5028377     9.32   0.000     3.698397    5.669484\n      Large  |   9.655742   .5261904    18.35   0.000     8.624428    10.68706\n             |\n     college |   4.495635   .5283579     8.51   0.000     3.460072    5.531197\n       taxes |  -3.640864   .4938209    -7.37   0.000    -4.608735   -2.672993\n       _cons |   79.48011   3.118008    25.49   0.000     73.36892    85.59129\n-------------+----------------------------------------------------------------\nlnsigma2     |\n       fines |  -.5261208    .082495    -6.38   0.000     -.687808   -.3644337\n             |\n       csize |\n     Medium  |    .331204   .1681709     1.97   0.049     .0015952    .6608129\n      Large  |   .5578834   .1662309     3.36   0.001     .2320768    .8836899\n             |\n     college |   .3186815   .1539424     2.07   0.038     .0169599    .6204032\n       taxes |  -.3988692   .1437708    -2.77   0.006    -.6806548   -.1170836\n       _cons |   8.257714   .8201063    10.07   0.000     6.650335    9.865093\n------------------------------------------------------------------------------\nLR test of lnsigma2=0: chi2(5) = 75.42                    Prob > chi2 = 0.0000\n\n\n**  make Policy\nclonevar fines_copy = fines\nreplace fines = 0.1*(12-fines)+fines \n\npredict xb, xb\npredict xbs, sigma\n\n** Simulate results\ncapture program drop sim1\nprogram sim1, eclass\n    capture drop cit_hat \n    gen cit_hat = rnormal(xb,xbs)   \n    qui:sum citations, d \n    local lp10 = r(p10)\n    local lp25 = r(p25)\n    local lp50 = r(p50) \n    local lpmn = r(mean)\n    local lp75 = r(p75)\n    local lp90 = r(p90)\n    qui:sum cit_hat, d \n    matrix b = r(p10)-`lp10',r(p25)-`lp25', r(p50)-`lp50' , r(mean) -`lpmn',r(p75)-`lp75',r(p90)-`lp90'\n    matrix colname b = p10 p25 p50 mean p75 p90\n    ereturn post b\nend\n\nsimulate, reps(1000): sim1\nsum\n\n-------------+---------------------------------------------------------\n      _b_p10 |      1,000    -1.08147    .3913698   -2.31713   .1689796\n      _b_p25 |      1,000   -.3262908    .3230118  -1.817808   .6465259\n      _b_p50 |      1,000   -.2085465     .316455   -1.09237   .7785921\n     _b_mean |      1,000   -1.675626    .2234377  -2.400322   -1.03909\n      _b_p75 |      1,000   -1.541725    .4210822  -2.857586  -.2505198\n-------------+---------------------------------------------------------\n      _b_p90 |      1,000   -3.543298    .6079578  -5.464802  -1.682991\n```\n\nEffects larger than Reweigthing. Statistical inference here may be flawed. (first stage error not carried over)\n\n## Example #2: Qregress {.scrollable}\n\n```stata\nwebuse dui, clear\ngen id = _n\n** Expand to 99 quantiles\nexpand 99 \nbysor id:gen q=_n\n** make policy\ngen fines_policy=0.1*(12-fines)+fines \ngen fines_copy  =fines \n** Estimate 99 quantiles (in theory one should do more..but choose at random)\nssc install qrprocess // Faster than qreg\n** Save Cit hat (prediction)\n** cit policy (with policy)\ngen cit_hat=.\ngen cit_pol=.\n\nforvalues  i = 1 / 99 {\n    if `i'==1   _dots 0 0\n    _dots `i' 0\n    qui {\n        local i100=`i'/100\n        capture drop aux\n        qrprocess citations c.fines##c.fines  (i.csize college taxes) if q==1, q(`i100')\n        ** predicts the values as if they were in q100\n        predict aux\n        replace cit_hat=aux if q==`i'\n        drop aux\n        replace fines = fines_policy\n        predict aux\n        replace cit_pol=aux if q==`i'   \n        replace fines = fines_copy \n    }   \n}\n\n tabstat citations cit_hat cit_pol, stats(p10 p25 p50 mean p75 p90)\n \n   Stats |  citati~s   cit_hat   cit_pol\n---------+------------------------------\n     p10 |      11.5  10.70744  9.911633\n     p25 |        15  15.42857  14.27302\n     p50 |        20  21.15557  19.68303\n    Mean |    22.018   22.0002  20.31824\n     p75 |        27  27.65936  25.56173\n     p90 |      34.5  34.03413  31.39192\n----------------------------------------\n```\n\nVery demanding (computationally) and may only capture effects to the extend that we have good coverage of the distribution.\n\nStandard Errors...Bootstrapping. Perhaps use random quantile assignment, and may have problems near boundaries.\n\n## Opt 1 and 2: Comments\n\nThe first two options allow you to estimate effects of changes in $f(x)$ on the unconditional distribution of $y$, and in consequence, the distributional statistics of interest.\n\nThey have limitations:\n\n1.  They both are limited to a single experiment. A different policy requires a change in the setup.\n2.  Reweighing is simple to apply, but has limitation on the type of policies. They all need to be within the support of $X$\n3.  Modeling the conditional distribution is a more direct approach, but more computationally intensive, specially for obtaining Stand errors.\n\n## Opt 3. Local Approximation: RIF regression\n\nThe third approach was first introduced by [Firpo, Fortin and Lemieux 2009](https://www.jstor.org/stable/40263848), as a computationally simple way to analyze how changes in $X's$ affect the unconditional quantiles of $y$.\n\nThis strategy was later extended to analyze the effects on a myriad of distributional statistics and rank dependent indices. as well as an approach to estimate distributional treatment effects. See [Rios-Avila (2020)](https://journals.sagepub.com/doi/pdf/10.1177/1536867X20909690).\n\nIn contrast with other approaches, it can be used to analyze multiple types of policies without re-estimating the model. However the identification and interpretation needs particular attention.\n\nIt also allows you to easily make Statistical inference. (except for quantiles...)\n\n## Opt 3. From ground up\n\nReconsider the Original question. How do you capture the effect of changes of distribution of $x$ on the distribution of $y$.\n\n$$\n\\Delta v=v(G_y) - v(F_y) \n$$\n\nNow, assume that $G_y$ is just marginally different from $F_y$ (different in a very particular way)\n\n$$\nG_y(y_i) = (1-\\epsilon)F_y+ \\epsilon 1(y>y_i)\n$$\n\nThis function puts just a bit more weight on observation $y_i$. Think of it as \"dropping\" a new person in the pool.\n\nIf this is the case, the $\\Delta v(y_i)$ Captures how would the Statistic $v$ changes if the distribution puts just a bit extra weight on 1 observation. (this would be very small)\n\n## Opt 3. One more thing\n\nLets Rescale it:\n\n$$\nIF(v,F_y,y_i) =lim_{\\epsilon \\rightarrow 0} \\frac{v(G_y(y_i))-v(F_y)}{\\epsilon}\n$$\n\nThe **influence function** is a measure of direction of change, we should expect the statistic $v$ will have as we change $F_y \\rightarrow G_y$ .\n\nFrom here the RIF is just $RIF(v,F_y,y_i) = v + IF(v,F_y,y_i)$\n\nWhich has some properties:\n\n$$\n\\begin{aligned}\n\\int IF(v,F_y,y_i)f_ydy=0 &; \n\\int RIF(v,F_y,y_i)f_y dy=v \\\\\nv(F_y) \\sim N \\left(v(F_y),\\frac{\\sigma^2_{IF}}{N} \\right) &;\n\\int IF^2f_ydy =\\sigma^2_{IF}\n\\end{aligned}\n$$\n\n## Opt 3. RIF Regression\n\nFirst:\n\n$$\nv(F_y) = \\iint RIF(v,F_Y,y_i) f(y|x)f(x)dy = \\int E(RIF(.)|x) f(x)\n$$\n\nFrom here is similar to Opt 3. Use ***some*** econometric model to estimate $E(RIF(.)|X)$, and use that to make predictions on how $v(F_y)$ would change, when there is a **distributional change** in $X$.\n\nRIF-OLS: Unconditional effect!\n\n$$\nRIF(v,F_y,y_i) = X\\beta+e  \\ \\rightarrow\\ \nE(RIF) = v(F_y) = \\bar X \\beta \\\\\n\\frac{dv(F_y)}{d\\bar X}=\\beta\n$$\n\nLogic. When $F_x$ changes, it will change the distribution of $F_y$, which will affect how the statistic $v$ will change. But, we can only consider changes in means! (and Var)\n\n## Why it works, and why it may not\n\nRIF regressions works by using a linear approximation of the statistic $v$ with the changes in $F_y$ which are caused by changes in $F_x$, proxied by changes in $\\bar X$.\n\n-   Changes at the individual $x_i$ are not interesting (in a population of 1million, what happens to person 99 may not be large enough to matter)\n\nDepending on the model specification, however, we may only be able to identify changes in first and second moments of the distribution of $x$. (Mean and variance).\n\n\\-\n\nHowever, as any linear approximation to a non-linear function, the approximations are BAD when the changes in $F_x$ are too large. The most relevant example...Dummies and treatment!\n\n## RIF-Reg and dummies\n\nDummies are a challenge. At individual or conditional level, we usually consider changes from 0 to 1 (off or on).\n\n-   For unconditional effects this is not correct (too large of a change) (No-one treated vs All treated). Thus you need to change the question...Not on and off changes, but Changes in proportion of treated!\n\n    -   Very important. a 1% increase in pop treated is different if current treatment is 10% or 90%.\n\n-   However, its possible to restructure RIF regressions to be partially conditional (Rios-Avila and Maroto 2023) (Combines CQREG with UQREG)\n\n-   Similar problems are experienced if the change in continuous variables is large!\n\n    -   Minor point. How do you construct RIFs? (analytically and Empirically)\n\n## Example {.scrollable}\n\n``` stata\nwebuse dui, clear\n**  Consider the policy change\ngen change_fines= 0.1*(12-fines)\n**  consider average change in fines.Since we are only considering this effect\nsum change_fines\n\nrifhdreg citations fines i.csize college taxes, rif(q(10)) \nest sto m1\nrifhdreg citations fines i.csize college taxes, rif(q(50)) \nest sto m2\nrifhdreg citations fines i.csize college taxes, rif(q(90)) \nest sto m3\n** This are Rescaled to show true effect\nrifhdreg citations fines i.csize college taxes, rif(q(10)) scale(.21048)\nest sto m4\nrifhdreg citations fines i.csize college taxes, rif(q(50)) scale(.21048)\nest sto m5\nrifhdreg citations fines i.csize collegetaxes, rif(q(90)) scale(.21048)\nest sto m6\n\n. esttab m1 m2 m3 m4 m5 m6, se mtitle(q10 q50 q90 r-q10 r-q50 r-q90) compress nogaps\n\n----------------------------------------------------------------------------------------\n                 (1)          (2)          (3)          (4)          (5)          (6)   \n                 q10          q50          q90        r-q10        r-q50        r-q90   \n----------------------------------------------------------------------------------------\nfines         -4.476***    -6.700***    -9.887***    -0.942***    -1.410***    -2.081***\n             (0.491)      (0.493)      (0.978)      (0.103)      (0.104)      (0.206)   \n1.csize            0            0            0            0            0            0   \n                 (.)          (.)          (.)          (.)          (.)          (.)   \n2.csize        4.603***     7.325***     6.370***     0.969***     1.542***     1.341***\n             (0.963)      (0.966)      (1.917)      (0.203)      (0.203)      (0.404)   \n3.csize        6.504***     13.54***     12.97***     1.369***     2.851***     2.729***\n             (0.914)      (0.917)      (1.820)      (0.192)      (0.193)      (0.383)   \ncollege        2.922**      5.948***     9.973***     0.615**      1.252***     2.099***\n             (0.890)      (0.892)      (1.771)      (0.187)      (0.188)      (0.373)   \ntaxes         -3.279***    -3.303***    -8.319***    -0.690***    -0.695***    -1.751***\n             (0.842)      (0.844)      (1.676)      (0.177)      (0.178)      (0.353)   \n_cons          53.71***     81.04***     129.2***     11.30***     17.06***     27.20***\n             (4.964)      (4.977)      (9.880)      (1.045)      (1.048)      (2.080)   \n----------------------------------------------------------------------------------------\nN                500          500          500          500          500          500   \n----------------------------------------------------------------------------------------\n```\n\nConsider the basic change. Fines increases in 1 unit, Cntys with taxes, increase 10%, etc\n\nOr consider rescaled effects\n\n## How Do they Compare\n\n![](resources/image-1421892762.png)\n\n## Other Considerations  {.scrollable}\n\nRIF Regressions are useful, but again, one must use them with care.\n\n-   Only Small changes! Larger changes may be meaningless\n\nExcept for `Stata` (see rif and rifhdreg), the applications of RIF regressions outside Mean, Variance and Quantiles are non-existent. (paper?)\n\n-   For most Common Statistics, RIF's automatically provide correct Standard errors (which can be Robustized!). In fact, a simple **LR** can be considered as a special case of **RIF's**\n\n$$\n\\begin{aligned}\n RIF(mean,y_i,F_y) &= y_i \\\\ \n RIF(variance,y_i,F_y) &= (y_i-\\bar y)^2 \\\\\nRIF(Q,y_i,F_Y) &= Q_y(\\tau) + \\frac{\\tau-1(y_i \\leq Q_y(\\tau))}{f_Y(y_i)}\n\\end{aligned}\n$$\n\nExcept for quantile related functions! ($f_y$ also needs estimation, thus errors!)\n\n-   Accounting for \"local\" unconditional effects beyond means require Center Polynomials:\n\n$$\nRIF(.,y) = b_0 + b_1 x + b_2 (x-\\bar x)^2+\\varepsilon\n$$\n\n-   Quantile treatment effects (on and off) are possible using PC-RIF (When you condition the distribution on just 1 variable)\n\n$$\nRIF(.,F_{Y|D},y) = b_0 + b_1 D+b_2 x + b_3 (x-\\bar x)^2+\\varepsilon\n$$\n\n## Final words on RIF\n\nBecause this implementation uses LR, you can add Multiple Fixed effects as well. (with limitations)\n\nAnd you can skip LR all together, and model RIF using Other approaches! (which may be even better than OLS).\n\n# NEXT\n\nTruly going nonlinear. When $\\beta$ is no longer linear in $y$ (nor is the error)\n\n",
    "supporting": [
      "05uqreg_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}