{
  "hash": "7a861fa3b756c92155c9556ad7da21e8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Semi- and Non- Parametric regression\"\nsubtitle: \"How flexible is flexible enough?\"\nauthor: Fernando Rios-Avila\nformat:\n  revealjs: \n    slide-number: true\n    width: 1600\n    height: 900\n    code-fold: true\n    echo: true\n    css: styles.css  \n  pdf: default  \nexecute:\n  freeze: true   \n---\n\n## Introduction\n\nWhat exactly do we mean with non parametric??\n\n-   First of all, everything we have done in the last class, concerned to the analysis of parametric relationships between $y$ and $X's$ .\n\n-   Why parametric? Because we assume that the relationship between those variables is linear, so we just need to estimate the *parameters* of that relationship. ($\\beta's$). Even tho the CEF was on itself non-parametric.\n\n-   This was just a matter of convince. Instead of trying to estimate all possible conditional means (impossible task?) we impose functional form conditions, to identify the relationship of interest.\n\n-   When we covered MLE (last semester) we even imposed functional forms assumptions on relationships and distribution!\n\n##\n### So what about non-parametric?\n\n-   Non-parametric is on the other side of the spectrum. There are no \"single\" parameters to estimate, but rather it tries to be as flexible as possible, to identify all possible relationships in the data.\n\n-   In terms of distributions, it may no longer assumes data distributes as normal, poisson, exponential, etc. Instead, it simply assumes it distributes, however it does. ðŸ¤” But isnt that a problem?\n\n-   Yes it can be.\n\n    -   On the one hand Parametric modeling is very \"strict\" regarding functional forms. (linear quadratic, logs, etc).\n\n    -   On the other, Non-parametric can be too flexible. Making the problem almost impossible to solve.\n\n-   However, the benefits of letting your data \"speak\" for itself, would allow you to avoid some problems with parametric models. \n\n##\n### Ok but what about Semi-parametric! \n\n-   Semi-parametric models try to establish a mid point between parametric and non-parametric models, attempting to draw from the benefit of both.\n\n    -   It also helps that it has a smaller computational burden (we will see what do I mean with this).\n\n-   What about an example? Say we are trying to explain \"wages\" as a function of age and education. (assume exogeneity)\n\n##\n### \n\n$$\\begin{aligned}\n\\text{Theoretical framework :}wage &= g(age, education, \\varepsilon) \\\\ \\\\\n\\text{Parametric model: }wage &= b_0 + b_1 age + b_2 education +\\varepsilon \\\\ \\\\\n\\text{Non-parametric model: }wage &= g(age,education) +\\varepsilon\n\\\\ \\\\\n\\text{Semi-parametric model: } wage &= b_0 + g_1(age) + g_2(education) +\\varepsilon \\\\ \nwage &= g_0(age)+b1 education+\\varepsilon \\\\ \nwage &= g_0(age)+g_1 (age)education+\\varepsilon\n\\end{aligned}\n$$\n\n# Preliminaries\n\n## Step1: Estimation of Density functions {.scrollable}\n\n-   The first step towards learning non-parmetric analysis, is learning to use the most basic task of all.\n\n-   **Estimating distributions (PDFs)** : why? in economics, and other social sciences, we care about distributions!\n\n    Distribution of income, how many live under poverty, how much is concentrated among the rich, how skew the distribution is, what is the level of inequality, etc, etc\n\n-   The parametric approach to estimating distribution, is by using some predefined functional form (say normal), and use the data to estimate the parameters that define that distribution:\n\n$$\n\\hat f(x) = \\frac{1}{\\sqrt{2\\pi\\hat\\sigma^2}}exp \\left(-\\frac{1}{2}\\left(\\frac{x-\\hat \\mu}{\\hat \\sigma}\\right)^2 \\right)\n$$\n\n## \n\n-   Which can be done rather easy in `Stata`\n\n```stata\nfrause oaxaca, clear\ndrop if lnwage==.\nsum lnwage\ngen f = normalden(lnwage, r(mean), r(sd))\nhistogram wages\n```\n\n![](resources/pdensity.png)\n\nBut as you can see, it does not fit well.\n\n## Histogram \n\nHistograms are a type of non-parametric estimator that imposes no functional form restrictions to estimate **probability density functions (PDFs).**\n\nConstruction histograms, is in fact, a fairly Straight forward task:\n\n1.  You select the width of bins, $h$ , and starting value $x_0$\n\n$$\\text{if } \\ x_i \\in [x_0 + m * h, x_0 + (m+1)h ) \\rightarrow \nbin(x_i)=m+1\n$$\n\n2.  And the Histogram estimator for density, is given by:\n\n$$\\hat f (x) = \\frac{1}{nh} \\sum_i 1(bin(x)=bin(x_i))\n$$\n\nSimple yet powerful, but sensitive to \"h\"\n\n##\n### Histograms with Varying h\n\n![](resources/hist_1.png)\n\n## Kernel density {.scrollable}\n\nAn alternative to Histograms is known as the kernel density estimator.\n\n$$\n\\hat f(x) = \\frac{1}{nh}\\sum_i K\\left(\\frac{X_i-x}{h}\\right)\n$$\n\nwhere $K$ is what is known as a kernel function.\n\n##\n### Kernel function\n\nA Kernel function is such that has the following properties:\n\n$$\n\\int K(z)dz = 1 ; \\int zK(z)dz = 0 ; \\int z^2K(z)dz < \\infty\n$$\n\nIs a well behaved pdf on its own, that is symmetric, with defined second moment.\n\n> as with the histogram estimator, the Kden is just an average of functions, that has the advantage of being smooth.\n>\n> Although it also depends strongly, on the choice of bandwidth.\n\n##\n### Kernel density: Visualization \n\n![](resources/kden_2.png)\n\n## {.scrollable}\n### Code in `Stata`\n\n`histogram var [if] [weight]`\n\n`kdensity var [if] [weight]`\n\n::: {#0489a562 .cell .larger execution_count=1}\n``` {.stata .cell-code}\nfrause oaxaca, clear\nkeep if lnwage!=.\nmata:\n    y = st_data(.,\"lnwage\")\n    fden = J(rows(y),1,0)\n    for(i=1; i<=rows(y); i++) {\n       fden[i] =mean(normalden(y, y[i], 0.08))\n    }\n    tg = fden,y\nend\ngetmata tg* = tg\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<style>div.jp-Notebook .datagrid-container {min-height: 448px; }</style>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     y = st_data(.,\"lnwage\")\n\n:     fden = J(rows(y),1,0)\n\n:     for(i=1; i<=rows(y); i++) {\n>        fden[i] =mean(normalden(y, y[i], 0.08))\n>     }\n\n:     tg = fden,y\n\n: end\n-------------------------------------------------------------------------------\n\n. \n```\n:::\n:::\n\n\n::: {#bea3d2ce .cell execution_count=2}\n``` {.stata .cell-code}\nscatter tg1 tg2, name(mx, replace)\n```\n\n::: {.cell-output .cell-output-display}\n![](03par_spar_npar_files/figure-revealjs/cell-3-output-1.png){}\n:::\n:::\n\n\n![](resources/kden_1.png)\n\n## Trade off: Bias vs variance {.scrollable}\n\nWhile these estimators are \"flexible\", there is one parameter that needs attention: The **bandwidth** $h$\n\nThis parameter needs to be calibrated to balance two problems in Non-parametric analysis. Bias vs Variance:\n\n1.  when $h\\rightarrow 0$ , the bias of your estimator goes to zero ( in average). Intuitively $\\hat f(x)$ is constructed based on information that comes from $x$ alone.\n\n    But the variance increases! Because things will vary for every $x$.\n\n2.  when $h \\rightarrow \\infty$ , the bias increases, because you start using data that is very different to $x$ to estimate $\\hat f(x)$.\n\n    But variance decreases. Since the \"function\" is now very smooth (a line?)\n\nThus, special attention is needed to choose the right h, which minimizes the problems (bias and variance).\n\n## Kdensity, Bias vs Variance\n\n![](resources/kden_3.png){fig-align=\"center\"}\n\n##\n### Other Considerations \n\n1.  As shown above, one needs to choose the bandwidth $h$ carefully, balancing the bias-variance trade off. Common approach is to simply use rule-of-thumb approaches to select this parameter:\n\n$$\nh = 1.059 \\sigma n ^ {-1/5} \\\\ h = 1.3643 * d * n ^ {-1/5} * min(\\sigma,iqr\\sigma)\n$$\n\nBut other approaches may work better.\n\n##\n### Other Considerations \n\n2.  A second consideration is the choice of Kernel function! (`see help kdensity -> kernel`)\n    -   Although, except in few cases, the choice of bandwidth matters more than the kernel function.\n3.  This method works well when your data is smooth and continuous. But not so much for discrete data.\n    -   Nevertheless, it is still possible to use it with discrete data, and kernel weights!\n4.  Can be \"easily\" extended to multiple dimensions $f(x,y,z,...)$, including mixture of continuous and discrete data. You just multiple Kernels!\n    -   But, beware of Curse of dimensionality.\n    -   But still better than just Subsampling!\n\n## Kfunctions\n\n![](resources/image-893322693.png)\n\n# Non-parametric Regression\n\n![](resources/smreg_1.png)\n\n## NP - Regression\n\n-   As hinted from the beginning, the idea of non-parametric regressions is related to estimate a model that is as flexible as it can be.\n\n-   This relates to the CEF, where we want to estimate a conditional mean for every combination of X's. In other words, you aim to estimate models that are valid \"locally\". A very difficult task.\n\n    -   You have a limited sample size\n    -   You may not see all possible X's combinations\n    -   and for some, you may have micro-samples (n=1) Can you really do something with this?\n\n-   Yes, make your model flexible, but not overly flexible! but how?\n\n    -   Kernel regression ; Spline regression\n    -   Polynomial regression; Smoothed Spline regression.\n\n## Univariate case {.scrollable}\n\n-   Consider a univariate case $y,X$ where you only have 1 indep variable, which are related as follows:\n\n$$\ny = m(x) + e\n$$\n\nwhich imposes the ***simplifying*** assumption that error is additive.\n\n-   In the parametric case, you could model this as a linear relationship:\n\n$$\ny =b_0 + b_1 x + b_2 x^2 +b_3 x^3 +...+e\n$$\n\n(this is, in fact, starting to become less parametric)\n\n##\n\n-   But in the full (unconstrained) model it would just be the conditional mean:\n\n$$\nE(y|X) = \\hat m(x) = \\frac{\\sum y_i 1(x_i=x)}{\\sum 1(x_i=x)}\n$$\n\nProblems? Impossible to do out of sample predictions, and if $n<42$ inference would be extremely unreliable.\n\n## {.scrollable}\n### Local Constant Regression \n\nWe can improve over the Unconstrained mean using the following connection:\n\n1.  $1(x_i=x)$ is a non-smooth indicator that shows if an observation is in-sample .\n\n2.  We can substitute this with a smooth indicator function\n\n$$\nK_w(x_i,x) = \\frac{K\\left(\\frac{x_i-x}{h}\\right)}{K(0)}\n$$\n\nObservations where $x_i=x$ will have a weight of 1, but depending on $h$, less weight is given the farther $x_i$ is to $x$.\n\n##\n\nThis gives what is known as the Nadaraya-Watson or Local constant estimator:\n\n$$\n\\hat m(x) = \\frac{\\sum y_i K_w(x_i,x)}{\\sum K_w(x_i,x)} = \\sum y_i w_i\n$$\n\nWhich, on its core, is simply a weighted regression, with weights given by $\\frac{K_w(x_i,x)}{\\sum K_w(x_i,x)}$\n\nKernel Regressions \"borrows\" info from neighboring observations to obtain a smooth estimator.\n\n## \n### Visuals\n\n![](resources/smreg_2.png){fig-align=\"center\"}\n\n## {.scrollable}\n### Implementation\n\n::: {#d8c07cb9 .cell execution_count=3}\n``` {.stata .cell-code}\nwebuse motorcycle, clear\nscatter accel time \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Motorcycle data from Fan & Gijbels (1996))\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](03par_spar_npar_files/figure-revealjs/cell-4-output-2.png){}\n:::\n:::\n\n\n::: {#b1caddc6 .cell execution_count=4}\n``` {.stata .cell-code}\nmata:\n    y = st_data(.,\"accel\")\n    x = st_data(.,\"time\")\n    yh = J(133,12,0)\n    for(k=1;k<=12;k=k++) {\n        for(i=1;i<=133;i++){\n            h = k/2\n            yh[i,k]=mean(y, normalden(x, x[i], h))\n        }\n    }    \nend\ngetmata yh* = yh, replace\ncolor_style viridis\ntwo scatter accel time || line yh* time, xsize(7) ysize(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n. mata:\n------------------------------------------------- mata (type end to exit) -----\n:     y = st_data(.,\"accel\")\n\n:     x = st_data(.,\"time\")\n\n:     yh = J(133,12,0)\n\n:     for(k=1;k<=12;k=k++) {\n>         for(i=1;i<=133;i++){\n>             h = k/2\n>             yh[i,k]=mean(y, normalden(x, x[i], h))\n>         }\n>     }\n\n: end\n-------------------------------------------------------------------------------\n\n. \n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](03par_spar_npar_files/figure-revealjs/cell-5-output-2.png){}\n:::\n:::\n\n\n## {.scrollable}\n### Considerations \n\n1.  Local Constant estimator is simple to estimate with a single variable. Multiple variables is just as easy:\n\n$$\n\\hat m(x,z) = \\frac{\\sum y_i K_h(x_i,x) \\times K_h(z_i,z)}{\\sum K_h(x_i,x) \\times K_h(z_i,z)}\n$$\n\nThe problem, however, lies on the curse of dimensionality. \n\nMore dimensions, less data per $(x,z)$ point, unless you \"increase\" bandwidths.\n\n2.  As before, it all depends on the Bandwidth $h$, which determines the \"flexibility\" of the model.\n\n3.  The local constant tends to have considerable bias (specially near limits of the distribution, or when $g$ has too much curvature)\n\n##\n### Choosing h\n\nThe quality of the NPK regression depends strongly on the choice of $h$. And as with density estimation, the choice translates into a tradeoff between bias and variance of the estimation.\n\nThere are various approaches to choose $h$. Some which depend strongly on the dimensionality of the model.\n\nFor Example, `Stata` command `lpoly` estimates local constant models, using the following:\n\n![](resources/image-1628248108.png){fig-align=\"center\" width=\"100\"}\n\nBut that is not the only approach.\n\nAn alternative (used for regularization) is using Cross-Validaton. (a method to evaluate the predictive power of a model)\n\n## Cross Validation: Intuition {.scrollable}\n\n1.  Separate your data in two parts: Training and testing Sample.\n\n2.  Estimate your model in the TrainS, and evaluate predictive power in TestS.\n\n3.  To obtain a full view of predictive power, Repeat the process rotating the training set\n\n$$\nmse = \\frac{1}{N}\\sum(y_i - g_{-k}(x))^2 \n$$\n\nThis should give you a better idea of the predictive power of the model.\n\n![](resources/image-1717331850.png){width=\"800\"}\n\n\n## {.scrollable}\n###  Cross-validation in `Stata` \n\n```stata{.larger}\n\nfrause oaxaca, clear\nssc install cv_kfold\n\nqui:reg lnwage educ exper tenure female age\n\ncv_kfold\nk-fold Cross validation\nNumber of Folds     :          5\nNumber of Repetions :          1\nAvg Root Mean SE    :    0.45838\n\nqui:reg lnwage c.(educ exper tenure female age)\n               ##c.(educ exper tenure female age)\n\ncv_kfold\nk-fold Cross validation\nNumber of Folds     :          5\nNumber of Repetions :          1\nAvg Root Mean SE    :    0.42768\n\n. qui:reg lnwage c.(educ exper tenure female age)\n                 ##c.(educ exper tenure female age)\n                 ##c.(educ exper tenure female age)\n\n. cv_kfold\nk-fold Cross validation\nNumber of Folds     :          5\nNumber of Repetions :          1\nAvg Root Mean SE    :    0.43038\n\nssc install cv_regress\n* Does lOOCV for regression\ncv_regress\n\nLeave-One-Out Cross-Validation Results \n-----------------------------------------\n         Method          |    Value\n-------------------------+---------------\nRoot Mean Squared Errors |       0.4244\nLog Mean Squared Errors  |      -1.7144\nMean Absolute Errors     |       0.2895\nPseudo-R2                |      0.36344\n-----------------------------------------\n```\n\n## LOOCV {.scrollable}\n\nBecause the \"choice\" of \"folds\" and Repetitions, and the randomness, may produce different results every-time, one also has the option of using the \"leave-one-out\" approach.\n\nThis means, leave one observation out, and use the rest to make the predictions.\n\n$$\nCV(h) = n^{-1}\\sum_{i=1}^n(y_i - \\hat g_{-i}(x_i))^2\n$$\n\nThis is not as bad as it looks, since we can use the shortcut\n\n$$\nCV(h) = n^{-1}\\sum_{i=1}^n\\left(\\frac{y_i - \\hat g(x_i)}{1-w_i/\\Sigma w_j}\\right)^2\n$$\n\nIn `Stata`, the command `npregress kernel` uses this type of cross-validation to determine \"optimal\" $h$\n\n```stata\nlpoly y x, kernel(gaussian) nodraw\ndisplay r(bwidth)\n.23992564\nnpregress kernel y x, estimator(constant) noderiv\n. Bandwidth\n-------------------------\n             |      Mean \n-------------+-----------\n           x |  .4064052 \n-------------------------\n```\n\n## Extending from constant to Polynomial {.scrollable}\n\nAn alternative way to understanding the simple NW (local constant) regressions, is to understand it as a local regression model with anything but a constant:\n\n$$\n\\hat m(x)=min\\sum(y_i - \\beta_0)^2 w(x,h)_i\n$$\n\nThis means that you could extend the analogy and include \"centered\" polynomials to the model.\n\n$$\n\\begin{aligned}\nmin &\\sum(y_i - \\beta_0 - \\beta_1 (x_i -x) -\\beta_2 (x_i - x) ^2 - ...-\\beta_k(x_i-x)^k)^2 w(x,h)_i \\\\\n\\hat m(x) &= \\hat \\beta_0\n\\end{aligned}\n$$\n\nThis is called the local polynomial regression.\n\n-   Because its more flexible, it shows less bias when the true function shows a lot of variation.\n\n-   Because of added polynomials, it requires more information (larger $h$)\n\n-   It can be used to easily obtain local marginal effects.\n\n-   And can also be used with multinomial models (local planes)\n\n$$min \\sum (y_i - \\beta_0 - \\beta_1 (x_i-x) - \\beta_2 (z_i-z))^2 w(x,z,h)\n$$\n\n## Local Constant to Local Polynomial {.scrollable}\n\n::: columns\n::: {.column width=\"40%\"}\n```stata\nwebuse motorcycle\ntwo scatter accel time || ///\nlpoly accel time , degree(0) n(100) || ///\nlpoly accel time , degree(1) n(100) || ///\nlpoly accel time , degree(2) n(100) || ///\nlpoly accel time , degree(3) n(100) , ///\nlegend(order(2 \"LConstant\" 3 \"Local Linear\" ///\n4 \"Local Cubic\" 5 \"Local Quartic\"))\n```\n:::\n\n::: {.column width=\"60%\"}\n![](resources/lllreg.png)\n:::\n:::\n\n## Statistical Inference {.scrollable}\n\nFor Statistical Inference, since each regression is just a linear model, standard errors can be obtained using the criteria as in Lecture 1. (Robust, Clustered, bootstrapped).\n\n-   With perhaps one caveat. Local estimation and standard errors may need to be estimated \"globally\", rather than locally.\n\nThe estimation of marginal effects becomes a bit more problematic.\n\n-   Local marginal effects are straightforward (when local linear or higher local polynomial is used)\n\n-   Global marginal effects, can be obtained averaging all local marginal effects.\n\n-   However, asymptotic standard errors are difficult to obtain (consider the multiple correlated components), but bootstrapping is still possible.\n\n## `Stata` Example {.scrollable}\n\n```stata\nfrause oaxaca\nnpregress kernel lnwage age exper\n\nComputing mean function\n  \nMinimizing cross-validation function:\nIteration 6:   Cross-validation criterion = -1.5912075  \n  \nComputing optimal derivative bandwidth\nIteration 3:   Cross-validation criterion =  .00196371  \n\nBandwidth\n------------------------------------\n             |      Mean     Effect \n-------------+----------------------\n         age |  2.843778   15.10978 \n       exper |  3.113587   16.54335 \n------------------------------------\n\nLocal-linear regression                    Number of obs      =          1,434\nKernel   : epanechnikov                    E(Kernel obs)      =          1,434\nBandwidth: cross-validation                R-squared          =         0.3099\n------------------------------------------------------------------------------\n      lnwage |   Estimate\n-------------+----------------------------------------------------------------\nMean         |\n      lnwage |   3.339269\n-------------+----------------------------------------------------------------\nEffect       |\n         age |   .0169326\n       exper |  -.0010196\n------------------------------------------------------------------------------\nNote: Effect estimates are averages of derivatives.\nNote: You may compute standard errors using vce(bootstrap) or reps().\n```\n\n## Other types of \"non-parametric\" models\n\nWe have explored the basic version of **non-parametric** modeling. But its not the only one.\n\nThere are at least two others that are *easy* to implement.\n\n1.  Nonparametric Series Regression (we will see this)\n2.  Smoothing series/splines: Which borrows from Series regression and Ridge Regression.\n\n## Non-parametric series {.scrollable}\n\nThis approach assumes that model flexibility can achieve by using \"basis\" functions in combination with Interactions, but using \"global\" regressions (OLS)\n\nBut what are \"basis\" functions? They are a collection of terms that approximates smooth functions arbitrarily well.\n\n$$\n\\begin{aligned}\ny &= m(x,z)+e  \\\\\nm(x,z)  &= B(x)+ B(z)+B(x)*B(z)  \\\\\nB(x)  &= (x, x^2, x^3,...) \\\\\nB(x)  & = fracPoly \\\\ \nB(x)  &= (x, max(0,x-c_1), max(0,x-c_2), ... \\\\\nB(x)  &= (x,x^2,max(0,x-c_1)^2, max(0,x-c_2)^2, ... \\\\\nB(x)  &= B-splines \n\\end{aligned}\n$$\n\n##\n\n-   Polynomials can be used, but there may be problems with high order polynomials. (*Runge's phenomenon,multiple-co-linearity*). They are \"global\" estimators.\n\n-   Fractional polynomials: More flexible than polynomials, without producing \"waves\" on the predictions\n\n-   Natural Splines, are better at capturing smooth transitions (depending on degree). Require choosing Knots appropriately.\n\n-   B-splines are similar to N-splines, but with better stat properties. Also require choosing knots\n\nExcept for correctly estimating the Bases functions (fracpoly and Bsplines are not straight forward), estimation requires simple OLS.\n\n## NP series - tuning\n\n-   While NP-series are easy to estimate, we also need to address problems of over-fitting.\n\n-   With Polynomial: What degree of polynomial is correct? What about the degree of the interactions?\n\n-   Fractional Polynomials: How many terms are needed, what would their \"degrees\" be.\n\n-   Nsplines, Bsplines: How to choose degree? and where to set the knots?\n\nThese questions are similar to the choosing $h$ in kernel regressions. However, model choice is simple...Cross validation.\n\n> Estimate a model under different specifications (cut offs), and compare the out-of-sample predictive power. (`see Stata: cv_kfold or cv_regress`)\n\nOne more problem left. Statistical Inference\n\n## NP series - SE and Mfx {.scrollable}\n\nLecture 1 applies here. Once the model has been chosen, you can estimate SE using appropriate methods. There is only one caveat\n\n-   Standard SE estimation ignores the uncertainty of choosing cut-offs or polynomial degrees. In principle, cut-offs uncertainty can be modeled. But requires non-linear estimation.\n\nMarginal effects are somewhat easier for some basis. Just take derivatives:\n\n$$\ny = b_0 + b_1 x + b_2 x^2 +b_3 max(0,x-c)^2 + e \\\\\n\\frac{dy}{dx}=b_1 + 2 b_2 x + 2 b_3 (x-c) 1(x>c)\n$$\n\n-   But keeping track of derivatives in a multivariate model is difficult, and often, the functions are hard to track down. so how to implement it?\n\n## NP series: Implementation marginal effects {.scrollable}\n\nAs always, it all depends on how are the models estimated.\n\n-   `Stata` command `npregress series` allows you to estimate this type of models using polynomials, splines and B-splines. And also allows estimates marginal effects for you. (can be slow)\n\n-   `fp` estimates fractional polynomials, but does not estimate marginal effects for you.\n\n-   In `Stata`, you can use the package `f_able` to estimate those marginal effects, however. see [here](https://journals.sagepub.com/doi/pdf/10.1177/1536867X211000005) for details. and `SSC` for the latest update.\n\n```stata\nfrause oaxaca, clear\ndrop agesq\nf_spline age = age, nk(1) degree(3)\nf_spline exper = exper, nk(1) degree(3)\nqui:regress lnwage c.(age*)##c.(exper*)\nf_able age? exper?, auto \nmargins, dydx(age exper) noestimcheck \n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         age |   .0360234   .0033909    10.62   0.000     .0293775    .0426694\n       exper |   .0082594   .0050073     1.65   0.099    -.0015547    .0180735\n------------------------------------------------------------------------------\n```\n\n## Semiparametric Regressions\n\n-   Full non-parametric estimations are powerful to identify very flexible functional forms. To avoid overfitting, however, one must choose tuning parameters appropriately ($h$ and $cutoffs$ ).\n\n-   A disadvantage: Curse of dimensionality. More variables need more data to provide good results. But, the more data you have, the more difficult to estimate (computing time).\n\n-   It also becomes extremly difficult to interpret. (too much flexibility)\n\n-   An alternative, Use the best of both worlds: Semiparametric regression\n\n    -   Flexibility when needed with the structure of standard regressions, to avoid the downfalls of fully nonparametric models\n\n## Partially Linear model {.scrollable}\n\n$$\ny = x\\beta +g(z) +e\n$$\n\nThis model assumes that only a smaller set of covariates need to be estimated non-parametrically in the model.\n\nEstimators:\n\n-   `npregress series`: Use Basis to estimate $g(z)$ \n   \n-   Yatchew 1997: For a single z, sort variables by it, and estimate: $\\Delta y=\\Delta X\\beta+ \\Delta g(z) + \\Delta e$. This works because $\\Delta g(z)\\rightarrow 0$\n\n    Estimate $g(z)$ regressing $y-x\\hat \\beta$ on $z$. See [plreg](https://journals.sagepub.com/doi/pdf/10.1177/1536867X0600600306)\n\n-   Robinson 1988: Application of FWL. Estimate $y = g_y(z)+e_y$ and $x = g_x(z)+e_x$ and estimate $\\beta = (e_x ' e_x)^{-1} e_x ' e_y$ . For $g(z)$ same as before. See [semipar](https://journals.sagepub.com/doi/pdf/10.1177/1536867X1201200411).\n\n-   Other methods available see [semi_stata](https://www.stata.com/meeting/uk13/abstracts/materials/uk13_verardi.pdf)\n\n## Generalized Additive model\n\n$$\ny = g(x) +g(z)+e\n$$\n\nThis model assumes the effect of X and Z (or any other variables) are additive separable, and may have a nonlinear effect on y.\n\n-   `npregress series`: with non-interaction option. Fractional polynomials `mfp`, cubic splines `mvrs` (see [mvrs](https://journals.sagepub.com/doi/pdf/10.1177/1536867X0700700103)) , or manual implementation.\n\n-   Kernel regression possible. (as in Robinson 1988), but requires an iterative method. (back fitting algorithm)\n\n    -   $g(x) = smooth (y-g(z)|x)$, center $g(x)$ , and $g(z) = smooth (y-g(x)|z)$, center $g(z)$ until convergence\n\n-   In general, it can be easy to apply, but extra work required for marginal effects.\n\n## Smooth Coefficient model\n\n$$\ny = g_0(z)+g_1(z)x + e \n$$\n\nThis model assumes that $X's$ have a locally linear effect on $y$, but that effect varies across values of $z$, in a non-parametric way.\n\n-   `fp` or manual implementation of basis functions, with interaction. May allow for multiple variables in $z$\n\n-   One can also use Local Kernel regressions. locally weighted regression where All X variables are considered fixed, or interacted with polynomials of Z. Choice of bandwidth problematic, but doable (LOOCV).\n\n    [vc_pack](https://journals.sagepub.com/doi/abs/10.1177/1536867X20953574) can estimate this models with a single z, as well as test it. Overall marginal effects still difficult to obtain.\n\n## Example\n\n```stata\n\nfrause oaxaca\nvc_bw lnwage educ exper tenure female married divorced, vcoeff(age)\nvc_reg lnwage educ exper tenure female married divorced, vcoeff(age) k(20)\nssc install addplot\nvc_graph educ exper tenure female married divorced, rarea\naddplot grph1:, legend(off) title(Education)\naddplot grph2:, legend(off) title(Experience)\naddplot grph3:, legend(off) title(Tenure)\naddplot grph4:, legend(off) title(Female)\naddplot grph5:, legend(off) title(Married)\naddplot grph6:, legend(off) title(Divorced)\n\ngraph combine grph1 grph2 grph3 grph4 grph5 grph6\n```\n\n## Example\n\n![Wage Profile across years](resources/vc_plot.png){fig-align=\"center\"}\n\n# The end: Next time Quantile regressions\n\n",
    "supporting": [
      "03par_spar_npar_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}