{
  "hash": "bbc37d43e727da6a283f6348b01140ad",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Instrumental Variables\"\nsubtitle: \"Many are called, only few answer\"\nauthor: Fernando Rios-Avila\nformat:\n  revealjs: \n    slide-number: true\n    width: 1500\n    height: 900\n    code-fold: true\n    echo: true\n    css: styles.css  \n    theme: serif\n    mermaid:\n      theme: forest\nexecute:\n  freeze: true       \n---\n\n## Recap: PO's and RCT's\n\n- Quick Recap. The goal of the methodologies we are covering is to identify treatment effects.\n\n- In the PO framework, that is done by simply comparing a group with itself, in two different States (treated vs untreated)\n\n- Since this is **impossible**, the next best solution is using RCT. Individuals are randomized, and *assuming* every body follows directions,\nwe can identify treatment effects of the experiments. \n\n- But only if the RCT is well executed! Sometimes even that may fail\n\n## Instrumental Variables\n\n- While here discussed 3rd, the second best approach to identify Treatment effects is by using Instrumental variables.\n\n- In fact with a **Good-Enough** instrument, one should be able to identify **ANY** causal effect. Assuming such IV exists.\n\nbut how?\n\n- If the instrument is good, it may create an exogenous variation, which will allow us to identify Treatment effects by looking ONLY at those affected by the treatment!\n\n- Using the external variation, we can Estimate TE comparing two groups who are identical in every aspect, except being expose to the Instrument, because they were exposed to the instrument. The randomization comes Because of the IV!\n\n## Cannonical IV \n\nAs we have mentioned, the estimation of TE require that we identify two groups of individuals with mostly similar (if not identical) characteristics.\nThis include unobserved characteristics.\n\nIf the latter is not true, we have a problem of confunders or Endogeneity. But why?\n\nConsider the following diagram\n\n:::{layout=\"[1,1]\"}\n\nHere the effect of $D$ on $Y$ is direct, because there is nothing else that would get people confuse why treatment affects outcome \n\n```{mermaid }\n%%| fig-width: 8.5\n%%| echo: off\nflowchart LR\n  e(error) --> Y(Outcome)  \n  D(Treatment) --> Y(Outcome)  \n```\n\n:::\n\n:::{layout=\"[1,1]\"}\n\nHere the effect of $D$ on $Y$ is not as clear, because there is an additional factor $v$ that affects $D$ and $Y$ (is in the way)\n\n```{mermaid }\n%%| fig-responsive: false\n%%| echo: off\nflowchart LR\n  e( error ) --> Y(Outcome)  \n  D( Treatment ) --> Y(Outcome)\n  v(unobserved) -.-> D(Treatment)  \n  v(unobserved) -.-> Y(Outcome)  \n```\n\n:::\n\n## Cannonical IV\n\nHere is where a good instrument comes into play. \n\n- Not everything in $D$ is affected by $v$. Some may, but some may be trully exogenous. What if we have an instrument that helps you ID this:\n\n\n```{mermaid }\n%%| fig-responsive: false\n%%| echo: off\nflowchart LR\ne( error ) --> Y(Outcome)  \n  subgraph Treatment\n    D1(Exog) \n    D2(Endog)\n  end\n  \n  D1( Exog ) --> Y(Outcome)\n  D2( Endog ) --> Y(Outcome)\n  Z(Instrument) --> D1  \n  v(unobserved) -.-> D2\n  \n  v(unobserved) -.-> Y(Outcome)  \n```\n\n\n- By Isolating those affected by the Instrument Alone, we do not need to worry about endogeneity anymore.\n\n## Properties\n\nInstrumental variables should have at the very list 2 Properties\n\n1. The instrumental variable $Z$ should not be correlated with the model error (Validity).\n2. But, it should explain the treatment Itself $D$ (Relevance).\n\nFailure of (1) may reintroduce problems of endogeneity. Faiture of (2) will make the instrument Irrelevant.\n\n## How does it work\n\nConsider the following. \n\n- People who study more, tend to earn higher wages\n- People with high ability tend to study more.\n- People with high ability, also earn higher wages.\n\nDoes Studying more generate higher wages?\n\n**Instrument**. We create a lottery that provides some people resources to pay for their education. This gives them a chance to study more (regardless of ability).\n$$Z \\rightarrow D$$\n\n## \n\nSo, we know the instrument was **Random**. We can analyze how much outcome increases among those benefited by the Lottery.\n\n$$E(W|Z=1)-E(W|Z=0)$$\n\n- This is often called the **reduced form** effect. \n\n- In principle, $Z$ only affects wages because of education. So looking at this differences should be similar to a treatment effect of Lotteries.\n\n- These are also known as Intention to treatment effect. Which will bias towards zero, because not everyone will effectively make use of the opporunities\n\n##\n\nIn othe words, not everyone will Study more...So we can see if the lotery had that effect.\n\n$$E(S|Z=1)-E(S|Z=0)$$\n\nThis is the equivalent to the first stage. Where we measure the impact of the \"instrument/lottery\" on Education (to see, say, relevance)\n\nFinally, the TE is given by the Ratio of thes two\n\n$$TE=\\frac{E(W|Z=1)-E(W|Z=0)}{E(S|Z=1)-E(S|Z=0)}\n$$\n\nThis is also known as the Wald Estimator. How much of the changes in wages is due to changes in the \"# treated\"\n\n## Some commnents\n\n- This was an example of a binary instrument, which was assigned at random. \n- In fact, this particular scenario is typical byproduct of \"failed\" RCTs!\n  - Partially failed RCTs: Not every body selected WAS treated\n\nConsider the following: \n\n- We make the RCT above giving bouchers to People so they Study more.\n- But, not everybody uses the bouchers:\n  - Some use them and study more.\n  - Some decide to not use them.\n\nComparing Wages among those who receive will only provide you the \"intention to treat\" effect. (**Reduced form**)\n\nBecause of imperfect compliance we need to \"readjust/inflate\" our TE estimate.\n\n## More Comments\n\n- In this scenario the Reduced form and second stage can be estimated by just comparing means, because the treatment was randonmized. \n\n  - In other words, something you really want is an instrument that is as good as random. \n\n- The effect we capture is a LOCAL treatment effect (LATE). \n- \n- However, it could be an ATE if:\n  - The effect is homogenous for everyone. \n  - The people affected by the treatment is a representative of the population\n\n- It all boils down to identifying who is or might be affected by the treatment. \n\n- For now, lets assume effects are Homogenous (So we get ATEs)\n  \n## Who Are Affected and who are not?\n\nEven if we are able to identify ATEs, its important to understand who can be affected by the instrument, because the population is generally selected in 3 groups\n\n- **never takers** & **always takers**: These are the individuals who would have never done anything different than their normal. \n  - Perhaps their likelihood was already too low (or high) to be affected.\n- **Compliers**: These are the ones who, given they receive \"instrument\", they comply and follow up. We use their variation for analysis.\n- **Defiers**: These are the ones who, given Z, will do the oposite. We cannot differentiate them from Compliers, so they will affect how treatment is estimated.\n\nWe do not want to have defiers!\n\n## Extension 1: Continuous Instrument\n\nThe Wald estimator is for the simplest case of binary treatment. However, if the treatment is continuous, one could modify the IV estimator as follows:\n\n$$\n\\delta_{IV} = \\frac{cov(y,z)}{cov(d,z)}\n$$\n\nThe logic remains. We are trying to see how variation in the outcome related to Z reates to changes in treatment because of Z. \n\nThe *treatment* here is very small (Small changes in d). The intuition is that we are averaging the variation in the outcome across all Zs to estimate the effect. \n\n## Extension 2: Controls \n\nAdding controls to the model is also straight forward, and you have quite a few options for it\n\n1. Adding exogenous controls may help improving model precision, even if instrument was randomized. The easiest way to do this is by applying the 2sls procedure (among others)\n\n$$\n\\begin{aligned}\n1st: d = z\\gamma_z + x\\gamma_x + e_1  \\\\\n2nd: y = x\\beta_x + \\delta \\hat d+ e_2\n\\end{aligned}\n$$\n\n- The 1st stage \"randomizes\" instrument to measure the effect on treatment. \n\n- The 2nd stage uses predicted values of the first to see what the impact on the outcome will be.\n\n- This works because $\\hat d$ is exogenous, \"carrying over\" exogenous changes in the treatment. \n\n## Extension 2: Controls \n\nOne can also think of the approach as a pseudo Wald Estimator, with continuous variables:\n\n$$\n\\begin{aligned}\n1st: d &= \\gamma_z * z + x\\gamma_x + e_1  \\\\\nrd:  y &= \\beta_z  * z+ x\\beta_x + e_2 \\\\\nATE &=\\frac{\\beta_z}{\\gamma_z}=\\frac{cov(y,\\tilde z)}{cov(d,\\tilde z)}\n\\end{aligned}\n$$\n\nThis compares average changes in the outcome to average changes in the treatment.\n\n## Extension 3: Multiple Endogenous Variables\n\nAlthough less common in Causal Analysis perspective, in other frameworks one may to consider more than 1 instrument or using instrument interactions. In these cases one still has two alternatives\n\n1. **2SLS**: One can model more than one endogenous variable at the same time, simply substituting the predicted values in the main regression. When using interactions, or polynomials, each will need its own first stage regression.\n2. **Control function approach**. In contrast with the \"prediction subtstitution\" approach, this method suggests using a \"residual inclusion approach\". This controls for endogeneity directly. If there is only one endogenous variable (with interactions of polynomials) only one model is needed.\n\nIn the first case, you need at least 1 instrument per regression. Even if its just a transformation of the original variable\n\nIn the second case, you need at least 1 instrument per endogenous variable. \n\n## Instrument Validity\n\nAs mentioned earlier, Instruments require to fullfill two conditions:\n\n- **Relevant**. They need to be Strongly related to the endogenous variable\n- **Exogenous**. instruments should not and cannot be endogenous. In fact, you want instruments that are as good as random, thus not defined by the \"system\" in anyway.\n\n## IV Validity: Exogeneity\n\nUnfortunately, for most cases, this assumption is not testable, because we do not observe the model unobservables, thus dont know if $z$ is related to those unobserved components.\n\nWhile most efforts for these are done through model design, or argumentation, there are at least 2 options to verify the exogeneity\n\n1. If truly exogenous, the instrument should be as good as random. Thus controls shouldnt be affected by the instrument. (Balance test)\n   \n2. Otherwise, one could test for exogeneity only by comparing Estimates across different IV's. Different results may suggest instruments are invalid.\n   \n   - Run a regression of Residuals from the main model against all exogenous variables plus other instruments.\n\nNote: Unless the instrument was randomized, assumed is going to be slighly endogenous.\n\n## IV Validity: Strength\n\nThe only thing we could probably do is try to analyze model strength. How much does the instrument affect treatment take up? is the effect marginal? or a large effect?\n\nWeaker instruments may create larger problems on the analysis because:\n\n1. With weaker instruments, the precision of the estimator drops substantially.\n2. With weaker instruments, any \"endogeneity\" problem (even due to randomness) will generate a bias\n\n- Stock and Yogo (2005) suggest and F~13.9 (or higher) for a 5% bias\n- Lee, et al (2020) suggest you need even higher F's if you want to avoid problems with CI\n\n3. With weak instruments, distribution of beta coefficients will no longer be normal!\n  \n## IV Strength {.scrollable}\n\n::: {#a75c88f2 .cell execution_count=1}\n``` {.stata .cell-code code-fold=\"true\"}\ncapture program drop simx\nprogram simx, eclass\n\tclear\n\tset obs 500\n\tgen z=rnormal()>0\n\tgen u1=rnormal()\n\tgen u2=rnormal()\n\tgen u3=rnormal()\n\tforvalues i = 1/5 {\n\t\tgen d`i' = ((-0.5+z) + (u1 + u2)*0.5*`i')>0\n\t\tgen y`i' = 1 + d`i'+u3+u2\n\t}\n\tforvalues i = 1/5 {\n\t\treg d`i' z\n\t\tmatrix b`i' =(_b[z]/_se[z])^2\n\t\tivregress  2sls y`i' (d`i'=z)\n\t\tmatrix b`i' = b`i',_b[d`i'],_se[d`i'],_b[d`i']/_se[d`i']\n\t\tmatrix colname b`i'=f_stat beta beta_se beta_t\n\t\tmatrix coleq b`i'=md`i'\n\t}\n\tmatrix b=b1\n\tforvalues i = 2/5 {\n\t\tmatrix b=b,b`i'\n\t}\n\tereturn post b\nend\n```\n:::\n\n\n```stata\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\nmd1_b_f_stat |        494    189.9262    38.63693   101.8865   337.4117\n  md1_b_beta |        494    .9912474    .2289997   .1758122    1.64053\nmd1_b_beta~e |        494    .2440663    .0235266   .1867024   .3293121\n-------------+---------------------------------------------------------\nmd2_b_f_stat |        494    43.33591    13.76347   12.07967   101.6624\n  md2_b_beta |        494    .9717824    .4421832  -.8390987   2.327243\nmd2_b_beta~e |        494    .4711093    .0945389   .3032212   .9873207\n-------------+---------------------------------------------------------\nmd3_b_f_stat |        494    19.25043     8.81203   2.051613   61.55846\n  md3_b_beta |        494    .9354647    .6820819  -1.964163   2.996998\nmd3_b_beta~e |        494    .7430147    .2493365    .383426    2.16554\n-------------+---------------------------------------------------------\nmd4_b_f_stat |        494    11.19082    6.428525   .0483399   38.44057\n  md4_b_beta |        494    .8711808    .9987467  -4.383311   5.634943\nmd4_b_beta~e |        494    1.135114    1.244907   .4609722   25.16503\n-------------+---------------------------------------------------------\nmd5_b_f_stat |        494    7.522731    5.144063   .0544148   28.23535\n  md5_b_beta |        494    .7696057    1.443986  -6.293723   8.951643\nmd5_b_beta~e |        494     1.69479    1.866909    .530196    18.3837\n```\n## IV Strength: Bias distribution \n\n::: {#768cbb4d .cell execution_count=2}\n``` {.stata .cell-code}\nuse resources/simiv.dta, clear\nforvalues i = 1/5 {\n  qui:sum md`i'_b_beta\n  gen new`i'=(md`i'_b_beta-1)/r(sd)\n}\nset scheme white2\ncolor_style tableau\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new1, name(m1, replace) , legend(off)\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new2, name(m2, replace) , legend(off)\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new3, name(m3, replace) , legend(off)\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new4, name(m4, replace) , legend(off)\ntwo function y=normalden(x), range(-5 5) lwidth(1) pstyle(p2) || histogram new5, name(m5, replace) , legend(off)\ngraph combine m1 m2 m3 m4 m5, col(3) xcommon ycommon\ngraph export resources/cmb.png, width(1500)  replace\n```\n:::\n\n\n![](resources/cmb.png)\n\n## IV Strength Solution: `weakiv`\n\n- Weak IV's are a problem in the sense that it may induce bias on the estimated coefficients, but also that it may affect how Standard Errors are estimated.\n\n  - The distribution of the Statistic is no longer normal\n  \n- One solution, in this case, is at least adjusting SE and CI So they better reflect the problem.\n\n- In Stata, this can be done with `weakiv` (ssc install weakiv)\n\n- At the end, however, if you weak instruments, you may be able to correct of potential biases, but you may need to get \nmore data, or better instruments\n\n## LATE: Local Average Treatement Effect\n\nUp to this point, we imposed the assumption that TE were homogenous. Thus, IV could identify Treatment effects for everyone. (Average Treatment effect)\n\nHowever, not everyone may be affected by the instrument, only by the compliers.\n\nTwo ways of thinking about it:\n\n1. Not everybody is affected by the instrument. (you have the always and never takers) \n2. the instrument was never suppoused to affect certain groups!\n\nSo, IV will identify TE for the compliers only.\n\nBecause of this, using different instruments may actually identify different effects, based on which population was affected. \n\nOverid tests may fail in this case.\n\n## Simulation Example: {.scrollable}\n\n::: {#bc4814cd .cell execution_count=3}\n``` {.stata .cell-code}\nclear\nset obs 10000\ngen sex = rnormal()>0\ngen z1 = rnormal()>0\ngen z2 = rnormal()>0\ngen e_1 =rnormal()\ngen e_2 =rnormal()\ngen e_3 =rnormal()\ngen D =(z1*(sex==0) + z2*(sex==1) + (e_1 + e_2)*.5)>0\ngen Ds =(  (e_1 + e_2)*.5)>0\ngen y = 1 + D*(sex==0) +2*D*(sex==1)+e_3+e_2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of observations (_N) was 0, now 10,000.\n```\n:::\n:::\n\n\n::: {#6891c1c9 .cell execution_count=4}\n``` {.stata .cell-code}\n%%echo\nivregress 2sls y (D=z1)\nivregress 2sls y (D=z2)\nivregress 2sls y (D=z1 z2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n. ivregress 2sls y (D=z1)\n\nInstrumental variables 2SLS regression            Number of obs   =     10,000\n                                                  Wald chi2(1)    =      68.61\n                                                  Prob > chi2     =     0.0000\n                                                  R-squared       =     0.2706\n                                                  Root MSE        =     1.5526\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           D |   1.170842   .1413543     8.28   0.000     .8937923    1.447891\n       _cons |   1.233464   .1015275    12.15   0.000     1.034474    1.432455\n------------------------------------------------------------------------------\nEndogenous: D\nExogenous:  z1\n\n. ivregress 2sls y (D=z2)\n\nInstrumental variables 2SLS regression            Number of obs   =     10,000\n                                                  Wald chi2(1)    =     211.38\n                                                  Prob > chi2     =     0.0000\n                                                  R-squared       =     0.3558\n                                                  Root MSE        =     1.4591\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           D |   1.948051   .1339876    14.54   0.000      1.68544    2.210662\n       _cons |   .6818014   .0962172     7.09   0.000     .4932192    .8703836\n------------------------------------------------------------------------------\nEndogenous: D\nExogenous:  z2\n\n. ivregress 2sls y (D=z1 z2)\n\nInstrumental variables 2SLS regression            Number of obs   =     10,000\n                                                  Wald chi2(1)    =     257.90\n                                                  Prob > chi2     =     0.0000\n                                                  R-squared       =     0.3223\n                                                  Root MSE        =     1.4966\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           D |   1.556085   .0968958    16.06   0.000     1.366172    1.745997\n       _cons |   .9600188   .0703862    13.64   0.000     .8220643    1.097973\n------------------------------------------------------------------------------\nEndogenous: D\nExogenous:  z1 z2\n\n. \n```\n:::\n:::\n\n\n# Popular IV Designs\n\n## Canonical Designs\n\n- The general message about using IV's is, and has always been, that they are hard to come by.\n\n- Applied research spends a quite good amount of time explaining why a particular instrument **IS** valid. (exogenous and relevant)\n\n- Relevance is generally easy to test, but exogeneity is difficult. Little can be done other than relying in other papers, and circumstances.\n\n- There are also those \"clever\" IVs, that tend to be case specific \n  - Scott Cunningham talks about Instruments being \"weird\", because you wouldnt expect them to be in the context of the research\n\n- There are, however, some designs that are used quite often, because they apply to different circumstances.\n\n## CD: Lotteries\n\n- In RCT, Lotteries are commonly used to decide who gets or doesnt get treatment among participants. Once treatment is assigned, however, not everyone will effectively taking up the treatment. \n  - Furthermore, some people may still end up being effectively treated because of other factors. \n- This is a case of imperfect compliance. \n\n- In cases like this, the lottery itself (which is randomized) can be used as instrument to identify the effect of being effectively treated. \n\nExamples:\n\n- Vietnam Draft Lottery\n- Oregon Medicaid Expansion Lottery\n\n## CD: Judge Fixed Effects\n\nThis design is also partially based on a kind of randomized assigment.\n\n1. Individuals are \"allocated\" to work, or be judge, under different officers \"judges\", at random.\n2. Judges are consistent among each other, with only difference being the severity of the judgment.\n3. Then Judge fixed effect can be used as an instrument on the judgment (treatment), and the final impact on the outcome of interest.\n\nThe idea here is that \"judgment-severity\" varies by judge. This difference in taste creates exogenous variation on some treatment, which is analyzed on some treatment.\n\nExample:\n\n- Teachers Grading? Driving test officers?  Performance tests?\n\n## CD: Shift-Share Bartik Instrument\n\nOriginally used in a study of regional labor market effects, this kind of instruments have also been used widely in other areas, such as imigration and trade.\n\nThe instrument was developed to analyze how changes in economic growth would affect market outcomes. (reverse Causality)\n\nTo do this, Bartik (1991) suggests, that it could be possible to create an instrument, making use of only exogenous variations, to first predict Potential local growth.\n\n1. Estimate **industry** shares by local region, based on some Ex ante information.\n2. Estimate national growth by industry (which should be exogenous to local growth)\n3. Estimate Potential Local growth using Shares x growth\n\nThis last one should represent the instrument to be used on actual local growth\n\nThis instrument depends strongly on the assumption that **Shares** are exogenous, and states are small compare to the national experience.\n\n# Til Next time: Matching and Reweithing\n\n",
    "supporting": [
      "09iv_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}