{
  "hash": "a0b1293b11a209f524f9161214a6b16f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Diff in Diff: From 2x2 to GxT DID'\nsubtitle: 'Advances, Problems and Solutions'\nauthor: Fernando Rios-Avila\ninstitute: Levy Economics Institute of Bard College\nformat:\n  revealjs:\n    slide-number: true\n    width: 1700\n    height: 900\n    code-fold: true\n    code-overflow: wrap\n    echo: true\n    css: styles.css\nbibliography: did_reg.bib\nnocite: |\n  @*\n---\n\n## Why differences in diffences?\n\n- Differences-in-Differences (DID) is one of the most popular tools in applied economics for analyzing causal effects of an intervention or policy treatment.\n- Under reasonable assumptions, DID allows you to identify these effects by comparing changes in the treatment group with changes in the control group.\n    - This method of identification even allows for controlling for self-selection in the treatment.\n- In recent years, several advances in DID have been developed, revealing issues with the traditional DID design, particularly with the application of TWFE.\n- Today, I will briefly present the problems and solutions that have been proposed, and how they are implemented in `Stata`.\n\n- So lets Start with the basics: 2x2 DID\n\n## 2x2 DID: Cannonic Design\n\n- In the 2x2 DID design, we have 2 groups:\n  - Control ($D=1$) y treated ($D=0$), \n- Which are observed for two periods of time:\n  - Before ($T=0$) and after ($T=1$) the treatment.\n- For all groups and periods of time, we observe the *realized* outcome $Y$, but cannot observe all *potential* outcomes.\n- This setup is valid both for panel data and repeat cross-sections, but will focus on panel data.\n   \n## Potential outcomes and Treatment Effects\n\n- **Potential outcomes** are the outcomes we would observe for each observation in the data if it were assigned to either the treatement or control group. \n  - $Y_{i,t}(D=0)$: Potential outcome for individual $i$ at time $t$ if he was never treated.\n  - $Y_{j,s}(D=1)$: Potential outcome for individual $j$ at time $s$ if he was treated.\n- However, we are only able to observe one of this outcomes:\n$$Y_{i,t} = Y_{i,t}(1)D_i + Y_{i,t}(0)(1-D_i)$$\n\n- Where $D_i$ is a dummy indicates the effective treatment status of individual $i$.\n\n- Now, we are interested in Treatment Effects for $i$, or any summary measures, after the treatment is applied:\n\n$$TE = Y_{i,1}(1)-Y_{i,1}(0) \\text{ or } ATT =E(Y_{i,1}(1)-Y_{i,1}(0)|D=1)$$\n\n\n## The DID Estimator: Assumptions\n\n- Because we can't observe both potential outcomes, we need to make assumptions to identify the treatment effect.\n  \n1. **Stable Unit Treatment Value Assumption (SUTVA)**: The treatment status of one unit does not affect the potential outcomes of other units. (No spillovers)\n\n2. **Parallel Trends**: In the absence of treatment, in average, both groups would have followed the same trend (changes) over time.\n\n$$\\color{blue}{E(Y_{i,1}(0) - Y_{i,0}(0)|D_i=1)} = \\color{green}{E(Y_{i,1}(0)-Y_{i,0}(0)|D_i=0)}$$\n   \n3. **No Anticipation**: Before the treatment takes place ($T=0$), the potential outcomes do not depend on the treatment status. \n\n$$Y_{i,0}(0) = Y_{i,0}(1)$$\n\n## The DID Estimator\n\n- Lets take a second look at the ATT definition:\n$$\\begin{aligned}\nATT &=E(Y_{i,1}(1)|D=1)-\\color{red}{E(Y_{i,1}(0)|D=1)} \\\\\n&=E(Y_{i,1}|D=1)-\\color{red}{E(Y_{i,1}(0)|D=1)}\n\\end{aligned}\n$$\n\n- The problem is to estimate the **red** component, to obtain an estimator for the ATT. \n\n- However, under the PTA and No Anticipation, the observed part can be written as:\n$$E(Y_{i,1}(0)|D=1) = E(Y_{i,1}-Y_{i,0}|D=0) + E(Y_{i,0}|D=1)\n$$\n\n- Which gives the usual ATT estimator:\n\n$$ATT =\\underbrace{E(Y_{i,1} - Y_{i,0}|D=1)}_{\\Delta treat} - \\underbrace{E(Y_{i,1}-Y_{i,0}|D=0)}_{\\Delta control}$$ \n\n## Graphically\n\n::: {#ea93f125 .cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<style>div.jp-Notebook .datagrid-container {min-height: 448px; }</style>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nchecking did_imputation consistency and verifying not already installed...\ninstalling into c:\\ado\\plus\\...\ninstallation complete.\n```\n:::\n:::\n\n\n::: {#d2de6764 .cell execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](12did_files/figure-revealjs/cell-3-output-2.png){fig-align='center'}\n:::\n:::\n\n\n## How to estimate it?\n\n- **Difference-in-Means**: \n  \n$$\\widehat{ATT}=\\bar Y^{D=1,T=1} - \\bar Y^{D=1,T=0} - [ \\bar Y^{D=0,T=1} - \\bar Y^{D=0,T=0} ]$$\n\n- **Regression**: \n\n$$Y_{i,t} = \\beta_0 + \\beta_1 D_i + \\beta_2 t + \\beta_3 (D_i \\times t) + \\epsilon_{i,t}$$\n\n- With Panel Data: **Fixed Effects**\n  \n$$\\begin{aligned}\nY_{i,t} &= \\beta_0 + \\sum \\delta_i  + \\beta_2 t + \\beta_3 (D_i \\times t) + \\epsilon_{i,t} \\\\\n\\Delta Y_{i} &= \\beta_2 + \\beta_3 D_i + \\epsilon_{i} \\text{ for } t=0 \\\\\n\\end{aligned}\n$$\n\n## Extension: Adding Controls\n\n- The DID estimator presented above relies on Unconditional Parallel Trends. It may work if both control and treated groups are similar in all aspects, except for the treatment. But they may not. \n- In this case, we can add controls to the model, to account for differences in the groups.\n\n$$Y_{i,t} = \\beta_0 + \\beta_1 D_i + \\beta_2 t + \\beta_3 (D_i \\times t) +  X_{it} \\gamma + \\epsilon_{i,t}$$\n\n- But this would be wrong. Why?\n\n## Problems of Adding Controls\n\n- Simply adding controls imposes assumption of homogenous treatement effects. As described in @santanna_doubly_2020, $\\gamma$ may not be the same as the ATT. \n  - Solution: Interactions between controls and all group\\\\time dummies:\n$$y_{i,t} = \\beta_0 + \\sum \\delta_i  + \\beta_2 t + \\beta_3 (D_i \\times t) +  X_{it} \\gamma + \\sum \\gamma_x (X-\\bar X_{D\\times T})*D*T + \\epsilon_{i,t}$$\n\n- Controlling for time varying variables may introduce biases (bad controls)\n  - Solution: use only pre-treatment variables as controls. (panel) or variables that should not be affected by the treatment (cross-sections)\n  - Also: If using Panel Data, one can add as controls all variables history. (see for example @caetano_difference_2022)\n\n## Potential Solution: @santanna_doubly_2020\n\n- DID is a straighforward estimator, that relies strongly on the Unconditional Parallel Trends Assumption. (UPTA)\n\n- One solution to the problem is to use the **Conditional Parallel Trends Assumption** (CPTA), which can be less restrictive.\n\n- This states that PTA holds, only after conditioning on a set of variables $X$. (looking at subgroups).\n\n$$\\color{blue}{E(Y_{i,1}(0) - Y_{i,0}(0)|D_i=1,X)} = \\color{green}{E(Y_{i,1}(0)-Y_{i,0}(0)|D_i=0,X)}$$\n\n- So, if you can \"control\" for individual characteristics, you could estimate the treatment effect, and still report Average Treatment Effects for the population.\n\n$$ATT = E\\big[ [E(Y_{i,1}(0) - Y_{i,0}(0)|D_i=1,X ] \\big]$$\n\n## Added Assumption\n\n- When accouting for covariates, one needs to add an additional assumption to the data:\n\n- There is an overlap in the distribution of $X$ between the treated and control groups. (common support)\n\n$$0 << Pr(D=1|X) << 1$$\n\n- This is important, because if there is no overlap, the treatment effect cannot be identified for all groups, and may create distorted estimates.\n\n## Potential Solution: `drdid`, `teffects` \n\n- @santanna_doubly_2020 propose various alternatives to estimate the treatment effect with controls, under CPTA. They contrast the implementation of methods that use linear regressions, Inverse Probability Weighting (IPW) and Doubly Robust Estimators (DR).\n- Similar estimates could be obtained using `teffects` in Stata.\n- But how does this work? Two Cases: \n  - Panel Data: This simplifies the problem, because we can use the Data structure to estimate the model.\n  - Crossection: This is more complicated because requires careful consideration of idenfication assumptions, and groups of interest.\n\n## Panel Data \n\n- Step 1: Estimate the changes in the outcome $\\Delta Y_i = Y_{i,1} - Y_{i,0}$. (the First D)\n  - Because of this, we are implicitly reducing our sample size: from $N$ to $N/2$ (because of the unestimated fixed effects)\n- Step 2: Estimate the treatment effect:\n  - Simplest case, no controls:\n$$\\Delta Y_i = \\beta_0 + \\beta_1 D_i + \\varepsilon_i$$ \n  \n- In other words, the model is now a simple linear regression, with 1 period.\n\n## Panel Data: Adding controls\n\n- Because you have now only 1 period, and two groups. All treatment effect estimators can be used.\n  - regression outcome, IPW, doubly robust, matching, etc.\n  - We use them to estimate the second D of DID\n- Empirically: With Transformed data ($\\Delta y$) as dependent variable, any of the methods in `teffects` can be used to estimate the treatment effect.\n- With untransformed data (RC), you can use `drdid` to estimate the treatment effect, and their Standard errors.\n    - You need to make sure you have only 2 periods of data.\n\n## Panel Data: Methods\n\n- **Regression**: reg (`drdid`) or ra (`teffects`) \n    - using $\\Delta y$ as dependent variable, estimate a linear model with covariates (No FE) for the untreated, and predict outcomes. This would become your \"Potential outcome change under no-treatment\"\n  \n$$ ATT = E(\\Delta Y_{i}|D=1) - E({\\hat\\gamma X|D=1})$$\n\n- **Inverse Probability Weighting**: stdipw (`drdid`) or ipw (`teffects`)\n    - Estimate the likelihood of being treated, and use it to identify a propensity score and IPW weights $p(D=1|X) = \\hat p(X)$\n  \n$$ ATT = E(\\Delta Y_{i}|D=1) - E\\left({\\Delta Y_{i} \\frac{p(X)}{1-p(X)}|D=0}\\right)$$\n\n## Panel Data: Methods\n\n- **Doubly Robust Estimation**: dript and dripw (`drdid`) or ipwra aipw (`teffects`)\n    - Estimate the likelihood of being treated, and use it to identify a propensity score and IPW weights $p(D=1|X) = \\hat p(X)$\n    - Estimate a weighted regression with $\\Delta y$ as dependent variable or estimate a weighted regression correction. (see [here](https://friosavila.github.io/app_metrics/app_metrics2.html) slide 17-18) for more details.\n    - Combinations of modeling outcome and modeling likelihood and pscores makes the estimator doubly robust.\n\n- For Repeated crossection, the math becomes more complicated to follow and identify the \"group of interest\"  \n- See [here](https://friosavila.github.io/app_metrics/app_metrics2.html) slide 53-54 for more details.\n  \n## Example {.scrollable}\n\nSome Data Preparation\n\n::: {#eb5e2f7c .cell .larger execution_count=3}\n``` {.stata .cell-code code-fold=\"false\"}\nqui:frause lalonde, clear\nkeep  if treated==0 | sample==2\nbysort id (year):gen dy = re[2]-re[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(5,574 observations deleted)\n```\n:::\n:::\n\n\nUsing `teffects`:\n\n::: {#d90994d8 .cell .larger execution_count=4}\n``` {.stata .cell-code code-fold=\"false\"}\nteffects ra (dy educ black married nodegree hisp re74) (experimental) if year==1975 , atet\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nIteration 0:  EE criterion =  4.433e-21  \nIteration 1:  EE criterion =  8.994e-26  \n\nTreatment-effects estimation                    Number of obs     =     16,417\nEstimator      : regression adjustment\nOutcome model  : linear\nTreatment model: none\n------------------------------------------------------------------------------\n             |               Robust\n          dy | Coefficient  std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATET         |\nexperimental |\n   (1 vs 0)  |  -1055.483   346.1459    -3.05   0.002    -1733.917   -377.0497\n-------------+----------------------------------------------------------------\nPOmean       |\nexperimental |\n          0  |   3118.849   185.0719    16.85   0.000     2756.114    3481.583\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nUsing `drdid`:\n\n::: {#ab7ba4c0 .cell .larger execution_count=5}\n``` {.stata .cell-code code-fold=\"false\"}\ndrdid re educ black married nodegree hisp re74 , ivar(id) time(year) tr(experimental) reg\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nDoubly robust difference-in-differences                 Number of obs = 32,834\nOutcome model  : regression adjustment\nTreatment model: none\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nATET         |\nexperimental |\n   (1 vs 0)  |  -1055.483   346.1451    -3.05   0.002    -1733.915   -377.0511\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nby hand:\n\n::: {#6f5152e5 .cell .larger execution_count=6}\n``` {.stata .cell-code code-fold=\"false\"}\nqui:reg dy educ black married nodegree hisp re74 if exper==0\npredict dy_hat, xb\ngen att_i = dy-dy_hat\ntabstat att_i, by(exper) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSummary for variables: att_i\nGroup variable: experimental \n\nexperimental |      Mean\n-------------+----------\n           0 | -9.61e-06\n           1 | -1055.483\n-------------+----------\n       Total | -27.32414\n------------------------\n```\n:::\n:::\n\n\n## Highlights I: Identification\n\n- The identification relies on the non-anticipation assumption and the parallel trends assumption.\n  - Neither of these assumptions can be tested, but they can be supported by the data.\n- For Implementation, If one has panel data, `teffects` and `drdid` can be used to estimate the treatment effect, and their standard errors. (after preparing the data)\n- Otherwise, `drdid` can be used for repeated crossection as well. (or GMM)\n\n## Highlighs II: Overlapping  \n\n- The **overlapping** assumption is important, and has different implications for the different methods.\n- When the estimation method is `reg` or `ra`, the overlapping assumption is less biding, because the model is extrapolates to created \"potential outcomes\" for the treated group.\n  - However, extrapolation may not be accurate, or create incorrect extrapolations (base category dummies)\n- With doubly robust methods, the overlapping assumption is more relevant. `dript` is particularly sensitive to this assumption, and may not even converge if there is weak overlap.\n- For most practical purposes, `dripw` may be the most stable, and preferred method.\n\n## Highlighs III: Controls\n\n- With panel data, `drdid` automatically constrains model specification to use pre-treatment information only.\n  - Added work is required to include post-treatment information.\n- With Repeated crossection, `drdid` would use both pre-treatment and post-treatment information.\n  \n# From 2x2 to GxT DID\n\n## How GxT DID was suppoused to work\n\n\n- While the 2x2 DID design is simple to understand, it does not reflect the type of information that is available in most cases. (G>2 and T>2)\n \n- Until few years ago, when this kind of data was available, the standard approach was to use a generalized DID design, which extended the use of Fixed Effects:\n\n$$\n\\begin{aligned}\n2\\times2: Y_{i,t} &= \\beta_0 + \\beta_1 T  + \\beta_2 D_i + \\beta_3 (D_i \\times T) + \\epsilon_{i,t} \\\\\nG\\times T: Y_{i,t} &= \\sum \\gamma_t + \\sum \\delta_i  + \\theta^{fe} PT_{i,t} + \\epsilon_{i,t} \\\\\n\\end{aligned}\n$$\n\nwhere $PT_{i,t}$ assume the value of 1 for the treated group After the treatment is applied, and 0 otherwise, and $\\theta^{fe}$ representing the treatment effect.\n\n- Little that we know that this approach only works if the treatment effect is homogeneous across all groups and periods.\n  \n## Why it doesn't work? \n\nThere are at least two-ways that have been used to explain why the Standard TWFE does not work:\n\n- **The \"Bad Controls\" Problem**: @goodmanbacon2021 shows that the TWFE estimator for ATT is a kind of weighted average of all possible DID one could estimate. Some of these would not be **good** DID designs.\n- **Negative Weights**: @dechaisemartin2020 and @borusyak2023revisiting show that because TWFE ATT estimator is a weighted average of all group-Specific ATT, some may include negative weights.\n  \n  This may produce negative ATT even if all group-specific ATT are positive. \n\n- However, when the treatment effect is homogeneous, Neither of these situations would be a problem.\n\n## The \"Bad Controls\" Problem\n\n\n\n:::{.panel-tabset}\n\n## All Cases\n\n![](fig1.png){height=50%}\n\n## Early Treated vs Later Treated (Good)\n\n![](fig2.png)\n\n## Early Treated vs Later Treated (bad)\n\n![](fig3.png)\n\n:::\n\n## The \"Bad Controls\" Problem\n\n\n\n:::{.panel-tabset}\n\n## All Cases\n\n![](fig1b.png)\n\n## Early Treated vs Later Treated (Good)\n\n![](fig2b.png)\n\n## Early Treated vs Later Treated (bad)\n\n![](fig3b.png)\n\n:::\n\n## Negative Weights\n\n- To understand the idea of Negative Weights, lets consider the following example:\n\n$$Y_{i,t} = \\sum \\delta_i + \\sum \\gamma_t  + \\theta PT + \\epsilon_{i,t}$$\n\nIf the panel data is balanced, and we apply FWL, we can use partialling out the Fixed Effects:\n\n$$\\widetilde{PT}_{it}=PT_{it}-\\overline{PT}_{i}-\\overline{PT}_{t}+\\overline{PT}$$\n\nand estimate $\\theta^{fe}$ as:\n\n$$\\hat\\theta^{fe} = \\frac{\\sum \\widetilde{PT_{it}} Y_{it}}{\\sum \\widetilde{PT}_{it}^2}\n= w_{it} Y_{it}\n$$\n\n##\n\n$$\\widetilde{PT}_{it}=PT_{it}-\\overline{PT}_{i}-\\overline{PT}_{t}+\\overline{PT}$$\n\n- $\\overline{PT}_{i}$ is the share of periods a unit is observed to be treated. (larger for early treated)\n- $\\overline{PT}_{t}$ is the share of units treated at time $t$. (increasing)\n- $\\overline{PT}$ Share of treated \"unit-periods\"\n\nThus,\n\n- Units that are treated early, (high $\\overline{PT}_{i}$)\n- but are analyzed at later periods ($\\overline{PT}_{t}$ increasing in $t$)\n\nWill be more likely to have a negative weight $w_{it}$, thus contaminating the ATT estimate.\n\n## Graphically\n\n\n\n::: {#e5b753e7 .cell execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![](12did_files/figure-revealjs/cell-11-output-1.png){fig-align='center'}\n:::\n:::\n\n\n## Summarizing the Problem\n\n- Both Negative Weights and Bad Controls are capturing the same problem.\n  \n1. By using already treated units as controls (bad controls), we are implicitly applying negative weights to units that are already treated.\n\n2. This would not be a problem on its own, if the treatment effect is homogeneous across all groups and periods.\n\n3. However, if the treatment effect is heterogeneous, Parallel Trends between already treated and late treated units may not hold. \n\n## GxT DID: Generalized DID\n### Setup\n- In the Generalized DID\n  - One has access to multiple periods of data: $t = 1,2,...,T$.\n  - And units can be treated at any point before, during or after the available Data $G = -k,..1,2,..,T+l$.\n    - This is the cohort or group.\n  - We also assume that once a unit is treated, it remains treated. (no reversals/no cohort-change)\n\n- From our perspective, units treated **at or before** $t=1$ will be considered as **Allways treated**.\n  - This units cannot be used for analysis.\n- For any practical purpose, if we do not observe a unit being treated in the window of time observed, we assume its **Never Treated**. \n  - For Notation we say these units belong to $g=\\infty$ (or that they could be treated at some point in the far future)\n\n## GxT DID: Generalized DID\n### Potential Outcomes\n\n- In contrast with previous setup, with the GDID, we believe observations have multiple potential outcomes, for each period of time.\n\t- $Y_{i,t}(G)$ is the potenital outcome for individual $i$ at time $t$, if this unit would be treated at time $G$. \n\t- Thus we state that depending on \"when\" a unit is treated, the potential outcomes could be different.\n  \t- This is what it means allowing for heterogeneity.\n  \n## GxT DID: Generalized DID\n### Parallel Trends Assumption\n\n- PTA assumption is also slightly modified. Because we can differentiate between **never-treated** and **not-yet-treated**, one could impose those PTA assumptions.\n  \n  Never Treated\n\t$$E(Y_{i,t}(\\infty) - Y_{i,t-s}(\\infty)|G=g) = E(Y_{i,t}(\\infty) - Y_{i,t-s}(\\infty)|G=\\infty) \\forall s>0 $$\n\n  Not Yet Treated\n\n  \t$$E(Y_{i,t}(\\infty) - Y_{i,t-s}(\\infty)|G=g) = E(Y_{i,t}(\\infty) - Y_{i,t-s}(\\infty)|G=g') \\forall s>0 $$\n\n- Which suggests PTA hold for all pre- and post-treatment periods. \n- Some methods only rely on Post-treatment PTA.\n  \n## GxT DID: Generalized DID\n### No Anticipation\n\n- As before, we also require no anticipation.\n  - That the before treatment takes place (or is announced), the potential outcomes do not depend on the treatment status.\n\n$$Y_{i,t}(G) = Y_{i,t}(\\infty) \\text{ if } t<G $$ \n\n- Also important.\n  - $E(Y_{i,t}(G)|g=G) = E(Y_{i,t}|g=G)$ if $t>G$ \n  - $E(Y_{i,t}(\\infty)|g=\\infty) = E(Y_{i,t}|g=\\infty)$\n\n# Solutions\n\n## General Overview\n\nThe root of the problem with TWFE: \n\n> If treatment effects are heterogenous, the TWFE estimator of treatment effects produces incorrect estimates\n> because of \"negative weights\" or \"bad controls\"\n\nTo solve the problem we need to do (at least) one of three things:\n\n  1) Avoid using bad controls (@borusyak2023revisiting and @gardner2022twostage)\n  2) Allow for heterogeneity in the treatment effect. (@wooldridge_2021 and @sun_estimating_2021)\n  3) Use *only* good DID designs. (@callaway_2021)\n\n## Avoid using bad controls: `did_imputation` and `did2s`\n#### @borusyak2023revisiting and @gardner2022twostage\n\n- Both methods are based on the idea of \"imputing\" the missing potential outcomes $Y_{it}(0)$ for the treated group, using a method similar to the one used for 2x2 DID. A two-stage approach.\n  \n1. We know that estimating the following model would be incorrect:\n\n$$y_{it} = \\delta_i + \\gamma_t + \\theta D_{it} + \\epsilon_{it}$$\n\n2. However, what this authors suggest is to identify $\\delta_i$ and $\\gamma_t$ using pre-treatment data only:\n\n$$y_{it} = \\delta_i + \\gamma_t + \\epsilon_{it} \\text{ if } t<g $$\n\nThis helps identifying the fixed effects, without any contamination. (its a model for the potential outcome of no treatment)\n\n## Avoid using bad controls: `did_imputation` and `did2s`\n#### @borusyak2023revisiting and @gardner2022twostage\n\n3. Use the previous model to re-estimate 1:\n\n$$y_{it} = \\hat \\delta_i + \\hat \\gamma_t + \\theta D_{it} + \\epsilon_{it}$$\n\n4. In fact, one could use many other specifications to identify Treatment effects, including by group, by calendar, or dynamic effects.\n\n$$y_{it} = \\hat \\delta_i + \\hat \\gamma_t + \\theta D_{it}*(other Heterogeneity) + \\epsilon_{it}$$\n\n## Avoid using bad controls: `did_imputation` and `did2s`\n#### @borusyak2023revisiting and @gardner2022twostage\n\n- The identification of TE using the imputation method relies on the same assumptions as the traditional DID design.\n- However, It also requires the Parallel Trends Assumption (PTA) to hold for all pre-treatment periods.\n  \n  - This why how we can use pre-treatment information to predict the potential outcomes for the treated group.\n    - At the extreme, one could even identify TE without access to non-treated units!\n  \n- This also means that the method is sensitive to problems with **long** PTA. Although it can be somewhat relaxed (based on model specification).\n\n- However, its more efficient than other models because it uses all-pretreatment data for estimation.\n  - Data requirements are similar to the traditional DID design.\n\n## Example {.scrollable}\n\n- There are two implementations of the method in Stata: `did_imputation` and `did2s`. In addition to the original GMM estimator described in @gardner2022twostage.\n- However, the most flexible and Robust implementation is `did_imputation`.\n\n::: {#c295e43b .cell .larger execution_count=11}\n``` {.stata .cell-code code-fold=\"false\"}\nuse http://pped.org/bacon_example.dta, clear\n**Estimate model for Pre-treatment data\nqui:reghdfe asmrs if post==0, abs(fe1 = stfips fe2 = year)\n** Extra polate\nbysort stfips (fe1):replace fe1=fe1[1]\nbysort year (fe2):replace fe2=fe2[1]\n\n** get TE for i\ngen te = asmrs-fe1-fe2-_b[_cons]\nsum te if post==0\nsum te if post==1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(843 real changes made)\n(1107 real changes made)\n(264 missing values generated)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          te |        510    2.92e-09    9.204828  -36.51952   51.26879\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n          te |        843   -5.530065    15.66787  -69.98216   48.63423\n```\n:::\n:::\n\n\nusing `did_imputation`:\n\n::: {#5e476dbf .cell .larger execution_count=12}\n``` {.stata .cell-code code-fold=\"false\"}\n** Gvar\negen gvar = csgvar(post), ivar(stfips) tvar(year)\n** event\ngen event = year - gvar if gvar!=0\n** Gvar = . (missing implies never treated)\nclonevar gvar2=gvar if gvar!=0\ndid_imputation asmrs stfips year gvar2, autosample\ndid_imputation asmrs stfips year gvar2, autosample horizon(1/10) pretrends(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(165 missing values generated)\n(165 missing values generated)\nWarning: part of the sample was dropped for the following coefficients because FE could not be imputed: tau.\n\n                                                         Number of obs = 1,353\n------------------------------------------------------------------------------\n       asmrs | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         tau |  -5.530065   2.978178    -1.86   0.063    -11.36719    .3070564\n------------------------------------------------------------------------------\nWarning: part of the sample was dropped for the following coefficients because FE could not be imputed: tau1 tau2 tau3 tau4 tau5 tau6 tau7 tau8 tau9 tau10.\n\n                                                           Number of obs = 870\n------------------------------------------------------------------------------\n       asmrs | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        tau1 |   .1501093   1.857768     0.08   0.936     -3.49105    3.791268\n        tau2 |   1.089884    2.60167     0.42   0.675    -4.009296    6.189064\n        tau3 |    1.02914   2.788825     0.37   0.712    -4.436857    6.495136\n        tau4 |   .3781413   2.549497     0.15   0.882     -4.61878    5.375063\n        tau5 |  -1.302828   2.063713    -0.63   0.528    -5.347632    2.741976\n        tau6 |  -1.137572    3.37171    -0.34   0.736    -7.746001    5.470858\n        tau7 |  -5.499892   2.053929    -2.68   0.007    -9.525519   -1.474265\n        tau8 |  -4.864576   3.374892    -1.44   0.149    -11.47924     1.75009\n        tau9 |  -4.645944   2.361134    -1.97   0.049    -9.273682   -.0182059\n       tau10 |  -6.772627   3.287879    -2.06   0.039    -13.21675   -.3285015\n        pre1 |   1.556381   3.517751     0.44   0.658    -5.338285    8.451046\n        pre2 |   1.200269   2.631038     0.46   0.648     -3.95647    6.357009\n        pre3 |  -.4639667   2.211242    -0.21   0.834    -4.797921    3.869987\n        pre4 |   1.921326   1.945554     0.99   0.323    -1.891888    5.734541\n        pre5 |  -1.315265   1.903228    -0.69   0.490    -5.045522    2.414992\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n## Allow for Heterogeneity: `jwdid` and `eventstudyinteract`\n#### @wooldridge_2021 and @sun_estimating_2021\n\n- The work by @wooldridge_2021 suggested that the GDID-TWFE estimator was not a problem perse.\n- The problem was that the GDID-TWFE estimator was simply misspecified.\n\n- Instead of modeling:\n$$Y_{i,t} = \\delta_i + \\gamma_t  + \\theta^{fe} PT_{i,t} + \\epsilon_{i,t}$$\n\n- One should allow for a full set of interactions between the group and time dummies:\n\n$$Y_{i,t} = \\delta_i + \\gamma_t  + \\sum_{g=2}^T \\sum_{t=g}^T \\theta_{g,t} \\mathbb{1}(G=g,T=t) + \\epsilon_{i,t}$$\n\n- In this framework, each $\\theta_{g,t}$ represents the ATT for each group at a particular period.\n\n## Allow for Heterogeneity: `jwdid` and `eventstudyinteract`\n#### @wooldridge_2021 and @sun_estimating_2021\n\n- In the basic setup, this approach is basically the same as the method proposed by @borusyak2023revisiting and @gardner2022twostage.\n- Wooldridge, however, was not the first approach that aim to \"allow for heterogeneity\" in the treatment effect. Early attempts were done by using a dynamic events structure, using both leads ands lags of the treatment variable.\n$$Y_{i,t} = \\delta_i + \\gamma_t  + \\sum_{e=-k}^{-2} \\theta_e \\mathbb{1}(t-G_i=e) + \\sum_{e=0}^L \\theta_e \\mathbb{1}(t-G_i=e) + \\epsilon_{i,t}$$\n\n- This not only allows for heterogenous effects across time, but also allows you to analyze pre-treatments effects.\n\n- However @sun_estimating_2021 showed that this approach could also be wrong, if dynamic effects are also heterogenous across groups.\n\n## Allow for Heterogeneity: `jwdid` and `eventstudyinteract`\n#### @wooldridge_2021 and @sun_estimating_2021\n\n- As solution, @sun_estimating_2021 propose to use a full set of interactions between the group dummies and the event-study dummies. This is similar to @wooldridge_2021.\n$$Y_{i,t} = \\delta_i + \\gamma_t  + \\sum_{g=2}^T \\sum_{e=-k}^{-2} \\theta_{g,e} \\mathbb{1}(t-G_i=e, G=g) + \\sum_{g=2}^T \\sum_{e=0}^L \\theta_{g,e} \\mathbb{1}(t-G_i=e, G=g) + \\epsilon_{i,t}$$\n\n- In fact, if we write the \"event\" as \"time\", it would look very similar to the model proposed by @wooldridge_2021.\n\n$$Y_{i,t} = \\delta_i + \\gamma_t  + \\sum_{g=2}^T \\sum_{t=1}^{g-2} \\theta_{g,t} \\mathbb{1}(T=t, G=g) \n+ \\sum_{g=2}^T \\sum_{t=g}^{T} \\theta_{g,t} \\mathbb{1}(T=t, G=g) + \\epsilon_{i,t}\n$$\n\n- Thus, both approaches are identical if we allow for Full interactions before and after treatment.\n\n## Allow for Heterogeneity: `jwdid` and `eventstudyinteract`\n### Assumptions and Limitations\n\n- The model proposed by @wooldridge_2021 follows the same assumptions as @borusyak2023revisiting. (Long PTA)\n- @sun_estimating_2021 and the modified @wooldridge_2021, however, only requires PTA to hold \"after\" treatment takes place.\n- Both methods require careful consideration of covariates (time constant), and they require additional work for adding it into a model (variable shifting).\n  - The limitations of sample size are somewhat more evident in this framework.\n- Wooldridge's approach, however, could also be used beyond the linear case as shown in @wooldridge_2023.\n  \n## Implementation {.scrollable}\n\nThere are various commands that implement @sun_estimating_2021 estimator, including her original command `eventsudyinteract`, as well as `xtevent`.\n\nFor @wooldridge_2021, there is now the official Stata18 command `xthdidregress twfe`, the one I developed: `jwdid` and a newer one `wooldid`.\n\nWill focus on `jwdid`.\n\nBase Estimation:\n\n::: {#2cbc34b3 .cell .larger execution_count=13}\n``` {.stata .cell-code code-fold=\"false\"}\nqui:ssc install frause, replace\nfrause mpdta, clear\n** \njwdid lemp, ivar(countyreal) tvar(year) gvar(first_treat) \nestat simple\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Written by R.              )\nWARNING: Singleton observations not dropped; statistical significance is biased (link)\n(MWFE estimator converged in 2 iterations)\n\nHDFE Linear regression                            Number of obs   =      2,500\nAbsorbing 2 HDFE groups                           F(   7,    499) =       3.82\nStatistics robust to heteroskedasticity           Prob > F        =     0.0005\n                                                  R-squared       =     0.9933\n                                                  Adj R-squared   =     0.9915\n                                                  Within R-sq.    =     0.0101\nNumber of clusters (countyreal) =        500      Root MSE        =     0.1389\n\n                                        (Std. err. adjusted for 500 clusters in countyreal)\n-------------------------------------------------------------------------------------------\n                          |               Robust\n                     lemp | Coefficient  std. err.      t    P>|t|     [95% conf. interval]\n--------------------------+----------------------------------------------------------------\nfirst_treat#year#c.__tr__ |\n               2004 2004  |  -.0193724   .0223818    -0.87   0.387    -.0633465    .0246018\n               2004 2005  |  -.0783191   .0304878    -2.57   0.010    -.1382195   -.0184187\n               2004 2006  |  -.1360781   .0354555    -3.84   0.000    -.2057386   -.0664177\n               2004 2007  |  -.1047075   .0338743    -3.09   0.002    -.1712613   -.0381536\n               2006 2006  |   .0025139   .0199328     0.13   0.900    -.0366487    .0416765\n               2006 2007  |  -.0391927   .0240087    -1.63   0.103    -.0863634     .007978\n               2007 2007  |   -.043106   .0184311    -2.34   0.020    -.0793182   -.0068938\n                          |\n                    _cons |    5.77807    .001544  3742.17   0.000     5.775036    5.781103\n-------------------------------------------------------------------------------------------\n\nAbsorbed degrees of freedom:\n-----------------------------------------------------+\n Absorbed FE | Categories  - Redundant  = Num. Coefs |\n-------------+---------------------------------------|\n  countyreal |       500         500           0    *|\n        year |         5           1           4     |\n-----------------------------------------------------+\n* = FE nested within cluster; treated as redundant for DoF computation\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      simple |  -.0477099    .013265    -3.60   0.000    -.0737088   -.0217111\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nAdding controls\n\n::: {#20bf08af .cell .larger execution_count=14}\n``` {.stata .cell-code code-fold=\"false\"}\njwdid lemp lpop, ivar(countyreal) tvar(year) gvar(first_treat) \nestat simple\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWARNING: Singleton observations not dropped; statistical significance is biased (link)\n(MWFE estimator converged in 2 iterations)\n\nHDFE Linear regression                            Number of obs   =      2,500\nAbsorbing 2 HDFE groups                           F(  18,    499) =       3.62\nStatistics robust to heteroskedasticity           Prob > F        =     0.0000\n                                                  R-squared       =     0.9933\n                                                  Adj R-squared   =     0.9916\n                                                  Within R-sq.    =     0.0212\nNumber of clusters (countyreal) =        500      Root MSE        =     0.1385\n\n                                                  (Std. err. adjusted for 500 clusters in countyreal)\n-----------------------------------------------------------------------------------------------------\n                                    |               Robust\n                               lemp | Coefficient  std. err.      t    P>|t|     [95% conf. interval]\n------------------------------------+----------------------------------------------------------------\n          first_treat#year#c.__tr__ |\n                         2004 2004  |   -.021248   .0216933    -0.98   0.328    -.0638695    .0213735\n                         2004 2005  |    -.08185   .0273307    -2.99   0.003    -.1355474   -.0281526\n                         2004 2006  |  -.1378704   .0307448    -4.48   0.000    -.1982756   -.0774651\n                         2004 2007  |  -.1095395   .0322696    -3.39   0.001    -.1729405   -.0461384\n                         2006 2006  |   .0025368   .0188523     0.13   0.893    -.0345029    .0395765\n                         2006 2007  |  -.0450935   .0219516    -2.05   0.040    -.0882223   -.0019646\n                         2007 2007  |  -.0459545    .017946    -2.56   0.011    -.0812136   -.0106954\n                                    |\nfirst_treat#year#c.__tr__#c._x_lpop |\n                         2004 2004  |   .0046278   .0175555     0.26   0.792     -.029864    .0391196\n                         2004 2005  |   .0251131   .0178749     1.40   0.161    -.0100063    .0602325\n                         2004 2006  |   .0507346   .0210361     2.41   0.016     .0094043    .0920648\n                         2004 2007  |   .0112497   .0265741     0.42   0.672    -.0409613    .0634607\n                         2006 2006  |   .0389352   .0164453     2.37   0.018     .0066246    .0712457\n                         2006 2007  |   .0380597   .0224407     1.70   0.091    -.0060301    .0821495\n                         2007 2007  |  -.0198351    .016172    -1.23   0.221    -.0516088    .0119385\n                                    |\n                        year#c.lpop |\n                              2004  |   .0110137   .0075431     1.46   0.145    -.0038064    .0258338\n                              2005  |   .0207333    .008093     2.56   0.011     .0048328    .0366339\n                              2006  |   .0105354   .0108004     0.98   0.330    -.0106845    .0317552\n                              2007  |    .020921   .0117917     1.77   0.077    -.0022465    .0440884\n                                    |\n                              _cons |   5.736532   .0215065   266.73   0.000     5.694277    5.778786\n-----------------------------------------------------------------------------------------------------\n\nAbsorbed degrees of freedom:\n-----------------------------------------------------+\n Absorbed FE | Categories  - Redundant  = Num. Coefs |\n-------------+---------------------------------------|\n  countyreal |       500         500           0    *|\n        year |         5           1           4     |\n-----------------------------------------------------+\n* = FE nested within cluster; treated as redundant for DoF computation\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      simple |   -.050627   .0124796    -4.06   0.000    -.0750866   -.0261675\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nCompared to `did_imputation`\n\n::: {#b9482ea5 .cell .larger execution_count=15}\n``` {.stata .cell-code code-fold=\"false\"}\ngen first2=first_treat if first_treat>0\ndid_imputation lemp countyreal year first2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(1,545 missing values generated)\n\n                                                         Number of obs = 2,500\n------------------------------------------------------------------------------\n        lemp | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         tau |  -.0477101   .0132225    -3.61   0.000    -.0736257   -.0217944\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n::: {#4711aa65 .cell .larger execution_count=16}\n``` {.stata .cell-code code-fold=\"false\"}\nqui:jwdid lemp, ivar(countyreal) tvar(year) gvar(first_treat) \nestat simple\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWARNING: Singleton observations not dropped; statistical significance is biased (link)\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      simple |  -.0477099    .013265    -3.60   0.000    -.0737088   -.0217111\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nEstimating an effect similar to @sun_estimating_2021\n\n::: {#3de3642b .cell .larger execution_count=17}\n``` {.stata .cell-code code-fold=\"false\"}\nqui:jwdid lemp, ivar(countyreal) tvar(year) gvar(first_treat) ///\n\tnever //<- request full interaction\nestat event\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWARNING: Singleton observations not dropped; statistical significance is biased (link)\n------------------------------------------------------------------------------\n             |            Delta-method\n             | Coefficient  std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   __event__ |\n         -4  |   .0033064   .0245551     0.13   0.893    -.0448207    .0514335\n         -3  |   .0250218   .0181543     1.38   0.168      -.01056    .0606037\n         -2  |   .0244587   .0142668     1.71   0.086    -.0035037    .0524211\n         -1  |          0  (omitted)\n          0  |  -.0199318   .0118575    -1.68   0.093    -.0431722    .0033085\n          1  |  -.0509574   .0168707    -3.02   0.003    -.0840233   -.0178914\n          2  |  -.1372587   .0365895    -3.75   0.000    -.2089728   -.0655447\n          3  |  -.1008114   .0345043    -2.92   0.003    -.1684385   -.0331842\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nIt is also possible to add further restrictions to \"event/calendar/group\" aggregates\n\n::: {#e2acf873 .cell .larger execution_count=18}\n``` {.stata .cell-code code-fold=\"false\"}\n** requires latest JWDID\nqui:net install jwdid, from(https://friosavila.github.io/stpackages) replace\nprogram drop _all\ngen subsample = inrange(__event__,2,6)\nestat event, other(subsample)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\noption other() not allowed\n```\n:::\n:::\n\n\n## Using Good DID Designs: `csdid` and `csdid2`\n#### @callaway_2021\n\n- A third, and last, approach has been proposed by @callaway_2021.\n- In contrast with previous methods (which focus on global estimators), this approach suggests deconstructing the estimation into smaller, but well define pieces: The ATTGTs for 2x2 DIDs\n\n  - Makes it easy to use using **GOOD** DID designs only \n  - and, once all ATTGT's are estimated, they can be aggregated in various ways\n\n- This approach takes full advantage of the fact we know quite well (`drdid` + others) how to estimate 2x2 DID. \n- and (or but) forces you to utilize time constant or pretreatment controls only.\n\n- However, one now needs to estimate as many as periods and cohorts are available in the data.\n\n## Using Good DID Designs: `csdid` and `csdid2`\n#### @callaway_2021\n\n- The proposed estimator starts with the assumption that one is interested in the ATTGT:\n\n$$ATT(g,t) = E(y_{i,t}(G) - y_{i,t}(\\infty) |G_i =g)\n$$\n\n- As in the simpler 2x2 case, however, we cannot observe the $y_{i,t}(\\infty)$ after they are treated.\n\n- What @callaway_2021 does is to impute this piece applyint PTA and no Anticipation.\n- \n$${E(y_{i,t}(\\infty)|G_i =g)} \\approx E(y_{i,G-k}|G_i =g) + E(y_{i,t}-y_{i,G-k}| G_i \\in Control)\n$$\n\n- Thus, the estimator for the ATT(g,t) becomes:\n$$\\widehat{ATT(g,t)} = E(y_{i,t}- y_{i,G-k}|G_i =g)- E(y_{i,t}- y_{i,G-k}|G_i \\in Control)\n$$\n\n## Using Good DID Designs: `csdid` and `csdid2`\n#### @callaway_2021\n\n$$\\widehat{ATT(g,t)} = E(y_{i,t}- y_{i,G-k}|G_i =g)- E(y_{i,t}- y_{i,G-k}|G_i \\in Control)$$\n\n1. With no anticipation and PTA, *ANY* period before treatment ($G-k$) could be used to construct the ATT(g,t). \n2. Because of this, it only relies on Post-treatment PTA. (Violations of PTA before treatment have no impact on the estimates)\n3. Depending on the analysis of interest, one could choose different \"control groups\"\n   - Never treated: Most common\n   - Not-yet-treated: Include observations that up to time $t$ have not been treated.\n   - For pre-treatment ATT(g,t)'s, the Not-yet treated cound include all cohorts not treated until $t$ `R` or those not treat until $t$ nor $g$ `Stata`.\n\n## Using Good DID Designs: `csdid` and `csdid2`\n#### @callaway_2021\n\n- This approach is relatively easy to implement. The main difficulty is keeping track of ALL the ATT(g,t)s that are estimated (and their VCV)\n- However, once all ATT(g,t)'s are obtained, aggregation is straight forward:\n\n$$\nAGG(ATT(g,t)) = \\frac{\\sum_{g,t \\in G\\times T}   ATT(g,t) * w_{g,t} * sel_{g,t} }\n{\\sum_{g,t \\in G\\times T} w_{g,t} * sel_{g,t} }\n$$\n\n- where $w_{g,t}$ represents the size of cohort $g$ at time $t$ used in the estimation of ATT(g,t). \n\n- and  $sel_{g,t}$ is an indicator for whether that ATTGT will be used in the aggregation.\n\n## Using Good DID Designs: `csdid` and `csdid2`\n#### @callaway_2021\n\nTypical Aggregations:\n\n- Simple: $sel_{g,t}=1$ if $t>=g$\n- Group/cohort: $sel_{g,t}=1$ if $t>=g$ and $g=G$\n- Calendar: $sel_{g,t}=1$ if $t>=g$ and $t=T$\n- Event: $sel_{g,t}=1$ if $t-g=e$\n- Cevent: $sel_{g,t}=1$ if $t-g \\in [ll,\\dots,uu]$\n\nand may be possible to combine some of these restrictions\n\n- Also, because the method is based on 2x2 DID, it allows to easily implement various methodologies, including the Doubly Robust.\n\n## Map: How @callaway_2021 relates to @wooldridge_2021 and @borusyak2023revisiting\n\nRelations: Simple case of no covariates\n\n- `jwdid` =  `did_imputation`: `jwdid` can be applied to nonlinear models, but `did_imputation` is more flexible for linear models.\n- `jwdid` = `did_imputation`  = `csdid, notyet`: When using the not-yet treated as control, `csdid` will produce the same estimates as the others if there is only **1 pretreatment** period.\n- `jwdid, never` = `csdid` = `eventinteract`: If `jwdid` uses full interactions for pre and post treatment periods, the results will be the same as `csdid` or `eventinteract`\n\nThus the main differences across models is:\n\n- How they use \"pre-treatment\" information for the estimation of ATTs (long vs short)\n- Whether they use only \"never-treated\" or \"not-yet-treated\" units for estimation\n- And how are covariates treated\n\n## Example {.scrollable}\n\n- `csdid` is the older command, with all functions well documented (helpfile).\n\n- However, it can be slow in large tasks, because of the bottle neck of using the Full Dataset for every task. \n\n- `csdid2` works almost identically to `csdid`, its faster, but some functions are not yet documented (helpfile is still missing). Will use this one here\n\n::: {#9c1cc693 .cell .larger execution_count=19}\n``` {.stata .cell-code code-fold=\"false\"}\nqui:net install csdid2, from(https://friosavila.github.io/stpackages) replace\nuse http://pped.org/bacon_example.dta, clear\n\n** create Gvar: still needs csdid\negen gvar = csgvar(post), ivar(stfips) tvar(year)\n\ncsdid2 asmrs, ///\n\tivar(stfips) /// <- required for panel Data. Otherwise RC\n\ttvar(year)   /// <- Time variable. Should be continuously (month 11, 12, 13, ...)\n\tgvar(gvar)   \n** Default uses \"never treated\", and produces Long gaps.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProducing Long Gaps by default\nUsing method reg\n----+--- 1 ---+--- 2 ---+--- 3 ---+--- 4 ---+--- 5 \n..................................................    50\n..................................................   100\n..................................................   150\n..................................................   200\n..................................................   250\n..................................................   300\n..................................................   350\n..................................\n```\n:::\n:::\n\n\n- `csdid2` is creating the full set of ATT(g,t) estimations for the data. But will produce no result\n- To obtain results and plotting, one must use post estimation commands:\n\n::: {#cd16abc7 .cell .larger execution_count=20}\n``` {.stata .cell-code code-fold=\"false\"}\nestat event, /// request event-type estimates\n\tnoavg    /// asks not to produce Averages\n\trevent(-10/10) // requests to limit the output to events between -10/10\n** Its also possible to restrict to specific groups/cohorts rgroup( list of numbers)\n** or restrict to specific years rcalendar( list of numbers)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        tm10 |  -1.456481   6.604164    -0.22   0.825     -14.4004    11.48744\n         tm9 |  -7.234872   4.564612    -1.58   0.113    -16.18135    1.711603\n         tm8 |  -4.652236   4.995688    -0.93   0.352    -14.44361    5.139134\n         tm7 |  -6.442625   6.308106    -1.02   0.307    -18.80629    5.921036\n         tm6 |  -4.424961   4.217086    -1.05   0.294     -12.6903    3.840376\n         tm5 |  -6.785855   5.032378    -1.35   0.178    -16.64913    3.077424\n         tm4 |  -1.428164   3.663803    -0.39   0.697    -8.609087    5.752758\n         tm3 |  -2.313338    3.60909    -0.64   0.522    -9.387025    4.760349\n         tm2 |  -.8035454   3.215177    -0.25   0.803    -7.105177    5.498086\n         tp0 |  -.7577979   2.763696    -0.27   0.784    -6.174542    4.658946\n         tp1 |  -2.687627   3.036269    -0.89   0.376    -8.638605     3.26335\n         tp2 |  -3.590762   4.207388    -0.85   0.393    -11.83709    4.655566\n         tp3 |  -3.341711   2.549732    -1.31   0.190    -8.339094    1.655672\n         tp4 |  -4.882915   2.892191    -1.69   0.091    -10.55151    .7856753\n         tp5 |  -6.205122   2.776364    -2.23   0.025     -11.6467   -.7635485\n         tp6 |  -6.301267    3.82955    -1.65   0.100    -13.80705    1.204514\n         tp7 |  -10.83263     3.3192    -3.26   0.001    -17.33814   -4.327119\n         tp8 |  -9.945774   3.590409    -2.77   0.006    -16.98285   -2.908703\n         tp9 |  -9.285554    3.65841    -2.54   0.011    -16.45591   -2.115202\n        tp10 |  -11.17722   3.836158    -2.91   0.004    -18.69596   -3.658494\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n`estat plot`, will produce a figure from the last estimation\n\n::: {#7b3224bb .cell .larger execution_count=21}\n``` {.stata .cell-code code-fold=\"false\"}\nestat plot, xsize(10) ysize(8)\ngraph export figev.png, replace width(800)\n```\n:::\n\n\n![a](figev.png)\n<img src=\"figev.png\" alt=\"Trulli\" style=\"width:1%\">\n\nAdvantage of `csdid2` over other methods. Uniform Confidence Intervals using Wildbootstrap SE.\n\n## Conclusions\n\n- In this workshop I aimed to provide a brief overview of the problems with TWFE and the potential solutions.\n- 3 main solutions were presented:\n  1. Avoid using bad controls (@borusyak2023revisiting and @gardner2022twostage)\n  2. Allow for heterogeneity in the treatment effect. (@wooldridge_2021 and @sun_estimating_2021)\n  3. Use *only* good DID designs. (@callaway_2021)\n\n## Conclusions\n\n- All methods have their own advantages and disadvantages.\n- 3) is the most robust based on the assumptions, but is the least efficient because of the sample size requirements. It also allows for flexiblity of DR estimators, and ensures you use pre-treatment controls only.\n- 1) and 2) are more efficient, (more data is used), but rely strongy on Long PTA assumption.\n- 2) can be applied in non-linear settings, using nonlinear models. But aggregations are slower to obtain.\n- 1) and 2) can also be adapted to consider -dose-response- effects. As well as Treatment Reversals.\n\n  \n# Thank you!\n### Comments? Questions?\n\n\n## Suggested readings\n\n::: {#refs}\n::: \n\n",
    "supporting": [
      "12did_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}