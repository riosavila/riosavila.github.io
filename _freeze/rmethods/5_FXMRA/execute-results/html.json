{
  "hash": "2e07f5a7b51fd772bba92d9fa0c2e9f4",
  "result": {
    "markdown": "---\ntitle: Multiple Regression Analysis\nsubtitle: Adding and Understanding features\ntitle-slide-attributes:\n  data-background-image: 'https://www.mysantacruzrealestate.com/uploads/shutterstock_179346260_500.jpg'\n  data-background-size: contain\n  data-background-opacity: '0.3'\nauthor: Fernando Rios-Avila\nformat:\n  revealjs:\n    slide-number: true\n    width: 1600\n    height: 900\n    code-fold: true\n    echo: true\n    css: styles.css\n    chalkboard: true\n---\n\n## Introduction\n\n- Multiple Linear Regression models (MLRM), estimated via OLS, have very good properties, if all Assumptions (A1-A5,A6') Hold.\n\n- Up until now, we have discussed how to estimate them, and analyze them under \"optimal\" assumptions, in simplified cases. \n\n- Today we will be adding other \"minor\" Features to MLR, and aim to better understand its features\n\n## Scaling and shifting\n\n1. Something that we do not emphasize enough. Before analyzing your data, its important to analyze the nature of the data (summary stats, ranges, scales)\n   \n2. When I talk about Scaling and shifting, I refer exclusibly to affine transormations of the following type:\n\n$$x^* = a*x+c \\text{ or } x^* = a*(x+c1)+c2\n$$\n\nThey either Shift, or change the scale of the data. Not the shape! (logs change shape)\n\n3. If one applies affine transformations to the data, it will have **NO** effect on your model what-so-ever. (Same t's same F's, same $R^2$)\n\n4. But, your $\\beta's$ will change. This could help understading and explaining the results.\n\n## Example: {.smaller}\n\n::: {#97a6476f .cell execution_count=1}\n``` {.stata .cell-code}\nset linesize 255\nfrause bwght, clear\ngen bwkg = bwghtlbs*0.454\ngen bwgr = bwkg*1000\nregress bwght male white cigs lfaminc \nest sto m1\nregress bwghtlbs male white cigs lfaminc\nest sto m2\nregress bwkg male white cigs lfaminc\nest sto m3\nregress bwgr male white cigs lfaminc\nest sto m4\n```\n:::\n\n\n\n\n|              |           Oz    |              |          lbs    |              |          Kgs    |              |           Gr    |              |\n| ------------ | :-------------: | :----------: | :-------------: | :----------: | :-------------: | :----------: | :-------------: | :----------: |\n| male         |        3.123*** |              |        0.195*** |              |        0.089*** |              |       88.605*** |              |\n|              |      (1.071)    |      [2.916] |      (0.067)    |      [2.916] |      (0.030)    |      [2.916] |     (30.389)    |      [2.916] |\n| white        |        5.404*** |              |        0.338*** |              |        0.153*** |              |      153.346*** |              |\n|              |      (1.392)    |      [3.882] |      (0.087)    |      [3.882] |      (0.039)    |      [3.882] |     (39.497)    |      [3.882] |\n| cigs         |       -0.480*** |              |       -0.030*** |              |       -0.014*** |              |      -13.628*** |              |\n|              |      (0.091)    |     [-5.288] |      (0.006)    |     [-5.288] |      (0.003)    |     [-5.288] |      (2.577)    |     [-5.288] |\n| lfaminc      |        1.053*   |              |        0.066*   |              |        0.030*   |              |       29.867*   |              |\n|              |      (0.632)    |      [1.664] |      (0.040)    |      [1.664] |      (0.018)    |      [1.664] |     (17.946)    |      [1.664] |\n| \\_cons       |      110.603*** |              |        6.913*** |              |        3.138*** |              |     3138.351*** |              |\n|              |      (2.071)    |     [53.410] |      (0.129)    |     [53.410] |      (0.059)    |     [53.410] |     (58.760)    |     [53.410] |\n| *N*          |         1388    |              |         1388    |              |         1388    |              |         1388    |              |\n| *R*<sup>2</sup> |        0.046    |              |        0.046    |              |        0.046    |              |        0.046    |              |\n\n: Birthweight and Cig {.striped .hover}\n\n\n\n## Scaling X's and Y's\n\n- Re-scaling $y$ will affect the all coefficients.\n  - Reducing Scale, reduces scale of coefficients\n- Re-scaling $x's$ will only affect its coefficient and possible the constant.\n  - Reducing (increasing) Scale will increase (reduce) Scale of coefficient\n- In both cases, Shifting the variable only affects the constant.\n\n:::{.callout-important}\n\nRe-Scaling is an important tool/trick that can be used for interpreting more complex models. \n\n:::\n\n## Beta or Standardized Coefficients\n\n- In some fields (health), making inferences based on default scales can be difficult (the impact of 1microgram ?).\n- To avoid this type of problem researchers may opt to use **Standardized** or **Beta** coefficients. \n  - How a $sd$ change in $X's$ affect the outcome (in $sd$)\n- Getting these coefficient is similar to applying the following transformation to all variables:\n\n$$\\tilde w = \\frac{w-\\bar w}{\\sigma_w} \\rightarrow E(\\tilde w)=0 \\text{ and } Var(\\tilde w) = 1\n$$\n\n```stata{style=\"font-size: 1.3em\"}\nreg y x1 x2 x3, beta\nest sto m1\nesttab m1, beta \n```\n\n- It also helps you make comparison of the relative importance of each covariate explanatory power.\n\n## Functional Forms: Single Dummies\n\n- Dummies are variables that take only two values (preferably 0 and 1). \n- They are used to capture qualitative (binary) characteristics (ie Democrat, Union worker, etc)\n- When used in regression analysis, they represent \"shifts\" in the Intercept:\n$$y = b_0 + b_1 male + b_2 x_1 + b_3 x_2 + e\n$$\n\n  - Here, $b_0$ would be the \"intercept\" for \"women\" (base) while $b_0+b_1$ would be the intercept for men. \n    - Under A4, $b_1$ is the expected outcome difference **men** have over **women**, everything else constant. \n- Unless further restrictions are used, you can't add Dummies for both categories in the model. \n\n```stata{style=\"font-size: 1.3em\"}\n* Stata Code\nreg y x1 x2 d    <-- Possible if d = 0 or 1\nreg y x1 x2 i.d  <-- Better\n```\n\n## Functional Forms: Multiple Dummies\n\n- We can use dummies to represent multiple (nonoverlapping) characteristics like Race, ranking or age group).\n- One needs a \"base\" or comparison group to analyze coefficients (or more).\n- Ordered variables can be used as continuous, but using them as dummies requires creating dummies for each category.\n\n$$\\begin{aligned}\ny &= b_0 + b_1 black + b_2 hispanic + b_3 other + b_4 x + e & || Base = White \\\\\ny &= b_0 + b_1 young + b_2 old + b_3 x + e & || Base = Adult\n\\end{aligned}\n$$\n\n- When using with ordered data,  multiple dummies may create somewhat counterintuitive results\n\n```stata{style=\"font-size: 1.3em\"}\ntab race, gen(race_)  <- creates dummies\nreg y i.race x1 x2 x3 <- generally uses first group as base\nreg y ib2.race x1 x2 x3 <- indicates a particular \"base\"\n```\n\n## Example {.scrollable}\n\n::: {#a19245cc .cell execution_count=3}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause beauty, clear\n** Union also a dummy. \n** looks as Continous\nqui:reg lwage exper union educ female looks\nest sto m1\ngen looks_good = looks>=4 if !missing(looks)\nqui:reg lwage exper union educ female looks_good\nest sto m2\nqui:reg lwage exper union educ female i.looks\nest sto m3\nqui:reg lwage exper union educ female ib3.looks\nest sto m4\nesttab m1 m2 m3 m4, se star( * 0.1 ** 0.05 *** 0.01  ) nogaps nomtitle\ndisplay _n \"Exact Change Union : \" %5.3f (exp(_b[union])-1)*100 \"%\"\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n----------------------------------------------------------------------------\nexper              0.0137***       0.0134***       0.0135***       0.0135***\n                (0.00119)       (0.00120)       (0.00120)       (0.00120)   \nunion               0.201***        0.201***        0.196***        0.196***\n                 (0.0305)        (0.0307)        (0.0306)        (0.0306)   \neduc               0.0737***       0.0750***       0.0735***       0.0735***\n                (0.00528)       (0.00528)       (0.00528)       (0.00528)   \nfemale             -0.448***       -0.450***       -0.446***       -0.446***\n                 (0.0293)        (0.0294)        (0.0293)        (0.0293)   \nlooks              0.0555***                                                \n                 (0.0201)                                                   \nlooks_good                         0.0276                                   \n                                 (0.0299)                                   \n1.looks                                                 0          -0.266** \n                                                      (.)         (0.134)   \n2.looks                                             0.146          -0.121***\n                                                  (0.139)        (0.0439)   \n3.looks                                             0.266**             0   \n                                                  (0.134)             (.)   \n4.looks                                             0.264*       -0.00255   \n                                                  (0.136)        (0.0312)   \n5.looks                                             0.422**         0.156   \n                                                  (0.173)         (0.111)   \n_cons               0.408***        0.565***        0.338**         0.604***\n                 (0.0968)        (0.0774)         (0.149)        (0.0781)   \n----------------------------------------------------------------------------\nN                    1260            1260            1260            1260   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p<0.1, ** p<0.05, *** p<0.01\n\nExact Change Union : 21.598%\n```\n:::\n:::\n\n\n## Functional Forms: Logarithms\n\n- Using Logarithms can help modeling some nonlinearities in the data.\n- Because it changes the \"shape\" of variables, it also changes the interpretation (Changes vs %Changes)\n- By reducing dispersion of dep. variable, CLM assumptions may hold.\n  \nBut:\n\n- Cannot or should not be applied to all data types (ie Dummies, negatives, shares)\n- (log-lin model): It is often better to use the exact percentage change rather than approximation:\n$$\\begin{aligned}\nlog(y) &= b_0 + b_1 x_1 + b_2 x_2 + b_3 D + e \\\\\n\\frac{\\% \\Delta y}{\\Delta D} &= 100 (exp(b_3)-1)\\% \n\\end{aligned}\n$$\n \n\n# Break?!\n\n## Functional Forms: Polynomials ($x^2, x^3, etc$)\n\n- Up to this point, we have only considered linear models ($X's$ enter asis or in logs). This almost always works! (Taylor expansion justification)\n  - Specially if interested in Average Effects\n- Some times, you may be interest in capturing some heterogeneity for $dy/dx$. That can be done just adding \"ANY\" transformation of $X$ in the model ($sin(x), 1/x, \\sqrt x$, etc)\n- For practical, and theoretical purposes, however, we usually concentrate on quadratic terms.\n  - For example: Increasing returns with decreasing marginal returns\n  - We may be interested in \"turning\" points\n- However, we now need to be careful about marginal effects!  \n\n## \n$$\\begin{aligned}\ny &=b_0+b_1 x_1 + b_2 x_1^2 + b_3 x_2 + e \\\\\n\\frac{dy}{dx_1} &= b_1+2b_2 x_1 =0 \\\\\nx_1^* &= - \\frac{b_1}{2b_2} x_1\n\\end{aligned}\n$$\n\n:::{.callout-note}\n\n## To consider \n\n1. Marginal effects are no longer constant. You need an $x_1$ value to obtain them (mean? average?)\n2. With Quadratic models, there is ***ALWAYS*** a turning point (but may not be relevant)\n3. MFX can be positive or negative for ***some*** value of $x_1$ (but may not be relevant)\n4. Unless something else is done, coefficients may not make sense on their own.\n\n:::  \n\n- Why not add further polynomials? \n  - Estimating them is easy (except for numerical precision), but adds complexity for interpretation. Nothing else.\n\n## Example {.scrollable}\n\n::: {#9c3aa1e8 .cell execution_count=4}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause hprice2, clear\ngen rooms2=rooms*rooms\nqui:reg lprice lnox dist rooms \nest sto m0\nqui:reg lprice lnox dist rooms rooms2\nest sto m1\nqui:reg lprice lnox dist c.rooms c.rooms#c.rooms\nest sto m2\nesttab m0 m1 m2, se varwidth(20) star(* 0.1 ** 0.05 *** 0.01) nogaps\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n--------------------------------------------------------------------\n                              (1)             (2)             (3)   \n                           lprice          lprice          lprice   \n--------------------------------------------------------------------\nlnox                       -0.968***       -0.975***       -0.975***\n                          (0.110)         (0.106)         (0.106)   \ndist                      -0.0291***      -0.0223**       -0.0223** \n                         (0.0102)       (0.00995)       (0.00995)   \nrooms                       0.302***       -0.724***       -0.724***\n                         (0.0189)         (0.171)         (0.171)   \nrooms2                                     0.0794***                \n                                         (0.0131)                   \nc.rooms#c.rooms                                            0.0794***\n                                                         (0.0131)   \n_cons                       9.793***        13.05***        13.05***\n                          (0.271)         (0.599)         (0.599)   \n--------------------------------------------------------------------\nN                             506             506             506   \n--------------------------------------------------------------------\nStandard errors in parentheses\n* p<0.1, ** p<0.05, *** p<0.01\n```\n:::\n:::\n\n\n- Negative coefficient for $rooms$, so is there a problem? \n  - Find \"turnpoint\" and summary Stats\n\nTurn point: 4.55\n\n\n::: {#e58a6bdc .cell execution_count=6}\n\n::: {.cell-output .cell-output-stdout}\n```\n\n    Variable |       Min        p1        p5       p10       p25       p50       p75       p90       p99       Max\n-------------+----------------------------------------------------------------------------------------------------\n       rooms |      3.56      4.52       5.3      5.59      5.88      6.21      6.62      7.15      8.34      8.78\n------------------------------------------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n- Does it make a difference how we estimate the model?\n\n::: {#fb3e2f39 .cell execution_count=7}\n``` {.stata .cell-code code-fold=\"false\"}\nqui:reg lprice lnox dist rooms rooms2\nmargins, dydx(rooms)\nqui:reg lprice lnox dist c.rooms c.rooms#c.rooms\nmargins, dydx(rooms) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nAverage marginal effects                                   Number of obs = 506\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  rooms\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       rooms |  -.7236433   .1706763    -4.24   0.000    -1.058973   -.3883139\n------------------------------------------------------------------------------\n\nAverage marginal effects                                   Number of obs = 506\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  rooms\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       rooms |   .2747106   .0188463    14.58   0.000     .2376831    .3117382\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n## Functional Forms: Interactions I ($d1*d2$)\n\n- It is possible to use multiple (unrelated) dummy variables. \n- Dummy interactions are feasible to allow for differential means across groups combined groups.\n- You still need a reference group that should be identified:\n$$\\begin{aligned}\ny &= a_0 + a_1 female + a_2 union + a_3 female \\times union + e \\\\\ny &= b_0 + b_1 female \\times nonunion + b_2 male \\times union +b_3 female \\times union + e\n\\end{aligned}\n$$\nBoth models are equivalent. Also\n$$\\begin{aligned}\n& E(y|male,nonunion)    && =a_0  &&= b_0   \\\\ \n& E(y|female,nonunion) && = a_0 + a_1 && = b_0 + b_1 \\\\\n& E(y|male,union) && =a_0+a_2  &&= b_0+b_2 \\\\ \n& E(y|female,union)  && = a_0 + a_1 + a_2 + a_3  &&= b_0 + b_3\n\\end{aligned}\n$$\n\n## \n\nFor this, you may need to use manual dummy creation, or use explicit interactions:\n\n```stata{style=\"font-size: 1.5em\"}\nreg y i.d1 i.d2 i.d1#i.d2\nreg y i.d1##i.d2\nreg y i.d1#i.d2\n```\n1. You set the interactions\n2. Similar to one, but `Stata` does it for you\n3. Creates full set of interactions, as in 2nd model before\n\nOptions 1 and 3 will allow you using `margins`. For overall groups (all women, all unions) you need to decide how to get representative samples.\n\n## Functional Forms: Interactions II ($x1*x2$)\n\n- You may be interested in allowing for some interaction across continuous variables.\n  - ie Interacted effect of household size and number of bedrooms\n- As with Polynomials, this allows for heterogeneity, thus effects are not constant.\n\n$$\\begin{aligned}\ny &= a_0 + a_1 x_1 + a_2 x_2 + a_3 x_1 x_2 + e \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_1} &= a_1  + a_3 x_2 \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_2} &= a_2  + a_3 x_1\n\\end{aligned}\n$$\n\n- Thus, coefficients, on their own, are difficult to interpret, unless $x_1$ or $x_2$ are zero\n\n##\n### Shortcut: Affine transformation \n\n- There is a trick that could help easy and direct interpretation. re-scaling variables:\n\n$$\\begin{aligned}\ny &= b_0 + b_1 x_1 + b_2 x_2 + b_3 (x_1-\\bar x_1)(x_2-\\bar x_2) + e \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_1} &= b_1  + b_3 (x_2-\\bar x_2) \\simeq b_1 \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_2} &= b_2  + b_3 (x_1-\\bar x_1) \\simeq b_2 \n\\end{aligned}\n$$\n\n- Also works with quadratic terms!\n\n$$\\begin{aligned}\ny &= b_0 + b_1 x_1 + b_2 (x_1-\\bar x_1)^2 + b_3 x_2 + e \\\\\n\\frac{\\Delta E(y|x_1,x_2) }{\\Delta x_1} &= b_1  + 2 b_2 (x_1-\\bar x_1) \\simeq b_1 \\\\\n\\end{aligned}\n$$\n\n## Functional Forms: Interactions III ($d1*x1$)\n\n- Dummy variables allows for shifts to the constant (intercept). \n- Interacting with continuous variables allows for shifts in slopes!. \n  - This can be useful to testing hypothesis: differences in returns to education by gender. \n  \n$$wage=b_0 + b_1 female + b_2 educ + b_3 educ \\times female + e\n$$\n\n  - $b_1$: Baseline wage differential between men and women.\n  - $b_2+b_3$: Returns to education for women.\n  - $b_1 + b_3 \\overline{educ}$: Average wage difference between men and women.\n\n`Stata`:\n```stata{style=\"font-size: 1.3em\"}\nreg y x1 i.d c.x1#i.d\n```\n\n## Functional Forms: Full Interactions (with dummies)\n\n- It is possible to estimate models where all variables are interacted with a single dummy. This allows you to test the hypothesis if two groups have the same underlying parameters.\n  - Do men and women have the same wage structure?\n\n- Full interactions is equivalent to estimating separate models:\n\n$$\\begin{aligned}\nFT: & y = b_0 + b_1 x_1 + b_2 x_2 + g_0 d +g_1 x_1 d +g_2 x_2 d +e \\\\\nD0: & y = b_0 + b_1 x_1 + b_2 x_2  +e  && \\text{ if d=0 } \\\\\nD1: & y = (b_0+g_0) + (b_1+g_1) x_1 + (b_2+g_2) x_2 +e && \\text{ if d=1 } \\\\\n    & y = a_0 + a_1 x_1 + a_1 x_2 +e && \\text{ if d=1 } \\\\\nCS1: & H_0: g_0=g_1=g_2=0 \n\\end{aligned}\n$$\n\n##\n### Chow test\n\n- $CS1$ can be tested using F-stat for multiple hypothesis.\n- But, under homoskedasticty, one could also use what is known as the **Chow test**\n\n$$\\begin{aligned}\n M1 &: y = b_0 + b_1 x_1 + b_2 x_2 + e \\\\\n M2 &: y = b_0 + b_1 x_1 + b_2 x_2 + b_3 d + e \\\\\n if \\ D=0 &: y = b_{00} + b_{01} x_1 + b_{02} x2 + e_0 \\\\\n if \\ D=1 &: y = b_{10} + b_{11} x_1 + b_{12} x2 + e_1 \n\\end{aligned}\n$$\n\n\n\nF-Stat (similar to before):\n\n$$\\begin{aligned}\nF_{M1} = \\frac{(SSR_{M1}-SSR_0-SSR_1)/(k+1)}{(SSR_0+SSR_1)/(n - 2(k+1))} \\\\\nF_{M2} = \\frac{(SSR_{M2}-SSR_0-SSR_1)/k}{(SSR_0+SSR_1)/(n - 2(k+1))}\n\\end{aligned}\n$$\n\n## Example {.scrollable}\n\n::: {#39150549 .cell execution_count=8}\n``` {.stata .cell-code code-fold=\"false\"}\n frause gpa3, clear\ndrop if cumgpa==0\nreplace sat = sat /100\nqui:reg cumgpa sat hsperc tothrs\nest sto m1\nqui:reg cumgpa sat hsperc tothrs female\nest sto m2\nqui:reg cumgpa sat hsperc tothrs if female==0\nest sto m3\nqui:reg cumgpa sat hsperc tothrs if female==1\nest sto m4\nqui:reg cumgpa i.female##c.(sat hsperc tothrs)\nest sto m5\nesttab m1 m2 m3 m4 m5, mtitle( Simple With_fem Men Women Full_int) ///\nse star(* .1 ** 0.05 *** 0.01) nogaps noomitted \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(98 observations deleted)\nvariable sat was int now float\n(634 real changes made)\n\n--------------------------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)             (5)   \n                   Simple        With_fem             Men           Women        Full_int   \n--------------------------------------------------------------------------------------------\nsat                0.0933***       0.0938***       0.0679***        0.177***       0.0679***\n                 (0.0133)        (0.0130)        (0.0151)        (0.0244)        (0.0146)   \nhsperc           -0.00865***     -0.00730***     -0.00748***     -0.00869***     -0.00748***\n                (0.00105)       (0.00106)       (0.00119)       (0.00219)       (0.00116)   \ntothrs          -0.000599       -0.000586        -0.00155**       0.00141        -0.00155** \n               (0.000662)      (0.000647)      (0.000771)       (0.00111)      (0.000748)   \nfemale                              0.277***                                                \n                                 (0.0493)                                                   \n0.female                                                                                0   \n                                                                                      (.)   \n1.female                                                                           -0.855** \n                                                                                  (0.333)   \n1.female#c~t                                                                        0.109***\n                                                                                 (0.0310)   \n1.female#c~c                                                                     -0.00121   \n                                                                                (0.00271)   \n1.female#c~s                                                                      0.00296** \n                                                                                (0.00145)   \n_cons               1.900***        1.782***        2.070***        1.215***        2.070***\n                  (0.149)         (0.147)         (0.173)         (0.257)         (0.168)   \n--------------------------------------------------------------------------------------------\nN                     634             634             483             151             634   \n--------------------------------------------------------------------------------------------\nStandard errors in parentheses\n* p<.1, ** p<0.05, *** p<0.01\n```\n:::\n:::\n\n\n::: {#e42ed156 .cell execution_count=9}\n``` {.stata .cell-code code-fold=\"false\"}\ntest 1.female#c.sat 1.female#c.hsperc 1.female#c.tothrs\ntest 1.female 1.female#c.sat 1.female#c.hsperc 1.female#c.tothrs\nmargins female, dydx(sat hsperc tothrs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n ( 1)  1.female#c.sat = 0\n ( 2)  1.female#c.hsperc = 0\n ( 3)  1.female#c.tothrs = 0\n\n       F(  3,   626) =    6.26\n            Prob > F =    0.0003\n\n ( 1)  1.female = 0\n ( 2)  1.female#c.sat = 0\n ( 3)  1.female#c.hsperc = 0\n ( 4)  1.female#c.tothrs = 0\n\n       F(  4,   626) =   12.75\n            Prob > F =    0.0000\n\nAverage marginal effects                                   Number of obs = 634\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  sat hsperc tothrs\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nsat          |\n      female |\n          0  |   .0679302    .014607     4.65   0.000     .0392454    .0966149\n          1  |   .1772582   .0273126     6.49   0.000     .1236229    .2308936\n-------------+----------------------------------------------------------------\nhsperc       |\n      female |\n          0  |    -.00748   .0011573    -6.46   0.000    -.0097526   -.0052073\n          1  |  -.0086922   .0024524    -3.54   0.000    -.0135081   -.0038763\n-------------+----------------------------------------------------------------\ntothrs       |\n      female |\n          0  |  -.0015482   .0007477    -2.07   0.039    -.0030165   -.0000798\n          1  |    .001412   .0012472     1.13   0.258    -.0010371    .0038612\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n# Break?!\n\n## Avg Partial effects vs Partial effects at $X_c$\n\n- Whenever you have interactions, higher order polynomials (or any nonlinear transformation of $X$), marginal effects are no longer constant, and may depend on additional information:\n\n$$y = b_0 + b_1 x_1 + b_2 x_1^2 + e \\rightarrow \\frac{dy}{dx} = b_1 + 2b_2 x_1\n$$\n\n- What to do in this cases? \n  1. Estimate Average marginal effects: $AME = E\\left(\\frac{dy}{dx}\\right) = b_1 + 2b_2 \\overline{x}_1$\n  2. Estimate Marginal effects at means: $MEM = \\frac{dy}{dx}\\Big|_{x=\\bar x} = b_1 + 2b_2 \\overline{x}_1$\n  3. Estimate Marginal effects at relevant values\n  4. Report ALL marginal effects\n\n## \n\n- In `Stata` you can do this only for interactions. For constructed variables you need `f_able`, or do it by hand.\n\n```stata{style=\"font-size: 1.3em\"}\nreg y c.x1##c.x1##c.x1\nmargins, dydx(x1) <-- Default is Average Marginal Effects\nmargins, dydx(x1) atmeans <-- Request marginal effects at means\nmargins, dydx(x1) at(x1=(1/5)) <-- Request marginal effects at specific values of x1\n* and plot afterwards\nmarginsplot\n```\n\n## Goodness of Fit: $R^2$ vs $R^2_{adj}$  \n#### With Great power...\n\n**IMPORTANT**: Low $R^2$ does not mean a bad model, nor high $R^2$ mean a good one.\n\n- If $N$ is constant, adding more variables to your model will increase the Goodness of fit $R^2$ (even if marginally)\n  - This may lead to the incorrect intuition of choosing models with the highest $R^2$\n  - This is wrong because $R^2$ only measures in-sample fitness. \n- Alternative, the Adjusted $R^2$ ($R_{adj}^2$), which penalizes using multiple controls\n\n$$R^2_{adj} = 1-\\frac{SSR/(n-k-1)}{SST/(n-1)}=1-(1-R^2)\\frac{n-1}{n-k-1}\n$$ \n\n- More controls $k$ will not always increase $R^2_{adj}$\n\n## \n### $R^2_{adj}$ and Model Selection\n\n- $R^2_{adj}$ can be used to choose between nested models. \n  - If adding variables improves $R_{adj}^2$, then choose that model.\n- But it can also be used to choose between non-nested models:\n\n$$\\begin{aligned}\nM1: & y = b_0 + b_1 x_1 + b_2 x_2 + e \\\\\nM2: & y = b_0 + b_1 x_1 + b_3 x_3 + e \\\\\nM3: & y = b_0 + b_1 ln(x_1) + b_2 ln(x_2) + e \\\\\nM4: & y = b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3 + e \n\\end{aligned}\n$$\n\n## log models and Prediction\n\n- Transforming the Depvariable with logs is quite useful for interpretation, and addressing overdispersion\n- However, obtaining predictions from such models is not straight forward:\n\n$$\\begin{aligned}\nln(y) &= a_0 + a_1 x_1 + a_2 x_2 + \\varepsilon \\\\\ny &= exp(a_0 + a_1 x_1 + a_2 x_2 + \\varepsilon ) \\\\\nE(y|x_1,x_2) &=E(e^{a_0 + a_1 x_1 + a_2 x_2}) \\times E(e^ \\varepsilon ) \\\\\n & E(e^ \\varepsilon )\\neq 1\n\\end{aligned}\n$$\n\n- To make Predictions in a log model we need some approximation for $E(e^ \\varepsilon )$\n\n##  \n### We have Options:\n\nLets call $E(e^ \\varepsilon ) = \\alpha_0$\n\n**Option 1** : $\\alpha_0 = n^{-1} \\sum( \\exp {\\hat\\varepsilon})$\n\n**Option 2** : Under Normality of $\\varepsilon$,   $\\alpha_0 = \\exp(\\hat \\sigma^2/2)$\n\n**Option 3** : Call $\\hat m = \\exp(a_0 + a_1 x_1 + a_2 x_2)$. \n\nRegress $y$ on $\\hat m$ without intercept.  $\\alpha_0 = \\frac{\\hat m'y}{\\hat m'\\hat m}$\n\n- Your $\\hat y$ prediction can now be used to estimate a comparable $R^2$\n\n$$R^2 = Corr(y,\\hat y)^2 \\text{ or } 1-\\frac{\\sum(y_i-\\alpha_0 \\hat m_i)^2}{\\sum(y-\\bar y)^2}\n$$\n\n## {.scrollable}\n### Example\n\n::: {#9c68ee6e .cell execution_count=10}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause oaxaca, clear\ndrop if lnwage==.\ngen wage = exp(lnwage)\nqui:reg lnwage educ exper tenure female married divorced\npredict lnw_hat\npredict lnw_res, res\n** Case 1:\negen alpha_01 = mean( exp(lnw_res))\n** Case 2:\nqui:sum lnw_res\ngen alpha_02 = exp(r(Var)/2)\ngen elnw_hat = exp(lnw_hat)\nqui: reg wage elnw_hat, nocons\ngen alpha_03 = _b[elnw_hat]\ngen wage_1 = elnw_hat\ngen wage_2 = elnw_hat*alpha_01\ngen wage_3 = elnw_hat*alpha_02\ngen wage_4 = elnw_hat*alpha_03\nmata:  y = st_data(.,\"wage\"); my = mean(y)\nmata:  yh = st_data(.,\"wage_1 wage_2 wage_3 wage_4\")\nmata:\"R2_1 \"; 1 - sum((y:-yh[,1]):^2)/sum( (y:-my):^2 )\nmata:\"R2_2 \"; 1 - sum((y:-yh[,2]):^2)/sum( (y:-my):^2 )\nmata:\"R2_3 \"; 1 - sum((y:-yh[,3]):^2)/sum( (y:-my):^2 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Excerpt from the Swiss Labor Market Survey 1998)\n(213 observations deleted)\n(option xb assumed; fitted values)\n  R2_1 \n  .1569552664\n  R2_2 \n  .1692562931\n  R2_3 \n  .1658805115\n```\n:::\n:::\n\n\n## Limited Dependent variables\n\n- So far, we have impliclity assumed your dep. variable is continuous and unbounded.\n- However, OLS imposes no distributional assumptions (A6 is more convinience)\n- This means that LRM using OLS can be used for variables with limited distribution! \n  - like Dummies or count variables\n  \n## Linear Probability Model - LPM\n\n- LPM can be used when the dep.variable is a dummy, and the goal is to explain the Likelihood of something to happen.\n\n$$\\begin{aligned}\nD &= b_0 + b_1 x_1 + b_2 x_2 +b_3 x_3 + e \\\\\nE(D|Xs) &= P(D=1|Xs) \\\\\n        &= b_0 + b_1 x_1 + b_2 x_2 +b_3 x_3 \n\\end{aligned}\n$$\n\nNote: \n\n- For marginal effects, we no longer consider effects at the individual level.\n- Instead we look into conditional means, and likelihood\n\n``` {.stata .cell-code}\nfrause mroz, clear\nqui: reg inlf  age educ exper kidsge6 kidslt6 nwifeinc \nmodel_display\n```\nE(inlf|X) = 0.707 - 0.018 age + 0.040 educ + 0.023 exper + 0.013 kidsge6 - 0.272 kidslt6 - 0.003 nwifeinc\n\nN =        753  R^2 = 0.254\n\n\n##\n### Problems with LPM\n\n- LPM are easy to estimate and interpret but it has some problems:\n  - Predictions could fall below 0 or above 1 (what does it mean?)\n  - Unless more flexible functional forms are allowed, mfx are fixed.\n  - The model is, by construction, Heteroskedastic:\n \n$$Var(y|x)=p(x)*(1-p(x))\n$$\n\nThus SE will be incorrect, affecting inference \n\n## Modeling Count Data\n\n- You could also use LRM (via OLS) to model count data. \n  - Count data is always possitive, but with discrete values\n\n$$Children = b_0 + b_1 age + b_2 education + e$$\n\n- Nothing changes for estimation, but its useful to change language:\n\n``` {.stata .cell-code}\nfrause fertil2, clear\nqui reg children age educ\nmodel_display\n```\nE(children|X) = -1.997 + 0.175 age - 0.090 educ\n\nN =       4361  R^2 = 0.560\n\n\n- 1 year of education decreases # of children in .09.\n- 1 year of education decreases Fertility .09 children per women.\n- Every 100 women, If they were 1 year more educated, we would expect to see 9 fewer children among them.\n\n## Prediction, Policy and Shifting {.scrollable}\n\n- As mentioned before, intercepts, or constant in model regressions are usually meaningless.\n  - Because $a_0 = E(y|X=0)$ (does it make sense)\n- Constant, however, can be useful if we apply some transformations to the data.\n$$y = b_0 +  b_1 (x_1 - c_1) +  b_2 (x_2 - c_2) +  b_3 (x_3 - c_3) +e\n$$\n\nIn this case $b_0$ is the expected value of $y$ when $x_1=c_1$, $x_2=c_2$ and $x_3=c_3$. Thus, its now Useful!\n\n- Using this **affine** transformation, we can easily make predictions (and get SE) for any specific values of interest.\n\n  - Granted, you could also use \"margins\"\n\n::: {#d41b64a2 .cell execution_count=13}\n``` {.stata .cell-code}\nfrause gpa2, clear\ngen sat0=sat-1200\ngen hsperc0=hsperc-30\ngen hsize0=hsize-5\ngen hsize20=hsize^2-25\nqui:reg colgpa sat hsperc c.hsize##c.hsize\nmargins, at(sat = 1200 hsperc = 30 hsize = 5)\nreg colgpa sat0 hsperc0 hsize0 hsize20\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nAdjusted predictions                                     Number of obs = 4,137\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\nAt: sat    = 1200\n    hsperc =   30\n    hsize  =    5\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       _cons |   2.700075   .0198778   135.83   0.000     2.661104    2.739047\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =     4,137\n-------------+----------------------------------   F(4, 4132)      =    398.02\n       Model |  499.030503         4  124.757626   Prob > F        =    0.0000\n    Residual |  1295.16517     4,132  .313447524   R-squared       =    0.2781\n-------------+----------------------------------   Adj R-squared   =    0.2774\n       Total |  1794.19567     4,136  .433799728   Root MSE        =    .55986\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        sat0 |   .0014925   .0000652    22.89   0.000     .0013646    .0016204\n     hsperc0 |  -.0138558    .000561   -24.70   0.000    -.0149557   -.0127559\n      hsize0 |  -.0608815   .0165012    -3.69   0.000    -.0932328   -.0285302\n     hsize20 |   .0054603   .0022698     2.41   0.016     .0010102    .0099104\n       _cons |   2.700075   .0198778   135.83   0.000     2.661104    2.739047\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n##\n### Policy Evaluation\n\n- When modeling $y = b_0 + \\delta \\ trt + b_1 x_1 + b_2 x_2 + e$ the treatment effect $\\delta$ was estimated under homogeneity assumption (only intercept shift)\n\n- This assumption can be relaxed by estimating separate models or using interactions.\n  \n- Effects can be estimated manually (separate models), margins or using shifts!\n\nUsing Separate models:\n$$\\begin{aligned} \ny &= b^0_0 +  b^0_1 x_1 + b^0_2 x_2 + e^0 \\text{ if trt=0} \\\\\ny &= b^1_0 +  b^1_1 x_1 + b^1_2 x_2 + e^1 \\text{ if trt=1} \\\\\n& ATE = E(\\hat y_1 - \\hat y_0 ) \\\\\n& ATT = E(\\hat y_1 - \\hat y_0 | trt=1) \\\\\n& ATU = E(\\hat y_1 - \\hat y_0 | trt=0) \n\\end{aligned}\n$$\n\n## \n\nOr using Model Shits\n\n$$\\begin{aligned} \ny  &= b_0 + \\delta_{ate} trt + b_1 x_1 + g_1 trt (x_1- E(x_1)) + e \\\\\ny  &= b_0 + \\delta_{att} trt + b_1 x_1 + g_1 trt (x_1- E(x_1|trt=1)) + e \\\\\ny  &= b_0 + \\delta_{atu} trt + b_1 x_1 + g_1 trt (x_1- E(x_1|trt=0)) + e \\\\\n\\end{aligned}\n$$\n\n::: {#58be300b .cell execution_count=14}\n``` {.stata .cell-code}\nfrause jtrain98, clear\nforeach i in earn96 educ age married {\n  sum `i' if train==0, meanonly\n  gen atu_`i' = (`i' - r(mean))*train\n  sum `i' if train==1, meanonly\n  gen att_`i' = (`i' - r(mean))*train\n  sum `i' , meanonly\n  gen ate_`i' = (`i' - r(mean))*train\n}\nqui:reg earn98 train earn96 educ age married\nest sto m1\nqui:reg earn98 train earn96 educ age married ate*\nest sto m2\nqui:reg earn98 train earn96 educ age married atu*\nest sto m3\nqui:reg earn98 train earn96 educ age married att*\nest sto m4\n\nesttab m1 m2 m3 m4, keep(train) mtitle(Homogenous ATE ATU ATT) se\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n               Homogenous             ATE             ATU             ATT   \n----------------------------------------------------------------------------\ntrain               2.411***        3.106***        3.533***        2.250***\n                  (0.435)         (0.532)         (0.667)         (0.449)   \n----------------------------------------------------------------------------\nN                    1130            1130            1130            1130   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n:::\n:::\n\n\n# Thanks all !\nNext week Lifting Assumptions: Heteroskedasticity\n\n",
    "supporting": [
      "5_FXMRA_files"
    ],
    "filters": [],
    "includes": {}
  }
}