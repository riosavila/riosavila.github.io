{
  "hash": "9b19a568508724b0f44c4cd4c511d4ed",
  "result": {
    "markdown": "---\ntitle: 'Multiple Regression Analysis: Estimation'\ntitle-slide-attributes:\n  data-background-image: images/paste-4.png\n  data-background-size: contain\n  data-background-opacity: '0.5'\nsubtitle: The first tool of Many\nauthor: Fernando Rios-Avila\nformat:\n  revealjs:\n    slide-number: true\n    width: 1600\n    height: 900\n    code-fold: true\n    echo: true\n    css: styles.css\n---\n\n## Why stay with 1 when you can use Many? ... Why not?\n\n-   The SLRM we cover last week is a powerful tool to understand the mechanics behind regression analysis, however is too limited.\n    -   Use one control? to fix everything ?!\n-   The Natural alternative is to relax the assumption and Make things more flexible.\n    -   In other words...Allow for adding More controls\n\nThus, instead of: $$y_i = \\beta_0 + \\beta_1 x_i + e_i\n$$\n\nwe have to consider:\n\n$$y_i = \\beta_0 + \\beta_1 x_{1i} +\\beta_2 x_{2i} + \\dots + \\beta_k x_{ki} + u_i\n$$\n\nHow many can we add? and why does it help?\n\n## The power of MLR: Why do more controls help?\n\n1.  One more explicitly accounts for variables that before were *hidden* in $e_i$. $x_{2i},x_{3i},\\dots,x_{ki} \\rightarrow$ Model not $u_i$ *ceteris paribus*\n\n2.  Allows for richer model specifications and nonlinearities:\n\n    Before: $y_i = \\beta_0 + \\beta_1 x_{1i} + e_i$\n\n    Now : $y_i = \\beta_0 + \\beta_1 x_{1i} +\\beta_2 x^2_{1i} + \\beta_3 x^{1/2}_{1i} + \\beta_4 x^{-1}_{1i} + \\beta_5 x_{2i}+\\dots+e_i$\n\nThus, we can get closer to the unknown Population function, and explicitly handle *some* endogeneity problems (we control for it).\n\n::: callout-caution\n## With great power...\n\nBeing able to add more controls is good, but:\n\n-   May make things worse (bad controls)\n-   Or might not be feasible (small Sample)\n-   Or may be difficult to interpret (unless you know how to)\n:::\n\n# Do assumptions change?\n\nNot really, but lets make some math changes:\n\n$$y=\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n\\end{bmatrix} \n; \nX=\\begin{bmatrix}x_1' \\\\ x_2' \\\\ \\vdots \\\\ x_n'\n\\end{bmatrix} = \\begin{bmatrix} 1 & x_{11} & x_{21} & \\dots &  x_{k1} \n\\\\ 1 & x_{12} & x_{22} & \\dots &  x_{k2} \n\\\\ \\vdots & \\vdots  & \\vdots & \\ddots & \\vdots\n\\\\ 1 & x_{1n} & x_{2n} & \\dots &  x_{kn} \n\\end{bmatrix};\n\\beta =\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_k \n\\end{bmatrix}; e=\\begin{bmatrix}e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n\n\\end{bmatrix} \n $$\n\n$$y=X\\beta + e\n$$\n\n## Mostly the same\n\n1.  **Linear in Parameters**: $y = X\\beta + e$ (And this is the pop function)\n\n2.  **Random Sampling** from the population of interest. (So errors $e_i$ is independent from $e_j$)\n\n3.  **No Perfect Collinearity**:\n\n    This is the alternative to $Var(x)>0$ (SLRM), and deserves more attention.\n\n-   We want each variable in $X$ to have [***some***]{.bluetxt} independent variation, from all other variables in the model.\n    -   In the SLRM, the independent variation idea was with respect to the constant.\n-   [**If**]{.redtxt} a variable was a linear combination of others, then $\\beta's$ cannot be identified. You need to choose what to keep: $$\\begin{aligned}\n    y &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1+X_2) + e \\\\ \n    &=  \\beta_0 + (\\beta_1+\\beta_3) X_1 + (\\beta_2+\\beta_3) X_2 + e  \n    \\end{aligned}\n    $$\n\n## \n\n4.  **Zero Conditional mean** (Exogeneity): $E(e_i|X)=0$\n\n    Requires that the errors and the explanatory variables are uncorrelated. This is \"easier\" to achieve, because we can now move variables form the error to the model.\n\n    However, there could be things you can't controls for (and remain lurking in your errors)\n\n> I call this the most important assumption, because is the hardest to deal with\n\n#### If A1-A4 Hold, then your estimates will be unbiased!\n\n5.  **Homoskedasticity** Same as before. Errors dispersion does not change with respect to **all** $X's$. $$Var(e|X)=c\n    $$\n\nJust as with SLRM, this assumption will help with the estimation of Standard Errors.\n\n## MLRM estimation\n\nAs before, not much has changed. We are still interested in finding $\\beta's$ that Minimizes the (squared) error of the model when compared to the observed data:\n\n$$\\hat \\beta = \\min_\\beta \\sum (y_i-X_i'\\beta)^2 = \\min_\\beta \\sum (y_i-\\beta_0-\\beta_1 x_{1i}-\\dots-\\beta_k x_{ki})^2\n$$\n\nThe corresponding FOC generate $K+1$ equations to identify $K+1$ parameters:\n\n$$\\begin{aligned}\n\\sum (y_i-X_i'\\beta) &= 0  \\\\\n\\sum x_{1i}(y_i-X_i'\\beta) &= 0 \\\\\n\\sum x_{2i}(y_i-X_i'\\beta) &= 0 \\\\ \\dots \\\\\\\n\\sum x_{ki}(y_i-X_i'\\beta) &= 0 \n\\end{aligned} \\rightarrow X'(y-X\\beta) =0 \\rightarrow \\hat \\beta = (X'X)^{-1}X'y\n$$\n\n## `mata` Interlute (for those curious) {.scrollable}\n\n::: {#2508ef87 .cell .larger execution_count=1}\n``` {.stata .cell-code}\nfrause gpa1, clear\ngen one =1 \nmata: y=st_data(.,\"colgpa\"); mata: x=st_data(.,\"hsgpa act one\")\nmata: xx=x'x ; ixx=invsym(xx) ; xy = x'y \nmata: b = ixx * xy ; b\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 1\n    +---------------+\n  1 |  .4534558853  |\n  2 |  .0094260123  |\n  3 |  1.286327767  |\n    +---------------+\n```\n:::\n:::\n\n\n## You got the $\\beta's$, how do you interpret them?\n\nInterpretation of MLRM is similar to the SLRM. For **most** cases, you simply look into the coefficients, and interpret effects in terms of Changes:\n\n$$\\begin{aligned}\ny_i = \\hat\\beta_0 + \\hat\\beta_1 x_{1i}  + \\hat\\beta_2 x_{2i} + e_i \\\\\n\\Delta y_i =  \\hat\\beta_1 \\Delta  x_{1i}  + \\hat\\beta_2 \\Delta  x_{2i} + \\Delta e_i\n\\end{aligned}\n$$\n\nUnder A1-A5 I can make use the above to make interpretations\n\n1.  $\\hat \\beta_0$ has no effect on \"changes\" of $y$. Only its levels.\n2.  $\\hat \\beta_1$ indicates how much $\\Delta y_i$ will be if $\\Delta x_{1i}$ increases in 1 unit, if both $\\Delta x_{2i}$ and $\\Delta e_i$ remain constant (*Ceteris Paribus*)\n\n$\\Delta e_i=0$ by assumption, and $\\Delta x_{2i}=0$ becuse we are explicilty controlling for it (We impute this based on extrapolations)\n\nYou could also analyze the effect of $\\Delta x_{1i}$ and $\\Delta x_{2i}$ Simultaneously!\n\n## Example\n\n``` {.stata .cell-code}\nqui: frause wage1, clear\nqui: reg lwage educ exper tenure\nlocal b0:display %5.3f _b[_cons]\nlocal b1:display %5.3f _b[educ]\nlocal b2:display %5.3f _b[exper]\nlocal b3:display %5.3f _b[tenure]\ndisplay \"\\$log(wage) = `b0' + `b1' educ + `b2' exper + `b3' tenure$\"\n```\n$log(wage) = 0.284 + 0.092 educ + 0.004 exper + 0.022 tenure$\n\n\n-   $\\beta_0$ has no effect on changes, but level.\n    -   If someone has no education, experience or tenure, log(wages) will be 0.284. Why not wages? and Does it make sense to assume 0 education, experience and tenure?\n-   $\\beta_1$: An additional year of education increases wages in 0.092log points or about 9.2%, if Experience and tenure do nor change (ceteris paribus).\n\n**Notes:**\n\n1.  Think of Interpretations as counterfactual: $y_{post} - y_{pre}$\n\n2.  Assumption: Other factors (unobserved $e$) remain fixed (is it always credible??)\n\n3.  Effects can be combined. What if a person gains 1 year of education but losses 3 of tenure?\n\n## More on Interpretation\n\nUnder A1-A5, you can still interpret results as \"counterfactual\" at the individual level. However, its more common to do it based on Conditional means:\n\n$$\\frac {\\Delta E(y|X)}{\\Delta X_k} \\simeq E(y|X_{-k},X_k+1)-E(y|X)\n$$\n\nWhich mostly changes Language.\n\n> The expected effect of an increase in $X$ in one unit.\n\n## Alternative Interpretation: *Partialling out*\n\n-   An alternative way of interpreting (and understanding) MLRM is to think about *partialling out* interpretation.\n\n-   This interpretation is based on the Frisch-Waugh-Lowell Theorem, which states that the following models should give you the **SAME** $\\beta's$:\n\n$$\\begin{aligned}\ny &= \\color{blue}{\\beta_1 } X_1 + \\beta_2 X_2 + e \\\\\n(I-P_{X^c_2}) y &= \\color{green}{\\beta_1} (I-P_{X^c_2}) X_1 + e \\\\\nP_{X^c_2} &= X^c_2 (X'^{c}_2  X^{c}_2) X'^{c}_2 : \\text{Projection Matrix}\n\\end{aligned}\n$$\n\n::: callout-tip\n## Partialling out\n\n$\\beta_1$ can be interpreted as the effect of $X_1$ on $y$, after all variation related to $X_2$ has been \"eliminated\".\n\nThus $\\beta_1$ is the effect uniquely driven by $X_1$.\n:::\n\n## Example {.scrollable}\n\n::: {#01cc697e .cell .larger execution_count=3}\n``` {.stata .cell-code code-fold=\"false\"}\nqui {\n  frause oaxaca, clear\n  drop if lnwage==.\n  reg lnwage educ exper tenure\n  est sto m1\n  reg educ        exper tenure\n  predict r_educ , res\n  reg lnwage      exper tenure\n  predict r_lnwage , res\n  reg r_lnwage r_educ\n  est sto m2\n  reg lnwage educ\n  est sto m3\n}\nesttab m1 m2 m3, se  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                   lnwage        r_lnwage          lnwage   \n------------------------------------------------------------\neduc               0.0870***                       0.0800***\n                (0.00516)                       (0.00539)   \n\nexper              0.0113***                                \n                (0.00154)                                   \n\ntenure            0.00837***                                \n                (0.00188)                                   \n\nr_educ                             0.0870***                \n                                (0.00516)                   \n\n_cons               2.140***     8.93e-10           2.434***\n                 (0.0650)        (0.0124)        (0.0636)   \n------------------------------------------------------------\nN                    1434            1434            1434   \n------------------------------------------------------------\nStandard errors in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n:::\n:::\n\n\n## Estimator Properties: Unbiased\n\nRecall, the estimator of $\\beta's$ when you have multiple dependent variables:\n\n$$\\begin{aligned}\n0  &: \\hat \\beta = (X'X)^{-1} X'y \\\\\nA1 \\text{ & }  A2 &: \\hat \\beta = (X'X)^{-1} X'(X\\beta + e) \\\\\n1  &: \\hat \\beta = (X'X)^{-1} X'X\\beta + (X'X)^{-1} X'e \\\\\nA3 &: det(X'X)\\neq 0 \\rightarrow (X'X)^{-1} \\text{ exists} \\\\\n2  &: \\hat \\beta = \\beta + (X'X)^{-1} X'e \\\\\nA4 &: E(e|X)=0 \\rightarrow E[(X'X)^{-1} X'e]=0 \\\\\n3  &: E(\\hat\\beta)= \\beta \\text{ unbiased} \n\\end{aligned}\n$$\n\n## Estimator Properties: Variance under Homoskedasticity\n\nLets start with (2). $\\beta's$ are random functions of the errors. Thus its variance will depend on $e$.\n\n$$\\begin{aligned}\n1 &: \\hat \\beta = \\beta + (X'X)^{-1} X'e \\\\\n2 &:\\hat \\beta - \\beta = (X'X)^{-1} X'e \\\\\n3 &: Var(\\hat \\beta - \\beta) = Var((X'X)^{-1} X'e) \\\\\n4 &: Var(\\hat \\beta - \\beta) = (X'X)^{-1} X' Var(e) X' (X'X)^{-1}  \\\\\n\\end{aligned}\n$$\n\n$Var(e)$ considers variance and covariance of each $e_i$ and its combinations.\n\n## \n\nBy assumption A2, $cov(e_i,e_j)=0$. And by assumption A5 $Var(e_i)=Var(e_j)$.\n\n$$\\begin{aligned}\nVar(\\hat \\beta - \\beta) &= (X'X)^{-1} X' \\sigma_e^2 I X' (X'X)^{-1} \\\\\nVar(\\hat \\beta - \\beta) &= \\sigma_e^2 (X'X)^{-1} \\\\\nVar(\\hat \\beta_j - \\beta_j) &= \\frac{\\sigma_e^2}{SST_j (1-R^2_j)} \n\\end{aligned}\n$$\n\nBut we do not know $\\sigma^2_e$. Thus, we also \"estimate it\"\n\n$$\\hat \\sigma^2_e = \\frac{\\sum \\hat e^2}{N-K-1}\n$$\n\nWhich is unbiased estimator for $\\sigma^2_e$ if A1-A5 hold.\n\n## Estimator Properties: Variance under Homoskedasticity\n\n$$\\begin{aligned}\nVar(\\hat \\beta - \\beta) &= \\sigma_e^2 (X'X)^{-1} \\\\\nVar(\\hat \\beta_j - \\beta_j) &= \\frac{\\sigma_e^2}{SST_j (1-R^2_j)}  \\\\\\\n& = \\frac{\\sigma_e^2}{(N-1)Var(X_j) (1-R^2_j)} = \\frac{\\sigma_e^2}{(N-1)Var(X_j)}VIF_j\n\\end{aligned}\n$$\n\nTo consider:\n\n-   $Var(\\beta)$ increases with $\\sigma_e^2$. More variation in the error, more variation of the coefficients.\n-   $Var(\\beta)$ decreases with Sample size $N$\n-   $Var(\\beta)$ also decreases with Variation in $X$\n-   However, it **increases** if there is less unique variation (Multicolinearity problem and VIF)\n\n## Quick Note\n\n-   $R^2$ are the same as SLRM: How much of variation is explained by the model.\n    -   Also $R^2 = corr(y,\\hat y)^2$\n-   The fitted line goes over the \"mean\" of all variables\n-   MLRM Fits *hyper*-planes to the data\n-   Regression through the origin still a bad idea\n\n# Using Many controls is Fun! \n\n::: {.incremental}\n- You can add more *stuff* for better fit (high R^2)\n- Making sure nothing remains in \"$e$\"\n- It would also allow you for very \"flexible\" models\n- But...(these things are not necessarily good)\n:::\n\n## {background-image=\"images/paste-5.png\" background-size=\"contain\"}\n\n## Ignoring Variables\n\nIn the MLRM framework, its easier to see what happens when important variables are ignored. \n\n$$\\text{True: } y = b_0 + b_1 x_1 + b_2 x_2 + e\n$$\n\nBut instead you estimate the following :\n\n$$\\text{Estimated: }y = g_0 + g_1 x_1 + v\n$$\n\nUnless stronger assumptions are imposed, $g_1$ will be a biased estimate of $b_1$. \n\n$$\\begin{aligned}\n\\hat g_1 &= \\frac{\\sum \\tilde x_1 \\tilde y}{\\sum \\tilde x_1^2} \n         = \\frac{\\sum \\tilde x_1 (b_1 \\tilde x_1 +\\tilde b_2 \\tilde x_2 + e) }{\\sum \\tilde x_1^2} \\\\\n         &= \\frac{b_1 \\sum \\tilde x_1^2}{\\sum \\tilde x_1^2}\n          + b_2 \\frac{\\sum \\tilde x_1\\tilde x_2}{\\sum \\tilde x_1^2}\n          +\\frac{\\sum \\tilde x_1 e}{\\sum \\tilde x_1^2} \\\\\n         &= b_1+b_2 \\delta_1 +\\frac{\\sum \\tilde x_1 e}{\\sum \\tilde x_1^2} \\\\\n\\end{aligned}\n$$\n\n## \n\nThis implies that $g_1$ is biased:\n\n$$E(\\hat g_1) = b_1+b_2 \\delta_1\n$$\n\nWhere $\\delta_1$ is the coefficient in $x_2=\\delta_0+\\delta_1 x_1 + v$.\n\nImplications: \n\n- Unless \n  - $\\delta_1$ is zero ($x_1$ and $x_2$ are linearly independent) or,\n  - $b_2$ is zero ($x_2$ was irrelevant)\n  \n  ignoring $x_2$ will generate biased (and inconsistent) estimates for $b_1$.\n\nIn models with more controls, the direction of the biases will be harder to define, but similar rule's of thumb can be used. \n\n## Adding irrelevant controls\n\nAdding irrelevant controls will have no effect on bias and consistency. \n\nif your model is:\n\n$$y=b_0+b_1 x_1 +e\n$$\n\nbut you estimate: \n\n$$y=g_0+g_1 x_1+g_2 x_2 +v\n$$\n\nyour model is still unbiased:\n$$\\begin{aligned}\ng &= (X'X)^{-1}X'(X \\beta^+ + e) \\\\\n    \\beta^+ &= [\\beta \\ ; 0] \\\\\ng &=  \\beta^+ + (X'X)^{-1}X'e \\rightarrow E(g) = \\beta^+\n\\end{aligned}\n$$\n\n## Adding \"bad\" Controls\n\nThe worst case, yet hard to see, is when you add \"bad\" Controls, also known as Colliers.\n\nFor example:\n\n- Say you want to analyze the effect of **education** on **wages**, and you control for **occupation**. \n  Will it create an unbiased estimate for education?\n  - No. Your education **affects**  your occupation choice. So some of the effect of education will be \"absorbed\" by occupation.\n- Say you want to see the impact of **health expenditure** on **health**, and you control for \"#visits to the doctor\"\n  - This may also affect your estimates, as expenditure may change how many times you Visits are highly related. \n\nIn general, you want to avoid using \"channels\" as Controls. \n\n\n\n## What about Standard Errors\n\n:::{.panel-tabset}\n\n## Case 1\n\nOmitting relevant variables that are correlated to $X's$\n\nWe wont talk about this. It violates A4, and creates endogeneity\n\n## Case 2\n\nOmitting relevant variables that are uncorrelated to $X's$\n\n- Omitted variables will be in the error $e$. Thus variance of coefficients will be larger\n\n$$\\begin{aligned}\nTrue: & y = b_0 + b_1 x_1 + b_2 x_2 + e  \\\\\nEstimated: & y = g_0 + g_1 x_1 + v   \\\\\n & Var(e)<Var(v) \\rightarrow Var(b_1)<Var(g_1)\n\\end{aligned}\n$$\n\nThus Adding controls in Randomized experiements is still a good idea!\n\n## Case 3\n\nAdding Irrelevant controls (related to X's)\n\nCoefficients are unbiased, and $\\sigma^2_e$ will also be unbiased. \n\nHowever, you may increase Multicolinearity in the model increasing $R_j^2$ and $VIF_j$.\n\nVariance of relevant coefficients will be larger. \n\n$$\\begin{aligned}\nTrue: & y = b_0 + b_1 x_1  + e  \\\\\nEstimated: & y = g_0 + g_1 x_1 + g_2 x_2 + v   \\\\\n& Var(b_1)<Var(g_1)\n\\end{aligned}\n$$\n\n:::\n\n\n# Examples!\n\n## Prediction\n\n## Efficient Market\n\n## Treatment Evaluation\n\n",
    "supporting": [
      "3_MLRM_files"
    ],
    "filters": [],
    "includes": {}
  }
}