{
  "hash": "ca491ba9ce8d7d171f31cb6ab793f23a",
  "result": {
    "markdown": "---\ntitle: Multiple Regression Analysis\nsubtitle: When A5 Fails\nauthor: Fernando Rios-Avila\nformat:\n  revealjs:\n    slide-number: true\n    width: 1600\n    height: 900\n    code-fold: true\n    echo: true\n    css: styles.css\n    chalkboard: true\n---\n\n## What is Heteroskedasticity?\n\n-   Mathematically: $$Var(e|x=c_1)\\neq Var(e|x=c_2)\n    $$\n\n-   This means: the conditional variance of the errors is not constant across control characteristics.\n\n::: {.cell execution_count=1}\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<style>div.jp-Notebook .datagrid-container {min-height: 448px; }</style>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](6_Heteros_files/figure-revealjs/cell-2-output-2.png){fig-align='center'}\n:::\n:::\n\n\n## Consequences\n\nWhat happens when you have heteroskedastic errors?\n\n-   In terms of $\\beta's$ and $R^2$ and $R^2_{adj}$, nothing. Coefficients and Goodness of fit are still unbiased and consistent.\n\n-   But, Coefficients standard errors are based on the simplifying assumption of normality. Thus Variances will be bias!.\n\n    -   If variances are biased, then all statistics will be wrong.\n\n##  {.scrollable}\n\n### How bad can it be?\n\nSetup:\n\n$y = e$ where $e~N(0,\\sigma_e^2h(x))$\n\n$x = uniform(-1,1)$\n\n::: {.cell execution_count=2}\n``` {.stata .cell-code code-fold=\"true\"}\n/*capture program drop sim_het\nprogram sim_het, eclass\n    clear\n    set obs 500 \n    gen x = runiform(-1,1)\n    gen u = rnormal()\n    ** Homoskedastic\n    gen y_1 = u*2\n    ** increasing first, decreasing later\n    gen y_4 = u*sqrt(9*abs(x))\n \treplace x = x-2\n    reg y_1 x\n    matrix b=_b[x],_b[x]/_se[x]\n    reg y_4 x\n    matrix b=b,_b[x],_b[x]/_se[x]\n    matrix coleq   b = h0 h0 h3 h3 \n    matrix colname b = b  t  b  t \n    ereturn post b\nend\nqui:simulate , reps(1000) dots(100):sim_het\nsave mdata/simulate.dta, replace*/\nuse mdata/simulate.dta, replace\ntwo (kdensity h0_b_t) (kdensity h3_b_t) ///\n    (function y = normalden(x), range(-4 4) lw(2) color(gs5%50)), ///\n    legend(order(3 \"Normal\" 1 \"With Homoskedasticty\" 2 \"with Heteroskedasticity\"))\ngraph export images/fig6_1.png, replace height(1000)\n```\n:::\n\n\n![](images/fig6_1.png){fig-align=\"center\"}\n\n# What to do about it?\n\n## What to do about it?\n\n-   So, If errors are heteroskedastic, then all statistics (t-stats, F-stats, chi2's) are wrong.\n\n-   But, there are solutions...many solutions\n\n    -   GLS: Generalized Least Squares\n    -   WLS: Weighted Least Squares\n    -   FGLS: Feasible Generealized Least Squares\n    -   WFLS: Weighted FGLS\n    -   HC0-HC3: Heteroskedasticity consistent SE\n\n-   Some of them are more involved than others.\n\n-   But before trying to do that, lets first ask...do we have a problem?\n\n## Detecting the Problem\n\n-   Consider the model:\n\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +\\beta_3 x_3 +e\n$$\n\n-   We usually start with the assumption that errors are homoskedastic $Var(e|x's)=\\sigma^2_c$.\n-   However, now we want to allow for the possibility of heteroskedasiticity. ie, that variance is some function of X.\n    -   We have to model is variance is a function that varies with $x$:\n\n$$Var(e|x)=f(x_1,x_2,\\dots,x_k) \\sim a_0+a_1x_1 + a_2 x_2 + \\dots + a_k x_k+v$$\n\n## \n\n$$Var(e|x)=f(x_1,x_2,\\dots,x_k) \\sim a_0+a_1x_1 + a_2 x_2 + \\dots + a_k x_k+v$$\n\n-   This expression says the onditional variance can vary with $X's$.\n-   It could be as flexible as needed, but linear is usually enough.\n\nWith this the Null hypothesis is: $$H_0: a_1 = a_2 = \\dots = a_k=0 \\text{ vs } H_1: H_0 \\text{ is false}\n$$\n\nEasy enough, but do we **KNOW** $Var(e|x)$ ? can we model the equation?\n\n## \n\n**We don't!.**\n\n-   But we can use $e^2$ instead. This implies we using the assumption that $e^2$ is a good enough approximation for the condional variance $Var(e|x)$.\n\n-   With this, the test for heteroskedasticty can be implemented using the following recipe.\n\n1.  Estimate $y=x\\beta+e$ and obtain predicted model errors $\\hat e$.\n2.  Model $\\hat e^2 = \\color{green}{h(x)}+v$, as a proxy for the variance model.\n    -   $h(x)$ could be estimated using some linear or nonlinear functional forms.\n3.  Test if variance changes with respect to any explanatory variables.\n    -   In this case, you can assume the heteroskedastic model is homoskedastic.\n\n**Note:** Depending on Model specification, and test used, there are various Heteroskedasticity *tests*.\n\n## Heteroskedasticity tests\n\n$$\\begin{aligned}\n\\text{Model}: y &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + e \\\\\n\\hat e & = y - (\\hat \\beta_0 + \\hat\\beta_1 x_1 +\\hat \\beta_2 x_2 +\\hat \\beta_3 x_3)\n\\end{aligned}\n$$\n\n### Breush-Pagan:\n\n$$\\begin{aligned}\n\\hat e^2 & = \\gamma_0 + \\gamma_1 x_1 +\\gamma_2 x_2 +\\gamma_3 x_3 + v \\\\\nH_0 &: \\gamma_1=\\gamma_2=\\gamma_3=0 \\\\\nF &= \\frac{R^2_{\\hat e^2}/k}{(1-R^2_{\\hat e^2})/(n-k-1)} \\\\\nLM &=N R^2_{\\hat e^2} \\sim \\chi^2(k) \\leftarrow BP-test\n\\end{aligned}\n$$\n\n-   Easy and simple, but only considers \"linear\" Heteroskedasticity\n\n## Heteroskedasticity tests:\n\n$$\\begin{aligned}\n\\text{Model}: y &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + e \\\\\n\\hat e & = y - (\\hat \\beta_0 + \\hat\\beta_1 x_1 +\\hat \\beta_2 x_2 +\\hat \\beta_3 x_3)\n\\end{aligned}\n$$\n\n### White:\n\n$$\\begin{aligned}\n\\hat e^2 & = \\gamma_0 + \\sum \\gamma_{1,k} x_k + \\sum \\gamma_{2,k} x_k^2 + \\sum_k \\sum_{j\\neq k} \\gamma_{3,j,k} x_j x_k + v \\\\\nH_0 &: \\text{ All } \\gamma's =0 \\\\\nF &= \\frac{R^2_{\\hat e^2}/q}{(1-R^2_{\\hat e^2})/(n-q-1)} \\\\\nLM &=N R^2_{\\hat e^2} \\sim \\chi^2(q) \n\\end{aligned}\n$$\n\n$q$ is the total number of coefficients in the model (not counting the intercept.)\n\n-   Accounts for nonlinearities, but gets \"messy\" with more variables.\n\n## Heteroskedasticity tests:\n\n$$\\begin{aligned}\n\\text{Model}: y &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + e \\\\\n\\hat e & = y - (\\hat \\beta_0 + \\hat\\beta_1 x_1 +\\hat \\beta_2 x_2 +\\hat \\beta_3 x_3)\n\\end{aligned}\n$$\n\n### Modified White:\n\n$$\\begin{aligned}\n\\hat y &= y - \\hat e \\\\\n\\hat e^2 & = \\gamma_0 + \\gamma_1 \\hat y + \\gamma_2 \\hat y^2 + \\dots + v \\\\\nH_0 &: \\gamma_1 = \\gamma_2 = \\dots =0 \\\\\nF &= \\frac{R^2_{\\hat e^2}/ h }{(1-R^2_{\\hat e^2})/(n-h-1)} \\\\\nLM &=N R^2_{\\hat e^2} \\sim \\chi^2(h) \n\\end{aligned}\n$$\n\n$h$ is the total number of coefficients in the model (not counting the intercept.)\n\n-   Accounts for nonlinearities (because of how $\\hat y$ is constructed), and is simpler to implement.\n-   But, nonlinearity is restricted.\n\n## Example {.scrollable}\n\nHousing prices:\n\n$$\\begin{aligned}\nprice &= \\beta_0 + \\beta_1 lotsize + \\beta_2 sqft + \\beta_3 bdrms + e_1 \\\\\nlog(price) &= \\beta_0 + \\beta_1 log(lotsize) + \\beta_2 log(sqft) + \\beta_3 bdrms + e_2 \\\\\n\\end{aligned}\n$$\n\n::: {.cell execution_count=3}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause hprice1, clear\nreg price lotsize sqrft bdrms \npredict res, res\npredict price_hat\ngen res2=res^2\ndisplay \"BP-test\"\nreg res2  lotsize sqrft bdrms, notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\ndisplay \"White Test\"\nreg res2  c.(lotsize sqrft bdrms)##c.(lotsize sqrft bdrms), notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\ndisplay \"MWhite Test\"\nreg res2  price_hat c.price_hat#c.price_hat, notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(3, 84)        =     57.46\n       Model |  617130.701         3  205710.234   Prob > F        =    0.0000\n    Residual |  300723.805        84   3580.0453   R-squared       =    0.6724\n-------------+----------------------------------   Adj R-squared   =    0.6607\n       Total |  917854.506        87  10550.0518   Root MSE        =    59.833\n\n------------------------------------------------------------------------------\n       price | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     lotsize |   .0020677   .0006421     3.22   0.002     .0007908    .0033446\n       sqrft |   .1227782   .0132374     9.28   0.000     .0964541    .1491022\n       bdrms |   13.85252   9.010145     1.54   0.128    -4.065141    31.77018\n       _cons |  -21.77031   29.47504    -0.74   0.462    -80.38466    36.84405\n------------------------------------------------------------------------------\n(option xb assumed; fitted values)\nBP-test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(3, 84)        =      5.34\n       Model |   701213780         3   233737927   Prob > F        =    0.0020\n    Residual |  3.6775e+09        84  43780003.5   R-squared       =    0.1601\n-------------+----------------------------------   Adj R-squared   =    0.1301\n       Total |  4.3787e+09        87  50330276.7   Root MSE        =    6616.6\n\nnR^2: 14.092386\np(chi2) 0.003\nWhite Test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(9, 78)        =      5.39\n       Model |  1.6784e+09         9   186492378   Prob > F        =    0.0000\n    Residual |  2.7003e+09        78    34619265   R-squared       =    0.3833\n-------------+----------------------------------   Adj R-squared   =    0.3122\n       Total |  4.3787e+09        87  50330276.7   Root MSE        =    5883.8\n\nnR^2: 33.731659\np(chi2) 0.000\nMWhite Test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(2, 85)        =      9.64\n       Model |   809489395         2   404744697   Prob > F        =    0.0002\n    Residual |  3.5692e+09        85  41991113.9   R-squared       =    0.1849\n-------------+----------------------------------   Adj R-squared   =    0.1657\n       Total |  4.3787e+09        87  50330276.7   Root MSE        =    6480.1\n\nnR^2: 16.268416\np(chi2) 0.000\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause hprice1, clear\nreg lprice llotsize lsqrft bdrms \npredict res, res\npredict price_hat\ngen res2=res^2\ndisplay \"BP-test\"\nreg res2  llotsize lsqrft bdrms, notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\ndisplay \"White Test\"\nreg res2  c.(llotsize lsqrft bdrms)##c.(llotsize lsqrft bdrms), notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n\ndisplay \"MWhite Test\"\nreg res2  price_hat c.price_hat#c.price_hat, notable\ndisplay \"nR^2:   \" e(N)*e(r2)\ndisplay \"p(chi2) \" %5.3f chi2tail(e(df_m),e(N)*e(r2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(3, 84)        =     50.42\n       Model |  5.15504028         3  1.71834676   Prob > F        =    0.0000\n    Residual |  2.86256324        84  .034078134   R-squared       =    0.6430\n-------------+----------------------------------   Adj R-squared   =    0.6302\n       Total |  8.01760352        87  .092156362   Root MSE        =     .1846\n\n------------------------------------------------------------------------------\n      lprice | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    llotsize |   .1679667   .0382812     4.39   0.000     .0918404     .244093\n      lsqrft |   .7002324   .0928652     7.54   0.000     .5155597    .8849051\n       bdrms |   .0369584   .0275313     1.34   0.183    -.0177906    .0917074\n       _cons |  -1.297042   .6512836    -1.99   0.050    -2.592191    -.001893\n------------------------------------------------------------------------------\n(option xb assumed; fitted values)\nBP-test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(3, 84)        =      1.41\n       Model |  .022620168         3  .007540056   Prob > F        =    0.2451\n    Residual |  .448717194        84  .005341871   R-squared       =    0.0480\n-------------+----------------------------------   Adj R-squared   =    0.0140\n       Total |  .471337362        87  .005417671   Root MSE        =    .07309\n\nnR^2: 4.2232484\np(chi2) 0.238\nWhite Test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(9, 78)        =      1.05\n       Model |  .051147864         9  .005683096   Prob > F        =    0.4053\n    Residual |  .420189497        78  .005387045   R-squared       =    0.1085\n-------------+----------------------------------   Adj R-squared   =    0.0057\n       Total |  .471337362        87  .005417671   Root MSE        =     .0734\n\nnR^2: 9.5494489\np(chi2) 0.388\nMWhite Test\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(2, 85)        =      1.73\n       Model |  .018464046         2  .009232023   Prob > F        =    0.1830\n    Residual |  .452873315        85  .005327921   R-squared       =    0.0392\n-------------+----------------------------------   Adj R-squared   =    0.0166\n       Total |  .471337362        87  .005417671   Root MSE        =    .07299\n\nnR^2: 3.4472889\np(chi2) 0.178\n```\n:::\n:::\n\n\nCan you do this in `Stata`? Yes, `estat hettest`. But look into the options. There are many more options in that command.\n\n#  {background-image=\"https://i.imgflip.com/7v1kuw.jpg\" background-size=\"contain\"}\n\n## What do you do when you have Heteroskedasticity?\n\n**We need to fix!**\n\n-   Recall, the problem is that $Var(e|X)\\neq c$\n-   This affects how standard errors are estimated (we required homoskedasticity). But what happens when Homoskedasticity doesnt hold?\n    1.  We can **\"fix/change\"** the model, so its no longer heteroskedastic, and Standard Inference works. (FGLS, WLS)\n    2.  We neec to account for heteroskedasticity when estimating the variance covariance model.\n\nSo lets learn to Fix it first\n\n## How do we **Fix** Heteroskedasticity?\n\n-   In order to address the problem of heteroskedasticity, we require knowledge of why the model is heteroskedastic, or what is generating it.\n\n$$Var(e|X)=h(x)\\sigma^2_e\n$$\n\n-   Where $h(x)$ is the \"source\" of heteroskedasticity, which may be a known or estimated function of $x$.\n    -   Which should be an strictly possitive function of $x's$.\n\n## \n\n### Knowledge is power\n\n-   If you know $h(x)$, correcting heteroskedasticity is \"easy\". Consider the following:\n\n$$\\begin{aligned}\ny &= b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3 +e \\\\\nVar(e|x) &=x_1 \\sigma_e^2 || h(x)=x_1\n\\end{aligned}\n$$\n\nYou can correct Heteroskedasticity in two ways:\n\n1.  Transform model by dividing everything by $\\sqrt{h(x)}$: $$\\begin{aligned}\n    \\frac{y}{\\sqrt{x_1}} &= b_0 \\frac{1}{\\sqrt{x_1}}+ b_1 \\sqrt{x_1} + b_2 \\frac{x_2}{\\sqrt{x_1}} + b_3 \\frac{x_3}{\\sqrt{x_1}} +\\frac{e}{\\sqrt{x_1}} \\\\\n    Var\\left(\\frac{e}{\\sqrt{x_1}}|x\\right) &= \\frac{1}{x_1} x_1\\sigma_e^2=\\sigma_e^2\n    \\end{aligned}\n    $$\n\nThe new error is Homoskedastic (but has no constant)!\n\n## \n\n1.  Estimate the model using by $\\frac{1}{h(x)}$ as weights: $$\\begin{aligned}\n    \\beta=\\min_\\beta \\sum \\frac{1}{h(x)} (y-(b_0 + b_1 x_1 + b_2 x_2 + b_3 x_3))^2\n    \\end{aligned}\n    $$\n\n-   Same solution as before, and there is no need to \"transform\" data, or keep track of a constant.\n\n-   This is often called WLS (weighted least squares) or GLS (Generalized Least Squares).\n\n## \n\n-   Interestingly: These approaches are more efficient than Standard OLS.\n    -   Uses more information (heteroskedasticity)\n    -   Makes better use of information (More weight to better data) Standard errors are smaller.\n-   t-stats, F-stats, etc now are valid.\n-   Coefficients will **NOT** be the same as before.\n-   $R^2$ is less useful\n-   Heteroskedasticty test on transformed data may required added work.\n\n## \n\n### FGLS: We do not know $h(x)$, but we can guess\n\n-   If $h(x)$ is not known, we can use an auxiliary model to estimate it:\n\n$$\\begin{aligned}\nVar(e|x) &= \\sigma^2 \\exp(\\delta_0 + \\delta_1 x_1 + \\delta_2 x_2 +\\dots) \\exp v \\\\\nlog(\\hat e^2) &= \\delta_0 + \\delta_1 x_1 + \\delta_2 x_2 +\\dots+ v \\\\\nlog(\\hat e^2) &= \\delta_0 + \\delta_1 \\hat y + \\delta_2 \\hat y^2 + \\dots+ v \\\\\nh(x) &= \\widehat{log(\\hat e^2)}\n\\end{aligned}\n$$\n\n-   Proceed as before (weighted or transformed)\n-   Its call Feasible GLS, because we need to estimate $h(x)$.\n\n## Do not Correct, account for it: GLS and FGLS\n\nRecall \"Long\" variance formula:\n\n$$Var(\\beta)=\\color{brown}{(X'X)^{-1}}\\color{green}{X}'\\color{red}{Var(e|X)}\\color{green}{X}\\color{brown}{(X'X)^{-1}}\n$$\n\n- The [red]{.redtxt} part is a $N\\times N$ VCOV matrix of ALL erros. It can be Simplified with what we know!\n\n$$\\begin{aligned}\nVar_{gls/fgls}(\\beta)&=\\sigma^2_{\\tilde e} \\color{brown}{(X'X)^{-1}}\\color{green}{X}'\\color{red}{ \\Omega_h(x) }\\color{green}{X}\\color{brown}{(X'X)^{-1}} \\\\\n\\sigma^2_{\\tilde e} &= \\frac{1}{N-k-1} \\sum \\frac{\\hat e^2}{h(x)} \\\\\n\\Omega_h(x) [i,j] &= h(x_i) & \\text{ if } i=j \\\\\n& = 0 & \\text{ if } i\\neq j \\\\\n\\end{aligned}\n$$\n\n- SE are corrected, but coefficients remain the same!\n\n## Do not Correct, account for it: White Sandwich Formula\n\n- What if we do not want to even try guessing $h(x)$?\n- you can use Robust Standard errors! \n  - Heteroskedastic Consistent SE to Heterosedasticity of unknown form.\n\nLet me present to you, the Sandwitch Formula:\n$$Var(\\beta)=c \\color{brown}{(X'X)^{-1}}\\color{green}{X}'\\color{red}{\\Omega}\\color{green}{X}\\color{brown}{(X'X)^{-1}}\n$$\n\n$$\\begin{aligned}\n\\Omega [i,j] &= \\hat e_i^2 & \\text{ if } i=j \\\\\n& = 0 & \\text{ if } i\\neq j \\\\\n\\end{aligned}\n$$\n\n- The best approximation to conditional variance is equal to $\\hat e_i^2$. (plus assuming no correlation)\n\n- Valid in large samples, but can be really bad in smaller ones. \n\n- There are other versions. See HC0 HC1 HC2 HC3.\n\n## What if did $h(x)$, and it was wrong\n\n1. Using FGLS will change coefficients a bit. If they change a lot, It could indicate other assumptions in the model are incorrect. (functional form or exogeneity)\n2. In either case, you could always combine FGLS with Robust Standard Errors!\n\n## Statistical Inference\n\n- If applying GLS or FGLS via transformations or reweighting. All we did before is valid.\n- If using Robust standard errors (HC), t-stats are constructed as usual, but\n- F-stats formulas are no longer valid. \n\nInstead...use the long formula\n\n$$\\begin{aligned}\nH_0: & R_{q,k+1}\\beta_{k+1,1}=c_{q,1} \\\\\n\\Sigma_R &= R_{q,k+1} V^r_\\beta R'_{q,k+1} \\\\\nF-stat &= \\frac 1 q (R\\beta-c)' \\Sigma_R^{-1} (R\\beta-c) \n\\end{aligned}\n$$\n\n\n\n##\n### Extra: Prediction and SE\n\nPrediction SE: \n\n- If you are using GLS, Formulas seen before apply with the following modification:\n$Var(e|X=x_0)=\\sigma^2_{\\tilde e} h(x_0)$ \n- If you are using FGLS, its not that simple because of the two-step modeling \n\nFor Prediction with Logs\n\n- You need to take into account Heteroskedasticity\n\n$$\\hat y_i = \\exp \\left( \\widehat{log y_i}+\\hat \\sigma_{\\tilde e}^2 \\hat h_i /2 \\right)\n$$\n\n## Example {.scrollable}\n\n::: {.cell execution_count=5}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause smoke, clear\ngen age_40sq=(age-40)^2\n** Default\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn\nest sto m1\npredict cig_hat\npredict cig_res,res\n** GLS: h(x)=lincome Weighted\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn [aw=lincome]\nest sto m2\n** FGLS: h(x) = f(cigs_hat)\ngen lcres=log(cig_res^2)\nqui:reg lcres c.cig_hat##c.cig_hat##c.cig_hat \npredict aux\ngen hx=exp(aux)\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn [aw=hx]\nest sto m3\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn , robust\nest sto m4\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn [aw=lincome], robust\nest sto m5\nqui:reg cigs lincome lcigpric educ age age_40sq restaurn [aw=hx], robust\nest sto m6\nset linesize 255\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.stata .cell-code}\nesttab m1 m2 m3 m4 m5 m6, gaps mtitle(default GLS FGLS Rob GLS-Rob FGLS-Rob) ///\nnonum cell( b( fmt( 3) star ) se( par(( )) ) p( par([ ]) ) ) ///\nstar(* .1 ** 0.05 *** 0.01  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n------------------------------------------------------------------------------------------------------------\n                  default             GLS            FGLS             Rob         GLS-Rob        FGLS-Rob   \n                   b/se/p          b/se/p          b/se/p          b/se/p          b/se/p          b/se/p   \n------------------------------------------------------------------------------------------------------------\nlincome             0.880           0.851           1.207           0.880           0.851           1.207   \n                  (0.728)         (0.783)         (0.937)         (0.596)         (0.637)         (0.897)   \n                  [0.227]         [0.277]         [0.198]         [0.140]         [0.182]         [0.179]   \n\nlcigpric           -0.751          -0.045          -0.947          -0.751          -0.045          -0.947   \n                  (5.773)         (5.840)         (6.095)         (6.035)         (6.051)         (7.412)   \n                  [0.897]         [0.994]         [0.877]         [0.901]         [0.994]         [0.898]   \n\neduc               -0.501***       -0.526***       -0.465**        -0.501***       -0.526***       -0.465** \n                  (0.167)         (0.168)         (0.200)         (0.162)         (0.166)         (0.223)   \n                  [0.003]         [0.002]         [0.021]         [0.002]         [0.002]         [0.037]   \n\nage                 0.049           0.049           0.033           0.049*          0.049*          0.033   \n                  (0.034)         (0.034)         (0.041)         (0.030)         (0.030)         (0.040)   \n                  [0.146]         [0.144]         [0.432]         [0.099]         [0.097]         [0.413]   \n\nage_40sq           -0.009***       -0.009***       -0.010***       -0.009***       -0.009***       -0.010***\n                  (0.002)         (0.002)         (0.003)         (0.001)         (0.001)         (0.002)   \n                  [0.000]         [0.000]         [0.001]         [0.000]         [0.000]         [0.000]   \n\nrestaurn           -2.825**        -2.878**        -2.417*         -2.825***       -2.878***       -2.417*  \n                  (1.112)         (1.115)         (1.451)         (1.008)         (1.023)         (1.385)   \n                  [0.011]         [0.010]         [0.096]         [0.005]         [0.005]         [0.081]   \n\n_cons              10.797           8.521           8.866          10.797           8.521           8.866   \n                 (24.145)        (24.604)        (26.340)        (25.401)        (25.536)        (32.622)   \n                  [0.655]         [0.729]         [0.736]         [0.671]         [0.739]         [0.786]   \n------------------------------------------------------------------------------------------------------------\nN                     807             807             807             807             807             807   \n------------------------------------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n## LPM revised\n\n- What was wrong with LPM?\n  - Fixed marginal effects (depends on functional form)\n  - May predict p>1 or p<0\n  - It is Heteroskedastic by construction\n- But now we know how to deal with this! GLS (why not FGLS) and Robust\n- In LPM: $Var(y|x)=p(x)(1-p(x)) = \\hat y (1-\\hat y)$\n  - We can use this to transform or weight the data!\n  - Only works if $0<p(x)<1$.\n\n## LPM Example {.scrollable}\n\n::: {.cell execution_count=7}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause gpa1, clear\n** LPM\ngen parcoll = (fathcoll | mothcoll)\nreg pc hsgpa act parcoll\npredict res_1, res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =      1.98\n       Model |  1.40186813         3  .467289377   Prob > F        =    0.1201\n    Residual |  32.3569971       137  .236182461   R-squared       =    0.0415\n-------------+----------------------------------   Adj R-squared   =    0.0205\n       Total |  33.7588652       140  .241134752   Root MSE        =    .48599\n\n------------------------------------------------------------------------------\n          pc | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .0653943   .1372576     0.48   0.635    -.2060231    .3368118\n         act |   .0005645   .0154967     0.04   0.971    -.0300792    .0312082\n     parcoll |   .2210541    .092957     2.38   0.019      .037238    .4048702\n       _cons |  -.0004322   .4905358    -0.00   0.999     -.970433    .9695686\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.stata .cell-code code-fold=\"false\"}\npredict pchat\ngen hx = pchat*(1-pchat)\nsum pchat hx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(option xb assumed; fitted values)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n       pchat |        141    .3971631    .1000667   .1700624   .4974409\n          hx |        141    .2294822    .0309768   .1411412   .2499934\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.stata .cell-code code-fold=\"false\"}\nreg pc hsgpa act parcoll [w=1/hx]\npredict res_2, res\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(analytic weights assumed)\n(sum of wgt is 628.1830743667746)\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =      2.22\n       Model |  1.54663033         3  .515543445   Prob > F        =    0.0882\n    Residual |  31.7573194       137  .231805251   R-squared       =    0.0464\n-------------+----------------------------------   Adj R-squared   =    0.0256\n       Total |  33.3039497       140  .237885355   Root MSE        =    .48146\n\n------------------------------------------------------------------------------\n          pc | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .0327029   .1298817     0.25   0.802    -.2241292     .289535\n         act |    .004272   .0154527     0.28   0.783    -.0262847    .0348286\n     parcoll |   .2151862   .0862918     2.49   0.014       .04455    .3858224\n       _cons |   .0262099   .4766498     0.05   0.956    -.9163323    .9687521\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.stata .cell-code code-fold=\"false\"}\n** Testing for Heteroskedasticity\nreplace res_1 = res_1^2\nreplace res_2 = res_2^2/hx\ndisplay \"Default\"\nreg res_1 hsgpa act parcoll, notable\ndisplay \"Weighted\"\nreg res_2 hsgpa act parcoll, notable\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(141 real changes made)\n(141 real changes made)\nDefault\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =      2.82\n       Model |  .133163365         3  .044387788   Prob > F        =    0.0412\n    Residual |  2.15497574       137   .01572975   R-squared       =    0.0582\n-------------+----------------------------------   Adj R-squared   =    0.0376\n       Total |  2.28813911       140  .016343851   Root MSE        =    .12542\n\nWeighted\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =      0.63\n       Model |  .874194807         3  .291398269   Prob > F        =    0.5980\n    Residual |  63.5472068       137  .463848225   R-squared       =    0.0136\n-------------+----------------------------------   Adj R-squared   =   -0.0080\n       Total |  64.4214016       140  .460152868   Root MSE        =    .68106\n\n```\n:::\n:::\n\n\n# The end...for now\nNext Week: Problems of Specification\n\n",
    "supporting": [
      "6_Heteros_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}