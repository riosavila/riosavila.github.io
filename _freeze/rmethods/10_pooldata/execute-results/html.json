{
  "hash": "bb3783eb050b013044be6aef804c0fd7",
  "result": {
    "markdown": "---\ntitle: Pool Cross-section and Panel Data\nsubtitle: One year is no longer enough\nauthor: Fernando Rios-Avila\nformat:\n  revealjs:\n    slide-number: true\n    width: 1600\n    height: 900\n    code-fold: true\n    code-overflow: wrap\n    echo: true\n    css: styles.css\n    chalkboard: true\n---\n\n## Pooling Data together: Cross-section and Panel Data\n\n- Up to this point, we have cover the analysis of cross-section data. \n  - Many individuals at a single point in time.\n- Towards the end of the semester, We will also cover the analysis of time series data.\n  - A single individual across time.\n- Today, we will cover the analysis of panel data and repeated crossection: Many individuals across time.\n  \n- This type of data, also known as longitudinal data, has advantages over crossection, as it provides more information that helps dealing with the unknown of $e$.\n\n- And its often the only way to answer certain questions.\n  \n## Pooling independent crossections\n\n- We first consider the case of independent crossections. \n  - We have access to surveys that may be collected regularly. (Household budget surveys)\n  - We assume that individuals across this surveys are independent from each other (no panel structure).\n- This scenario is typically used for increasing sample-sizes and thus power of analysis (*larger N smaller SE*)\n- Only minor considerations are needed when analyzing this type of data.\n  - We need to account for the fact Data comes from different years. This can be done by including year dummies.\n  - May need to Standardize variables to make them comparable across years. (inflation adjustments, etc.)\n\n## Example {.scrollable}\n\nLets use the data `fertil1` to estimate the changes in fertility rates across time. This data comes from the *General Social Survey*.\n\n::: {.cell execution_count=1}\n``` {.stata .cell-code}\nfrause fertil1, clear\nregress kids educ age agesq black east northcen west farm othrural town smcity i.year, robust  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLinear regression                               Number of obs     =      1,129\n                                                F(17, 1111)       =      10.19\n                                                Prob > F          =     0.0000\n                                                R-squared         =     0.1295\n                                                Root MSE          =     1.5548\n\n------------------------------------------------------------------------------\n             |               Robust\n        kids | Coefficient  std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |  -.1284268    .021146    -6.07   0.000    -.1699175   -.0869362\n         age |   .5321346   .1389371     3.83   0.000     .2595258    .8047433\n       agesq |   -.005804   .0015791    -3.68   0.000    -.0089024   -.0027056\n       black |   1.075658   .2013188     5.34   0.000     .6806496    1.470666\n        east |    .217324    .127466     1.70   0.088    -.0327773    .4674252\n    northcen |    .363114   .1167013     3.11   0.002     .1341342    .5920939\n        west |   .1976032   .1626813     1.21   0.225     -.121594    .5168003\n        farm |  -.0525575   .1460837    -0.36   0.719    -.3391886    .2340736\n    othrural |  -.1628537   .1808546    -0.90   0.368    -.5177087    .1920014\n        town |   .0843532   .1284759     0.66   0.512    -.1677295    .3364359\n      smcity |   .2118791   .1539645     1.38   0.169    -.0902149    .5139731\n             |\n        year |\n         74  |   .2681825   .1875121     1.43   0.153    -.0997353    .6361003\n         76  |  -.0973795   .1999339    -0.49   0.626    -.4896701    .2949112\n         78  |  -.0686665   .1977154    -0.35   0.728    -.4566042    .3192713\n         80  |  -.0713053   .1936553    -0.37   0.713    -.4512767    .3086661\n         82  |  -.5224842   .1879305    -2.78   0.006    -.8912228   -.1537456\n         84  |  -.5451661   .1859289    -2.93   0.003    -.9099776   -.1803547\n             |\n       _cons |  -7.742457   3.070656    -2.52   0.012     -13.7674   -1.717518\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n- This allow us to see how fertility rates have changed across time.\n- One could even interact the year dummies with other variables to see how the effect of other variables have changed across time.\n\n::: {.cell execution_count=2}\n``` {.stata .cell-code}\nfrause cps78_85, clear\nregress lwage i.year##c.(educ i.female) exper expersq union, robust cformat(%5.4f)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLinear regression                               Number of obs     =      1,084\n                                                F(8, 1075)        =     110.48\n                                                Prob > F          =     0.0000\n                                                R-squared         =     0.4262\n                                                Root MSE          =      .4127\n\n------------------------------------------------------------------------------\n             |               Robust\n       lwage | Coefficient  std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     85.year |     0.1178     0.1239     0.95   0.342      -0.1253      0.3609\n        educ |     0.0747     0.0060    12.40   0.000       0.0629      0.0865\n    1.female |    -0.3167     0.0347    -9.12   0.000      -0.3848     -0.2486\n             |\n year#c.educ |\n         85  |     0.0185     0.0095     1.94   0.053      -0.0002      0.0371\n             |\n year#female |\n       85 1  |     0.0851     0.0518     1.64   0.101      -0.0165      0.1866\n             |\n       exper |     0.0296     0.0037     8.10   0.000       0.0224      0.0368\n     expersq |    -0.0004     0.0001    -5.11   0.000      -0.0006     -0.0002\n       union |     0.2021     0.0293     6.89   0.000       0.1446      0.2597\n       _cons |     0.4589     0.0855     5.37   0.000       0.2911      0.6267\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n## Good old Friend: Chow test\n\n- The Chow test can be used to test whether the coefficients of a regression model are the same across two groups. \n  - we have seen this test back when we were discussing dummy variables.\n- We can also use this test to check if coefficients of a regression model are the same across two time periods. (Has the wage structure changed across time?)\n  - This is the case of interest here.\n- Not much changes with before. Although it can be a bit more tedious to code.\n\n## Example {.scrollable}\n\n::: {.cell execution_count=3}\n``` {.stata .cell-code}\nfrause cps78_85, clear\nregress lwage i.year##c.(educ i.female exper expersq i.union), robust\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLinear regression                               Number of obs     =      1,084\n                                                F(11, 1072)       =      82.83\n                                                Prob > F          =     0.0000\n                                                R-squared         =     0.4276\n                                                Root MSE          =     .41278\n\n------------------------------------------------------------------------------\n             |               Robust\n       lwage | Coefficient  std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     85.year |   .1219978   .1521927     0.80   0.423    -.1766315    .4206271\n        educ |   .0768148   .0063312    12.13   0.000     .0643918    .0892378\n    1.female |  -.3155108   .0348402    -9.06   0.000    -.3838737    -.247148\n       exper |   .0249177   .0042985     5.80   0.000     .0164833    .0333522\n     expersq |  -.0002844   .0000918    -3.10   0.002    -.0004645   -.0001043\n     1.union |   .2039824   .0381315     5.35   0.000     .1291616    .2788033\n             |\n year#c.educ |\n         85  |    .013927   .0103252     1.35   0.178    -.0063329    .0341869\n             |\n year#female |\n       85 1  |   .0846136   .0524618     1.61   0.107    -.0183258     .187553\n             |\nyear#c.exper |\n         85  |   .0095289   .0073767     1.29   0.197    -.0049454    .0240033\n             |\n        year#|\n   c.expersq |\n         85  |  -.0002399   .0001592    -1.51   0.132    -.0005522    .0000724\n             |\n  year#union |\n       85 1  |  -.0018095   .0594387    -0.03   0.976    -.1184389      .11482\n             |\n       _cons |    .458257     .09386     4.88   0.000     .2740868    .6424271\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.stata .cell-code code-fold=\"false\"}\ntest 85.year#c.educ 85.year#1.female 85.year#c.exper   85.year#c.expersq 85.year#1.union\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n ( 1)  85.year#c.educ = 0\n ( 2)  85.year#1.female = 0\n ( 3)  85.year#c.exper = 0\n ( 4)  85.year#c.expersq = 0\n ( 5)  85.year#1.union = 0\n\n       F(  5,  1072) =    1.65\n            Prob > F =    0.1443\n```\n:::\n:::\n\n\n## Using Pool Crossection for Causal Inference\n\n- One advantage of pooling crossection data is that it could to be used to estimate causal effects using a method known as Differences in Differences (DnD)\n\n- Consider the following case:\n  - There was a project regarding the construction of an incinerator in a city. You are asked to evaluate what the impact of this was on the prices of houses around the area. \n  - You have access to data for two years: 1978 and 1981.\n  - In 1978, there was no information about the project. In 1981, the project was announced, but it only began operations in 1985.\n\n##\n\n- we could start estimating the project using the simple model:\n$$rprice = \\beta_0 + \\beta_1 nearinc + e$$\n\nusing only 1981 data. But this would not be a good idea. Why?\n\n::: {.cell .larger execution_count=5}\n``` {.stata .cell-code}\nfrause kielmc, clear\nregress rprice nearinc if year == 1981, robust\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLinear regression                               Number of obs     =        142\n                                                F(1, 140)         =      24.35\n                                                Prob > F          =     0.0000\n                                                R-squared         =     0.1653\n                                                Root MSE          =      31238\n\n------------------------------------------------------------------------------\n             |               Robust\n      rprice | Coefficient  std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     nearinc |  -30688.27   6219.265    -4.93   0.000     -42984.1   -18392.45\n       _cons |   101307.5   2951.195    34.33   0.000     95472.84    107142.2\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n## \n\n- We could also estimate the model using only 1971 data.\n  What would this be showing us?\n\n::: {.cell .larger execution_count=6}\n``` {.stata .cell-code}\nregress rprice nearinc if year == 1978, robust\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLinear regression                               Number of obs     =        179\n                                                F(1, 177)         =       9.87\n                                                Prob > F          =     0.0020\n                                                R-squared         =     0.0817\n                                                Root MSE          =      29432\n\n------------------------------------------------------------------------------\n             |               Robust\n      rprice | Coefficient  std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     nearinc |  -18824.37   5992.564    -3.14   0.002    -30650.44   -6998.302\n       _cons |   82517.23   1881.165    43.86   0.000     78804.83    86229.63\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n##\n\n- So, using 1981 data we capture the Total price difference between houses near and far from the incinerator. \n  - This captures both the announcement effect of the project, but also other factors (where would an incinerator be built?).\n- Using 1978 data we capture the price difference between houses near and far from the incinerator in the absence of the project. \n  - This captures the effect of other factors that may be correlated with the incinerator project.\n- Use both to see the impact!\n\n$$Effect = -30688.27-(-18824.37)= -11863.9$$\n\n- This is in essence a DnD model\n\n## Difference in Differences\n\n\n|  | Control| Treatment | Treat-Control |\n|---|---|---|---|\n| Pre-            | $\\bar y_{00}$ | $\\bar y_{10}$| $\\bar y_{10}$-$\\bar y_{00}$ |\n| Post-           | $\\bar y_{01}$ | $\\bar y_{11}$ | $\\bar y_{10}$-$\\bar y_{00}$ |\n| Post-pre        | $\\bar y_{01}$-$\\bar y_{00}$ | $\\bar y_{11}$-$\\bar y_{10}$ | DD  |\n  \n- Post-Pre: \n  - Trend changes for the control\n  - Trend changes for the treated: A mix of the impact of the treatment and the trend change.\n- Treat-Control: \n  - Baseline difference when looking at Pre-period\n  - Total Price differentials when looking at Post-period: Mix of the impact of the treatment and the baseline difference.\n\n- Take the Double Difference and you get the **treatment effect**.\n\n## Difference in Differences: Regression {.scrollable}\n\n- This could also be achieved using a regression model:\n\n$$ y = \\beta_0 + \\beta_1 post + \\beta_2 treat + \\beta_3 post*treat + e$$\n\nWhere $\\beta_3$ is the treatment effect. (only for 2x2 DD)\n\n::: {.cell execution_count=7}\n``` {.stata .cell-code}\nregress rprice nearinc##y81, robust\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLinear regression                               Number of obs     =        321\n                                                F(3, 317)         =      17.75\n                                                Prob > F          =     0.0000\n                                                R-squared         =     0.1739\n                                                Root MSE          =      30243\n\n------------------------------------------------------------------------------\n             |               Robust\n      rprice | Coefficient  std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n   1.nearinc |  -18824.37    5996.47    -3.14   0.002    -30622.28   -7026.461\n       1.y81 |   18790.29   3498.376     5.37   0.000     11907.32    25673.26\n             |\n nearinc#y81 |\n        1 1  |   -11863.9   8635.585    -1.37   0.170    -28854.21    5126.401\n             |\n       _cons |   82517.23   1882.391    43.84   0.000     78813.67    86220.79\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n## Difference in Differences: Regression + controls \n\n- One advantage of DD is that it can control for those unobserved factors that may be correlated with outcome. \n  - Without controls, however, estimates may not have enough precision.\n- But, we could add controls!\n\n$$ y = \\beta_0 + X \\gamma + \\beta_1 post + \\beta_2 treat + \\beta_3 post*treat + e$$\n\n- But its not as easy as it may seem! (just adding regressions is not a good approach)\n\n- This method requires other assumptions! ($\\gamma$ is fixed), which may be very strong.\n\n\n>[**Note:**]{.redtxt} For DD to work, you need to assume the two groups follow the same path in the absence of the treatment. (Parallel trends assumption)\n>\n>Otherwise, you are just using trend differences!\n\n## Diff in Diff in Diff\n\nAn Alternative approach is to use a triple difference model.\n\nSetup:\n\n- You still have two groups: Control and Treatment (which are easily identifiable)\n- You have two time periods: Pre and Post (which are also easily identifiable)\n- You have a different sample, where you can identify controls and treatment, as well as the pre- and post- periods. This sample was not treated!\n\nEstimation: \n\n- Estimate the DD for the Original Sample, and the new untreated sample. \n- Obtaining the difference between these two estimates will give you the triple difference.\n\nExample: Smoking ban analysis based on age. (DD) But using both treated and untreated States (DDD)\n\n## General Framework and Pseudo Panels\n\n- One general Structure for Policy analysis is the use of Pseudo Panels structure.\n  - Pseudo panels are a way to use repeated crossection data, but controlling for some unobserved heterogeneity across specific groups. (the pseudo panels)\n- For Pseudo-panels, we need to identify a group that could be followed across time. \n  - This cannot be a group of individuals (repeated crosection). \n  - But we could use groups of states, cohorts (year of birth), etc.\n- In this case, the data would look like this:\n$$y_{igt} = \\lambda_t + \\alpha_g + \\beta x_{gt} + z_{igt}\\gamma +  e_{igt}$$\n\n- Where $g$ is the group, $t$ is the time, and $i$ is the individual.\n- This model can be estimated by using dummies. (one dummy for each group and time-period)\n- And $\\beta$ is the coefficient of interest. (impact of the Policy $x_{gt}$).\n  - This may ony work if we assume $\\beta$ is constant across time and groups.\n\n## Alternative\n\n- We could also use a more general model:\n$$y_{igt} = \\lambda_{gt}+ \\beta x_{gt} + z_{igt}\\gamma +  e_{igt}$$\n\n- where $\\lambda_{gt}$ is a group-time fixed effect. (Dummy for each group-time combination) \n  - Nevertheless, while more flexible, this also imposes other types of assumptions, and might even be unfeasible if we have a large number of groups and time periods.\n\n- Still, we require $\\beta$ to be homogenous. If that is not the case, you may still suffer from contamination bias.\n\n# Panel data\nBaby steps: 2 period panel data\n\n## 2-period Panel data\n\n- Panel Data, or longitudinal data, is a type of data that has information about the same individual across time. \n\n- The simplest Structure is one where individuals are followed over only 2 periods. \n  \n- The main advantage of panel data (even two periods version) is that it allows us to control for unobserved heterogeneity across individuals.\n  - But only if you want to assume fixed effects are constant across time.\n\n## \n\n- So how does this reflects in the model specification?\n\n$$y_{it} = \\beta_0 + \\beta_1 x_{it} + \\beta_2 z_{t} + \\beta_3 w_{i} + e_i + e_t + e_{it}$$\n\n- Where $i$ refers to individuals or panel units, and $t$ refers to time periods.\n- Also, $X's$, $X's$ $W's$ are variables that vary across individual and time, across time or across individuals.\n- There are also three types of errors. Those that contains unobserved that vary across individuals $e_i$, across time $e_t$, and across individuals and time $e_{it}$ (Idiosyncratic error).\n  \n- $e_i$ is usually referred to as the individual fixed effect, and $e_t$ as the time fixed effect.\n\n- In a 2 period panel, controlling for time-effects is may not be necessary (its just one dummy)\n\n- What is more concerning is the unobserved individual fixed effect. \n \nThis is pretty similar to the generalized Pooling model we saw before.\n\n## How estimation changes\n\nFor time use, we assume we control with a single dummy.\n\n1. You can choose to \"ignore\" individual effects. \n\n$$y_{it} = \\beta_0 + \\beta_1 x_{it} + \\beta_2 w_{i} + \\delta t + \\underbrace{e_i + e_{it}}_{v_{it}}$$\n\n   - Requires $e_i$ to be uncorrelated with $x_{it}$ (otherwise is biased), and Standard Errors will need to be clustered at the individual level.\n\n2. You can aim to estimate **all** individual fixed effects using dummies (FE estimator).\n$$y_{it} = \\beta_0 + \\beta_1 x_{it} + \\delta t + \\sum \\alpha_i D_i + e_{it})$$\n\n   - Time fixed variables cannot be estimated anymore\n  \n## \n\n3. You can estimate the model in differences (FD estimator)\n\n$$\\begin{aligned}\ny_{i1} &= \\beta_0 + \\beta_1 x_{i1} + \\delta + e_i + e_{i1} \\\\\ny_{i0} &= \\beta_0 + \\beta_1 x_{i0} + e_i + e_{i0} \\\\\n\\Delta y_{i} &= \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta_1 \\Delta x_{i1} + \\delta + \\Delta e_{i}\n\\end{aligned}\n$$\n\n- Now you have only 1 observation per panel, instead of 2. And the result would be identical to FE estimator.\n\n## Example {.scrollable}\n\n::: {.cell execution_count=8}\n``` {.stata .cell-code}\n** This data is in wide format\nfrause slp75_81, clear\n** Lets reshape it so its in standard long format\nqui:gen id = _n\nqui:reshape long educ gdhlth marr slpnap totwrk yngkid, i(id) j(year)\nqui:gen dtime = year==81\nqui:xtset id dtime\n** Regression as Pool Crossection\nqui: reg slpnap dtime totwrk educ marr yngkid gdhlth male,  \nlocal cname:colnames e(b)\ndisplay \"`cname'\"\nest sto m1\n** using FE\nqui: areg slpnap dtime totwrk educ marr yngkid gdhlth male, absorb(id)  \nest sto m2\n** using FD\nqui: reg d.slpnap d.dtime d.totwrk d.educ d.marr d.yngkid d.gdhlth d.male, \nmatrix b = e(b)\nmatrix V = e(V)\nmatrix colname b = `cname'\nmatrix colname V = `cname'\nmatrix rowname V = `cname'\nadde repost b=b V=V, rename\nest sto m3\nesttab m1 m2 m3, se r2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndtime totwrk educ marr yngkid gdhlth male _cons\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                   slpnap          slpnap        D.slpnap   \n------------------------------------------------------------\ndtime              -79.28          -92.63*              0   \n                  (48.04)         (45.87)             (.)   \n\ntotwrk             -0.156***       -0.227***       -0.227***\n                 (0.0254)        (0.0361)        (0.0361)   \n\neduc               -21.09**       -0.0245         -0.0245   \n                  (8.057)         (48.76)         (48.76)   \n\nmarr               -51.38           104.2           104.2   \n                  (54.07)         (92.86)         (92.86)   \n\nyngkid              69.78           94.67           94.67   \n                  (77.50)         (87.65)         (87.65)   \n\ngdhlth              27.03           87.58           87.58   \n                  (59.63)         (76.60)         (76.60)   \n\nmale                45.53               0               0   \n                  (50.93)             (.)             (.)   \n\n_cons              3965.5***       3696.6***       -92.63*  \n                  (122.0)         (664.5)         (45.87)   \n------------------------------------------------------------\nN                     478             478             239   \nR-sq                0.099           0.682           0.150   \n------------------------------------------------------------\nStandard errors in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n:::\n:::\n\n\n## Nx2 DID with Panel \n\n- Using panel data also allows you to analyze policies using canonical DID approach (2x2).\n  - It simplifies the process because one of the differences can be estiamated right away.\n- Consider the Standard DID Model:\n$$y_{it} = a_0 + a_1 post + \\color{red}{a_2 treat} + \\color{green}{a_3 post \\times treat}+ e_{it} $$\n\n- With panel data, you can extend this allowing for \"multiple groups\", and controling for whether Received treatment or not: \n\n$$y_{it} = a_0 + a_1 dtime + \\color{red}{a_i} + \\color{green}{a_3 treated} + e_{it} $$\n$$\\Delta y_{i} =  a_1 + \\color{green}{a_3 treated} + \\Delta e_{i} $$\n\n- Most effective way to control for unobserved individual effects!\n  \n##\n\n:::{.callout-warning}\n\n## Beware of the TWFE\n\n**warning** you may be tempted to say, with more time-periods, just add more dummies. But that would be wrong!\n\nLook for all the new literature on DID with Multiple periods and treatment timing\n\n:::\n\n## Panel Data with more than 2 periods\n\n- So far we have covered the case when you observe individuals for two periods only. What if you observe them for more than two periods?\n\n- The model is pretty much the same.\n  - You will mostly need to add an additional time dummy.\n  - You also have multiple approaches that would allow you to estimate the model.\n  - Adding Dummies still work.\n  - And Difference in Differences still work.\n  - You can also choose Randome-effect model (next class)\n\n## Differencing\n\n- Consider the following model:\n\n$$y_{it} = \\beta_0 + \\beta_1 x_{it} +a_i + \\delta_2 T_2 + \\delta_3 T_3 + e_{it}$$\n\n- Where $T_2$ and $T_3$ are time dummies for the second and third periods. If apply a first differences transformation, we get:\n\n$$\\Delta y_{it} = \\beta_1 \\Delta x_{it} + \\delta_2 + \\delta_3 T_3 + \\Delta e_{it}$$\n\n## \n\n- Mechanically, its the same as before. However, Analytically, this imposes stronger assumptions\n\n$$Corr(\\Delta x_{it}, \\Delta e_{it}) =Corr(x_{it}-x_{it-1}, e_{it}-e_{it-1}) = 0$$\n\n- This is a stronger assumption than $Corr(x_{it}, e_{it}) = 0$. It implies that $x_{it}$ has to be strictly exogenous to errors $e_{it}$, regardless of timing.\n\n- One may want to also consider using Other Standard error corrections.\n\n## FD is to be used with caution\n\n- FD is an easy estiamtion with panel data. However, it should be used with caution\n  - It depends much more on the strict exogeneity assumption. (may create biases)\n  - If one adds Lagged Dep variables as controls, further problems may arise. \n  - Issues with measurement error are magnified when using First Differences as well. (and can be difficult to adress)\n\n- But, there are still other methods that could be used to estimate panel data models with multiple periods:\n  - FE (dummy inclusion approach)\n  - Random Effects (next class)\n  - Correlated Random Effects (next class)\n\n# {background-image=\"https://i.imgflip.com/7zjmz3.jpg\" background-size=\"contain\"}\n\n# Thats all for today \nNext class...Advance Panel Data Methods\n\n",
    "supporting": [
      "10_pooldata_files"
    ],
    "filters": [],
    "includes": {}
  }
}