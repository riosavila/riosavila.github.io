{
  "hash": "e0d7df26a4ee98c606c01bd6ce9f6d17",
  "result": {
    "markdown": "---\ntitle: Simple Regression Model\ntitle-slide-attributes:\n  data-background-image: images/paste-2.png\n  data-background-size: contain\n  data-background-opacity: '0.5'\nsubtitle: The first tool of Many\nauthor: Fernando Rios-Avila\nformat:\n  revealjs:\n    slide-number: true\n    width: 1500\n    height: 900\n    code-fold: true\n    echo: true\n    css: styles.css\n---\n\n## The Simple Regression Model\n\n-   As we saw in the previous slides, one of the important steps when doing empirical analysis is to develop a model that describes reality.\n\n-   This model is quite abstract, as it rarely provides guidance regarding on How should you build your econometric model.\n\n-   In this chapter, we introduce the first (boring) tool to solve this problem. The simple Regression model or SRL\n\n## What is a Simple Regression Model (SRM) ?\n\n-   A Simple regression model is known as such because it aims to capture the relationship between **two** variables.\n\n-   It does not mean it ignores other factors, but rather, bundles them together as part of a **Bag of Holding** or error. In its most flexible setup, a simple regression model can be written as:\n\n$$y = f(x,u)\n$$\n\nThis model simply says that there is some relationship between:\n\n-   $y$, your outcome, dependent, explained, response, variable\n\n-   and $x$, your independent, explanatory, regression, variable\n\nwhereas everything else not considered is assumed to be part of the unobserved $u$.\n\n## From Abstract to Concrete\n\n-   A good reason why one should start thinking about the model as shown earlier is to acknowledge that we **Do not know** the functional form between $x$ and $y$.\n\n-   Further, we don't even know how $u$ interacts with $x$.\n\nThis brings us to the first step one should do (almost always) when analyzing data...Create a plot to see if there is any relationship in the data\n\n## Simple Scatter 1\n\n::: {.cell execution_count=1}\n``` {.stata .cell-code}\n** To download all Wooldrige Files\nqui: ssc install frause, replace\n** for some additional color schemes\nqui: ssc install color_style\nset scheme white2\ncolor_style tableau\n** Loads file wage1\nfrause wage1, clear\nscatter wage educ\n```\n\n::: {.cell-output .cell-output-display}\n![](2_SRA_files/figure-revealjs/cell-2-output-1.png){fig-align='center'}\n:::\n:::\n\n\n-   We observe a positive relationship between Wages and years of education\n-   This relationship does not seem to be linear\n\n## Simple Scatter 2\n\n![](images/paste-3.png){fig-align=\"center\"}\n\n## Even more Concrete\n\n-   This first \"model\" provides little guidance for the modeling itself.\n-   The Simple Linear Regression Model corrects for that, establishing a specific relationship between the variables of interest and the error: $$y = \\beta_0 + \\beta_1 x + u$$\n\nThis model has a lot packed in.\n\n-   It imposes a relationship between $y$ and $x$ (linear)\n-   And addresses the fact that there could be other factors not considered $u$. Impossing the assumption they are additive errors.\n\nIt also assumes the population relationships: $$E(y|x) = \\beta_0 + \\beta_1 x$$\n\n## What can we learn from it?\n\n$$E(y|x) = \\beta_0 + \\beta_1 x$$\n\nThis is your Population Regresson function. To interpret it, we need to assume $u$ is fixed (*ceteris paribus*). This implies that $$E(u|x)=c=0$$\n\nWhich says that the errors are **mean independent** of $x$. Thus, for all practical purposes, when $x$ changes, we will assume $u$ is as good as fixed.\n\nUnder these conditions, we can interpret the coefficients:\n\n-   $\\beta_0$ is the constant, or expected outcome when $x=0$.\n-   $\\beta_1$ is the slope of $x$, or the expected change in $y$ when $x$ changes in 1 unit:\n\n$$\\Delta y = \\beta_1 \\Delta x \\rightarrow \\frac{\\Delta y}{\\Delta x} = \\beta_1\n$$\n\n## Example\n\n-   Soybean and Yield Fertilizer:\n\n$$yield = \\beta_0 + \\beta_1 fertilizer + u$$\n\n$\\beta_1$ Effect of Fertilizer (an additional dosage) on Soybean Yield\n\n-   Simple wage equation\n\n$$wage = \\beta_0 + \\beta_1 educ + u\n$$\n\n$\\beta_1$ Change in wages given an additional year of education.\n\n## Deriving Coefficients: Ordinary Least Squares - OLS\n\n-   There are an infinite number of candiates for $\\beta_0 \\& \\beta_1$.\n\n-   OLS, is **one** of the multiple methods that allows us to estimate the coefficients of a SLRM[^1].\n\n-   The goal is to Choose parameters $\\beta={\\beta_0,\\beta_1}$ that \"minimizes\" the Squared of the residuals.\n\n    -   In other words, OLS aims to maximize Explantion power by minimizing errors.\n\n[^1]: Simple Linear Regression Model\n\n## Visualization\n\n::: {.cell execution_count=2}\n``` {.stata .cell-code}\nset seed 10\nclear\nrange x -2 2 20\ngen y = 1 + x + rnormal()\ncolor_style tableau\ntwo (scatter  y x) ///\n    (function y = 0.5 + 2*x, range(-1.8 2.4)) ///\n    (function y = 2 + 0.5*x, range(-1.8 2.4)) ///\n    (function y = 1 + 1*x, range(-1.8 2.4)) , ///\n    legend(order(2 \"y=0.5+2x\" 3 \"y=2+0.5x\" 4 \"y=1+1x\"))\ngraph export images/fig2_1.png , replace width(1000)\n\ngen y1=.5+2*x\ngen y2=2+0.5*x\ngen y3=+1+1*x\n\ntwo (scatter y x) ///\n    (function y = 0.5 + 2*x, range(-1.8 2.4)) ///\n    (rspike y y1 x), ylabel(-4/6) ytitle(y) ///\n    legend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))\ngraph export images/fig2_2.png , replace width(1000) \n    \ntwo (scatter y x) ///\n    (function y = 2 + 0.5*x, range(-1.8 2.4)) ///\n    (rspike y y2 x), ylabel(-4/6) ytitle(y) ///\n    legend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))\ngraph export images/fig2_3.png , replace width(1000) \n    \ntwo (scatter y x) ///\n    (function y = 1 + 1*x, range(-1.8 2.4)) ///\n    (rspike y y3 x) , ylabel(-4/6) ytitle(y) ///\n    legend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))\ngraph export images/fig2_4.png , replace width(1000)    \n\nreg y x\npredict yh\ntwo (scatter y x) ///\n    (function y = _b[_cons] + _b[x]*x, range(-1.8 2.4)) ///\n    (rspike y yh x) , ylabel(-4/6) ytitle(y) ///\n    legend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))    \ngraph export images/fig2_5.png , replace width(1000)    \n```\n:::\n\n\n::: panel-tabset\n## Options\n\n![](images/fig2_1.png)\n\n## Opt1\n\n![](images/fig2_2.png)\n\n## Opt2\n\n![](images/fig2_3.png)\n\n## Opt3\n\n![](images/fig2_4.png)\n\n## Opt4\n\n![](images/fig2_5.png)\n:::\n\n## Just a Minimization Problem\n\n$$y_i =\\beta_0 + \\beta_1 x_i + u_i \\rightarrow u_i = y_i - \\beta_0 - \\beta_1 x_i \n$$\n\n$${\\hat\\beta_0,\\hat\\beta_1} = \\min_{\\beta_0,\\beta_1} = SSR =\\sum_{i=1}^N u_i^2 = \\sum_{i=1}^N (y-\\beta_0 - \\beta_1 x_i)^2 \\\\\n$$\n\nFirst Order Conditions: $$\n\\begin{aligned}\n\\frac{\\partial SSR}{\\partial \\beta_0} &= -2 \\sum (y_i-\\beta_0 - \\beta_1 x_i) = -2 \\sum u_i =0 \\\\\n\\frac{\\partial SSR}{\\partial \\beta_1} &= -2 \\sum x_i (y_i-\\beta_0 - \\beta_1 x_i) =- 2 \\sum x_i u_i =0\n\\end{aligned}\n$$\n\n## Just a Minimization Problem\n\nSimilar conditions as before (but now Mathematically):\n\n$$\n\\begin{aligned} \n\\sum u_i &=0 \\rightarrow nE(e) = 0 \\\\ \n\\sum x_i u_i &=0 \\rightarrow nE(x*e) \\rightarrow  n Cov(x,e) =0  \n\\end{aligned}\n$$\n\nAnd the First Order Conditions simply provide a system of $k+1$ equations with $k+1$ unknowns.\n\n$$\\begin{aligned}\n\\hat\\beta_0 &= \\bar y - \\beta_1 \\bar x \\\\\n\\hat\\beta_1 &= \\frac{\\sum (x_i-\\bar x)(y_i-\\bar y)}{\\sum (x_i-\\bar x)^2} \n= \\frac{\\hat \\rho \\hat \\sigma_x \\hat \\sigma_y}{\\hat \\sigma_x^2} \n= \\frac{\\hat \\rho \\hat \\sigma_y}{\\hat \\sigma_y}\n\\end{aligned}\n$$\n\n## Interpretation?\n\n$$\\hat\\beta_0 = \\bar y - \\hat \\beta_1 \\bar x$$\n\n-   $\\beta_0$ is usually estimated as a \"residual\", thus is often of little of no interest.\n    -   Expected outcome when $X=0$\n\n$$\n\\hat\\beta_1 = \\frac {cov(x,y)}{var(x)}= \\hat \\rho\\frac{ \\hat \\sigma_x \\hat \\sigma_y}{\\hat \\sigma_x^2} \n= \\hat \\rho\\frac{ \\hat \\sigma_y}{\\hat \\sigma_x}\n$$\n\n-   $\\beta_1$ is a slope, which is directly related to the correlation between $y$ and $x$.\n    -   It can only be estimated if $\\sigma_x$\\>\\>0\n\nAlso, this $\\hat y = \\hat \\beta_0 + \\hat \\beta_1 x$ becomes your sample regression function\n\n-   where $\\hat y$ is the fitted value of $y$ (proyection or prediction), given some value of $x$.\n\n## Visualization\n\n::: {.cell execution_count=3}\n``` {.stata .cell-code}\ngen y0 = 0\ntwo (scatter y x) ///\n    (function y = _b[_cons] + _b[x]*x, range(-2.1 2.1)) ///\n    (rspike y y0 x, color(gs9%50) lw(1)) ///\n    , ylabel(-4/6) ytitle(y) yline(0) ///\n    title(\"Data\")   legend(off)\ngraph export images/fig2_6.png, replace width(1000)\ntwo (scatter y x) ///\n    (function y = _b[_cons] + _b[x]*x, range(-2.1 2.1)) ///\n    (rspike yh y0 x, color(gs9%50) lw(1)) ///\n    , ylabel(-4/6) ytitle(y) yline(0) ///\n    title(\"Prediction\") legend(off)\ngraph export images/fig2_7.png, replace width(1000) \ntwo (scatter y x) ///\n    (function y = _b[_cons] + _b[x]*x, range(-2.1 2.1)) ///\n    (rspike y yh x, color(gs9%50) lw(1)) ///\n    , ylabel(-4/6) ytitle(y) yline(0) ///\n    title(\"Residual\")   legend(off)\ngraph export images/fig2_8.png, replace width(1000) \n```\n:::\n\n\n::: panel-tabset\n## Data\n\n![](images/fig2_6.png)\n\n## Prediction\n\n![](images/fig2_7.png)\n\n## Residuals\n\n![](images/fig2_8.png)\n:::\n\n## Properties of the Estimator\n\n1.  Based on F.O.C., we know the following:\n\n$$\n\\sum_i^n \\hat u_i = 0 \\ \\& \\ \\sum_i^n x_i \\hat u_i = 0\n$$\n\nIn average $u_i$ is zero, and uncorrelated with $x$, and $\\bar y , \\bar x$ **is** on the regression line\n\n2.  By construction $y_i = \\hat y_i + \\hat u_i$, so that\n\n$$\n\\begin{aligned}\n    \\sum_{i=1}^n(y_i-\\bar y)^2 &= \n    \\sum_{i=1}^n(y_i-\\hat y)^2  + \n    \\sum_{i=1}^n(\\hat y-\\bar y)^2  \\\\\n    SST &= SSE + SSR\n\\end{aligned}\n$$\n\n## Properties of the Estimator\n\n4.  Goodness of FIT is defined as\n\n$$R^2= 1-\\frac {SSR} {SST}=\\frac {SSE} {SST}$$\n\n-   How much of the Data variation (SST) is explained by the model (SSE), or\n-   1-amount not explained by the model\n\n## Some Discussion\n\nWe now know how to estimate coefficients given some data, but we need to ask the questions:\n\n-   How do we know if the estimated coefficients are indeed appropriate for the population parameters?\n\n-   How can we know the precision (or lack there off) of the estimates\n\n-   Remember, $\\hat \\beta's$ depend on the sample. Different Samples will lead to different estimates. Thus $\\hat \\beta's$ are random.\n\n-   In repeated sampling scenarios, we could empirically obtain the distribution of the estimated parameters, and verify if estimations are unbiased.\n\n-   However, we can also do that based on analytical solutions. Lets see those assumptions\n\n# Assumptions: For unbiased estimations\n\nAn estimator is said to be unbiased if $E(\\hat\\beta)=\\beta$\n\n## Assumption 1:\n\n::: callout-important\n## Linear in Parameters\n\nWe need to assume the population model is linear in parameters:\n\n$$y_i = \\beta_0 + \\beta_1 x_i + u_i$$\n:::\n\nIn other words, we need to assume that the model we chose is a good representation of what the true population model is.\n\n-   Additive error, with a linear relationship between $x_i$ on $y_i$.\n\n-   We can make it more flexible using some transformations of $x_i$.\n\n## Assumption 2:\n\n::: callout-important\n## Random Sampling\n\nThe data we are using is collected from a Random sample of the population, for which the linear model is valid.\n:::\n\n-   Data should be representative from the population (for whom the Linear model Holds)\n-   The Data Sampling should not depend the data collected, specially the dependent variable.\n-   Also helps to ensure units \"unobservables\" $u's$ are independent from each other.\n\n## Assumption 3:\n\n::: callout-important\n## There is variation in the explanatory variable\n\n$$\\sum_{i=1}^n (x_i - \\bar x)^2 >0\n$$\n:::\n\nIf there is no variation in the data, there are no slopes to estmate, and a solution cannot be found to the linear model.\n\n## Assumption 4:\n\n::: callout-important\n## Zero Conditional Mean\n\n$$E(u_i)= E(u_i|x_i) = 0\n$$\n:::\n\n-   We expect unobserved factors $u_i$ to have a zero average effect on the outcome. This helps identify the constant $\\beta_0$.\n\n-   We also expect that the expected value of $u_i$ to be zero for any value of $x$.\n\n## Unbiased Coefficients:\n\nIf Assumptions 1-4 Hold, then OLS allows you to estimate the coefficents of the linear Regression model.\n\n$$\\hat \\beta_1 = \\frac {\\sum \\tilde x_i \\tilde y_i}{\\sum \\tilde x_i^2} , \\tilde x_i=x_i - \\bar x\n$$\n\n$$\\begin{aligned}\n\\hat \\beta_1 &= \\frac {\\sum \\tilde x_i (\\beta_1 \\tilde x_i +e)}{\\sum \\tilde x_i^2} = \\beta_1 \\frac {\\sum  \\tilde x_i^2 }{\\sum \\tilde x_i^2} + \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2}  \\\\  \nE(\\hat \\beta_1) &= \\beta_1\n\\end{aligned}\n$$\n\nWhile coefficients can be different for each sample, In average, they will be the same as the true parameters.\n\n## Variance of OLS Estimators\n\nHow precise are the estimates?\n\n$$\\hat \\beta_1 = \\beta_1 + \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2}\n$$\n\n-   If we assume $x's$ are assume fixed, the distribution from $\\beta's$ will depend only on the variation of the error $u_i$.\n\n-   Thus we need to impose an additional assumption on this errors, to estimate the variance of $\\beta's$. (at least for convinience)\n\n## Assumption 5:\n\n::: callout-important\n## Errors are Homoskedastic\n\n$$E(u_i^2)= E(u_i ^2 | x_i) = \\sigma_u ^2\n$$\n:::\n\n-   This simplifying assumption states that the \"distribution\" of the errors is constant, regardless of $x$.\n\n## Visualization\n\n::: {.cell execution_count=4}\n``` {.stata .cell-code}\nclear\nset scheme white2\nset obs 1000\ngen x = runiform(-2 , 2)    \ngen u = rnormal()\ngen y1 = x + u\ngen y2 = x + u*abs(x)\ngen y3 = x + u*(2-abs(x))\ngen y4 = x + u*(sin(2*x))\n\n\ntwo scatter y1 x, name(m1, replace)\nscatter y2 x, name(m2, replace)\nscatter y3 x, name(m3, replace)\nscatter y4 x, name(m4, replace)\ngraph combine m1 m2 m3 m4\ngraph export images/fig2_9.png, replace width(1000)\n```\n:::\n\n\n![](images/fig2_9.png)\n\n## Sampling Variance of OLS\n\nWe Start with:\n\n$$\n\\hat \\beta_1 - \\beta_1 = \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2}\n$$\n\nAnd apply the Variance operator:\n\n$$\\begin{aligned}\nVar(\\hat \\beta_1 - \\beta_1) &= Var \\left( \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2} \\right)=\n            \\frac{\\tilde x_1 Var(u_i)}{(\\sum x_i^2)^2}+\\frac{\\tilde x_2^2 Var(u_i)}{(\\sum x_i^2)^2}+...+\\frac{\\tilde x_n^2 Var(u_i)}{(\\sum x_i^2)^2} \\\\\n            &= \\frac {\\sum  \\tilde x_i^2 Var( u_i) }{(\\sum \\tilde x_i^2)^2} =\\frac {\\sum  \\tilde x_i^2 \\sigma_u^2 }{(\\sum \\tilde x_i^2)^2} \\\\\n            &= \\sigma_u^2 \\frac {\\sum  \\tilde x_i^2 }{(\\sum \\tilde x_i^2)^2} = \\frac{\\sigma_u^2}{\\sum \\tilde x_i^2}\n\\end{aligned}\n$$\n\n## Last Piece of the Puzze $\\sigma^2_u$\n\nTo estimate $Var(\\beta's)$ we also need $\\sigma^2_u$. But $u$ is not observed, since we only observe $\\hat u$.\n\n$$\\hat u_i = u_i + (\\beta_0 - \\hat \\beta_0) + (\\beta_1 - \\hat \\beta_1)*x_i\n$$\n\nAnd since to estimate $\\hat u_i$ we need to estimate $\\beta_0$ and $\\beta_1$, we \"lose\" degrees of freedom that will require adjustment.\n\nSo, we use the following:\n\n$$\\hat \\sigma^2_u = \\frac {1}{N-2} \\sum_{i=1}^N \\hat u_i^2\n$$\n\n# Another Break?\n\n## Examples {.scrollable}\n\nDeriving OLS $\\beta's$:\n\n::: {.cell execution_count=5}\n``` {.stata .cell-code code-fold=\"false\"}\n** Wage and Education: Example 2.7\n\nfrause wage1, clear\nmata: y = st_data(.,\"wage\")\nmata: x = st_data(.,\"educ\")\nmata: b1 = sum( (x :- mean(x)) :* (y :- mean(y)) ) / sum( (x :- mean(x)):^2 ) \nmata: b0 = mean(y)-mean(x)*b1\nmata: b1, b0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 1             2\n    +-----------------------------+\n  1 |  .5413592547   -.904851612  |\n    +-----------------------------+\n```\n:::\n:::\n\n\nSST = SSE + SSR\n\n::: {.cell execution_count=6}\n``` {.stata .cell-code code-fold=\"false\"}\nmata: yh  = b0:+b1*x\nmata: sst = sum( (y:-mean(y)):^2 )\nmata: sse = sum( (yh:-mean(y)):^2 )\nmata: ssr = sum( (y:-yh):^2 )\nmata: sst, sse, ssr, sse + ssr\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 1             2             3             4\n    +---------------------------------------------------------+\n  1 |  7160.414291   1179.732036   5980.682255   7160.414291  |\n    +---------------------------------------------------------+\n```\n:::\n:::\n\n\n$R^2$:\n\n::: {.cell execution_count=7}\n``` {.stata .cell-code code-fold=\"false\"}\nmata: sse/sst , 1-ssr/sst\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                1            2\n    +---------------------------+\n  1 |  .164757511   .164757511  |\n    +---------------------------+\n```\n:::\n:::\n\n\n$\\hat\\sigma_\\beta$\n\n::: {.cell execution_count=8}\n``` {.stata .cell-code code-fold=\"false\"}\nmata: sig2_u = ssr / (rows(y)-2)\nmata: sst_x  = sum( (x:-mean(x)):^2 )\nmata: sig_b1 = sqrt( sig2_u / sst_x )\nmata: sig_b0 = sqrt( sig2_u * mean(x:^2) / sst_x ) \nmata: sig_b1 , sig_b0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 1             2\n    +-----------------------------+\n  1 |  .0532480368   .6849678211  |\n    +-----------------------------+\n```\n:::\n:::\n\n\nStata command:\n\n::: {.cell execution_count=9}\n``` {.stata .cell-code}\nregress wage educ\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =    103.36\n       Model |  1179.73204         1  1179.73204   Prob > F        =    0.0000\n    Residual |  5980.68225       524  11.4135158   R-squared       =    0.1648\n-------------+----------------------------------   Adj R-squared   =    0.1632\n       Total |  7160.41429       525  13.6388844   Root MSE        =    3.3784\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .5413593    .053248    10.17   0.000     .4367534    .6459651\n       _cons |  -.9048516   .6849678    -1.32   0.187    -2.250472    .4407687\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n## Expanding on SLRM: Units of Measure {.scrollable}\n\nFirst thing you should always consider doing is obtaining some summary statistics.\n\nwithout that its difficult o understand the magnitud of the coefficients and their effects.\n\n::: {.cell execution_count=10}\n``` {.stata .cell-code}\nfrause ceosal1, clear\ndisplay \"***Variables Description***\"\ndes salary roe\ndisplay \"***Summary Statistics***\"\nsum salary roe\ndisplay \"***Simple Regression***\"\nreg salary roe\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n***Variables Description***\n\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nsalary          int     %9.0g                 1990 salary, thousands $\nroe             float   %9.0g                 return on equity, 88-90 avg\n***Summary Statistics***\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      salary |        209     1281.12    1372.345        223      14822\n         roe |        209    17.18421    8.518509         .5       56.3\n***Simple Regression***\n\n      Source |       SS           df       MS      Number of obs   =       209\n-------------+----------------------------------   F(1, 207)       =      2.77\n       Model |  5166419.04         1  5166419.04   Prob > F        =    0.0978\n    Residual |   386566563       207  1867471.32   R-squared       =    0.0132\n-------------+----------------------------------   Adj R-squared   =    0.0084\n       Total |   391732982       208  1883331.64   Root MSE        =    1366.6\n\n------------------------------------------------------------------------------\n      salary | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         roe |   18.50119   11.12325     1.66   0.098    -3.428196    40.43057\n       _cons |   963.1913   213.2403     4.52   0.000     542.7902    1383.592\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nNow, Changing scales has no effect on $R^2$, nor the coefficient to Standard error ratio ($t-stat$)\n\nIt could allow for easier interpretation of the results.\n\n::: {.cell execution_count=11}\n``` {.stata .cell-code code-fold=\"false\"}\ngen saldol=salary*1000\ngen roedec=roe / 100\nqui: reg salary roe\nest sto m1\nqui: reg saldol roe\nest sto m2\nqui: reg salary roedec\nest sto m3\nesttab m1 m2 m3, se r2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                   salary          saldol          salary   \n------------------------------------------------------------\nroe                 18.50         18501.2                   \n                  (11.12)       (11123.3)                   \n\nroedec                                             1850.1   \n                                                 (1112.3)   \n\n_cons               963.2***     963191.3***        963.2***\n                  (213.2)      (213240.3)         (213.2)   \n------------------------------------------------------------\nN                     209             209             209   \nR-sq                0.013           0.013           0.013   \n------------------------------------------------------------\nStandard errors in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n:::\n:::\n\n\n## Expanding on SLRM: Nonlinearities {.scrollable}\n\nIt is possible to incorporate some nonlinearities by using \"log\" transformations:\n\n$$\n\\begin{aligned}\nlog(y) &= \\beta_0 + \\beta_1 x + e \\rightarrow & 100\\beta_1 \\simeq \\frac{\\% \\Delta y}{\\Delta x} \\\\\ny &= \\beta_0 + \\beta_1 log(x) + e \\rightarrow & \\frac {\\beta_1}{100} \\simeq \\frac{\\Delta y}{\\% \\Delta x} \\\\\nlog(y) &= \\beta_0 + \\beta_1 log(x) + e \\rightarrow & \\beta_1 =\\frac{\\% \\Delta y}{\\% \\Delta x}\n\\end{aligned} \n$$\n\n-   This allows us to estimate other interesting relationships with a the SRM. Specifically Semi-elasticities and Elasticities.\n\n-   This transformation compresses the distribution of a variable, potentially addressing problems of Heteroskedasticity (non-constant variance)\n\n::: {.cell execution_count=12}\n``` {.stata .cell-code}\n*** Example 2.10\nfrause wage1, clear\ngen lnwage = log(wage)\ngen lneduc = log(educ)\ntwo scatter wage educ     || lfit wage educ    , ///\n    name(m1, replace) title(lin-lin) legend(off)\ntwo scatter lnwage educ   || lfit lnwage educ  , ///\n    name(m2, replace) title(log-lin) legend(off)\ntwo scatter wage lneduc   || lfit wage lneduc  , ///\n    name(m3, replace) title(lin-log) legend(off)\ntwo scatter lnwage lneduc || lfit lnwage lneduc, ///\n    name(m4, replace) title(log-log) legend(off)\ngraph combine m1 m2 m3 m4, \ngraph export images/fig2_10.png, width(1000) replace\n```\n:::\n\n\n![](images/fig2_10.png){height=\"70%\"}\n\n::: columns\n::: {.column width=\"50%\"}\n\n::: {.cell execution_count=13}\n``` {.stata .cell-code}\nset linesize 255\nqui: reg wage educ\nest sto m1\nqui: reg lnwage educ\nest sto m2\nqui: reg wage lneduc\nest sto m3\nqui: reg lnwage lneduc\nest sto m4\nesttab m1 m2 m3 m4, se r2 nonumber  compress nostar nogaps\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n--------------------------------------------------\n                wage    lnwage      wage    lnwage\n--------------------------------------------------\neduc           0.541    0.0827                    \n            (0.0532) (0.00757)                    \nlneduc                             5.330     0.825\n                                 (0.608)  (0.0864)\n_cons         -0.905     0.584    -7.460    -0.445\n             (0.685)  (0.0973)   (1.532)   (0.218)\n--------------------------------------------------\nN                526       526       524       524\nR-sq           0.165     0.186     0.128     0.149\n--------------------------------------------------\nStandard errors in parentheses\n```\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n```         \n\nAn additional year of education\n\n1. Increases hourly wages in 54cnts\n\n2. Increases hourly wages in 8.3%\n\n3. A 1% increase in years of education (about 1.5months) increases wages in 5.3cnts\n\n4. A 1% increase in years of education would increase wages in 0.82%.\n```\n:::\n:::\n\n## Expanding on SLRM: Using Dummies {.scrollable}\n\n-   A SLRM can also be done using **Dummy** variables. (Those that take only two values: 0 or 1)\n\n-   This type of modeling may be observed when evaluating programs (Were you treated?(Tr=1) or not (Tr=0))\n\n-   And can be used to Easily compare means across two groups:\n\n$$wage = \\beta_0 + \\beta_1 female + e\n$$\n\nIn this particular case, both $\\beta_0 \\& \\beta_1$ have clear interpretation:\n\n$$\n\\begin{aligned}\n\\beta_0 &= E(wage|female=0) \\\\\n\\beta_1 &= E(wage|female=1) - E(wage|female=0)\n\\end{aligned}\n$$\n\nIn most Software, you need to either Create the new variable explicitly, or use internal code to make it for you:\n\n::: {.cell execution_count=14}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause wage1, clear\n** verify Coding\nssc install fre, replace\nfre female\n** create your own\ngen is_male = female==0\n** Regression using Newly created variable\nreg wage is_male\n** Regression using Stata \"factor notation\"\nreg wage i.female\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nchecking fre consistency and verifying not already installed...\nall files already exist and are up to date.\n\nfemale -- =1 if female\n-----------------------------------------------------------\n              |      Freq.    Percent      Valid       Cum.\n--------------+--------------------------------------------\nValid   0     |        274      52.09      52.09      52.09\n        1     |        252      47.91      47.91     100.00\n        Total |        526     100.00     100.00           \n-----------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =     68.54\n       Model |  828.220467         1  828.220467   Prob > F        =    0.0000\n    Residual |  6332.19382       524  12.0843394   R-squared       =    0.1157\n-------------+----------------------------------   Adj R-squared   =    0.1140\n       Total |  7160.41429       525  13.6388844   Root MSE        =    3.4763\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     is_male |    2.51183   .3034092     8.28   0.000     1.915782    3.107878\n       _cons |   4.587659   .2189834    20.95   0.000     4.157466    5.017852\n------------------------------------------------------------------------------\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =     68.54\n       Model |  828.220467         1  828.220467   Prob > F        =    0.0000\n    Residual |  6332.19382       524  12.0843394   R-squared       =    0.1157\n-------------+----------------------------------   Adj R-squared   =    0.1140\n       Total |  7160.41429       525  13.6388844   Root MSE        =    3.4763\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    1.female |   -2.51183   .3034092    -8.28   0.000    -3.107878   -1.915782\n       _cons |   7.099489   .2100082    33.81   0.000     6.686928     7.51205\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nif the Dummy is a treatment, and Assumption 4 Holds, then you can use this to estimate Average Treatment Effects (ATE) aka Average Casual Effects. (Usually requires randomization)\n\n$$\n\\begin{aligned}\ny_i &= y_i(0)(1-D) + y_i(1)D  \\\\\ny_i &= y_i(0) + (y_i(1)-y_i(0))*D \\\\\ny_i &= \\bar y_0 + u_i(0) + (\\bar y_1 - \\bar y_0)*D + (u_i(1)-u_i(0))*D \\\\\ny_i &= \\alpha_0 + \\tau_{ate} D + u_i\n\\end{aligned}\n$$\n\nBut we expect $u_i(1)-u_i(0)=0$\n\n::: {.cell execution_count=15}\n``` {.stata .cell-code code-fold=\"false\"}\n** Example 2.14\nfrause jtrain2, clear\n** Training was Randomly assigned\nreg re78 i.train\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n      Source |       SS           df       MS      Number of obs   =       445\n-------------+----------------------------------   F(1, 443)       =      8.04\n       Model |  348.013451         1  348.013451   Prob > F        =    0.0048\n    Residual |  19177.6432       443  43.2903909   R-squared       =    0.0178\n-------------+----------------------------------   Adj R-squared   =    0.0156\n       Total |  19525.6566       444  43.9767041   Root MSE        =    6.5795\n\n------------------------------------------------------------------------------\n        re78 | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     1.train |   1.794343   .6328536     2.84   0.005     .5505748    3.038111\n       _cons |   4.554802    .408046    11.16   0.000     3.752856    5.356749\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n# Thats all for today\n\n## { background-image=\"images/paste-7.png\" background-size=\"contain\"} \n\n",
    "supporting": [
      "2_SRA_files"
    ],
    "filters": [],
    "includes": {}
  }
}