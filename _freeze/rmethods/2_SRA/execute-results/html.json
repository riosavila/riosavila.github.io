{
  "hash": "74c4a2e488e92878be00069d89ddd28e",
  "result": {
    "markdown": "---\ntitle: Simple Regression Model\ntitle-slide-attributes:\n  data-background-image: images/paste-2.png\n  data-background-size: contain\n  data-background-opacity: '0.5'\nsubtitle: The first tool of Many\nauthor: Fernando Rios-Avila\nformat:\n  revealjs:\n    slide-number: true\n    width: 1500\n    height: 900\n    code-fold: true\n    echo: true\n    css: styles.css\n---\n\n## The Simple Regression Model\n\n-   As we saw in the previous slides, one of the important steps when doing empirical analysis is to develop a model that describes reality.\n\n-   This model is quite abstract, as it rarely provides guidance regarding on How should you build your econometric model.\n\n-   In this chapter, we introduce the first (boring) tool to solve this problem. The simple Regression model or SRL\n\n## What is a Simple Regression Model (SRM) ?\n\n-   A Simple regression model is known as such because it aims to capture the relationship between **two** variables.\n\n-   It does not mean it ignores other factors, but rather, bundles them together as part of a **Bag of Holding** or error. In its most flexible setup, a simple regression model can be written as:\n\n$$y = f(x,u)\n$$\n\nThis model simply says that there is some relationship between:\n\n-   $y$, your outcome, dependent, explained, response, variable\n\n-   and $x$, your independent, explanatory, regression, variable\n\nwhereas everything else not considered is assumed to be part of the unobserved $u$.\n\n## From Abstract to Concrete\n\n-   A good reason why one should start thinking about the model as shown earlier is to acknowledge that we **Do not know** the functional form between $x$ and $y$.\n\n-   Further, we don't even know how $u$ interacts with $x$.\n\nThis brings us to the first step one should do (almost always) when analyzing data...Create a plot to see if there is any relationship in the data\n\n## Simple Scatter 1\n\n```stata\n*| fig-align: center\n** To download all Wooldrige Files\nqui: ssc install frause, replace\n** for some additional color schemes\nqui: ssc install color_style\n** Loads file wage1\nfrause wage1, clear\nscatter wage educ\n```\n\n-   We observe a positive relationship between Wages and years of education\n-   This relationship does not seem to be linear\n\n## Simple Scatter 2\n\n![](images/paste-3.png){ fig-align=\"center\"}\n\n## Even more Concrete\n\n- This first \"model\" provides little guidance for the modeling itself. \n- The Simple Linear Regression Model corrects for that, establishing a specific relationship between the variables of interest and the error: \n$$y = \\beta_0 + \\beta_1 x + u$$\n\nThis model has a lot packed in.\n\n-   It imposes a relationship between $y$ and $x$ (linear)\n-   And addresses the fact that there could be other factors not considered $u$. Impossing the assumption they are additive errors. \n\nIt also assumes the population relationships:\n$$E(y|x) = \\beta_0 + \\beta_1 x$$\n\n## What can we learn from it?\n\n$$E(y|x) = \\beta_0 + \\beta_1 x$$\n\nThis is your Population Regresson function. To interpret it, we need to assume $u$ is fixed (*ceteris paribus*). This implies that\n$$E(u|x)=c=0$$\n\nWhich says that the errors are **mean independent** of $x$. Thus, for all practical purposes, when $x$ changes, we will assume $u$ is as good as fixed.\n\nUnder these conditions, we can interpret the coefficients:\n\n- $\\beta_0$ is the constant, or expected outcome when $x=0$.\n- $\\beta_1$ is the slope of $x$, or the expected change in $y$ when $x$ changes in 1 unit:\n\n$$\\Delta y = \\beta_1 \\Delta x \\rightarrow \\frac{\\Delta y}{\\Delta x} = \\beta_1\n$$\n\n## Example\n\n- Soybean and Yield Fertilizer:\n\n$$yield = \\beta_0 + \\beta_1 fertilizer + u$$\n\n$\\beta_1$ Effect of Fertilizer (an additional dosage) on Soybean Yield\n\n- Simple wage equation\n\n$$wage = \\beta_0 + \\beta_1 educ + u\n$$\n\n$\\beta_1$ Change in wages given an additional year of education.\n\n## Deriving Coefficients: Ordinary Least Squares - OLS\n\n- There are an infinite number of candiates for $\\beta_0 \\& \\beta_1$. \n\n- OLS, is **one** of the multiple methods that allows us to estimate the coefficients of a SLRM^[Simple Linear Regression Model].\n\n- The goal is to Choose parameters $\\beta={\\beta_0,\\beta_1}$ that \"minimizes\" the Squared of the residuals.\n  - In other words, OLS aims to maximize Explantion power by minimizing errors.\n\n## Visualization\n\n```stata\n*| output: false\nset seed 10\nclear\nrange x -2 2 20\ngen y = 1 + x + rnormal()\ncolor_style tableau\ntwo (scatter  y x) ///\n\t(function y = 0.5 + 2*x, range(-1.8 2.4)) ///\n\t(function y = 2 + 0.5*x, range(-1.8 2.4)) ///\n\t(function y = 1 + 1*x, range(-1.8 2.4)) , ///\n\tlegend(order(2 \"y=0.5+2x\" 3 \"y=2+0.5x\" 4 \"y=1+1x\"))\ngraph export images\\fig2_1.png , replace width(1000)\n\ngen y1=.5+2*x\ngen y2=2+0.5*x\ngen y3=+1+1*x\n\ntwo (scatter y x) ///\n\t(function y = 0.5 + 2*x, range(-1.8 2.4)) ///\n\t(rspike y y1 x), ylabel(-4/6) ytitle(y) ///\n\tlegend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))\ngraph export images\\fig2_2.png , replace width(1000) \n\t\ntwo (scatter y x) ///\n\t(function y = 2 + 0.5*x, range(-1.8 2.4)) ///\n\t(rspike y y2 x), ylabel(-4/6) ytitle(y) ///\n\tlegend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))\ngraph export images\\fig2_3.png , replace width(1000) \n\t\ntwo (scatter y x) ///\n\t(function y = 1 + 1*x, range(-1.8 2.4)) ///\n\t(rspike y y3 x)\t, ylabel(-4/6) ytitle(y) ///\n\tlegend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))\ngraph export images\\fig2_4.png , replace width(1000) \t\n\nreg y x\npredict yh\ntwo (scatter y x) ///\n\t(function y = _b[_cons] + _b[x]*x, range(-1.8 2.4)) ///\n\t(rspike y yh x)\t, ylabel(-4/6) ytitle(y) ///\n\tlegend(order(1 \"Data\" 2 \"Prediction \" 3 \"Residual\"))\t\ngraph export images\\fig2_5.png , replace width(1000) \t\n\n\n```\n\n:::{.panel-tabset}\n\n## Options\n\n![](images\\fig2_1.png)\n\n## Opt1\n\n![](images\\fig2_2.png)\n\n## Opt2\n\n![](images\\fig2_3.png)\n\n## Opt3\n\n![](images\\fig2_4.png)\n\n## Opt4\n\n![](images\\fig2_5.png)\n\n:::\n\n## Just a Minimization Problem\n\n$$y_i =\\beta_0 + \\beta_1 x_i + u_i \\rightarrow u_i = y_i - \\beta_0 - \\beta_1 x_i \n$$\n\n$${\\hat\\beta_0,\\hat\\beta_1} = \\min_{\\beta_0,\\beta_1} = SSR =\\sum_{i=1}^N u_i^2 = \\sum_{i=1}^N (y-\\beta_0 - \\beta_1 x_i)^2 \\\\\n$$\n\nFirst Order Conditions:\n$$\n\\begin{aligned}\n\\frac{\\partial SSR}{\\partial \\beta_0} &= -2 \\sum (y_i-\\beta_0 - \\beta_1 x_i) = -2 \\sum u_i =0 \\\\\n\\frac{\\partial SSR}{\\partial \\beta_1} &= -2 \\sum x_i (y_i-\\beta_0 - \\beta_1 x_i) =- 2 \\sum x_i u_i =0\n\\end{aligned}\n$$\n\n## Just a Minimization Problem\n\nSimilar conditions as before (but now Mathematically):\n\n$$\n\\begin{aligned} \n\\sum u_i &=0 \\rightarrow nE(e) = 0 \\\\ \n\\sum x_i u_i &=0 \\rightarrow nE(x*e) \\rightarrow  n Cov(x,e) =0  \n\\end{aligned}\n$$ \n\nAnd the First Order Conditions simply provide a sysmtem of $k+1$ equations with $k+1$ unknowns.\n\n$$\\begin{aligned}\n\\hat\\beta_0 &= \\bar y - \\beta_1 \\bar x \\\\\n\\hat\\beta_1 &= \\frac{\\sum (x_i-\\bar x)(y_i-\\bar y)}{\\sum (x_i-\\bar x)^2} \n= \\frac{\\hat \\rho \\hat \\sigma_x \\hat \\sigma_y}{\\hat \\sigma_x^2} \n= \\frac{\\hat \\rho \\hat \\sigma_y}{\\hat \\sigma_y}\n\\end{aligned}\n$$\n\n## Interpretation?\n\n$$\\hat\\beta_0 = \\bar y - \\hat \\beta_1 \\bar x$$\n\n- $\\beta_0$ is usually estimated as a \"residual\", thus is often of little of no interest.\n  - Expected outcome when $X=0$\n\n$$\n\\hat\\beta_1 = \\frac{\\hat \\rho \\hat \\sigma_x \\hat \\sigma_y}{\\hat \\sigma_x^2} \n= \\frac{\\hat \\rho \\hat \\sigma_y}{\\hat \\sigma_x}\n$$\n\n- $\\beta_1$ is a slope, which is directly related to the correlation between $y$ and $x$. \n  - It can only be estimated if $\\sigma_x$>>0\n\nAlso, this $\\hat y = \\hat \\beta_0 + \\hat \\beta_1 x$ becomes your sample regression function\n\n- where $\\hat y$ is the fitted value of $y$ (proyection or prediction), given some value of $x$.\n\n## Visualization\n\n```stata\n*| output: false\ngen y0 = 0\ntwo (scatter y x) ///\n\t(function y = _b[_cons] + _b[x]*x, range(-2.1 2.1)) ///\n\t(rspike y y0 x, color(gs9%50) lw(1)) ///\n\t, ylabel(-4/6) ytitle(y) yline(0) ///\n\ttitle(\"Data\")\tlegend(off)\ngraph export images\\fig2_6.png, replace width(1000)\ntwo (scatter y x) ///\n\t(function y = _b[_cons] + _b[x]*x, range(-2.1 2.1)) ///\n\t(rspike yh y0 x, color(gs9%50) lw(1)) ///\n\t, ylabel(-4/6) ytitle(y) yline(0) ///\n\ttitle(\"Prediction\")\tlegend(off)\ngraph export images\\fig2_7.png, replace width(1000)\t\ntwo (scatter y x) ///\n\t(function y = _b[_cons] + _b[x]*x, range(-2.1 2.1)) ///\n\t(rspike y yh x, color(gs9%50) lw(1)) ///\n\t, ylabel(-4/6) ytitle(y) yline(0) ///\n\ttitle(\"Residual\")\tlegend(off)\ngraph export images\\fig2_8.png, replace width(1000)\t\n```\n\n:::{.panel-tabset}\n\n## Data\n\n![](images\\fig2_6.png)\n\n## Prediction\n\n![](images\\fig2_7.png)\n\n## Residuals\n\n![](images\\fig2_8.png)\n\n:::\n\n## Properties of the Estimator\n\n1. Based on F.O.C., we know the following:\n\n$$\n\\sum_i^n \\hat u_i = 0 \\ \\& \\ \\sum_i^n x_i \\hat u_i = 0\n$$\n\nIn average $u_i$ is zero, and uncorrelated with $x$, \nand  $\\bar y ,  \\bar x$ **is** on the regression line\n\n2. By construction $y_i = \\hat y_i + \\hat u_i$, so that\n\n$$\n\\begin{aligned}\n\t\\sum_{i=1}^n(y_i-\\bar y)^2 &= \n\t\\sum_{i=1}^n(y_i-\\hat y)^2  + \n\t\\sum_{i=1}^n(\\hat y-\\bar y)^2  \\\\\n\tSST &= SSE + SSR\n\\end{aligned}\n$$\n\n## Properties of the Estimator\n \n4. Goodness of FIT is defined as\n\n$$R^2= 1-\\frac {SSR} {SST}=\\frac {SSE} {SST}$$\n\n- How much of the Data variation (SST) is explained by the model (SSE)\n\n## Some Discussion\n\nWe now know how to estimate coefficients given some data, but we need to ask the questions:\n\n- How do we know if the estimated coefficients are indeed appropriate for the population parameters?\n- How can we know the precision (or lack there off) of the estimates\n\n- Remember, $\\hat \\beta's$ depend on the sample. Different Samples will lead to different estimates. Thus  $\\hat \\beta's$ are random.\n\n- In repeated sampling scenarios, we could empirically obtain the distribution of the estimated parameters, and verify if estimations are unbiased. \n  \n- However, we can also do that based on analytical solutions. Lets see those assumptions\n\n# Assumptions: For unbiased estimations\nAn estimator is said to be unbiased if $E(\\hat\\beta)=\\beta$\n\n## Assumption 1:\n\n:::{.callout-important}\n\n## Linear in Parameters\n\nWe need to assume the population model is linear in parameters:\n\n$$y_i = \\beta_0 + \\beta_1 x_i + u_i$$\n:::\n\nIn other words, we need to assume that the model we chose is a good representation of what the true population model is. \n\n- Additive error, with a linear relationship between $x_i$ on $y_i$.\n\n- We can make it more flexible using some transformations of $x_i$.\n\n\n## Assumption 2:\n\n:::{.callout-important}\n\n## Random Sampling\n\nThe data we are using is collected from a Random sample of the population, for which the linear model is valid.\n\n:::\n\n- Data should be representative from the population (for whom the Linear model Holds)\n- The Data Sampling should not depend the data collected, specially the dependent variable.\n- Also helps to ensure units \"unobservables\" $u's$ are independent from each other. \n\n\n## Assumption 3:\n\n:::{.callout-important}\n\n## There is variation in the explanatory variable\n\n$$\\sum_{i=1}^n (x_i - \\bar x)^2 >0\n$$\n\n:::\n\nIf there is no variation in the data, there are no slopes to estmate, and a solution cannot be found to the linear model. \n\n## Assumption 4:\n\n:::{.callout-important}\n\n## Zero Conditional Mean\n\n$$E(u_i)= E(u_i|x_i) = 0\n$$\n\n:::\n\n- We expect unobserved factors $u_i$ to have a zero average effect on the outcome. This helps identify the constant $\\beta_0$.\n\n- We also expect that the expected value of $u_i$ to be zero for any value of $x$. \t\n\n## Unbiased Coefficients:\n\nIf Assumptions 1-4 Hold, then OLS allows you to estimate the coefficents of the linear Regression model. \n\n$$\\hat \\beta_1 = \\frac {\\sum \\tilde x_i \\tilde y_i}{\\sum \\tilde x_i^2} , \\tilde x_i=x_i - \\bar x\n$$\n\n\n$$\\begin{aligned}\n\\hat \\beta_1 &= \\frac {\\sum \\tilde x_i (\\beta_1 \\tilde x_i +e)}{\\sum \\tilde x_i^2} = \\beta_1 \\frac {\\sum  \\tilde x_i^2 }{\\sum \\tilde x_i^2} + \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2}  \\\\  \nE(\\hat \\beta_1)\t&= \\beta_1\n\\end{aligned}\n$$\n\nWhile coefficients can be different for each sample, In average, they will be the same as the true parameters.\n\n## Variance of OLS Estimators\n\nHow precise are the estimates? \n\n$$\\hat \\beta_1 = \\beta_1 + \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2}\n$$\n\n- If we assume $x's$ are assume fixed, the distribution from $\\beta's$ will depend only on the variation of the error $u_i$. \n\n- Thus we need to impose an additional assumption on this errors, to estimate the variance of $\\beta's$. (at least for convinience)\n\n## Assumption 5:\n\n:::{.callout-important}\n\n## Errors are Homoskedastic\n\n$$E(u_i^2)= E(u_i ^2 | x_i) = \\sigma_u ^2\n$$\n\n:::\n\n- This simplifying assumption states that the \"distribution\" of the errors is constant, regardless of $x$. \n\n## Visualization\n\n```stata\n*| output: false\nclear\nset scheme white2\nset obs 1000\ngen x = runiform(-2 , 2)\t\ngen u = rnormal()\ngen y1 = x + u\ngen y2 = x + u*abs(x)\ngen y3 = x + u*(2-abs(x))\ngen y4 = x + u*(sin(2*x))\n\n\ntwo scatter y1 x, name(m1, replace)\nscatter y2 x, name(m2, replace)\nscatter y3 x, name(m3, replace)\nscatter y4 x, name(m4, replace)\ngraph combine m1 m2 m3 m4\ngraph export images/fig2_9.png, replace width(1000)\n```\n\n![](images\\fig2_9.png)\n\n## Sampling Variance of OLS\n\nWe Start with:\n$$\n\\hat \\beta_1 - \\beta_1 = \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2}\n$$\n\nAnd apply the Variance operator:\n\n$$\n\\begin{aligned}\nVar(\\hat \\beta_1 - \\beta_1) &= Var \\left( \\frac {\\sum  \\tilde x_i u_i }{\\sum \\tilde x_i^2} \\right) \\\\\n\t\t\t&= \\frac {\\sum  \\tilde x_i^2 Var( u_i) }{(\\sum \\tilde x_i^2)^2} =\\frac {\\sum  \\tilde x_i^2 \\sigma_u^2 }{(\\sum \\tilde x_i^2)^2} \\\\\n\t\t\t&= \\sigma_u^2 \\frac {\\sum  \\tilde x_i^2 }{(\\sum \\tilde x_i^2)^2} = \\frac{\\sigma_u^2}{\\sum \\tilde x_i^2}\n\\end{aligned}\n$$\n\n## Last Piece of the Puzze $\\sigma^2_u$\n\nTo estimate $Var(\\beta's)$ we also need $\\sigma^2_u$. But $u$ is not observed, since we only observe $\\hat u$.\n\n$$\\hat u_i = u_i + (\\beta_0 - \\hat \\beta_0) + (\\beta_1 - \\hat \\beta_1)*x_i\n$$\n\nAnd since to estimate $\\hat u_i$ we need to estimate $\\beta_0$ and $\\beta_1$, we \"lose\" degrees of freedom that will require adjustment.\n\nSo, we use the following:\n\n$$\\hat \\sigma^2_u = \\frac {1}{N-2} \\sum_{i=1}^N \\hat u_i^2\n$$\n\n# Another Break?\n\n## Examples{.scrollable}\n\nDeriving OLS $\\beta's$: \n\n::: {.cell execution_count=1}\n``` {.stata .cell-code code-fold=\"false\"}\n** Wage and Education: Example 2.7\n\nfrause wage1, clear\nmata: y = st_data(.,\"wage\")\nmata: x = st_data(.,\"educ\")\nmata: b1 = sum( (x :- mean(x)) :* (y :- mean(y)) ) / sum( (x :- mean(x)):^2 ) \nmata: b0 = mean(y)-mean(x)*b1\nmata: b1, b0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 1             2\n    +-----------------------------+\n  1 |  .5413592547   -.904851612  |\n    +-----------------------------+\n```\n:::\n:::\n\n\nSST = SSE + SSR\n\n::: {.cell execution_count=2}\n``` {.stata .cell-code code-fold=\"false\"}\nmata: yh  = b0:+b1*x\nmata: sst = sum( (y:-mean(y)):^2 )\nmata: sse = sum( (yh:-mean(y)):^2 )\nmata: ssr = sum( (y:-yh):^2 )\nmata: sst, sse, ssr, sse + ssr\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 1             2             3             4\n    +---------------------------------------------------------+\n  1 |  7160.414291   1179.732036   5980.682255   7160.414291  |\n    +---------------------------------------------------------+\n```\n:::\n:::\n\n\n$R^2$:\n\n::: {.cell execution_count=3}\n``` {.stata .cell-code code-fold=\"false\"}\nmata: sse/sst , 1-ssr/sst\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                1            2\n    +---------------------------+\n  1 |  .164757511   .164757511  |\n    +---------------------------+\n```\n:::\n:::\n\n\n$\\hat\\sigma_\\beta$ \n\n::: {.cell execution_count=4}\n``` {.stata .cell-code code-fold=\"false\"}\nmata: sig2_u = ssr / (rows(y)-2)\nmata: sst_x  = sum( (x:-mean(x)):^2 )\nmata: sig_b1 = sqrt( sig2_u / sst_x )\nmata: sig_b0 = sqrt( sig2_u * mean(x:^2) / sst_x ) \nmata: sig_b1 , sig_b0\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 1             2\n    +-----------------------------+\n  1 |  .0532480368   .6849678211  |\n    +-----------------------------+\n```\n:::\n:::\n\n\nStata command:\n\n::: {.cell execution_count=5}\n``` {.stata .cell-code}\nregress wage educ\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n      Source |       SS           df       MS      Number of obs   =       526\n-------------+----------------------------------   F(1, 524)       =    103.36\n       Model |  1179.73204         1  1179.73204   Prob > F        =    0.0000\n    Residual |  5980.68225       524  11.4135158   R-squared       =    0.1648\n-------------+----------------------------------   Adj R-squared   =    0.1632\n       Total |  7160.41429       525  13.6388844   Root MSE        =    3.3784\n\n------------------------------------------------------------------------------\n        wage | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .5413593    .053248    10.17   0.000     .4367534    .6459651\n       _cons |  -.9048516   .6849678    -1.32   0.187    -2.250472    .4407687\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n## Expanding on SLRM: Units of Measure {.scrollable}\n\n::: {.cell execution_count=6}\n``` {.stata .cell-code}\nfrause ceosal1, clear\ndisplay \"***Variables Description***\"\ndes salary roe\n\ndisplay \"***Summary Statistics***\"\nsum salary roe\n\ndisplay \"***Simple Regression***\"\nreg salary roe\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n***Variables Description***\n\nVariable      Storage   Display    Value\n    name         type    format    label      Variable label\n-------------------------------------------------------------------------------\nsalary          int     %9.0g                 1990 salary, thousands $\nroe             float   %9.0g                 return on equity, 88-90 avg\n***Summary Statistics***\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n      salary |        209     1281.12    1372.345        223      14822\n         roe |        209    17.18421    8.518509         .5       56.3\n***Simple Regression***\n\n      Source |       SS           df       MS      Number of obs   =       209\n-------------+----------------------------------   F(1, 207)       =      2.77\n       Model |  5166419.04         1  5166419.04   Prob > F        =    0.0978\n    Residual |   386566563       207  1867471.32   R-squared       =    0.0132\n-------------+----------------------------------   Adj R-squared   =    0.0084\n       Total |   391732982       208  1883331.64   Root MSE        =    1366.6\n\n------------------------------------------------------------------------------\n      salary | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         roe |   18.50119   11.12325     1.66   0.098    -3.428196    40.43057\n       _cons |   963.1913   213.2403     4.52   0.000     542.7902    1383.592\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nChanging Scale\n\n::: {.cell execution_count=7}\n``` {.stata .cell-code}\ngen saldol=salary*1000\ngen roedec=roe / 100\nqui: reg salary roe\nest sto m1\nqui: reg saldol roe\nest sto m2\nqui: reg salary roedec\nest sto m3\nesttab m1 m2 m3, se r2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                   salary          saldol          salary   \n------------------------------------------------------------\nroe                 18.50         18501.2                   \n                  (11.12)       (11123.3)                   \n\nroedec                                             1850.1   \n                                                 (1112.3)   \n\n_cons               963.2***     963191.3***        963.2***\n                  (213.2)      (213240.3)         (213.2)   \n------------------------------------------------------------\nN                     209             209             209   \nR-sq                0.013           0.013           0.013   \n------------------------------------------------------------\nStandard errors in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n:::\n:::\n\n\n## Expanding on SLRM: Nonlinearities {.scrollable}\n\nIt is possible to incorporate some nonlinearities by using \"log\" transformations:\n\n$$log(y) = \\beta_0 + \\beta_1 x + e \\rightarrow 100\\beta_1 = \\frac{\\% \\Delta y}{\\Delta x}$$\n$$y = \\beta_0 + \\beta_1 log(x) + e \\rightarrow \\frac {\\beta_1}{100} = \\frac{\\Delta y}{\\% \\Delta x}$$\n$$log(y) = \\beta_0 + \\beta_1 log(x) + e \\rightarrow \\beta_1 =\\frac{\\% \\Delta y}{\\% \\Delta x}$$\n\n",
    "supporting": [
      "2_SRA_files"
    ],
    "filters": [],
    "includes": {}
  }
}