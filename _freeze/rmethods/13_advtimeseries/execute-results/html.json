{
  "hash": "5f53c88ae7f9ccbe67f2d399f2706183",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Times Series Part-II\nsubtitle: Building Time Machines\nauthor: Fernando Rios-Avila\nformat:\n  revealjs:\n    slide-number: true\n    width: 1600\n    height: 900\n    code-fold: true\n    code-overflow: wrap\n    echo: true\n    css: styles.css\n    chalkboard: true\n---\n\n## Defending the Sacred time line\n\n![](images/paste-13.png)\n\n## Last week\n\n-   What we learned last week a few methodologies for analyzing time series data.\n    -   Static model, dynamic models, use of trends and seasonality, etc.\n-   All those models, however, were based on very strong assumptions.\n    -   Strict Exogeneity, strict Homoskedasticity, and no serial correlation are difficult to defend.\n    -   Statistical inference validity depends on normality assumption of the error.\n-   Can we relax this assumptions?\n    -   Yes, but we need to impose additional assumtions to the data\n\n## This week\n\n-   One way to relax the assumption of Strict Exogeneiety is assume time series are stationary and weakly dependent.\n\n    -   With this assumptions, LLN and CLT will hold for TS model.\n\n-   What do these two assumptions mean? (stationary and weakly dependent?)\n\n-   Namely, these assumptions impose restrictions on how data SHOULD behaive, so we can learn something from the data.\n\n    -   This means that what we observe in the data are based on data that has stable patterns. Otherwise, if they are unpredictable, we cannot learn anything. (This is the idea of stationarity)\n    -   At the same time, data cannot depend on its own past **too much**. So if we look at two periods that are farther apart, its like they are independent for most practical purposes.\n\n## Can you recognized this type of data?\n\n\n\n::: panel-tabset\n## Trends\n\n![](images/fig13_1.png)\n\n## Density\n\n![](images/fig13_2.png)\n:::\n\n## Stationary Process\n\n-   A Stationary process is one where the characteristics of the distribution remains Stable across time.\n    -   So we can learn something about it\n\nFor example consider the series $x=[x_1,x_2,\\dots,x_T]$, where $T$ can be vary large (infinite)\n\n-   If the series is stationary, then:\n\n$$f(x_t, x_{t+k}) = f(x_s, x_{s+k}) \\forall s, k$$\n\n-   In other words, the distribution across time does not change. (is stable, thus stationary)\n\n-   Otherwise, if the distribution function changes constantly, we cannot learn about it, thus making sense of regressions would be even more difficult.\n\n## Testing for Stationarity\n\n-   Testing for stationarity is not easy. (how do you test for stability of densities?)\n\n-   Instead, from an empirical point of view, we focus on the first 2 moments of the distribution: mean, variance and covariance.\n\nThus:\n\nA series $x=[x_1,x_2,\\dots,x_T]$ is stationary if:\n\n1.  $E(x_t)=E(x_s)=\\mu_x$. Constant mean\n2.  $E(x_t^2)<\\infty$. Finite variance\n3.  $Var(x_t)=Var(x_s)=\\sigma_x^2$. Constant variance\n4.  $cov(x_t,x_{t+k})=cov(x_s,x_{s+k})$. Constant covariance\n\nThe Series repeats itself.\n\n## Weakly Dependent Series\n\n-   Dependence should be understood as a measure of how much a series depends on its own past.\n\n    -   Weakly dependent series are those that depend on its own past, but not too much.\n\n-   From the technical point of view:\n\n    -   If $x_t$ is weakly dependent, then $\\lim_{k\\rightarrow \\infty} corr(x_t,x_{t+k})=0$ sufficiently fast.\n\n-   If this happens, LLN and CLT will hold for TS data.\n\n-   Why?\n\n    -   Omiting $x_{t+1}$ would typically generate an Omitted Variable Bias.\n    -   If data are weakly dependent, however, we won't suffer from OMB\n    -   This is because omitting $x_{t+1}$ is like omitting a unrelated variable.\n\n> NOTE: Weakly dependent variables can be non-stationary. (we call them stationary around a trend)\n\n## Visualizing weakly dependent vs strongly dependent\n\n![](images/paste-14.png)\n\n## Example of Weakly Dependent Series\n\n-   AR(1) process: $x_t = \\rho x_{t-1} + \\epsilon_t$\n    -   Mean in constant (not depent on time)\n    -   Variance is constant (not depent on time)\n    -   Covariance does not depend on time (just on lag)\n    -   If $|\\rho|<1$, then $x_t$ is weakly dependent.\n    \n-   MA(1) process: $x_t = \\epsilon_t + \\theta \\epsilon_{t-1}$\n    - Constant mean and variance. \n    - Covariance does not depend on time. \n    - Covariance bettween $x_t$ and $x_{t+k}$ is zero for $k>1$.\n\n- ARMA(1,1) $x_t = \\rho x_{t-1} + \\epsilon_t + \\theta \\epsilon_{t-1}$\n    \n\n## Special Case AR(1) with $\\rho=1$\n\n-   If $\\rho=1$, we have a random walk process $x_t = x_{t-1} + \\epsilon_t$\n-   This is a process that holds grudges (it remembers its past)\n\n$$x_t = x_{t=0}+e_1+e_2+\\dots+e_t$$\n\n-   This process has constant mean ($x_0$), but variance and covariance are not constant. With a correlation that dissapears slowly.\n\n$$Var(x_t) = t\\sigma^2 \\text{ and } cov(x_t,x_{t+h}) = t\\sigma^2$$    \n\n$$corr(x_t,x_{t+h}) = \\frac{t \\sigma^2}{\\sqrt{t(t+h)}\\sigma^2}=\\sqrt{\\frac{t}{t+h}}$$\n\n**Economics**: With weakly dependent data, policies are transitory, with persistent data effects are longlasting.\n\n## Special Case: non-stationary weakly dependent\n\n- There are few cases where a series is weakly dependent but non-stationary.\n\n$$x_t = \\rho x_{t-1} + \\delta t + \\epsilon_t$$\n\n- $E(x_t)$ is not constant, but this series is still weakly dependent if $|\\rho|<1$.\n- If the data is detrended, however, it becomes stationary.\n\n## Example: Stationary around a trend\n\n$x_t = 0.5*x_{t-1}+0.1 t + u_t$\n\n![](images/paste-15.png)\n\n## Example: Random Walks look with a drift\n\n$$x_t = x_{t-1} + 0.1 + \\epsilon_t$$\n\n![](images/paste-16.png)\n\n\n\n## How do things change: Assumptions\n\n**A1.** Linear in Parameters (same as before), but all variables are stationary and weakly dependent.\n\n**A2.** No Perfect Colinearity\n\n**A3.** Zero Conditional Mean: $E(u_t|x_t)=0$ Contemporaneous exogeneity!\n\nOmitting lags of $x_t$ is not a problem, because they are weakly \"independent\" of $x_t$.\n\n**A1-A3** OLS is consistent.\n\n**Why does this matter??**\n\n- Because, under Strict exogeneity, we cannot allow for Lags of the outcome to be included in the model.\n- Under weak exogeneity, lags can be included.\n\n## How do things change: Assumptions\n\n**A4.** Homoskedasticity: $Var(u_t|x_t)=\\sigma^2$ (also we just need contemporaenous homoskedasticity)\n\n**A5.** No Serial Correlation: $cov(u_t,u_s|x_t)=0$ for $t\\neq s$ \n\nThese two assumptions that make \"life\" easier for estimating standard errors because:\n\n- Under **A1-A5**, OLS estimators are asymptotically normal, and all Standard Statistics are applicable\n\n\n\n## Order of Integration\n\n- As you may expect, many interesting time series are not stationary. However, we may want to use for analysis\n  - to do so, we need to understand their taxonomy (in TS) so we can make them stationary.\n  \n-  A series is said to be integrated of order $d$, denoted $I(d)$, if it can be made stationary by taking $d$ differences.\n\n- A weakly dependent series is an $I(0)$ process. (is already stationary)\n- A random walk is an $I(1)$ process. It could be made stationary by taking first differences.\n  \n$$x_t = x_{t-1} + \\epsilon_t \\rightarrow \\Delta x_t = \\epsilon_t$$\n\n- A series that is $I(2)$ would required two differences to be made stationary.\n\n$$x_t =2x_{t-1} - x_{t-2} + \\epsilon_t \\rightarrow \\Delta^2 x_t = \\epsilon_t$$\n\n## Unit roots and Spurious Regressions\n\n- If a series is $I(1)$, it is said to have a unit root. an $I(2)$ series has two unit roots, etc.\n\n- Data that are $I(1)$ tend to look like data with Trends\n  - If we analyze this data, we may find spurious relationships. t-stats may be high, as well as $R^2$.\n  - This may lead to incorrect conclusions (unless other stronger assumptions are made)\n  - In this cases, using trends will not help.\n\n## Spurious Regressions: Example\n\n$x_t = x_{t-1} + v_t$ & $y_t = y_{t-1} + u_t$\n\n\n\n![](images/fig13_4.png)\n\n# How to decide if a series is $I(1)$ or has a unit root? \nor if its not stationary\n\n## Naive Approach. Look into $\\rho$\n\n- Naive approach: Look at auto correlation:\n  - if $corr(x_t,x_{t-1})>0.9$ then $x_t$ is highly persistent, and probably $I(1)$\n  - Differentiate data and look at auto correlation again.\n\n## Formal Approach: Dickey-Fuller Test for Unit Root\n\nModel: $y_t = \\alpha + \\rho y_{t-1} + e_t$\nAModel: $\\Delta y_t = \\alpha + \\theta y_{t-1} + e_t$\n\n- $H_0: \\rho=1$ (has a unit root) vs $H_1: \\rho<1$ (is stationary)\n- $H_0: \\theta=0$ (has a unit root) vs $H_1: \\theta<0$ (is stationary)\n\nIts a one tail test, however, the statistic of interest does not follow a t-distribution, but a DF distribution\n\n| SL \t| 1% \t| 2.5% \t| 5% \t| 10% |\n|----\t|----\t|----\t|----\t|----\t|\n| DF \t|  -3.43  \t| -3.12   \t| -2.86| -2.57|\n\n## Augmented Dickey-Fuller Test\n\nAllowing for serial correlation, and uses same critial values as before:\n\nModel: $\\Delta y_t = \\alpha + \\theta y_{t-1} + \\lambda_1 \\Delta y_{t-1} + \\dots + \\lambda_k \\Delta y_{t-k} + e_t$\n\nSame as before. But in practice the additional lags should be choosen based on information criteria.\n\n## ADF with a trend\n\nModel: $\\Delta y_t = \\alpha + \\delta t +\\theta y_{t-1} + \\lambda_1 \\Delta y_{t-1} + \\dots + \\lambda_k \\Delta y_{t-k} + e_t$\n\n- This allows for even more flexibility, or if you believe data is stationary around a trend.\n\n- Main difference... critical values are even larger:\n\n| SL \t| 1% \t| 2.5% \t| 5% \t| 10% |\n|----\t|----\t|----\t|----\t|----\t|\n| DF \t|  -3.96 \t| -3.66   \t| -3.41| -3.12|\n\nAs before, If you find evidence of unit root, Differentiate and test again.\n\n## Example {.scrollable}\n\n::: {#a3beebd2 .cell .larger execution_count=3}\n``` {.stata .cell-code}\nqui:frause fertil3, clear\n** Setup as time series\ntsset year\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTime variable: year, 1913 to 1984\n        Delta: 1 unit\n```\n:::\n:::\n\n\nThe model: $gfr = \\alpha + \\delta_0 pe_t + \\delta_1 pe_{t-1} + \\delta_2 pe_{t-2}+ e_t$\n\n::: {#ff19355f .cell .larger execution_count=4}\n``` {.stata .cell-code}\nreg gfr pe l.pe l2.pe  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n      Source |       SS           df       MS      Number of obs   =        70\n-------------+----------------------------------   F(3, 66)        =      0.14\n       Model |  159.461148         3   53.153716   Prob > F        =    0.9383\n    Residual |  25832.9717        66  391.408663   R-squared       =    0.0061\n-------------+----------------------------------   Adj R-squared   =   -0.0390\n       Total |  25992.4329        69  376.701926   Root MSE        =    19.784\n\n------------------------------------------------------------------------------\n         gfr | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          pe |\n         --. |  -.0158445    .140256    -0.11   0.910    -.2958747    .2641856\n         L1. |  -.0213365   .2152292    -0.10   0.921    -.4510555    .4083826\n         L2. |   .0539005   .1381132     0.39   0.698    -.2218513    .3296524\n             |\n       _cons |   93.15791   4.499654    20.70   0.000     84.17406    102.1418\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nAre $gfr$ and $pe$ Stationary?\n\nNaive approach: Look at auto correlation:\n\n::: {#1bd76411 .cell .larger execution_count=5}\n``` {.stata .cell-code}\n** Naive apprach\ncorr pe l.pe gfr l.gfr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(obs=71)\n\n             |                 L.                L.\n             |       pe       pe      gfr      gfr\n-------------+------------------------------------\n          pe |\n         --. |   1.0000\n         L1. |   0.9636   1.0000\n         gfr |\n         --. |   0.0086   0.0188   1.0000\n         L1. |  -0.0300  -0.0296   0.9765   1.0000\n\n```\n:::\n:::\n\n\nFormal Approach: Dickey-Fuller Test for Unit Root\n\n::: {#0aca5d5c .cell execution_count=6}\n``` {.stata .cell-code}\nreg d.pe l.pe, nohead\nreg d.gfr l.gfr, nohead\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n------------------------------------------------------------------------------\n        D.pe | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          pe |\n         L1. |  -.0521147   .0316692    -1.65   0.104    -.1152931    .0110637\n             |\n       _cons |   6.426196   3.808601     1.69   0.096    -1.171754    14.02415\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\n       D.gfr | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         gfr |\n         L1. |  -.0222798   .0260053    -0.86   0.395     -.074159    .0295994\n             |\n       _cons |   1.304937   2.548821     0.51   0.610    -3.779822    6.389695\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nSame conclusions. Are their differences stationary?\n\n::: {#eeae1688 .cell execution_count=7}\n``` {.stata .cell-code}\ngen dpe = d.pe \ngen dgfr = d.gfr\nreg d.dpe l.dpe, nohead\nreg d.dgfr l.dgfr, nohead\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(1 missing value generated)\n(1 missing value generated)\n------------------------------------------------------------------------------\n       D.dpe | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         dpe |\n         L1. |  -.7666022   .1181882    -6.49   0.000    -1.002443   -.5307613\n             |\n       _cons |   .8901863   2.103074     0.42   0.673    -3.306432    5.086804\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\n      D.dgfr | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        dgfr |\n         L1. |   -.713481   .1158143    -6.16   0.000    -.9445848   -.4823772\n             |\n       _cons |  -.6332004   .5027213    -1.26   0.212    -1.636365    .3699644\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nNow it should be better to use model in differences for analysi:\n\n::: {#b607c2ca .cell execution_count=8}\n``` {.stata .cell-code}\nreg dgfr dpe l.dpe l2.dpe  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(3, 65)        =      6.56\n       Model |  293.259859         3  97.7532864   Prob > F        =    0.0006\n    Residual |  968.199959        65   14.895384   R-squared       =    0.2325\n-------------+----------------------------------   Adj R-squared   =    0.1971\n       Total |  1261.45982        68  18.5508797   Root MSE        =    3.8595\n\n------------------------------------------------------------------------------\n        dgfr | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         dpe |\n         --. |  -.0362021   .0267737    -1.35   0.181     -.089673    .0172687\n         L1. |  -.0139706   .0275539    -0.51   0.614    -.0689997    .0410584\n         L2. |   .1099896   .0268797     4.09   0.000     .0563071    .1636721\n             |\n       _cons |  -.9636787   .4677599    -2.06   0.043     -1.89786   -.0294976\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n## The Problem of Serial Correlation\n\n- Up to this point, we have assumed that the error term is uncorrelated across time. (no serial correlation)\n- As with RC analysis, violation of this assumption does not lead to biased estimators of the coefficients (under usual situations), but it does lead to biased standard errors.\n- Why? If errors are correlated across time, (say possitively) then the variance of the OLS estimator is biased downwards.\n\n$$Var(u_t+u_{t+h})=2\\sigma^2 + \\color{red}{2\\rho_{t,t+h}} \\sigma^2$$\n\n## Serial Correlation and Lags\n\n- If one has a model with Lags, then serial correlation is likely to happen.\n\n$$y_t = \\beta_0 + \\beta_1 y_{t-1} + u_t$$\n\n- This model simply assumes that $y_{t-1}$ should be uncorrelated with $u_t$. But, it may be that $y_{t-2}$ is correlated with $u_t$.\n- If that is the case then $Corr(u_t, u_{t-1})\\neq 0$ because it may be picking up that correlation. \n\n- On the other hand, if $u_t$ is serially correlated, then $y_{t-1}$ is correlated with $u_t$, causing OLS to be inconsistent. \n  - This, however, may also indicate that one needs to consider a different model:\n\n$$y_t = \\alpha_0 + \\alpha_1 y_{t-1} + \\alpha_2 y_{t-2} + e_t$$\n\n- Where $e_t$ is not serially correlated, not correlated with $y_{t-1}$, nor $y_{t-2}$.\n\n## Test for Serial Correlation\n### Strictly Exogenous Regressors\n\nModel: $y_t = \\beta_0 + \\beta_1 x_{1,t} + \\dots + \\beta_k x_{k,t} + u_t$\nand: $u_t=\\rho u_{t-1}+e_t$\n\nIf there is no serial correlation, then we simply need to test if $\\rho=0$, using a t-statistic.\n\n### Durbin Watson Test\n\nUnder Classical assumptions, one could also use the DW statistic:\n\n$$DW = \\frac{\\sum_{t=2}^T (u_t-u_{t-1})^2}{\\sum_{t=1}^T u_t^2}$$\n\nwhere $DW\\simeq 2(1-\\hat{\\rho})$. \n\n- if there is no serial correlation, then $DW\\simeq 2$.\n- If there is possitive serial correlation, then $DW<2$.\n- A less practical test, but valid in small samples\n\n## Test for Serial Correlation\n### Weakly Exogenous Regressors\n\nModel: $y_t = \\beta_0 + \\beta_1 x_{1,t} + \\dots + \\beta_k x_{k,t} + u_t$\nand: $u_t=\\rho u_{t-1}+\\gamma_1 x_{1,t} + \\dots + \\gamma_k x_{k,t}+e_t$\n\n$H0: \\rho=0$ vs $H1: \\rho\\neq 0$\n\n### Testing for higher order correlation\n\nand: $u_t=\\rho_1 u_{t-1}+\\rho_2 u_{t-2}+\\gamma_1 x_{1,t} + \\dots + \\gamma_k x_{k,t}+e_t$\n\n$H0: \\rho_1=0 \\& \\rho_2=0$ vs $H1: \\text{one is not equal to }0$\n\nThis test is called the Breusch-Godfrey test.\n\n## Correcting for Serial Correlation: \n\nThere are two ways to correct for Serial Correlation:\n\n- Prais-Winsten and Cochrane-Orcutt regression (Feasible GLS)\n  - Requires variables to be strictly exogenous regressors (no lagged dependent variables)\n- Newey-West Standard Errors (this is the equivalent to Robust)\n  - General setup.\n\n## Prais-Winsten and Cochrane-Orcutt regression\n\nConsider the model:\n\n$$y_t = \\beta_0 + \\beta_1 x_{1,t} + \\beta_2 x_{2,t} + u_t$$\n\nwhere $u_t=\\rho u_{t-1}+e_{t}$\n\nThis model has serial correlation, which will affect the standard errors of the OLS estimator.\n\n- if we know (or estimate) $\\rho$, we can transform the data and eliminate the serial correlation\n\n$$\\begin{aligned}\ny_t &= \\beta_0 + \\beta_1 x_{1,t} + \\beta_2 x_{2,t} + u_t \\\\\\\n\\rho y_{t-1} &= \\rho \\beta_0 + \\rho \\beta_1 x_{1,t-1} + \\rho \\beta_2 x_{2,t-1} + \\rho  u_{t-1} \\\\\\\n\\tilde y_t &= \\beta_0 (1-\\rho) + \\beta_1 \\tilde x_{1,t} + \\beta_2 \\tilde x_{2,t} +  e_t\n\\end{aligned}\n$$\n\n- From here, we can obtain the errors $e_t$ and $u_t$, re estimate $\\rho$, and re estimate the model, until $\\rho$ no longer changes. \n\n- This is called the Cochrane-Orcutt procedure.\n \n## Prais-Winsten and Cochrane-Orcutt regression {.scrollable}\n\n- The Prais-Winsten procedures is also similar to the Cochrane-Orcutt procedure, but you do not \"loose\" the first observation.\n- Specifically, the first observation is estimated as:\n\n$$(1-\\rho^2)^{1/2} y_1 =(1-\\rho^2)^{1/2}\\beta_0 + (1-\\rho^2)^{1/2}\\beta_1 x_{1,1} + (1-\\rho^2)^{1/2}\\beta_2 x_{2,1} + (1-\\rho^2)^{1/2} u_1$$\n\n- Other features:\n  - PW can be more efficient than CO, because of the \"saved observation\"\n  - Both can be used when serial correlation is of higher order, but only CO can be used if order is 3 or higher\n  - Both methods are iterative\n\n::: {#d22a62d8 .cell execution_count=9}\n``` {.stata .cell-code}\nfrause phillips, clear\ntsset year\nreg inf unem\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTime variable: year, 1948 to 2003\n        Delta: 1 unit\n\n      Source |       SS           df       MS      Number of obs   =        56\n-------------+----------------------------------   F(1, 54)        =      3.58\n       Model |   31.599858         1   31.599858   Prob > F        =    0.0639\n    Residual |  476.815691        54   8.8299202   R-squared       =    0.0622\n-------------+----------------------------------   Adj R-squared   =    0.0448\n       Total |  508.415549        55  9.24391907   Root MSE        =    2.9715\n\n------------------------------------------------------------------------------\n         inf | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |   .5023782   .2655624     1.89   0.064    -.0300424    1.034799\n       _cons |   1.053566   1.547957     0.68   0.499    -2.049901    4.157033\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n::: {#39d05533 .cell execution_count=10}\n``` {.stata .cell-code}\nprais inf unem, corc\n\nprais inf unem, \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nIteration 0:   rho = 0.0000\nIteration 1:   rho = 0.5721\nIteration 2:   rho = 0.7204\nIteration 3:   rho = 0.7683\nIteration 4:   rho = 0.7793\nIteration 5:   rho = 0.7815\nIteration 6:   rho = 0.7819\nIteration 7:   rho = 0.7820\nIteration 8:   rho = 0.7820\nIteration 9:   rho = 0.7820\nIteration 10:  rho = 0.7820\n\nCochrane–Orcutt AR(1) regression with iterated estimates\n\n      Source |       SS           df       MS      Number of obs   =        55\n-------------+----------------------------------   F(1, 53)        =      5.09\n       Model |  23.3857044         1  23.3857044   Prob > F        =    0.0282\n    Residual |  243.353574        53  4.59157686   R-squared       =    0.0877\n-------------+----------------------------------   Adj R-squared   =    0.0705\n       Total |  266.739278        54  4.93961626   Root MSE        =    2.1428\n\n------------------------------------------------------------------------------\n         inf | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |  -.6639584   .2942027    -2.26   0.028    -1.254054   -.0738626\n       _cons |   7.287078   2.163178     3.37   0.001     2.948291    11.62586\n-------------+----------------------------------------------------------------\n         rho |   .7820093\n------------------------------------------------------------------------------\nDurbin–Watson statistic (original)    = 0.801482\nDurbin–Watson statistic (transformed) = 1.600203\n\nIteration 0:   rho = 0.0000\nIteration 1:   rho = 0.5721\nIteration 2:   rho = 0.7350\nIteration 3:   rho = 0.7792\nIteration 4:   rho = 0.7871\nIteration 5:   rho = 0.7883\nIteration 6:   rho = 0.7885\nIteration 7:   rho = 0.7885\nIteration 8:   rho = 0.7885\nIteration 9:   rho = 0.7885\n\nPrais–Winsten AR(1) regression with iterated estimates\n\n      Source |       SS           df       MS      Number of obs   =        56\n-------------+----------------------------------   F(1, 54)        =      8.39\n       Model |   38.377534         1   38.377534   Prob > F        =    0.0054\n    Residual |  246.917431        54  4.57254502   R-squared       =    0.1345\n-------------+----------------------------------   Adj R-squared   =    0.1185\n       Total |  285.294965        55  5.18718118   Root MSE        =    2.1384\n\n------------------------------------------------------------------------------\n         inf | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |  -.7139659   .2897858    -2.46   0.017    -1.294951   -.1329804\n       _cons |   7.999443   2.048343     3.91   0.000     3.892762    12.10612\n-------------+----------------------------------------------------------------\n         rho |   .7885234\n------------------------------------------------------------------------------\nDurbin–Watson statistic (original)    = 0.801482\nDurbin–Watson statistic (transformed) = 1.913928\n```\n:::\n:::\n\n\n## Newey-West Standard Errors {.scrollable}\n\n- The Newey-West standard errors are similar to the robust standard errors, but they take into account the serial correlation of the error term.\n\n- The idea is to estimate an inflation factor that corrects Standard errors for serial correlation.\n\n$\\hat v = \\sum_{t=1}^T \\hat a_t^2 + 2 \\sum_{h=1}^g \\left[1-\\frac{h}{g+1}\\right] \\left( \\sum_{t=h+1}^{T} \\hat a_t \\hat a_{t-h}\\right)$\n\nwith $\\hat a_t = \\hat r_t \\hat u_t$\n\n- Then $SE_c (\\beta) = \\sqrt{\\hat v} \\left(\\frac{SE(\\beta)}{\\sigma}\\right)^2$\n  \n- In other words, this kind of corrects for the fact that the error term is correlated across time.\n\n::: {#0c1c023a .cell execution_count=11}\n``` {.stata .cell-code}\nnewey inf unem, lag(0)\nnewey inf unem, lag(1)\nnewey inf unem, lag(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRegression with Newey–West standard errors      Number of obs     =         56\nMaximum lag = 0                                 F(  1,        54) =       4.11\n                                                Prob > F          =     0.0477\n\n------------------------------------------------------------------------------\n             |             Newey–West\n         inf | Coefficient  std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |   .5023782   .2479249     2.03   0.048     .0053187    .9994378\n       _cons |   1.053566   1.379772     0.76   0.448    -1.712711    3.819842\n------------------------------------------------------------------------------\n\nRegression with Newey–West standard errors      Number of obs     =         56\nMaximum lag = 1                                 F(  1,        54) =       3.25\n                                                Prob > F          =     0.0769\n\n------------------------------------------------------------------------------\n             |             Newey–West\n         inf | Coefficient  std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |   .5023782    .278577     1.80   0.077    -.0561351    1.060892\n       _cons |   1.053566   1.464589     0.72   0.475    -1.882759     3.98989\n------------------------------------------------------------------------------\n\nRegression with Newey–West standard errors      Number of obs     =         56\nMaximum lag = 2                                 F(  1,        54) =       3.13\n                                                Prob > F          =     0.0827\n\n------------------------------------------------------------------------------\n             |             Newey–West\n         inf | Coefficient  std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        unem |   .5023782   .2841794     1.77   0.083    -.0673673    1.072124\n       _cons |   1.053566   1.424115     0.74   0.463    -1.801613    3.908744\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n## Cointegration\n\n- As previously mentioned, most interesting time series are not stationary. \n- And, when using non-stationary data, we may find spurious relationships. But what if the relation is not spurious?\n\n- Consider the following model: \n\n  $y_t = \\beta_0 + \\beta_1 x_{1,t} + \\beta_2 x_{2,t} + u_t$\n\n- If $y_t$ and $x_{1,t}$ are $I(1)$, then the model is likely to be spurious (common trends). However, it may be possible that there is a causal relationship between these variables.\n\n- If they indeed have a causal relationship, then they are said to be cointegrated.\n\n## Cointegration \n\n- But how do we know if two variables are cointegrated?\n\n**s1:** Check if all variables are $I(1)$. If they are, then you can check for cointegration.\n\n**s2:** Estimate the model, and obtain the residuals $\\hat u_t$. \n\n**s3:** Test if $\\hat u_t$ is $I(0)$. \n\n  - If that is the case, then the variables are cointegrated (Share a long term relationship)\n  - If not, the relationship is spurious\n\n- How do we test if $\\hat u_t$ is $I(0)$? $\\rightarrow$ Unit Root test! \n\n| SL \t| 1% \t| 2.5% \t| 5% \t| 10% |\n|----\t|----\t|----\t|----\t|----\t|\n| No Trend \t|  -3.9 \t| -3.59   | -3.34| -3.04|\n| With Trend \t|  -4.32 \t| -4.03 \t| -3.78 | -3.50|\n\n## Error Correction Models\n\n- If two variables are cointegrated, then they share a long term relationship.\n- However, you may also be interested in the short term dynamics of the relationship.\n- To do this, you can use an Error Correction Model (ECM)\n\n$\\Delta y_t = \\beta_0 + \\beta_1 \\Delta x_{1,t} + \\beta_2 \\Delta x_{2,t} + \\gamma \\hat u_{t-1}+ e_t$\n\nWhere $\\gamma$ is the short term correction term. \n\n## Other topics of interest\n\n \n- Forcasting\n  - ARIMA models, and VAR models (Vector Autoregressions) can be used for forcasting.\n  - Forcating implies making predictions about the future, based on the past, accounting for the errors propagation.\n  - Variable selection, temporal causation (Granger Causality), and other techniques are used for this.\n  \n# And we are done! \n\n",
    "supporting": [
      "13_advtimeseries_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}