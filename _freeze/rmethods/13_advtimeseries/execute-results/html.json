{
  "hash": "8fa479894dafd897a8839634a6a32c48",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Times Series Part-II\nsubtitle: Building Time Machines\nauthor: Fernando Rios-Avila\nformat:\n  revealjs:\n    slide-number: true\n    width: 1600\n    height: 900\n    code-fold: true\n    code-overflow: wrap\n    echo: true\n    css: styles.css\n    chalkboard: true\n---\n\n## Defending the Sacred time line\n\n![](images/paste-13.png)\n\n## Last week\n\n-   What we learned last week a few methodologies for analyzing time series data.\n    -   Static model, dynamic models, use of trends and seasonality, etc.\n-   All those models, however, were based on very strong assumptions.\n    -   Strict Exogeneity, strict Homoskedasticity, and no serial correlation are difficult to defend.\n    -   Statistical inference validity depends on normality assumption of the error.\n-   Can we relax this assumptions?\n    -   Yes, but we need to impose additional assumtions to the data\n\n## This week\n\n-   One way to relax the assumption of Strict Exogeneiety is assume time series are stationary and weakly dependent.\n\n    -   With this assumptions, LLN and CLT will hold for TS model.\n\n-   What do these two assumptions mean? (stationary and weakly dependent?)\n\n-   Namely, these assumptions impose restrictions on how data SHOULD behaive, so we can learn something from the data.\n\n    -   This means that what we observe in the data are based on data that has stable patterns. Otherwise, if they are unpredictable, we cannot learn anything. (This is the idea of stationarity)\n    -   At the same time, data cannot depend on its own past **too much**. So if we look at two periods that are farther apart, its like they are independent for most practical purposes.\n\n## Can you recognized this type of data?\n\n```stata\n*| echo: false\n*| output: false\nset obs 250\nset seed 1321\ngen t = _n\ntsset t\ngen y1 =0\ngen y2 =0\ngen r = rnormal()\nset scheme white2\ncolor_style tableau\nreplace y1 =l.y1 + r if t>1\nreplace y2 =0.8*l.y2 + r if t>1\nscatter y1 t, title(\"Non-Stationary\") xtitle(\"time\") name(fig1, replace) connect(l)\nscatter y2 t, title(\"Stationary\") xtitle(\"time\") name(fig2, replace) connect(l)\ngraph combine fig1 fig2, ysize(6) xsize(12) ycommon\ngraph export images/fig13_1.png, replace\n\n\ntwo kdensity y1 if t<=125 ||  kdensity y1 if t>125, title(\"Non-Stationary\") name(fig1, replace) legend(off)\ntwo kdensity y2 if t<=125 ||  kdensity y2 if t>125, title(\"Stationary\") name(fig2, replace) legend(off)\ngraph combine fig1 fig2, ysize(6) xsize(12) \ngraph export images/fig13_2.png, replace\n\n```\n\n::: panel-tabset\n## Trends\n\n![](images/fig13_1.png)\n\n## Density\n\n![](images/fig13_2.png)\n:::\n\n## Stationary Process\n\n-   A Stationary process is one where the characteristics of the distribution remains Stable across time.\n    -   So we can learn something about it\n\nFor example consider the series $x=[x_1,x_2,\\dots,x_T]$, where $T$ can be vary large (infinite)\n\n-   If the series is stationary, then:\n\n$$f(x_t, x_{t+k}) = f(x_s, x_{s+k}) \\forall s, k$$\n\n-   In other words, the distribution across time does not change. (is stable, thus stationary)\n\n-   Otherwise, if the distribution function changes constantly, we cannot learn about it, thus making sense of regressions would be even more difficult.\n\n## Testing for Stationarity\n\n-   Testing for stationarity is not easy. (how do you test for stability of densities?)\n\n-   Instead, from an empirical point of view, we focus on the first 2 moments of the distribution: mean, variance and covariance.\n\nThus:\n\nA series $x=[x_1,x_2,\\dots,x_T]$ is stationary if:\n\n1.  $E(x_t)=E(x_s)=\\mu_x$. Constant mean\n2.  $E(x_t^2)<\\infty$. Finite variance\n3.  $Var(x_t)=Var(x_s)=\\sigma_x^2$. Constant variance\n4.  $cov(x_t,x_{t+k})=cov(x_s,x_{s+k})$. Constant covariance\n\nThe Series repeats itself.\n\n## Weakly Dependent Series\n\n-   Dependence should be understood as a measure of how much a series depends on its own past.\n\n    -   Weakly dependent series are those that depend on its own past, but not too much.\n\n-   From the technical point of view:\n\n    -   If $x_t$ is weakly dependent, then $\\lim_{k\\rightarrow \\infty} corr(x_t,x_{t+k})=0$ sufficiently fast.\n\n-   If this happens, LLN and CLT will hold for TS data.\n\n-   Why?\n\n    -   Omiting $x_{t+1}$ would typically generate an Omitted Variable Bias.\n    -   If data are weakly dependent, however, we won't suffer from OMB\n    -   This is because omitting $x_{t+1}$ is like omitting a unrelated variable.\n\n> NOTE: Weakly dependent variables can be non-stationary. (we call them stationary around a trend)\n\n## Visualizing weakly dependent vs strongly dependent\n\n![](images/paste-14.png)\n\n## Example of Weakly Dependent Series\n\n-   AR(1) process: $x_t = \\rho x_{t-1} + \\epsilon_t$\n    -   Mean in constant (not depent on time)\n    -   Variance is constant (not depent on time)\n    -   Covariance does not depend on time (just on lag)\n    -   If $|\\rho|<1$, then $x_t$ is weakly dependent.\n    \n-   MA(1) process: $x_t = \\epsilon_t + \\theta \\epsilon_{t-1}$\n    - Constant mean and variance. \n    - Covariance does not depend on time. \n    - Covariance bettween $x_t$ and $x_{t+k}$ is zero for $k>1$.\n\n## Special Case AR(1) with $\\rho=1$\n\n-   If $\\rho=1$, we have a random walk process $x_t = x_{t-1} + \\epsilon_t$\n-   This is a process that holds grudges (it remembers its past)\n\n$$x_t = x_{t=0}+e_1+e_2+\\dots+e_t$$\n\n-   This process has constant mean ($x_0$), but variance and covariance are not constant. With a correlation that dissapears slowly.\n\n$$Var(x_t) = t\\sigma^2 \\text{ and } cov(x_t,x_{t+h}) = t\\sigma^2$$    \n\n$$corr(x_t,x_{t+h}) = \\frac{t \\sigma^2}{\\sqrt{t(t+h)}\\sigma^2}=\\sqrt{\\frac{t}{t+h}}$$\n\n**Economics**: With weakly dependent data, policies are transitory, with persistent data effects are longlasting.\n\n## Special Case: non-stationary weakly dependent\n\n- There are few cases where a series is weakly dependent but non-stationary.\n\n$$x_t = \\rho x_{t-1} + \\delta t + \\epsilon_t$$\n\n- $E(x_t)$ is not constant, but this series is still weakly dependent if $|\\rho|<1$.\n- If the data is detrended, however, it becomes stationary.\n\n## Example: Stationary around a trend\n\n$x_t = 0.5*x_{t-1}+0.1 t + u_t$\n\n![](images/paste-15.png)\n\n## Example: Random Walks look with a drift\n\n$$x_t = x_{t-1} + 0.1 + \\epsilon_t$$\n\n![](images/paste-16.png)\n## How do things change: Assumptions\n\n**A1.** Linear in Parameters (same as before), but all variables are stationary and weakly dependent.\n\n**A2.** No Perfect Colinearity\n\n**A3.** Zero Conditional Mean: $E(u_t|x_t)=0$ Contemporaneous exogeneity!\n\nOmitting lags of $x_t$ is not a problem, because they are weakly \"independent\" of $x_t$.\n\n**A1-A3** OLS is consistent.\n\n**Why does this matter??**\n\n- Because, under Strict exogeneity, we cannot allow for Lags of the outcome to be included in the model.\n- Under weak exogeneity, lags can be included.\n\n## How do things change: Assumptions\n\n**A4.** Homoskedasticity: $Var(u_t|x_t)=\\sigma^2$ (also we just need contemporaenous homoskedasticity)\n\n**A5.** No Serial Correlation: $cov(u_t,u_s|x_t)=0$ for $t\\neq s$ \n\nThese two assumptions that make \"life\" easier for estimating standard errors because:\n\n- Under **A1-A5**, OLS estimators are asymptotically normal, and all Standard Statistics are applicable\n\n\n\n## Order of Integration\n\n- As you may expect, many interesiting time series are not stationary. However, we may want to use for analysis\n  - to do so, we need to understand their taxonomy (in TS) so we can make them stationary.\n  \n-  A series is said to be integrated of order $d$, denoted $I(d)$, if it can be made stationary by taking $d$ differences.\n\n- A weakly dependent series is an $I(0)$ process. (is already stationary)\n- A random walk is an $I(1)$ process. It could be made stationary by taking first differences.\n  \n$$x_t = x_{t-1} + \\epsilon_t \\rightarrow \\Delta x_t = \\epsilon_t$$\n\n- A series that is $I(2)$ would required two differences to be made stationary.\n\n$$x_t =2x_{t-1} - x_{t-2} + \\epsilon_t \\rightarrow \\Delta^2 x_t = \\epsilon_t$$\n\n## Unit roots and Spurious Regressions\n\n- If a series is $I(1)$, it is said to have a unit root. an $I(2)$ series has two unit roots, etc.\n\n- Data that are $I(1)$ tend to look like data with Trends\n  - If we analyze this data, we may find spurious relationships. t-stats may be high, as well as $R^2$.\n  - This may lead to incorrect conclusions (unless other stronger assumptions are made)\n  - In this cases, using trends will not help.\n\n## Spurious Regressions: Example\n\n$x_t = x_{t-1} + v_t$ & $y_t = y_{t-1} + u_t$\n\n\n\n![](images/fig13_4.png)\n\n# How to decide if a series is $I(1)$ or has a unit root?\n\n## Naive Approach. Look into $\\rho$\n\n- Naive approach: Look at auto correlation:\n  - if $corr(x_t,x_{t-1})>0.9$ then $x_t$ is highly persistent, and probably $I(1)$\n  - Differentiate data and look at auto correlation again.\n\n## Formal Approach: Dickey-Fuller Test for Unit Root\n\nModel: $y_t = \\alpha + \\rho y_{t-1} + e_t$\nAModel: $\\Delta y_t = \\alpha + \\theta y_{t-1} + e_t$\n\n- $H_0: \\rho=1$ (has a unit root) vs $H_1: \\rho<1$ (is stationary)\n- $H_0: \\theta=0$ (has a unit root) vs $H_1: \\theta<0$ (is stationary)\n\nIts a one tail test, however, the statistic of interest does not follow a t-distribution, but a DF distribution\n\n| SL \t| 1% \t| 2.5% \t| 5% \t| 10% |\n|----\t|----\t|----\t|----\t|----\t|\n| DF \t|  -3.43  \t| -3.12   \t| -2.86| -2.57|\n\n## Augmented Dickey-Fuller Test\n\nAllowing for serial correlation, and uses same critial values as before:\n\nModel: $\\Delta y_t = \\alpha + \\theta y_{t-1} + \\lambda_1 \\Delta y_{t-1} + \\dots + \\lambda_k \\Delta y_{t-k} + e_t$\n\nSame as before. But in practice the additional lags should be choosen based on information criteria.\n\n## ADF with a trend\n\nModel: $\\Delta y_t = \\alpha + \\delta t +\\theta y_{t-1} + \\lambda_1 \\Delta y_{t-1} + \\dots + \\lambda_k \\Delta y_{t-k} + e_t$\n\n- This allows for even more flexibility, or if you believe data is stationary around a trend.\n\n- Main difference... critical values are even larger:\n\n| SL \t| 1% \t| 2.5% \t| 5% \t| 10% |\n|----\t|----\t|----\t|----\t|----\t|\n| DF \t|  -3.96 \t| -3.66   \t| -3.41| -3.12|\n\nAs before, If you find evidence of unit root, Differentiate and test again.\n\n## Example {.scrollable}\n\n::: {#dba7ab34 .cell .larger execution_count=2}\n``` {.stata .cell-code}\nqui:frause fertil3, clear\n** Setup as time series\ntsset year\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTime variable: year, 1913 to 1984\n        Delta: 1 unit\n```\n:::\n:::\n\n\nThe model: $gfr = \\alpha + \\delta_0 pe_t + \\delta_1 pe_{t-1} + \\delta_2 pe_{t-2}+ e_t$\n\n::: {#cbc4a45a .cell .larger execution_count=3}\n``` {.stata .cell-code}\nreg gfr pe l.pe l2.pe  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n      Source |       SS           df       MS      Number of obs   =        70\n-------------+----------------------------------   F(3, 66)        =      0.14\n       Model |  159.461148         3   53.153716   Prob > F        =    0.9383\n    Residual |  25832.9717        66  391.408663   R-squared       =    0.0061\n-------------+----------------------------------   Adj R-squared   =   -0.0390\n       Total |  25992.4329        69  376.701926   Root MSE        =    19.784\n\n------------------------------------------------------------------------------\n         gfr | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          pe |\n         --. |  -.0158445    .140256    -0.11   0.910    -.2958747    .2641856\n         L1. |  -.0213365   .2152292    -0.10   0.921    -.4510555    .4083826\n         L2. |   .0539005   .1381132     0.39   0.698    -.2218513    .3296524\n             |\n       _cons |   93.15791   4.499654    20.70   0.000     84.17406    102.1418\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nAre $gfr$ and $pe$ Stationary?\n\nNaive approach: Look at auto correlation:\n\n::: {#0889bb9a .cell .larger execution_count=4}\n``` {.stata .cell-code}\n** Naive apprach\ncorr pe l.pe gfr l.gfr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(obs=71)\n\n             |                 L.                L.\n             |       pe       pe      gfr      gfr\n-------------+------------------------------------\n          pe |\n         --. |   1.0000\n         L1. |   0.9636   1.0000\n         gfr |\n         --. |   0.0086   0.0188   1.0000\n         L1. |  -0.0300  -0.0296   0.9765   1.0000\n\n```\n:::\n:::\n\n\nFormal Approach: Dickey-Fuller Test for Unit Root\n\n::: {#7a0a1dc9 .cell execution_count=5}\n``` {.stata .cell-code}\nreg d.pe l.pe, nohead\nreg d.gfr l.gfr, nohead\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n------------------------------------------------------------------------------\n        D.pe | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n          pe |\n         L1. |  -.0521147   .0316692    -1.65   0.104    -.1152931    .0110637\n             |\n       _cons |   6.426196   3.808601     1.69   0.096    -1.171754    14.02415\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\n       D.gfr | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         gfr |\n         L1. |  -.0222798   .0260053    -0.86   0.395     -.074159    .0295994\n             |\n       _cons |   1.304937   2.548821     0.51   0.610    -3.779822    6.389695\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nSame conclusions. Are their differences stationary?\n\n::: {#16a5d700 .cell execution_count=6}\n``` {.stata .cell-code}\ngen dpe = d.pe \ngen dgfr = d.gfr\nreg d.dpe l.dpe, nohead\nreg d.dgfr l.dgfr, nohead\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(1 missing value generated)\n(1 missing value generated)\n------------------------------------------------------------------------------\n       D.dpe | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         dpe |\n         L1. |  -.7666022   .1181882    -6.49   0.000    -1.002443   -.5307613\n             |\n       _cons |   .8901863   2.103074     0.42   0.673    -3.306432    5.086804\n------------------------------------------------------------------------------\n------------------------------------------------------------------------------\n      D.dgfr | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        dgfr |\n         L1. |   -.713481   .1158143    -6.16   0.000    -.9445848   -.4823772\n             |\n       _cons |  -.6332004   .5027213    -1.26   0.212    -1.636365    .3699644\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nNow it should be better to use model in differences for analysi:\n\n::: {#ef9e8558 .cell execution_count=7}\n``` {.stata .cell-code}\nreg dgfr dpe l.dpe l2.dpe  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n      Source |       SS           df       MS      Number of obs   =        69\n-------------+----------------------------------   F(3, 65)        =      6.56\n       Model |  293.259859         3  97.7532864   Prob > F        =    0.0006\n    Residual |  968.199959        65   14.895384   R-squared       =    0.2325\n-------------+----------------------------------   Adj R-squared   =    0.1971\n       Total |  1261.45982        68  18.5508797   Root MSE        =    3.8595\n\n------------------------------------------------------------------------------\n        dgfr | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         dpe |\n         --. |  -.0362021   .0267737    -1.35   0.181     -.089673    .0172687\n         L1. |  -.0139706   .0275539    -0.51   0.614    -.0689997    .0410584\n         L2. |   .1099896   .0268797     4.09   0.000     .0563071    .1636721\n             |\n       _cons |  -.9636787   .4677599    -2.06   0.043     -1.89786   -.0294976\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n## The Problem of Serial Correlation\n\n- Up to this point, we have assumed that the error term is uncorrelated across time. (no serial correlation)\n- As with RC analysis, violation of this assumption does not lead to biased estimators of the coefficients (under usual situations), but it does lead to biased standard errors.\n- Why? If errors are correlated across time, (say possitively) then the variance of the OLS estimator is biased downwards.\n\n$$Var(u_t+u_{t+h})=2\\sigma^2 + \\color{red}{2\\rho_{t,t+h}} \\sigma^2$$\n\n## Serial Correlation and Lags\n\n- If one has a model with Lags, then serial correlation is likely to happen.\n\n$$y_t = \\beta_0 + \\beta_1 y_{t-1} + u_t$$\n\n- This model simply assumes that $y_{t-1}$ should be uncorrelated with $u_t$. But, it may be that $y_{t-2}$ is correlated with $u_t$.\n- If that is the case then $Corr(u_t, u_{t-1})\\neq 0$ because it may be picking up that correlation. \n\n- On the other hand, if $u_t$ is serially correlated, then $y_{t-1}$ is correlated with $u_t$, causing OLS to be inconsistent. \n  - This, however, may also indicate that one needs to consider a different model:\n\n$$y_t = \\alpha_0 + \\alpha_1 y_{t-1} + \\alpha_2 y_{t-2} + e_t$$\n\n- Where $e_t$ is not serially correlated, not correlated with $y_{t-1}$, nor $y_{t-2}$.\n\n## Test for Serial Correlation\n### Strickly Exogenous Regressors\n\nModel: $y_t = \\beta_0 + \\beta_1 x_{1,t} + \\dots + \\beta_k x_{k,t} + u_t$\nand: $u_t=\\rho u_{t-1}+e_t$\n\nIf there is no serial correlation, then we simply need to test if $\\rho=0$, using a t-statistic.\n\n### Durbin Watson Test\n\nUnder Classical assumptions, one could also use the DW statistic:\n\n$$DW = \\frac{\\sum_{t=2}^T (u_t-u_{t-1})^2}{\\sum_{t=1}^T u_t^2}$$\n\nwhere $DW\\simeq 2(1-\\hat{\\rho})$. \n\n- if there is no serial correlation, then $DW\\simeq 2$.\n- If there is possitive serial correlation, then $DW<2$.\n- A less practical test, but valid in small samples\n\n",
    "supporting": [
      "13_advtimeseries_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}