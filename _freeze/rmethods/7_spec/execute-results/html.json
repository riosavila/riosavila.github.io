{
  "hash": "1a13e5e3ecb6766f5aa1824f2d995d1c",
  "result": {
    "markdown": "---\ntitle: Multiple Regression Analysis\nsubtitle: 'Specification and Data Issues: A1 how could you!'\nauthor: Fernando Rios-Avila\nformat:\n  revealjs:\n    slide-number: true\n    width: 1600\n    height: 900\n    code-fold: true\n    echo: true\n    css: styles.css\n    chalkboard: true\n---\n\n## What do we mean with model miss-specification\n\n- There are various kinds of model specification we will talk about.\n  - There are important variables you did not include in your model: Endogeneity\n  - You added all relevant variables...just not in the right way. \n  - You added proxies for variables you had no access to (Question change)\n  - You have all relevant data, but with errors.\n  - You have some missing data\n\n# Functional Form Misspecification\n\n## \n\n- Simple linear functions work in almost ALL cases. They can be thought as first order Taylor expansions:\n$$\\begin{aligned}\ny &= f(x) + e \\\\\nf(x) &\\simeq f(x_0) \n+\\frac{\\partial f(x)}{\\partial x}|_{x=x_0}\n(x-x_0)+R+e \\\\\nf(x) &\\simeq \\color{red}{ f(x_0)}\n\\color{red}{-\\frac{\\partial f(x)}{\\partial x}|_{x=x_0} x_0}\n+\\frac{\\partial f(x)}{\\partial x}|_{x=x_0} x+R+e \\\\\ny &= \\color{red}{\\beta_0}+\\beta_1 x + R+ e\n\\end{aligned}\n$$\n\nSo, for \"reasonable\" values of X, or when analyzing average marginal effects $R$ should be small enough to be ignored.\n\n- In other words, for Overall effects Simple linear model works reasonably well! (most of the time)\n    \n## {.scrollable}\n\n- If you are interested in individuals (or alike people), you may need flexiblity!\n  \n- Ignoring functional form misspecification imposes unwanted assumptions (homogeneity), that could create further problems.\n    - Specially if data is skewed\n\n- But how flexible is flexible enough?\n\n    - We will only consider quadratic terms and interactions,\n    - but there is a large literature on making very flexible estimations (non-paramatric analysis)\n\n::: {#d633668a .cell execution_count=1}\n``` {.stata .cell-code}\nclear\nset seed 10\nset obs 1000\ngen p = (2*_n-1)/(2*_N) \ngen x = invchi2(5, p)/2\ngen y = 1 + x + (x-2.5)^2 + rnormal()  \nreg y x\ndisplay \"Quadratic\"\nqui:reg y c.x##c.x\nmargins, dydx(x)\ndisplay \"Cubic\"\nqui:reg y c.x##c.x##c.x\nmargins, dydx(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of observations (_N) was 0, now 1,000.\n\n      Source |       SS           df       MS      Number of obs   =     1,000\n-------------+----------------------------------   F(1, 998)       =   1287.16\n       Model |  22189.0552         1  22189.0552   Prob > F        =    0.0000\n    Residual |  17204.2788       998  17.2387563   R-squared       =    0.5633\n-------------+----------------------------------   Adj R-squared   =    0.5628\n       Total |  39393.3339       999  39.4327667   Root MSE        =     4.152\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |   2.983973   .0831722    35.88   0.000      2.82076    3.147185\n       _cons |  -1.467351   .2458875    -5.97   0.000    -1.949866   -.9848348\n------------------------------------------------------------------------------\nQuadratic\n\nAverage marginal effects                                 Number of obs = 1,000\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  x\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |   1.033401   .0258934    39.91   0.000     .9825894    1.084213\n------------------------------------------------------------------------------\nCubic\n\nAverage marginal effects                                 Number of obs = 1,000\nModel VCE: OLS\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  x\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           x |   1.041387   .0292892    35.56   0.000     .9839117    1.098863\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n## Reset Ramsey test\n\n- Intuition: If the model is misspecified, perhaps we need to control for more non-linearities and interactions.\n- Naive test: Add more controls (quadratics and interactions) (like White test, this will grow fast)\n- Reset - Ramsey test: Get predictions from original model, and add it as control\n\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\delta_1 \\hat y^2 + \\delta_2 \\hat y^3 +e\n$$\n\n$H_0: \\delta_1 = \\delta_2 = 0$: (everything is awesome)\n\n$H_1: H_0$ is false: we need to fix the problem \n\n- RRT does not tell you \"How\" to fix the problem.\n  \n```stata\nestat ovtest\n```\n(bad name tho)\n\n## Davidson-MacKinnon test\n\nTwo non-tested models:\n\n$$\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + e \\\\\ny &= \\gamma_0 + \\gamma_1 log(x_1) + \\gamma_2 log(x_2) + e \\\\\n\\end{aligned}\n$$\n\n- Which one is more appropriate? eq1? or eq2? This are non-nested models, so its difficult to say.\n  - You could nest them:\n\n$$y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 log(x_1) + \\theta_4 log(x_2) + e \n$$\n\n  and test $\\theta_1=\\theta_2=0$ or $\\theta_3=\\theta_4=0$.\n\n##\n\n- or the \"true\" Davidson-MacKinnon test:\n  - First Obtain predictions from competing models:\n$$\\begin{aligned}\n\\hat y &= \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2 x_2 \\\\\n\\check y &= \\hat \\gamma_0 + \\hat\\gamma_1 log(x_1) + \\hat\\gamma_2 log(x_2) \\\\\n\\end{aligned}\n$$\n\n  - Then add the predictions as added controls in the alternative model:\n$$\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\theta_1 \\check y +e \\\\\ny &= \\gamma_0 + \\gamma_1 log(x_1) + \\gamma_2 log(x_2) + \\theta_1 \\hat y + e \\\\\n\\end{aligned}\n$$\n\n- Unfortunately, you may ended up with conflicting results.\n\n# Proxy Variables \nFor unobserved variables\n\n## A re-tell of Omitted variable Bias\n\n- We know this. If a variable that SHOULD be in the model is not added, it will generate an OMV, unless it was uncorrelated to the model error. \n  - Lesson: add important variables!\n- What if those variables are not available? how do you solve the problem?\n  - IV (we will talk about that later) or\n  - Proxy Variable (a bandaid)\n\n## Proxies\nConsider:\n$$log(wages) = \\beta_0 + \\color{blue}{\\beta_1} exper + \\color{blue}{\\beta_2} educ + \\beta_3 skill + e\n$$\n\nWhere you are really interested in $\\beta_1 \\And \\beta_2$.\n\n- Since we dont have $skill$, and omitting it will bias our coefficients, we can use a proxy $ASVAB$.\n\n$$log(wages) = \\beta_0 + \\color{blue}{\\beta_1} exper + \\color{blue}{\\beta_2} educ + \\gamma_3 ASVAB + e\n$$\n\n- and done?\n\n##\n\nUsing a Proxy will work only under the following condition:\n\n- Conditioning on the observed variable and proxy, the unobserved variable **has** to be uncorrelated to other variables in the model:\n  \n$$\\begin{aligned}\nE(x_3^*|x_1,x_2,x_3)&=\\alpha_0 + \\alpha_1 x_3 \\\\\nE(skill|exper,educ,ASVAB)&=\\alpha_0 + \\alpha_1 ASVAB \n\\end{aligned}\n$$\n\nIf this happens, you can still estimate $\\beta_1 \\And \\beta_2$, although the constant and slope of the proxy varible will be biased for the proxied variable.\n\n$$\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x^*_3 + e \\ ; \\ \n\\color{blue}{x^*_3 =  \\delta_0 + \\delta_1 x_3 + v} \\\\\ny &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (\\delta_0 + \\delta_1 x_3 + v) + e \\\\\n &= \\color{brown}{\\beta_0 +\\beta_3\\delta_0} \\color{black}{+ \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 \\delta_1 x_3 +} \\color{green}{\\beta_3 v + e} \\\\\n &=\\color{brown}{\\alpha_0} + \\beta_1 x_1 + \\beta_2 x_2 + \\alpha_1 x_3 + \\color{green}{u} \\\\\n\\end{aligned}\n$$\n\n## {.scrollable}\n### What about Lags (of dep variable)?\n\n- Increses Data requirements (panel? pseudo panel?)\n- Further assumptions are required (Past exogenous of present)\n- But allows controlling for underlying factors or historical factors\n\n::: {#5f655e24 .cell execution_count=2}\n``` {.stata .cell-code}\nfrause crime2, clear\nqui:reg crmrte unem llawexpc if year == 87\nest sto m1\nqui:reg crmrte unem llawexpc lcrmrt_1 if year == 87\nest sto m2\nqui:reg ccrmrte unem llawexpc if year==87  \nest sto m3\nesttab m1 m2 m3, se star(* .1 ** 0.05 *** 0.01) b(3) ///\nmtitle(crimert crimert change_crrt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                  crimert         crimert     change_crrt   \n------------------------------------------------------------\nunem               -3.659           0.346          -0.125   \n                  (3.471)         (2.127)         (2.152)   \n\nllawexpc           16.452         -20.059*        -10.377   \n                 (18.531)        (11.842)        (11.487)   \n\nlcrmrt_1                          127.111***                \n                                 (14.399)                   \n\n_cons              10.655        -337.106***       79.288   \n                (134.223)        (89.507)        (83.200)   \n------------------------------------------------------------\nN                      46              46              46   \n------------------------------------------------------------\nStandard errors in parentheses\n* p<.1, ** p<0.05, *** p<0.01\n```\n:::\n:::\n\n\nNote: Skip 9-2c and 9-3\n\n# Measurement error\n\n## Why is $X$ not the real $X$?\n\n- Often we treat data as if it they were perfect measures of the true data. But is that the case? \n  - Age: Do you report age in years, months, days, hours, minutes, etc\n  - Weight and Height: Even if measured, how accurate it can be? and do they make mistakes?\n  - Income: Do people report income accurately? or they Lie? why?\n\n- Depending on the type of error, magnitude, and if the affected variable is dep or indep, it may have diffrent consequences for OLS.\n\n- For now we will concentrate on a specific kind of measurement error: Classical measurement error\n\n$$\\begin{aligned} \ny_{obs} &= y_{true} + \\varepsilon \\\\\nE(\\varepsilon) &=0; cov(\\varepsilon,y_{true})=0; cov(\\varepsilon,X's)=0\n\\end{aligned}\n$$\n\n## Error in $y$ (dep variable) {.scrollable}\n\n- Instead of: $y^* = x\\beta + e$\n\n- We estimate $y^*+\\varepsilon = x\\beta + e \\rightarrow y^* = x\\beta + e-\\varepsilon$\n\n- This implies that $\\beta's$ can still be **unbiased** when applying OLS.\n\n- However **variance** will be larger than when using true data:\n\n::: {#f0933da5 .cell execution_count=3}\n``` {.stata .cell-code}\nqui: frause oaxaca, clear\nset seed 101\ngen lnwage2=lnwage + rnormal(2) \nqui:reg lnwage educ exper female\nest sto m1\nqui:reg lnwage2 educ exper female\nest sto m2\nesttab m1 m2, se\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(213 missing values generated)\n\n--------------------------------------------\n                      (1)             (2)   \n                   lnwage         lnwage2   \n--------------------------------------------\neduc               0.0858***       0.0902***\n                (0.00521)        (0.0120)   \n\nexper              0.0147***       0.0171***\n                (0.00126)       (0.00291)   \n\nfemale            -0.0949***      -0.0759   \n                 (0.0251)        (0.0580)   \n\n_cons               2.219***        4.132***\n                 (0.0687)         (0.159)   \n--------------------------------------------\nN                    1434            1434   \n--------------------------------------------\nStandard errors in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n:::\n:::\n\n\n## Error in $X$ (indep variable) {.scrollable}\n\n- Instead of: $y = \\beta_0 + \\beta_1 x^* + e$\n- We estimate $y = \\gamma_0 + \\gamma_1 (x^* + \\varepsilon) + v$\n  \n- By adding an error $\\varepsilon$ that has a zero relationship with $y$, the \"average\" coefficient $\\gamma_1$ will be between the true $\\beta_1$ and 0.\n$$\\begin{aligned}\n\\gamma_1 &=\\frac{\\sum (y-\\bar y)(x^* + \\varepsilon - \\bar x)}{\\sum (x^* + \\varepsilon - \\bar x)^2} =\\frac{\\sum (y-\\bar y)(x^* - \\bar x)+ \\sum (y-\\bar y) \\varepsilon}{\\sum (x^* - \\bar x)^2 + \\sum \\varepsilon^2} \\\\\n &= \\frac{\\sum (y-\\bar y)(x^* - \\bar x)}{\\sum (x^* - \\bar x)^2 + \\sum \\varepsilon^2} \\frac{\\sum (x^* - \\bar x)^2}{\\sum (x^* - \\bar x)^2} \\\\\n & =\\beta_1 \\frac{\\sigma^2_x}{\\sigma^2_x + \\sigma^2_\\varepsilon}\n\\end{aligned}\n$$\n\n## \n\n::: {#3cc7dab5 .cell execution_count=4}\n``` {.stata .cell-code}\nfrause oaxaca, clear\nqui:sum educ\ngen educ_error = educ + rnormal()*r(sd)\nsum educ educ_error\nqui:reg lnwage educ\nest sto m1\nqui:reg lnwage educ_error\nest sto m2\nesttab m1 m2, se\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Excerpt from the Swiss Labor Market Survey 1998)\n\n    Variable |        Obs        Mean    Std. dev.       Min        Max\n-------------+---------------------------------------------------------\n        educ |      1,647    11.40134    2.374952          5       17.5\n  educ_error |      1,647    11.36352    3.400767   .6707422   26.90462\n\n--------------------------------------------\n                      (1)             (2)   \n                   lnwage          lnwage   \n--------------------------------------------\neduc               0.0800***                \n                (0.00539)                   \n\neduc_error                         0.0399***\n                                (0.00395)   \n\n_cons               2.434***        2.898***\n                 (0.0636)        (0.0475)   \n--------------------------------------------\nN                    1434            1434   \n--------------------------------------------\nStandard errors in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n:::\n:::\n\n\n# Missing Data, Nonrandom samples, and outliers\n\n## Missing Data (Assume Sample is complete)\n\n- What is it? you dont have data! Your $N$ falls.\n  - Some data for some observations are missing.\n  - We may or may not know why they are missing\n  - and they maybe missing at random, or following unknown patterns.\n\n- If we are Missing data, and we do not know why, its a problem. We cant know if the sample represents the population, thus cannot be used for analysis.\n\n##\n### How to deal with it?\n\n- if Missing **completely** at random (MCAR), analysis can be done as usual (no effects except smaller N)\n- if Missing **at** random (MAR), the analysis can be done, often using standard methods:\n  - Missingness depends on observed factors ($X's$).\n  - It is also known as exogenous sample selection.\n  - **Intuitively**, because all factors that determine selection are exogenous, you can identify who in the population is identified (Regression for men, women, high education, etc)\n- If Missing **not** at random (MNAR), you cant address the problem with standard analysis.\n  - Some methods such as Heckman selection or truncated regression, could be used. (advanced)\n  - Other wise, you can't analyze the data (in a satisfactory manner)\n  - **Intuitively**, missingness is determined by unobserved factors, which also determines the outcome. (ie Analyze high wage population only)\n\n## Outliers and influencers\n\n- Not all data is made equal, and not all data has the same weight when estimating regressions.\n\n- Observations with high Influence are those with outliers based on the conditional distribution ($y|x$).\n- \n  - While outliers are not necessarily bad for analysis, it is important to understand how sensitive your results are to excluding some observations.\n\n- Observations with high **leverage** are those with unusual characteristics.($X's$)\n\n- Combination of both may have strong impacts on the regression analysis.\n\n##\n\n- Leverage of an observation is determined by the following:\n\nDefine $H = X(X'X)^{-1}X'$\n\nLeverage $h_i = H[i,i]$\n\nHigh $h_i$ denotes more influence in the model. (sensitive)\n\n- Influence is typically detected based on \"studentized\" residuals\n  \n$$r_i =  \\frac{\\hat e}{s_{-i}\\sqrt{1-h_i}}\n$$\n\n## Example\n\n::: {#7021947e .cell execution_count=5}\n``` {.stata .cell-code}\nqui:{\nfrause oaxaca, clear\ndrop if lnwage==.\nreg lnwage educ exper tenure female age\npredict lev, lev\nsum lev, meanonly\nreplace lev=lev/r(mean)\npredict rst, rstud\n}\nset scheme white2\ncolor_style tableau\nscatter lev rst\n```\n\n::: {.cell-output .cell-output-display}\n![](7_spec_files/figure-revealjs/cell-6-output-1.png){}\n:::\n:::\n\n\n## Solutions\n\n- The problem with OLS is that it provides \"too much weight\" to outliers.\n\n- This is similar to the mean, which may not be very stable with extreme distributions. \n\nThere are at least two solutions to problems with outliers.\n\n- Robust Regression (different from regression with robust Standard errors)\n  - The idea is to penalize outliers, to reduce the impact on the estimated coefficients.\n\n## \n\n- Quantile (median) Regression\n  - Modifies the objective function to be minized:\n  \n$$\\beta's=\\min_\\beta \\sum |y-x\\beta|\n$$\n\n- Instead of using the squared of errors, it uses the absolute value. \n  - by doing this, coefficients are not sensitive to outliers! (as the median is better than the mean to capture typical values)\n  - Drawbacks: Its slower than OLS, and it can be difficult to interpret\n  \n```stata{style=\"font-size: 1.3em\"}\nrreg <- Robust Regression\nqreg <- Quantile Regression\n```\n\n# Done for now\nNext week Midterm!\nand after that Limited Dep Variables\n\n",
    "supporting": [
      "7_spec_files"
    ],
    "filters": [],
    "includes": {}
  }
}