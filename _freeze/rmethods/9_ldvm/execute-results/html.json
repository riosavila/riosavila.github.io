{
  "hash": "4b4953f8262f25f3195cbcd1b588a7de",
  "result": {
    "markdown": "---\ntitle: Limited Dependent Variable Models\nsubtitle: MLE-Mua ha ha ha\nauthor: Fernando Rios-Avila\nformat:\n  revealjs:\n    slide-number: true\n    width: 1600\n    height: 900\n    code-fold: true\n    code-overflow: wrap\n    echo: true\n    css: styles.css\n    chalkboard: true\n---\n\n## What do we mean Limited??\n\n::: {#617d7cc6 .cell execution_count=1}\n``` {.stata .cell-code}\nclear\nset obs 2000\ngen r1 = runiform()\ngen r2 = rchi2(5)/5 \ngen r3 = round(rchi2(3))*3\ngen r4 = rnormal()\nset scheme white2\ncolor_style tableau\nhistogram r1, name(m1, replace) \nhistogram r2, name(m2, replace)\nhistogram r3, name(m3, replace) width(1)  \nhistogram r4, name(m4, replace)\ngraph combine m1 m2 m3 m4\ngraph export images/fig9_1.png, width(1000) replace\n```\n:::\n\n\n![Limited Dependent variables](images/fig9_1.png)\n\n##\n\n### What do we mean Limited?? {.middle}\n\n- When we think about \"limited dependent variable\" models, we refer to models when the distribution of the dep.variable is \"limited\"\n  - In other words. The values it can take are restricted! (positive, or only integer), within a range, etc\n\n- Can you still use LRM for them? \n- Will anything change if you do?\n- Do we care?\n\n## No we dont, but..\n\n- We dont really care. In fact we have already use LRM on that fashion:\n  - LPM: Dep variable was a Dummy\n  - Wages: Always positive\n  - \\# Children: Countable\n\n- But, there are couple of things one should consider.\n  1. Models of this kind are usually heteroskedastic by construction. (robust? Weighted?)\n  2. Predictions could made no sense. \n  3. There are better models we could use to analyze the data\n\n**Better** under some assumptions\n\n- However, this models cannot be estimated using OLS (there is no \"close form solution\")\n- We ***may*** need to learn a new method: Maximum Likelihood \n\n\n## Probits and Logits\n\n- **LPM** are easy, fast, and good for most data analysis (exploration). But they have some limitations.\n- Most limitations can be overcome with alternative models: Logit or Probit\n- In constrast with LPM (which aims to explain individual outcomes), Logit/probit aims to explain Conditional Probabilities:\n\n$$p(y=1|x) = G(x\\beta)$$\n\n- where the function $G()$ makes sure the predicted outcome is always between 0 and 1. \n- Caveat: Because $G()$ is nonlinear, this is a nonlinear model, and marginal effects are harder to estimate.\n\n##\n\n### What to use for $G()$\n\n- Two leading options:\n\n$$logit: G(x\\beta) = \\frac{\\exp{x\\beta}}{1+\\exp{x\\beta}}$$\n$$probit: G(x\\beta) = \\Phi(x\\beta)=\\int_{-\\infty}^{x\\beta}\\phi(z)dz$$\n\n- But in practice Either will work. Then why the difference?\n\n## \n\n### Probits and Logits: Latent variables\n\n- It all comes down to the Latent variable!\n\n- Assumption: \n  - Everybody has a latent score on every \"binary\" decision: The value to a decision $y^*$\n    $$y^* = x\\beta + e $$\n\n  - If $y^*$ is above certain threshold ($y^*>0$), you \"do\" something ($y=1$). If not you dont ($y=0$).\n- Thus the choice between logit and probit depends on the distribution of $e$.\n  - $e$ is normal, then probit\n  - $e$ is logistic, then logit\n\n\n##\n\n### Some Math\n\nLatent Model:\n\n$$ y^* = x\\beta + e $$\n\nWe aim to measure the probablity of a positive latent. \n\n$$\\begin{aligned}\nP(y^*>0|x) & = P(x\\beta + e>0|x) \\\\\n& = P( e>- x\\beta|x) \\\\\n& = 1 - P( e < - x\\beta|x) = 1-G( - x\\beta|x) \\\\\n& = G(x\\beta)\n\\end{aligned}\n$$\n\nlast step valid only if $G()$ is symetrical. \n\n\n##\n\n###  Marginal Effects?\n\n- Same as before. The partial derivative!\n\n$$\\begin{aligned}\np(y=1|x) &= G(\\beta_0 + \\beta_1 x_1 +\\beta_2 x_2 ) \\\\\n\\frac{\\partial p(y=1|x)}{\\partial x_1} = G'(x\\beta)\\beta_1=g(x\\beta)\\beta_1\n\\end{aligned}\n$$\n\n- But if variables are dummies, we need to estimate true effect.\n\n$$\\begin{aligned}\np(y=1|x) &= G(\\beta_0 + \\beta_1 x_1 +\\beta_2 D_2 ) \\\\\n\\frac{\\partial p(y=1|x)}{\\partial D_2} = G(\\beta_0 + \\beta_1 x_1 +\\beta_2 )-G(\\beta_0 + \\beta_1 x_1 )\n\\end{aligned}\n$$\n\nand yes, you could also have interactions, polynomials, etc\n\n## MLE: How does this work?\n\n- MLE: Maximum Likelihood Estimator, is an alternative method to OLS that allows you to estimate parameters in nonlinear models.\n- The idea of the method is to \"model\" the conditional distribution of the data $F(y|x,\\theta)$ or $f(y|x,\\theta)$, assuming $X's$ are given and modifying values of $\\theta$ (distribution parameters).\n\n- $LRM$ **could** be estimated via MLE, but you will need More assumptions:\n  - The error $e$ is normal.\n- Then \"simply\" find the parameters for the mean and variance that \"maximizes\" the probability that data Comes a given distribution.\n\n- In the case of Probit/logit, there is \"only\" one paramter we need to identify. The conditional probabilty $p(y=1|X)$.\n  - Except that we allow this to vary by $X$\n\n## Likelihood function for Logit/probit\n\n$$L_i = G(x\\beta)^{y=1}*(1-G(x\\beta))^{y=0} \n$$\n\nUnder Independence:\n\n$$L_D = L_1 \\times L_2 \\times \\dots L_N\n$$\n\nThus we need to find the $\\beta's$ that make $L_D$ the largest.\n\nBut because we like sums over products:\n\n$$LL_D = \\sum_{i=1}^N log(L_i)\n$$\n\n##\n\n::: {#c040d68b .cell execution_count=2}\n``` {.stata .cell-code}\n  clear\n  set obs 25\n  gen r = runiform()<.7\n  mata: \n    r = st_data(.,\"r\")\n    ll = J(99,2,0)\n    for(i=1;i<=99;i++){\n      theta = i/100\n      // Log Properties\n      ll[i,]= theta,exp(sum(log(theta:^(r:==1) :* (1-theta):^(r:==0))))\n    }\n  end\n  qui getmata ll*=ll , force\n  ren ll1 theta\n  ren ll2 likelihood\n  *scatter likelihood theta \n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of observations (_N) was 0, now 25.\n\n```\n:::\n:::\n\n\n::: {#26a93be4 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](9_ldvm_files/figure-revealjs/cell-4-output-1.png){}\n:::\n:::\n\n\n## Testing?\n\n- You can test two things:\n  - Test coefficients ($\\beta$)\n  - Test marginal effects ($G'(x\\beta)\\beta$)\n- Both test will most likely agree with each other, but some contradictions may arise.\n### How? \n- z-test and/or Wald test: Similar to t-test and Joint F-test we cover before. But, we now make the assumption of normality (not t-distribution)\n- Log-Likelihood test. Similar to F-test for restricted and unrestricted model:\n\n  - Estimate both Restricted and unrestricted model. And obtain their Log Likelihoods ($\\mathcal{L}_ur$) and ($\\mathcal{L}_r$).\n\n  $$LR = 2 (\\mathcal{L}_ur-\\mathcal{L}_r) \\overset{a}\\sim \\chi^2_q$$\n\n## Stata - Example {.scrollable}\n\n::: {#247bea5d .cell .larger execution_count=4}\n``` {.stata .cell-code}\nfrause mroz, clear\n* LPM with Robust Standard errors\nqui:reg inlf nwifeinc educ exper expersq age kidslt6 kidsge6, robust\nest sto m1\nqui:logit inlf nwifeinc educ exper expersq age kidslt6 kidsge6, \nest sto m2a\nqui:margins, dydx(*) post\nest sto m2b\nprobit inlf nwifeinc educ exper expersq age kidslt6 kidsge6, \nest sto m3a\nqui:margins, dydx(*) post\nest sto m3b\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nIteration 0:   log likelihood =  -514.8732  \nIteration 1:   log likelihood = -402.06651  \nIteration 2:   log likelihood = -401.30273  \nIteration 3:   log likelihood = -401.30219  \nIteration 4:   log likelihood = -401.30219  \n\nProbit regression                                       Number of obs =    753\n                                                        LR chi2(7)    = 227.14\n                                                        Prob > chi2   = 0.0000\nLog likelihood = -401.30219                             Pseudo R2     = 0.2206\n\n------------------------------------------------------------------------------\n        inlf | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    nwifeinc |  -.0120237   .0048398    -2.48   0.013    -.0215096   -.0025378\n        educ |   .1309047   .0252542     5.18   0.000     .0814074     .180402\n       exper |   .1233476   .0187164     6.59   0.000     .0866641    .1600311\n     expersq |  -.0018871      .0006    -3.15   0.002     -.003063   -.0007111\n         age |  -.0528527   .0084772    -6.23   0.000    -.0694678   -.0362376\n     kidslt6 |  -.8683285   .1185223    -7.33   0.000    -1.100628    -.636029\n     kidsge6 |    .036005   .0434768     0.83   0.408     -.049208    .1212179\n       _cons |   .2700768    .508593     0.53   0.595    -.7267473    1.266901\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n::: {#716c5602 .cell execution_count=5}\n``` {.stata .cell-code}\nset linesize 255\n*| classes: larger\ndisplay \"Prob Models\"\nesttab m1 m2a m2b m3a m3b, scalar(r2 ll) cell(b(fmt(%5.3f)) ///\nse(par([ ])) p( par(( )) ) )  gap  mtitle(LPM Logit Logit-mfx Probit Probit-mfx)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProb Models\n\n-----------------------------------------------------------------------------\n                      (1)          (2)          (3)          (4)          (5)\n                      LPM        Logit    Logit-mfx       Probit   Probit-mfx\n                   b/se/p       b/se/p       b/se/p       b/se/p       b/se/p\n-----------------------------------------------------------------------------\nmain                                                                         \nnwifeinc           -0.003       -0.021       -0.004       -0.012       -0.004\n                  [0.002]      [0.008]      [0.001]      [0.005]      [0.001]\n                  (0.026)      (0.011)      (0.010)      (0.013)      (0.012)\n\neduc                0.038        0.221        0.039        0.131        0.039\n                  [0.007]      [0.043]      [0.007]      [0.025]      [0.007]\n                  (0.000)      (0.000)      (0.000)      (0.000)      (0.000)\n\nexper               0.039        0.206        0.037        0.123        0.037\n                  [0.006]      [0.032]      [0.005]      [0.019]      [0.005]\n                  (0.000)      (0.000)      (0.000)      (0.000)      (0.000)\n\nexpersq            -0.001       -0.003       -0.001       -0.002       -0.001\n                  [0.000]      [0.001]      [0.000]      [0.001]      [0.000]\n                  (0.002)      (0.002)      (0.001)      (0.002)      (0.001)\n\nage                -0.016       -0.088       -0.016       -0.053       -0.016\n                  [0.002]      [0.015]      [0.002]      [0.008]      [0.002]\n                  (0.000)      (0.000)      (0.000)      (0.000)      (0.000)\n\nkidslt6            -0.262       -1.443       -0.258       -0.868       -0.261\n                  [0.032]      [0.204]      [0.032]      [0.119]      [0.032]\n                  (0.000)      (0.000)      (0.000)      (0.000)      (0.000)\n\nkidsge6             0.013        0.060        0.011        0.036        0.011\n                  [0.014]      [0.075]      [0.013]      [0.043]      [0.013]\n                  (0.337)      (0.422)      (0.421)      (0.408)      (0.407)\n\n_cons               0.586        0.425                     0.270             \n                  [0.152]      [0.860]                   [0.509]             \n                  (0.000)      (0.621)                   (0.595)             \n-----------------------------------------------------------------------------\nN                     753          753          753          753          753\nr2                  0.264                                                    \nll                 -423.9       -401.8                    -401.3             \n-----------------------------------------------------------------------------\n```\n:::\n:::\n\n\n::: {#5d04e7c6 .cell .larger execution_count=6}\n``` {.stata .cell-code code-fold=\"false\"}\ndisplay \"LR test\"\nqui:probit inlf nwifeinc educ exper expersq age kidslt6 kidsge6 motheduc fatheduc, \nest sto unrestricted\nqui:probit inlf nwifeinc educ exper expersq age kidslt6 kidsge6 , \nest sto restricted\nlrtest unrestricted restricted\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLR test\n\nLikelihood-ratio test\nAssumption: restricted nested within unrestricted\n\n LR chi2(2) =   0.29\nProb > chi2 = 0.8668\n```\n:::\n:::\n\n\n# Break? (5 mins!)\n\n## Censored and Truncated Data\n\n- Logits and Probits, are not the only models that require MLE for estimation. \n  - Among Discrete data models, you also have ologit/oprobit for ordered responses. mlogit/mprobit for unordered ones. Extends on logit/probit.\n- There are other interesting cases:\n  - When Data is censored.\n  - When Data is truncated.\n\n## Three Cases\n\n:::{.panel-tabset}\n\n## Case 1\n\n- $y$ is \"conditionally-normal\" and is Fully Observed.\n- You can estimate the model using OLS or ML\n\n::: {#6d7ac9ef .cell execution_count=7}\n``` {.stata .cell-code}\nqui:{\n  clear\n  set obs 999\n  gen p   = _n/(_N+1)\n  gen fob = invnormal(p)\n}\nqui:histogram fob\n```\n\n::: {.cell-output .cell-output-display}\n![](9_ldvm_files/figure-revealjs/cell-8-output-1.png){fig-align='center'}\n:::\n:::\n\n\n## Case 2\n\n- Data is observed for everyone, but is \"censored\" for some. `tobit`\n  - Either corner solution (how many hours you study) or Recoded: $y_{obs} = max(c,y^*)$\n\n::: {#8d4153e6 .cell execution_count=8}\n``` {.stata .cell-code}\nqui: replace fob = -2 if fob<-2\nqui:histogram fob, xlabel(-4 (2) 4)\n```\n\n::: {.cell-output .cell-output-display}\n![](9_ldvm_files/figure-revealjs/cell-9-output-1.png){fig-align='center'}\n:::\n:::\n\n\n## Case 3\n\n- Below (or above) some threshold, you do not have information on $y$. `truncreg` \n$$y_{obs} = y^* \\text{ if } y^*>c$$\n\n::: {#28e0b776 .cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](9_ldvm_files/figure-revealjs/cell-10-output-1.png){fig-align='center'}\n:::\n:::\n\n\n:::\n\n## Estimation: Censored and Corner Solution\n\nIf data is censored or corner solution the estimation strategy is based on:\n\n$$\\begin{aligned}\nL_i &= \\frac{1}{\\sigma} \\phi\\left( \\frac{y-x\\beta}{\\sigma} \\right) \\text{ if } y>c \\\\\n    &= 1-\\Phi\\left(\\frac{x\\beta}{\\sigma} \\right) \\text{ if } y\\leq c \\\\\n\\end{aligned}\n$$\n\nIf data is truncated, we need to \"adjust\" the distribution of what is observed\n\n$$\\begin{aligned}\nL_i &= \\frac{1}{\\Phi\\left( x\\beta/\\sigma \\right)} \\frac{1}{\\sigma} \\phi\\left( \\frac{y-x\\beta}{\\sigma} \\right) \\text{ if } y>c \\\\  \n\\end{aligned}\n$$\n\nWe will put -truncated regression- on the side for now. But see [here](https://stats.oarc.ucla.edu/stata/output/truncated-regression/) for an example.\n\n## Interpretation: It depends!\n\n- What are you interested in analyzing? and what type of data you have?  \n\n:::{.panel-tabset}\n\n## Latent variable\n\n- Easiest Case. Just need to consider the coefficients (as in LRM)\n\n$$\n\\begin{aligned}\nE(y^*|x) &= x\\beta \\\\\n\\frac{\\partial E(y^*|x)}{\\partial x } &= \\beta_x\n\\end{aligned}\n$$\n\n- The same applies if model was censored.\n\n## $P(y>0|x)$ \n\n- Its an alternative approach to Probit models, where you are interest in analyzing why is data Not censored, or why is it above some threshold. (why people work)\n\n- Extensive margin effect.\n$$\n\\begin{aligned}\nP(y>0|x) &= \\Phi\\left(\\frac{x\\beta}{\\sigma}\\right) \\\\\n\\frac{\\partial P(y>0|x)}{\\partial x } &= \\frac{\\beta_x}{\\sigma} \\phi\\left(\\frac{x\\beta}{\\sigma}\\right)\n\\end{aligned}\n$$\n\nNote: Coefficients $\\beta$ need to be Standardized.\n\n## $E(y|y>0,x)$ \n\n- If corner solution, one may be interested in the effect of those with positive outcomes only. \n\n- This is the intensive margin effect.\n$$\n\\begin{aligned}\nE(y|y>0,x) &= x\\beta + \\sigma \\frac{\\phi(x\\beta / \\sigma )}{\\Phi(x\\beta / \\sigma )} \\\\\n\\frac{\\partial E(y|y>0,x)}{\\partial x } &= \\beta_x\n\\left[ 1-\\frac{\\phi(x\\beta / \\sigma )}{\\Phi(x\\beta / \\sigma )} \\left( \\frac{x\\beta }{\\sigma }+ \\frac{\\phi(x\\beta / \\sigma )}{\\Phi(x\\beta / \\sigma )}\\right) \\right]\n\\end{aligned}\n$$\n\n## $E(y|x)$ \n\n- In this case, one may be interested in estimating the expected effect on everyone.\n- Combines both Intensive and extensive margin effects. Comparable to OLS.\n\n$$\n\\begin{aligned}\nE(y|x) &= p(y>0|x)*E(y|y>0,x) + (1-p(y>0|x))*0 \\\\\n\\frac{\\partial E(y|x)}{\\partial x } &= \\beta_x \\Phi(x\\beta)\n\\end{aligned}\n$$\n\n:::\n\n## Example {.scrollable}\n\n::: {#53f5357b .cell execution_count=10}\n``` {.stata .cell-code}\nfrause mroz, clear\nqui:tobit hours nwifeinc educ c.exper##c.exper   age kidslt6 kidsge6 , ll(0)\nqui:emargins, dydx(*) estore(m1)\nqui:emargins, dydx(*) predict(p(0,.)) estore(m2)\nqui:emargins, dydx(*) predict(e(0,.)) estore(m3)\nqui:emargins, dydx(*) predict(ystar(0,.)) estore(m4)\nesttab m1 m2 m3 m4, mtitle(Latent P(y>0) E(y|y>0) E(y) ) b(3) se\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                   Latent          P(y>0)        E(y|y>0)            E(y)   \n----------------------------------------------------------------------------\nnwifeinc           -8.814*         -0.002*         -3.969*         -5.189*  \n                  (4.459)         (0.001)         (2.008)         (2.621)   \n\neduc               80.645***        0.022***       36.312***       47.473***\n                 (21.583)         (0.006)         (9.703)        (12.621)   \n\nexper              91.929***        0.026***       37.593***       48.793***\n                  (7.997)         (0.002)         (2.966)         (3.587)   \n\nage               -54.405***       -0.015***      -24.497***      -32.026***\n                  (7.418)         (0.002)         (3.362)         (4.292)   \n\nkidslt6          -894.020***       -0.246***     -402.551***     -526.278***\n                (111.878)         (0.028)        (50.749)        (64.706)   \n\nkidsge6           -16.218          -0.004          -7.303          -9.547   \n                 (38.641)         (0.011)        (17.404)        (22.752)   \n----------------------------------------------------------------------------\nN                     753             753             753             753   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n:::\n:::\n\n\n## Tobit has problems too\n\n> That simple equation, too much aggregation  \n>    Hayek (in [Fear the Boom and Bust](https://www.youtube.com/watch?v=d0nERTFo-Sk))\n\n- Tobit, when addressing corner solutions, aims to explain two different actions (Engagement and intensity) with the same model. However, this may not be appropriate all the time.\n  - HW-Examples?\n- When this happens, other models may be more appropritate like\n  - two part model: (literally model using two equations)\n  - Hurdle Model (`craggit` or `churdle`)\n- Also...Normality...\n\n## Censored Regression {.scrollable}\n\n- Applies to the same cases as Tobit model. But, it usually refers to Censoring at other points of the distribution (upper censoring? mixed censoring?) \n- Furthermore, applies to cases with different censoring thresholds!\n  - Typical Example, Unemployment duration\n\n::: {#5e8ada0f .cell execution_count=11}\n``` {.stata .cell-code}\nqui:frause recid, clear\ngen lldur = ldurat             // Lower Limit\ngen uudur = ldurat if cens==0  // upper limit = . if censored.\nintreg lldur uudur workprg priors tserved felon alcohol drugs black married educ age\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(893 missing values generated)\n\nFitting constant-only model:\n\nIteration 0:   log likelihood = -2188.8689  \nIteration 1:   log likelihood = -1732.7406  \nIteration 2:   log likelihood = -1680.7927  \nIteration 3:   log likelihood =  -1680.427  \nIteration 4:   log likelihood =  -1680.427  \n\nFitting full model:\n\nIteration 0:   log likelihood = -2116.9831  \nIteration 1:   log likelihood = -1639.9495  \nIteration 2:   log likelihood =  -1597.634  \nIteration 3:   log likelihood = -1597.0592  \nIteration 4:   log likelihood =  -1597.059  \nIteration 5:   log likelihood =  -1597.059  \n\nInterval regression                                 Number of obs     =  1,445\n                                                           Uncensored =    552\n                                                        Left-censored =      0\n                                                       Right-censored =    893\n                                                       Interval-cens. =      0\n\n                                                    LR chi2(10)       = 166.74\nLog likelihood = -1597.059                          Prob > chi2       = 0.0000\n\n------------------------------------------------------------------------------\n             | Coefficient  Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     workprg |  -.0625715   .1200369    -0.52   0.602    -.2978396    .1726965\n      priors |  -.1372529   .0214587    -6.40   0.000    -.1793111   -.0951947\n     tserved |  -.0193305   .0029779    -6.49   0.000    -.0251672   -.0134939\n       felon |   .4439947   .1450865     3.06   0.002     .1596303     .728359\n     alcohol |  -.6349092   .1442166    -4.40   0.000    -.9175686   -.3522499\n       drugs |  -.2981602   .1327356    -2.25   0.025    -.5583171   -.0380033\n       black |  -.5427179   .1174428    -4.62   0.000    -.7729014   -.3125343\n     married |   .3406837   .1398431     2.44   0.015     .0665964    .6147711\n        educ |   .0229196   .0253974     0.90   0.367    -.0268584    .0726975\n         age |   .0039103   .0006062     6.45   0.000     .0027221    .0050984\n       _cons |   4.099386    .347535    11.80   0.000      3.41823    4.780542\n-------------+----------------------------------------------------------------\n    /lnsigma |   .5935864   .0344122    17.25   0.000     .5261398     .661033\n-------------+----------------------------------------------------------------\n       sigma |    1.81047   .0623022                      1.692387    1.936792\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n## Truncated {.scrollable}\n\n- If Data is simply not there, as shown before, one needs to adjust Estimates.\n- marginal effects decisions are similar to Tobit\n\n::: {#a5d0dc5d .cell execution_count=12}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause mroz, clear\nqui:truncreg hours nwifeinc educ c.exper##c.exper   age kidslt6 kidsge6 , ll(0)\nemargins, dydx(*) estore(m1b)\nemargins, dydx(*) predict(e(0,.)) estore(m2b)\nesttab m1 m1b m3 m2b, mtitle(Lat-Tobit Lat-Trunc E(y>0)-Tobit E(y>0)-Trunc ) b(3) se\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nAverage marginal effects                                   Number of obs = 428\nModel VCE: OIM\n\nExpression: Linear prediction, predict()\ndy/dx wrt:  nwifeinc educ exper age kidslt6 kidsge6\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    nwifeinc |   .1534399   5.164279     0.03   0.976    -9.968361    10.27524\n        educ |  -29.85254   22.83935    -1.31   0.191    -74.61684    14.91176\n       exper |   48.00824   8.578316     5.60   0.000     31.19504    64.82143\n         age |  -27.44381   8.293458    -3.31   0.001    -43.69869   -11.18893\n     kidslt6 |  -484.7109   153.7881    -3.15   0.002      -786.13   -183.2918\n     kidsge6 |  -102.6574   43.54347    -2.36   0.018    -188.0011   -17.31379\n------------------------------------------------------------------------------\n\nAverage marginal effects                                   Number of obs = 428\nModel VCE: OIM\n\nExpression: E(hours|hours>0), predict(e(0,.))\ndy/dx wrt:  nwifeinc educ exper age kidslt6 kidsge6\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    nwifeinc |   .1094149   3.682546     0.03   0.976    -7.108243    7.327072\n        educ |  -21.28723   16.25065    -1.31   0.190    -53.13793    10.56346\n       exper |   32.66986   5.277772     6.19   0.000     22.32562    43.01411\n         age |  -19.56962   5.823226    -3.36   0.001    -30.98293   -8.156303\n     kidslt6 |  -345.6374   107.9599    -3.20   0.001    -557.2349   -134.0399\n     kidsge6 |  -73.20291   30.80594    -2.38   0.017    -133.5814   -12.82438\n------------------------------------------------------------------------------\n\n----------------------------------------------------------------------------\n                      (1)             (2)             (3)             (4)   \n                Lat-Tobit       Lat-Trunc    E(y>0)-Tobit    E(y>0)-Trunc   \n----------------------------------------------------------------------------\nnwifeinc           -8.814*          0.153          -3.969*          0.109   \n                  (4.459)         (5.164)         (2.008)         (3.683)   \n\neduc               80.645***      -29.853          36.312***      -21.287   \n                 (21.583)        (22.839)         (9.703)        (16.251)   \n\nexper              91.929***       48.008***       37.593***       32.670***\n                  (7.997)         (8.578)         (2.966)         (5.278)   \n\nage               -54.405***      -27.444***      -24.497***      -19.570***\n                  (7.418)         (8.293)         (3.362)         (5.823)   \n\nkidslt6          -894.020***     -484.711**      -402.551***     -345.637** \n                (111.878)       (153.788)        (50.749)       (107.960)   \n\nkidsge6           -16.218        -102.657*         -7.303         -73.203*  \n                 (38.641)        (43.543)        (17.404)        (30.806)   \n----------------------------------------------------------------------------\nN                     753             428             753             428   \n----------------------------------------------------------------------------\nStandard errors in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n:::\n:::\n\n\n# Break?\n\n## Poisson\n\n- Some times, Data may be non-negative, and/or countable. OLS works well, but we could do better\n\n- With Count data, some data transformations (logs) are not possible, because of the zeroes. \n\n- So instead of assuming $y|x \\sim N(\\mu_x,\\sigma)$, one could assume $y|x \\sim poisson(\\mu_x)$\n\n$$P(y=k,\\mu_x) = \\frac{\\mu_x^k e ^{-\\mu_x}}{k!} \\text{ with } \\mu_x=\\exp(x\\beta)$$\n\n- For a Poisson:\n  - $E(y|x) = \\exp{x\\beta}$ and $Var(y|x) = \\exp{x\\beta}$\n\n- As hinted before, Count data is heteroskedastic. And Poisson assumes some structure to that.\n\n## \n\n- Also convinient that Poisson models are very easy to interpret! (just like Log-lin models)  \n  After estimation:\n\n$$\\frac{\\Delta \\% E(y|x)}{\\Delta x} \\simeq \\beta_x \\times 100 \\text{ or } (\\exp \\beta_x-1)\\times 100 $$\n\n- Other points.\n  - The variance imposed in Poisson is very restrictive. This is a problem for Variance estimation!  \n    Solution: use Robust Standard Errors!\n  - Like LRM, poisson is robust to errors when modeling the conditional mean.\n  - Poisson is a very good alternative for continuous data too (if using Robust SE)\n    - Wage models, trade models\n\n## Example{.scrollable}\n\n::: {#e799eb41 .cell execution_count=13}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause crime1, clear\nqui: reg narr86 pcnv avgsen tottime ptime86 qemp86 inc86 black hispan born60\nest sto m1\nqui:poisson narr86 pcnv avgsen tottime ptime86 qemp86 inc86 black hispan born60, robust\nest sto m2\nqui:emargins, dydx(*) estore(m3)\nesttab m1 m2 m3, se b(3) mtitle(LRM Poisson Poisson-mfx) ///\nkeep(pcnv ptime86  qemp86 inc86 black hispan) label varwidth(20) wrap\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n--------------------------------------------------------------------\n                              (1)             (2)             (3)   \n                              LRM         Poisson     Poisson-mfx   \n--------------------------------------------------------------------\nmain                                                                \nproportion of prior        -0.132**        -0.402***       -0.162***\nconvictions               (0.040)         (0.101)         (0.040)   \n\nmos. in prison             -0.041***       -0.099***       -0.040***\nduring 1986               (0.009)         (0.022)         (0.009)   \n\n# quarters employed,       -0.051***       -0.038          -0.015   \n1986                      (0.014)         (0.034)         (0.014)   \n\nlegal income, 1986,        -0.001***       -0.008***       -0.003***\n$100s                     (0.000)         (0.001)         (0.001)   \n\n=1 if black                 0.327***        0.661***        0.267***\n                          (0.045)         (0.099)         (0.042)   \n\n=1 if Hispanic              0.194***        0.500***        0.202***\n                          (0.040)         (0.092)         (0.038)   \n--------------------------------------------------------------------\nObservations                 2725            2725            2725   \n--------------------------------------------------------------------\nStandard errors in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n:::\n:::\n\n\n## Other Methods of interest\n\n- MLE opens the door to other methods that may be more approriate to analyze data\n- They may even be able to handle otherwise unsolvable data problems. \n  - ologit, oprobit: Ordered qualitative variables\n  - mlogit, mprobit: Unordered Qualitative variables\n  - heckman: Endogenous Sample Selection\n  - fractional regression model: When the depvariable is an index\n  - etc etc\n- Worth knowing, but not for the exam!\n\n# Thats all. \nNext class: Pool and Basic Panel data analysis\n\n",
    "supporting": [
      "9_ldvm_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}