{
  "hash": "b2d850580fd9851b6699e25038763d76",
  "result": {
    "markdown": "---\ntitle: 'Multiple Regression Analysis: Inference and Asymptotics'\nsubtitle: Are they Significant?\nauthor: Fernando Rios-Avila\nformat:\n  revealjs:\n    slide-number: true\n    width: 1600\n    height: 900\n    code-fold: true\n    echo: true\n    css: styles.css\n---\n\n# {background-image=\"https://i.imgflip.com/7u1i0l.jpg\" background-size=\"contain\"}\n\n## How do you know if what you see is relevant?\n\n- Last time, we talk a bit about the estimation of MLRM. For those who do not remember:\n\n$$\\hat\\beta=(X'X)^{-1}X'y\n$$\n\n- We also defined how, under A5 (homoskedasticity), we can estimate the variance covariance of coefficients:\n\n$$Var(\\beta) = \\frac{\\sum \\hat e^2}{N-K-1} (X'X)^{-1}\n$$\n\n- The next question: how to know how precise your estimates are?\n\n- That *should* be simple, just divide coefficient by its Standard error. The larger this is, the more precise, and more significant.  \n\n- Is this enough to say something about the population coefficients?\n\n(lets assume A1-A5 holds)\n\n## Distribution of coefficients\n\n- The right answer is...Perhaps.\n\n- Unless you know something about the distribution of $\\beta's$, it would be hard to make any inferences from the estimates. Why?\n\n- Because not all distributions are made equal!\n\n::: {#bbcd5d20 .cell execution_count=1}\n``` {.stata .cell-code code-fold=\"true\"}\nclear\nrange x -4 4 1000\ngen funiform = 0 \nreplace funiform = 1/(2*sqrt(3)) if inrange(x,-sqrt(3),sqrt(3))\n\ngen fnormal = 0 \nreplace fnormal = normalden(x)\n\ngen fchi2 = 0 \nreplace fchi2 = sqrt(8)*chi2den(4,x*sqrt(8)+4)\n\ninteg funiform x, gen(F1)\ninteg fnormal x, gen(F2)\ninteg fchi2 x, gen(F3)\n\nset scheme white2\ncolor_style egypt\nreplace x = x + 1.5\ntwo (area funiform x           , pstyle(p1) color(%20)) ///\n    (area funiform x if F1<0.05, pstyle(p1) color(%80)) /// \n    (area funiform x if F1>0.95, pstyle(p1) color(%80)) /// \n    (area fnormal  x           , pstyle(p2) color(%20)) ///  \n    (area fnormal  x if F2<0.05, pstyle(p2) color(%80)) /// \n    (area fnormal  x if F2>0.95, pstyle(p2) color(%80)) /// \n    (area fchi2    x           , pstyle(p3) color(%20)) /// \n    (area fchi2    x if F3>0.95, pstyle(p3) color(%80)) /// \n    (area fchi2    x if F3<0.05, pstyle(p3) color(%80)), ///\n    xlabel(-4 / 4) legend(order(2 \"Uniform\" 5 \"Normal\" 8 \"C-Chi2\")) /// \n    xtitle(\"Beta hat Distribution\") ///\n\txline( 0, lstyle(1) lwidth(1)) xline(1.5)\n\ngraph export images/f4_1.png, replace width(1200)  \n```\n:::\n\n\n## Not all Distributions are the Same\n\n![](images/f4_1.png){size=\"contained\" fig-align=\"center\"}\n\n## New Assumption\n\n- A6: Errors are normal $e\\sim N(0,\\sigma^2_e)$.\n  - A1-A6 are the Classical Linear Model Assumption\n  - This assumes the outcome is \"conditionally\" normal. $y|X \\sim N(X\\beta,\\sigma^2_e)$\n  - And with this assumption OLS is no longer [blue]{.bluetxt}. Its now **BUE**!\n\n## \n\n### Why does it matter?\n\n  - If you combine two variables with the same distributions, the combined variable will not have the same distributions as the \"parents\"\n  - Except with normals! if you add two -normal- distributions together. The outcome will also be normal. (Dont believe me try it)\n\nRecall:\n\n$$\\hat \\beta=\\beta + (X'X)^{-1}X'e\n$$\n\nIf $e$ is normal, then $\\beta's$ will also be normal\n\nAnd this works for ANY Sample size!\n\n##\n### If $e$ normal then $\\beta$ is normal\n\n- If $\\hat \\beta's$ are normal, then we can use this distribution to make inferences about $\\beta's$ using normal distribution.\n\n- This is good, because we know how to do math with Normal distributions. And can used the modified Ratio:\n  \n$$z_j = \\frac{\\hat \\beta_j - \\beta_j}{sd(\\hat\\beta)}\\sim N(0,1)\n$$\n\n- Where $\\beta_j$ is what you think the True Population parameter is (your hypothesis), and $\\hat\\beta_j$ is what you estimate in your data. \n- Depending on the size of this, you can either reject your hypothesis, or **not** Reject it.\n\nbut do we \"know\"   $sd(\\beta)$?\n\n## \n\n### Do we \"know\" $sd(\\beta)$?\n\nWe don't, which is why we can use a normal directly. Instead we use a t-distribution, which uses $se(\\hat\\beta )$\n\n$$t_j = \\frac{\\hat \\beta_j - \\beta_j}{se(\\hat\\beta)}\\sim t_{N-k-1}\n$$\n\nThen \n\n- If $e$ is normal, $\\beta$ will be normal.\n- When Samples are \"small\" Standardized $\\beta$ will follow a t-distribution\n- But, as $N\\rightarrow \\infty$, $t_{N-k-1}\\sim N(0,1)$\n\n## Testing Hypothesis\n\n- The idea of hypothesis testing is contrasting the \"evidence\" from your data (estimates) with the beliefs we have about the population.\n\n$$y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + e\n$$\n\nSay I have two hypothesis. \n\n- $x_2$ has no effect on $y$. ie $H_0: \\beta_2 = 0$ \n- $x_1$ has an effect equal to 1. ie $H_0: \\beta_1 = 1$ \n\n    - Notice we make hypothesis about the population coefficients not the estimates\n\nI can \"test\" each hypothesis separately using a \"t-statistic\"\n\n$$ \\color{green}{t_2=\\frac{\\hat \\beta_2 - 0}{se(\\hat \\beta_2)}} ;\nt_1=\\frac{\\hat \\beta_1 - 1}{se(\\hat \\beta_1)} \n$$\n\n## Types of Hypothesis:\n\nWhen talking about hypothesis testing there are two types:\n\n- **One sided**: when your alternative hypothesis compares your null to something either strictly larger, or strictly smaller than your hypothesis.\n\t- Education has **no effect** on wages vs Returns to education are positive.\n\t- Skipping class has **no effect** on grades vs Skiping class reduces grades.\n- **Two sided**: When your alternative hypothesis is to say, \"its different than\"\n    - Returns to education is 10%, vs is not 10%\n    - Skipping class reduces grades in 0.5 points, vs not 0.5points\n\nIn both cases, you use the same t-statistic. \n\n$$t_\\beta=\\frac{\\hat\\beta - \\beta_{hyp}}{se(\\hat \\beta)} \\sim t_{N-k-1}\n$$\n\n##\n\nWhat changes are the \"thresholds\" to Judge something significant or not.\n\n#### One sided test:\n\n$$\\begin{aligned}\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k>\\beta^{hyp}_k \\\\\n & t_{\\beta_k}>t_{N-k-1}(1-\\alpha) \\\\\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k<\\beta^{hyp}_k \\\\\n & t_{\\beta_k}<-t_{N-k-1}(1-\\alpha)\n\\end{aligned}\n$$\n\n- Where $\\alpha$ is your level of **significance**, and $t_{N-k-1}(1-\\alpha)$ is the critical value.\n \n- $\\alpha$ determines the \"risk\" of commiting an **error type I**: Rejecting the Null when its true.\n\n- Intuitively, the smaller $\\alpha$ is, the more possitive (negative) \"t\" needs to be reject the Null.\n\n##\n\n#### Two sided test:\n\n$$\\begin{aligned}\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k \\neq \\beta^{hyp}_k \\\\\n & | t_{\\beta_k} | >t_{N-k-1}(1-\\alpha/2) \n\\end{aligned}\n$$\n\n- Similar to before, except the one needs to consider both tails of the distribution to determine critical values (see $t_{N-k-1}(1-\\alpha/2)$)\n \n- Intuitively, the smaller $\\alpha$ is, the larger the absolute value of \"t\" needs to be reject the Null.\n\n#### But, what is an error type I? and why we don't we \"accept\" $H_0$ s?\n\n# {background-image=\"https://i.imgflip.com/7u2hqd.jpg\" background-size=\"contain\"}\n\n## Why we never accept?:\n\n- As stated few times before, $\\hat \\beta$ are just approximations to the true $\\beta$ coefficients. Its the \"evidence\" you have based on the data available.\n- With this evidence, you can **reject** some hypothesis. (Some more strongly than others) \n- However, there could exists many scenarios that would fit the evidence.\n\n##\n\n::: {#4b5487f7 .cell execution_count=2}\n``` {.stata .cell-code code-fold=\"true\"}\nclear\nrange x -5 5 1000\ngen fx = normalden(x) \nset scheme white2\ncolor_style tableau\ngen xx = x+1\ntwo (area fx x , pstyle(p1) color(%20)) ///\n\t(area fx x if x<invnormal(.025), pstyle(p1) color(%80) ) ///\n\t(area fx x if x>invnormal(.975), pstyle(p1) color(%80) ) ///\n\t(area fx xx , pstyle(p2) color(%20)) ///\n\t(area fx xx if x<invnormal(.025), pstyle(p2) color(%80) ) /// \n\t(area fx xx if x>invnormal(.975), pstyle(p2) color(%80) ) ///\n    , xline(1.8) legend(order(1 \"H0: b=0\" 4 \"H0: b=1\"))\t ///\n\txlabel(-4(2)4)\tylabel(0(.1).5)  xsize(8) ysize(4)\ngraph export images/f4_2.png, height(1000)\treplace\n```\n:::\n\n\n![](images/f4_2.png){fig-align=\"center\"}\n\n## What about Type error I and II?\n\n:::{.incremental}\n\n- Because we do not know the truth, we are bound to commit errors in our assessment of the data.\n\n- So given the data evidence and the hypothesis, there could be 2 scenarios:\n\n  - GOOD: You either reject when $H_0$ is false, or not reject when $H_0$ is true.\n  - $TE-I$: You reject $H_0$ when it is true, \n  - $TE-II$: Not reject $H_0$ when it is false (Something else was true)\n\n:::\n\n## \n\n::: {#62afc2e4 .cell execution_count=3}\n``` {.stata .cell-code code-fold=\"true\"}\nclear\nrange x -5 5 1000\ngen fx = normalden(x) \n\ngen xxx=x+3\t\ntwo (area fx x , pstyle(p1) color(%20)) ///\n    (area fx xxx, pstyle(p2) color(%20)) ///\n\t(area fx x if x>2, pstyle(p1) color(%80)) ///\n    (area fx xxx if xxx <2, pstyle(p2) color(%80))  ///\n    ,legend(order(1 \"H0\"3 \"Type I \" 2 \"H1\"  4 \"Type II \") cols(2))\t ///\n\txlabel(-4(2)7)\tylabel(0(.1).5) xsize(8) ysize(4)  \ngraph export images/f4_3.png, height(1000)\treplace\n\n```\n:::\n\n\n![](images/f4_3.png){fig-align=\"center\"}\n\n## Example: Determinants of College GPA {.scrollable}\n\n::: {#92448eb0 .cell execution_count=4}\n``` {.stata .cell-code}\nfrause gpa1, clear\nreg colgpa hsgpa act skipped\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n      Source |       SS           df       MS      Number of obs   =       141\n-------------+----------------------------------   F(3, 137)       =     13.92\n       Model |  4.53313314         3  1.51104438   Prob > F        =    0.0000\n    Residual |  14.8729663       137  .108561798   R-squared       =    0.2336\n-------------+----------------------------------   Adj R-squared   =    0.2168\n       Total |  19.4060994       140  .138614996   Root MSE        =    .32949\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       hsgpa |   .4118162   .0936742     4.40   0.000     .2265819    .5970505\n         act |   .0147202   .0105649     1.39   0.166    -.0061711    .0356115\n     skipped |  -.0831131   .0259985    -3.20   0.002    -.1345234   -.0317028\n       _cons |   1.389554   .3315535     4.19   0.000     .7339295    2.045178\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n- Hypothesis: Skipping classes has no effect on College GPA.\n\n$$H_0: \\beta_{skip} = 0 \\text{ vs } H_1: \\beta_{skip} \\neq 0\n$$\n\n- Test, $a=95%$, $|t_{skip}|=3.2$ vs $t_{n-k-1}(0.975)$: \n\n``` {.stata .cell-code}\ndisplay invt(141-4,0.975)\n```\n1.9774312\n\n\n- Conclusion: $H_0$ is rejected.\n\n## \n\n- Hyp: Skipping college has no effect on College GPA vs has a negative effect\n \n$$H_0: \\beta_{skip} = 0 \\text{ vs } H_1: \\beta_{skip}<0\n$$\n\n- Test, $a=95\\%$, $|t_{skip}|=3.2$ vs $t_{n-k-1}(0.95)=1.6560$\n- Also Reject $H_0$\n\n##\n\n- $t_{ACT}=1.39$\n- Hyp: ACT has no effect on College GPA vs It has a non-zero effect\n- Hyp: ACT has no effect on College GPA vs it has a positive effect\n- Critical: \n  - $t_{137}(0.95)=1.6560$ Donot Reject$H_0$ with $\\alpha = 5\\%$\n  - $t_{137}(0.90)=1.2878$ Reject $H_0$ with $\\alpha = 10\\%$ \n  \n- Each GPA point in highschool translates into half a point in College GPA. vs Is less than .5\n\n::: {#edb7c32a .cell execution_count=6}\n``` {.stata .cell-code code-fold=\"false\"}\ntest hsgpa = 0.5\nlincom hsgpa - 0.5\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n ( 1)  hsgpa = .5\n\n       F(  1,   137) =    0.89\n            Prob > F =    0.3482\n\n ( 1)  hsgpa = .5\n\n------------------------------------------------------------------------------\n      colgpa | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         (1) |  -.0881838   .0936742    -0.94   0.348    -.2734181    .0970505\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n- Critical at 5%: $t=-1.6560$\n- Cannot Reject $H_0$\n\n## p-values\n\n- Something you may or may not have noticed. The significance level $\\alpha$ can be choosen by the researcher.\n  - Conventional levels are 10%, 5% and 1%. \n- This may lead to researchers choosing any value that would make their theory fit.  \n- There is a better alternative. Using $p-values$ to capture the smallest significance level that you could use to reject your Null.\n\n$$p-value = P(|t|>|t-stat|) \\text{ or } p-value = 2*P(|t|>|t-stat|)\n$$\n\n  - The smallest the better! (for rejection)\n- How? \n  - One tail : `display 1-t(df = n-k-1, |t-stat|)`\n  - two tails: `display 2-2*t(df = n-k-1, |t-stat|)`\n\n## \n\n::: {#270743be .cell execution_count=7}\n``` {.stata .cell-code}\nclear\nrange x -5 5 1000\ngen fx = normalden(x) \nlocal p1:display %5.1f 100*2*(1-normal(1.5)) \nlocal p2:display %5.1f 100*2*(1-normal(2.5)) \ntwo (area fx x , pstyle(p1) color(%20)) ///\n\t(area fx x if x<-1.5, pstyle(p3) color(%80) ) ///\n\t(area fx x if x>+1.5, pstyle(p3) color(%80) ) ///\n\t(area fx x if x<invnormal(.025), pstyle(p1) color(%80) ) ///\n\t(area fx x if x>invnormal(.975), pstyle(p1) color(%80) ) ///\n\t(area fx x if x<-2.5, pstyle(p2) color(%80) ) ///\n\t(area fx x if x>+2.5, pstyle(p2) color(%80) ) , ///\n\txline(-1.5 2.5) legend(order(4 \"{&alpha}=5%\" 6 \"p-value = `p2'%\" 2 \"p-value = `p1'%\")) ///\n\tylabel(0(.1).5) xlabel(-5 0 5 -1.5 2.5)\ngraph export images/f4_4.png, replace width(1200)\n```\n:::\n\n\n![](images/f4_4.png){fig-align=\"center\"}\n\n## Note on Statistical Significance\n\n1. Statistically significant doesnt mean meaninful. And lack of it, doesnt mean is not important\n   - Keep in mind that SE may be larger or smaller due to other factors (N or Mcollinearity)\n2. Be careful of discussing the effect size. (a 1US increase in min wage is different from 1chp in min Wage)\n3. If non-significant, pay attention to the magnitude and relevance for your research. Does it have the correct sign?\n4. Incorrect signs with significant results. Either there is something wrong, or you found something interesting.\n\n# {background-image=\"https://i.imgflip.com/6qvajf.jpg\" background-size=\"contain\"}\n\n## Confidence Intervals\n\n- This is the third approach to assess how precise or significant an estimate is. You provide a Range of possible values, given the level of coverage, and SE.\n\n$$CI(\\beta_i) = [\\hat \\beta_i - t_{n-k-1}(1-\\alpha),\\hat \\beta_i + t_{n-k-1}(1-\\alpha)]\n$$\n\n- Interpretation:\n  - If we were to draw M samples, the true beta would be in this interval $1-\\alpha\\%$ of the time.\n- It allows you to see what other \"hypothesis\" would be consistent with the evidence of the estimate (you wouldnt be able to reject the Null)\n\n##\n\n::: {#d42f51a9 .cell execution_count=8}\n``` {.stata .cell-code}\nclear\nrange x -5 5 1000\ngen fx = normalden(x) \nlocal p1:display %5.1f 100*2*(1-normal(1.5)) \nlocal p2:display %5.1f 100*2*(1-normal(2.5)) \ngen xx = x+2\n\ntwo (area fx x , pstyle(p1) color(%10)) ///\n\t(area fx x if x<invnormal(.025), pstyle(p1) color(%60) ) ///\n\t(area fx x if x>invnormal(.975), pstyle(p1) color(%60) ) ///\n\t(area fx xx , color(gs1%10) ) ///\n\t(area fx xx if x<invnormal(.005),  color(gs1%80) ) ///\n\t(area fx xx if x>invnormal(.995),  color(gs1%80) ) ///\n\t(area fx xx if x<invnormal(.025),  color(gs1%60) ) ///\n\t(area fx xx if x>invnormal(.975),  color(gs1%60) ) ///\n\t(area fx xx if x<invnormal(.05),  color(gs1%40) ) ///\n\t(area fx xx if x>invnormal(.95),  color(gs1%40) ), ///\n\txline(2) legend(order(5 \"CI-1%\" 7 \"CI-5%\" 9 \"CI-10%\")) ///\n\tylabel(0(.1).5) xlabel(-5 0 5 )  \ngraph export images/f4_5.png, replace width(1000)\n```\n:::\n\n\n![](images/f4_5.png){fig-align=\"center\"}\n\n# Lets make things interestings (Harder)\n\n## Testing Linear Combinations:\n\n- You may be interested in testing particular linear combinations of coefficients:\n\n$b_1 - b_2 =0 ; b_2+b_3=1 ; 2*b_4-b_5=b_6$\n\n- Doing this is \"simple\". Because is a single linear combination, you can still use \"t-stat\".\n\n$t-stat = \\frac{2*\\hat b_4 -\\hat b_5 -\\hat b_6}{se(2*\\hat b_4 -\\hat b_5 -\\hat b_6)}$\n\n- Just need SE for combined coefficients (requires knowing Variances and Covariances)\n\n- Easy way, you could use `Stata`:\n\n```{stata}{style=\"font-size: 50px\"}\nreg y x1 x2 x3 x4 x5 x6\nlincom x1-x2 or lincom 2*x4-x5-x6\ntest (x1-x2=0) (x2+x3=1) (2*x4-x5=x6), mtest\n```\n\n## \n\n### Harder Way: (if you dare)\n\n**Matrix Multiplication**\n\nAssume Constant is the last coefficient:\n$$V( 2*b_4-b_5- b_6) = R' V R ; R = [0,0,0,2,-1,-1]\n$$\n\nwhere R are the restrictions, and V is the variance covariance matrix of $\\beta's$.\n\nThen your t-stat\n\n$$t-stat = \\frac{2*b_4-b_5- b_6}{\\sqrt{V(2*b_4-b_5- b_6)}}\n$$\n\n## \n\n### Alternative: Substitution\n\n- One can manipulate the regression model to consider a model with the contrained coefficient.\n- Once model is estimated, it simplifies testing:\n\n$$\\begin{aligned}\n & y = b_0 + b_1 x_1 + b_2  x_2 + b_3 x_3 + e  \\\\\nh0: & b_1 - 2b_2 +b_3=0 \\rightarrow \\theta = b_1 - 2b_2 +b_3 \\rightarrow b_1 = \\theta + 2b_2 - b_3 \\\\\n& y = b_0 + ( \\theta + 2b_2 - b_3) x_1 + b_2 x_2 + b_3 x_3 + e \\\\\n& y = b_0 +  \\theta x_1 + b_2( x_2 +2 x_1) + b_3 (x_3-x_1) + e \\\\\n& y = b_0 +  \\theta x_1 + b_2 \\tilde x_2 + b_3 \\tilde x_3 + e \\\\\n\\end{aligned}\n$$\n\nHere testing for $\\theta=0$ is the same as testing for $b_1 - 2b_2 +b_3$ in the original model.\n\n:::{.callout-note}\n\nAlways ask something like this in Midterm, so brush up your math.\n\n:::\n\n## Testing Multiple Restrictions\n\nWhat if you are interested in testing multiple restrictions:\n\n$$\\begin{aligned}\ny &= b_0 + b_1 x_1 + b_2 x_2 +b_3 x_3 + e \\\\\n& H_0: b_1 = 0 ; b_2 - b_3 =0 \\\\\n& H_1: H_0 \\text{ is false}\n\\end{aligned}\n$$\n\nEasy way: `Stata` command `test` allows you to do this\n\nOtherwise, you can do it by hand:\n\n##\n\n  1. Estimate unrestricted model (original) and \"save\" $SSR_{ur}$ or $R_{ur}^2$\n  2. Impose restrictions on the model and \"save\" $SSR_r$ or $R_{r}^2$\n  3. Estimate F-stat:\n\n$$F_{q,n-k-1} = \\frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(n-k-1)}  \\text{ or }\n\\frac{(R^2_{ur}-R^2_r)/q}{(1-R^2_{ur})/(n-k-1)} \\sim F(q,n-k-1)\n$$\n\n$SSR$ Sum of Squared Residuals, $q$ number of restrictions\n\n- Idea, you are comparing how the overall fitness of the model changes with restrictions.\n- If restrictions slightly decreases the model Fitness, you cannot be rejected them.\n- Otherwise, They are rejected! (you just dont know which)\n\n## Overall Model Significance\n\nOne test, we often don't do anymore, is testing the overall fitness of a model:\n\n$$H_0: x_1, x_2, \\dots , x_k \\text{ do not explain y}\n$$\n\n$$H_0: \\beta_1=\\beta_2=\\dots=\\beta_k =0\n$$\n\nWhere we kind of suggest that a model with only an intercept is better than the one with covariates.\n\n$$F_{q,n-k-1} = \\frac{(R^2_{ur}-\\color{red}{R^2_r})/q}{(1-R^2_{ur})/(n-k-1)} \\sim F(q,n-k-1)\n$$\n\nIn this case $\\color{red}{R^2_r}=0$\n\n## **Not** For the faint for heart\n\nMatrix form for F-Stat!\nRestrictions:\n\n$$H_0: R_{q,k+1}\\beta_{k+1,1}=c_{q,1}\n$$\n\nFirst. Define matrix with all Matrix Restriction\n$$\n\\Sigma_R = R_{q,k+1} V_\\beta R'_{q,k+1}\n$$\n\nSecond: F-statistic\n\n$$\nF-stat = \\frac 1 q (R\\beta-c)' \\Sigma_R^{-1} (R\\beta-c) \n$$\n\n## Example {.scrollable}\n\n::: {#9f995736 .cell execution_count=9}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause hprice1, clear\nreg lprice lasses bdrms llotsize lsqrft\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n      Source |       SS           df       MS      Number of obs   =        88\n-------------+----------------------------------   F(4, 83)        =     70.58\n       Model |  6.19607473         4  1.54901868   Prob > F        =    0.0000\n    Residual |  1.82152879        83   .02194613   R-squared       =    0.7728\n-------------+----------------------------------   Adj R-squared   =    0.7619\n       Total |  8.01760352        87  .092156362   Root MSE        =    .14814\n\n------------------------------------------------------------------------------\n      lprice | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n     lassess |   1.043065    .151446     6.89   0.000     .7418453    1.344285\n       bdrms |   .0338392   .0220983     1.53   0.129    -.0101135    .0777918\n    llotsize |   .0074379   .0385615     0.19   0.848    -.0692593    .0841352\n      lsqrft |  -.1032384   .1384305    -0.75   0.458     -.378571    .1720942\n       _cons |    .263743   .5696647     0.46   0.645    -.8692972    1.396783\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n::: {#0d523328 .cell execution_count=10}\n``` {.stata .cell-code code-fold=\"false\"}\ntest (lasses=1)\ntest (lasses=1) (bdrms=llotsize=lsqrft=0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n ( 1)  lassess = 1\n\n       F(  1,    83) =    0.08\n            Prob > F =    0.7768\n\n ( 1)  lassess = 1\n ( 2)  bdrms - llotsize = 0\n ( 3)  bdrms - lsqrft = 0\n ( 4)  bdrms = 0\n\n       F(  4,    83) =    0.67\n            Prob > F =    0.6162\n```\n:::\n:::\n\n\n::: {#a0f13fe5 .cell execution_count=11}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause mlb1, clear\nreg lsalary years gamesyr bavg hrunsyr rbisy\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n      Source |       SS           df       MS      Number of obs   =       353\n-------------+----------------------------------   F(5, 347)       =    117.06\n       Model |  308.989208         5  61.7978416   Prob > F        =    0.0000\n    Residual |  183.186327       347  .527914487   R-squared       =    0.6278\n-------------+----------------------------------   Adj R-squared   =    0.6224\n       Total |  492.175535       352  1.39822595   Root MSE        =    .72658\n\n------------------------------------------------------------------------------\n     lsalary | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n       years |   .0688626   .0121145     5.68   0.000     .0450355    .0926898\n     gamesyr |   .0125521   .0026468     4.74   0.000     .0073464    .0177578\n        bavg |   .0009786   .0011035     0.89   0.376    -.0011918     .003149\n     hrunsyr |   .0144295    .016057     0.90   0.369    -.0171518    .0460107\n      rbisyr |   .0107657    .007175     1.50   0.134    -.0033462    .0248776\n       _cons |   11.19242   .2888229    38.75   0.000     10.62435    11.76048\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n::: {#03e6bc1a .cell execution_count=12}\n``` {.stata .cell-code code-fold=\"false\"}\ntest bavg hrunsyr rbisy\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n ( 1)  bavg = 0\n ( 2)  hrunsyr = 0\n ( 3)  rbisyr = 0\n\n       F(  3,   347) =    9.55\n            Prob > F =    0.0000\n```\n:::\n:::\n\n\n# Lets take one more Step: What if errors are not normal?\n\n# {background-image=\"https://i.imgflip.com/6qwgsh.jpg\" background-size=\"contain\"}\n\n## Introduction\n\n- When considering the topic of asymptotic theory, there are few concepts that are important ton consider.\n\n  1. Asymtotics refer to properties of OLS when $N\\rightarrow \\infty$\n  2. When samples grow, we are more concern about consistency rather than \"just\" unbiased estimators.\n  3. We are also concern with how flexible is the normality assumption when samples grow large.\n\n## What is consistency?\n\n- Up until now, we have been concerned with Unbiased estimates\n\n$$E(\\hat\\beta)=\\beta\n$$\n\n- In large samples, this is no longer enough. One requires Consistency!\n  - Consistency says that as $N\\rightarrow \\infty$ then $plim \\hat \\beta = \\beta$.\n  - $p(|\\hat \\beta - \\beta|<\\varepsilon) = 1$ or that The variance shrinks to zero, or we can estimate $\\beta$ almost surely.\n  - This is also known as asymptotic unbiasness.\n  \n- In linear regression analysis, consistency can be achieved with a weaker A4': $Cov(e,x)=0$, assuming that we require only linear independence.\n\n## Consistency vs Bias\n\n\n\n:::{.panel-tabset}\n\n## Consistent and Unbiased\n\n![](images/f4_6.png){fig-align=\"center\" width=70%}\n\n## Consistent and Biased\n\n![](images/f4_7.png){fig-align=\"center\" width=70%}\n\n:::\n\n## What about Normality Assumption?\n\n- Everything we have seen so far was possible under the normality assumption of the errors.\n  - if $e$ is normal, then $b$ is normal (even in small samples), thus we can use $t$, $F$, etc\n\n- But what if this assumption fails? would we care?\n\n::: {.incremental}\n\n  - Perhaps. If your sample is small, $b$ will not be normal, and standard procedures will not work.\n  - In large Samples, however, $\\beta's$ will be normal, even if $e$ is not. Thanks to CLT\n\n:::\n\n##\n\n### Good news\n\n- Bottom line, when $N$ is large, you do not need $e$ to be normal.\n  \n- if A1-A5 hold, you can rely on asymptotic normality!\n\n- Thus you can still use t's and F's, but you can also use LM\n  \n## LM-Lagrange Multiplier\n\n- While you can still use t-stat and F-stat to draw inference from your model, there is a better test (given the large sample): Lagrange Multiplier Statistic\n- The idea: Does impossing restrictions affect the model Fitness? \n\n1. Regress $y$ on restricted $x_1,\\dots,x_{k-q}$, and obtain $\\tilde e$\n2. Regress $\\tilde e$ on all $x's$, and obtain $R^2_e$. \n   \nIf the excluded regressors were not significant, the $R^2_e$ should be very small.\n\n3. Compare $nR^2_e$ with $\\chi^2(n,1-\\alpha)$, and draw conclusions.\n\n## Example:{.scrollable}\n\n::: {#160757a9 .cell .larger execution_count=14}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause crime1, clear\nqui: reg narr86 pcnv avgsen tottime ptime86 qemp86\n** H0: avgsen=0 and tottime=0\ntest (avgsen=0) (tottime=0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n ( 1)  avgsen = 0\n ( 2)  tottime = 0\n\n       F(  2,  2719) =    2.03\n            Prob > F =    0.1310\n```\n:::\n:::\n\n\n::: {#d8520de6 .cell .larger execution_count=15}\n``` {.stata .cell-code code-fold=\"false\"}\nqui: reg narr86 pcnv                ptime86 qemp86\n* Predict residuals of constrained model\npredict u_tilde , res\n* regress residuals againts all variables\nreg u_tilde  pcnv avgsen tottime ptime86 qemp86\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n      Source |       SS           df       MS      Number of obs   =     2,725\n-------------+----------------------------------   F(5, 2719)      =      0.81\n       Model |  2.87904835         5  .575809669   Prob > F        =    0.5398\n    Residual |  1924.39392     2,719  .707757969   R-squared       =    0.0015\n-------------+----------------------------------   Adj R-squared   =   -0.0003\n       Total |  1927.27297     2,724  .707515773   Root MSE        =    .84128\n\n------------------------------------------------------------------------------\n     u_tilde | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        pcnv |  -.0012971    .040855    -0.03   0.975    -.0814072    .0788129\n      avgsen |  -.0070487   .0124122    -0.57   0.570     -.031387    .0172897\n     tottime |   .0120953   .0095768     1.26   0.207    -.0066833     .030874\n     ptime86 |  -.0048386   .0089166    -0.54   0.587    -.0223226    .0126454\n      qemp86 |   .0010221   .0103972     0.10   0.922    -.0193652    .0214093\n       _cons |  -.0057108   .0331524    -0.17   0.863    -.0707173    .0592956\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\n``` {.stata .cell-code}\ndisplay \"Chi2(2)=\" `=e(N)*e(r2)'\ndisplay \"Its p-value=\" %5.4f `=1-chi2(2, `=e(N)*e(r2)' )'\n```\nChi2(2)=4.0707294\nIts p-value=0.1306\n\n\nTry making it a program if you \"dare\"\n\n# Thats All folks!\n\n# Part II: \n# Addressing Problems with MRA\n\n",
    "supporting": [
      "4_MLRM_IA_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}