{
  "hash": "d02eaed563c3dd59348adf461411cf58",
  "result": {
    "markdown": "---\ntitle: 'Multiple Regression Analysis: Inference and Asymptotics'\nsubtitle: Are they Significant?\nauthor: Fernando Rios-Avila\nformat:\n  revealjs:\n    slide-number: true\n    width: 1600\n    height: 900\n    code-fold: true\n    echo: true\n    css: styles.css\n---\n\n# {background-image=\"https://i.imgflip.com/7u1i0l.jpg\" background-size=\"contain\"}\n\n## How do you know if what you see is relevant?\n\n- Last time, we talk a bit about the estimation of MLRM. For those who do not remember:\n\n$$\\hat\\beta=(X'X)^{-1}X'y\n$$\n\n- We also defined how, under A5 (homoskedasticity), we can estimate the variance covariance of coefficients:\n\n$$Var(\\beta) = \\frac{\\sum \\hat e^2}{N-K-1} (X'X)^{-1}\n$$\n\n- The next question: how to know how precise your estimates are?\n\n- That *should* be simple, just divide coefficient by its Standard error. The larger this is, the more precise, and more significant.  \n\n- Is this enough to say something about the population coefficients?\n\n(lets assume A1-A5 holds)\n\n## Distribution of coefficients\n\n- The right answer is...Perhaps.\n\n- Unless you know something about the distribution of $\\beta's$, it would be hard to make any inferences from the estimates. Why?\n\n- Because not all distributions are made equal!\n\n::: {#b89e2ef3 .cell execution_count=1}\n``` {.stata .cell-code code-fold=\"true\"}\nclear\nrange x -4 4 1000\ngen funiform = 0 \nreplace funiform = 1/(2*sqrt(3)) if inrange(x,-sqrt(3),sqrt(3))\n\ngen fnormal = 0 \nreplace fnormal = normalden(x)\n\ngen fchi2 = 0 \nreplace fchi2 = sqrt(8)*chi2den(4,x*sqrt(8)+4)\n\ninteg funiform x, gen(F1)\ninteg fnormal x, gen(F2)\ninteg fchi2 x, gen(F3)\n\nset scheme white2\ncolor_style egypt\nreplace x = x + 1.5\ntwo (area funiform x           , pstyle(p1) color(%20)) ///\n    (area funiform x if F1<0.05, pstyle(p1) color(%80)) /// \n    (area funiform x if F1>0.95, pstyle(p1) color(%80)) /// \n    (area fnormal  x           , pstyle(p2) color(%20)) ///  \n    (area fnormal  x if F2<0.05, pstyle(p2) color(%80)) /// \n    (area fnormal  x if F2>0.95, pstyle(p2) color(%80)) /// \n    (area fchi2    x           , pstyle(p3) color(%20)) /// \n    (area fchi2    x if F3>0.95, pstyle(p3) color(%80)) /// \n    (area fchi2    x if F3<0.05, pstyle(p3) color(%80)), ///\n    xlabel(-4 / 4) legend(order(2 \"Uniform\" 5 \"Normal\" 8 \"C-Chi2\")) /// \n    xtitle(\"Beta hat Distribution\") ///\n\txline( 0, lstyle(1) lwidth(1)) xline(1.5)\n\ngraph export images/f4_1.png, replace width(1200)  \n```\n:::\n\n\n## Not all Distributions are the Same\n\n![](images/f4_1.png){size=\"contained\" fig-align=\"center\"}\n\n## New Assumption\n\n- A6: Errors are normal $e\\sim N(0,\\sigma^2_e)$.\n  - A1-A6 are the Classical Linear Model Assumption\n  - This assumes the outcome is \"conditionally\" normal. $y|X \\sim N(X\\beta,\\sigma^2_e)$\n  - And with this assumption OLS is no longer [blue]{.bluetxt}. Its now **BUE**!\n\n## \n\n### Why does it matter?\n\n  - If you combine two variables with the same distributions, the combined variable will not have the same distributions as the \"parents\"\n  - Except with normals! if you add two -normal- distributions together. The outcome will also be normal. (Dont believe me try it)\n\nRecall:\n\n$$\\hat \\beta=\\beta + (X'X)^{-1}X'e\n$$\n\nIf $e$ is normal, then $\\beta's$ will also be normal\n\nAnd this works for ANY Sample size!\n\n##\n### If $e$ normal then $\\beta$ is normal\n\n- If $\\hat \\beta's$ are normal, then we can use this distribution to make inferences about $\\beta's$ using normal distribution.\n\n- This is good, because we know how to do math with Normal distributions. And can used the modified Ratio:\n  \n$$z_j = \\frac{\\hat \\beta_j - \\beta_j}{sd(\\hat\\beta)}\\sim N(0,1)\n$$\n\n- Where $\\beta_j$ is what you think the True Population parameter is (your hypothesis), and $\\hat\\beta_j$ is what you estimate in your data. \n- Depending on the size of this, you can either reject your hypothesis, or **not** Reject it.\n\nbut do we know   $SE(\\beta)$?\n\n## \n\n### Do we \"know\" $SE(\\beta)$?\n\nWe don't, which is why we can use a normal directly. Instead we use a t-distribution\n\n$$t_j = \\frac{\\hat \\beta_j - \\beta_j}{se(\\hat\\beta)}\\sim t_{N-k-1}\n$$\n\nThen \n\n- If $e$ is normal, $\\beta$ will be normal.\n- When Samples are \"small\" Standardized $\\beta$ will follow a t-distribution\n- But, as $N\\rightarrow \\infty$, $t_{N-k-1}\\sim N(0,1)$\n\n## Testing Hypothesis\n\n- The idea of hypothesis testing is contrasting the \"evidence\" from your data (estimates) with the beliefs we have about the population.\n\n$$y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + e\n$$\n\nSay I have two hypothesis. \n\n- $x_2$ has no effect on $y$. ie $H_0: \\beta_2 = 0$ \n- $x_1$ has an effect equal to 1. ie $H_0: \\beta_1 = 1$ \n\n    - Notice we make hypothesis about the population coefficients not the estimates\n\nI can \"test\" each hypothesis separately using a \"t-statistic\"\n\n$$ \\color{green}{t_2=\\frac{\\hat \\beta_2 - 0}{se(\\hat \\beta_2)}} ;\nt_1=\\frac{\\hat \\beta_1 - 1}{se(\\hat \\beta_1)} \n$$\n\n## Types of Hypothesis:\n\nWhen talking about hypothesis testing there are two types:\n\n- **One sided**: when your alternative hypothesis compares your null to something either strictly larger, or strictly smaller than your hypothesis.\n\t- Education has **no effect** on wages vs Returns to education are positive.\n\t- Skipping class has **no effect** on grades vs Skiping class reduces grades.\n- **Two sided**: When your alternative hypothesis is to say, \"its different than\"\n    - Returns to education is 10%, vs is not 10%\n    - Skipping class reduces grades in 0.5 points, vs not 0.5points\n\nIn both cases, you use the same t-statistic. \n\n$$t_\\beta=\\frac{\\hat\\beta - \\beta_{hyp}}{se(\\hat \\beta)} \\sim t_{N-k-1}\n$$\n\n##\n\nWhat changes are the \"thresholds\" to Judge something significant or not.\n\n#### One sided test:\n\n$$\\begin{aligned}\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k>\\beta^{hyp}_k \\\\\n & t_{\\beta_k}>t_{N-k-1}(1-\\alpha) \\\\\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k<\\beta^{hyp}_k \\\\\n & t_{\\beta_k}<t_{N-k-1}(\\alpha) \n\\end{aligned}\n$$\n\n- Where $\\alpha$ is your level of **significance**, and $t_{N-k-1}(\\alpha)$ and $t_{N-k-1}(1-\\alpha)$ are critical values.\n \n- $\\alpha$ determines the \"risk\" of commiting an **error type I**: Rejecting the Null when its true.\n\n- Intuitively, the smaller $\\alpha$ is, the more possitive (negative) \"t\" needs to be reject the Null.\n\n##\n\n#### Two sided test:\n\n$$\\begin{aligned}\nH_0: & \\beta_k=\\beta^{hyp}_k \\text{ vs } H_1: \\beta_k \\neq \\beta^{hyp}_k \\\\\n & | t_{\\beta_k} | >t_{N-k-1}(1-\\alpha/2) \n\\end{aligned}\n$$\n\n- Similar to before, except the one needs to consider both tails of the distribution to determine critical values (see $t_{N-k-1}(1-\\alpha/2)$)\n \n- Intuitively, the smaller $\\alpha$ is, the larger the absolute value of \"t\" needs to be reject the Null.\n\n#### But, what is an error type I? and why we don't we \"accept\" $H_0$ s?\n\n# {background-image=\"https://i.imgflip.com/7u2hqd.jpg\" background-size=\"contain\"}\n\n## Why we never accept?:\n\n- As stated few times before, $\\hat \\beta$ @b  \n\n",
    "supporting": [
      "4_MLRM_IA_files"
    ],
    "filters": [],
    "includes": {}
  }
}