{
  "hash": "1bd3609a2103bd7a3012a0e512d9400c",
  "result": {
    "markdown": "---\ntitle: Instrumental Variables and 2SLS\nsubtitle: Time to save A4 (hopefully)\nauthor: Fernando Rios-Avila\nformat:\n  revealjs:\n    slide-number: true\n    width: 1600\n    height: 900\n    code-fold: true\n    code-overflow: wrap\n    echo: true\n    css: styles.css\n    chalkboard: true\n---\n\n# The problem \n\n## The problem: Endogeneity\n\n- As I mentioned at the beginning, one of the most important assumptions required to analyze data and obtain correct estimations and draw inference was A4: **No endogeneity** or $E(e|X)=0$\n  - Endogeneity is a problem that occurs because the error is related to $X$.\n  - This is a proble,, because we can no longer assume the error is, in average, constant when analyzing changes in $X's$\n\n## \n\n- Why did it happen?\n  - Usually because important variables are omitted\n    - Add them back, or at least proxies?\n  - Incorrect functional form\n    - Try making it more flexible?\n  - Data has measurement error\n    - Get better data?\n  - Sample is endogenous (other treatments are necessary)\n  - Reverse causality (you dont know which cause which)\n  - Simultenaity, similar to omitted variables. There is another factor that caused both the outcome and explanatory variable\n\n## A pair of Solutions\n\n- Today we will cover one approach that could help with many (not all) the situations that could cause endogeneity.\n- To apply this approach, however, we need:\n  - More data (more variables with specific properties)\n  - More information on how the \"system\" works.\n\n- These methods are:\n  - IV - Instrumental variable\n  - 2sls - Two-stage Least squares\n\n> NOTE: These two methods are almost interchangable. \n> \n> - IV refers to cases with 1 endogenous variable and 1 \"instrument\"\n> \n> - 2sls refers to cases with 1+ endogenous variables and \"instruments\"\n\n## What is an \"***Instrumental Variable***\"\n\n- I have mentioned a few times the word \"instrument\" But what are they really?\n\n- Instruments: the heros/variables that will \"save us\" of endogeneity.\n\n- They have at least 2 properties:\n  \n  1. Instruments should be exogenous to the model \n   \n    - This means that they do not appear in the theoretical specification, thus there is no **DIRECT** conection between the instrument $Z$ and the outcome $y$.\n      - Should only be connected through the endogenous variable.\n\n    - Also that there is no correlation between the model error and $Z$.\n\n  2.  The instrument is relevant and related to $x$ (endogenous variable)\n   \n    - Preferably, you need a variable that is not only correlated with $X$ but determines changes in $X$.\n    - We also need this effect to be monotonic! \n\n- In many instances we even want an instrument that is just as good as [conditionally] random. \n\n## How Instruments work: The math\n\nThe problem $corr(x_1,e) \\neq 0$:\n$$\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + e || \\tilde w = w-\\bar w  \\\\\n\\tilde \\beta_1 &=\\frac{\\sum \\tilde x_1 \\tilde y}{\\sum \\tilde x_1^2}=\n\\frac{\\sum \\tilde x_1 (\\beta_1 \\tilde x_1 + e)}{\\sum \\tilde x_1^2} \\\\\n& = \\beta_1 + \\frac{\\sum \\tilde x_1 e}{\\sum \\tilde x_1^2} \n\\end{aligned}\n$$\n\nHow IV works $corr(z_1,e) \\neq 0$:\n\n$$\\begin{aligned}\n\\hat \\beta_1 &=\\frac{\\sum \\tilde z_1 \\tilde y}{\\sum \\tilde z_1 \\tilde x_1}=\n\\frac{\\sum \\tilde z_1 (\\beta_1 \\tilde x_1 + e)}{\\sum \\tilde z_1 \\tilde x_1} \\\\\n& = \\beta_1 + \\frac{\\sum \\tilde z_1 e}{\\sum \\tilde z_1 \\tilde x_1} \n\\end{aligned}\n$$\n\n## How Instruments work: The intuition\n\n- One way of thinking about how IV works is by realizing not ALL changes in $X_1$ are endogenous. Some **are** due to $e$, but some are due other factors.\n  - we just can't differentiate them\n  - If we could use (in the regression), only exogenous changes, (or omit endogenous ones), we could estimate our models correctly.\n  \n- What IV's do is to identify **Part** of the exogenous component (the one related to $Z$), and use only THAT variation to identify coefficients.\n  - This would do a reasonable work, as long as the instrument is relevant, and instrument exogenous. \n\n## Example\n\n$$\\begin{aligned}\ne &\\sim chi(2)-2 ; z = chi(2)-2 ; x = chi(2)-2+z+e \\\\\ny &=1+x+e\n\\end{aligned}\n$$\n\n::: {.cell execution_count=1}\n``` {.stata .cell-code}\n** Montecarlo Simulation\nclear\nset seed 10101\nset obs 1000\nqui:mata:\nk = 1000; n=1000\nb1=bc = b = J(k,2,0)\nfor(i=1;i<=k;i++){\n    e = rchi2(n,1,2):-2\n    e1 = rchi2(n,1,2):-2\n    z = rchi2(n,1,2):-2,J(n,1,1)\n    x = rchi2(n,1,2):-2:+z[,1]:+e ,J(n,1,1)\n    x1 = rchi2(n,1,2):-2:+z[,1]:+e1,J(n,1,1)\n    y = 1:+x[,1]:+e\n    y1 = 1:+x[,1]:+e1\n    xx = cross(x,x)\n    b[i,] = (invsym(xx)*cross(x,y))'\n    bc[i,] = (invsym(cross(z,x))*cross(z,y))'\n    b1[i,] = (invsym(xx)*cross(x,y1))'\n}\nend\ngetmata bb*=b\ngetmata bc*=bc\ngetmata b_*=b1\nset scheme white2\ncolor_style tableau\ntwo kdensity bb1 ||  kdensity bc1  || kdensity b_1, ///\nlegend(order(1 \"X Endogenous\" 2 \"IV\" 3 \"X Exogenous\"))\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<style>div.jp-Notebook .datagrid-container {min-height: 448px; }</style>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](8_iv2sls_files/figure-revealjs/cell-2-output-3.png){fig-align='center'}\n:::\n:::\n\n\n## SE and Statistical Inference\n\n- SE have a different structure compared to OLS.\n- In the simplest case (one dep variable that is endogenous), and under the assumption of Homoskedasticity,  SE for $\\beta_1$ are given by:\n\n$$\\begin{aligned}Var_{iv}(\\beta_1) = \\frac{\\hat\\sigma^2_e}{SST_x R^2_{x|z}} \\\\\n\\hat\\sigma^2_e =\\frac{ \\sum (y-\\hat\\beta_0-\\hat\\beta_1 x)^2}{n-2}\n\\end{aligned}\n$$\n\n- Once they are obtained t-stats can be used as usual\n\n## Examples of IV's\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\nEducation:\n\n- Fathers Education\n- Distance to School\n- \\# Siblins\n \n:::\n\n::: {.column width=\"50%\"}\nVeteran Status:\n\n- Vietnam Lottery Ticket\n\nOther:\n\n- Rain \n- Shift-Share\n- Judge FE\n:::\n\n::::\n\n\n- IV SE will be necessarily larger than OLS, because there is less variation of $x$ used to identify $\\beta$.\n- If $x$ is used as its own instrument, the estimator will be that of OLS.\n\n# Weak and Enogenous, and MLR\n\n## Weak and endogenous instruments:\n\n- While instruments can be used to address Endogeneity problems, finding good instruments can be hard.\n  - They can be \"weak-instruments\"\n  - or not fully exogenous\n\n- If this happens, IV can be worse than endogeneity:\n\n$$\\begin{aligned}\nplim \\beta_{ols} &= \\beta_1 + corr(x,u) \\frac{\\sigma_u}{\\sigma_x} \\\\\nplim \\beta_{iv}  &= \\beta_1 + \\frac{corr(z,u)}{corr(z,x)} \\frac{\\sigma_u}{\\sigma_x} \n\\end{aligned}\n$$\n\n- we will talk about the \"weak-instruments\" later today.\n \n\n## IV with MLR\nAdding controls...is easy!\n\n$$y = \\beta_0 + \\gamma_1 y_1 + X\\beta + e ; z \\text{ instrument for } y\n$$\n\nWe still assume 1 endogenous variable ($y_1$) and one instrument ($z$)\n\n$$X =\\begin{bmatrix} 1 & y_1 & x_1 & x_2 & x_3 \\end{bmatrix} ; \nZ =\\begin{bmatrix} 1 & z & x_1 & x_2 & x_3 \\end{bmatrix}\n$$\n\nthen $\\hat\\beta_{iv}$ is given by\n\n$$\\hat\\beta_{iv} = (Z'X)^{-1}{Z'y}\n$$\n\n- Same assumptions needed, except that instrument strength is measured by the $corr(y_1-E[y_1|X], z-E[z|X] )$\n  - Multicollinearity problem can be a problem here\n  \n## IV and Treatment Effects\n\nOne small note:\n\n- When analyzing the model of interest, we could also try to analyze the reduced form model:\n\n$$y = \\beta_0 + \\lambda z + X \\beta + e$$\n\n- This model will not give you the effect of $y_1$ (endogenous variable), but can be just as interesting, specially when instrument and endogenous variables are dummies.\n\n  - In such case $\\lambda$ will represent the \"intention-to-treat\" effect, rather than \"treatment\" effect.\n\n## 2SLS: Many Ys many Zs\n\n- There could be situations where you have not one, but many instruments.\n  - This may be rare, as even finding a single instrument can be hard\n- You may also have the situation where there is more than one endogenous variable!\n  - In such case, you need at least as many IVs as endogenous variables\n\n1. The same assumptions as before apply. IV's need to be exogenous, but relevant to explain the endogenous variables.\n\n2. Contrary to intuition, ALL instruments are used to analyze ALL exogenous variables\n\n## 2SLS: Estimation\n\nConsider the following:\n\n- $y$ is the variable of interest\n- $x1, x2$ set of exogenous variables \n- $y1, y2$ set of endogenous variables\n- $z1,z2,z3$ set of instruments for $y1$ and $y2$\n\n$X's$ and $Z's$ are exogenous:\n\n##\n\nModel of interesed:\n$$y = a_0 + a_1 y_1 + a_2 y_2 + b_1 x_1 + b_2 x_2 +  b_3 x_3 + e\n$$\n\nFirst Stage \n\n$$\\begin{aligned}\ny_1 = \\gamma^1_0 + \\gamma^1_1 z_1+ \\gamma^1_2 z_2+ \\gamma^1_3 z_3+\\lambda^1_1 x_1 + \\lambda^1_2 x_2 +  \\lambda^1_3 x_3  + v_1 \\rightarrow \\hat y_1 \\\\\ny_2 = \\gamma^2_0 + \\gamma^2_1 z_1+ \\gamma^2_2 z_2+ \\gamma^2_3 z_3+\\lambda^2_1 x_1 + \\lambda^2_2 x_2 +  \\lambda^2_3 x_3  + v_2 \\rightarrow \\hat y_1\n\\end{aligned}\n$$\n\nSecond Stage:\n\n$$y = a_0 + a_1 \\hat y_1 + a_2 \\hat y_2 + b_1 x_1 + b_2 x_2 +  b_3 x_3 + e\n$$\n\nThis last model should work, because $\\hat y_1$ and $\\hat y_2$ are exogenous (if $Zs$ are).\n\nStandard Errors, however, need to be adjusted appropietly. (all two-step estimators need this)\n\n## Matrix Math\n\n$$\\begin{aligned}\nX &= \\begin{bmatrix} 1 & y_k & x  \\end{bmatrix} \\\\\nZ &=\\begin{bmatrix} 1 & z   & x \\end{bmatrix} \n\\end{aligned}\n$$\n\nThen \n\n$$\\begin{aligned}\n\\beta_{2sls} &= [X'Z(Z'Z)^{-1}Z'X]^{-1}[X'Z(Z'Z)^{-1}Z'y] \\\\\n\\beta_{2sls} &= [X'P_z X]^{-1}[X'P_z y] \\\\\n\\beta_{2sls} &= [\\hat X'X]^{-1}[\\hat X'y]\n\\end{aligned}\n$$\n\n## Multicolinearity and 2sls\n\n- When Appplying 2sls, the problem of Multicolinearity could be stronger. \n  - Because of *MCL*, Standard errors will increase (less individual variation.)\n  - Because of IV, only a fraction of the variation is used for the analysis.\n  - Botton line: Combined have conisderate results. \n\n## Weak Instruments\n\n- Weak Instruments create problems when using IV's and 2SLS.\n  - A weak instrument is one that doesnt have much explanatory power on dep variable, once all other controls are taken into account.\n  - If the instrument is too weak, the bias it generates could larger than OLS.\n  - The distribution of coefficent is no longer normal, so its harder to make inference.\n  \n> How do we test for it?\n\n## \n\nWe usually test for weak instruments when analyzing the \"first-stage\" regression.\n\n$$y_2 = \\gamma_0 + \\gamma_1 z_1 + \\gamma_2 z_2 + \\gamma_3 x_1 + \\gamma_4 x_2 + e\n$$\n\n- The null is: $H_0: \\gamma_1 = \\gamma_2 =0$, or the instruments are weak.\n\n- Based on Stock and Yogo (2005), the general recommendation is to get an F>10, to reject the Null.\n\n- However, new evidence and research suggest that F=10 is not large enough.\n  - One should either use F>104 (To keep the same t) (Lee McCrary Moreira Porter, 2020)\n  - or use an alternative t-critica (3.4 for an F=10)\n\n## Examples\n\n::: {.cell execution_count=2}\n``` {.stata .cell-code}\n/*\n* This code is only to show the process. Too long to run in quarto\ncapture program drop simx \nprogram simx, eclass\nclear\nlocal N `1'\nlocal F `2'\nlocal sig = sqrt(`N'/ `F')\nset obs `N'\ngen e = rnormal()\ngen z = rnormal() \ngen x = 1 + z + (e+rnormal())*sqrt(.5)*`sig'\n*sqrt(10) \ngen y = 1 + (e+rnormal())*sqrt(.5)\nreg x z, \nmatrix b=(_b[z]/_se[z])^2\nivreg y (x=z), \nmatrix b=b,_b[x],_b[x]/_se[x]\nereturn post b\nend\n\ntempfile f1 f2 f3 f4 f5\nparallel initialize 14\nparallel sim, reps(5000): simx 500 10\ngen F=10\nsave `f1'\nparallel sim, reps(5000): simx 500 20\ngen F=20\nsave `f2'\nparallel sim, reps(5000): simx 500 40\ngen F=40\nsave `f3'\nparallel sim, reps(5000): simx 500 80\ngen F=80\nsave `f4'\nparallel sim, reps(5000): simx 500 160\ngen F=160\nsave `f5'\n\nclear \nappend using `f1'\nappend using `f2'\nappend using `f3'\nappend using `f4'\nappend using `f5'\n\nren (*) (f_stat b_coef t_stat)\n*/\nuse mdata/ivweak, clear\nset scheme white2\ncolor_style tableau\n\njoy_plot t_stat, over(F) xline(-1.96 1.96) xtitle(t-Stat) dadj(2)\n```\n\n::: {.cell-output .cell-output-display}\n![](8_iv2sls_files/figure-revealjs/cell-3-output-1.png){fig-align='center'}\n:::\n:::\n\n\n## IV for Measurement errors: Two wrongs make one right\n\n- As described previously when independent variables have measurement errors (Classical), using the variable with errors will produced biased coefficients. \n- If you have multiple variables with Mesurement error, however, its possible to use IV to correct the problem.\n  - This is done by using one variable as the instrument of the other.\n\n- FOr this strategy to work, we need the error to be classical. That is, Uncorrelated across each other, and unrelated to the model error.\n\nConsider the following:\n\n$$\\begin{aligned}\ny &= \\beta_0 + \\beta_1 x_1 + e \\\\\n\\check x_1 &= x_1 + u_1 \\\\\n\\tilde x_1 &= x_1 + u_2 \\\\\n\\end{aligned}\n$$\n\n##\n\n- If we use each model, independently, we will have biased coefficients\n- If we combined the, Biases will remain present (albeit lower)\n- If we use one as instrument of the other tho:\n\n$$\n\\begin{aligned}\n\\beta_{1,iv} &= \\frac{cov(\\check x_1, y)}{cov(\\check x_1, \\tilde x_1)} or \\beta_{1,iv} = \\frac{cov(\\tilde x_1, y)}{cov(\\check x_1, \\tilde x_1)} \\\\\n& = \\frac{cov(x_1 + u_1, \\beta_0 + \\beta_1 x_1 + e)}{cov(x_1 + u_2, x_1 + u_1)} \\\\\n& = \\frac{\\beta_1 cov(x_1,x_1) + cov(x_1, e) + \\beta_1 cov(u_1, x_1) + cov(x_1,e)}{cov(x_1 , x_1 )} \\\\\n& = \\beta_1\n\\end{aligned}\n$$\n\n## Example\n\n::: {.cell execution_count=3}\n``` {.stata .cell-code}\nclear\nset obs 1000\ngen x = rnormal()\ngen y = 1 + x + rnormal() \ngen x1 = x + rnormal()\ngen x2 = x + rnormal()\nqui: regress y x\nest sto m1\nqui: regress y x1\nest sto m2\nqui: regress y x2\nest sto m3\ngen x1_x2 = (x1 + x2)/2\nqui: regress y x1_x2\nest sto m3b\nqui:ivregress 2sls  y (x1=x2)\nest sto m4\nqui:ivregress 2sls  y (x2=x1)\nest sto m5\nesttab m1 m2 m3 m3b m4 m5, se\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of observations (_N) was 0, now 1,000.\n\n-------------------------------------------------------------------------------\n> -----------------------------\n                      (1)             (2)             (3)             (4)      \n>        (5)             (6)   \n                        y               y               y               y      \n>          y               y   \n-------------------------------------------------------------------------------\n> -----------------------------\nx                   1.006***                                                   \n>                              \n                 (0.0320)                                                      \n>                              \n\nx1                                  0.520***                                   \n>      1.018***                \n                                 (0.0277)                                      \n>   (0.0640)                   \n\nx2                                                  0.502***                   \n>                      1.038***\n                                                 (0.0277)                      \n>                   (0.0654)   \n\nx1_x2                                                               0.682***   \n>                              \n                                                                 (0.0301)      \n>                              \n\n_cons               1.010***        0.996***        0.988***        0.983***   \n>      0.974***        0.956***\n                 (0.0313)        (0.0380)        (0.0384)        (0.0360)      \n>   (0.0438)        (0.0451)   \n-------------------------------------------------------------------------------\n> -----------------------------\nN                    1000            1000            1000            1000      \n>       1000            1000   \n-------------------------------------------------------------------------------\n> -----------------------------\nStandard errors in parentheses\n* p<0.05, ** p<0.01, *** p<0.001\n```\n:::\n:::\n\n\n# Endogenous IVs and Too many IVs\n\n## Testing for Endogeneity and Overidentifying restrictions\n\n- In general, Instrumental variables are a great tool to deal with **A4** violations.\n  - but, it can be hard to find the perfect IV, and you may still have problems if its Weak. \n\n- In that case, it may be useful to asnwer...Do you have an Endogeneity problem? (empirically rather than theoretically)\n\nTest:\n\n1. Estimate first stage, and save predicted residuals $\\hat v$: \n\n$$y_2 = \\gamma_0 + \\gamma_1 z_1 + \\gamma_2 x_1 + \\gamma_3 x_2 + v\n$$\n\nYou expect residuals to be endogenous\n\n2. Estimate main model \"adding\" the residuals first stage\n\n$$y = \\beta_0 + \\beta_1 y_2 + \\beta_2 x_1 + \\beta_3 x_2 + \\theta \\hat v + e\n$$\n\nTest for $H_0: \\theta=0$.\n\n## \n\n- If the model was endogenous, Then $\\theta$ will be different from zero, and $\\beta's$ different from the case without controlling for it.\n- Otherwise, we reject presence of endogeneity.\n\n- This method has the added advantage:\n\n- For the simple and exactly identified case, 2sls and adding residuals provide the same solution, except for SE. (its called Control function approach)\n\n## overidentifying restrictions\n\n- Some times, you **may** have access to multiple potential instruments. \n\n  - But what if this instruments give you different results? \n  - If you expect/believe a single effect exists, then there may be a problem. (one of they may be endogenous)\n  \n- So how do we test if the instruments are exogenous?\n  - First you need more instruments than endogenous variables.\n  \n\n## \n\n- S1. Estimate Structural Equation \n  $$y=\\beta_0 + \\gamma y_2 + \\beta_1 x_1 + e | y_2 \\sim z_1, z_2\n  $$\n\n- S2. Auxiliary equation\n  $$\n  \\hat e = \\delta_0 + \\delta_1 z_1 + \\delta_2 z2 + \\delta_3 x_1 + v\n  $$ \n\n- S3. Test for Overall Fitness. $nR^2\\sim \\chi^2(q_{iv})$ with $q_{iv} = \\# over IVs$\n\n## IV's as LATES\n\nNOTE\n\n:::{.callout-important}\n\nWhen describing this test, I mentioned that one would be typically worried if observing multiple coefficients when using different IV's.\n\nHowever, IV's can identify different effects, because different IV's may affect different people differently. \n\n$Z1$ may affect, say, only men. $z_2$ only women, $z_3$ only highly educated, etc.\n\nWhen analyzing IV's will be important to undertand who would be affected by the instrument the most, because that may explain why effects vary.\n\n:::\n\n\n## Example {.scrollable}\n\nIgnoring Potential endogeneity\n\n::: {.cell execution_count=4}\n``` {.stata .cell-code code-fold=\"false\"}\nfrause mroz, clear\ndrop if lwage==.\n** But with Endogeneity\nreg lwage educ exper expersq\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(325 observations deleted)\n\n      Source |       SS           df       MS      Number of obs   =       428\n-------------+----------------------------------   F(3, 424)       =     26.29\n       Model |  35.0222967         3  11.6740989   Prob > F        =    0.0000\n    Residual |  188.305144       424  .444115906   R-squared       =    0.1568\n-------------+----------------------------------   Adj R-squared   =    0.1509\n       Total |  223.327441       427  .523015084   Root MSE        =    .66642\n\n------------------------------------------------------------------------------\n       lwage | Coefficient  Std. err.      t    P>|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        educ |   .1074896   .0141465     7.60   0.000     .0796837    .1352956\n       exper |   .0415665   .0131752     3.15   0.002     .0156697    .0674633\n     expersq |  -.0008112   .0003932    -2.06   0.040    -.0015841   -.0000382\n       _cons |  -.5220406   .1986321    -2.63   0.009    -.9124667   -.1316144\n------------------------------------------------------------------------------\n```\n:::\n:::\n\n\nFirst Stage for different IVs\n\n::: {.cell execution_count=5}\n``` {.stata .cell-code code-fold=\"false\"}\nqui: reg educ fatheduc exper expersq\npredict r1 , res\ntest fatheduc\nadde scalar fiv = r(F)\nadde scalar pfiv = r(p)\nest sto m1\nqui: reg educ motheduc exper expersq\npredict r2 , res\ntest motheduc\nadde scalar fiv = r(F)\nadde scalar pfiv = r(p)\nest sto m2\nqui: reg educ fatheduc motheduc exper expersq\npredict r3 , res\ntest motheduc fatheduc\nadde scalar fiv = r(F)\nadde scalar pfiv = r(p)\nest sto m3\n\nesttab m1 m2 m3 , scalar(fiv pfiv) sfmt(%5.2f %5.3f) order(fatheduc motheduc exper expersq) se ///\nstar(* 0.1 ** 0.05 *** 0.01)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n ( 1)  fatheduc = 0\n\n       F(  1,   424) =   87.74\n            Prob > F =    0.0000\n\n ( 1)  motheduc = 0\n\n       F(  1,   424) =   73.95\n            Prob > F =    0.0000\n\n ( 1)  motheduc = 0\n ( 2)  fatheduc = 0\n\n       F(  2,   423) =   55.40\n            Prob > F =    0.0000\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                     educ            educ            educ   \n------------------------------------------------------------\nfatheduc            0.271***                        0.190***\n                 (0.0289)                        (0.0338)   \n\nmotheduc                            0.268***        0.158***\n                                 (0.0311)        (0.0359)   \n\nexper              0.0468          0.0489          0.0452   \n                 (0.0411)        (0.0417)        (0.0403)   \n\nexpersq          -0.00115        -0.00128        -0.00101   \n                (0.00123)       (0.00124)       (0.00120)   \n\n_cons               9.887***        9.775***        9.103***\n                  (0.396)         (0.424)         (0.427)   \n------------------------------------------------------------\nN                     428             428             428   \nfiv                 87.74           73.95           55.40   \npfiv                0.000           0.000           0.000   \n------------------------------------------------------------\nStandard errors in parentheses\n* p<0.1, ** p<0.05, *** p<0.01\n```\n:::\n:::\n\n\nUsing parents education as instruments\n\n::: {.cell execution_count=6}\n``` {.stata .cell-code code-fold=\"false\"}\n* SMALL requests Df adjustment\nqui:ivregress 2sls lwage (educ= fatheduc ) exper expersq, small\nest sto m1\nqui:ivregress 2sls lwage (educ= motheduc ) exper expersq, small\nest sto m2\nqui:ivregress 2sls lwage (educ= fatheduc motheduc) exper expersq, small\nest sto m3\nesttab m1 m2 m3 , se mtitle(IV:Father IV:Mother IV:Parents) ///\nstar(* 0.1 ** 0.05 *** 0.01)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                IV:Father       IV:Mother      IV:Parents   \n------------------------------------------------------------\neduc               0.0702**        0.0493          0.0614*  \n                 (0.0344)        (0.0374)        (0.0314)   \n\nexper              0.0437***       0.0449***       0.0442***\n                 (0.0134)        (0.0136)        (0.0134)   \n\nexpersq         -0.000882**     -0.000922**     -0.000899** \n               (0.000401)      (0.000406)      (0.000402)   \n\n_cons             -0.0611           0.198          0.0481   \n                  (0.436)         (0.473)         (0.400)   \n------------------------------------------------------------\nN                     428             428             428   \n------------------------------------------------------------\nStandard errors in parentheses\n* p<0.1, ** p<0.05, *** p<0.01\n```\n:::\n:::\n\n\n** Testing for endogeneity\n\n::: {.cell execution_count=7}\n``` {.stata .cell-code code-fold=\"false\"}\n* Instrumenting education\nqui:reg  lwage educ  exper expersq r1\nest sto m1\nqui:reg  lwage educ  exper expersq r2\nest sto m2\nqui:reg  lwage educ  exper expersq r3\nest sto m3\n\nesttab m1 m2 m3 , se mtitle(IV:Father IV:Mother IV:Parents) ///\nstar(* 0.1 ** 0.05 *** 0.01)\n\n*see -estat endogenous- after ivregress 2sls\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n------------------------------------------------------------\n                      (1)             (2)             (3)   \n                IV:Father       IV:Mother      IV:Parents   \n------------------------------------------------------------\neduc               0.0702**        0.0493          0.0614** \n                 (0.0341)        (0.0366)        (0.0310)   \n\nexper              0.0437***       0.0449***       0.0442***\n                 (0.0133)        (0.0133)        (0.0132)   \n\nexpersq         -0.000882**     -0.000922**     -0.000899** \n               (0.000397)      (0.000398)      (0.000396)   \n\nr1                 0.0450                                   \n                 (0.0375)                                   \n\nr2                                 0.0684*                  \n                                 (0.0397)                   \n\nr3                                                 0.0582*  \n                                                 (0.0348)   \n\n_cons             -0.0611           0.198          0.0481   \n                  (0.433)         (0.463)         (0.395)   \n------------------------------------------------------------\nN                     428             428             428   \n------------------------------------------------------------\nStandard errors in parentheses\n* p<0.1, ** p<0.05, *** p<0.01\n```\n:::\n:::\n\n\nOverid Test\n\n::: {.cell execution_count=8}\n``` {.stata .cell-code code-fold=\"false\"}\nqui:ivregress 2sls lwage (educ= fatheduc motheduc) exper expersq, small\npredict resid, res\nestat overid \nreg resid fatheduc motheduc exper expersq, notable robust\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  Tests of overidentifying restrictions:\n\n  Sargan (score) chi2(1) =  .378071  (p = 0.5386)\n  Basmann chi2(1)        =  .373985  (p = 0.5408)\n\nLinear regression                               Number of obs     =        428\n                                                F(4, 423)         =       0.11\n                                                Prob > F          =     0.9791\n                                                R-squared         =     0.0009\n                                                Root MSE          =     .67521\n\n```\n:::\n:::\n\n\n# Thats all for today!\nNext week: Limited Dep Variables \nWhen data has kinks\n\n",
    "supporting": [
      "8_iv2sls_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}