{
  "hash": "bbbc20882e2c7c27ec2b4d869d8cdcda",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Math Refresher\"\nsubtitle: Basic Statistics and Probability\nformat: \n    revealjs: \n        theme: [ clean2.scss]\n        slide-number: true\n        width:  1500\n        height: 800\n    pdf: default\n---\n\n\n\n\n\n\n\n## Random Variables\n\n- A random variable is a variable whose value is determined by the outcome of a random experiment. \n  - For example, if we **toss a coin**, the outcome is random, but the possible values of $X$ are 0 and 1. \n  - If we roll a die, the outcome is random with possible values 1, 2, 3, 4, 5, and 6.\n  - **Exact** temperature in a room\n\nThere are two kinds of random variables:\n\n- **Discrete random variables** can only take on a finite number of values. For example, the number of heads in 10 coin tosses is a discrete random variable.\n  - The probability of observing a particular value is not always zero\n\n- **Continuous random variables** can take on any value in a range. For example, the height of a randomly selected person is a continuous random variable.\n\n##\n\n- If $X$ is discrete random variable, then $P(X=c)$ is the probability that $X$ takes on the value $c$. It can be any value between 0 and 1. ()\n\n- By definition, the sum of all probabilities for all feasible values of $X$ is 1. That is, $\\sum_{c} P(X=c)=1$.\n\n- If $X$ is continuous random variable, then $P(X=c)=0$ for any value $c$. \n  - The probability to observe a particular number is zero. \n  - Instead, when using continuous data, we focus on the probability of observing a value in a range. For example, $P(1.7 \\leq X \\leq 1.8)$ is the probability that $X$ is between 1.7 and 1.8, which can be any value between 0 and 1.\n\n## `Stata` and Random Variables\n\n- Computers **CANNOT** generate random numbers. They can only generate pseudo-random numbers.\n  - Random numbers cannot be reproduced.\n  - Pseudo-random numbers can be reproduced, if we know initial conditions. (seed)\n    - For most purposes, pseudo-random numbers are good enough.\n- `Stata` has many built-in function to generate random numbers.\n  - `help random` for more information.\n   \n## Probability Distributions\n\n- A probability distribution is a function that assigns probabilities to the values of a random variable. \n  - For discrete random variables, we can use a table to describe the probability distribution. For example, the probability distribution of the number of heads in 5 coin tosses is:\n\n| Number of heads | Probability |\n|-----------------|-------------|\n| 0               | 0.03125     |\n| 1               | 0.15625     |\n| 2               | 0.3125      |\n| 3               | 0.3125      |\n| 4               | 0.15625     |\n| 5               | 0.03125     |\n\nIn this case, the sum of all probabilities is 1.\n\n## Probability Density Functions\n\n- For continuous random variables, we can use a function to describe the probability distribution. \n  - For example, we can say that the probability distribution of the height of a randomly selected person is:\n\n$$f(x)$$\n\nThis function has important properties:\n\n- $f(x) \\geq 0$ for all $x$.\n- $\\int_{-\\infty}^{\\infty} f(x) dx = 1$.\n- $P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) dx$.\n- $P(X \\leq a) + P(X > a) = 1$.\n- $P(a \\leq X \\leq b) = P(X < b) - P(X < a)$.\n\n## `Stata` and Empirical Distributions\n\n:::{.panel-tabset}\n\n## Theory\n\n- Given a dataset, you can use different tools to estimate the probability distribution or the probability density function of a random variable.\n  - For example, you can use histograms, or frequency tables, to estimate the probability distribution of a discrete random variable.\n  - You can use kernel density plots to estimate the probability density function of a continuous random variable.\n\n## Dicreet\n\n::: {.cell execution_count=2}\n``` {.stata .cell-code code-fold=\"true\"}\nsysuse nlsw88.dta, clear\nreplace grade  = 11 if grade <11\nfre grade\n```\n\n::: {.cell-output .cell-output-display}\n```\n<IPython.core.display.HTML object>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(NLSW, 1988 extract)\n(211 real changes made)\n\ngrade -- Current grade completed\n-----------------------------------------------------------\n              |      Freq.    Percent      Valid       Cum.\n--------------+--------------------------------------------\nValid   11    |        334      14.87      14.88      14.88\n        12    |        943      41.99      42.02      56.91\n        13    |        176       7.84       7.84      64.75\n        14    |        187       8.33       8.33      73.08\n        15    |         92       4.10       4.10      77.18\n        16    |        252      11.22      11.23      88.41\n        17    |        106       4.72       4.72      93.14\n        18    |        154       6.86       6.86     100.00\n        Total |       2244      99.91     100.00           \nMissing .     |          2       0.09                      \nTotal         |       2246     100.00                      \n-----------------------------------------------------------\n```\n:::\n:::\n\n\n## Continuous\n\n::: {.cell execution_count=3}\n``` {.stata .cell-code code-fold=\"true\"}\nkdensity wage, scale(1.25) title(\"Wage Density f(X)\")\n```\n\n::: {.cell-output .cell-output-display}\n![](math_3_files/figure-pdf/cell-3-output-1.png){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n:::\n\n## Joint Probability Distributions\n\n- The joint probability distribution of $X$ and $Y$ is a function that assigns probabilities to the values of $X$ and $Y$. \n\n- For discrete random variables, we can use a table to describe the joint probability distribution.\n\n::: {.cell execution_count=4}\n``` {.stata .cell-code code-fold=\"true\"}\ntab race married, cell nofreq\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n           |        Married\n      Race |    Single    Married |     Total\n-----------+----------------------+----------\n     White |     21.68      51.20 |     72.89 \n     Black |     13.76      12.20 |     25.96 \n     Other |      0.36       0.80 |      1.16 \n-----------+----------------------+----------\n     Total |     35.80      64.20 |    100.00 \n```\n:::\n:::\n\n\n- It must be the case that the sum of all probabilities is 1. \n \n## \n\n-  For continuous variables, estimation and graphical representation is tricky\n  \n-  it must be the case that:\n\n$$\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x,y) dx dy = 1$$\n\n- You may be able to use scatter plots, or contour plots, to represent the joint probability distribution of two continuous random variables.\n\n## \n\n:::{.panel-tabset}\n\n## Scatter Plot\n\n::: {.cell execution_count=5}\n``` {.stata .cell-code code-fold=\"true\"}\nscatter wage ttl_exp , msize(2) mcolor(%10)\n```\n\n::: {.cell-output .cell-output-display}\n![](math_3_files/figure-pdf/cell-5-output-1.png){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n## Heatplot\n\n::: {.cell execution_count=6}\n``` {.stata .cell-code code-fold=\"true\"}\nqui:ssc install heatplot\nheatplot wage ttl_exp , \n```\n\n::: {.cell-output .cell-output-display}\n![](math_3_files/figure-pdf/cell-6-output-1.png){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n## BiDensity\n\n::: {.cell execution_count=7}\n``` {.stata .cell-code code-fold=\"true\"}\nqui:ssc install bidensity\nbidensity wage ttl_exp, levels(10)\n```\n\n::: {.cell-output .cell-output-display}\n![](math_3_files/figure-pdf/cell-7-output-1.png){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n:::\n\n## Conditional Probability\n\nThe conditional probability of $X$ given $Y$ is:\n\n$$P(x|y) = \\frac{P(x,y)}{P(y)}$$\n\nor, the conditional probabilty density function:\n\n$$f(x|y) = \\frac{f(x,y)}{f(y)}$$\n\nAnd if $X$ and $Y$ are independent, then:\n\n$P(x|y) = P(x)$ or $f(x|y) = f(x)$.\n \n## Marginal Probability Distributions\n\nThe marginal probability distribution of $X$ is the probability distribution of $X$ ignoring/regardless the values of $Y$. This can be expressed as:\n\n$$P(x) = \\sum_{z=-\\infty}^{\\infty} P(X=x,y=z) \\text{ or }\nf_x(x) = \\int_{z=-\\infty}^{\\infty} f(x,z)dz$$\n\nThis is also refer to \"integrating out\" the variable $Y$ or averaging over $Y$.\n\n$$P(x) = \\sum_{z=-\\infty}^{\\infty} P(X=x|y=z)P_y(z) \\text{ or }\nf_x(x) = \\int_{z=-\\infty}^{\\infty} f(x|z)f_y(z)dz$$\n\n## Independence\n\nTwo random variables $X$ and $Y$ are independent if and only if:\n\n$$P(x,y) = P(x)P(y) \\text{ or } f(x,y)=f(x)*f(y)$$ \n\nThat means the conditional probability of $X$ given $Y$ is the same as the marginal probability of $X$.\n\n$P(x|y) = P(x)$ or $f(x|y) = f(x)$.\n\n## Summary Statistics\n\nGiven a random variable $X$, there are several summary statistics that can be used to describe the distribution of $X$, without describing the entire distribution\n\n## Central Tendency\n\n- Mean: average value of $X$. \n\n$$\\bar x = E(X) = \\sum_{x} xP(X=x) \\text{ or } E(X) = \\int_{-\\infty}^{\\infty} xf(x)dx$$\n\n- Median: middle value of $X$.\n- Percentile: values that identify the boundaries of the distribuion. Median is the 50th percentile.\n\n$$Q_y(p) = E(Y \\leq Q_y) = p$$\n\n- Mode: most frequent value of $X$.\n\n\n`sum var,d` in `Stata` will give you the mean, median, and selected quantiles.\n\n`mode` can be estimated using egen, or based on empirical distribution.\n\n\n## Dispersion\n\n- Variance: Average squared deviation from the mean.\n\n$$Var(X) = E(X-\\mu)^2 = \\sum_{x} (x-\\mu)^2P(X=x) \\text{ or } Var(X) = \\int_{-\\infty}^{\\infty} (x-\\mu)^2f(x)dx$$\n\n- Standard deviation: square root of the variance. Easier to interpret.\n\n- Range: difference between the maximum and minimum values of $X$. \n\n- Interquartile range: difference between the 75th and 25th percentiles of $X$.\n\n`sum var,d` and `tabstat`  can provide you with most of this information.\n\n\n## Some useful distributions\n\n### Discrete distributions\n\n- **Bernoulli distribution**: $X \\sim Bernoulli(p)$, where $p \\in [0,1]$. \n  - $E(X)=p$ and variance $Var(X)=p(1-p)$. \n  - Flip a coin with probability $p$ of getting heads.\n  - `rbinomial(1, p)`\n\n- **Binomial distribution**: $X \\sim Binomial(n,p)$, where $p \\in [0,1]$ and $n>0$\n  - $E(x)=np$ and $Var(X)=np(1-p)$. \n  - Distribution of the number of successes in $n$ independent Bernoulli trials.\n  - `rbinomial(n, p)`\n\n- **Poisson distribution**: \n  $X \\sim Poisson(\\lambda)$, where $\\lambda>0$\n  - $E(X)=Var(x)=\\lambda$, Typically used for counts. \n  - For example, the number of customers arriving at a store in a given hour.\n  - `rpoisson(lambda)`\n\n##\n\n### Continuous distributions\n\n- **Uniform distribution**: $X \\sim Uniform(a,b)$\n  - $f(x)=\\frac{1}{b-a}$ for $a \\leq x \\leq b$, and $f(x)=0$ otherwise. \n  - $E(X)=\\frac{a+b}{2}$ and $Var(X)=\\frac{(b-a)^2}{12}$. \n  - `runiform(a, b)`\n\n- **Normal distribution**: $X \\sim Normal(\\mu,\\sigma^2)$\n  - $f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$. \n  - $E(X)=\\mu$ and $Var(X)=\\sigma^2$. \n  - `rnormal(mu, sigma)`\n\nOther useful distributions include:\n\n- **t-distribution**, **Chi-squared distribution**, **F-distribution**  \n- `help density_functions`  `help random_number_functions`\n\n",
    "supporting": [
      "math_3_files\\figure-pdf"
    ],
    "filters": []
  }
}