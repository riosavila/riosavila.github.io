{
  "hash": "78e1a747245ee9a928a80f5d5c34250e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Math Refresher\"\nsubtitle: \"Basic Calculus\"\nformat: \n    revealjs: \n        theme: [ clean2.scss]\n        slide-number: true\n        width:  1500\n        height: 800\n    pdf: default\n---\n\n\n\n\n\n\n## Introduction\n\n- This is a refresher on basic calculus. It is not meant to be a substitute for a full course on calculus but rather a quick review of the fundamental concepts and techniques that will be used this semester.\n\n## Limits\n\nThe limit of a function $f(x)$ as $x$ approaches $a$ is the value that $f(x)$ approaches as $x$ gets closer and closer to $a$. We write this as:\n\n$$\n\\lim_{{x \\to a}} f(x) = L\n$$\n\nHere, $L$ is the limit of the function $f(x)$ as $x$ approaches $a$. \n\nFor example, consider the function $f(x) = x^2$. The limit of $f(x)$ as $x$ approaches 2 is 4:\n\n$$\n\\lim_{{x \\to 2}} x^2 = 4\n$$\n\n## Limits to Derivatives\n\nLimits can also be used to define derivatives. The derivative of a function $f(x)$ is the slope of the function at a given point. The derivative of $f(x)$ at $x = a$ is written as $f'(a)$. The derivative is defined as:\n\n$$\nf'(a) = \\lim_{{h \\to 0}} \\frac{f(a+h) - f(a)}{h}\n$$\n\nIn other words, the derivative is the slope of the function at a particular point $a$. This can be approximated numerically by choosing a very small value for $h$.  \n\n## \n\nFor example, consider the function $f(x) = x^2$. The derivative of $f(x)$ at $x = a$ is:\n\n$$\n\\begin{aligned}\nf'(a) &= \\lim_{{h \\to 0}} \\frac{(a+h)^2 - a^2}{h} \\\\\n&= \\lim_{{h \\to 0}} \\frac{a^2 + 2ah + h^2 - a^2}{h} \\\\\n&= \\lim_{{h \\to 0}} (2a + h) = 2a.\n\\end{aligned}\n$$\n\nIf other methods fail, one can always rely on numerical differentiation.\n\n## Stata and Numerical Differentiation\n\n`Stata` can be used to calculate numerical derivatives. `mata` (matrix algebra language) has powerful rutines for numerical differentiation.  `Stata` also has some capabilities, and you can always do it manually.\n\n::: {.cell execution_count=2}\n``` {.stata .cell-code code-fold=\"true\"}\nclear\nrange x -_pi _pi 100\ngen y = sin(x)\ngen dydx = (sin(x+0.01) - sin(x)) / 0.01\ndydx y x, gen(dydx2)\ngen dydx3 = cos(x)\ngen diff1 = (dydx - dydx3)\ngen diff2 = (dydx2 - dydx3)\nline diff1 diff2 x\n```\n\n::: {.cell-output .cell-output-display}\n```\n<IPython.core.display.HTML object>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of observations (_N) was 0, now 100.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](math_1_files/figure-pdf/cell-2-output-3.png){fig-align='center' fig-pos='H'}\n:::\n:::\n\n\n## Derivatives of Common Functions\n\nFor most common functions, the derivative can be calculated using the following rules:\n\n- The derivative of a constant is zero.  \n- The derivative of $x^n$ is $nx^{n-1}$.  \n- The derivative of $\\ln(x)$ is $\\frac{1}{x}$.  \n- The derivative of $e^x$ is $e^x$.  \n- The derivative of $a^x$ is $a^x \\ln a$.  \n\nThere are other rules for derivatives, but these are the ones that will be used most often.\n\n## Derivatives of Composite Functions\n\nThe derivative of a composite function $f(g(x))$ is given by the chain rule:\n\n$$\n\\frac{d}{dx} f(g(x)) = f'(g(x)) \\cdot g'(x).\n$$\n\nFor example, consider the function $f(x) = \\ln(x^2)$. The derivative of $f(x)$ is:\n\n$$\n\\begin{aligned}\n\\frac{d}{dx} \\ln(x^2) &= \\frac{1}{x^2} \\cdot \\frac{d}{dx} (x^2) \\\\\n&= \\frac{1}{x^2} \\cdot 2x \\\\\n&= \\frac{2}{x}.\n\\end{aligned}\n$$\n\n## Derivatives of Sums and Products\n\nThe derivative of a sum of functions is the sum of the derivatives of the functions:\n\n$$\n\\frac{d}{dx} (f(x) + g(x)) = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x).\n$$\n\nThe derivative of a product of functions is given by the product rule:\n\n$$\n\\frac{d}{dx} (f(x) \\cdot g(x)) = f'(x) \\cdot g(x) + f(x) \\cdot g'(x).\n$$\n\nThe derivative of a quotient of functions is given by the quotient rule:\n\n$$\n\\frac{d}{dx} \\left( \\frac{f(x)}{g(x)} \\right) = \\frac{f'(x) \\cdot g(x) - f(x) \\cdot g'(x)}{g(x)^2}.\n$$\n\nThis is a special case of the product rule.\n\n## Maximization and Minimization\n\n- Derivatives can be used to identify the maximum and minimum values of a function. Consider a function $f(x)$. \n\n- To find the maximum (or minimum) value of $f(x)$, we take the derivative of $f(x)$ and set it equal to zero. \n  - This is called the first-order condition. \n  - **The idea** is that at the maximum (or minimum), the value of $f(x)$ shouldn't change anymore (it should be flat). Thus, the derivative of $f(x)$ should be zero.\n\n## \n\nFor example, consider the function $f(x) = 5x^2 - 4x + 2$. The derivative of $f(x)$ is:\n\n$$\n\\begin{aligned}\nf'(x) &= 10x - 4 = 0 \\\\\nx &= \\frac{4}{10} = 0.4.\n\\end{aligned}\n$$\n\nSo when $x$ is equal to 0.4, the function $f(x)$ does not change anymore. \n\n- This, however, is insufficient to determine whether the function is at a maximum or a minimum.\n\n## \n\nTo determine this, we take the second derivative of $f(x)$, known as the second-order condition:\n\n$$\nf''(x) = 10 > 0.\n$$\n\n- Because the second derivative is positive, we know that $f(x)$ is at a minimum when $x = 0.4$. \n  - If the second derivative were negative, we would know that $f(x)$ is at a maximum when $x = 0.4$.\n\n## Why is this the case?\n\n- $f'(x)$ measures the changes in $f(x)$ along $x$. When $f'(x) = 0$, $f(x)$ is not changing anymore.\n- $f''(x)$ measures the changes in $f'(x)$ (the changes in those changes). \n  - Because it is positive, we know that $f'(x)$ is increasing. This means that at $x = 0.4$, the changes in $f(x)$ are going from negative to positive, indicating a minimum.\n\n## Optimization with Multiple Variables\n\nWhen considering multiple variables, we also need to rely on the first- and second-order conditions to find minimum and maximum values. Consider a function $f(x, y)$. The first-order conditions are:\n\n$$\n\\begin{aligned}\n\\frac{\\partial}{\\partial x} f(x, y) &= 0, \\\\\n\\frac{\\partial}{\\partial y} f(x, y) &= 0.\n\\end{aligned}\n$$\n\nThese conditions indicate that, in the direction of $x$ and $y$, the function $f(x, y)$ is not changing anymore. Thus, we have a potential maximum or minimum. To identify a minimum, we need second-order conditions:\n\n$$H = \\begin{bmatrix}\nf_{xx} & f_{xy} \\\\\nf_{yx} & f_{yy}\n\\end{bmatrix}\n$$\n\n##\n$$H = \\begin{bmatrix}\nf_{xx} & f_{xy} \\\\\nf_{yx} & f_{yy}\n\\end{bmatrix}\n$$\n\nwhere $H$ is the **Hessian** matrix. \n\n- If $\\text{Det}(H) > 0$ and $f_{xx} > 0$, then we have a minimum. \n- If $\\text{Det}(H) > 0$ and $f_{xx} < 0$, then we have a maximum. \n- If $\\text{Det}(H) < 0$, then we have a saddle point. \n- If $\\text{Det}(H) = 0$, the result is inconclusive.\n\n## Optimization with Constraints\n\nWhen optimizing a function with constraints, we can use the method of Lagrange multipliers. Consider a function $f(x, y)$ subject to the constraint $g(x, y) = z$. The Lagrangian is:\n\n$$\nL(x, y, \\lambda) = f(x, y) + \\lambda (z - g(x, y)).\n$$\n\n- The Lagrangian is the function $f(x, y)$ plus the constraint $g(x, y)$ multiplied by a constant $\\lambda$. \n- The constant $\\lambda$ is called the Lagrange multiplier. \n- The constraint is written as the difference between the constant $z$ and the function $g(x, y)$. \n- The Lagrangian is then optimized with respect to $x$, $y$, and $\\lambda$. \n\n##\n\nThese are the equivalent first-order conditions:\n\n$$\n\\begin{aligned}\n\\frac{\\partial}{\\partial x} L(x, y, \\lambda) &= 0, \\\\\n\\frac{\\partial}{\\partial y} L(x, y, \\lambda) &= 0, \\\\\n\\frac{\\partial}{\\partial\\lambda} L(x, y, \\lambda) &= z - g(x, y) = 0.\n\\end{aligned}\n$$\n\nThe last condition is the constraint, and it implies that the constraint must be satisfied. The second-order conditions are the same as before.\n\n",
    "supporting": [
      "math_1_files\\figure-pdf"
    ],
    "filters": []
  }
}