{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Research Methods II\"\n",
        "subtitle: \"Significance and Missing Data\"\n",
        "author: Fernando Rios-Avila\n",
        "format: \n",
        "  revealjs: \n",
        "    slide-number: true\n",
        "    width: 1600\n",
        "    height: 900\n",
        "    code-fold: true\n",
        "    echo: true\n",
        "    css: styles.css \n",
        "    highlight-style: github\n",
        "execute: \n",
        "  freeze: auto    \n",
        "---\n",
        "\n",
        "\n",
        "# Statistical Significance\n",
        "\n",
        "## Statistical Significance\n",
        "\n",
        "-   What is statistical significance?\n",
        "    -   Statistical significance is a way of determining if an observed effect is due to chance.\n",
        "-   We typically use statistical significance to determine if the results of a study are meaningful, using various criteria.\n",
        "    -   Is the p-value less than 0.05?\n",
        "    -   Does the 95% confidence interval include zero?\n",
        "    -   is the t-statistic greater than 1.96?\n",
        "-   But what exactly does that tell us?\n",
        "\n",
        "## \n",
        "\n",
        "### \n",
        "\n",
        "![](https://imgs.xkcd.com/comics/p_values.png)\n",
        "\n",
        "## Back to the basics\n",
        "\n",
        "-   Assume you are testing for the effectiveness of a medicine that treats the common cold.\n",
        "    -   How do you know if the treatment is effective? (i.e., Does reduce the duration of the cold?)\n",
        "    -   We make an hypothesis!\n",
        "    -   The null hypothesis is that there is no effect of the treatment. (H0: no effect)\n",
        "    -   You collect some data and find out that it reduces the duration of the cold by 1 days.\n",
        "    -   Is this a significant effect?\n",
        "\n",
        "## \n",
        "\n",
        "### Depends...\n",
        "\n",
        "-   The sample size (how many people were in the study?)\n",
        "    -   If the sample size is 10,000, then a 1 day reduction in the duration of the cold may be significant.\n",
        "    -   But if the sample size is 10, then a 1 day reduction may be due to chance.\n",
        "-   The effect size\n",
        "    -   If the effect size is 0.1 days, may not be significant.\n",
        "    -   If the effect size is 10 days, may be significant.\n",
        "\n",
        "## \n",
        "\n",
        "### Then what?\n",
        "\n",
        "-   Statistical significance is a way of determining if the observed effect is due to chance.\n",
        "\n",
        "-   To do this, we required assumptions about the distribution under the Null Hypothesis.\n",
        "\n",
        "    -   Assume that the data is normally distributed.\n",
        "    -   And that there is a 4% chance you observe a value as extreme as the one you observed\n",
        "        -   In that case you would say, the effect is significant.\n",
        "\n",
        "-   Just couple of caveats:\n",
        "\n",
        "    -   Signicance can be achieved by either measuring a \"large\" effect\n",
        "    -   or by measuring a \"small\" effect with a large sample size. (very precisely)\n",
        "\n",
        "-   Thus, finding no significance could mean the effect is noise or that the sample size is too small.\n",
        "-   Recall Hypothesis can be true or not. We only know if the data is consistent with the hypothesis or not.\n",
        "\n",
        "## \n",
        "\n",
        "### From Significance to Power\n",
        "\n",
        "-   The power of a statistical test represents the probability of detecting an effect, given that the effect is real.\n",
        "\n",
        "-   For example, say a drug has the effect of reducing the duration of the common cold by 1 day. But you do not know this\n",
        "\n",
        "-   Instead you make your hypothesis. How likely is that the effect is significant?\n",
        "\n",
        "    -   You find the effect is not significant at 10% level.\n",
        "\n",
        "-   What is happening?\n",
        "\n",
        "    -   The effect was true, yet we find no significance.\n",
        "    -   The power of the test was low. (sample size was too small)\n",
        "\n",
        "-   In General, Setting high significance levels will reduce the power of the test.\n",
        "\n",
        "# Multiple Hypothesis Testing\n",
        "\n",
        "##\n",
        "### If At First You Donâ€™t Succeed, Try, Try Again\n",
        "\n",
        ":::{.panel-tabset}\n",
        "\n",
        "## RQuestion\n",
        "\n",
        "![](images/paste-2.png)\n",
        "\n",
        "## Testing\n",
        "\n",
        "![](images/paste-3.png)\n",
        "\n",
        "## Results\n",
        "![](images/paste-5.png)\n",
        "\n",
        ":::\n",
        "\n",
        "    Cartoon from xkcd, by Randall Munroe\n",
        "\n",
        "## \n",
        "### Multiple Hypothesis Testing\n",
        "\n",
        "-   In the previous example, the \"SAME\" hypothesis was tested multiple times. \n",
        "-   Yet, it was still compared to the same significance level. Is this correct?\n",
        "\n",
        "Consider the following:\n",
        "\n",
        "- You collect 100 data points, from a normal distribution, with mean 0 and standard deviation 1.\n",
        "- You know there is only a 5% chance that the mean is greater (abs) than 0.196 (95% confidence interval)\n",
        "\n",
        "So you run the same experiment 100 times\n",
        "    - How many times do you expect to find a mean greater than 0.196?\n",
        "    - What are the chances of finding a mean greater than 0.196 at least once?\n",
        "\n",
        "##\n",
        "###\n",
        "\n",
        "- **A1**: 5% of the time\n",
        "- **A2**: \n",
        "  - Pr of finding any \"significant\" effect in one experiment: $1 - 0.95 = 0.05$\n",
        "  - Pr of finding any \"significant\" effect in two experiments: $1-0.95^2 = 0.0975$\n",
        "  - Pr of finding no effect in 100 experiments: $1-0.95^{100}  = 0.99408$\n",
        "  \n",
        "So if you run the experiment enough times, you are almost certain to find a \"significant\" effect.\n",
        "\n",
        "- Also, While a single experiment has a 5% chance of finding a \"significant\" effect (alpha = 0.05),  the \"alpha\" for 2 experiments is 0.0975! \n",
        "\n",
        "##\n",
        "### Controlling for Multiple Hypothesis Testing\n",
        "\n",
        "- There are various ways to control for multiple hypothesis testing.\n",
        "   \n",
        " - SIDAK = $\\alpha_{adj} = (1-\\alpha_{tg})^{1/n}$\n",
        " - BONFERRONI = $\\alpha_{adj} = \\alpha_{tg}/n$\n",
        " - HOLM = $\\alpha_{adj,i} = \\alpha_{tg}/(n-i+1)$\n",
        "\n",
        "Where $\\alpha_{tg}$ is the target $\\alpha$ level (e.g., 0.05), and $n$ is the number of tests, and $\\alpha_{i,adj}$ is the adjusted $\\alpha$ level.\n",
        "\n",
        "> There is also Uniform Confidence Intervals (see [here](https://friosavila.github.io/stata_do/stata_do5.html))\n",
        "\n",
        "- There is a caveat. They are designed to control for Type I errors (False positives), but they increase the chances of Type II errors. (Less power)\n",
        "\n",
        "# Missing/incomplete Data\n",
        "\n",
        "## Missing Data\n",
        "\n",
        "- Missing data is a common problem in empirical research.\n",
        "- Due to various reasons, some observations may be missing.\n",
        "  - Refusal to answer a question\n",
        "  - Data entry errors/ommissions\n",
        "  - Data loss\n",
        "  - etc.\n",
        "- This can be a problem for various reasons:\n",
        "  - Missing data can produced biased and inconsistent estimates. \n",
        "  - It may also reduce sample size, and thus power. (Potentially making estimation unfeasible)\n",
        "- So what can we do?\n",
        "\n",
        "## Types of Missing Data\n",
        "\n",
        "-   Missing Completely at Random (MCAR)\n",
        "    -   The probability of missing data does not depend on any observed or unobserved data.\n",
        "    -   This is the best case scenario. (this is like sampling)\n",
        "-   Missing at Random (MAR)\n",
        "    -   The probability of missing data depends on observed data.\n",
        "    -   Second Best: Its possible to address the problem using various methods.\n",
        "-   Missing Not at Random (MNAR)\n",
        "    -   The probability of missing data depends on unobserved data.\n",
        "    -   Worst case scenario: It is usually very difficult to address \n",
        "\n",
        "## What its done, and what can be done\n",
        "\n",
        "-   Complete Case Analysis (CCA)\n",
        "    -   Drop observations with missing data.\n",
        "    -   This is the default in most statistical software.\n",
        "    -   This is a bad idea, unless the data is MCAR.\n",
        "-   Imputation\n",
        "    -   Replace missing values with a value.\n",
        "    -   This is a better idea, but it depends on the type of missing data.\n",
        "    -   Requires modeling the missing data mechanism, and outcome model.\n",
        "-   Reweighting\n",
        "    -   Weight observations to account for missing data.\n",
        "    -   Requires modeling the missing data mechanism\n",
        "\n",
        "## Reweighting\n",
        "\n",
        "Consider the following example:\n",
        "$$\\begin{aligned}\n",
        "\\text{Pop}&: y = x\\beta+ \\epsilon \\\\\n",
        "\\text{Miss Mech }&: p(nmiss|x) = F(x\\gamma) \\\\\n",
        "\\text{Miss Reg }&: m\\times y = m\\times x \\beta + m\\times \\epsilon  \n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "-   Where $m$ is an indicator of missingness, and $F$ is the function of missing.\n",
        "\n",
        "- Define the Weights as $w = \\frac{1}{1-p(nmiss|x)}$\n",
        "\n",
        "- Then, we could use WLS to estimate the model of interest:\n",
        "\n",
        "$$w \\times m\\times y = w \\times  m\\times x \\beta + w \\times  m\\times \\epsilon$$\n",
        "\n",
        "## Example{.scrollable}\n",
        "\n",
        ":::{.panel-tabset}\n",
        "\n",
        "## Code\n",
        "\n",
        "```stata\n",
        "frause oaxaca, clear\n",
        "drop if lnwage ==.\n",
        "** Modeling Missing\n",
        "reg lnwage c.(educ exper tenure female age)## c.(educ exper tenure female age)  \n",
        "predict lxb\n",
        "qui:sum lxb \n",
        "replace lxb = normal((lxb -r(mean))/r(sd))\n",
        "gen lnwage2 = lnwage if lxb <runiform()\n",
        "gen dwage = lnwage2!=.\n",
        "** Modeling Missing data\n",
        "logit dwage educ exper tenure female age\n",
        "predict prw, pr\n",
        "gen wgt = 1/prw\n",
        "** Estimating the model\n",
        "reg lnwage educ exper tenure female age\n",
        "reg lnwage2 educ exper tenure female age\n",
        "reg lnwage2 educ exper tenure female age [w=wgt]\n",
        "** Repeat the process 1000 times\n",
        "```\n",
        "\n",
        "## Results\n",
        "\n",
        "![](s5_fig1.png)\n",
        "\n",
        "\n",
        ":::\n",
        "\n",
        "## Imputation: Mean and Predictive Mean\n",
        "\n",
        "- The second approach is to impute the missing values. AKA Substitute the unobserved values with some prediction we can construct. \n",
        "\n",
        "Consider the case of a single variable $Z$ with missing values, and assume we have a model for $Z$:\n",
        "\n",
        "$$Z = X\\beta + \\epsilon\n",
        "$$\n",
        "\n",
        "##\n",
        "###\n",
        "\n",
        "- We could \"predict\" missing values using the mean of the observed values:\n",
        "\n",
        "$$\\hat{Z} = \\bar{Z} = \\frac{1}{n}\\sum_{i=1}^n Z_i$$\n",
        "\n",
        "- Or we could use the predicted values from the model:\n",
        "\n",
        "$$\\hat{Z} = X\\hat{\\beta}$$\n",
        "\n",
        "Neither is a good idea, even under MCAR. (Why?)\n",
        "\n",
        "- We are getting rid of ALL uncertainty (variance) in the missing values.\n",
        "\n",
        "## \n",
        "### Better Approach: Stochastic Imputation\n",
        "\n",
        "- A better approach of imputation is to use a model to predict not only the \"known\" variation (Conditional mean), but also the \"unknown\" variation (Conditional variance).\n",
        " \n",
        "- So, we can use the model to predict the missing values, but we add some noise to the prediction.\n",
        "\n",
        "$$\\tilde z = X\\hat{\\beta} + \\hat \\epsilon$$\n",
        "\n",
        "- Where $\\hat \\epsilon$ is a \"random\" residual obtain based on the model assumptions.\n",
        "- $\\tilde z$ is a stochastic imputation of $z$.\n",
        "\n",
        "##\n",
        "### Even Better: Account for the uncertainty in the model\n",
        "\n",
        "- We can also account for the uncertainty in the model by considering the uncertainty in the model parameters, and the error:\n",
        "\n",
        "$$z = X\\beta + \\epsilon\n",
        "$$\n",
        "\n",
        "- Under normality assumptions, we could estimate the model using MLE, and obtain the variance covariance matrix of the parameters.\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "\\hat{\\beta} \\\\ \\hat{\\sigma}^2 \\end{pmatrix} \n",
        "\\sim N\\left(\\begin{bmatrix} \\beta \\\\\n",
        "\\sigma\n",
        "\\end{bmatrix}, \\begin{bmatrix}\n",
        "V_{\\beta} & 0 \\\\\n",
        "0 & V_{\\sigma}\n",
        "\\end{bmatrix}\\right)\n",
        "$$\n",
        "\n",
        "- So, we can get $\\tilde \\beta$ and $\\tilde \\sigma$, from random draws from the distribution above, and then use them to impute the missing values.\n",
        "\n",
        "##\n",
        "### Even Better than before: Multiple Imputation\n",
        "\n",
        "- The previous methods assumed you only need one imputation to solve the Imputation problem.\n",
        "\n",
        "- One, however, may not be enough to account for the uncertainty in the imputation process.\n",
        "\n",
        "- So, we can repeat the imputation process multiple times, and obtain multiple imputed values for each missing data.\n",
        "\n",
        "- With multiple imputed values, we can estimate the model of interest multiple times, and then combine the results using Rubin's rules.\n",
        "\n",
        "##\n",
        "###\n",
        "\n",
        "- Call M the number of imputations, and $m$ the imputation index.\n",
        "\n",
        "$$\\beta_{MI} = \\frac{1}{M}\\sum_{m=1}^M \\beta_m$$\n",
        "\n",
        "$$V_{MI} = \\frac{1}{M}\\sum_{m=1}^M V_m + \\left(\\frac{M+1}{M}\\right)Var(\\beta_m)$$\n",
        "\n",
        "- Where $V_m$ is the VCV matrix of the parameters for each imputation, and $Var(\\beta_m)$ is the variance of the parameters across imputations.\n",
        "\n",
        "$$df = (M-1) \\left( 1 + \\frac{M}{M+1}\\frac{Var_m}{Var_B}\\right)^2$$\n",
        "\n",
        "## {.scrollable}\n",
        "### Example: `Stata`\n"
      ],
      "id": "84694d59"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| classes: larger\n",
        "*| code-fold: false\n",
        "frause oaxaca, clear\n",
        "drop if lnwage ==.\n",
        "\n",
        "** Modeling Missing\n",
        "foreach i in  educ exper tenure age {\n",
        "    gen m_`i' = `i' if runiform()>.25\n",
        "}"
      ],
      "id": "52288277",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting data for -`mi`- commands\n"
      ],
      "id": "2a282430"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| classes: larger\n",
        "*| code-fold: false\n",
        "mi set wide\n",
        "mi register imputed m_*\n",
        "mi impute chain (reg) m_* = lnwage single female, add(10) "
      ],
      "id": "a1a971f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estmating the model(s):\n"
      ],
      "id": "c2c4e739"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| classes: larger\n",
        "*| code-fold: false\n",
        "mi estimate, post: regress lnwage m_* single female\n",
        "est sto m1\n",
        "regress lnwage educ exper tenure age single  female"
      ],
      "id": "3db58441",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What about LDV models?\n",
        "\n",
        "- The method sketched above (OLS) can also be extended to other models\n",
        " \n",
        "- Consider Logit models\n",
        "\n",
        "**S1**: Estimate Logit model: $P(y=1|X) = F(X\\beta)$  \n",
        "\n",
        "**S2**: Draw $\\beta$ from the distribution, call it $\\tilde\\beta$  \n",
        "\n",
        "**S3**: Draw $y$ from a Bernoulli distribution: $y \\sim Bernoulli(F(X\\tilde\\beta))$\n",
        "\n",
        "- Similar procedures can be done for other models.\n",
        "\n",
        "## Other Methods: HotDecking\n",
        "\n",
        "-   Hotdecking is a method of imputation that uses the observed values of the data to impute the missing values.\n",
        "-   Because it uses data from the empirical distribution (observed data), it produces \"valid\" imputations for any kind of data.\n",
        "-   The idea is to find a pool of potential \"donors\" for the one with missing data. (similar observations)\n",
        "-   Then select one candidate and use its data to impute the missing values.\n",
        "\n",
        "##\n",
        "### Definition of a \"donor\"\n",
        "\n",
        "- Donors are identified as observations with similar characteristics to the one with missing data. (close to the missing observation)\n",
        "\n",
        "- Finding potential donors is easy when there is low dimensional data, but it becomes more difficult as the number of variables increases.\n",
        "\n",
        "## Example {.scrollable}\n",
        "\n",
        "Imputing data for wages in `oaxaca`.\n"
      ],
      "id": "2259c59f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| classes: larger\n",
        "*| code-fold: false\n",
        "frause oaxaca, clear\n",
        "** ID pool of donors based on age and gender\n",
        "egen id_pool = group(age female)\n",
        "** Now, for each missing observation select a \"random\" donor\n",
        "gen misswage =missing(lnwage)\n",
        "bysort id_pool (misswage):egen smp = sum(misswage==0)\n",
        "bysort id_pool: gen draw = runiformint(1, smp)\n",
        "bysort id_pool: replace lnwage = lnwage[draw] if misswage==1\n",
        "sum lnwage if misswage==0\n",
        "sum lnwage if misswage==1 \n",
        "sum lnwage "
      ],
      "id": "88167559",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##\n",
        "\n",
        "With many variables, we often estimating some distance measure and/or data reduction to ID \"close\" observations:\n",
        "\n",
        "- Propensity Score (based on logit/probit/regress).\n",
        "  \n",
        "$$D(X,X_0) = abs(G(X) - G(X_0))$$\n",
        "\n",
        "- Mahalanobis distance (based on X covariance matrix)\n",
        "\n",
        "$$D(X,X_0) = \\sqrt{(X-X_0)'\\Sigma^{-1}(X-X_0)}$$\n",
        "\n",
        "- Affinity score: based on some linear combination of variables.\n",
        "\n",
        "$$D(X,X_0) = \\frac{1}{K}\\sum_{i=1}^K \\alpha_i f\\left(\\frac{X_i - X_{0i}}{h}\\right)$$\n",
        "\n",
        "##\n",
        "\n",
        "- Once distances are estimated, donor pools can be defined based on the distance measure.\n",
        "  - Say, all observations with distance less than 0.1.\n",
        "- And the donor can be selected randomly from the pool. (or weighted by distance)\n",
        "\n",
        "- This approaches could be vary computationally intensive, because it requires estimating $N\\times N$ distances.\n",
        "\n",
        "- Complexity may be reduced by using data reduction techniques, such as PCA, FA or propensity scores\n",
        "  \n",
        "## {.scrollable}\n",
        "### Example: `Stata`\n",
        "\n",
        "\n",
        "Using single Score (Data reduction)\n"
      ],
      "id": "5f1d1f2d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| classes: larger\n",
        "*| code-fold: false\n",
        "frause oaxaca, clear\n",
        "drop if lnwage ==.\n",
        "// 25% of data is missing\n",
        "gen mlnwage = lnwage if runiform()>.25\n",
        "gen misswage =missing(mlnwage)\n",
        "qui:logit misswage  educ age agesq female single married\n",
        "predict psc, xb\n",
        "qui:reg mlnwage  educ age agesq female single married\n",
        "predict lnwh, xb\n",
        "qui:pca educ age agesq female single married \n",
        "qui:predict pc1, score"
      ],
      "id": "70836805",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For imputation, lets do something simple, Use data from the closet observation (with lower score) as donor.\n"
      ],
      "id": "0731eae5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| classes: larger\n",
        "*| code-fold: false\n",
        "\n",
        "foreach i in psc lnwh pc1 {\n",
        " \n",
        "    drop2   lnwage_`i'\n",
        "    gen lnwage_`i' = mlnwage    \n",
        "\tsort `i'\n",
        "\treplace lnwage_`i'=lnwage_`i'[_n-1] if lnwage_`i'==. & lnwage_`i'[_n-1]!=.\n",
        "\t*replace lnwage_`i'=lnwage_`i'[_n+1] if lnwage_`i'==. & lnwage_`i'[_n+1]!=.\n",
        "    qui:_regress lnwage_`i'  educ age agesq female single married\n",
        "    matrix b`i' = e(b)\n",
        "\tmatrix coleq b`i'=`i'\n",
        "\t\n",
        "}\n"
      ],
      "id": "c3568ed0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estimate models 1000 times, and lets see results\n",
        "\n",
        "## Comparison of methods\n",
        "\n",
        "![](s5_fig7.png)\n",
        "\n",
        "\n",
        "# Till next time!\n",
        "What happens when not some but ALL data is missing?"
      ],
      "id": "802b1313"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "nbstata",
      "language": "stata",
      "display_name": "Stata (nbstata)",
      "path": "C:\\Users\\Fernando\\AppData\\Roaming\\jupyter\\kernels\\nbstata"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}