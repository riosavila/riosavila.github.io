{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Research Methods II\"\n",
        "subtitle: \"Session 6: Imputation: Statistical Matching\"\n",
        "author: Fernando Rios-Avila\n",
        "format: \n",
        "  revealjs: \n",
        "    slide-number: true\n",
        "    width: 1600\n",
        "    height: 900\n",
        "    code-fold: true\n",
        "    echo: true\n",
        "    css: styles.css \n",
        "    highlight-style: github\n",
        "execute: \n",
        "  freeze: auto    \n",
        "---\n",
        "\n",
        "\n",
        "# Missing Data vs MISSING DATA\n",
        "\n",
        "## Missing Data\n",
        "\n",
        "-   As we described before, missing data is a problem for micro data analysis.\n",
        "    -   Reduces sample size, statistical power, and may bias estimates. (depending on the type of missingness)\n",
        "-   We have also discussed that there are few ways to deal with missing data.\n",
        "    -   Complete case analysis\n",
        "    -   Reweighting\n",
        "    -   Imputation: Prediction\n",
        "    -   Imputation: Hotdecking\n",
        "-   This methods allows you solve for missing data if data is MCAR or MAR.\n",
        "    -   with MNAR, dealing with missing data is difficult\n",
        "-   Nevertheless, you can deal with Missing data, because you have some observed data that can be used to impute it.\n",
        "\n",
        "## Types of Missing data\n",
        "\n",
        "::: panel-tabset\n",
        "## T1\n",
        "\n",
        "![](images/paste-8.png){width=\"900\"}\n",
        "\n",
        "## T2\n",
        "\n",
        "![](images/paste-9.png){width=\"900\"}\n",
        "\n",
        "## T3\n",
        "\n",
        "![](images/paste-10.png){width=\"900\"}\n",
        ":::\n",
        "\n",
        "## MISSING DATA\n",
        "\n",
        "-   What would happen if all data is missing?\n",
        "\n",
        "-   Example:\n",
        "\n",
        "    -   You are working with the CPS, but are interested in looking at the relationship between income and time use.\n",
        "        -   CPS does NOT have time use data.\n",
        "\n",
        "-   We are going for the -lion hunt-\n",
        "\n",
        "    -   You can't impute time use data\n",
        "    -   You can't use complete case analysis\n",
        "    -   You can't use reweighting\n",
        "    -   You can't use hotdecking\n",
        "    -   what do we do?\n",
        "\n",
        "## \n",
        "\n",
        "### What do we do?\n",
        "\n",
        "-   One option would be using a different data set.\n",
        "\n",
        "    -   In the US, the American Time Use Survey (ATUS) could be a good option.\n",
        "    -   But...The data has no income information!\n",
        "\n",
        "-   What if we could combine the two data sets?\n",
        "\n",
        "-   This changes the problem from Missing all data, to one of Missing Data by design.\n",
        "\n",
        "    -   Some segment of the population was asked about income, and some other segment was asked about time use.\n",
        "\n",
        "## Imputation and Statistical Matching\n",
        "\n",
        "-   If you consider the idea of combining two data sets, you can treat the problem as one of imputation.\n",
        "\n",
        "    -   You have a sample (two) that represents the population of interest.\n",
        "    -   We can reasonably assume the data is MCAR. But the data of interest is not observed at the same time.\n",
        "    -   Then, we can use the combine data to impute the missing data, using many of the approaches we have discussed before.\n",
        "\n",
        "-   And there is also another method that is more commonly used (at Levy) to deal with this problem.\n",
        "\n",
        "    -   **Statistical Matching** (aka Data Fusion).\n",
        "\n",
        "-   What does this imply?:\n",
        "\n",
        "    -   Match individuals across datasets (\"Donor\" and \"Recipient\")\n",
        "    -   Transfer information based on the matching links\n",
        "\n",
        "## Official examples:\n",
        "\n",
        "There is a lot of work on this topic. Many statistical agencies use this approach to combine data Survey data with administrative data.\n",
        "\n",
        "- Administrative data is usually more accurate, but it is not collected for the purpose of research.\n",
        "- Survey data is collected for research purposes, but may not have accurate data in some areas (income)\n",
        "- Unless Survey Data was collected with the purpose of being linked with administrative data, one requires methods similar to statistical matching to combine both data sets.\n",
        "\n",
        "## In house Some examples:\n",
        "\n",
        "- At Levy we have used this approach to produce relevant datasets:\n",
        "  - LIMEW: Levy Institute Measure of Economic Well-Being\n",
        "    Combines Time use, wealtgh and Survey data (in addition to other aggregate data)\n",
        "\n",
        "  - LIMTIP: Levy Institute Measure of Time and Income Poverty\n",
        "    Combines ATUS, with income/consumption data \n",
        "\n",
        "# Framework    \n",
        "\n",
        "## What do we need?\n",
        "\n",
        "- Consider two data sets: $A$ and $B$.  \n",
        "- $A$ has informtion on $X$ and $Z$\n",
        "- $B$ has information on $Y$ and $Z$\n",
        "- We want a file that has $X$, $Y$ and $Z$.\n",
        "  \n",
        "## Assumptions\n",
        "\n",
        "- ($X,Y,Z$) are multivariate random variables with joint distribution $f(x,y,z)$, that represents the population of interest. \n",
        "- Both datasets are random samples from the same population of interest.\n",
        "\n",
        "  $\\frac{P_w(D=A|X,Y,Z)}{P_w(D=B|X,Y,Z)} = \\frac{P(D=A)}{P(D=B)} = 1$\n",
        "\n",
        "- Conditional Independence assumption: \n",
        "  - $Y$ and $Z$ are independent from each other given $X$.\n",
        "  $$f(x,y|z) = f(x|z)f(y|z)$$\n",
        "\n",
        "-   The goal is to combine the two data sets to produce a file that has data on $X$, $Y$ and $Z$. by  identifying $f(x,y,z)$. \n",
        "  \n",
        "## Statistical Matching: Limitations\n",
        "\n",
        "-   The quality of this identification will depend on how well the conditional independence assumption holds.\n",
        "   \n",
        "-   Because of this, synthetic datasets can't tell you much about covariances or causal relationships  \n",
        "   \n",
        "$$Cov(z,y,z) = \\begin{pmatrix} \n",
        "V(X) & \\color{red}{V(X,Y)} & V(X,Z) \\\\ \n",
        "\\color{red}{V(X,Y)'} & V(Y) & V(Y,Z) \\\\ \n",
        "V(X,Z)' & V(Y,Z)' & V(Z) \n",
        "\\end{pmatrix}\n",
        "$$\n",
        "    \n",
        "albeit, you can impose certain bounderies on the covariance matrix.\n",
        "\n",
        "## Matching Approaches:  \n",
        "\n",
        "There are two types of **statistical matching** procedures:\n",
        "\n",
        "::: {.panel-tabset}\n",
        "\n",
        "## Unconstrained Matching\n",
        "\n",
        "- Records from $A$ and $B$ can be used multiple times (or none) in the matching.\n",
        "  - Absurd case: One observation from $A$ is matched with all observations from $B$. \n",
        "- This is the most common approach in the literature for policy evaluation\n",
        "  \n",
        "  Pros: Uses the \"best\" candidate for the matching.\n",
        "  Cons: It may not transfer the uncoditional distribution of the data.\n",
        "\n",
        "- Does not necessarily required $A$ and $B$ to be from the same population. (weighted size)\n",
        "\n",
        "## Constraints Matching\n",
        "\n",
        "- All records from $A$ and $B$ are used once and only once in the matching. (without replacement)\n",
        "- When using weighted samples, records are matched until the weights are exhausted.\n",
        "  - Requires $A$ and $B$ to be from the same population. (weighted size)\n",
        "  \n",
        "  Pros: It transfers the unconditional distribution of the data.\n",
        "  Cons: My not use \"best\" candidate for the matching.\n",
        "  \n",
        ":::\n",
        "\n",
        "## Matching Records:\n",
        "\n",
        "- Matching records, requires defining a measure of similarity between records.\n",
        " \n",
        "- This measures can vary depending on the data type, and dimensionality of the data\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\text{ Euclidian: } d(r^A,r^b) &= \\sqrt{\\sum_i^k(x^A_i-x^B_i)^2 } \\\\\n",
        "\\text{ SdEuclidian: } d(r^A,r^b) &= \\sqrt{\\sum_i^k\\left(\\frac{x^A_i-x^B_i}{\\sigma_j}\\right)^2 } \\\\\n",
        "\\text{ Mahalanobis: } d(r^A,r^b) &= \\sqrt{(x^A-x^B)'\\Sigma_x^{-1}(x^A-x^B)} \\\\ \n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "- All this measures are useful when one has high dimensional data.\n",
        " \n",
        "## Matching: Reducing Dimensionality\n",
        "\n",
        "- A second alternative is to reduce data dimensionality before estimating distances.\n",
        "\n",
        "::: {.panel-tabset}\n",
        "\n",
        "## Predictive mean matching:\n",
        "\n",
        "  - Model $x = z\\beta + \\epsilon$ using $A$. \n",
        "  - Make predictions $z\\hat\\beta$ for both samples.\n",
        "  - Match records based on $z\\hat\\beta$\n",
        "  - Good results to match individuals with similar \"predicted\" income.\n",
        "  - Puts more \"weight\" on the variables used to predict the outcome.\n",
        "\n",
        "## Propensity score matching:\n",
        "\n",
        "  - Model the likehood of an observation being in $A$ using $Z$.\n",
        "  $$P(D=A|Z) = G(Z\\gamma)$$\n",
        "\n",
        "  - Make predictions $\\hat P$ or $z\\hat\\gamma$ for both samples.\n",
        "  - Match records based on $\\hat\\pi$\n",
        "  - General purpose score. \n",
        "  - May be problematic if $A$ and $B$ have very similar distributions of $Z$.  \n",
        "  - Puts more \"weight\" on the variables with different distributions between $A$ and $B$.\n",
        "     \n",
        "## PCA\n",
        "\n",
        "  - Use PCA to reduce dimensionality of $Z$ into a single index.\n",
        "    - Can use either a single dataset or both\n",
        "  - Make predictions of the first principal component $PC1$\n",
        "  - Match records based on $PC1$\n",
        "  - Puts more weight on variables that explain most of the variance in $Z$.\n",
        "\n",
        ":::\n",
        "\n",
        "## Matching: Rank Matching\n",
        "\n",
        "- Most of distance based matching is usually feasible with unconstrained matching.\n",
        "  - thus, best records are always matched.\n",
        "\n",
        "- When considering constrained matching, distance based matching may not be adecuate\n",
        "  - While first records are matched the best, last records may be matched poorly match.\n",
        "\n",
        "- A balance therefore is to use rank matching.\n",
        "  - Rank observations based on a single variable (pscore, predicted mean, etc)\n",
        "  - Match records based on rank.\n",
        "\n",
        "- No match would be \"best\", but reduces changes of poor matches.\n",
        " \n",
        "# Levy Matching Algorithm\n",
        "- At Levy, we use a constrained matching algorithm, with stratification and rank matching.\n",
        "\n",
        "## 1. Data Harmonization \n",
        "\n",
        "- Because Data files come from different data sources, they may have different variables names, coding schemes, or definitions.\n",
        "- We need to set $Z$ variables to be defined as identically as possible  in both files\n",
        "- Beyond definition harmonization, one must also be mindful of the distribution of the variables in both files.\n",
        "    - If the distribution of $Z$ is different in both files, the matching may not be adequate.\n",
        "- The weights schemes in both files should be adjusted to add up to the same population size (typically the \"recipient\" values)\n",
        "    - Weight adjustment could be done by selected strata\n",
        "\n",
        "## 2. Estimation of Matching Score\n",
        "\n",
        "- Either using full or sub (strata) samples, estimate a matching score\n",
        "  - This could be a propensity score, predicted mean, or first principal component.\n",
        "- You may want to create \"further cells\" to improve matching. (not necessarily re estimate the matching score)\n",
        "  - For example, You consider Gender as strata (two scores), but further create cells by \"age\" (5 groups)\n",
        "\n",
        "## 3. Perform the match\n",
        "\n",
        "- Using the finest definition of \"cells\", rank observations based on Matching Scores\n",
        "- Using rank, match observations till all weights are exhausted.(from either Sample)\n",
        "- \"unmatched\" observations are left for later rounds using coarser definitions of cells.\n",
        "- Matching continues until all units (recipients) are matched.\n",
        "\n",
        "## 4. Assessing the quality of the match\n",
        "\n",
        "- The idea is to compare the distribution of the \"transfered/imputed\" data with the distribution from the \"donor\" data.\n",
        "  - Overall distribution of the data will be the same by construction.\n",
        "- Compare distributions by Strata, smaller cells, or specific variables or interest.\n",
        "- Rule of thumb +/- 10% is acceptable (mean, median, Standard error).\n",
        "  - But it may depend on the variable of interest.\n",
        "  \n",
        "- One may also use other approaches like \"regression\" to compare all variables at once.\n",
        "  \n",
        "- If the distribution of the data is not adequate, one may want re-do the matching, with different \"cell\" definitions or matching scores.\n",
        "\n",
        "## Example {.scrollable}\n"
      ],
      "id": "b67cdc21"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| code-fold: false\n",
        "frause wage2, clear\n",
        "set seed 312\n",
        "xtile smp = runiform()\n",
        "replace smp=smp==1\n",
        "gen wage_s = wage if smp==1\n",
        "**three Matching scores\n",
        "** Pmm\n",
        "reg wage_s hours iq kww educ exper tenure age married black south urban sibs \n",
        "predict wageh\n",
        "** pscore\n",
        "logit smp hours iq kww educ exper tenure age married black south urban sibs \n",
        "predict pscore, xb\n",
        "** pca\n",
        "pca hours iq kww educ exper tenure age married black south urban sibs , comp(1)\n",
        "predict pc1\n",
        "\n",
        "foreach i in wageh pscore pc1 {\n",
        "\tqui:sum `i'\n",
        "\treplace `i' = (`i'-r(mean))/r(sd)\n",
        "}"
      ],
      "id": "666b0518",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we create ranks for each observation, assuming no stratification.\n"
      ],
      "id": "defcbc73"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "bysort smp (wageh) :gen rnk1=_n\n",
        "bysort smp (pscore):gen rnk2=_n\n",
        "bysort smp (pc1)   :gen rnk3=_n"
      ],
      "id": "002f12f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, the imputation. Simply \"matching\" information from the donor to the recipient."
      ],
      "id": "201378ef"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| code-fold: false\n",
        "* Imputation\n",
        "clonevar wage1 = wage_s\n",
        "clonevar wage2 = wage_s\n",
        "clonevar wage3 = wage_s\n",
        "\n",
        "gsort -smp rnk1\n",
        "replace wage1 = wage_s[rnk1] if smp==0\n",
        "\n",
        "gsort -smp rnk2\n",
        "replace wage2 = wage_s[rnk2] if smp==0\n",
        "\n",
        "gsort -smp rnk3\n",
        "replace wage3 = wage_s[rnk3] if smp==0"
      ],
      "id": "572da186",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simple quality assessment. "
      ],
      "id": "605e2ec0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "*| code-fold: false\n",
        "qui:reg wage hours iq kww educ exper tenure age married black south if smp==0\n",
        "est sto m1\n",
        "qui:reg wage1 hours iq kww educ exper tenure age married black south if smp==0\n",
        "est sto m2\n",
        "qui:reg wage2 hours iq kww educ exper tenure age married black south if smp==0\n",
        "est sto m3\n",
        "qui:reg wage3 hours iq kww educ exper tenure age married black south if smp==0\n",
        "est sto m4\n",
        "esttab m1 m2 m3 m4 , se mtitle(True Wageh pscore pca)"
      ],
      "id": "efbb8601",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Next Class: Micro Simulation\n",
        "just more imputations"
      ],
      "id": "2a5d9dc0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "nbstata",
      "language": "stata",
      "display_name": "Stata (nbstata)",
      "path": "C:\\Users\\Fernando\\AppData\\Roaming\\jupyter\\kernels\\nbstata"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}